{"id": "1703.07015", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks", "abstract": "Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) to extract short-term local dependency patterns among variables, and the Recurrent Neural Network (RNN) to discover long-term patterns and trends. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods.", "histories": [["v1", "Tue, 21 Mar 2017 00:33:36 GMT  (1198kb,D)", "https://arxiv.org/abs/1703.07015v1", null], ["v2", "Wed, 5 Jul 2017 01:24:10 GMT  (1204kb,D)", "http://arxiv.org/abs/1703.07015v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guokun lai", "wei-cheng chang", "yiming yang", "hanxiao liu"], "accepted": false, "id": "1703.07015"}, "pdf": {"name": "1703.07015.pdf", "metadata": {"source": "CRF", "title": "Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks", "authors": ["Guokun Lai", "Wei-Cheng Chang"], "emails": ["guokun@cs.cmu.edu", "wchang2@andrew.cmu.edu", "yiming@cs.cmu.edu", "hanxiaol@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, the number of people receiving benefits has multiplied, not only in the US, but in other countries as well."}, {"heading": "2 Related Background", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "3 Framework", "text": "In this section, we first formulate the problem of time series prediction and then discuss the details of the proposed LSTNet architecture (Figure 2) in the next part. Finally, we present the objective function and optimization strategy."}, {"heading": "3.1 Problem Formulation", "text": "In this paper, we are interested in the task of multivariate prediction of time series. Formally, given a series of fully observed time series signals Y = {y1, y2,..., yT}, where yt, yRn, and n are the variable dimension, we aim to predict a series of future signals in rolling fashion. Likewise, in order to predict the value of the next time stamp yT + h + 1, we assume that {y1, y2,.., yT + 1} are available. To predict the value of the next time stamp yT + h + 1, we assume that {y1, y2,.., yT, yT + 1} are available. Therefore, we formulate the input matrix for time stamp T as XT = {y1, y2,.., yT}, yT}, yRn \u00d7 T. In most cases, the horizon of the prediction task is selected in accordance with environmental protection requirements."}, {"heading": "3.2 Convolutional Component", "text": "The first layer of LSTNet is a wavy network without pooling, which aims to extract short-term patterns in the time dimension as well as local dependencies between variables. The wavy layer consists of several filters of width \u03c9 and height n (the height is set to be identical with the number of variables).The k-th filter sweeps through the input matrix X and produceshk = RELU (Wk \u0445 X + bk) (1), where \u043d denotes the convolution operation and the output hk would be a vector, and the RELU function RELU (x) = max (0, x).We create each vector hk of length T by zero padding on the input matrix X. The output matrix of the waveyor is of size dc \u00b7 T, where dc denotes the number of filters."}, {"heading": "3.3 Recurrent Component", "text": "The recurrent component is a recursive layer with the gated recurrent unit (GRU) [6] and uses the RELU function as a hidden update function. The hidden state of the recursive units at a given time t is calculated as, rt = \u03c3 (xtWxr + ht \u2212 1Whr + br) ut = \u03c3 (xtWxu + ht \u2212 1Whu + bu) ct = RELU (xtWxc + rt (ht \u2212 1Whc) + bc) ht = (1 \u2212 ut) ht \u2212 1 + ut ct (2), where the element-wise product is, whereas \u03c3 is the sigmoid function and xt the input of this layer at a given time. The output of this layer is the hidden state at each time stamp. While researchers are accustomed to using tanh function as a hidden function, it leads us back to the activation."}, {"heading": "3.4 Recurrent-skip Component", "text": "The recurrent layers with GRU [6] and LSTM [15] units are carefully designed to store historical information and thus become aware of relatively long-term dependencies. However, due to the disappearance of gradients, GRU and LSTM usually fail to capture very long-term correlation in practice. If we want to predict power consumption in t o'clock for today, a classic trick in the seasonal forecasting model that implements the periodic patterns in the real world \u2212 rt-time is next to the most recent records. This kind of dependencies can hardly be captured by off-theshelf recurrent units that have problems due to the extremely long length of a period (24 hours) and the subsequent optimization."}, {"heading": "3.5 Autoregressive Component", "text": "Due to the non-linear nature of the convolutionary and recursive components, a major disadvantage of the neural network model is that the scale of output is not sensitive to the scale of input. Unfortunately, in certain real datasets, the scale of input signals is constantly changing in a non-periodic manner, which significantly reduces the predictive accuracy of the neural network model. A concrete example of this failure is given in Section 4.6. To address this deficiency, we decompile the final prediction of LSTNet into a linear part that focuses primarily on the problem of local scaling, plus a non-linear part with recurring patterns. In the LSTNet architecture, we adopt the classical autoregressive (AR) model as a linear component. Let's call the predictive result of the AR component the hLt matrix and the coefficients of the AR model as the input Y-Rqar, where the component is."}, {"heading": "3.6 Objective function", "text": "The squared error is the standard loss function for many forecasting tasks, the corresponding optimization goal is formulated in such a way that the set of timestamps used for training is the Frobenius standard, and h is the horizon, as mentioned in Section 3.1. Traditional linear regression model with the quadratic loss function is called a linear ridge, which corresponds to the vector autoregressive model with crest control. However, experiments show that linear support vector regression (linear SVR) [30] dominates the linear ridge model in certain datasets. The only difference between linear SVR and linear ridge is the objective function of linear SVR. The objective function for linear SVR is to minimize linear losses (linear SVR) in certain datasets."}, {"heading": "3.7 Optimization Strategy", "text": "Our optimization strategy is the same as in the traditional prediction model for time series. Suppose the input time series is Y = {y1, y2,..., yT}, we define a tunable window size q and reformulate the input in the timestamp t as Xt = {yt \u2212 q + 1, yt \u2212 q + 2,...., yt}. The problem then becomes a regression task with a series of attribute-value pairs {Xt, yt + h} and can be solved by stochastic gradient decimence (SGD) or its variants like Adam [18]."}, {"heading": "4 Evaluation", "text": "We conducted extensive experiments with 8 methods (including our new methods) on 4 benchmark datasets for prediction tasks in time series."}, {"heading": "4.1 Methods for Comparison", "text": "The methods used in our comparative evaluation are the following. \u2022 AR stands for the auto-regressive model, which corresponds to the one-dimensional VAR model. \u2022 LSVR is the vector auto-regression model (VAR) with L2 regression, which is most popular for predicting multivariate time series. \u2022 LSVR is the vector auto-regression model (VAR) with support for the vector regression objective function [30]. \u2022 TRMF is the auto-regressive model with time-regulated matrix factorization by [32]. \u2022 GP is the Gaussian process for modelling time series. [11, 28] \u2022 VAR-MLP is the model proposed in [34], combining multilayer perception (MLP) and auto-regressive models. \u2022 LST-L1 is our proposed LSTNet model (Section 3) with the absolute loss object Func-Linell-LST \u2022 our single-LomLAR model is a semi-LomLAR."}, {"heading": "4.2 Metrics", "text": "We used three conventional evaluation variables defined as: \u2022 Root Relative Squared Error (RSE): RSE = \u221a (i, t) and Relative Absolute Error (RAE) RAE = \u2211 (i, t). \u2022 Relative Absolute Error (RAE) RAE = \u2211 (i, t). \u2022 Test: Yit \u2212 Y (i, t) and Test: Yit \u2212 mean (Y). (11) \u2022 Empirical correlation coefficient (CORR) CORR = 1n; i = 1t (Yit \u2212 mean (Y i)) (Y \u2212 it \u2212 mean (Y-i))."}, {"heading": "4.3 Data", "text": "We used four benchmark data sets that are publicly available. Table 1 summarizes the corpus statistics. \u2022 Electricity2: Electricity consumption in kWh was recorded every 15 minutes from 2012 to 2014, for n = 321 customers. We converted the data to reflect hourly consumption; \u2022 Traffic3: A collection of 48 months (2015-2016) hourly data from the California Department of Transportation. The data describe road utilization rates (ranging from 0 to 1) measured by various sensors on the San Francisco Bay Area freeways. \u2022 Solar-Energy4: the records of solar power production in 2006 collected every 10 minutes from 137 PV plants in Alabama State. \u2022 Exchange rate: the collection of daily exchange rates from eight foreign countries including Australia, the United Kingdom, Canada, Switzerland, China, New Zealand and Singapore from 1990 to 2016. All data sets were broken down into training sets (60%), validation set (20%) in chronological order."}, {"heading": "4.4 Experimental Details", "text": "We perform a network search of all tunable hyperparameters on the provided validation set for each method and each dataset. Specifically, all methods share the same search range of the grid of window size q, which is sufficient when using {20, 21,.., 29}. For LRidge and LSVR, the regulation coefficient \u03bb is chosen of {2 \u2212 10, 2 \u2212 8,.., 28, 210}. For GP, the hidden dimension of {22,., 26} is chosen and the regulation coefficient \u03bb of {0,1, 1, 10}. For LST-L1 and LSTL2,., 28, 210}. For TRMF, the hidden dimension of {22,.., 26} is chosen and the regulation coefficient \u03bb of {0,1, 1, 10}. For LST-L1 and LSTL2, we have adopted the training strategy described in Section 3.7. The hidden dimension of the recurrent and convolutional layer is {22,.., 26} and the regulation coefficient \u03bb of {0,1, 1, 10}. For LST-L1 and LSTL2, we have adopted the training strategy described in Section 3.7."}, {"heading": "4.5 Main Results", "text": "Table 2 summarizes the evaluation results of all methods (8) for all test sets (4) in all measurements (3). We set the horizon = {3, 6, 12, 24}, which means that the horizons are set from 3 to 24 hours for predicting the current and traffic data, from 30 to 240 minutes for data on solar energy, and from 3 to 24 days for data on the exchange rate. The larger the horizon, the more difficult the prediction tasks. The best result for each (data, metric) pair is highlighted in bold front in this table. The total number of bold results is 28 for LST-L1 (a version of the proposed LSTNet), 10 for LST-L2 (the other version of our LSTNet), 5 for AR, 4 for LRidge, and between 0 and 2 for the rest of the data flood."}, {"heading": "4.6 Ablation Study", "text": "To examine the importance of each component in our framework, we performed a series of ablation tests, using the following notation for different settings in the study. \u2022 LSTw / oskip: The LSTNet models (LST-L1 or LST-L2) without the recurrent skip component. \u2022 LSTw / oCNN: The LSTNet models (LST-L1 or LST-L2) without the convolutional component. \u2022 LSTw / oAR: The LSTNet models (LST-L1 or LST-L2) without the AR component. The test results measured with RSE are shown in Figure 57. Several observations from these results are worth highlighting: \u2022 The best result in each dataset is achieved with either LST-L1 or LST-L2; which of them is better depends on the AR data component. \u2022 The removal of the AR component (in LSTw / ow) from the most significant performance breaks caused by the model."}, {"heading": "4.7 Mixture of long- and short-term patterns", "text": "To illustrate the success of LSTNet in modelling the mix of short-term and long-term recurrent patterns in time series data, Figure 6 compares the performance of LSTNet and VAR on a particular time series (one of the output variables) in the traffic dataset. As discussed in Section 4.3, the traffic data show two types of repetitive patterns, namely daily and weekly. Figure 6 shows that the actual patterns (in blue) of traffic occupancy on Fridays and Saturdays and another on Sundays and Mondays are very different. However, the VAR model (part (a) of the figure) cannot learn such a distinction, but instead predicts the similar local patterns for both days. LSTNet, on the other hand (part (b) of the figure) successfully captures both the different repetitive patterns on Mondays and Saturdays as well as the local patterns within each day."}, {"heading": "5 Conclusion", "text": "In this paper, we introduced a novel Deep Learning Framework (LSTNet) for the task of multivariate prediction of time series. By combining the strengths of Convolutionary and Recurrent Neural Networks with an autoregressive component, the proposed approach greatly improved current results in predicting time series across multiple benchmark datasets. In-depth analysis and empirical evidence, we demonstrate that LSTNet does indeed capture both short-term and long-term repeating patterns in data, combining both linear and non-linear models for robust predictions. For future research, we would like to expand the LSTNet framework for detecting anomalies from time series data and for causality analysis based on the development of long- and short-term patterns."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Time series analysis: forecasting and control", "author": ["G.E. Box", "G.M. Jenkins", "G.C. Reinsel", "G.M. Ljung"], "venue": "John Wiley & Sons", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Distribution of residual autocorrelations in autoregressiveintegrated moving average time series models", "author": ["G.E. Box", "D.A. Pierce"], "venue": "Journal of the American statistical Association, 12  65(332):1509\u20131526", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1970}, {"title": "Support vector machine with adaptive parameters in financial time series forecasting", "author": ["L.-J. Cao", "F.E.H. Tay"], "venue": "IEEE Transactions on neural networks, 14(6):1506\u20131518", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Z. Che", "S. Purushotham", "K. Cho", "D. Sontag", "Y. Liu"], "venue": "arXiv preprint arXiv:1606.01865", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent networks and narma modeling", "author": ["J. Connor", "L.E. Atlas", "D.R. Martin"], "venue": "NIPS, pages 301\u2013308", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Nonlinear dynamic boltzmann machines for time-series prediction", "author": ["S. Dasgupta", "T. Osogami"], "venue": "AAAI-17. Extended research report available at goo. gl/Vd0wna", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Bayesian inference and learning in gaussian process state-space models with particle mcmc", "author": ["R. Frigola", "F. Lindsten", "T.B. Sch\u00f6n", "C.E. Rasmussen"], "venue": "Advances in Neural Information Processing Systems, pages 3156\u20133164", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian Time Series Learning with Gaussian Processes", "author": ["R. Frigola-Alcade"], "venue": "PhD thesis, PhD thesis, University of Cambridge", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series analysis", "author": ["J.D. Hamilton"], "venue": "volume 2. Princeton university press Princeton", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Deep", "author": ["N.Y. Hammerla", "S. Halloran", "T. Ploetz"], "venue": "convolutional, and recurrent models for human activity recognition using wearables. arXiv preprint arXiv:1604.08880", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["G. Hinton", "L. Deng", "D. Yu"], "venue": "E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u2013 97", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Hybrid neural network models for hydrologic time series forecasting", "author": ["A. Jain", "A.M. Kumar"], "venue": "Applied Soft Computing, 7(2):585\u2013592", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Financial time series forecasting using support vector", "author": ["K.-j. Kim"], "venue": "machines. Neurocomputing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal convolutional networks: A unified approach to action segmentation", "author": ["C. Lea", "R. Vidal", "A. Reiter", "G.D. Hager"], "venue": "Computer Vision\u2013ECCV 2016 Workshops, pages 47\u201354. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Forecasting macroeconomic time series: Lasso-based approaches and their forecast combinations with dynamic factor models", "author": ["J. Li", "W. Chen"], "venue": "International Journal of Forecasting, 30(4):996\u20131015", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to diagnose with lstm recurrent neural networks", "author": ["Z.C. Lipton", "D.C. Kale", "C. Elkan", "R. Wetzell"], "venue": "arXiv preprint arXiv:1511.03677", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "New introduction to multiple time series analysis", "author": ["H. L\u00fctkepohl"], "venue": "Springer Science & Business Media", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "General exponential smoothing and the equivalent arma process", "author": ["E. McKenzie"], "venue": "Journal of Forecasting, 3(3):333\u2013344", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1984}, {"title": "Estimating structured vector autoregressive model", "author": ["I. Melnyk", "A. Banerjee"], "venue": "arXiv preprint arXiv:1602.06606", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust estimation of transition matrices in high dimensional heavy-tailed vector autoregressive processes", "author": ["H. Qiu", "S. Xu", "F. Han", "H. Liu", "B. Caffo"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1843\u20131851", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Gaussian processes for time-series modelling", "author": ["S. Roberts", "M. Osborne", "M. Ebden", "S. Reece", "N. Gibson", "S. Aigrain"], "venue": "Phil. Trans. R. Soc. A, 371", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1984}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1505.00387", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["V. Vapnik", "S.E. Golowich", "A. Smola"], "venue": "Support vector method for function approximation, regression estimation, and signal processing. Advances in neural information processing systems, pages 281\u2013287", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep convolutional neural networks on multichannel time series for human activity recognition", "author": ["J.B. Yang", "M.N. Nguyen", "P.P. San", "X.L. Li", "S. Krishnaswamy"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI), Buenos Aires, Argentina, pages 25\u201331", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Temporal regularized matrix factorization for highdimensional time series prediction", "author": ["H.-F. Yu", "N. Rao", "I.S. Dhillon"], "venue": "Advances in Neural Information Processing Systems, pages 847\u2013855", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Forecasting with artificial neural networks:: The state of the art", "author": ["G. Zhang", "B.E. Patuwo", "M.Y. Hu"], "venue": "International journal of forecasting, 14(1):35\u201362", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Time series forecasting using a hybrid arima and neural network model", "author": ["G.P. Zhang"], "venue": "Neurocomputing, 50:159\u2013175", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 11, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 21, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 31, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 33, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 8, "context": "The recurrent neural networks (RNN) models [9], for example, have become most popular in recent natural language processing (NLP) research.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 13, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 18, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 18, "context": "In the field of computer vision, as another example, convolution neural network (CNN) models [19, 21] have shown outstanding performance by successfully extracting local and shiftinvariant features (called \u201dshapelets\u201d sometimes) at various granularity levels from input images.", "startOffset": 93, "endOffset": 101}, {"referenceID": 20, "context": "In the field of computer vision, as another example, convolution neural network (CNN) models [19, 21] have shown outstanding performance by successfully extracting local and shiftinvariant features (called \u201dshapelets\u201d sometimes) at various granularity levels from input images.", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": "For instance, RNN architectures have been studied for extracting informative patterns from health-care sequential data [5, 23] and classifying the data with respect diagnostic categories.", "startOffset": 119, "endOffset": 126}, {"referenceID": 22, "context": "For instance, RNN architectures have been studied for extracting informative patterns from health-care sequential data [5, 23] and classifying the data with respect diagnostic categories.", "startOffset": 119, "endOffset": 126}, {"referenceID": 12, "context": "RNN has also been applied to mobile data, for classifying the input sequences with respect to actions or activities [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "CNN models have also been used in action/activity recognition [13, 20, 31], for the extraction of shift-invariant local patterns from input sequences as the features of classification models.", "startOffset": 62, "endOffset": 74}, {"referenceID": 19, "context": "CNN models have also been used in action/activity recognition [13, 20, 31], for the extraction of shift-invariant local patterns from input sequences as the features of classification models.", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "CNN models have also been used in action/activity recognition [13, 20, 31], for the extraction of shift-invariant local patterns from input sequences as the features of classification models.", "startOffset": 62, "endOffset": 74}, {"referenceID": 6, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 17, "endOffset": 20}, {"referenceID": 15, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 43, "endOffset": 55}, {"referenceID": 32, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 43, "endOffset": 55}, {"referenceID": 33, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 43, "endOffset": 55}, {"referenceID": 2, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 28, "context": "Finally, the LSTNet incorporates a traditional autoregressive linear model in parallel to the non-linear neural network part; which is similar to a highway component [29].", "startOffset": 166, "endOffset": 170}, {"referenceID": 1, "context": "The popularity of the ARIMA model is due to its statistical properties as well as the well-known Box-Jenkins methodology [2] in the model selection procedure.", "startOffset": 121, "endOffset": 124}, {"referenceID": 24, "context": "ARIMA models are not only adaptive to various exponential smoothing techniques [25] but also flexible enough to subsume other types of time series models including autoregression (AR), moving average (MA) and Autoregressive Moving Average (ARMA).", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "However, ARIMA models, including their variants for modeling long-term temporal dependencies [2], are rarely used in high dimensional multivariate time series forecasting due to their high computational cost.", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series [2, 12, 24] due to its simplicity.", "startOffset": 115, "endOffset": 126}, {"referenceID": 11, "context": "On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series [2, 12, 24] due to its simplicity.", "startOffset": 115, "endOffset": 126}, {"referenceID": 23, "context": "On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series [2, 12, 24] due to its simplicity.", "startOffset": 115, "endOffset": 126}, {"referenceID": 26, "context": "Significant progress has been made in recent years in a variety of VAR models, including the elliptical VAR model [27] for heavy-tail time series and structured VAR model [26] for better interpretations of the dependencies between high dimensional variables, and more.", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "Significant progress has been made in recent years in a variety of VAR models, including the elliptical VAR model [27] for heavy-tail time series and structured VAR model [26] for better interpretations of the dependencies between high dimensional variables, and more.", "startOffset": 171, "endOffset": 175}, {"referenceID": 31, "context": "To alleviate this issue, [32] proposed to reduce the original high dimensional signals into lower dimensional hidden representations, then applied VAR for forecasting with a variety choice of regularization.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "support vector regression (SVR) [4, 17] learns a max margin hyperplane based on the regression loss with a hyper-parameter controlling the threshold of prediction errors.", "startOffset": 32, "endOffset": 39}, {"referenceID": 16, "context": "support vector regression (SVR) [4, 17] learns a max margin hyperplane based on the regression loss with a hyper-parameter controlling the threshold of prediction errors.", "startOffset": 32, "endOffset": 39}, {"referenceID": 21, "context": "Lastly, [22] applied LASSO models to encourage sparsity in the model parameters so that interesting patterns among different input signals could be manifest.", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "GP can be applied to multivariate time series forecasting task as suggested in [28], and can be used as a prior over the function space in Bayesian inference.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "For example, [10] presented a fully Bayesian approach with the GP prior for nonlinear state-space models, which is capable of capturing complex dynamical phenomena.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The Recurrent component is a recurrent layer with the Gated Recurrent Unit (GRU) [6] and uses the RELU function as the hidden update activation function.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "The Recurrent layers with GRU [6] and LSTM [15] unit are carefully designed to memorize the historical information and hence to be aware of relatively long-term dependencies.", "startOffset": 30, "endOffset": 33}, {"referenceID": 14, "context": "The Recurrent layers with GRU [6] and LSTM [15] unit are carefully designed to memorize the historical information and hence to be aware of relatively long-term dependencies.", "startOffset": 43, "endOffset": 47}, {"referenceID": 28, "context": "To address this deficiency, similar in spirit to the highway network [29], we decompose the final prediction of LSTNet into a linear part, which primarily focuses on the local scaling issue, plus a non-linear part containing recurring patterns.", "startOffset": 69, "endOffset": 73}, {"referenceID": 29, "context": "However, experiments show that the Linear Support Vector Regression (Linear SVR) [30] dominates the Linear Ridge model in certain datasets.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "The problem then becomes a regression task with a set of feature-value pairs {Xt,yt+h}, and can be solved by Stochastic Gradient Decent (SGD) or its variants such as Adam [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "\u2022 LSVR is the vector autoregression (VAR) model with Support Vector Regression objective function [30] .", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "\u2022 TRMF is the autoregressive model using temporal regularized matrix factorization by [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "[11, 28] \u2022 VAR-MLP is the model proposed in [34] that combines Multilayer Perception (MLP) and autoregressive model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[11, 28] \u2022 VAR-MLP is the model proposed in [34] that combines Multilayer Perception (MLP) and autoregressive model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "[11, 28] \u2022 VAR-MLP is the model proposed in [34] that combines Multilayer Perception (MLP) and autoregressive model.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "The Adam[18] algorithm is utilized to optimize the parameters of our model.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these realworld applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Longand Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) to extract short-term local dependency patterns among variables, and the Recurrent Neural Network (RNN) to discover long-term patterns and trends. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. The dataset and experiment code both are uploaded to Github.", "creator": "LaTeX with hyperref package"}}}