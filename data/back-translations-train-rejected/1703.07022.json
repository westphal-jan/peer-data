{"id": "1703.07022", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation", "abstract": "A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.", "histories": [["v1", "Tue, 21 Mar 2017 01:43:12 GMT  (2327kb,D)", "https://arxiv.org/abs/1703.07022v1", "10 pages, 6 figures"], ["v2", "Thu, 23 Mar 2017 20:06:15 GMT  (3728kb,D)", "http://arxiv.org/abs/1703.07022v2", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["xiaodan liang", "zhiting hu", "hao zhang", "chuang gan", "eric p xing"], "accepted": false, "id": "1703.07022"}, "pdf": {"name": "1703.07022.pdf", "metadata": {"source": "CRF", "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation", "authors": ["Xiaodan Liang", "Zhiting Hu", "Hao Zhang", "Chuang Gan", "Eric P. Xing"], "emails": ["xiaodan1@cs.cmu.edu", "zhitingh@cs.cmu.edu", "hao@cs.cmu.edu", "ganchuang1990@gmail.com", "epxing@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before to an agreement between the two countries."}, {"heading": "2. Related Work", "text": "In this context, it should be noted that the measures we have mentioned are not only measures, but also measures that must be taken by politicians to defuse and defuse the situation."}, {"heading": "3. Recurrent Topic-Transition GAN", "text": "The proposed Recurrent Topic-Transition GAN (RTTGAN) aims to describe the rich content of a given image / video by generating a paragraph of natural language. Figure 2 provides an overview of the frame. In an input image, we first recognize a series of semantic regions using a method of dense subtitling [12]. Each semantic region is represented with a visual feature vector and a short text phrase (e.g. person on a horse). The paragraph generator then generates sequentially significant sentences by selectively incorporating the fine-grained visual and textual cues. To ensure high-quality single sentences and a coherent whole paragraph, we apply a sentence discriminator and a theme transition discriminator to each sentence generated to measure the plausibility and smoothness of the semantic transition with preceding sentences. The generator and multilevel discriminators are jointly learned within an adverse frame, each image sequencing RTTGAN, but also supporting a sequenced image sequence of image pairs with each other."}, {"heading": "3.1. Adversarial Objective", "text": "The question of the causes and the causes of the crisis arises not only in the question of the causes of the crisis, but also in the question of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the causes of the crisis, of the causes of the causes of the causes of the crisis, of the causes of the causes of the crisis, the causes of the causes of the causes of the crisis, the causes of the causes of the causes of the crisis, the causes of the causes of the causes of the causes of the crisis, the causes of the causes of the causes of the causes of the crisis, the causes of the causes of the crisis, the causes of the causes of the causes of the causes of the crisis, the causes of the causes of the causes of the crisis, the causes of the causes of the causes of the crisis, the causes of the causes of the causes of the crisis, the causes of the causes of the crisis, the causes of the causes of the causes of the crisis and the causes of the crisis, the causes of the causes of the crisis, the causes of the causes of the causes of the crisis, the causes of the causes after the causes of the causes of the crisis, and the causes after the causes and the causes of the"}, {"heading": "3.2. Paragraph Generator", "text": "Figure 3 shows the architecture of the semantic regions of the image and their local phrases. Each region retains different levels of context states with a hierarchy defined by a paragraph RNN, a sentence RNN, and a word RNN, and two attention modules. First, the paragraph RNN encodes the current paragraph state based on all previous sentences. Second, the spatial visual attention of the viewer is selectively directed to semantic regions with the guidance of the current paragraph state in order to produce the visual representation of the sentence. Therefore, the sentence RNN is able to encode a topic that contains a vector for the new sentence. Third, the Language Attention Module learns to integrate linguistic knowledge into the local phrases of the focused semantic regions in order to facilitate word generation by the word RNN region. Faced with an input image, we adopt the se model [12] of the visual regions to generate their local and visual phrases."}, {"heading": "3.3. Paragraph Discriminators", "text": "The paragraph discriminators {Ds, Dr} aim to distinguish between real and synthesized paragraphs based on the linguistic characteristics of a natural paragraph description. Specifically, the sentence discriminator Ds evaluates the plausibility of individual sentences, while the topic transition discriminator Dr evaluates the topic coherence of all sentences generated to date. By means of such a multi-level evaluation, the model is able to create long but realistic descriptions. Specifically, the sentence discriminator Ds is an LSTM RNN that repeatedly takes every word embedded within a sentence as an input and calculates the factual flatness value of the currently constructed paragraph description at each recurring step."}, {"heading": "3.4. Implementation Details", "text": "The discriminators Ds and Dr are both implemented as single-layer LSTM with a hidden dimension of 512. To the generator, the RNN paragraph is a single-layer LSTM with a hidden size of 512 and the initial hidden memory cells set to zero. Likewise, the RNN sentence and the word RNN are single-layer LSTMs with a hidden dimension of 1024 and 512. Each input word is encoded as an embedding vector of 512 dimensions. The visual characteristic vj of each semantic region has a dimension of 4096. The opposing frame is trained according to the Waterstone GAN (WGAN), in which we alternate between the optimization of {Ds, Dr} with equivalent (2) and the optimization of G with equivalent (4)."}, {"heading": "4. Experiments", "text": "In this section, we perform detailed comparisons with state-of-the-art visual paragraph generation methods in both monitored and semi-monitored environments."}, {"heading": "4.1. Experimental Settings", "text": "To generate a paragraph for an image, we continue to run the paragraph generator until the STOP sentence status is predicted, or after Smax = 6 sentences, whichever comes first. RNN is redirected repeatedly to try out the most likely word in each time step, and stops after selecting the STOP token or after Nmax = 30 words. We use beam size 2 search to generate paragraph descriptions. Training details are presented in Section 3.4, and all models are implemented in the torch platform. With regard to the fully monitored setting, to make a fair comparison with the most modern methods [13, 16], the experiments are conducted on the basis of the public paragraph, using 14,575 picture paragraph pairs for training, 2,487 for validation and 2,489 for testing. With regard to semi-monitored setting, our RTT-GAN is combined with the individual sentences that are provided in paragraph pairs."}, {"heading": "4.2. Comparison with the State-of-the-arts", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We've never waited so long to be able to find a solution, \"he said.\" But we've never waited so long to be able to find a solution, \"he said.\" We've never waited so long to be able to find a solution, \"he said.\" We've never waited so long to be able to find a solution, \"he said."}, {"heading": "4.3. The Importance of Adversarial Training", "text": "After eliminating the discriminators during model optimization in both fully and semi-monitored settings (i.e. RTT-GAN (full-without discriminator) and RTT-GAN (half-without discriminator)), we observe consistent performance declines across all metrics compared to the full models, i.e. 1.80% and 4.11% for CIDER, respectively. RTT-GAN (half-without discriminator) can be considered a caption model, similar to Sentence-Concat, due to the lack of adversarial losses. It justifies that the plausibility of the sentence and the theme coherence with previous sentences are very important in generating long, compelling stories. Furthermore, the loss of pure word predictions largely impedes the extension of the model to unattended or semi-monitored generative modelling of the model."}, {"heading": "4.4. The Importance of Region-based Attention", "text": "We also evaluate the effectiveness of spatial visual attention and linguistic attention mechanisms to facilitate paragraph prediction, as shown in Table 2. RTT-GAN (Fully- w / o phrase att) directly bundles the visual characteristics of all regions into a compact representation for sequential sentence prediction, such as region hierarchical. RTT-GAN (Fully- w / o phrase att) represents the variant that removes the linguistic attention module. It can be observed that the attention mechanism effectively facilitates prediction of RTT-GAN by including selectively appropriate visual and linguistic keywords. In particular, the advantages of explicit use of words from local formulations suggest that transferring visual linguistic knowledge from more basic tasks (e.g. recognition) is beneficial for generating high-level and holistic descriptions. As an exploratory experiment, we examine the generation of 10 generative sentences (and 50) of certain regions (with only a small number of paragraphs)."}, {"heading": "4.5. Personalized Paragraph Generation", "text": "Unlike previous work, our model supports personalized paragraph generation, which generates diverse descriptions by manipulating the first sentences. It can be easily achieved by initializing the RNN paragraph with the embedding of a predefined first sentence. The generator can output different and thematic sentences in succession to form a personalized paragraph for an image. Qualitative results of our model are presented in Figure 6. Interesting features of our predictions include its use of the co-references in the first sentence and its ability to capture thematic relationships with previous sentences. In view of the first sentences, the following sentences give some details about the scene elements previously mentioned in the description and also combine with other related contents. We also report on the human evaluation results in Table 3 based on randomly selected 100 test images, comparing three model variants, i.e. RTT-GAN (Semiw / o discriminator), RTT-GAN (Semi-), RTT-GAN (Semi) and Semi (Semi)."}, {"heading": "4.6. Extension to Video Domain", "text": "As in Table 4, we also extend our RTT-GAN to the task of video paragraph generation and evaluate it on the TACoSMultiLevel dataset [24], which contains 185 long videos filmed indoors. [35] In order to model the spatial appearance, we also extract 50 semantic regions for the images every second. To capture the motion patterns, we extend the feature representation with motion features. Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a feature vector of fixed length on all 16 images."}, {"heading": "5. Conclusion and Future Work", "text": "In this article, we propose a recurrent topic-transition GAN (RTT-GAN) for visual paragraph generation. Thanks to the contrarian generative modelling, our RTT-GAN is able to generate different paragraphs if only initial sentence annotations are given for training. It integrates visual attention and linguistic attention mechanisms to repeatedly ponder over fine-grained semantic regions. Two discriminators assess the quality of generated paragraphs from two aspects: sentence plausibility and theme transition coherence. Extensive experiments demonstrate the effectiveness of our model in both fully monitored and semi-monitored environments. In the future, we will extend our generative model to other visual tasks that require shared visual and linguistic modeling."}], "references": [{"title": "Wasserstein gan", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "ICCV, pages 2422\u20132431,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual  recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, pages 2625\u20132634,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, pages 15\u201329,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, pages 2672\u20132680,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["J. Gu", "Z. Lu", "H. Li", "V.O. Li"], "venue": "ACL,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013 899,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Controllable text generation", "author": ["Z. Hu", "Z. Yang", "X. Liang", "R. Salakhutdinov", "E.P. Xing"], "venue": "arXiv preprint arXiv:1703.00955,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Visual storytelling", "author": ["T.K. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R.B. Girshick", "X. He", "P. Kohli", "D. Batra", "C.L. Zitnick", "D. Parikh", "L. Vanderwende", "M. Galley", "M. Mitchell"], "venue": "NAACL- HLT, pages 1233\u20131239,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 4565\u20134574,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 3128\u20133137,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F.F.F. Li"], "venue": "NIPS, pages 1889\u20131897,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A hierarchical approach for generating descriptive image paragraphs", "author": ["J. Krause", "J. Johnson", "R. Krishna", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "ACL,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep variation-structured reinforcement learning for visual relationship and attribute detection", "author": ["X. Liang", "L. Lee", "E.P. Xing"], "venue": "arXiv preprint arXiv:1703.03054,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP, pages 899\u2013907,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "German Conference on Pattern Recognition, pages 184\u2013195. Springer,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "ICCV, pages 433\u2013440,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.  COURSERA: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "C3D: generic features for video analysis", "author": ["D. Tran", "L.D. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "CoRR, abs/1412.0767, 2:7,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Msr-vtt: A large video description dataset for bridging video and language", "author": ["J. Xu", "T. Mei", "T. Yao", "Y. Rui"], "venue": "CVPR, pages 5288\u20135296,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML, volume 14, pages 77\u201381,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Review networks for caption generation", "author": ["Z. Yang", "Y. Yuan", "Y. Wu", "W.W. Cohen", "R.R. Salakhutdinov"], "venue": "NIPS, pages 2361\u20132369,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "ICCV, pages 4507\u20134515,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "CVPR, pages 4651\u20134659,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u2013 78,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CVPR, pages 4584\u20134593,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "arXiv preprint arXiv:1609.05473,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 2, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 3, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 30, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 22, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 32, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 21, "context": "datasets [22, 34, 17] pairing images with natural language descriptions.", "startOffset": 9, "endOffset": 21}, {"referenceID": 33, "context": "datasets [22, 34, 17] pairing images with natural language descriptions.", "startOffset": 9, "endOffset": 21}, {"referenceID": 16, "context": "datasets [22, 34, 17] pairing images with natural language descriptions.", "startOffset": 9, "endOffset": 21}, {"referenceID": 29, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 32, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 22, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 30, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 9, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 15, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 34, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 28, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 15, "context": "Existing methods [16, 35, 19] deterministically optimizing over single annotated paragraph thus suffer from losing massive information expressed in the image.", "startOffset": 17, "endOffset": 29}, {"referenceID": 34, "context": "Existing methods [16, 35, 19] deterministically optimizing over single annotated paragraph thus suffer from losing massive information expressed in the image.", "startOffset": 17, "endOffset": 29}, {"referenceID": 18, "context": "Existing methods [16, 35, 19] deterministically optimizing over single annotated paragraph thus suffer from losing massive information expressed in the image.", "startOffset": 17, "endOffset": 29}, {"referenceID": 5, "context": "Inspired by Generative Adversarial Networks (GANs) [6], we establish an adversarial training mechanism between a structured paragraph generator and multi-level paragraph discriminators, where the discriminators learn to distinguish between real and synthesized paragraphs while the generator aims to fool the discriminators by generating diverse and realistic paragraphs.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Early works that posed this problem as a ranking and template retrieval tasks [5, 8, 14] performed poorly since it is hard to enumerate all possibilities in one collected dataset due to the compositional nature of language.", "startOffset": 78, "endOffset": 88}, {"referenceID": 7, "context": "Early works that posed this problem as a ranking and template retrieval tasks [5, 8, 14] performed poorly since it is hard to enumerate all possibilities in one collected dataset due to the compositional nature of language.", "startOffset": 78, "endOffset": 88}, {"referenceID": 13, "context": "Early works that posed this problem as a ranking and template retrieval tasks [5, 8, 14] performed poorly since it is hard to enumerate all possibilities in one collected dataset due to the compositional nature of language.", "startOffset": 78, "endOffset": 88}, {"referenceID": 17, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 2, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 3, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 30, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 22, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 32, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 19, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 2, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 3, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 30, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 32, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 3, "context": "Similar success has been already seen in video captioning fields [4, 32].", "startOffset": 65, "endOffset": 72}, {"referenceID": 31, "context": "Similar success has been already seen in video captioning fields [4, 32].", "startOffset": 65, "endOffset": 72}, {"referenceID": 11, "context": "Dense captioning [12] is recently proposed to describe each region of interest with a short phrase, considering more details than standard image captioning.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 20, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 34, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 15, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 34, "context": "[35] generate multi-sentence video descriptions for cooking videos to capture strong temporal dependencies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] combine semantics of all regions of interest to produce a generic paragraph for an image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Given an input image, we first detect a set of semantic regions using dense captioning method [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "As the original GAN [6] that optimizes over binary probability distance suffers from mode collapse and instable convergence, we follow the new Wasserstein GAN [1] method that theoretically remedies this by minimizing an approximated Wasserstein distance.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "As the original GAN [6] that optimizes over binary probability distance suffers from mode collapse and instable convergence, we follow the new Wasserstein GAN [1] method that theoretically remedies this by minimizing an approximated Wasserstein distance.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "The discrete nature of text samples hinders gradient back-propagation from the discriminators to the generator [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 35, "context": "We address this issue following SeqGAN [36].", "startOffset": 39, "endOffset": 43}, {"referenceID": 35, "context": "The gradients are passed back to the intermediate action value via Monte Carlo search [36].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Given an input image, we adopt the dense captioning model [12, 16] to detect semantic regions of the image and generate their local phrases.", "startOffset": 58, "endOffset": 66}, {"referenceID": 15, "context": "Given an input image, we adopt the dense captioning model [12, 16] to detect semantic regions of the image and generate their local phrases.", "startOffset": 58, "endOffset": 66}, {"referenceID": 6, "context": "Following the copy mechanism [7] firstly proposed in natural language processing, we selectively incorporate the embeddings of local phrases based on the topic vector ht and preceding word state ht,i\u22121, i \u2208 {1, .", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "The adversarial framework is trained following the Wasserstein GAN (WGAN) [1] in which we alternate between the optimization of {D, D}with Eq.", "startOffset": 74, "endOffset": 77}, {"referenceID": 26, "context": "We use minibatch SGD and apply the RMSprop solver [27] with the initial learning rate set to 0.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "For stable training, we apply batch normalization [11] and", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "In terms of the fully-supervised setting, to make a fair comparison with the state-of-the-art methods [13, 16], the experiments are conducted on the public image paragraph dataset [16], where 14,575 image-paragraph pairs are used for training, 2,487 for validation and 2,489 for testing.", "startOffset": 102, "endOffset": 110}, {"referenceID": 15, "context": "In terms of the fully-supervised setting, to make a fair comparison with the state-of-the-art methods [13, 16], the experiments are conducted on the public image paragraph dataset [16], where 14,575 image-paragraph pairs are used for training, 2,487 for validation and 2,489 for testing.", "startOffset": 102, "endOffset": 110}, {"referenceID": 15, "context": "In terms of the fully-supervised setting, to make a fair comparison with the state-of-the-art methods [13, 16], the experiments are conducted on the public image paragraph dataset [16], where 14,575 image-paragraph pairs are used for training, 2,487 for validation and 2,489 for testing.", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "In terms of semi-supervised setting, our RTT-GAN is trained with the single sentence annotations provided in MSCOCO image captioning dataset [2] which contains 123,000 images.", "startOffset": 141, "endOffset": 144}, {"referenceID": 15, "context": "For both fully-supervised and semi-supervised settings, we use the word vocabulary of image-paragraph dataset as [16] does and the 14,575 paragraph descriptions on public image paragraph dataset [16] are adopted as the standalone paragraph corpus for training discriminators.", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "For both fully-supervised and semi-supervised settings, we use the word vocabulary of image-paragraph dataset as [16] does and the 14,575 paragraph descriptions on public image paragraph dataset [16] are adopted as the standalone paragraph corpus for training discriminators.", "startOffset": 195, "endOffset": 199}, {"referenceID": 15, "context": "We obtain the results of all four baselines from [16].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "Image-Flat [13] directly decodes an image into a paragraph token by token.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "And Region-Hierarchical [16] uses a hierarchical recurrent neural network to decompose the paragraphs into the corresponding sentences.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Same with all baselines, we adopt VGG-16 net [26] to encode the visual representation of an image.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "Note that our RTT-GAN and RegionHierarchical [16] use the same dense captioning model [12] to extract semantic regions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Note that our RTT-GAN and RegionHierarchical [16] use the same dense captioning model [12] to extract semantic regions.", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Human shows the results by collecting an additional paragraph for 500 randomly chosen images as [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "38 Image-Flat [13] 12.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "71 Regions-Hierarchical [16] 15.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "As in Table 4, we also extend our RTT-GAN to the task of video paragraph generation and evaluate it on TACoSMultiLevel dataset [24] that contains 185 long videos filmed in an indoor environment, following [35].", "startOffset": 127, "endOffset": 131}, {"referenceID": 34, "context": "As in Table 4, we also extend our RTT-GAN to the task of video paragraph generation and evaluate it on TACoSMultiLevel dataset [24] that contains 185 long videos filmed in an indoor environment, following [35].", "startOffset": 205, "endOffset": 209}, {"referenceID": 34, "context": "Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a fixed-length feature vector every 16 frames.", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a fixed-length feature vector every 16 frames.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a fixed-length feature vector every 16 frames.", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "Method BLEU-4 METEOR CIDEr CRF-T [25] 25.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "8 CRF-M [24] 27.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "7 LRCN [4] 29.", "startOffset": 7, "endOffset": 10}, {"referenceID": 34, "context": "4 h-RNN [35] 30.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.", "creator": "LaTeX with hyperref package"}}}