{"id": "1703.07055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems", "abstract": "Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.", "histories": [["v1", "Tue, 21 Mar 2017 04:56:14 GMT  (127kb,D)", "http://arxiv.org/abs/1703.07055v1", "5 pages, 5 figures"]], "COMMENTS": "5 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["xiujun li", "yun-nung chen", "lihong li", "jianfeng gao", "asli celikyilmaz"], "accepted": false, "id": "1703.07055"}, "pdf": {"name": "1703.07055.pdf", "metadata": {"source": "CRF", "title": "Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems", "authors": ["Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz"], "emails": ["xiul@microsoft.com", "lihongli@microsoft.com", "jfgao@microsoft.com", "y.v.chen@ieee.org", "asli@ieee.org"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive themselves without being able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "2. Approach", "text": "Natural Language Understanding (NLU) is a fundamental component for many downstream tasks in a dialogue system, such as state tracking [16] and political learning [17, 13]. A dialogue policy is often sensitive to the noise or other types of errors (e.g. incorrect classification of an area or dialog intentions) accumulated by the NLU module, especially in modular pipeline-based dialogue systems, where NLU and policy learning are trained separately. Lately, end-to-end learning approaches to building dialogue systems with different optimization goal functions can offer many advantages for both NLU [12] and policy learning, where policy learning can be adapted to the noise in the NLU component, and the NLU part can be fine-tuned in a way guided by the performance of the policy learner. In this work, we thoroughly examine the actual impact of the NLU on the performance of a dialogue system."}, {"heading": "2.1. User Simulation", "text": "In the dialogue community, researchers usually try to optimize dialogue strategies with either supervised learning (SL) or enhanced learning (RL). In SL approaches, a policy is trained to mimic an expert's observed actions. Supervised learning approaches often require a large amount of expert-designated data for training. Task-specific areas typically require intensive domain knowledge to collect and comment on actual human-machine conversations, and it is often expensive and time-consuming. Furthermore, parts of the dialogue state are not well covered in the training data, as there is insufficient exploration to prevent a superior learner from finding an optimal policy. In contrast, RL approaches allow an agent to learn without expert-generated examples who can only send a reward signal to optimize a dialogue policy."}, {"heading": "2.2. Error Model Controller", "text": "When training or testing a policy based on semantic frameworks of user actions, an error model [24] is introduced to simulate the noise of the NLU component and the loud communication between user and agent. At this point, we introduce different levels of noise in the error model: one at the intention level, the other at the slot level. For each level, there is more fine-grained noise."}, {"heading": "2.2.1. Intent Error", "text": "At the intention level, we categorize the intention into three groups: \u2022 Group 1: general greeting, thanksgiving, closure, etc. \u2022 Group 2: users can inform, for example, to inform the agent of the slot val-ues (or restrictions) (movie name = \"Titanic,\" start time = \"19 o'clock\"). \u2022 Group 3: users can request information for certain slots. In a movie booking scenario, the user can make \"request (start time; movie name =\" Titanic \").\" In a specific task, for example, in movie booking scenarios, there are several information and request contents, such as request theater, request start time, request moviename, etc. are different intentions, but in the same group.Based on the above-mentioned intention categorization, there are three types of intentional errors: \u2022 Random error (0): Random vociferous intention from the same category (within a group error) or other categories (between group error intention). \u2022 For example, the intention of the intention of the group is pronounced (1)."}, {"heading": "2.2.2. Slot Error", "text": "At the slot level, there are four types of error types: \u2022 Random error (0): to simulate noise that is randomly set to the following three types. \u2022 Slot deletion (1): to simulate the scenario in which the slot was not detected by the NLU; \u2022 Wrong slot value (2): to simulate the scenario in which the slot name was correctly detected but the slot value was not, for example, the wrong word segmentation; \u2022 Wrong slot (3): to simulate the scenario in which neither the slot nor its value was correctly detected."}, {"heading": "2.3. Dialogue Manager", "text": "The symbolic dialog form of NLU is passed on to the dialogue manager (DM). A classic DM is both state tracking and political learning. The state tracker continues to track the evolving slot value pairs of agent and user, and based on the conversation history, a query can be formed to interact with an external database to retrieve the available result. At each round of the dialog, the state tracker is updated based on the retrieved results from the database and the latest dialog action of the user and gives a dialogue state (in a compact representation). The dialog state often includes the latest actions of the user, latest actions of the agent, database results, turn information and conversation history, etc. In accordance with the dialog state, the dialog policy consists of generating the next available action of the agent (a | s). To optimize this policy, we apply reinforced learning in a network to the mode of working with a positional policy (Q) to the end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end-end."}, {"heading": "3. Experiments", "text": "The experiments are conducted on the basis of a neural task fulfillment dialog system that helps users book movie tickets. The system collects information about the desires of customers in multi-turn conversations and ultimately books the intended movie tickets. At the end of the call, the environment then judges a binary result (success or not): It is a success if a movie is booked and the movie booked meets the restrictions for all users. To measure the quality of the agent, there are three evaluation measures: {success rate, average reward, average turns}. Each of these metrics provides different information about the quality of the agents. Three metrics are strongly correlated: Generally, a good policy should have a higher success rate, a higher average reward and lower average turns. We train the reinforcement of learning-based agents by interacting with a simulated user in an end-to-end mode under different error settings and report success rate and average turns for the analysis of all the elements of the course plan (1) and the analysis of the intention of each item in an average of 10."}, {"heading": "3.1. Datasets", "text": "The data was collected through Amazon Mechanical Turk and commented on using an internal scheme. There are 11 intentions (i.e., inform, request, confirm questions, confirm answers, etc.) and 29 slots (i.e., movie name, start time, theater, number of people, etc.) Most slots are informable slots that the user can use to narrow the search, and some are requestable slots from which the user can query values from the agent. For example, no number of people can be requested as the user knows how many tickets he or she wants to buy. There are a total of 280 marked dialogs in the movie domain, and the average number of spins per dialog is approximately 11."}, {"heading": "3.2. Basic Experiments", "text": "The group of basic experiments (from B1 to B3) is located in the settings that combine the noise of intention and slot: 1) 1Success rate is sometimes referred to as task fulfillment rate - the fraction of dialogs that are successfully completed. In both intention and slot, the error types are random, and the error rates are {0.00, 0.10, 0.20}. The rules-based agent reports 41%, 21%, and 12% success rates below 0.00, 0.10, and 0.20 error rates, respectively. In contrast, the RL-based agent achieves 91%, 79%, and 76% success rates under the same error rates, respectively. We compare the performance between two types of agents and find that the RL-based agent has greater robustness and is less sensitive to loud input. Therefore, the following experiments are conducted based on consideration of the robustness of an RL dialog agent."}, {"heading": "3.3. Intent Experiments", "text": "In order to better understand the effects of intention-level noise on dialog systems, two experimental groups are conducted: the first group (I0-I2) focuses on the difference between all intention error types; the second group (I3-I5) focuses on the effects of intent error rates. Other factors are identical for both groups, with the random slot error type and a 5% error rate."}, {"heading": "3.3.1. Intent Error Type", "text": "Experiments with the settings of I0-I2 are subject to the same slot errors and the same error rate for intention (10%), but with different error types for intention: I1 contains the loud intentions from the same categories, I2 the loud intentions from different categories, and I0 both by random selection. Figure 2 (a) shows the learning curves for all types of intention errors where the difference between three curves is insignificant, indicating that the wrong intentions have similar effects regardless of which categories they belong to."}, {"heading": "3.3.2. Intent Error Rate", "text": "Experiments with Settings I3-I5 examine the difference between different error rates for intent errors. If the error rate for intent errors increases, the dialog agent performs slightly worse, but the difference is subtle. It indicates that the RL-based agent has better robustness with rough intentions. As shown in Fig. 2 (a, b), all RL agents can converge to a similar success rate for both content error types and content error rates."}, {"heading": "3.4. Slot Experiments", "text": "We also conducted two sets of experiments to investigate the effects of slot-level noise, where other factors are fixed, with the error type being random intent and a 10% error rate intentional."}, {"heading": "3.4.1. Slot Error Type", "text": "Experiments with the settings of S0-S3 examine the effects of different slot error types, and corresponding learning curves are given in Fig. 2 (c). Among the individual error types (S1-S3), incorrect slot value (S2) performs worst, meaning that the slot name is correctly detected but an incorrect value is extracted with the slot (e.g. incorrect word segmentation); in this case, the agent receives an incorrect value for the slot and ultimately books the wrong ticket or does not book it. Probable reason for this is that the dialog agent has difficulty identifying the errors based on RL-based belief, and the incorrect slot values for subsequent dialog actions could significantly impair performance. Between slot deletion (S1) and incorrect slot (S3), the difference is limited, suggesting that the RL agent has similar capabilities in dealing with these two types of noise."}, {"heading": "3.4.2. Slot Error Rate", "text": "Experiments with the S4-S6 settings focus on different slot error rates (0%, 10% and 20%) and report the results in Fig. 2 (d). Fig. 2 (d) shows that the dialog agent performs worse when the slot error rate increases (the success rate curve drops and the average spin curve increases). Compared to Fig. 2 (b), the performance of the dialog system is more sensitive to the slot error rate than to the deliberate error rate."}, {"heading": "3.5. Discussion", "text": "One important finding that our empirical results suggest is that errors at the slot level are more important than errors at the intention level. One possible explanation is related to our dialog action representation, the intention (slot value pairs). For example, if an intention is wrongly predicted, information as a request ticket was wrongly predicted, the dialog agent can deal with the unreliable situation and decide to make a confirmation in order to keep the correct information for the following conversation. In contrast, the dialog agent can still maintain a correct intention based on slot information, although the predicted intention is wrong. To verify the hypotheses, further experiments are required that we leave as future work. Finally, it should be noted that the experiments in this essay are based on a dialog setting for task completion, but Chit chat is another setting with other goals to optimize the chat system."}, {"heading": "4. Conclusion", "text": "In this paper, we are conducting a series of extensive experiments to understand the impact of errors in the understanding of natural language on the performance of a neural dialog system to complete the tasks. Our results suggest several interesting conclusions: 1) slot-level errors have a greater impact than intention-level errors; 2) different slot error types have different effects on RL agents; 3) RL agents are more resilient to certain slot-level errors - agents can learn to verify or confirm with users, which requires slightly longer conversations."}, {"heading": "5. References", "text": "[1] G. Tur and R. De Mori, Spoken language understanding: Systems for extracting semantic information from speech. \"John Wiley & Sons, 2011. [2] P. Xu and R. Sarikaya,\" Convolutional Neural Network based triangular crf for joint intention detection and slot filling, \"in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 78-83. [3] D. Hakkani-Tu \u00bc r, G. Tur, A. Celikyilmaz, Y.-N. Chen, J. Gao, L. Deng, and Y.-Y. XiXiXiXiXiXiXi. Xi. Learning to bi-directional rnn-lstm,\" in Proceedings of the 17th Annual Meeting of the International Speech Communication Association, 2016. [4] B. Liu and I. Lane, Attention-based recurrent network models joint purpose. \""}], "references": [{"title": "Spoken language understanding: Systems for extracting semantic information from speech", "author": ["G. Tur", "R. De Mori"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 78\u201383.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm", "author": ["D. Hakkani-T\u00fcr", "G. Tur", "A. Celikyilmaz", "Y.-N. Chen", "J. Gao", "L. Deng", "Y.-Y. Wang"], "venue": "Proceedings of The 17th Annual Meeting of the International Speech Communication Association, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "author": ["B. Liu", "I. Lane"], "venue": "Interspeech, pp. 685\u2013689, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Syntax or semantics? knowledge-guided joint semantic frame parsing", "author": ["Y.-N. Chen", "D. Hakkani-T\u00fcr", "G. Tur", "A. Celikyilmaz", "J. Gao", "L. Deng"], "venue": "Proceedings of 6th IEEE Workshop on Spoken Language Technology, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "JUPITER: a telephone-based conversational interface for weather information", "author": ["V. Zue", "S. Seneff", "J.R. Glass", "J. Polifroni", "C. Pao", "T.J. Hazen", "L. Hetherington"], "venue": "IEEE Transactions on speech and audio processing, vol. 8, no. 1, pp. 85\u201396, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Conversational interfaces: Advances and challenges", "author": ["V.W. Zue", "J.R. Glass"], "venue": "Proceedings of the IEEE, vol. 88, no. 8, pp. 1166\u20131180, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "End-to-end lstm-based dialog control optimized with supervised and reinforcement learning", "author": ["J.D. Williams", "G. Zweig"], "venue": "arXiv preprint arXiv:1606.01269, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "author": ["T. Zhao", "M. Eskenazi"], "venue": "arXiv preprint arXiv:1606.02560, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end task-completion neural dialogue systems", "author": ["X. Li", "Y.-N. Chen", "L. Li", "J. Gao"], "venue": "arXiv preprint arXiv:1703.01008, 2017.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end joint learning of natural language understanding and dialogue manager", "author": ["X. Yang", "Y.-N. Chen", "D. Hakkani-Tur", "P. Crook", "X. Li", "J. Gao", "L. Deng"], "venue": "arXiv preprint arXiv:1612.00913, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["B. Dhingra", "L. Li", "X. Li", "J. Gao", "Y.-N. Chen", "F. Ahmed", "L. Deng"], "venue": "arXiv preprint arXiv:1609.00777, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Dialogue policy learning for combinations of noise and user simulation: transfer results", "author": ["O. Lemon", "X. Liu"], "venue": "Proc. SIGdial, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Continuously learning neural dialogue management", "author": ["P.-H. Su", "M. Gasic", "N. Mrksic", "L. Rojas-Barahona", "S. Ultes", "D. Vandyke", "T.-H. Wen", "S. Young"], "venue": "arXiv preprint arXiv:1606.02689, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "The dialog state tracking challenge series: A review", "author": ["J. Williams", "A. Raux", "M. Henderson"], "venue": "Dialogue & Discourse, vol. 7, no. 3, pp. 4\u201333, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient exploration for dialogue policy learning with bbq networks & replay buffer spiking", "author": ["Z.C. Lipton", "J. Gao", "L. Li", "X. Li", "F. Ahmed", "L. Deng"], "venue": "arXiv preprint arXiv:1608.05081, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A user simulator for task-completion dialogues", "author": ["X. Li", "Z.C. Lipton", "B. Dhingra", "L. Li", "J. Gao", "Y.-N. Chen"], "venue": "arXiv preprint arXiv:1612.05688, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "User modeling for spoken dialogue system evaluation", "author": ["W. Eckert", "E. Levin", "R. Pieraccini"], "venue": "Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on. IEEE, 1997, pp. 80\u201387.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning user simulations for information state update dialogue systems.", "author": ["K. Georgila", "J. Henderson", "O. Lemon"], "venue": "in INTER- SPEECH,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Consistent goal-directed user model for realisitc man-machine task-oriented spoken dialogue simulation", "author": ["O. Pietquin"], "venue": "2006 IEEE International Conference on Multimedia and Expo. IEEE, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "A survey of statistical user simulation techniques for reinforcementlearning of dialogue management strategies", "author": ["J. Schatzmann", "K. Weilhammer", "M. Stuttle", "S. Young"], "venue": "The knowledge engineering review, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "The hidden agenda user simulation model", "author": ["J. Schatzmann", "S. Young"], "venue": "IEEE transactions on audio, speech, and language processing, vol. 17, no. 4, pp. 733\u2013747, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Error simulation for training statistical dialogue systems", "author": ["J. Schatzmann", "B. Thomson", "S. Young"], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, pp. 529\u2013533, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1606.01541, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 2, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 3, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 4, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 5, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 118, "endOffset": 127}, {"referenceID": 6, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 118, "endOffset": 127}, {"referenceID": 7, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 160, "endOffset": 171}, {"referenceID": 8, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 160, "endOffset": 171}, {"referenceID": 9, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 160, "endOffset": 171}, {"referenceID": 10, "context": ", from policy learner or NLG) can be back-propagated to fine tune the LU component [12, 13].", "startOffset": 83, "endOffset": 91}, {"referenceID": 11, "context": ", from policy learner or NLG) can be back-propagated to fine tune the LU component [12, 13].", "startOffset": 83, "endOffset": 91}, {"referenceID": 12, "context": "[14] compared the policy transfer properties under different environments, showing that policies trained in high-noise conditions have better transfer properties than those trained in low-noise conditions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] briefly investigated the effect of dialogue action level semantic error rates (SER) on the dialogue performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Natural language understanding (NLU) is a fundamental component to many downstream tasks in a dialogue system, such as state tracking [16] and policy learning [17, 13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "Natural language understanding (NLU) is a fundamental component to many downstream tasks in a dialogue system, such as state tracking [16] and policy learning [17, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 11, "context": "Natural language understanding (NLU) is a fundamental component to many downstream tasks in a dialogue system, such as state tracking [16] and policy learning [17, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 10, "context": "Recently, end-to-end learning approaches to building dialog systems with varying optimization objective functions offer many benefits for both NLU [12] and policy learning, in which the policy learning can be adapted to the noise in the NLU component, and the NLU part can be fine tuned in a way that is guided by the policy learner\u2019s performance.", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "All experiments are conducted in a user simulation environment [18].", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 18, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 19, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 20, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 16, "context": "The user goals are generated using a labeled set of conversational data [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "During the course of a dialogue, the user simulator maintains a compact, stack-like representation called user agenda [23].", "startOffset": 118, "endOffset": 122}, {"referenceID": 22, "context": "When training or testing a policy based on semantic frames of user actions, an error model [24] is introduced to simulate the noise from the NLU component, and noisy communication between the user and agent.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "In this work, we represent the policy using a deep Q-network (DQN) [25], which takes the state st from the state tracker as input, and outputs Q(st, a; \u03b8) for all actions a using network parameter \u03b8.", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "Two important DQN tricks, target network and experience replay are applied [25].", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "Finally, it should be noted that the experiments in this paper are based on a task-completion dialogue setting, but chit-chat is another setting with different optimization goals [26].", "startOffset": 179, "endOffset": 183}], "year": 2017, "abstractText": "Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.", "creator": "LaTeX with hyperref package"}}}