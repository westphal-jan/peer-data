{"id": "1703.07075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Pseudorehearsal in value function approximation", "abstract": "Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Q-learning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.", "histories": [["v1", "Tue, 21 Mar 2017 07:09:27 GMT  (2120kb,D)", "http://arxiv.org/abs/1703.07075v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vladimir marochko", "leonard johard", "manuel mazzara"], "accepted": false, "id": "1703.07075"}, "pdf": {"name": "1703.07075.pdf", "metadata": {"source": "CRF", "title": "Pseudorehearsal in value function approximation", "authors": ["Vladimir Marochko", "Leonard Johard", "Manuel Mazzara"], "emails": [], "sections": [{"heading": null, "text": "Keywords: learning amplification, rehearsal, pseudorearsal, catastrophic forgetting"}, {"heading": "1 Introduction", "text": "Reinforcement learning is a more general formulation of a problem than the commonly used supervised learning framework. As such, it can be applied to a wider range of problems. It is also a more difficult problem to optimize because feedback is more limited.Reinforcement learning covers the space of prediction and control problems in partially unknown rooms with unknown behavior. The agent should explore the environment and find optimal actions that he needs to perform to achieve the goal. It is used in cases where the optimal strategy is unknown, so there is no way to train agents using supervised learning algorithms. The basic idea can be described by the metaphor that a player starts an unknown game and receives a message that he or she has lost or won after a series of rounds. After a number of games, he / she will figure out how he / she can act as often as possible to win."}, {"heading": "1.1 Supervised agents in reinforcement learning problems", "text": "In order to solve the practical problems that can be assumed as approximate Markov decision-making process (MDP) [1], such as robot navigation, chess or stock market trading, we can use a value adjustment to accelerate the learning process [2]. The weakness of this approach is that convergence is not guaranteed if the MDP approach is wrong, or in cases where the inputs are continuous and a nonlinear functional approximation is required [4]. Furthermore, the value adjustment for continuous outputs must be combined with an additional optimization technique, such as REINFORCE [5], in order to search for optimal outputs. If the outputs are discreet, a simple ar Xiv: 170 3.07 075v 1 [cs.A I] 2 1M ar2 017 maximization is sufficient and allows us to use the Q-Learning Framework [6] [7]. Although a value adjustment problem effectively amplifies the learning problem by simplifying the learning effect into a conventional application, it is a learning risk."}, {"heading": "1.2 Catastrophic forgetting", "text": "Another problem that has received more attention lately is catastrophic forgetting [10], which is most commonly described in an online unattended Hebrew learning task, where the ability to retrieve previously stored patterns is lost when we update weights in training new patterns. This catastrophic forgetting of the original patterns occurs even if the parameter space is more than enough to store both sets of patterns, and is a consequence of the limited mixing of input objects. Simply switching between the new and old pattern groups with a sufficiently low learning rate would theoretically solve the problem, but this has a potential impact on the convergence rate and requires explicit storage of all training patterns. To minimize the effect on the convergence rate, we want to maximize the mization of the mixing of presented inputs."}, {"heading": "2 Catastrophic forgetting in reinforcement learning", "text": "The online nature of reinforcement learning means that catastrophic forgetfulness is a key bottleneck. There are two basic, non-sharpening approaches to the catastrophic forgetfulness problem proposed in the literature: sample and pseudorearsal. Furthermore, other methods based on sparse representations have been used less frequently [11] [12]. The latter approach has a theoretical disadvantage in its negative impact on the ability to generalize, but at least has mixed results and is applicable in conjunction with the sampling methods."}, {"heading": "2.1 Rehearsal approaches", "text": "The first and most direct principle approach to mitigating catastrophic forgetfulness is to sample [13], [14]. A sample strategy stores only a subset of all previous experiences in a buffer. If a new pattern is presented, this pattern is combined with several patterns from the buffer to form a learning battery with a good mix. There are several possible heuristics for selecting patterns for the sample.The importance of catastrophic forgetfulness in amplification learning was recognized early on. Lin introduced the term Experience Replay [15] to indicate the use of sample strategies in the amplification learning environment. Such a sample has shown promising results in robotics [16] and in more complex environments, such as deep Q learning for playing Atari games [17]."}, {"heading": "2.2 Pseudorehearsal approaches", "text": "This year we will be able to find the way we want to go, \"he said."}, {"heading": "2.3 Biological forgetting", "text": "An interesting specific case of catastrophic forgetfulness is learning in the human brain. Dual network models were originally inspired by biological learning [22]. As a result of promising experimental results of such networks, pseudorearsal actually proved to be the most plausible explanation for the otherwise cryptic need for dual learning systems in the brain [23]. Biologically more detailed extensions of these models have recently been researched by Hattori, where they showed excellent improvements in the ability to store information [24]. The pseudorearsal approach also contributes to the urgent need for new biological rules of plasticity in large-scale neurosimulation, especially for their developmental variants (e.g. BioDynaMo [25]). A true full-scale brain simulation is approaching, but we still lack a fundamental understanding of the role dreams play in the learning process, even though sleep stages are of considerable length and are evident even in the simplest biological neural networks."}, {"heading": "3 Experimental design", "text": "We're going to re-evaluate the analytical results of Frean and Robins - here you just know where they stand. - We're going to evaluate and compare two algorithms for pseudorehearsal on the one hand, while we're on the other hand trying to explore the impact of thrift on the way it's applied to the request for a high-dimensional input space of these algorithms on the other hand. - We're going to do it with a classical method, which is based on the way it's based on the way it's applied to the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's based on the way it's like it's like it's like it's based on the way it's based on the way it's based on the way it's based on it's like it's based on it's like it's like it's based on the way it's like it's based on the way it's like it's based on the way it's like it"}, {"heading": "3.1 Results", "text": "The results for all the observations show different learning rates for different cases, while the averaged approach and comparison with the random agent assure us that learning has its place and, in the case of non-pseudorearsal, mainly depends on the learning rate. For the agents using pseudorearsal learning, the learning rate depends on parameters - learning rate, pseudorearsizes and learning gap. Some of them can cause agents to balance the pole for significantly more time than a random run, while the others perform the same or worse or even worse than a random agent. For both metrics used, we discovered a roughly bell-shaped diagram of dependencies for each of the learning rates used and for each of the techniques used. All pseudorearearsal approaches have different parameter sets that provide optimal learning, suboptimal or even worse - in the case of parameters that differ slightly from the optimal learning rates and for each of the techniques used."}, {"heading": "3.2 MDP", "text": "With fully observable MDP, where the agent knows all about the current state of the bar wagon and the bar wagon, the learning time when the agent's performance moves from an initial random case to the final one is very short, the agent converges quickly in a number of steps that he can equalize, and most of his next steps keep around this result with some variations, sometimes very large ones, caused by catastrophic forgetfulness. If a case of learning causes the agent to change his behavior significantly - change is as fast as initial learning and on the chart it looks like immediate change in performance."}, {"heading": "3.3 POMDP", "text": "Partial observability tends to suffer less from forgetting, possibly because each part of the smaller room is visited more frequently. Pseudorearsal has a more significant impact here: Although it has the same optimal-suboptimal-worst parameter sets, optimal parameters always reduce the number of runs necessary to learn catastrophic forgetting and reduce the impact: If the agent in the current parameter set does not deviate from the worst case scenario - the median for all runs is always higher than the mean, indicating relatively stable behavior after training. On the other hand, agents in POMDPs change their policies more often than agents in MDPs, and while the agent in the fully observable MDP has a slight chance of achieving good performance after achieving a suboptimal solution, the agent in POMDP slightly changes its policies in both directions. The pseudorearsal approach of Frean and Robins reduces these serious contexts and all of these effects show that they are the same agents."}, {"heading": "4 Discussion and conclusions", "text": "The experiment showed us that pseudorehearsal can deal with catastrophic interference, but it has its own effects, which in some cases cause divergences that worsen performance, so this tool should be used carefully and the parameters - learning rate, pseudorehearsal size and recognition frequency - must be chosen correctly to ensure high performance on the current task. For the fully observable MDPs, pseudorehearsal is the influence of catastrophic forgetfulness when the optimal parameters for the task are known. At best, the optimal performance was achieved quickly with pseudorehearsal, but the other parameters were of the optimal one, which was worse performance. In the case when the modelling of this environment could be too complex - optimal parameters can only be defined empirically before starting learning, which can be unacceptable when the costs of errors are high."}], "references": [{"title": "A survey of pomdp solution", "author": ["K.P. Murphy"], "venue": "techniques,\u201d environment,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "A connectionist actor-critic algorithm for faster learning and biological plausibility,", "author": ["L. Johard", "E. Ruffaldi"], "venue": "IEEE International Conference on Robotics and Automation (ICRA). IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning to predict by the methods of temporal differences,", "author": ["R.S. Sutton"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "An analysis of temporal-difference learning with function approximation,", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning,", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "A brief survey of parametric value function approximation,", "author": ["M. Geist", "O. Pietquin"], "venue": "Rapport interne, Supe\u0301lec,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "On evaluating stream learning algorithms,", "author": ["J. Gama", "R. Sebasti\u00e3o", "P.P. Rodrigues"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem,", "author": ["M. McCloskey", "N.J. Cohen"], "venue": "Psychology of learning and motivation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Semi-distributed representations and catastrophic forgetting in connectionist networks,", "author": ["R.M. French"], "venue": "Connection Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.", "author": ["R. Ratcliff"], "venue": "Psychological review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Using fast weights to deblur old memories,", "author": ["G.E. Hinton", "D.C. Plaut"], "venue": "Proceedings of the ninth annual conference of the Cognitive Science Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "Reinforcement learning for robots using neural networks,", "author": ["L.-J. Lin"], "venue": "DTIC Document, Tech. Rep.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "Experience replay for real-time reinforcement learning control,", "author": ["S. Adam", "L. Busoniu", "R. Babuska"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Catastrophic forgetting, rehearsal and pseudorehearsal,", "author": ["A. Robins"], "venue": "Connection Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Catastrophic forgetting in simple networks: an analysis of the pseudorehearsal solution,", "author": ["M. Frean", "A. Robins"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Neuron clustering for mitigating catastrophic forgetting in supervised and reinforcement learning,", "author": ["B.F. Goodrich"], "venue": "Ph.D. dissertation, University of Tennessee,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Reinforcement learning in continuous time and space: Interference and not ill conditioning is the main problem when using distributed function approximators,", "author": ["B. Baddeley"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.", "author": ["J.L. McClelland", "B.L. McNaughton", "R.C. O\u2019Reilly"], "venue": "Psychological review,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1995}, {"title": "The consolidation of learning during sleep: comparing the pseudorehearsal and unlearning accounts,", "author": ["A. Robins", "S. McCallum"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Talanov, \u201cThe biodynamo project: Creating a platform for large-scale reproducible biological simulations,", "author": ["L. Breitwieser", "R. Bauer", "A.D. Meglio", "L. Johard", "M. Kaiser", "M. Manca", "M. Mazzara", "F. Rademakers"], "venue": "4th Workshop on Sustainable Software for Science: Practice and Experiences", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In order to solve the practical problems that can be assumed to be approximately Markov decision process (MDP) [1], like robot\u2019s navigation, playing chess or trading on stock exchange, we can use a value function approximation to speed up the learning process [2] [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "In order to solve the practical problems that can be assumed to be approximately Markov decision process (MDP) [1], like robot\u2019s navigation, playing chess or trading on stock exchange, we can use a value function approximation to speed up the learning process [2] [3].", "startOffset": 260, "endOffset": 263}, {"referenceID": 2, "context": "In order to solve the practical problems that can be assumed to be approximately Markov decision process (MDP) [1], like robot\u2019s navigation, playing chess or trading on stock exchange, we can use a value function approximation to speed up the learning process [2] [3].", "startOffset": 264, "endOffset": 267}, {"referenceID": 3, "context": "The weakness of this approach is that convergence is not guaranteed if the MDP approximation is incorrect, or in cases where the inputs are continuous, and which necessitates non-linear function approximation [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "Furthermore if we have continuous outputs the value approximation needs to be combined with an additional optimization technique, such as REINFORCE [5], in order to search for optimal outputs.", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "maximization is sufficient and allows us to make use of the Q-learning framework [6][7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "Although a value function simplifies the learning problem by effectively converting a reinforcement learning problem to a supervised learning problem through the use of bootstrapping [8], it is still a more difficult problem than conventional supervised learning.", "startOffset": 183, "endOffset": 186}, {"referenceID": 7, "context": "One of these additional difficulties is that the policydependent rewards introduces a concept drift [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "A different problem which recently got more attention is catastrophic forgetting [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "In addition, other methods based on sparse representations [11] [12] have been used less frequently.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "The first and most straight-forward principal approach for mitigating catastrophic forgetting is rehearsal [13], [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The first and most straight-forward principal approach for mitigating catastrophic forgetting is rehearsal [13], [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "Lin introduced the term Experience Replay [15] for referring to the", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Such rehearsal has shown very promising results in robotics [16] and on more complex environments, such as Deep Q-learning for playing Atari games [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "A second principal approach to solving catastrophic forgetting is pseudorehearsal [18], which does not require explicit storage of patterns.", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "In the original work in this area by [18], pure noise fed to the network was able to almost completely eliminate catastrophic interference.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "An analytical approach by Frean and Robins [19] in single perceptrons suggest an alternative explanation for the surprising efficiency of random pseudopatterns.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "Further work in this direction was done in a thesis by Goodrich [20], where some of these results where expanded to multilayer perceptrons.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Regardless of the reason for such networks, pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [18], supervised learning [13] and reinforcement learning [21].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "Regardless of the reason for such networks, pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [18], supervised learning [13] and reinforcement learning [21].", "startOffset": 228, "endOffset": 232}, {"referenceID": 17, "context": "Regardless of the reason for such networks, pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [18], supervised learning [13] and reinforcement learning [21].", "startOffset": 260, "endOffset": 264}, {"referenceID": 15, "context": "The first pseudorehearsal algorithm is the one used by Frean and Robins [19] with a simplified weighting equation and changed for non-linear neural network inner assignments.", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "Dual network models were initially inspired by biological learning [22].", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "As a consequence of promising experimental results of such networks, pseudorehearsal was indeed found to be the most plausible explanation for the otherwise cryptic need for dual learning systems in the brain [23].", "startOffset": 209, "endOffset": 213}, {"referenceID": 20, "context": "BioDynaMo [25]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "We will reevaluate the analytic results of Frean and Robins [19] in a real reinforcement learning task.", "startOffset": 60, "endOffset": 64}], "year": 2017, "abstractText": "Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Qlearning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.", "creator": "LaTeX with hyperref package"}}}