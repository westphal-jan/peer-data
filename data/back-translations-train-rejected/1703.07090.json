{"id": "1703.07090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Deep LSTM for Large Vocabulary Continuous Speech Recognition", "abstract": "Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, are effective network for sequential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recognition, because of their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-wise training and exponential moving average methods for deeper LSTM models. It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for online streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the distillation process. Therefore, the model trained with the proposed training framework reduces relative 14\\% character error rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also introduced in the framework. The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning.", "histories": [["v1", "Tue, 21 Mar 2017 08:24:50 GMT  (71kb)", "http://arxiv.org/abs/1703.07090v1", "8 pages. arXiv admin note: text overlap witharXiv:1703.01024"]], "COMMENTS": "8 pages. arXiv admin note: text overlap witharXiv:1703.01024", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu tian", "jun zhang", "zejun ma", "yi he", "juan wei", "peihao wu", "wenchang situ", "shuai li", "yang zhang"], "accepted": false, "id": "1703.07090"}, "pdf": {"name": "1703.07090.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Xu Tian", "Jun Zhang", "Zejun Ma", "Yi He", "Juan Wei", "Peihao Wu", "Wenchang Situ", "Shuai Li", "Yang Zhang"], "emails": ["zy80232}@alibaba-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.07 090v 1 [cs.C L] 21 Mar 201 7Recursive neural networks (RNNs), especially long-term short-term memory RNNNs (LSTM), are effective networks for sequential tasks such as speech recognition. Lower-lying LSTM models are well suited for continuous speech recognition with a large vocabulary due to their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-by-layer training and exponentially mobile average methods for deeper LSTM models. Furthermore, for online streaming speech recognition applications, the low-real-time model with more than 7 layers of Shenma language search data is successfully trained in Mandarin and they surpass the deep LSTM models trained by conventional approach. Furthermore, for low-real-time voice recognition applications, the flat model with a very deep real-time profile from the very deep model is comparable to the low-frame model distilled. Therefore, the accuracy of the initial distillation process is comparable to the initial loss of the strategy."}, {"heading": "1. INTRODUCTION", "text": "In fact, it is the case that most of them are able to survive themselves by blaming themselves and others. (...) It is not the case that they see themselves as being able to survive themselves. (...) It is not the case that they see themselves as being able to survive themselves. (...) It is not the case that they see themselves as being able to survive themselves. (...) It is the case that they are not able to survive themselves. (...) It is the case that they have survived themselves. (...) It is the case that they have survived themselves. (...) It is the case that they have survived themselves. (...) It is the case that they have survived themselves. (...) It is the case that they have survived themselves. (...) It is the case. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. It is. () It is. () It is. It is. () It is. (... It is. () It is. It is. It is. (... It is. () It is. It is. It is. () It is. (... It is. It is. () It is. It is. () It is. It is. It is. () It is. It is. It is. It is. (... It is. It is. It is. It is. (). It is. It is. It is. It is. It is. (). It is. It is. It is. It is. It is. (). It is. It is. It is. It is. It is. (). It is. It is. It is. () It is. It is. It is."}, {"heading": "2. OUR TRAINING FRAMEWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Layer-wise Training with Soft Target and Hard Target", "text": "In fact, the method of layered pre-schooling is an effective way to learn the weights of the very deep architecture [7, 21]. In layered pre-schooling, a single-layer LSTM model is first formed with a normalized initialization. Sequentially, the two-layer pre-schooling of the LSTM model is an effective method to learn the weights of the very deep architecture [7, 21]. In layered pre-schooling, a single-layer LSTM model is first formed with a normalized initialization. Sequentially, the two-layer pre-schooling of the LSTM model is initialized by a trained single-layer model, and its second layer is initialized regularly. In this way, a deep architecture is formed layer by layer, and it can be well converted.In the conventional layer of pre-schooling, only the parameters of the flatter networks are transmitted, and the learning goals are even deeper."}, {"heading": "2.2. Differential Saturation Check", "text": "The objects of the conventional saturation test are the gradients and the cell activations [5]. Gradients are truncated to the range [-5, 5], while the cell activations are truncated to the range [-50, 50]. Apart from that, the differences of the recurring layers are also limited. If the differentials exceed the range, the corresponding reverse propagation is skipped, while the gradients and cell activations exceed the limit, values are set as limit values. Excessive or too small differentials cause the gradients to disappear slightly, and this shows the failure of this propagation. As a result, the parameters are not updated and the next propagation."}, {"heading": "2.3. Sequence Discriminative Training", "text": "Cross-entropy (CE) is often used in the speech recognition system as a framework discriminatory training criterion, but is not well suited for speech recognition because speech recognition training is a sequential learning problem. In contrast, the sequence discriminatory training criterion has been shown to further improve the performance of neural networks that were first trained with cross-entropy [22, 23, 24]. We choose Minimum Bayes Risk (sMBR) [22] at the state level among a number of sequence discriminating criteria, such as Maximum Mutual Information (MMI) [25] and Minimum Phone Error (MPE) [26]. MPE and sMBR are designed to minimize the expected error of varying granularity of the labels, while CE aims to minimize the expected frame error and MMI aims to minimize the expected typesetting error. Information at the state level is obtained through sMBRa at the overall school-level function, and then the MBR is trained for the loss of life in the MBR model."}, {"heading": "2.4. Parallel Training", "text": "However, a larger data set means more training samples and more model parameters. Therefore, parallel training with multiple GPUs is indispensable and uses data parallelism [10]. All training data is divided into several groups without overlapping and distributed among different GPUs. Each GPU trains locally with a split of the training data set. All GPUs synchronize their local models with the average method of the model after minibatch optimization [13, 14]."}, {"heading": "2.4.1. Block-wise Model Updating Filter", "text": "The Average Method model achieves linear acceleration in the training phase, but detection accuracy decreases compared to the individual GPU training. Block-wise model update filter (BMUF) is another successful effort in parallel training with linear acceleration. It cannot achieve a deterioration in detection accuracy with multi-GPUs [15]. In the Average Method model, the aggregated model \u03b8 (t) is calculated and transferred to GPUs. On this basis, the BMUF proposed a novel model update strategy, whereby G (t) refers to the update of the model, and vice versa (t) to the update of the global model. There are two parameters in the BMUF, block impulse and block learning rate. Then, the global model is updated, with G (t) denoting the update of the model and vice versa (t) the update of the global model."}, {"heading": "2.4.2. Exponential Moving Average Model", "text": "The averaged SGD is proposed to further accelerate the convergence rate of the SGD. The averaged SGD uses the Moving Average (MA) \u03b8 as an estimator of the SGD. [16] It is shown that the sufficiently large training data set can be well converged within the framework of a single GPU training. It can be considered a non-interference strategy that \u03b8 does not participate in the main optimisation process and only takes effect after the end of the entire optimisation. However, for the parallel implementation of the training each learning data set is calculated by modelling and BMUF with several models, and the shift of the average model is not well converged compared to individual GPU training sessions. The averaged methods are applied within the framework of the parallel training of large data sets, as the adjustment of the model is limited in time and, in particular, no deterioration of the implementation of the BMUF."}, {"heading": "3. DEPLOYMENT", "text": "Shenma Voice Search is one of the most popular mobile search engines in China, and it is a streaming service that displays intermediate results while users are still speaking. LSTM unidirectional network is used instead of bidirectional because it is well suited for real-time streaming speech recognition. Shenma Voice Search is one of the most popular mobile search engines in China."}, {"heading": "3.1. Distillation", "text": "It is shown that deep neural network architecture can bring about improvements in LVCSR. However, it also leads to much more computation and higher RTF, so that the detection result cannot be displayed in real time. It should be noted that deeper neural networks contain more knowledge, but are also cumbersome. Knowledge is the key to improving performance. If it can be transferred from a cumbersome model to a small model, the detection capability can also be transferred to the small model. Knowledge transfer to a small model is referred to as distillation [20]. The small model can work just as well after distillation as a cumbersome model. It offers a way to use a powerful but high RTF model in real-time. The class probability generated by the cumbersome model is considered a soft target, and the generalization capability of a cumbersome model is transferred to a small model. Distillation is the knowledge transfer approach, so that there is no need to use the hard target, which differs from the hard one."}, {"heading": "3.2. Transfer Learning with sMBR", "text": "For a particular specific scenario, the model that has been trained with the data collected from it requires a better adaptation than the model that has been trained with a generic scenario. However, it spends too much time training a model from the outset if there is a well-trained model for generic scenarios. In addition, labeling a large amount of training data in a new scenario is both costly and time-consuming. If a model transfer trained with smaller datasets can achieve a detection accuracy similar to that of the model that has been trained directly with larger datasets, it is undoubtedly more practical that transfer learning is more practical. As a particular scenario is a subset of a generic scenario, some knowledge can be shared between them. In addition, a generic scenario consists of different conditions so that its model has greater robustness. As a result, not only common knowledge but also robustness can be transferred from the model of a generic scenario to the model of a specific scenario. Since the model that achieves a well-sequenced scenario on top of a generic scenario requires a good performance in addition to the framework classification, a good discriminating performance is required for a specific scenario."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Training Data", "text": "We collect the 17,000 hours of tagged data from Shenma Language Search, one of the most popular mobile search engines in China. The data set is compiled from anonymous online searches by users in Mandarin, and the sampling rate of all audio files is 16 kHz, recorded by mobile phones. This data set consists of many different conditions, such as varied noise, even with low signal-to-noise, chatter, dialects, accents, hesitation, etc. At Amap, one of the most popular web mapping and navigation services in China, users can use language search to browse locations and navigate to locations they desire. To showcase the performance of transfer learning with discriminatory sequence training, the Shenma-trained model avoids language search, which is a gruesome scenario, dividing its knowledge into the Amap Language Search model. 7,300 hours of tagged data will be split into three sets of tagged data sets simultaneously."}, {"heading": "4.2. Experimental setup", "text": "It is the first time that in a country where there is a global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global, global,"}, {"heading": "5. RESULTS", "text": "The Shenma test set with about 9000 samples and the Amap test set with about 7000 samples contain different real-world conditions. It simulates the majority of user scenarios and can assess the performance of a trained model well. First, we show the results of models trained with the EMA method. Second, for real-world applications very deep LSTM is distilled to a flat to lower the RTF. The Amap model is also needed to train for mapping and navigation scenarios."}, {"heading": "5.1. Layer-wise Training", "text": "In layer-by-layer training, the deeper model learns both parameters and knowledge from the flatter model. The deeper model is initialized by the flatter model, and its orientation is the combination of hard target and soft target of the flatter model. Two targets have the same weights in our framework. The teacher model is trained with CE. For each layer-by-layer CE model, the corresponding sMBR model is also trained, as sMBR could achieve additional improvement. In our framework, 1000 hours of data are randomly selected from the total data set for sMBR training, but there is no further improvement in performance when the size of the sMBR training data set increases. For very deep unidirectional LSTM initialized with the Xavier initialization algorithm, the 6-layer model matches well, but there is no further improvement in increasing the number of layers. Therefore, the first 6 layers of the 7-layer model CE models are as promising as the Xafour model."}, {"heading": "5.2. Distillation", "text": "In order to ensure the real-time of the system, the number of layers must be reduced, and the flatter network can learn the knowledge of a deeper network with distillation. It was found that RTF of a 2-layer network is acceptable, so knowledge of the 9-layer model is distilled to a 2-layer model. Table 3 shows that distillation of 9-layers to 2-layers results in an RTF decrease of relative 53%, while CER only increases by 5%. Knowledge of deep networks is almost transferred with distillation, distillation brings promising RTF reduction, but little knowledge of deep networks is lost. Furthermore, CER of 2-layers distilled LSTM is reduced by relative 14%, compared to 2-layers regularly trained LSTM."}, {"heading": "5.3. Transfer Learning", "text": "The 2-layer model of the Shenma language search has shown impressive performance in Shenma test, and we call it the Shenma model. It is designed for generic search scenarios, but has less adaptation for specific scenarios such as Amap language search. Training with very large data sets using CE loss is considered to improve recognition accuracy at image level, and sMBR with fewer data sets improves accuracy as sequence discriminatory training. If a robust model is trained for generic scenarios, there is no need to train a model with very large data sets, and sequence discriminatory training with fewer data sets is sufficient. Therefore, on the basis of Shenma model, it is sufficient to train a new Amap model with small data sets using sMBR. As shown in Table 4, Shenma model represents the worst performance among three methods, as it is not trained for Amap scenarios. 2-layer Shenma model achieves a further reduction of AmaBR relative to MBR data, since it costs 8.1% less than the MBR model compared with MBR model."}, {"heading": "6. CONCLUSION", "text": "Distillation allows the deep LSTM model to transmit its knowledge lossless to a flat model, which could be distilled to a 2-layer model with very low RTF, so that it can present immediate detection results. As a result, its CER decreases by relative 14% compared to the 2-layer model that is regularly trained. In addition, transfer learning with sMBR is also beneficial. If a great model from generic scenarios is well trained, only 14% of the size of the training dataset is needed to train a more precise acoustic model for specific scenarios. Our future work includes 1) more effective methods for reducing CER by increasing the number of layers; 2) applying this training framework to connectionist temporal classification (CTC) and attention-based neural networks."}, {"heading": "7. REFERENCES", "text": "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun, \"Deep residual learning for image recognition,\" arXiv preprint arXiv: 1512.03385, 2015. [2] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\" IEEE Signal Processing Magazine, vol. 29, no. 6, Patrick Nguyen, 2012. [3] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton \"Speech recognition with deep recurrent neural networks,\" in 2013 IEEE international conference on acoustics, speech and signal processing."}], "references": [{"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "AndrewW Senior", "Fran\u00e7oise Beaufays"], "venue": "IN- TERSPEECH, 2014, pp. 338\u2013342.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding intermediate layers using linear classifier probes", "author": ["Guillaume Alain", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1610.01644, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2012, pp. 1223\u20131231.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Asynchronous stochastic gradient descent for dnn training", "author": ["Shanshan Zhang", "Ce Zhang", "Zhao You", "Rong Zheng", "Bo Xu"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660\u20136663.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Revisiting distributed synchronous sgd", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456\u2013464.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola"], "venue": "Advances in neural information processing systems, 2010, pp. 2595\u20132603.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering", "author": ["Kai Chen", "Qiang Huo"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal on Control and Optimization, vol. 30, no. 4, pp. 838\u2013 855, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["Wei Xu"], "venue": "arXiv preprint arXiv:1107.2490, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Exponential moving average model in parallel speech recognition training", "author": ["Tian Xu", "Zhang Jun", "Ma Zejun", "He Yi", "Wei Juan"], "venue": "arXiv preprint arXiv:1703.01024, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan Irsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modelling with cd-ctcsmbr lstm rnns", "author": ["Ha\u015fim Sak", "F\u00e9lix de Chaumont Quitry", "Tara Sainath", "Kanishka Rao"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden Markov models, maximum mutual information estimation, and the speech recognition", "author": ["Yves Normandin"], "venue": "problem, Ph.D. thesis,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1991}, {"title": "Discriminative training for large vocabulary speech recognition", "author": ["Daniel Povey"], "venue": "Ph.D. thesis, University of Cambridge,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Increasing the depth of neural network is a effective way to improve the performance, and convolutional neural network (CNN) has benefited from it in visual recognition task[1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 2, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 3, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 4, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 5, "context": "A conceptual tool called linear classifier probe is introduced to better understand the dynamics inside a neural network [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "Layer-wise pre-training is a successful method to train very deep neural networks [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "The convergence becomes harder with increasing the number of layers, even though the model is initialized with Xavier or its variants [8, 9].", "startOffset": 134, "endOffset": 140}, {"referenceID": 8, "context": "The convergence becomes harder with increasing the number of layers, even though the model is initialized with Xavier or its variants [8, 9].", "startOffset": 134, "endOffset": 140}, {"referenceID": 9, "context": "Asynchronous SGD is a successful effort for parallel training based on it [10, 11].", "startOffset": 74, "endOffset": 82}, {"referenceID": 10, "context": "Asynchronous SGD is a successful effort for parallel training based on it [10, 11].", "startOffset": 74, "endOffset": 82}, {"referenceID": 11, "context": "Besides, synchronous SGD is another effective effort, where the parameter server waits for every works to finish their computation and sent their local models to it, and then it sends updated model back to all workers [12].", "startOffset": 218, "endOffset": 222}, {"referenceID": 12, "context": "Model averaging method achieves linear speedup, as the final model is averaged from all parameters of local models in different workers [13, 14], but the accuracy decreases compared with single GPU training.", "startOffset": 136, "endOffset": 144}, {"referenceID": 13, "context": "Model averaging method achieves linear speedup, as the final model is averaged from all parameters of local models in different workers [13, 14], but the accuracy decreases compared with single GPU training.", "startOffset": 136, "endOffset": 144}, {"referenceID": 14, "context": "It can achieve improvement or no-degradation of recognition performance compared with mini-batch SGD on single GPU [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "It is demonstrated that the moving average of the parameters obtained by SGD performs as well as the parameters that minimize the empirical cost, and moving average parameters can be used as the estimator of them, if the size of training data is large enough [16].", "startOffset": 259, "endOffset": 263}, {"referenceID": 16, "context": "One pass learning is then proposed, which is the combination of learning rate schedule and averaged SGD using moving average [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "Exponential moving average (EMA) is proposed as a noninterference method[18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "Frame stacking can also speed up the training time [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "As a result, the knowledge of deep model can be distilled to a shallow model [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Xavier initialization can partially solve this problem [8], so this method is the regular initialization method of all training procedure.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Instead, layer-wise pre-training method is a effective way to train the weights of very deep architecture[7, 21].", "startOffset": 105, "endOffset": 112}, {"referenceID": 20, "context": "Instead, layer-wise pre-training method is a effective way to train the weights of very deep architecture[7, 21].", "startOffset": 105, "endOffset": 112}, {"referenceID": 4, "context": "The objects of conventional saturation check are gradients and the cell activations [5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "Gradients are clipped to range [-5, 5],", "startOffset": 31, "endOffset": 38}, {"referenceID": 21, "context": "In contrast, sequence discriminative training criterion has shown to further improve performance of neural network first trained with cross-entropy [22, 23, 24].", "startOffset": 148, "endOffset": 160}, {"referenceID": 22, "context": "In contrast, sequence discriminative training criterion has shown to further improve performance of neural network first trained with cross-entropy [22, 23, 24].", "startOffset": 148, "endOffset": 160}, {"referenceID": 23, "context": "In contrast, sequence discriminative training criterion has shown to further improve performance of neural network first trained with cross-entropy [22, 23, 24].", "startOffset": 148, "endOffset": 160}, {"referenceID": 21, "context": "We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and minimum phone error (MPE) [26].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and minimum phone error (MPE) [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and minimum phone error (MPE) [26].", "startOffset": 193, "endOffset": 197}, {"referenceID": 9, "context": "Therefore, parallel training with multiple GPUs is essential, and it makes use of data parallelism [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "All GPUs synchronize their local models with model average method after a mini-batch optimization [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 13, "context": "All GPUs synchronize their local models with model average method after a mini-batch optimization [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "It can achieve no-degradation of recognition accuracy with multi-GPUs [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Averaged SGD leverages the moving average (MA) \u03b8\u0304 as the estimator of \u03b8\u2217 [16]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "Knowledge transferring to small model is called distillation [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [27, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 23, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [27, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 4, "context": "Differentials of recurrent layers is limited to range [-10000,10000], while gradients are clipped to range [-5, 5] and cell activations clipped to range [-50, 50].", "startOffset": 107, "endOffset": 114}], "year": 2017, "abstractText": "Recurrent neural networks (RNNs), especially long shortterm memory (LSTM) RNNs, are effective network for sequential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recognition, because of their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-wise training and exponential moving average methods for deeper LSTM models. It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for online streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the distillation process. Therefore, the model trained with the proposed training framework reduces relative 14% character error rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also introduced in the framework. The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning.", "creator": "LaTeX with hyperref package"}}}