{"id": "1703.07115", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Layer-wise training of deep networks using kernel similarity", "abstract": "Deep learning has shown promising results in many machine learning applications. The hierarchical feature representation built by deep networks enable compact and precise encoding of the data. A kernel analysis of the trained deep networks demonstrated that with deeper layers, more simple and more accurate data representations are obtained. In this paper, we propose an approach for layer-wise training of a deep network for the supervised classification task. A transformation matrix of each layer is obtained by solving an optimization aimed at a better representation where a subsequent layer builds its representation on the top of the features produced by a previous layer. We compared the performance of our approach with a DNN trained using back-propagation which has same architecture as ours. Experimental results on the real image datasets demonstrate efficacy of our approach. We also performed kernel analysis of layer representations to validate the claim of better feature encoding.", "histories": [["v1", "Tue, 21 Mar 2017 09:53:51 GMT  (2632kb,D)", "http://arxiv.org/abs/1703.07115v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mandar kulkarni", "shirish karande"], "accepted": false, "id": "1703.07115"}, "pdf": {"name": "1703.07115.pdf", "metadata": {"source": "CRF", "title": "Layer-wise training of deep networks using kernel similarity", "authors": ["Mandar Kulkarni", "Shirish Karande"], "emails": ["shirish.karande)@tcs.com"], "sections": [{"heading": null, "text": "In the neeisrcnlhsAe\u00fccnlh hacu ide rf\u00fc ide eeisrrcnlhsrteeaeVnlhsrtee\u00fccnlhsrteeeee\u00fccnln rf\u00fc ide eeisrrlhsrgc\u00fce rf\u00fc ide eeisrmtlrrrteeaeVnlrlrrrrrteeeeeeerrrrlllllrrrrlrrlrrlrlrrlrlrrlrlrrlrlrlrrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrrlrlrlrlrlrlrlrlrlrrlrlrrlrlrlrrlrlrrrrlrrrlrlrrrrrrlrrrlrrrrrlrrrrrrrlrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "II. RELATED WORKS", "text": "It has been shown that unattended training leads to good generalization performance by appropriately initializing weightar Xiv: 170 3.07 115v 1 [cs.L G] 21 Mar 201 7 in a region near a good local minimum that provides feature representations that are abstractions at a high level of input. Larochelle et. al. [9] confirmed the hypothesis that the greedy, unattended training strategy actually contributes to optimization by initializing weights near a good local minimum. Moreover, it implicitly acts as a regulation that brings better generalization and produces good internal data representations."}, {"heading": "III. METHODOLOGY", "text": "In this section we describe an approach to developing MLP layer by layer. We use image data sets to validate the proposed approach. However, our training process is fairly general and should work with other data sets as well."}, {"heading": "A. Data pre-processing", "text": "We perform a pre-processing of the input characteristics before using them for training a shift. Each characteristic vector is independently normalized by subtracting its mean and dividing it by its norm. We observe that this process is necessary to avoid the saturation of neurons due to nonlinearity."}, {"heading": "B. Layer training", "text": "An MLP hierarchy of feature representation, in which a subsequent layer builds a representation on top of the data computed from previous layers, is directly linked to the previous layers. An input to our first layer is the set of n-labeled images [(t1, l1),..., (tn, ln), where t denotes a vectorized training image and l denotes the corresponding label. If M and N denote the dimension of an inputimage, t \u2264 Rd is where for a color image, d = MN \u00b7 3, while for a grayscale image, d = MN the input data is projected onto a p-dimensional space, in which p denotes the layer dimension. Figure 1 shows our network architecture. An operation on the kth layer of an MLP can be represented as the following wsXk = tanh (Dk \u2212 1Wk) Dk \u2212 1, Dk \u00b7 d \u00b7 Rk \u2212 1, where \u2212 1 (training matp \u2212 1), where \u2212 1 (training matp \u2212 1)."}, {"heading": "IV. NETWORK EVALUATION", "text": "To confirm the effectiveness of our approach, we have developed two evaluation methods as described below."}, {"heading": "A. Kernel PCA analysis procedure", "text": "Let Xk denote the characteristic representation of the training data at the lowest layer. Let [(xk1, l1), (xk2, l2),..., (xkn, ln) denote the characteristic representation of the individual characteristic vectors and the corresponding designations of the training data. xli denotes the characteristic representation of ith data point and li the characteristic representation of the data point (Figure). The characteristic representation at each layer is analyzed in a similar way as described in [12]. The kernel matrix K is calculated on the basis of the characteristic set Xk, as described in Equation. 2. Own value decomposition of the core matrix is determined as K = Uig UT (8), where eigenvectors (columns of U) are sorted unit length and eigenvalues (diagonal entries in equation) according to decreasing size Xk."}, {"heading": "B. Comparison with DNN", "text": "To evaluate the classification performance of our approach, we compared our results with a DNN. To have a valid comparison, we train a DNN that has exactly the same architecture as us, for example, if we train only a single layer, the DNN has a fully interconnected layer of dimension p, FC100 layer and a Softmax unit. We also included L2 regularization while we trained the DNN. The entire network is trained in the supervised manner by backpropagation. In the experimental results, we demonstrate the comparison of classification accuracy across multiple datasets with different sizes of training samples."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In order to verify the effectiveness of our method, we conducted experiments with real image data sets such as the handwritten digit data set MNIST [10], the image data set CIFAR - 10 [8]."}, {"heading": "A. MNIST handwritten digit dataset", "text": "The MNIST data set consists of grayscale images of handwritten digits. Each image has the dimension 28 x 28. The data set consists of 60k training images and 10k test images. In order to perform a kernel PCA analysis on layer representations, we trained two layers successively on the 5k training samples of the MNIST data set. The layer dimension p is set to 784 for each layer. \u03c3 value was set to 1 for all experiments. Fig. 3 shows the training and testing errors obtained in three layers from the core analysis. From Fig. 3 (a) one can see that when creating deeper representations, more energy is concentrated on fewer proprietary vectors with better training accuracy (lower training errors). Fig. 3 (b) shows the graph of the test error. In contrast to our representation, our representation coincides with the core analysis."}, {"heading": "B. CIFAR-10", "text": "The CIFAR-10 dataset consists of 32 x 32 color images corresponding to 10 object classes. It contains 50K training samples and 10k test samples. We conducted an experiment similar to the MNIST dataset, in which layers are learned sequentially, the layer dimension was set to 1500 and the \u03c3 value was set to 1 for the experiments. With the proposed optimization, we trained two layers one after the other. Kernel PCA analysis was performed on the layer representation, which was constructed from 5k training samples. Figure 5 shows the training and test error pattern with different numbers of Kernel PCA components. Note that here too, simpler and more accurate representations of input are achieved with our approach. Figure 2 (c) shows the first layer filters. Note that our approach learns rich features that allow better differentiation.Next, we compare the results with a DNN for different sizes of the test layer, which is trained in comparison with a training accuracy shown in Figure 6 of the DNN."}, {"heading": "VI. CONCLUSION", "text": "The core analysis of the layer-by-layer training shows that with each layer a better representation of the input data is achieved. As we train one shift at a time, the number of parameters to be done is relatively small. Therefore, the effect of the overfitting is minimized, which is clearly evident from the result of the CIFAR-10 dataset. Furthermore, we compared our classification accuracy with the DNN that is trained in the supervised way. Experimental results show that our approach provides comparable results with the DNN."}], "references": [{"title": "Autoencoders, unsupervised learning, and deep architectures. Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7 p", "author": ["P. Baldi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Greedy layer-wise training of deep networks. Advances in neural information processing systems", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H Larochelle"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Deep neural networks for object detection", "author": ["A.T. Christian Szegedy", "D. Erhan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Small-footprint keyword spotting using deep neural networks", "author": ["P.C. Guoguo Chen", "H. G"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.r", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T.N"], "venue": "Signal Processing Magazine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in cognitive sciences 11(10),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research 10,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L.B.Y. Bottou", "P. H"], "venue": "Proceedings of IEEE", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Kernel analysis of deep networks", "author": ["G. Montavon", "M.L. Braun", "K.R. M\u00fcller"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Layer-wise analysis of deep networks with gaussian kernels", "author": ["G. Montavon", "K.R. M\u00fcller", "M.L. Braun"], "venue": "Advances in Neural Information Processing Systems. pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Training mlps layer-by-layer with the information potential", "author": ["D. Xu", "J.C. Principe"], "venue": "Neural Networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}], "referenceMentions": [{"referenceID": 2, "context": "In computer vision, it has been successfully applied to problems such as object detection [4], English character recognition [10].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "In computer vision, it has been successfully applied to problems such as object detection [4], English character recognition [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "It also showed promising results for speech data where it has been applied for speech recognition [6] and spoken keyword spotting [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "It also showed promising results for speech data where it has been applied for speech recognition [6] and spoken keyword spotting [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "It has been studied that the effectiveness of deep networks lies in the layered representation [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "[11] performed the kernel analysis of the data representations at each layer of the network trained for a supervised classification task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Interestingly, it has been observed that as deeper and deeper kernels are built, simpler and more accurate representations of the learning problem are obtained [11].", "startOffset": 160, "endOffset": 164}, {"referenceID": 9, "context": "In addition to this, we also perform a kernel PCA analysis similar to [11],[12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "In addition to this, we also perform a kernel PCA analysis similar to [11],[12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 1, "context": "[2] proposed greedy layer-wise unsupervised training strategy for training a multi-layer deep network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] confirmed the hypothesis that the greedy layer-wise unsupervised training strategy indeed helps the optimization by initializing weights near a good local minimum.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] thoroughly studied the linear and non-linear auto encoders in the aspects such as their learning complexity, their horizontal and vertical composability in deep architectures and their fundamental connections to clustering, Hebbian learning, and information theory.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[13] demonstrated how information potential can be used to train a MLP (multilayer perceptron) layer-by-layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11],[12] performed a kernel analysis of a deep networks such as Convolution Neural Network (CNN), Multi-layer Perceptron (MLP) and Pretrained Multi-layer Perceptron (PMLP) trained for a supervised multi-class classification task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11],[12] performed a kernel analysis of a deep networks such as Convolution Neural Network (CNN), Multi-layer Perceptron (MLP) and Pretrained Multi-layer Perceptron (PMLP) trained for a supervised multi-class classification task.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "The feature representation at each layer is analyzed similar to the procedure as described in [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "(a) first layer 210 features of MNIST dataset using our approach, (b) first layer features for MLP and Pre-trained MLP (PMLP) trained on target and alternate tasks as explained in [12].", "startOffset": 180, "endOffset": 184}, {"referenceID": 8, "context": "To validate the efficacy of our method, we performed experiments with real world image datasets such as MNIST handwritten digit dataset [10], CIFAR - 10 image dataset [8].", "startOffset": 136, "endOffset": 140}, {"referenceID": 6, "context": "To validate the efficacy of our method, we performed experiments with real world image datasets such as MNIST handwritten digit dataset [10], CIFAR - 10 image dataset [8].", "startOffset": 167, "endOffset": 170}, {"referenceID": 10, "context": "It can be seen that, our representation agrees with the kernel analysis results reported in [12].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "2(b) shows the filters obtained in the first layer of MLP and Pretrained MLP (PMLP) when an architecture of four layers is trained [12].", "startOffset": 131, "endOffset": 135}], "year": 2017, "abstractText": "Deep learning has shown promising results in many machine learning applications. The hierarchical feature representation built by deep networks enable compact and precise encoding of the data. A kernel analysis of the trained deep networks demonstrated that with deeper layers, more simple and more accurate data representations are obtained. In this paper, we propose an approach for layer-wise training of a deep network for the supervised classification task. A transformation matrix of each layer is obtained by solving an optimization aimed at a better representation where a subsequent layer builds its representation on the top of the features produced by a previous layer. We compared the performance of our approach with a DNN trained using back-propagation which has same architecture as ours. Experimental results on the real image datasets demonstrate efficacy of our approach. We also performed kernel analysis of layer representations to validate the claim of better feature encoding.", "creator": "LaTeX with hyperref package"}}}