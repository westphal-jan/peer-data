{"id": "1703.07261", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Black-Box Data-efficient Policy Search for Robotics", "abstract": "The most data-efficient algorithms for reinforcement learning (RL) in robotics are based on uncertain dynamical models: after each episode, they first learn a dynamical model of the robot, then they use an optimization algorithm to find a policy that maximizes the expected return given the model and its uncertainties. It is often believed that this optimization can be tractable only if analytical, gradient-based algorithms are used; however, these algorithms require using specific families of reward functions and policies, which greatly limits the flexibility of the overall approach. In this paper, we introduce a novel model-based RL algorithm, called Black-DROPS (Black-box Data-efficient RObot Policy Search) that: (1) does not impose any constraint on the reward function or the policy (they are treated as black-boxes), (2) is as data-efficient as the state-of-the-art algorithm for data-efficient RL in robotics, and (3) is as fast (or faster) than analytical approaches when several cores are available. The key idea is to replace the gradient-based optimization algorithm with a parallel, black-box algorithm that takes into account the model uncertainties. We demonstrate the performance of our new algorithm on two standard control benchmark problems (in simulation) and a low-cost robotic manipulator (with a real robot).", "histories": [["v1", "Tue, 21 Mar 2017 15:13:04 GMT  (2942kb,D)", "https://arxiv.org/abs/1703.07261v1", "8 pages, 6 figures"], ["v2", "Sat, 22 Jul 2017 10:39:11 GMT  (2942kb,D)", "http://arxiv.org/abs/1703.07261v2", "Accepted at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017; Code atthis http URL; Video atthis http URL"]], "COMMENTS": "8 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["konstantinos chatzilygeroudis", "roberto rama", "rituraj kaushik", "dorian goepp", "vassilis vassiliades", "jean-baptiste mouret"], "accepted": false, "id": "1703.07261"}, "pdf": {"name": "1703.07261.pdf", "metadata": {"source": "CRF", "title": "Black-Box Data-efficient Policy Search for Robotics", "authors": ["Konstantinos Chatzilygeroudis", "Roberto Rama", "Rituraj Kaushik", "Dorian Goepp", "Vassilis Vassiliades", "Jean-Baptiste Mouret"], "emails": ["jean-baptiste.mouret@inria.fr"], "sections": [{"heading": null, "text": "In fact, most people who are able to move in the world, to move in the world, to move in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live"}, {"heading": "II. RELATED WORK", "text": "In this context, it should be noted that this project is based on the needs of people in developing countries."}, {"heading": "III. PROBLEM FORMULATION", "text": "We consider dynamic systems of form: xt + 1 = xt + f (xt, ut) + w (1) with continuously evaluated states x-RE and control u-RF, i.e. Gaussian system noise w and unknown transition dynamics f. Our goal is to find a deterministic policy \u03c0, u = \u03c0 (x | \u03b8) that maximizes the expected long-term reward by following political \u03c0 for T-time steps: J (\u03b8) = E [T-T-T = 1 r (xt)."}, {"heading": "IV. APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Learning dynamics model with Gaussian processes", "text": "We would like to have a model f \u2212 d \u2212 that matches the unknown dynamics f = q of our systems as accurately as possible and provides uncertainty information, relying on Gaussian processes (GPs). \u2212 A GP is an extension of the multivariate Gaussian distribution to a stochastic process of the finite dimension (for which any finite combination of dimensions will be a Gaussian distribution [14]. \u2212 As input quantities, we use tuples consisting of the state vector xt and the action vector ut, i.e., x \u00b2 t = (xt, ut). \u2212 For training purposes, we use the difference between the current state vector and the next: x \u2212 xt + 1 \u2212 xt \u00b2 RE. \u2212 We use E independent GPs to model each dimension of the difference vector GP xt."}, {"heading": "B. Learning the immediate reward function with a GP", "text": "Similar to the dynamic model, we use a general practitioner to learn the immediate reward function, which assigns a reward r (x) \u0445 R to each condition x: r (x) \u0445 GP (\u00b5r (x), kr (x, x \") (7) The GP predictions are calculated similarly to Eq.4 and 5."}, {"heading": "C. Policy Evaluation", "text": "Our goal is to maximize the expected cumulative reward (Eq. 2), which requires a prediction of state development (J q), since there is an uncertain transition model and an uncertain reward model. \u2212 \u2212 To do this in a deterministic manner, PILCO proposes to approximate the distribution of the state xt + 1, since the distribution of the state xt + 1 is mathematically expensive. \u2212 Alternatively, we can calculate a Monte Carlo approximation of the final distribution: at each step, we try a new state according to the GP of the model and its reward according to the reward model, Query the2In addition, PILCO requires the reward function as a prioritized policy to select the actions and use this new state to prioritize the next state. \u2212 xt By carrying out this process, we can obtain a good estimate of the expected cumulative reward, but many J-reward models we need to obtain a different state."}, {"heading": "D. Policy search", "text": "If we regard the maximization of J (\u03b8) as the optimization of a noise function, we can explicitly maximize these functions without calculation or estimation: we use only the noise measurements in the optimization algorithms. To do this, we build on all the work on noise function optimization [34], [35], and in particular on CMA-ES, one of the most successful black box optimization measures for noise functions. [15] CMA-ES (fig. 1) performs four steps in each generation: (1) Sample of new candidates according to a multi-stage distribution of mk and covariance \u03c32kCk, that is, we must have N (mk, \u03c32kCk) for i. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "E. Black-box Data-efficient Robot Policy Search", "text": "When we put everything together, we get the Black-DROPS algorithm (Alg. 1). First, random episodes of T-time steps are performed on the robot (Alg. 1: lines 4-12). In the learning loop, we first learn a probabilistic model of dynamics and a model of the reward function, and then we optimize the learned models using BIPOPCMAES with uncertainty management (Alg. 1: lines 14-16). Finally, the best policy procedure is executed on the robot, further data is collected and the main loop continues until the task is learned. Algorithm 1 Black-DROPS 1: Procedure BLACK-DROPS 2: Define policy: x-DROPS 2: Define policy \u2022 u 3: D = 4: for i = 1 \u2192 NR do."}, {"heading": "V. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Policy Representations", "text": "To emphasize the flexibility of Black-DROPS, we use a GP-based policy [13] and a feedback-based neural network function. Any other parameterized policy can be used (e.g. dynamic motion primitives).1) GP policy: If we only look at the mean, a Gaussian process can be used to map states to actions, that is, to define a policy: \u03c0 (x) = umax\u0443 (\u00b5 (x)) = umax\u0445 (k T (K + \u03c32nI) \u2212 1x) (11), where umax is the maximum value of u (different for each action dimension), \u03ba is a squeeze function like the one3https: / / github.com / beniz / libcmaesused in [13], x is the input state vector for the policy, K is the covariance matrix and its elements are calculated using the exponential kernel with automatic relevance determination as in equation."}, {"heading": "B. Metrics", "text": "1) Reward with increasing interaction time: This metric evaluates the quality of the solutions and the data efficiency of each algorithm.2) Acceleration when more cores are available: This metric evaluates how well each algorithm scales as the available hardware resources increase, regardless of the implementation (e.g. MATLAB vs C + +)."}, {"heading": "C. Remarks", "text": "We evaluate Black-DROPS at the pendulum and the cart pole and compare it with PILCO using 120 replicas across different CPU configurations. As an additional baseline, we evaluate a variant of our approach using deterministic GP models of dynamics (i.e., we only use the mean of the GPs) to quantify the importance of taking into account the uncertainty (variance) of the model in policy optimization. For BlackDROPS and the baseline, we use two different guidelines: a neural network policy (with a hidden layer and 10 hidden units) and a GP policy (with 10 and 20 pseudo-observations for the pendulum and the cart pole task, respectively). For PILCO, we used only the GP policy with the same parameters as for the other algorithms: a neural network policy (with a hidden layer and 10 hidden units) and a GP policy (with 10 and 20 and 20 pseudo-pole observations for the pendulum task, respectively)."}, {"heading": "VI. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Task 1: Inverted Pendulum", "text": "This simulated system consists of a free-swinging pendulum with mass m = 1 kg and length l = 1m. The goal is to learn a controller to swing the pendulum upwards and balance it in the inverse position by applying a torque. \u2022 Condition: xpend = [\u03b8, \u03b8]. \u2022 Consequences: upend = upend, \u2212 2.5 \u2264 upend \u2264 2.5N. \u2022 To avoid angular discontinuities, we transform the GPs \"input, the reward function and the policy that has to be: xinput = [0, 0], sinful (\u03b8), sinful (\u03b8)]. The MATLAB implementation of PILCO uses this transformation by default4. \u2022 Reward: We use the same reward function as PILCO5. This is a saturated reward function based on distance (r (x) = exp (\u2212 x)."}, {"heading": "B. Task 2: Cart-pole Swing-Up", "text": "This simulated system consists of a shopping cart with mass M = 0.5 kg, which runs along a distance, and a freely oscillating pendulum. \u2022 While some of the time differences may be due to the language used (e.g. that C + + is faster than MATLAB or MATLAB in matrix calculations), it is important that a parallel algorithm with enough CPUs can ultimately surpass a sequential approach based on gradients. \u2212 The goal is to learn a controller that applies horizontal forces to the cart to avoid the position of the shopping cart, to find the speed of the shopping cart, the angle of the pendulum and the angular velocity of the pendulum. \u2212 The goal is to learn a controller that applies horizontal forces to the cart in order to swing the pendulum upwards and to find it in the inverse position of the shopping cart (CO), the speed of the shopping cart, the velocity of the pendulum, the velocity of the pendulum and the angle of the pendulum, and the velocity of the pendulum."}, {"heading": "C. Task 3: 4-DOF Manipulator", "text": "We have applied Black-DROPS to a physical velocitycontrolled 4-DOF robotic arm (Fig. 6, 10 replicated). We assume that we can observe only the angles of the joints of the arm and that the reward function rarm is initially unknown. \u2022 Condition: xarm = [q0, q1, q2, q3] that the end effect quickly reaches a certain position (Fig. 6A). We compare Black-DROPS with the baseline without variance. \u2022 Condition: xarm = [q0, q1, q2, q3] that the sequences R4, x0, 0, 0, 0, 0, 0]. \u2022 Measures: uarm = [v0, v1, v2, v3], where \u2212 1.0 \u2264 vi \u2264 1.0 rad / s, i = 0, 1, 2, 3. \u2022 Reward: The unknown (to the algorithm) reward function has a similar shape to Eq. 13: rarm (qx) that the initial position (x) of the results (1)."}, {"heading": "VII. CONCLUSION AND DISCUSSION", "text": "Black-DROPS removes several constraints imposed by analytical approaches (reward and policy types) while remaining competitive in terms of data efficiency and computing time. In three different tasks, it achieved results similar to the state of the art (PILCO) and is faster when using multiple cores at the same time. We expect that Black-DROPS \"ability to scale with the number of cores will be even more advantageous on future computers with more cores and / or GPUs. Using variance in optimization is one of the key components to learn with as little interaction time as possible. However, the learned dynamic models are safe only in previously visited areas of government space and could therefore drive optimization to local optimum when multiple and diverse solutions exist. In future work, we will explore ways to explore more without compromising data efficiency."}], "references": [{"title": "How UGVs physically fail in the field", "author": ["J. Carlson", "R.R. Murphy"], "venue": "IEEE Trans. on Robotics, vol. 21, no. 3, pp. 423\u2013437, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Robots that can adapt like animals", "author": ["A. Cully", "J. Clune", "D. Tarapore", "J.-B. Mouret"], "venue": "Nature, vol. 521, no. 7553, pp. 503\u2013507, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Reset-free Trial-and-Error Learning for Data-Efficient Robot Damage Recovery", "author": ["K. Chatzilygeroudis", "V. Vassiliades", "J.-B. Mouret"], "venue": "arXiv:1610.04213, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Emergency response to the nuclear accident at the Fukushima Daiichi Nuclear Power Plants using mobile rescue robots", "author": ["K. Nagatani"], "venue": "Journal of Field Robotics, vol. 30, no. 1, pp. 44\u201363, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on policy search for robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics, vol. 2, no. 1, pp. 1\u2013142, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Policy gradient reinforcement learning for fast quadrupedal locomotion", "author": ["N. Kohl", "P. Stone"], "venue": "Proc. of ICRA, vol. 3. IEEE, 2004, pp. 2619\u20132624.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Bayesian optimization for learning gaits under uncertainty", "author": ["R. Calandra", "A. Seyfarth", "J. Peters", "M. Deisenroth"], "venue": "Annals of Mathematics and Artificial Intelligence (AMAI), 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic gait optimization with Gaussian process regression", "author": ["D.J. Lizotte", "T. Wang", "M.H. Bowling", "D. Schuurmans"], "venue": "IJCAI, vol. 7, 2007, pp. 944\u2013949.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Model learning for robot control: a survey", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": "Cognitive Processing, vol. 12, no. 4, pp. 319\u2013340, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian processes for data-efficient learning in robotics and control", "author": ["M.P. Deisenroth", "D. Fox", "C.E. Rasmussen"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 2, pp. 408\u2013423, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Completely derandomized selfadaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary computation, vol. 9, no. 2, pp. 159\u2013195, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 229\u2013256, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Policy gradients with parameter-based exploration for control", "author": ["F. Sehnke"], "venue": "Proc. of Artificial Neural Networks. Springer, 2008, pp. 387\u2013396.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "Machine Learning, vol. 84, pp. 171\u2013203, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "JMLR, vol. 11, pp. 3137\u2013 3181, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural evolution strategies", "author": ["D. Wierstra"], "venue": "JMLR, vol. 15, no. 1, pp. 949\u2013980, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional relation between CMA evolution strategies and natural evolution strategies", "author": ["Y. Akimoto", "Y. Nagata", "I. Ono", "S. Kobayashi"], "venue": "Proc. of PPSN. Springer, 2010, pp. 154\u2013163.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Robot skill learning: From reinforcement learning to evolution strategies", "author": ["F. Stulp", "O. Sigaud"], "venue": "Paladyn, Journal of Behavioral Robotics, vol. 4, no. 1, pp. 49\u201361, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Taking the human out of the loop: A review of Bayesian optimization", "author": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"], "venue": "Proc. of the IEEE, vol. 104, no. 1, pp. 148\u2013175, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Using confidence bounds for exploitation-exploration tradeoffs", "author": ["P. Auer"], "venue": "JMLR, vol. 3, pp. 397\u2013422, 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Autonomous helicopter control using reinforcement learning policy search methods", "author": ["J.A. Bagnell", "J.G. Schneider"], "venue": "Proc. of ICRA, vol. 2. IEEE, 2001, pp. 1615\u20131620.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Autonomous inverted helicopter flight via reinforcement learning", "author": ["A.Y. Ng", "A. Coates", "M. Diel", "V. Ganapathi", "J. Schulte", "B. Tse", "E. Berger", "E. Liang"], "venue": "Experimental Robotics IX. Springer, 2006, pp. 363\u2013372.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "Proc. of NIPS, 2014, pp. 1071\u20131079.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Gaussian processes and reinforcement learning for identification and control of an autonomous blimp", "author": ["J. Ko", "D.J. Klein", "D. Fox", "D. Haehnel"], "venue": "Proc. of ICRA, 2007, pp. 742\u2013747.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Model-based contextual policy search for dataefficient generalization of robot skills", "author": ["A. Kupcsik"], "venue": "Artificial Intelligence, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Model-based policy gradients with parameter-based exploration by least-squares conditional density estimation", "author": ["V. Tangkaratt", "S. Mori", "T. Zhao", "J. Morimoto", "M. Sugiyama"], "venue": "Neural Networks, vol. 57, pp. 128\u2013140, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "PEGASUS: a policy search method for large MDPs and POMDPs", "author": ["A.Y. Ng", "M. Jordan"], "venue": "Proc. of Uncertainty in Artificial Intelligence. Morgan Kaufmann, 2000, pp. 406\u2013415.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural Networks, vol. 21, no. 4, pp. 682\u2013697, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Limbo: A fast and flexible library for Bayesian optimization", "author": ["A. Cully", "K. Chatzilygeroudis", "F. Allocati", "J.-B. Mouret"], "venue": "arxiv:1611.07343, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Evolutionary optimization in uncertain environments-a survey", "author": ["Y. Jin", "J. Branke"], "venue": "IEEE Trans. on Evolutionary Computation, vol. 9, no. 3, pp. 303\u2013317, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "A method for handling uncertainty in evolutionary optimization with an application to feedback control of combustion", "author": ["N. Hansen", "A.S. Niederberger", "L. Guzzella", "P. Koumoutsakos"], "venue": "IEEE Trans. on Evolutionary Computation, vol. 13, no. 1, pp. 180\u2013197, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Genetic algorithms, selection schemes, and the varying effects of noise", "author": ["B.L. Miller", "D.E. Goldberg"], "venue": "Evolutionary Computation, vol. 4, no. 2, pp. 113\u2013131, 1996.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Benchmarking a BI-population CMA-ES on the BBOB- 2009 function testbed", "author": ["N. Hansen"], "venue": "Proc. of GECCO. ACM, 2009, pp. 2389\u2013 2396.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Benchmarking a BI-population CMA-ES on the BBOB-2009 noisy testbed", "author": ["\u2014\u2014"], "venue": "Proc. of GECCO. ACM, 2009, pp. 2397\u20132402.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Patchwork kriging for large-scale gaussian process regression", "author": ["C. Park", "D. Apley"], "venue": "arXiv preprint arXiv:1701.06655, 2017.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed gaussian processes", "author": ["M.P. Deisenroth", "J.W. Ng"], "venue": "arXiv:1502.02843, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving PILCO with bayesian neural network dynamics models", "author": ["Y. Gal", "R.T. McAllister", "C.E. Rasmussen"], "venue": "Data-Efficient Machine Learning workshop, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "Proc. of ICML, 2015.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Reinforcement Learning (RL) can help robots adapt to unforeseen situations, such as being damaged [1], [2], [3] or stranded [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": ", deep learning) that rely on the availability of very large datasets or fast simulations1 [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "1As an illustration, the deep Q-learning algorithm needed about 38 days of interaction to learn to play Atari 2600 games [5] (only four possible actions), which would be hardly conceivable to a achieve with a robot.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": ", policy gradient algorithms [7], [8] or Bayesian optimization [2], [9], [10]) which only use the (cumulative) reward at the end of each episode.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "One of the best ways to take advantage of this sequential state recording is to learn a dynamical model of the robot [11], and then exploit it either for model-predictive control [12] or to find an optimal policy offline [7].", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "One of the best ways to take advantage of this sequential state recording is to learn a dynamical model of the robot [11], and then exploit it either for model-predictive control [12] or to find an optimal policy offline [7].", "startOffset": 221, "endOffset": 224}, {"referenceID": 11, "context": "The PILCO (Probabilistic Inference for Learning COntrol) algorithm [13], which is one of the state-of-theart algorithms for data-efficient model-based policy search, follows this strategy by alternating between two steps, (1) learning a dynamical model with Gaussian processes [14], (2) using a gradient-based optimizer to search for a policy that maximizes the expected reward, taking the uncertainty of the model into account.", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "The PILCO (Probabilistic Inference for Learning COntrol) algorithm [13], which is one of the state-of-theart algorithms for data-efficient model-based policy search, follows this strategy by alternating between two steps, (1) learning a dynamical model with Gaussian processes [14], (2) using a gradient-based optimizer to search for a policy that maximizes the expected reward, taking the uncertainty of the model into account.", "startOffset": 277, "endOffset": 281}, {"referenceID": 8, "context": ", parameterized state automata, like in [9]).", "startOffset": 40, "endOffset": 43}, {"referenceID": 11, "context": ", typically more than 5 minutes on a modern computer between each episode for the cart-pole benchmark), because they rely on computationally expensive methods to do approximate inference for each step of the policy evaluation [13].", "startOffset": 226, "endOffset": 230}, {"referenceID": 13, "context": "By contrast, Monte Carlo approaches and population-based black-box optimizers like CMA-ES [15] (1) do not put any constraint on the reward functions and policies, and (2) are straightforward to parallelize, which can make them competitive with analytical approaches when several cores are available.", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": ", CMA-ES [15]), which saves a lot of computation: only the ranking of potential solutions matters.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Direct policy search (PS) methods have been successful in robotics as they can easily be applied in highdimensional continuous state-action RL problems [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 14, "context": "REINFORCE [16] is an early policy gradient method which", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "Policy gradients with parameter-based exploration (PGPE) [17] address this problem by transferring exploration to parameter space.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "The PoWER (Policy learning by Weighting Exploration with the Returns) algorithm [18] uses probability-weighted averaging, which has the property", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "of following the natural gradient without computing it [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "The Policy Improvements with Path Integrals (PI) [19] algorithm does not make such an assumption.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "When the reward function is compatible with both PoWER and PI, the algorithms have identical performance [19].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "This can be addressed by the Natural Evolution Strategies (NES) [20] and Covariance Matrix Adaptation ES (CMA-ES) [15] families of algorithms, which are population-based Black-Box Optimizers (BBO).", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "This can be addressed by the Natural Evolution Strategies (NES) [20] and Covariance Matrix Adaptation ES (CMA-ES) [15] families of algorithms, which are population-based Black-Box Optimizers (BBO).", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "NES and CMA-ES are closely related, as the latter performs an approximate natural gradient ascent [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "and was shown to be a special case of CMA-ES [22].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Bayesian optimization [23] is a particular family of BBO that can be very data-efficient by building a surrogate model of the objective function (i.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": ", using upper confidence bounds [24]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "It can drastically decrease the evaluation time when optimizing gaits [10] or when finding compensatory behaviors for damaged robots [2].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "It can drastically decrease the evaluation time when optimizing gaits [10] or when finding compensatory behaviors for damaged robots [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": ", transition and reward function) of the system from data and inferring the optimal policy from the model [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": "Probabilistic models have been more successful than deterministic ones, as they provide an estimate about the uncertainty of their approximation which can be incorporated into long-term planning [13].", "startOffset": 195, "endOffset": 199}, {"referenceID": 23, "context": "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "For example, local linear models have been used in [25], [26], [27], Gaussian processes (GPs) in [28], [13], [29] and least-squares", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "conditional density estimation in [30].", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "Early examples of such model-based PS include applications on helicopter hovering [25], [26] and blimp control [28].", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Early examples of such model-based PS include applications on helicopter hovering [25], [26] and blimp control [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "Early examples of such model-based PS include applications on helicopter hovering [25], [26] and blimp control [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 29, "context": "(MDP) or partially-observable MDP (POMDP) into a deterministic POMDP [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 28, "context": "Both the model-based PGPE [30] and the PILCO [13] algorithm use gradient-based policy updates.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "Both the model-based PGPE [30] and the PILCO [13] algorithm use gradient-based policy updates.", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "Gradient-free methods, such as the Model-Based Relative Entropy PS (M-REPS) [29] and the Model-Based Guided PS (M-GPS) [27], do not have these requirements.", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "Gradient-free methods, such as the Model-Based Relative Entropy PS (M-REPS) [29] and the Model-Based Guided PS (M-GPS) [27], do not have these requirements.", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "This constraint limits the information loss of the updates [32].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "Overall, the current consensus [7] is that (1) modelbased algorithms are more data-efficient than direct PS, (2) in model-based PS, it is crucial to account for potential model errors during policy learning, and (3) deterministic approximate inference and analytic computation of policy gradients is required to make model-based PS computationally tractable.", "startOffset": 31, "endOffset": 34}, {"referenceID": 12, "context": "A GP is an extension of multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 12, "context": "In this paper, we use the exponential kernel with automatic relevance determination [14]:", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "where \u03b4pq equals to 1 when p = q and 0 otherwise, and [\u039bd, \u03c3 2 d, \u03c3 2 nd ] is the vector of hyper-parameters of the kernel (length scales for each dimension of the observations, signal variance and noise) found through Maximum Likelihood Estimation [14].", "startOffset": 249, "endOffset": 253}, {"referenceID": 31, "context": "We use the limbo C++11 library for GP regression [33].", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "To do so in a deterministic way2, PILCO proposes to approximate the distribution of state xt+1 given the distribution of state xt and the action ut using moment matching [13], and then propagates from state to state until reaching the end of the episode; however, this sequential approach accumulates errors over time, is not easy to parallelize, and is compu-", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "Please note that in this example we use a bigger population than in our work, as advised by the authors of CMA-ES [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "By performing this process many times, we can get a good estimate of the expected cumulative reward, but many samples are needed to obtain a good estimate [29].", "startOffset": 155, "endOffset": 159}, {"referenceID": 32, "context": "To do so, we build on all the work about noisy function optimization [34], [35], and especially on CMA-ES, one of the most successful blackbox optimizer for noisy functions [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "To do so, we build on all the work about noisy function optimization [34], [35], and especially on CMA-ES, one of the most successful blackbox optimizer for noisy functions [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "To do so, we build on all the work about noisy function optimization [34], [35], and especially on CMA-ES, one of the most successful blackbox optimizer for noisy functions [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 32, "context": "Overall, these steps are only marginally impacted by noise in the performance function, as confirmed by empirical experiments with noisy functions [34].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "One can also observe that because CMA-ES samples several solutions around a mean mk, it performs many evaluations of similar parameters, which are then averaged: this implicit averaging [34], [36] has many similarities with re-evaluating noisy solutions to estimate their expectation.", "startOffset": 186, "endOffset": 190}, {"referenceID": 34, "context": "One can also observe that because CMA-ES samples several solutions around a mean mk, it performs many evaluations of similar parameters, which are then averaged: this implicit averaging [34], [36] has many similarities with re-evaluating noisy solutions to estimate their expectation.", "startOffset": 192, "endOffset": 196}, {"referenceID": 35, "context": "In this work, we use BIPOP-CMA-ES with restarts [37], which is one of best CMA-ES variants on benchmarks with both noiseless and noisy functions [37], [38].", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "In this work, we use BIPOP-CMA-ES with restarts [37], which is one of best CMA-ES variants on benchmarks with both noiseless and noisy functions [37], [38].", "startOffset": 145, "endOffset": 149}, {"referenceID": 36, "context": "In this work, we use BIPOP-CMA-ES with restarts [37], which is one of best CMA-ES variants on benchmarks with both noiseless and noisy functions [37], [38].", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "[35] to improve the behavior of CMA-ES with noisy functions (called UH-CMA-ES).", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "We therefore implement the following strategy: (1) at each generation, we quantify the uncertainty of the ranking by reevaluating \u03bbreev < \u03bb randomly selected candidates from the population and count the number of rank changes (see [35]", "startOffset": 231, "endOffset": 235}, {"referenceID": 11, "context": "To highlight the flexibility of Black-DROPS, we use a GP-based policy [13] and a feed-forward neural networkbased one.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "com/beniz/libcmaes used in [13], x is the input state vector to the policy, K is the covariance matrix and its elements are computed using the exponential kernel with automatic relevance determination as in Eq.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "where \u03c3c controls the width of the reward function, Q is a weight matrix, x\u2217 is the target state and r(x) \u2208 [0, 1].", "startOffset": 108, "endOffset": 114}, {"referenceID": 0, "context": "1, px corresponds to the end-effector position in state x, p\u2217 is the goal position of the endeffector and rarm(x) \u2208 [0, 1].", "startOffset": 116, "endOffset": 122}, {"referenceID": 37, "context": "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].", "startOffset": 43, "endOffset": 47}, {"referenceID": 38, "context": "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].", "startOffset": 49, "endOffset": 53}, {"referenceID": 39, "context": "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].", "startOffset": 153, "endOffset": 157}, {"referenceID": 40, "context": "Possible solutions include using local GPs [39], [40] or to stop using GPs and make use of recent advances in neural networks with uncertain predictions [41], [42].", "startOffset": 159, "endOffset": 163}], "year": 2017, "abstractText": "The most data-efficient algorithms for reinforcement learning (RL) in robotics are based on uncertain dynamical models: after each episode, they first learn a dynamical model of the robot, then they use an optimization algorithm to find a policy that maximizes the expected return given the model and its uncertainties. It is often believed that this optimization can be tractable only if analytical, gradient-based algorithms are used; however, these algorithms require using specific families of reward functions and policies, which greatly limits the flexibility of the overall approach. In this paper, we introduce a novel model-based RL algorithm, called BlackDROPS (Black-box Data-efficient RObot Policy Search) that: (1) does not impose any constraint on the reward function or the policy (they are treated as black-boxes), (2) is as dataefficient as the state-of-the-art algorithm for data-efficient RL in robotics, and (3) is as fast (or faster) than analytical approaches when several cores are available. The key idea is to replace the gradient-based optimization algorithm with a parallel, blackbox algorithm that takes into account the model uncertainties. We demonstrate the performance of our new algorithm on two standard control benchmark problems (in simulation) and a low-cost robotic manipulator (with a real robot).", "creator": "LaTeX with hyperref package"}}}