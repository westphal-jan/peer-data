{"id": "1703.07348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "CNN-MERP: An FPGA-Based Memory-Efficient Reconfigurable Processor for Forward and Backward Propagation of Convolutional Neural Networks", "abstract": "Large-scale deep convolutional neural networks (CNNs) are widely used in machine learning applications. While CNNs involve huge complexity, VLSI (ASIC and FPGA) chips that deliver high-density integration of computational resources are regarded as a promising platform for CNN's implementation. At massive parallelism of computational units, however, the external memory bandwidth, which is constrained by the pin count of the VLSI chip, becomes the system bottleneck. Moreover, VLSI solutions are usually regarded as a lack of the flexibility to be reconfigured for the various parameters of CNNs. This paper presents CNN-MERP to address these issues. CNN-MERP incorporates an efficient memory hierarchy that significantly reduces the bandwidth requirements from multiple optimizations including on/off-chip data allocation, data flow optimization and data reuse. The proposed 2-level reconfigurability is utilized to enable fast and efficient reconfiguration, which is based on the control logic and the multiboot feature of FPGA. As a result, an external memory bandwidth requirement of 1.94MB/GFlop is achieved, which is 55% lower than prior arts. Under limited DRAM bandwidth, a system throughput of 1244GFlop/s is achieved at the Vertex UltraScale platform, which is 5.48 times higher than the state-of-the-art FPGA implementations.", "histories": [["v1", "Wed, 22 Mar 2017 01:31:23 GMT  (1675kb)", "http://arxiv.org/abs/1703.07348v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AR", "authors": ["xushen han", "dajiang zhou", "shihao wang", "shinji kimura"], "accepted": false, "id": "1703.07348"}, "pdf": {"name": "1703.07348.pdf", "metadata": {"source": "CRF", "title": "CNN-MERP: An FPGA-Based Memory-Efficient Reconfigurable Processor for Forward and Backward Propagation of Convolutional Neural Networks", "authors": ["Xushen Han", "Dajiang Zhou", "Shihao Wang", "Shinji Kimura"], "emails": ["hanxushen@fuji.waseda.jp"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive themselves if they do not feel able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "A. Forward propagation phase", "text": "As shown in Figure 2, k \u00d7 k elements in an input card are combined to form an input window guided by a filter. A filter is defined as the winding operations for a pair of input windows and a kernel. Output of a filter is a partial result of the output element represented in the following equation. [] [] [1 10 0,,,, k kpartielle i jy r x r i c j Ker i j - = = = + + + \u00e5\u00e5, where [r, c] corresponds to the coordinates in the output card y. Ker means cores. A group of filters contributes the same output element of y, while colocalized filters are defined as filters in different groups referring to the same window. Activation layers provide nonlinearity using activation functions. Currently, one popular activation function is the Rectified Linear Units (LU)."}, {"heading": "B. Backward propagation phase", "text": "The BP phase consists of two steps: \u03b4 propagation and kernel update. \u03b4 is defined as a partial derivative of the cost function J taking into account the output characteristic y in the FP. \u03b4 can be determined on the basis of the propagation relationship of y between layers and the chain rule of partial derivatives over propagations. Since each element has Y derivatives, the dimensions of \u03b4 and y remain the same. However, the calculations of the propagation layers are different. Convolution layers are similar to those in the FP phase. Differences include 180-degree rotated cores and input cards with zero padding of k-1. Activation layers accept outputs of corresponding activation layers in FP to calculate derivatives of ReLU. Then the derivatives and inputs are multiplied to obtain output volumes."}, {"heading": "A. Memory traffic evaluation", "text": "The parallelism of the processing units (CUs) determines both the throughput and bandwidth requirements of a processor. Figure 3 shows the arrangement of three parallel CUs with a detailed design of multiplications and additions. If you look at operations in a filter, all multiplications are arranged in parallel and equipped with additions. This parallelism utilizes a high throughput at the cost of high bandwidth consumption, which is discussed in the following paragraph.We use a typical revolutionary layer, followed by activation and pooling layers as an example of the evaluation of external storage traffic. These layers are the 2nd layer of AlexNet [4], which is represented in Table 1.Two characters should be noted. One is the revolutionary layer, followed by activation and pooling layers as an example of the evaluation of external storage traffic. These layers are the 2nd layer of AlexNet [4], which duplicates data traffic."}, {"heading": "B. Strategy 1: on/off-chip data allocation", "text": "In theory, however, such allocation is impractical for our FPGA due to space constraints. In the typical layer shown in Table 1, kernels require 614.4 KB of memory, while the requirement for function boards is 17.9 MB + 47.8 MB \u00d7 4 + 11.0MB = 220.1MB. Meanwhile, the caching of all cores and the caching of all function boards contribute to the same degree of traffic reduction - each of them reduces half of the inputs, as each multiplication requires only one off-chip operation. By comparison, storing all cores on the chip is more appropriate. As a result, the external traffic for the calculation only comes from the function boards. Such a strategy can halve the external traffic of the inputs, thereby reducing the normalized traffic to the chip (14.1GB / 6MB flop = 28.5GB)."}, {"heading": "C. Strategy 2: reuse input windows between filters", "text": "Figure 3: Parallelism within CUs and parallelism between CUs. Figure 4 illustrates the reuse of data from an active window. m: The filters corresponding to the current active window are processed continuously by CUs (green part), the active window is held by a 32 x k x k bit register so that CUs can access this window at the same time. m: This data reuse strategy can reduce the input memory traffic to 1 / m of the conventional data flow. Normalized data traffic decreases to (114.7 GB \u00b2 2 / 128 + 2.3 GB) / 28.6 GFlop = 96.1 MB / GFlop."}, {"heading": "D. Strategy 3: avoid off-chip traffic of intermediate results", "text": "Strategy 2 generates intermediate results of the output elements, while Strategy 3 collects intermediate results to avoid off-chip traffic. 198.2MB of disk space is required for the second revolutionary layer of AlexNet [4]. Figure 4 contains an efficient data flow based on the accumulation of distributed outputs. Accumulators are designed for m-outputs that correspond to all output functions. In order to accumulate output functions, the selection of input windows must jump between distributed windows, as these windows contribute to the same group of output elements. Specifically, in our design we determine the sequence of jumps from the first to the last. Thereafter, the calculations are completed in connection with a group of distributed output elements. Therefore, these elements can be forwarded for the next new elements. The accumulators for the distributed elements take 32 x mbits registers. Only 1 / n of original data are streamed after accumulation + 2.71MB of data traffic decreased to 14.3GB."}, {"heading": "E. Strategy 4: reuse input elements in a feature map", "text": "In our view, most of the overlapping elements can be reused for the next window if we promise that the window flow is as close to each other as possible. Figure 5 shows the overlapping relationship between 3 \u00d7 3 windows. We decide the window quickly from left to right. Due to the dependency, the k-lines in which the current window is located can be cached on the chip. If the next window is needed, only one additional new element is displayed instead of the entire window. To avoid conflicts with strategy 2 and 3, we decide to store k-lines in all input feature maps. As shown in Figure 4, when a group of collocalized output elements is finished, we flip over the next adjacent window of the input feature maps. Cache is required for all input feature maps. Therefore, a total of m x Cin x 4B is needed on the chip memory, where caches in the number of input feature maps can represent the typical number of 16 maps of the input feature maps."}, {"heading": "F. Strategy 5: super layer integration", "text": "Strategies 2-4 use data in Convolutionary Layers. Between layers, the redundancy of load / storage of feature maps is still present, as the output and input feature maps of adjacent layers are exactly the same. In addition, the structure of CNNs can be broken down into several cascades of Convolutionary Layers, followed by activation and pooling layers. As a result, we combine each cascade of the three layers as a superlayer to reduce storage traffic between them. Figure 4 Data flow for strategies 2-4. Solid squares in input feature maps describe current co-localized windows, while dash squares stand for adjacent co-localized windows. Reuse in a feature map corresponds to black overlapped part. The active windows are reused by co-localized filters (green part), which corresponds to Strategy 2. The data flow of Strategy 3 and 4 is displayed as red arrows and blue arrows, again corresponding to adjacent data mapping 5."}, {"heading": "A. Processor overview", "text": "Figure 7 shows an overview of CNN-MERP. All cores are loaded into SRAM before executing a batch. To facilitate kernel transfer, each processing unit is equipped with an SRAM to store all associated data. We also design an input memory hierarchy corresponding to the strategy 2-4 presented in Section IV."}, {"heading": "B. Computational units", "text": "The implementation of arithmetic units is based on the parallelism examined in Section 4. As shown in Figure 8, we arrange k \u00b7 k floating-point multipliers in parallel, followed by k \u00b7 k-1 floating-point adapters. To achieve higher throughput, we introduce add-ons and multipliers in two stages. Input memory hierarchy A problem with unbalanced data width exists according to Strategy 2 and 4. On the one hand, input memory only receives collocalized elements for one place in each cycle. On the other hand, CU needs input windows with k \u00b7 k layer. Therefore, we use k \u00b7 k two-location SRAM banks to store the input data. As shown in Figure 9, one of the SRAM banks receives a new element from the router in each cycle, while each SRAM bank provides an element as an input window. As windows are moved, we implement a logic circle to restore the position of the windows."}, {"heading": "D. Activation and pooling", "text": "Figure 10 shows the implementation of activation and pooling layers. ReLUs and pooling units (PUs) are designed for the calculation of each element. Since collocalized output elements are generated from the revolutionary layers simultaneously, we use R ReLU and PU to calculate collocalized elements in character cards in parallel. The parallelism of ReLU and PU depends on the throughput of the CUs. In the typical 2nd layer of AlexNet, shown in Table 1, we implement 16 CUs, which requires 48 x 128 x 16 x 384 clock cycles to calculate 128 parallel elements. Therefore, PUs have 384 clock cycles to complete the subsampling of 128 output elements. To ensure throughput, we implement R = 2 ReLUs and PUs.Figure 7 architecture of CNN-MERP. This architecture works for CNN hardware configured for the differences between KBP and KBP."}, {"heading": "A. Combining two levels of reconfigurability", "text": "Compatibility of different layers for the same hardware is a difficult problem, for which there are generally two solutions: one solution takes advantage of the reconfigurable functionality of the FPGA, but costs time to reconfigure; the other is based on logical circuits to support the new desirable data flow on the chip, which has an unsatisfactory usage of computing resources when the size of the kernel changes; and since the size of the CUs is difficult to change, we need to implement a large-format CU to support all smaller cores, which leaves a lot of idle multipliers and crosses at a small core size. Therefore, we combine two levels of configurability: If the size of the cores does not change, we implement a module to change the data flow so that the inactivity of the CUs is avoided as much as possible."}, {"heading": "B. Logic based reconfiguration", "text": "Logic modules are implemented to monitor the current active input and output feature maps that prevent CUs from using invalid input / output feature maps. A naive idea is that we treat all invalid input / output maps as 0. This method can lead to the correct results of the calculation, but many zeros are passed to CUs that reduce effective performance. Our solution is to control the flow of data in such a way that invalid input / output maps are not included in the calculation. Registers are designed for the current index of adjacent windows and output elements. As a result, the current index must vary within the valid range, i.e. the real number of input and output feature maps. These two numbers also come from outside and can be updated for a new layer. As a result, CUs provide higher effective throughput in different layers. For example, the same idea of 3rd and 4th level of Alexa Maps are included."}, {"heading": "C. Multi-boot based reconfiguration of FPGA", "text": "In [20], Xilinx FPGA can be reconfigured from flash to chip, resulting in a different functionality. Multi-boot is realized through the BPI interface on the FPGA Board. Before multi-boot, we store all bit streams, each of which corresponds to the functionality of one or more superlayers in the Flash, and write down the corresponding addresses. When the calculation of a superlayer is completed, the soft microprocessor core of MicroBlaze controls the loading of a next layer. Figure 13 shows the flow of the reconfiguration. The HWICAP IP core of the corresponding layer is triggered by the MicroBlaze Soft microprocessor when the next bit stream is to be loaded. Current Xilinx Kintex 325T FPGA cards need about 0.7 s to receive the second superlayer of AlexNet with a batch size of 128 soft microprocessors for bit transfer layers of 0.012%."}, {"heading": "D. Reconfiguration flow for FP/BP", "text": "The CNN MERP supports both FP and BP with the data flow shown in Figure 12. FP comprises the second superlayer, which is combined with the third convolutional layer, the fourth activation layer and the fourth pooling layer. While BP combines the (l + 1) th convolutional layer, the second FPBit activation layer and the third activation layer as the (l + 1) th superlayer, the different sequence in BP is due to the reverse layers and the data dependency for the KU. Since the output cards are inputs of kernel updates, superlayers should be separated by external traffic so that they can be stored outside. Due to the design above, the first layer is calculated in the second superlayer and the data dependency for the KU. Since the output cards are inputs of kernel updates, superlayers should be separated by external traffic so that they can be stored outside."}, {"heading": "1 27.01 7.11 4.18 --- 8.36", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 57.34 3.13 1.01 4.25 2.29", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 38.27 4.26 1.45 3.37 1.45", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 28.74 4.21 2.31 2.31 2.31", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 19.14 4.13 1.98 2.89 2.89", "text": "Total 170.50 4.31 3.45 3.92Table 2 Implementation results of the 2nd superlayer in AlexNet reso-urce (slices) Kintex Kintex-7 Vertex UltraScaleFP \u03b4P KU FP \u03b4P KULUT 182367 178435 173195 1505983 1356150 1302193 FF 121498 114082 117959 854134 868097 850025BRAM 213 238 209 1538 1498 DSP 408 405 2848 CN48 2848Comparison with other related work is shown in Table 4. Our implementation supports both FP and BP, while other work only considers FP. FP's general standardized bandwidth requirement is 45.7% lower than previous FPGA implementations. We also evaluate the maximum acceptable throughput in extensions shown in Figure 14."}], "references": [{"title": "Video Surveillance: Past, Present, and Now the Future", "author": ["F. Porikli", "F. Br\u00e9mond", "S.L. Dockstader", "J. Ferryman", "A. Hoogs", "B.C. Lovell", "S. Pankanti", "B. Rinner", "P. Tu", "P.L. Venetianer"], "venue": "IEEE Signal Process. Mag., vol. 30, no. 3, 2013, pp. 190\u2013198.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Face recognition: a convolutional neural-network approach", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back"], "venue": "IEEE Trans. Neural Netw., vol. 8, no. 1, Jan. 1997, pp. 98\u2013113.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "CNP: An FPGAbased processor for Convolutional Networks", "author": ["C. Farabet", "C. Poulet", "J.Y. Han", "Y. LeCun"], "venue": "2009 International Conference on Field Programmable Logic and Applications, 2009, pp. 32\u201337.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Caffe", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "the ACM International Conference on Multimedia - MM \u201914, 2014, pp. 675\u2013678.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert"], "venue": "BigLearn, NIPS Workshop, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Chainer: a Next-Generation Open Source Framework for Deep Learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "C.S.A.S. Mateo", "J. Clayton"], "venue": "learningsys.org.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 0}, {"title": "A dynamically configurable coprocessor for convolutional neural networks", "author": ["S. Chakradhar", "M. Sankaradas", "V. Jakkula", "S. Cadambi"], "venue": "ACM SIGARCH Comput. Archit. News, vol. 38, no. 3, Jun. 2010, p. 247.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks", "author": ["C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong"], "venue": "2015 ACM/SIGDA FPGA \u201915, 2015, pp. 161\u2013170.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "NeuFlow: A runtime reconfigurable dataflow processor for vision", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "E. Culurciello", "Y. LeCun"], "venue": "2011 IEEE CVPR WORKSHOPS, 2011, pp. 109\u2013116.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks", "author": ["V. Gokhale", "J. Jin", "A. Dundar", "B. Martini", "E. Culurciello"], "venue": "IEEE CVPR Workshops, 2014, pp. 682\u2013687.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "NeuFlow: Dataflow vision processing system-on-a-chip", "author": ["P.-H. Pham", "D. Jelaca", "C. Farabet", "B. Martini", "Y. LeCun", "E. Culurciello"], "venue": "2012 IEEE 55th International Midwest Symposium on Circuits and Systems, 2012, pp. 1044\u20131047.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Y. Chen", "O. Temam"], "venue": "ACM SIGARCH Comput. Archit. News, vol. 42, no. 1, Apr. 2014, pp. 269\u2013284.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Origami: A Convolutional Network Accelerator", "author": ["L. Cavigelli", "D. Gschwend", "C. Mayer", "S. Willi", "B. Muheim", "L. Benini"], "venue": "the 25th edition on Great Lakes Symposium on VLSI - GLSVLSI \u201915, 2015, pp. 199\u2013204.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A 1.42tops/w deep convolutional neural network recognition processor for intelligent ioe systems", "author": ["L.-S.K.J. Sim", "J-S. Park", "M. Kim", "D. Bae", "Y. Choi"], "venue": "IEEE Int. Solid-State Circuits Conf. , 2016, pp. 1\u201343.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Y. Chen", "T. Luo", "S. Liu", "S. Zhang", "L. He", "J. Wang", "L. Li", "T. Chen", "Z. Xu", "N. Sun", "O. Temam"], "venue": "2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 2014, pp. 609\u2013622.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Eyeriss : An Energy- Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks Future of Deep Learning Recognit ion DCNN Accelerator is Crucial \u2022 High Throughput for Real-time", "author": ["Y. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "IEEE Int. Solid-State Circuits Conf. , Feb. 2016, pp. 1\u201343.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Memorycentric accelerator design for Convolutional Neural Networks", "author": ["M. Peemen", "A.A.A. Setio", "B. Mesman", "H. Corporaal"], "venue": "2013 IEEE 31st ICCD, 2013, pp. 13\u201319.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "the 27th ICML, 2010, no. 3, pp. 807\u2013814.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Mixed Design of Integrated Circuits and Systems, 2008. MIXDES 2008. 15th International Conference on", "author": ["K. Arshak", "C.S. Ibala"], "venue": "Mixed Design of Integrated Circuits and Systems, 2008. MIXDES 2008. 15th International Conference on, 2008. pp. 619\u2013623.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "State-of-the-art CNNs are demonstrating high performance and flexibility in a wide range of applications including video surveillance, object recognition and mobile robot vision [1]\u2013[3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "State-of-the-art CNNs are demonstrating high performance and flexibility in a wide range of applications including video surveillance, object recognition and mobile robot vision [1]\u2013[3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "For example, in AlexNet [4], a deep CNN plays a crucial role in significantly improving image identification accuracy.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Many of today\u2019s deep CNNs are implemented on GPU platforms, based on which some fast and friendly frameworks are developed such as Caffe [5], Torch [6], and Chainer [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Many of today\u2019s deep CNNs are implemented on GPU platforms, based on which some fast and friendly frameworks are developed such as Caffe [5], Torch [6], and Chainer [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 6, "context": "Many of today\u2019s deep CNNs are implemented on GPU platforms, based on which some fast and friendly frameworks are developed such as Caffe [5], Torch [6], and Chainer [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "[8]\u2019s 16-bit processor can automatically analyze workloads and configure hardware for higher speed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "To optimize the throughput with limited memory bandwidth, a roofline-model-based design was proposed by Chen Zhang, et al [9].", "startOffset": 122, "endOffset": 125}, {"referenceID": 17, "context": "optimize the usage of on-chip memory, in 2013, Maurice et al [18] take advantage of high-level synthesis tools to develop a configurable accelerator template, which comprises 140MACC with 150MB/s memory.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "[16] proposed to store the whole network on chip with eDRAM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In 2016, Yu-Hsin Chen, et al [17] published a processor Eyeriss, which takes advantage of an 108KB on-chip memory to reduce normalized bandwidth requirement to 4.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "the AlexNet [4], 32-bit operands and more processing elements are necessary, since the requirement for higher precision and workloads have been demonstrated in training.", "startOffset": 12, "endOffset": 15}, {"referenceID": 18, "context": "In [19], Vinod Nair, et al stated that ReLU has better generalized ability to prevent training model from saturating.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "These layers are the 2 layers from the AlexNet [4] that is shown in Table 1.", "startOffset": 47, "endOffset": 50}, {"referenceID": 10, "context": "The conventional method [11] reloads one window for different filters, which aggravates external bandwidth requirement.", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "For the 2 convolutional layer of the AlexNet [4], 198.", "startOffset": 45, "endOffset": 48}, {"referenceID": 19, "context": "In [20], Xilinx FPGA can be reconfigured from flash on chip, which results in a different functionality.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "We program different bit streams for every super layer in AlexNet [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 16, "context": "Layer Operations (Gop/s) Eyeriss [17] (MB/Gop) CNN-MERP(32-bit) (MB/GFlop) Normalized BW of FP (16-bit) Normalized BW of FP Normalized BW of \u03b4P Normalized BW of KU 1 27.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "[18] meets the bottleneck at 5.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Large-scale deep convolutional neural networks (CNNs) are widely used in machine learning applications. While CNNs involve huge complexity, VLSI (ASIC and FPGA) chips that deliver high-density integration of computational resources are regarded as a promising platform for CNN\u2019s implementation. At massive parallelism of computational units, however, the external memory bandwidth, which is constrained by the pin count of the VLSI chip, becomes the system bottleneck. Moreover, VLSI solutions are usually regarded as a lack of the flexibility to be reconfigured for the various parameters of CNNs. This paper presents CNN-MERP to address these issues. CNN-MERP incorporates an efficient memory hierarchy that significantly reduces the bandwidth requirements from multiple optimizations including on/offchip data allocation, data flow optimization and data reuse. The proposed 2-level reconfigurability is utilized to enable fast and efficient reconfiguration, which is based on the control logic and the multiboot feature of FPGA. As a result, an external memory bandwidth requirement of 1.94MB/GFlop is achieved, which is 55% lower than prior arts. Under limited DRAM bandwidth, a system throughput of 1244GFlop/s is achieved at the Vertex UltraScale platform, which is 5.48 times higher than the state-of-the-art FPGA implementations. Keywords\u2014convolutional neural networks; FPGA; memory bandwidth; reconfigurable processor; backward propagation", "creator": "Word"}}}