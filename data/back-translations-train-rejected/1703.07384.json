{"id": "1703.07384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Ontology Based Pivoted normalization using Vector Based Approach for information Retrieval", "abstract": "The proposed methodology is procedural i.e. it follows finite number of steps that extracts relevant documents according to users query. It is based on principles of Data Mining for analyzing web data. Data Mining first adapts integration of data to generate warehouse. Then, it extracts useful information with the help of algorithm. The task of representing extracted documents is done by using Vector Based Statistical Approach that represents each document in set of Terms.", "histories": [["v1", "Tue, 21 Mar 2017 18:34:34 GMT  (2599kb)", "http://arxiv.org/abs/1703.07384v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["vishal jain", "dr mayank singh"], "accepted": false, "id": "1703.07384"}, "pdf": {"name": "1703.07384.pdf", "metadata": {"source": "CRF", "title": "Ontology Based Pivoted normalization using Vector \u2013 Based Approach for information Retrieval", "authors": ["Vishal Jain", "Mayank Singh"], "emails": ["vishaljain83@ymail.com,", "mayanksingh2005@gmail.com"], "sections": [{"heading": null, "text": "Ontology Based Pivoted normalization using Vector - Based Approach for information RetrievalVishal Jain1 and Dr. Mayank Singh21Research Scholar, Computer Science and Engineering Department, Lingaya's University, Faridabad 2Associate Professor, Computer Science and Engineering Department, Lingaya's University, Faridabad1vishaljain83 @ ymail.com, 2mayanksingh2005 @ gmail.com ABSTRACT An abundance of documents available on the web puts the user in a quandary. Users are confused about the relevance of documents. Relevance means how closely the given query matches a large number of documents. Many information extraction techniques are used to extract documents, but they are all in vain. The paper deals with the problem of classifying, analyzing and extracting web documents using one of the methods known as Ontology Based Web Content Mining Methodology."}, {"heading": "1. INTRODUCTION", "text": "It is multi-level, i.e. it covers various areas such as database systems, information retrieval (IR), machine learning, etc. Prediction and description are considered two objectives of data mining, where prediction involves the use of some variables or data sets in databases, while the description finds useful patterns that describe the given data. Prediction data Pre-Processed Final Data Data Data Data Data (P2) Data (P3) (P5) Figure 1: KDD Process [2] Building Ontology needs the attention of a domain expert who represents concepts and relationships between them for a particular domain. There are many algorithms used for extracting and discovering knowledge from structured data such as Na\u00efve Bayes, K-Means, etc."}, {"heading": "2.1 Ontology Web Based Content Mining", "text": "Ontology Based Web Content Mining represents conceptual information about specific domains. It shows the presentation of documents, the extraction of relevant information from text documents and creates classification models. This method is used to analyze web data based on the ideas and principles of data mining."}, {"heading": "2.2 Building Ontology for given domain", "text": "Importance: - As traditional domain experts were not sufficiently intelligent to represent complete knowledge in connection with the query, there is often knowledge in need of updating. It leads to the construction of ontology. It takes data from a given database located on a backend server. Data preparation is included to fully understand the meaning of data and lists all tables and attributes present in the database. Data mapping states that data should be represented according to a specific algorithm. Mapper is used for data mapping. Mapper converts input data into normal format so that it meets the requirements of the user at the building classification stage."}, {"heading": "2.3 Gathering of Information about Documents", "text": "The specification is as follows: \u2022 Input = Web Documents + Domain Ontology and User Abstraction level (K) \u2022 Output = Documents associated with vector terms (ti) and weights (wi). Process: - Extractor prepares XML file containing instances of ontology with their relationships at hierarchical level. In WORDnet, phrase collection means relevant phrases with their associated concepts of ontology. To extract concepts, we use the disambiguous function dis (t), which semantically shows the concept of terms (ti) based on a specific topic. Phrase Extractor scans the phrases, as the name suggests, and refers to related concepts when it finds a relevant matter. Each web document is presented as a vector of < Term ti >, < weight wi > pairs, which is extracted from the Phrase Extractor module."}, {"heading": "2.4 Classification Algorithm", "text": "In fact, it is as if it were a way of doing things, as has been the case in recent years in the United States and Europe. \"It is as if it is a way of doing things,\" he said in an interview with Welt am Sonntag: \"It is the only way in which we are able to change the world.\""}, {"heading": "3.2 Assignment of weights to terms", "text": "Weighting of terms means meaning of the term, i.e. how relevant it is. Weighting is assigned by a special scheme called term frequency * Reverse frequency of the document (tf * idf) Term frequency (tf): - It defines the number of terms that have occurred in the document. Thus, it varies from document to document. Reverse document frequency (idf): - It means how often the given term is distributed in the document. There is the probability of terms that have occurred in the document. idf = ln N / n, where N = number of documents n = number of relevant documentsInference: - If all documents are relevant, idf is zero. We can say that in order to distinguish relevant and non-relevant documents, the terms in the document must deviate from the given topic so that they can be used for comparison with other documents. Why is idf multiplied by tf? It is done in such a way that good terms of the describer have more meaning than bad terms."}, {"heading": "3.3 Normalization of Term Vectors", "text": "Here tf is divided by the maximum frequency of the term tfmax, i.e. tf / tfmax. It is defined as the frequency of the term that occurs mainly in documents. So we create a factor that varies between 0 and 1. This type of normalization is called maximum normalization. Terms are variables. They can change at any time. Disadvantage: - Because the normalization factor only differs from the frequency of the documents. The problem is that terms with higher weights can replace terms with lower weights. E.g. A document comprises different components, hardware."}, {"heading": "3.3.1 Normalization by Vector Length", "text": "Each component is divided by the Euclidean length of the vector = IV. R = x i + y j Euclidean length of R = \u221a x2 + y2 Cosine normalization = x / \u221a x2 + y2, y / \u221a x2 + y2 = x / \u221a r2cos2 + r2sin2 It is called Cosine normalization because the normalized vector \u221a cos2 + sin2 has length = 1. It is written as n ^ = 1. Cosine normalization reduces the effect of the single term with high frequency by combining it with other low-weighted terms. Since vector length is the function of all vector components, i.e. tdf * idf weights. Weight of the radio frequency terminus is thus reduced by the idf factor. Cosine normalization takes into account the normalization relevance of all terms in a given document."}, {"heading": "4. CONCLUSION", "text": "The paper introduces the Ontology Web Based Content Mining methodology, which helps classify, identify and extract a large number of documents on the Web. It follows a certain number of steps to generate ontology. We conducted an experiment with WORDnet. The main merit of this work lies in the domain ontology in the representation of documents. The use of WORDnet leads to an improvement in the classification of web documents using synonyms, as it has a large collection of similar words associated with a particular search. Ontology Phrase Extractor produces web documents consisting of several pages with multiple categories. Each web document is presented as a vector of < term ti >, < weight wi > pairs. They are presented using a statistical information retrieval (IR) approach known as a vector-based approach."}, {"heading": "ACKNOWLEDGEMENT", "text": "I, Vishal Jain, would like to express my sincere gratitude to Prof. M. N. Hoda, Director of the Bharati Vidyapeeth's Institute of Computer Applications and Management (BVICAM) in New Delhi, for giving me the opportunity to complete my PhD at Lingaya's University in Faridabad."}], "references": [{"title": "Uthuruswamy, \u201cData Mining and Knowledge discovery in databases", "author": ["R.U. Fayyad"], "venue": "\u201cCommunications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Implementation of Multi Agent Systems with ontology in Data Mining\u201d, \u201cInternational", "author": ["Vishal Jain", "Gagandeep Singh", "Dr. Mayank Singh"], "venue": "Journal Of Research In Computer Application & Management (IJRCM),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Improving Classification of Multi-Lingual Web  Documents using Domain Ontologies", "author": ["Litvak", "M. Last", "Kisilevich"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Applying data mining for ontology building", "author": ["Abd-Elraham Elsayed", "Samhaa Ram", "Mahmod Rafea"], "venue": "\u201cACM Conference", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Wordnet: An Online Lexical Database", "author": ["G.A. Miller", "Beckwith", "Gross"], "venue": "\u201cInternational Journal of Lexicography\u201d,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Combining Statistical and semantic approaches to translation of ontologies and taxonomies", "author": ["John McCrae", "Mauricio Espinoza"], "venue": "\u201cIn Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation\u201d,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "A Vector Space Model for Semantic Similarity Calculation and OWL Ontology Alignment\u201d, \u201cDEXA", "author": ["R. Tous", "J. Delgado"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A Vector Based Method of Ontology Matching", "author": ["Zahra Eidoon", "Naseer Yazdani"], "venue": "\u201cIEEE Third National Conference on Semantics, Knowledge and Grid\u201d,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Ontology Matching: Machine Learning Approach", "author": ["A. Doan", "J. Madhavan", "A. Halevy"], "venue": "\u201cHandbook on Ontologies in Information Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Data Mining is called as Knowledge Discovery in Databases (KDD) [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Figure 1: KDD Process [2] Building Ontology needs attention of domain expert that represents concepts and relations between them for a given domain.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "uses concept of Domain Ontology [3].", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "4 Classification Algorithm This ontology building algorithm [4] is written on basis of decision tree as follows:", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "4 Increa document About WO Miller [5].", "startOffset": 34, "endOffset": 37}], "year": 2013, "abstractText": "Research Scholar, Computer Science and Engineering Department, Lingaya\u2019s University, Faridabad Associate Professor, Computer Science and Engineering Department, Lingaya\u2019s University, Faridabad vishaljain83@ymail.com, mayanksingh2005@gmail.com ABSTRACT An ample amount of documents present on web puts the users in state of dilemma. Users get confused about relevance of documents. Relevance means how closely the given query matches large number of documents. Many information extraction techniques are used for extracting documents but they all are in vain. The paper deals with the problem of classification, analyzing and extraction of web documents by using one of information extraction methods called Ontology Based Web Content Mining Methodology. We have evaluated proposed methodology in two specific domainsweather domain (web pages containing information about weather forecasting and analysis) and Google TM collection (web pages containing news). The proposed methodology is procedural i.e. it follows finite number of steps that extracts relevant documents according to user\u2019s query. It is based on principles of Data Mining for analyzing web data. Data Mining first adapts integration of data to generate warehouse. Then, it extracts useful information with the help of algorithm. The task of representing extracted documents is done by using Vector Based Statistical Approach that represents each document in set of Terms.", "creator": "PScript5.dll Version 5.2"}}}