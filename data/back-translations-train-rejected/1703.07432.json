{"id": "1703.07432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Efficient PAC Learning from the Crowd", "abstract": "In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.", "histories": [["v1", "Tue, 21 Mar 2017 21:05:27 GMT  (28kb)", "https://arxiv.org/abs/1703.07432v1", null], ["v2", "Thu, 13 Apr 2017 21:21:41 GMT  (28kb)", "http://arxiv.org/abs/1703.07432v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["pranjal awasthi", "avrim blum", "nika haghtalab", "yishay mansour"], "accepted": false, "id": "1703.07432"}, "pdf": {"name": "1703.07432.pdf", "metadata": {"source": "CRF", "title": "Efficient PAC Learning from the Crowd", "authors": ["Pranjal Awasthi", "Avrim Blum", "Nika Haghtalab", "Yishay Mansour"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.07 432v 2 [cs.L G] 13 Apr 201 7In recent years, crowdsourcing has become the method of choice for collecting labeled training data for learning algorithms. Standard approaches to crowdsourcing consider the process of collecting labeled data separately from the process of learning a classifier from the data collected. This can lead to a large number of queries per example. In this essay, we show how by linking the process of labeling and learning, we can achieve computing efficiency with much less effort in labeling costs. In particular, we consider the viable environment in which labels exist as labels and consider a pool of labels. If a noticeable fraction of labels is perfect, we cannot efficiently learn the traditional model we are learning by using the amount we efficiently require in F."}, {"heading": "1 Introduction", "text": "Over the past decade, research in machine learning and AI has experienced tremendous growth, in part due to the fact that we can collect and annotate vast amounts of data across different areas. [This rate of data annotation has been enabled by crowd-sourcing tools such as Amazon Mechanical TurkTM that allow individuals to participate in a labeling task.] In the context of classification, a crowd-sourcing model will use a large pool of workers to collect labels for a certain amount of training data used to assert a good classifier. Such learning environments, which include the crowd, result in a variety of design decisions that do not occur in traditional learning environments, including: How does the goal of learning from the crowd annotating data from the crowd by the crowd work? What challenges does the high level of noise typically present in abbreviated data sets. [Wais al, 2010 Kittur al]"}, {"heading": "1.1 Overview of Results", "text": "We study different versions of the model described above and assume that a large percentage of, say, 70% of labelers are perfect, i.e. they must always learn the target function. The remaining 30% of labelers may behave arbitrarily and we do not make assumptions about them. However, since the perfect labelers are in a strong majority, a simple approach is to label each example with a label showing the size of the labeling group and the acceptable probability of failure to produce the correct labeling in each instance with a high probability. However, such an approach results in a query tied to O (log m3) per labeling example where m is the size of the labeling group and the acceptable probability of failure. In other words, the cost per labeling example is O (log principle m3) and scales with the size of the datas. another easy approach is to choose a few more random examples and ask them all."}, {"heading": "1.2 Related Work", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2 Model and Notations", "text": "We have to ask ourselves whether we want the distribution effects for the distribution effects D over X x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 A Baseline Algorithm and a Road-map for Improvement", "text": "In this section, we briefly describe a simple algorithm and the approach we use to improve it. Consider a very simple baseline algorithm in the case of \u03b1 > 12: BASELINE: Draw a sample of size m = m, \u03b4 of D | X and label each x-S of MajL (x), where L-P for k = O (\u03b1 \u2212 0.5) -2 ln (m) labeled labels are correct, is a series of randomly drawn labels. Return classifier OF (S). That is, the baseline algorithm queries enough labelists for each sample that all labels are likely to be 1 \u2212 clinically correct. Then, it learns a classifier that uses these marked label sets. It is clear that the performance of BASELINE ideas is anything but desirable. First, this approach takes Log (m / clinically) more labels than it requires samples, resulting in average costs per sample increasing with the sample size."}, {"heading": "4 An Interleaving Algorithm", "text": "In this section, we improve the average cost per labeled example of the BASELINE algorithm by interrupting the process of learning and acquiring high-quality labels. (Our algorithm 2 facilitates interaction between the learning process and the query process based on ideas from the classic PAC learning and customization techniques that we are developing in this work. (To facilitate presentation, let's first consider the case where we assume a value of 12- (1), let's say a discrete space, in which we introduce an algorithm and techniques that work in that regime.) In Section 4.1, we show how our algorithm can be modified to work with any value of alpha. (For convenience, we assume that distribution D occurs over a discrete space.) This is in fact without loss of convergence, since you can instead work with a uniform distribution over an unlabeled sample multiset of size. (d) Here, we provide an overview of the techniques and ideas used in this section."}, {"heading": "4.1 The General Case of Any \u03b1", "text": "We show that we can no longer assume that if we take the majority vote via a few random labelers, we will get the correct labeling of an instance. So there are two key challenges that we have to overcome if we are to return a highly noisy-titled sample set. This is problematic because we learn efficiently h1, h2 and h3, which depends crucially on the correctness of the input label. Second, FILTER (S, h1) does not have \"filters\" based on the classification errors of h1."}, {"heading": "5 No Perfect Labelers", "text": "We assume that at least half of the labels are good, while others have performed considerably worse. Formally, we have a number of labels g1, gn and a distribution D with an unknown target class f & F. We assume that more than half of the labels are good, while others have performed considerably inferiory. gn and a distribution D with an unknown target class f & F. We assume that more than half of these labels are \"good,\" that is, they have errors in distribution D."}, {"heading": "A Additional Related Work", "text": "In this model, however, the learning algorithm can question the label complexity of the algorithm and use it to produce a precise hypothesis. The goal is to use as few labels as possible. Label complexity of the algorithm is referred to as such. It is well known that certain hypotheses can be learned in this model."}, {"heading": "B Proof of Lemma 4.2", "text": "First, note that since D and D \"are both labeled to f-F, for every f-F we have, errD\" (f) = x-xd \"(x) 1f (x) 6 = f-xc (x) 1f (x) 6 = f-x (x) = c-errD (f).So if errD\" (f) \u2264 c2, then errD. \"Leave m\" = mc2, \"we have\" > Pr S \"= D\" m \"[f-F, s.t. errS\" (f) = 0-errD \"(f) = 0-c2]."}, {"heading": "C Proof of Lemma 4.7", "text": "First of all, let us consider the expected size of the SI, W I and WC rates. Using Lemma 4.6 we have O (m \u221a, \u03b4) \u2265 12 \u221a | S2 | + \u221a | S2 | \u2265 E [| SI |] \u2265 12 (12 \u221a) | S2 | S2 (m \u221a, \u03b4).Similarly, it is with O (m \u221a, \u03b4) \u2265 E [SI] + SC | \u2265 E [W I] \u2265 12 (12 \u221a) | S2 | \u2265 O (m \u221a, \u03b4).Similarly, it is with O (m \u221a, \u03b4) \u2265 E [SI] + SC | \u2265 E [W I] \u2265 12 (12 \u221a) | S2 | \u2265 E (m \u221a, \u043c).Similarly, with E [SI] + SC | \u2265 E [WC] (1 \u2212 2) | m (m)"}, {"heading": "D Remainder of the Proof of Lemma 4.9", "text": "We prove that the amber inequality applies to the total number of queries y1, y2,.. before their majority matches f \u0445 (x). Lx is the random variable that indicates the number of queries the algorithm makes on instance x, for which h (x) = f \u0445 (x). Consider the probability that Lx = 2k + 1 is for some k. That is, Maj (y1: t) = f \u0445 (x) for the first time when t = 2k + 1. This is at most the probability that Maj (y1: 2k \u2212 1) 6 = f \u0445 (x). After Chernoff limit, we have thatPr [Lx = 2k + 1] \u2264 Pr [Maj (y1: 2k \u2212 1) 6 = f \u0435 (x) \u2264 exp (\u2212 2k \u2212 2k \u2212 1) \u2264 Pr [Maj (y1: 2k \u2212 1) 6 = f."}, {"heading": "E Probability Lemmas", "text": "Theorem E.1 (probability of demise [Feller, 2008]). Consider a player who competes with i-dollars against an opponent who has N-dollars. In each game of chance, the player bets one dollar, which he has probability p.The probability that the player is without money at any point in the game is 1 \u2212 (p 1 \u2212 p) N1 \u2212 (p 1 \u2212 p) N + i.Theorem E.2 (amber inequality). Letter X1,..., Xn will be independent random variables with expectation p. Suppose that for any positive real number L and any k > 1, E [(Xi \u2212 \u00b5) k] \u2264 1 2 E [(Xi \u2212 \u00b5) 2] Lk \u2212 2k!. Then Prn is instead of i = 1Xi \u2212 n\u00b5 \u2265 2t."}, {"heading": "F Omitted Proofs from Section 4.1", "text": "In this section, we prove theorem 4.13 and present the proofs omitted in section 4.1. (Theorem 4.13 (reformulated) Suppose the percentage of perfect labellers is greater and allows it. (Algorithm 3 uses the oracle OF, runs in time poly (d, 1o, 1o, 1o), uses a total number of labels of size O (1o, 1 x), size and probability. (1 \u2212) Returns f) F with errD (f). (1) Returns golden queries, load of 1o per labeller and a total number of labels O (1 x), and a total number of labels O (1 x). (1) Returns a protocol (1 x), a protocol (1 x)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example. In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in F and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any F that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.", "creator": "LaTeX with hyperref package"}}}