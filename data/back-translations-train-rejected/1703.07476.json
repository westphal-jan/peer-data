{"id": "1703.07476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Topic Identification for Speech without ASR", "abstract": "Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.", "histories": [["v1", "Wed, 22 Mar 2017 00:37:33 GMT  (168kb,D)", "https://arxiv.org/abs/1703.07476v1", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v2", "Tue, 11 Jul 2017 17:11:15 GMT  (168kb,D)", "http://arxiv.org/abs/1703.07476v2", "5 pages, 2 figures; accepted for publication at Interspeech 2017"]], "COMMENTS": "5 pages, 2 figures, submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunxi liu", "jan trmal", "matthew wiesner", "craig harman", "sanjeev khudanpur"], "accepted": false, "id": "1703.07476"}, "pdf": {"name": "1703.07476.pdf", "metadata": {"source": "CRF", "title": "Topic Identification for Speech without ASR", "authors": ["Chunxi Liu", "Jan Trmal", "Matthew Wiesner", "Craig Harman", "Sanjeev Khudanpur"], "emails": ["khudanpur}@jhu.edu,", "craig@craigharman.net"], "sections": [{"heading": null, "text": "In fact, it is as if most people who are able to survive themselves, to survive themselves and to survive themselves, have to survive themselves. (...) It is not as if they could survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...) It is as if they would survive themselves. (...). \"(...) It is as if they would survive themselves. (...).\" (...). \"(...).\" (...). \"(...) It is as if.\" (...). \"(...).\" (...). \"It is.\" (...). \"(...).\" It is. \"(...).\" It is. (...). (...). (...). \"It is. (...). (...).\" It is. (...). (. \"It is. (...).\" It is. (...). (. \"It is. (...).\" It is. (...). (. \"It is. (...).\" It is. (...). \"It is. (...).\" It is. (. (). (...). (...). It is. (. (). (...). It is. (. (...). (...). It is. (...). It is. (. (...). (...). (...). It is. (...). (...). It is. (. (...). It is. (. (...). (). (). (). It is. It is. (. (). (). (). (). It is. (. (). It is. (). It is. (. (). (). (). (). It is. (). (). (). It is. (). It is. (). (). It is. (). (). It is. (). (. (). (). It is. (. (). (). It is. (. (). (). (). It is. (). ()."}, {"heading": "2.1. Unsupervised term discovery (UTD)", "text": "UTD aims to automatically identify and cluster repeated terms (e.g. words or phrases) from the language. To bypass the exhaustive DTW-based search, which is limited by O (n2) time [7], we use the scalable UTD framework in the Zero Resource Toolkit (ZRTools) [8], which allows searching in O (n logn) time. We briefly describe the UTD procedures in ZRTools four steps below, and the full details can be found in [8].1 Construct the sparse approximate acoustic similarity matrices between speech expression pairs. Detect word repetitions through quick diagonal line searches and segmental DTW.3. The resulting matches are used to construct an acoustic similarity graph in which nodes represent the matching acoustic segments and edges of other distances."}, {"heading": "2.2. Acoustic unit discovery (AUD)", "text": "We use the non-parametric Bayesian AUD framework in [11], which is based on varying conclusions rather than the maximum probability training in [5], which could simplify parameter estimations too much, nor the Gibbs sampling training in [19], which is not suitable for large-scale applications. As part of the Dirichlet process, we formulate a phone loop model, in which each phoneme-like unit is modeled as HMM with a Gaussian mixing model of production densities (GMM-HMM). As part of the Dirichlet process, we consider the telephone loop as an endless mixture of GMM-HMMs, and the mixing weights are based on the pitch-breaking construction of the Dirichlet process. The infinite number of units in the mixture is cut off in practice, giving zero mixing weight to each unit beyond a large number of GMM-HMMs."}, {"heading": "3. Learning document representations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Bag-of-words representation", "text": "After we have received the tokenizations of the language either by UTD or AUD, each spoken document is represented by a vector of the unigram frequency over the detected term or a vector of the ngram count over the acoustic units. Each feature vector can be further scaled by the Inverse Document Frequency (IDF), resulting in a TF-IDF attribute."}, {"heading": "3.2. Convolutional neural network-based representation", "text": "AUD allows the complete capture of continuous language into a sequence of acoustic units, which we can use in a CNN-based framework to learn a vector representation for each spoken document. As shown in Figure 1, each unit is encoded in an acoustic unit sequence a with a length of 1 \u2264 i \u2264 m as a fixed-dimensional continuous vector, and the entire sequence a is represented as a concatenated vector x. A common convolutional feature transforms T into a fixed-dimensional n-gram window n m and glides over the entire sequence. Then, the hidden feature layer h1 with nonlinearity consists of each feature vector h1i, which is centered from the common convolution window in position i. Max pooling is executed on each h1i, 1 \u2264 i \u2264 m to obtain a fixed-dimensional vector representation for the entire sequence, i.e. a hierarchical representation of the acoustic unit."}, {"heading": "4. Supervised document classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Single-label classification", "text": "For the sack-of-words representation we use a stochastic gradient descend (SGD) based on linear SVM [20, 21] with hinge loss and L1 / L2 standard regulation. For the CNN-based framework we use a Softmax function at the output level for classification as described in Section 3.2."}, {"heading": "4.2. Multi-label classification", "text": "In the setting in which each spoken document can be associated with several topics / labels, we proceed to perform a classification task with several labels. The basic approach is the binary reliance-vance method, which independently forms a binary classifier for each label, and the spoken document is evaluated by each classifier in order to determine whether the respective label applies to this. Specifically, we use a series of SVMs (Section 4.1), one for each label, on the label characteristics. In order to adapt the CNN-based framework for multiple label classification, we replace the Softmax at the output level with a series of sigmoid output nodes, one for each label, as in Figure 1. Since a sigmoid naturally delivers output values between 0 and 1, we train the neural network (NN) to minimize the binary cross-identification in the output level, \u2212 the unit (K.x, = 1) (as shown in Figure 1)."}, {"heading": "5.1. Single-label classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1. Experimental setup", "text": "For our one-page topic selection, we use the Telephone Speech Corpus switchboard [23], a collection of two-page telephone conversations. We use the same development (dev) and eval (eval) of data as in [9, 10]. Each entire conversation has two pages and a single topic, and Topic ID is performed on each page of the conversation (i.e., each page is considered a single spoken document). In the 35.7-hour debate, there are 360 conversation pages spread evenly over six different topics (recycling, capital punishment, drug testing, family financing, job purchase), meaning each topic has the same number of 60 pages."}, {"heading": "5.1.2. Results on Switchboard", "text": "Table 1 shows the topic ID results on the switchboard. In UTD-based classifications, we note that the standard rescoring method in ZRTools [8], which is designed to filter out the filled pauses, provides a performance comparable to the raw DTW similarity values, but the rescoring method can lead to a much faster networking of components (Section 2.1). Note that this rescoring model is estimated by using some of the transcribed switching software, but is still a legitimate language-independent UTD approach, while it is operated in languages other than English. While a diagonal median filter duration of 0.6 or 0.7 leads to similar results, inversely CNN without word2vec pre-training usually produces comparable results with SVM; using word2vec = 0.7 in the following UTD experiments (Section 5.2), we proceed with rescoring methods."}, {"heading": "5.2. Multi-label classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1. Experimental setup", "text": "We evaluate our ID performance in three languages released by the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program. For each language, there are a number of audio language files, and each language file is divided into segments of different lengths (up to 120 seconds). Each language segment is considered to be in-domain or out-of-domain. In-domain data is defined as any speech segment related to an incident or events, and in-domain data is fall in a set of domain-specific categories are known as situation types, or in-domain topics topics."}, {"heading": "5.2.2. Results on LORELEI datasets", "text": "As shown in Table 2, UTD-based SVMs are more competitive than AUD-based SVMs on the smaller corpora, i.e. Uzbek and Mandarin, while they are less competitive on the larger corpus Turkish. We study this behavior on each language by varying the amount of training data; we split the data into 10 folds and perform the 10-fold CV nine times, with the number of folds for training from 1 to 9. As illustrated in Figure 2 for Turkish, the AUD-based system is more competitive than UTD as we use more folds for training. Affected ASR-based systems still deliver the best results in various cases, while UTD and AUD-based systems provide comparable performance. Note that CNN-based systems perform better than SVMs on Turkish and Uzbek, while we lose on the smaller Mandarin, indicating that more labeled data is required to generate SVMs on a larger corpus, as well as to allow for smaller CN6s to generate SV5s on a larger corpus."}, {"heading": "6. Concluding remarks", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}], "references": [{"title": "MCE Training Techniques for Topic Identification of Spoken Audio Documents", "author": ["T.J. Hazen"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 8, pp. 2451\u20132460, Nov 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Limited resource term detection for effective topic identification of speech", "author": ["J. Wintrode", "S. Khudanpur"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, vol. 3, no. Jan, pp. 993\u20131022, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Topic identification and discovery on text and speech", "author": ["C. May", "F. Ferraro", "A. McCree", "J. Wintrode", "D. Garcia-Romero", "B. Van Durme"], "venue": "Proc. EMNLP, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-h. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Topic identification of spoken documents using unsupervised acoustic unit discovery", "author": ["S. Kesiraju", "R. Pappagari", "L. Ondel", "L. Burget", "N. Dehak", "S. Khudanpur", "J. \u010cernock\u1ef3", "S. Gangashetty"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011, https: //github.com/arenjansen/ZRTools.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "NLP on spoken documents without ASR", "author": ["M. Dredze", "A. Jansen", "G. Coppersmith", "K. Church"], "venue": "Proc. EMNLP, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical evaluation of zero resource acoustic unit discovery", "author": ["C. Liu", "J. Yang", "M. Sun", "S. Kesiraju", "A. Rott", "L. Ondel", "P. Ghahremani", "N. Dehak", "L. Burget", "S. Khudanpur"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Variational inference for acoustic unit discovery", "author": ["L. Ondel", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. ICML, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep contextual language understanding in spoken dialogue systems.", "author": ["C. Liu", "P. Xu", "R. Sarikaya"], "venue": "in Proc. INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in neural information processing systems, 2015, pp. 649\u2013657.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale multi-label text classification - revisiting neural networks", "author": ["J. Nam", "J. Kim", "E.L. Menc\u0131\u0301a", "I. Gurevych", "J. F\u00fcrnkranz"], "venue": "Proc. ECML-PKDD, 2014, pp. 437\u2013452.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparametric bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "Proc. ICML, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["J. Godfrey", "E. Holliman", "J. McDaniel"], "venue": "Proc. ICASSP, 1992.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Y. Zhang", "B. Wallace"], "venue": "arXiv preprint arXiv:1510.03820, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 2010, pp. 45\u201350.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A keyword search system using open source software", "author": ["J. Trmal", "G. Chen", "D. Povey", "S. Khudanpur", "P. Ghahremani", "X. Zhang", "V. Manohar", "C. Liu", "A. Jansen", "D. Klakow"], "venue": "Proc. SLT, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "First, speech is tokenized into words or phones by automatic speech recognition (ASR) systems [1], or by limited-vocabulary keyword spotting [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "First, speech is tokenized into words or phones by automatic speech recognition (ASR) systems [1], or by limited-vocabulary keyword spotting [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "Second, standard text-based processing techniques are applied to the resulting tokenizations, and produce a vector representation for each spoken document, typically a bag-of-words multinomial representation, or a more compact vector given by probabilistic topic models [3, 4].", "startOffset": 270, "endOffset": 276}, {"referenceID": 3, "context": "Second, standard text-based processing techniques are applied to the resulting tokenizations, and produce a vector representation for each spoken document, typically a bag-of-words multinomial representation, or a more compact vector given by probabilistic topic models [3, 4].", "startOffset": 270, "endOffset": 276}, {"referenceID": 4, "context": "In this scenario, while previous work demonstrates that the cross-lingual phoneme recognizers can produce reasonable speech tokenizations [5, 6], the performance is highly dependent on the language and environmental condition (chan-", "startOffset": 138, "endOffset": 144}, {"referenceID": 5, "context": "In this scenario, while previous work demonstrates that the cross-lingual phoneme recognizers can produce reasonable speech tokenizations [5, 6], the performance is highly dependent on the language and environmental condition (chan-", "startOffset": 138, "endOffset": 144}, {"referenceID": 6, "context": "Raw acoustic featurebased unsupervised term discovery (UTD) is one such approach that aims to identify and cluster repeating word-like units across speech based around segmental dynamic time warping (DTW) [7, 8].", "startOffset": 205, "endOffset": 211}, {"referenceID": 7, "context": "Raw acoustic featurebased unsupervised term discovery (UTD) is one such approach that aims to identify and cluster repeating word-like units across speech based around segmental dynamic time warping (DTW) [7, 8].", "startOffset": 205, "endOffset": 211}, {"referenceID": 8, "context": "[9] shows that using the word-like units from UTD for spoken document classification can work well; however, the results in [9] are limited since the acoustic features on which UTD is performed are produced by acoustic models trained from the transcribed speech of its evaluation corpus.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] shows that using the word-like units from UTD for spoken document classification can work well; however, the results in [9] are limited since the acoustic features on which UTD is performed are produced by acoustic models trained from the transcribed speech of its evaluation corpus.", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "In this paper, we investigate UTD-based topic ID performance when UTD operates on language-independent speech representations extracted from multilingual bottleneck networks trained on languages other than the test language [10].", "startOffset": 224, "endOffset": 228}, {"referenceID": 10, "context": "We exploit the Variational Bayesian inference based acoustic unit discovery (AUD) framework in [11] that allows parallelized large-scale training.", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "In topic ID tasks, such AUD-based systems have been shown to outperform other systems based on cross-lingual phoneme recognizers [6], and this paper aims to further investigate how the performance compares among UTD, AUD and ASR based systems.", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 1, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 4, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 5, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 8, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 9, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 11, "context": "5 \u2013 1 sec) repeated terms, AUD/ASR enables full-coverage segmentation of continuous speech into a sequence of units/words, and such a resulting temporal sequence enables another feature learning architecture based on convolutional neural networks (CNNs) [12]; instead of treating the sequential tokens as a bag of acoustic units or words, the whole token sequence is encoded as concatenated continuous vectors, and followed by convolution and temporal pooling operations that capture the local and global dependencies.", "startOffset": 254, "endOffset": 258}, {"referenceID": 12, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 139, "endOffset": 147}, {"referenceID": 14, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 172, "endOffset": 180}, {"referenceID": 15, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 172, "endOffset": 180}, {"referenceID": 16, "context": "However, three questions are worth investigating in our AUD-based setting: (i) if such a CNN-based framework can perform as well on noisy automatically discovered phoneme-like units as on orthographic words/characters, (ii) if pre-trained vectors of phonemelike units from word2vec [17] provide superior performance to random initialization as evidenced by the word-based tasks, and (iii) if CNNs are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than the large/medium sized datasets as in previous work [15, 16].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "However, three questions are worth investigating in our AUD-based setting: (i) if such a CNN-based framework can perform as well on noisy automatically discovered phoneme-like units as on orthographic words/characters, (ii) if pre-trained vectors of phonemelike units from word2vec [17] provide superior performance to random initialization as evidenced by the word-based tasks, and (iii) if CNNs are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than the large/medium sized datasets as in previous work [15, 16].", "startOffset": 556, "endOffset": 564}, {"referenceID": 15, "context": "However, three questions are worth investigating in our AUD-based setting: (i) if such a CNN-based framework can perform as well on noisy automatically discovered phoneme-like units as on orthographic words/characters, (ii) if pre-trained vectors of phonemelike units from word2vec [17] provide superior performance to random initialization as evidenced by the word-based tasks, and (iii) if CNNs are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than the large/medium sized datasets as in previous work [15, 16].", "startOffset": 556, "endOffset": 564}, {"referenceID": 6, "context": "To circumvent the exhaustive DTW-based search limited by O(n) time [7], we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) [8], which permits search inO(n logn) time.", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "To circumvent the exhaustive DTW-based search limited by O(n) time [7], we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) [8], which permits search inO(n logn) time.", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "We briefly describe the UTD procedures in ZRTools by four steps below, and full details can be found in [8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": "We exploit the nonparametric Bayesian AUD framework in [11] based on variational inference, rather than the maximum likelihood training in [5] which may oversimplify the parameter estimations, nor the Gibbs Sampling training in [19] which is not amenable to large scale applications.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "We exploit the nonparametric Bayesian AUD framework in [11] based on variational inference, rather than the maximum likelihood training in [5] which may oversimplify the parameter estimations, nor the Gibbs Sampling training in [19] which is not amenable to large scale applications.", "startOffset": 139, "endOffset": 142}, {"referenceID": 18, "context": "We exploit the nonparametric Bayesian AUD framework in [11] based on variational inference, rather than the maximum likelihood training in [5] which may oversimplify the parameter estimations, nor the Gibbs Sampling training in [19] which is not amenable to large scale applications.", "startOffset": 228, "endOffset": 232}, {"referenceID": 21, "context": "Specifically, we apply the skip-gram model of word2vec [22] to pre-train one embedding vector for each acoustic unit, based on the hierarchical softmax with Huffman codes.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "For the bag-of-words representation, we use a stochastic gradient descent (SGD) based linear SVM [20, 21] with hinge loss and L/L norm regularization.", "startOffset": 97, "endOffset": 105}, {"referenceID": 20, "context": "For the bag-of-words representation, we use a stochastic gradient descent (SGD) based linear SVM [20, 21] with hinge loss and L/L norm regularization.", "startOffset": 97, "endOffset": 105}, {"referenceID": 22, "context": "For our single-label topic classification experiments, we use the Switchboard Telephone Speech Corpus [23], a collection of two-sided telephone conversations.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "We use the same development (dev) and evaluation (eval) data sets as in [9, 10].", "startOffset": 72, "endOffset": 79}, {"referenceID": 9, "context": "We use the same development (dev) and evaluation (eval) data sets as in [9, 10].", "startOffset": 72, "endOffset": 79}, {"referenceID": 7, "context": "For UTD, we use the ZRTools [8] implementation with the default parameters except that, we use cosine similarity threshold \u03b4 = 0.", "startOffset": 28, "endOffset": 31}, {"referenceID": 10, "context": "0}, and other hyperparameters are the same as [11].", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "The acoustic features on which UTD and AUD operate are extracted using the same multilingual bottleneck (BN) network as described in [10] with Kaldi toolkit [24].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "The acoustic features on which UTD and AUD operate are extracted using the same multilingual bottleneck (BN) network as described in [10] with Kaldi toolkit [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "Complete specifications can be found in [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Note that our data size here is relatively small (only 360 or 600) and the SGD training may give high variance in the performance [25].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "We implemented the CNNs in Keras [26] with Theano [27] backend.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "We implemented the CNNs in Keras [26] with Theano [27] backend.", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "For SGD training we use the Adadelta optimizer [28] and mini-batch size 18.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Dropout [29] rate 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "When we initialize the vector representation of each acoustic unit with a set of pre-trained vectors (instead of random initializations), we apply the skip-gram model of word2vec [22] to the acoustic unit tokenizations of each data set.", "startOffset": 179, "endOffset": 183}, {"referenceID": 29, "context": "We use the gensim implementation [30], which includes a vector space of embedding dimension 50 (tuned over {50, 80}), a skip-gram window of size 5, and SGD over 20 epochs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "For UTDbased classifications, we find that the default rescoring in ZRTools [8] which is designed to filter out the filled pauses produces comparable performance to the raw DTW similarity scores, but the rescoring can result in much faster connectedcomponent clustering (Section 2.", "startOffset": 76, "endOffset": 79}, {"referenceID": 30, "context": "Additionally, we also implement another two separate topic ID baselines using the decoded word outputs from two supervised ASR systems, trained from 80 hours transcribed Babel Turkish speech [31] and about 170 hours transcribed HKUST Mandarin telephone speech (LDC2005T32 and LDC2005S15), respectively.", "startOffset": 191, "endOffset": 195}], "year": 2017, "abstractText": "Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.", "creator": "LaTeX with hyperref package"}}}