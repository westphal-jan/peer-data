{"id": "1703.07588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries", "abstract": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.", "histories": [["v1", "Wed, 22 Mar 2017 10:08:51 GMT  (2823kb,D)", "https://arxiv.org/abs/1703.07588v1", "5 pages"], ["v2", "Thu, 31 Aug 2017 12:01:36 GMT  (2823kb,D)", "http://arxiv.org/abs/1703.07588v2", "5 pages, The code is available atthis https URL"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["yu-hsuan wang", "cheng-tao chung", "hung-yi lee"], "accepted": false, "id": "1703.07588"}, "pdf": {"name": "1703.07588.pdf", "metadata": {"source": "CRF", "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries", "authors": ["Yu-Hsuan Wang", "Cheng-Tao Chung", "Hung-yi Lee"], "emails": ["r04922167@ntu.edu.tw,", "f01921031@ntu.edu.tw,", "hungyilee@ntu.edu.tw"], "sections": [{"heading": null, "text": "Index terms: autoencoder, recursive neural network"}, {"heading": "1. Introduction", "text": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved through gated recurrent neural networks (GRNN), where the hidden neurons form a directed cycle suitable for processing sequential data [4] [5] [6] [7]. In addition to the neural network outputs to be used in target applications, internal signals within the neural networks have also been considered useful. A good example are the bottleneck characteristics [8] [9]. In the age of big data, however, huge amounts of unmarked voice data are available, but difficult to comment on, and uncontrolled approaches to extract useful information from such unmarked data are highly attractive [10] [11]. Autoencoder structures have been used to extract bottlenecks, while GRNs with different structures can be learned very well without labeled data."}, {"heading": "2. Approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Gate Activation Signals (GAS) for LSTM and GRU", "text": "Recursive neural networks (RNN) are neural networks whose hidden neurons form a directed cycle. In the face of a sequence x = (x1, x2,..., xT), RNN achieves better performance than RNN by introducing gates into the units to control the flow of information. Two commonly used gated units are LSTM and GRU [15] [16].The signals within a recursive LSTM unit are formulated as follows: ft = temporal units (Wfxt + Ufht \u2212 1 + bf) (1) it is a constant (Wixt + Uiht \u2212 1 + bi) (2) c = tanh (Wcxt + Ucht \u2212 1 + bc) the recursive unit is formulated as follows: ft = temporary units (Wfxt + Ufht \u2212 bf)."}, {"heading": "2.2. Autoencoder GRNN", "text": "Autoencoders can be trained unattended and have proven to be very useful for many applications [19]. We can have an autoencoder with GRNN as shown in Figure 2 as AEGRNN here. Faced with an input expression represented by its acoustic characteristic sequence {x1, x2,..., xT}, AEGRNN takes the input vector xt at any time and generates the output x-GRNN (x1, x2,..., xt). Due to the recursive structure, AE-GRNN actually takes into account all information up to xt in the sequence, x1, x2,..., xt or x-GRNN = AE-GRNN (x1, x2,..., xt).The loss function L of AE-GRNN in (11) is the averaged squared \"-2 standard for the reconstruction error of xt.L = N, n, x-Nx1, x-Nx2, where the index number, Nx2, and Nn are used in the index, Nx2, and Nn."}, {"heading": "3. Initial Experiments and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experimental Setup", "text": "We conducted our first experiments with TIMIT, including 4620 training expressions and 1680 test expressions. Here, the phoneme limits provided for in TIMIT are used. We train models on the training expressions and perform analyses on the tests.In the tested AE-GRNN, both the encoder and the decoder consisted of a recurring layer and a forward-facing layer. The recurring layers consisted of 32 recurring units, while the forward-facing layers consisted of 64 neurons. We used ReLU as an activation function for the forward-facing layers [20]. The drop-off rate should be 0.3 [21]. The networks were trained with Adam [22]. The acoustic characteristics used were the 39-dim MFCCs with outwardly pronounced mean and variance normalization (CMVN)."}, {"heading": "3.2. Initial Results and Observations", "text": "Figure 3 shows the mean of gate activation signals across all gate units in the AE-GRNN encoder in relation to the frame index using an example expression. The top three subnumbers (a) (b) (c) are for LSTM gates, while the bottom two (d) (e) are for GRU gates. The temporal variations of the GRU gates are to some extent similar to the LSTM forge gate and, except for a level shift, resemble the negative input gate of LSTM. More importantly, a very strong correlation can be found between the phoneme boundaries, which are represented as blue dashed lines. In other words, whenever the signal switches from one phoneme to the next across the phoneme boundary, the changes in signal properties are reflected in the gate activation signals. This is consistent with the earlier finding that the sudden outbursts of gate activations indicated that there were language boundaries [23]."}, {"heading": "3.3. Difference GAS", "text": "With the above-mentioned observations, we define the difference GAS as follows: For a GAS vector to the time index t, gt calculates its mean over all units to obtain a real value g, t. We can calculate the difference GAS as follows: \u2206 g, t = g, t + 1 \u2212 g, t (12) The difference GAS can also be evaluated for each individual gated unit for each dimension of the vector gt, \u2206 g, jt = g, j + 1 \u2212 g, j t (13), where gjt is the j-th component of the vector gt. We have plotted the difference GAS and the individual difference GAS for the first 8 units of a GRNN over the frame index for an expression, as shown in Figure 4. We see that these differences have an even stronger correlation with phonemes boundaries, which are shown by vertical line lines. All of these results agree with the finding that the gate activations of GAS do not have a recursive GAS over the STM-units in this case."}, {"heading": "4. Phoneme Segmentation", "text": "The accuracy of segmentation can be a good indicator to show the degree of correlation between GAS and phoneme boundaries. In this section, we first describe the recurrent predictor model (RPM), an unattended phoneme segmentation approach that serves as a baseline, and then describe how GAS can be used in phoneme segmentation."}, {"heading": "4.1. Baseline: Recurrent Predictor Model", "text": "It was found that the discontinuity on the model output has to do with phoneme boundaries [13]. An RPM has only the lower half of Figure 2. The model output at the time t, x-t = RPM (x1, x2,..., xt) is to predict the next input xt + 1. The loss function L, which is used in training, is the averaged square '-2 norm of the prediction error, L = N, n Tn \u2212 1, x-t, 1 d-xnt + 1 \u2212 RPM (xn1, xn2,... xnt), which is actually parallel to (11). The superscript n indicates the n-th training expression and d the number of dimensions of xnt. Since difficult to predict or with significantly larger errors are probably phoneme boundaries, the error signals Et of RPM, Et = 1d-t, xn-t, and d are exceeded by the number of dimensions (xnt)."}, {"heading": "4.2. GAS for Phoneme Segmentation", "text": "Figure 4 shows a direct approach to using GAS for phoneme segmentation, using a time index as the phoneme limit when \"g\" is a local maximum, i.e. \"g,\" \"g\" and \"g,\" \"g\" and \"g\" exceed a selected threshold. GAS can also be integrated into the speed. Since the speed also includes GRNN in its structure, GAS can also be obtained and interpolated with the error signals in (15), as in (16), where weight is the weight. A time index is taken as the phoneme limit when it is a local maximum and exceeds a selected threshold."}, {"heading": "5. Experiments Results for Phoneme Segmentation", "text": "Here we take the accuracy of phoneme segmentation as an indicator of the correlation between GAS and phoneme boundaries. The structure is the same as in Section 3.1. In the segmentation experiments a tolerance window of 20 ms is used for evaluation. All GAS were obtained from the first recurrent layer."}, {"heading": "5.1. R-value Evaluation", "text": "It is well known that the F1 value is not suitable for segmentation, since over-segmentation can lead to a very high recall resulting in a high F1 value, even with relatively low accuracy. [14] In our preliminary experiments, a periodic predictor that predicted a limit for every 40 ms yielded the F1 value 71.07 with accuracy 55.13 and the recall 99.99, which did not seem reasonable. It has been shown that a better rating value is the R value [24], which adequately punishes the phenomenon of over-segmentation. The approach [25] proposed in a previous paper reached an r value of 76.0, while the periodic predictor reached only 30.53. Therefore, we opted for the R value on the performance measurement."}, {"heading": "5.2. Comparison among different gates", "text": "The results for LSTM gates are consistent with the results of previous work [23] [26]. In LSTM, the forge gate clearly captures the temporal structure most closely related to phoneme boundaries. GRU exceeded LSTM, which is also consistent with previous results [17]. The highest rate is achieved with the update gate of GRU. The update gate in GRU is similar to the forge gate in LSTM. Both control whether the memory units should be overwritten. Interestingly, the reset gate of GRU achieved an R value that is significantly higher than the corresponding input gate in LSTM. The reason for this is probably the location of the reset gate. In GRU, the reset gate does not independently control the amount of the hidden candidate state, but divides some information from the update gate, thus has better access to temporal information for phoneme segmentation [17]."}, {"heading": "5.3. Comparison among different approaches", "text": "In Table 2, we compared the R value obtained from the temporal information of RPM, with or without its GAS (lines (a) (b) (c) (d)). The best result in Table 1 is in line (e), which was achieved with the most recent GPM plus a recurring layer, which used the same number of parameters as AE-GRNN. We also tested the conventional approach of using hierarchical agglomerative clustering (HAC) as shown in line (f), which used the same number of parameters as AE-GRNN. We tested the conventional approach of using hierarchical agglomerative clustering (HAC) as shown in line. [28] We added a white noise with 6dB SNR to the original TIMIT corpus (the right column)."}, {"heading": "6. Conclusions", "text": "We show that the unattended gate activation signals (GAS) have temporal structures that strongly correlate with the phoneme changes in the signals, and this correlation has been confirmed in the phoneme segmentation experiments. Our experiments also showed that GAS brings speed improvements without additional parameters. Like bottlenecks, GAS is derived from the neural network element and not from network outputs, and both have been shown to bring improvements. With the promising results of the GAS shown in the paper, we hope that GAS can bring the same improvements that are caused by bottlenecks."}, {"heading": "7. References", "text": "[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al. [2] H. Hermansky, D. P. Ellis, and S. Sharma. \"The Divided Views of Four Research Groups,\" IEEE Signal Processing Magazine, vol. 29, no., S. S. S., S., S., S., S., S., S., and S., S., Sharma., \"Views on Conventional Hmm Systems,\" in Acoustics, Speech, and Signal Processing, 2000. ICASSP. \"XiXiXius.\" International Conference on, vol. 3. IEEE, 2000, pp. 1635-1638. B.-P. Network, \"Handwritten digit Recognition with,\" 1989. [4] S. Yeung, O. Russakovsky, G. Mori, L. Fei-84. \""}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Tandem connectionist feature extraction for conventional hmm systems", "author": ["H. Hermansky", "D.P. Ellis", "S. Sharma"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on, vol. 3. IEEE, 2000, pp. 1635\u20131638.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Handwritten digit recognition with", "author": ["B.-P. Network"], "venue": "1989.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Endto-end learning of action detection from frame glimpses in videos", "author": ["S. Yeung", "O. Russakovsky", "G. Mori", "L. Fei-Fei"], "venue": "CoRR, vol. abs/1511.06984, 2015. [Online]. Available: http://arxiv.org/abs/1511.06984", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "CoRR, vol. abs/1603.01360, 2016. [Online]. Available: http: //arxiv.org/abs/1603.01360", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder", "author": ["Y.-A. Chung", "C.-C. Wu", "C.-H. Shen", "H.-Y. Lee", "L.-S. Lee"], "venue": "arXiv preprint arXiv:1603.00982, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence segmentation using joint rnn and structured prediction models", "author": ["Y. Adi", "J. Keshet", "E. Cibelli", "M. Goldrick"], "venue": "arXiv preprint arXiv:1610.07918, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic and bottle-neck features for lvcsr of meetings", "author": ["F. Gr\u00e9zl", "M. Karafi\u00e1t", "S. Kont\u00e1r", "J. Cernocky"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, vol. 4. IEEE, 2007, pp. IV\u2013757.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Improved bottleneck features using pretrained deep neural networks.", "author": ["D. Yu", "M.L. Seltzer"], "venue": "in Interspeech,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A nonparametric bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J. Glass"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Extracting deep bottleneck features using stacked auto-encoders", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3377\u20133381.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep unsupervised learning from speech", "author": ["J.F. Drexler"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving phoneme segmentation with recurrent neural networks", "author": ["P. Michel", "O. R\u00e4s\u00e4nen", "R. Thiolli\u00e8re", "E. Dupoux"], "venue": "CoRR, vol. abs/1608.00508, 2016. [Online]. Available: http://arxiv.org/ abs/1608.00508", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1506.02078, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder.", "author": ["L. Deng", "M.L. Seltzer", "D. Yu", "A. Acero", "A.-r. Mohamed", "G.E. Hinton"], "venue": "in Interspeech. Citeseer,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigating gated recurrent networks for speech synthesis", "author": ["Z. Wu", "S. King"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5140\u20135144.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a prelinguistic level.", "author": ["O. R\u00e4s\u00e4nen"], "venue": "CogSci,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["W. Zaremba"], "venue": "2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons", "author": ["Y. Qiao", "N. Shimomura", "N. Minematsu"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 3989\u2013 3992.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised spoken term detection with spoken queries", "author": ["C.-a. Chan"], "venue": "Ph.D. dissertation, National Taiwan University, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning has achieved great success in many areas [1][2][3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Deep learning has achieved great success in many areas [1][2][3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Deep learning has achieved great success in many areas [1][2][3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 248, "endOffset": 251}, {"referenceID": 4, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 251, "endOffset": 254}, {"referenceID": 5, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 254, "endOffset": 257}, {"referenceID": 6, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 257, "endOffset": 260}, {"referenceID": 7, "context": "A good example is the bottleneck features [8][9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "A good example is the bottleneck features [8][9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "On the other hand, in the era of big data, huge quantities of unlabeled speech data are available but difficult to annotate, and unsupervised approaches to effectively extract useful information out of such unlabeled data are highly attractive [10][11].", "startOffset": 244, "endOffset": 248}, {"referenceID": 10, "context": "On the other hand, in the era of big data, huge quantities of unlabeled speech data are available but difficult to annotate, and unsupervised approaches to effectively extract useful information out of such unlabeled data are highly attractive [10][11].", "startOffset": 248, "endOffset": 252}, {"referenceID": 11, "context": "Autoencoder structures have been used for extracting bottleneck features [12], while GRNN with various structures can be learned very well without labeled data.", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "As one example, the outputs of GRNN learned in an unsupervised fashion have been shown to carry phoneme boundary information and used in phoneme segmentation [13][14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "As one example, the outputs of GRNN learned in an unsupervised fashion have been shown to carry phoneme boundary information and used in phoneme segmentation [13][14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Two popularly used gated units are LSTM and GRU [15][16].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Two popularly used gated units are LSTM and GRU [15][16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "where zt, rt, ht and h\u0303t are the signals over the update gate, reset gate, hidden state and candidate hidden state at time t, respectively; means element-wise product [17].", "startOffset": 167, "endOffset": 171}, {"referenceID": 17, "context": "Here we wish to analyze the gate activations computed in equations (1), (2), (5), (8), (10) [18] and consider their temporal structures.", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Autoencoders can be trained in an unsupervised way, and have been shown to be very useful for many applications [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "We used ReLU as the activation function for the feed-forward layers [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "3 [21].", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": "The networks were trained with Adam [22].", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "This is consistent to the previous finding that the sudden bursts of gate activations indicated that there were boundaries of phonemes in a speech synthesis task [23].", "startOffset": 162, "endOffset": 166}, {"referenceID": 22, "context": "All these results are consistent with the finding that the gate activations of forget gate over recurrent LSTM units in the same layer have close correlation with phoneme boundaries in speech synthesis [23], although here the experiments were performed with AE-GRNN.", "startOffset": 202, "endOffset": 206}, {"referenceID": 12, "context": "RPM was proposed earlier to train GRNN without labeled data, and it was found the discontinuity on model outputs have to do with phoneme boundaries [13][14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "RPM was proposed earlier to train GRNN without labeled data, and it was found the discontinuity on model outputs have to do with phoneme boundaries [13][14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "It is well-known that the F1-score is not suitable for segmentation, because over segmentation may give very high recall leading to high F1-score, even with a relatively low precision[14].", "startOffset": 183, "endOffset": 187}, {"referenceID": 23, "context": "The approach proposed in a previous work [25] achieved an r-value 76.", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "The results for LSTM gates are consistent with the findings in the previous works [23][26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "The results for LSTM gates are consistent with the findings in the previous works [23][26].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "GRU outperformed LSTM which is also consistent with earlier results [17][26].", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "GRU outperformed LSTM which is also consistent with earlier results [17][26].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "In GRU, the reset gate does not control the amount of the candidate hidden state independently, but shares some information of the update gate, thus has better access to more temporal information for phoneme segmentation [17].", "startOffset": 221, "endOffset": 225}, {"referenceID": 25, "context": "We also tested the conventional approach of using hierarchical agglomerative clustering (HAC) as shown in row (f) [27][28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "We also tested the conventional approach of using hierarchical agglomerative clustering (HAC) as shown in row (f) [27][28].", "startOffset": 118, "endOffset": 122}], "year": 2017, "abstractText": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.", "creator": "LaTeX with hyperref package"}}}