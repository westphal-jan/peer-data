{"id": "1703.07684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Predicting Deeper into the Future of Semantic Segmentation", "abstract": "The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we focus on predicting semantic segmentations of future frames. More precisely, given a sequence of semantically segmented video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Our models predict trajectories of cars and pedestrians much more accurately (25%) than baselines that copy the most recent semantic segmentation or warp it using optical flow. Prediction results up to half a second in the future are visually convincing, the mean IoU of predicted segmentations reaching two thirds of the real future segmentations.", "histories": [["v1", "Wed, 22 Mar 2017 14:45:15 GMT  (3838kb,D)", "http://arxiv.org/abs/1703.07684v1", null], ["v2", "Tue, 28 Mar 2017 13:54:24 GMT  (3839kb,D)", "http://arxiv.org/abs/1703.07684v2", null], ["v3", "Tue, 8 Aug 2017 10:02:36 GMT  (3926kb,D)", "http://arxiv.org/abs/1703.07684v3", "Accepted to ICCV 2017. Supplementary material available on the authors' webpages"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["pauline luc", "natalia neverova", "camille couprie", "jakob verbeek", "yann lecun"], "accepted": false, "id": "1703.07684"}, "pdf": {"name": "1703.07684.pdf", "metadata": {"source": "CRF", "title": "Predicting Deeper into the Future of Semantic Segmentation", "authors": ["Natalia Neverova", "Pauline Luc", "Camille Couprie", "Yann LeCun"], "emails": ["neverova@fb.com", "paulineluc@fb.com", "coupriec@fb.com", "jakob.verbeek@inria.fr", "yann@fb.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2. Related work", "text": "In contrast, our work focuses on predicting future segments, in particular the use of counterproductive training methods. Several authors have developed methods to improve the temporal stability of semantic video segmentation. Jin et al. [14] use a model to predict the semantic segmentation of the immediate next image, and merge this prediction with the segmentation calculated from the next input frame. Nilsson and Sminchisescu use a conventional RNN model with a spatial transformer component [13] to accumulate information from past and future frames to improve the segmentation of the current frame."}, {"heading": "3. Predicting future frames and segmentations", "text": "This section first presents the various scenarios we have examined to predict RGB pixel values and / or segmentation of the next frame, and then describes two enhancements to the single frame prediction model to predict further into the future."}, {"heading": "3.1. Single frame prediction models", "text": "In fact, most of them will be able to move to another world, in which they are able, in which they are able to move, and in which they are able to move, to move, to move, to move, to move."}, {"heading": "3.2. Predicting deeper into the future", "text": "We consider two extensions of the previous models as input in order to predict further into the future than a single frame. The first is to expand the output of the network to a batch of m frames, i.e. to the output Xt + 1: t + m and / or St + 1: t + m. We call this the \"batch\" approach. The disadvantage of this approach is that it ignores the repetition structure of the problem, i.e. the number of parameters in the last layer scales linearly with the number of output frames. In our second approach, we use the Recurrence property in the same way as St + 2 of S2: t + 1. As a result, the capacity of the model is split to predict them output frames, and the number of output frames scales linearly with the number of output frames. In our second approach, we use the Recurrence property and iteratively apply a model that predicts a single step as the future input by predicting the number of input frames for 1."}, {"heading": "4. Experiments", "text": "Before presenting our experimental results, we first describe the dataset and evaluation metrics in Section 4.1. Then we present the results for single frame prediction, half-time prediction (0.5 seconds) and long-term prediction (10 seconds)."}, {"heading": "4.1. Dataset and evaluation metrics", "text": "The Cityscapes dataset [4] contains 2,975 training sessions, 500 validations and 500 test video sequences of 1.8 seconds. Each sequence consists of 30 frames, and semantic segmentation on the ground is available for the 20th frame. We measure the performance of our models on the 20th frame in each sequence (IoU GT) and refer to the supplementary material for the results on the testset.We evaluate performance using the standard network center (IoU), calculate the truth segmentation of the 20th frame in each sequence (IoU GT), and calculate the IoU measure w.r.t. the segmentation produced with the extended network (IoU) for the 20th frame (IoU SEG)."}, {"heading": "4.2. Short-term prediction", "text": "In our first set of experiments, we used frames 8, 11, 14 and 17 to predict frames 20 for which we have the segmentation of truth at our disposal. In Table 2, we compare our five models. For models that do not directly predict future segmentation, we generate segmentation based on the predicted RGB frames. We also include two baselines that copy the last input frame to the output. For the second baseline, we use FlowNet [6] to estimate the optical flow between the last two inputs and the last input with the estimated flow.From the result, we can make several observations related to the RGB frame prediction (PSNR and SSIM), the performance is comparable to the three models XS2X, XS2XS2XS2XS - and significantly better than the two base models."}, {"heading": "4.3. Mid-term prediction", "text": "We address the more difficult task of predicting the medium-term future, i.e., the next 0.5 seconds. In these experiments, we take in input frames 2, 5, 8, and 11, and forecast outputs for frame 14, 17, and 20. We compare different strategies: batch models, autoregressive models (AR), and models with autoregressive fine-tuning (AR fine-tuning). We compare these strategies with our two baselines, which copy the last input, and the second relies on optical flow baseline, after the first prediction we shift the flow field so that the flow is applied to the correct locations, and so on. Qualitative prediction results are shown in Figure 5. For models XS2X and XS2S, the automatic flow baseline baseline baseline baseline, after the first prediction, we shift the flow field so that the flow flow flow flow flow flow flow flow flow flow is applied to the correct locations."}, {"heading": "4.4. Long-term prediction", "text": "Finally, we look at what happens when we execute our best auto-regressive model to predict longer sequences of up to 10 seconds into the future. In this experiment, we applied our S2S model in auto-regressive mode to ten sequences with 238 images from the Frankfurt Long Movie of the Cityscapes Validation Dataset. At four frame segmentations with a frame rate of 17 frames, the model predicts the ten nanotones. Thus, in this setting, the images are roughly sampled at 1 Hz. In Figure 7, we report IoU SEG performance as a function of time. In this extremely demanding setting, the prediction power quickly declines over time. Fine-tuning of the model in auto-regressive mode improves its performance, but gives only a clear advantage over the input baseline for predicting in one and two seconds. We also applied our model at a frame rate of 3 to predict up to 55 segments, so that the model in auto-regressive mode may look different, but this model may look much worse in terms of auto-regressive classes."}, {"heading": "5. Conclusion", "text": "We introduced a new visual comprehension task, predicting future semantic segmentation. We examined five different models for this task, based on RGB and / or segmentation of previous frames. To predict beyond a single future frame, we looked at stack models that predict all future frames at once, and auto-regressive models that sequentially predict future frames. We found that auto-egressive training provides the best results for our problem, and that models that predict in the segmentation space work better than those that rely on RGB frames. Although our results are encouraging, there is room for improvement. Where the Dilated 10 network for semantic image segmentation delivers about 69% IoU, this value drops to about 47% when predicting 3 frames ahead (0.18 sec.), when predicting 9 frames (0.54 sec.)."}], "references": [{"title": "Socially-aware large-scale crowd forecasting", "author": ["A. Alahi", "V. Ramanathan", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards principled methods for training generative adversarial networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "ICLR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Do deep nets really need to be deep", "author": ["L. Ba", "R. Caruana"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "The Cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Krahenbuhl", "T. Darrell"], "venue": "ICLR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "P", "author": ["A. Dosovitskiy", "P. Fischer", "E. Ilg", "P. Hausser", "C. Hazirbas", "V. Golkov"], "venue": "v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In ICCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to act by predicting the future", "author": ["A. Dosovitskiy", "V. Koltun"], "venue": "ICLR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915\u2013 1929", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial networks", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Video scene parsing with predictive feature learning", "author": ["X. Jin", "X. Li", "H. Xiao", "X. Shen", "Z. Lin", "J. Yang", "Y. Chen", "J. Dong", "L. Liu", "Z. Jie", "J. Feng", "S. Yan"], "venue": "arXiv:1612.00119", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["N. Kalchbrenner"], "venue": "van den Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. arXiv:1610.00527", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Auto-encoding variational Bayes", "author": ["D. Kingma", "M. Welling"], "venue": "ICLR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Activity forecasting", "author": ["K. Kitani", "B. Ziebart", "J. Bagnell", "M. Hebert"], "venue": "ECCV", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Refinenet: Multi-path refinement networks for high-resolution semantic segmentation", "author": ["G. Lin", "A. Milan", "C. Shen", "I. Reid"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic segmentation using adversarial networks", "author": ["P. Luc", "C. Couprie", "S. Chintala", "J. Verbeek"], "venue": "NIPS Workshop on Adversarial Training", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of long-term motion dynamics for videos", "author": ["Z. Luo", "B. Peng", "D.-A. Huang", "A. Alahi", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "ICLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["V. Patraucean", "A. Handa", "R. Cipolla"], "venue": "ICLR Workshop", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": "arXiv:1412.6604", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "Transformation-based models of video sequences", "author": ["J. Van Amersfoort", "A. Kannan", "M. Ranzato", "A. Szlam", "D. Tran", "S. Chintala"], "venue": "arXiv:1701.08435", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Anticipating the future by watching unlabeled video", "author": ["C. Vondrick", "P. Hamed", "A. Torralba"], "venue": "CVPR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating videos with scene dynamics", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "NIPS", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "venue": "ECCV", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing, 13(4):600\u2013612", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P. Werbos"], "venue": "Neural Networks, 1(4):339\u2013356", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1988}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["T. Xue", "J. Wu", "K. Bouman", "W. Freeman"], "venue": "NIPS", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "LR-GAN: Layered recursive generative adversarial networks for image generation", "author": ["J. Yang", "A. Kannan", "D. Batra", "D. Parikh"], "venue": "ICLR", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Energy-based generative adversarial networks", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "ICLR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 25, "context": "Prediction and anticipation of future events is a key component to intelligent decision-making [27].", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "ation and hypotheses made on what could happen next [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 23, "context": "The task of predicting future RGB video frames given preceding ones is an interesting one to assess if current vision systems are able to reason about future events, and it has recently received significant attention [25, 26, 22].", "startOffset": 217, "endOffset": 229}, {"referenceID": 24, "context": "The task of predicting future RGB video frames given preceding ones is an interesting one to assess if current vision systems are able to reason about future events, and it has recently received significant attention [25, 26, 22].", "startOffset": 217, "endOffset": 229}, {"referenceID": 21, "context": "The task of predicting future RGB video frames given preceding ones is an interesting one to assess if current vision systems are able to reason about future events, and it has recently received significant attention [25, 26, 22].", "startOffset": 217, "endOffset": 229}, {"referenceID": 0, "context": "by explicitly forecasting trajectories of people and other objects in future video frames [1, 17].", "startOffset": 90, "endOffset": 97}, {"referenceID": 16, "context": "by explicitly forecasting trajectories of people and other objects in future video frames [1, 17].", "startOffset": 90, "endOffset": 97}, {"referenceID": 7, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 18, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 17, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 22, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 34, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 3, "context": "Our experiments on the Cityscapes dataset [4] suggest that it is advantageous to directly predict future frames at the more abstract semantic-level, rather than to predict at the low-level RGB appearance of future frames [22, 25] and then to apply a semantic segmentation model to these.", "startOffset": 42, "endOffset": 45}, {"referenceID": 21, "context": "Our experiments on the Cityscapes dataset [4] suggest that it is advantageous to directly predict future frames at the more abstract semantic-level, rather than to predict at the low-level RGB appearance of future frames [22, 25] and then to apply a semantic segmentation model to these.", "startOffset": 221, "endOffset": 229}, {"referenceID": 23, "context": "Our experiments on the Cityscapes dataset [4] suggest that it is advantageous to directly predict future frames at the more abstract semantic-level, rather than to predict at the low-level RGB appearance of future frames [22, 25] and then to apply a semantic segmentation model to these.", "startOffset": 221, "endOffset": 229}, {"referenceID": 34, "context": "5 seconds into the future, the mean IoU of our predictions reaches two thirds of the one obtained by a state-of-the-art semantic segmentation model [36].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "[14] train a model to predict the semantic segmentation of the immediate next image from the preceding input frames, and fuse this prediction with the segmentation computed from the next input frame.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Nilsson and Sminchisescu [23] use a convolutional RNN model with a spatial transformer component [13] to accumulate the information from past and future frames in order to improve prediction of the current frame segmentation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "[24] employ a convolutional RNN to explicitly predict the optical flow, and use these to warp and aggregate per-frame segmentations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] introduced the first baseline of next video frame prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] developed a Long Short Term Memory (LSTM) [12] architecture for the task, and demonstrated a gain in action classification using the learned features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[26] developed a Long Short Term Memory (LSTM) [12] architecture for the task, and demonstrated a gain in action classification using the learned features.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "[22] improved the predictions using a multi-scale convolutional architecture, adversarial training [10], and a gradient difference loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[22] improved the predictions using a multi-scale convolutional architecture, adversarial training [10], and a gradient difference loss.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "To reduce the number of parameters to estimate, several authors re-parameterize the problem to predict frame transformations instead of raw pixels [9, 28].", "startOffset": 147, "endOffset": 154}, {"referenceID": 26, "context": "To reduce the number of parameters to estimate, several authors re-parameterize the problem to predict frame transformations instead of raw pixels [9, 28].", "startOffset": 147, "endOffset": 154}, {"referenceID": 20, "context": "[21] employ a convolutional LSTM architecture to predict sequences of up to eight frames of optical flow in RGBd videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] also uses LSTMs, and factorizes the temporal and spatial/color dimensions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] instead predict features in future frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Generative Adversarial Networks (GAN) [10] and Variational Auto-Encoders (VAE) [16] are recent unsupervised learning methods that can be used to deal with the inherent uncertainty in futureprediction tasks.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Generative Adversarial Networks (GAN) [10] and Variational Auto-Encoders (VAE) [16] are recent unsupervised learning methods that can be used to deal with the inherent uncertainty in futureprediction tasks.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "GAN training has recently been improved using the Wasserstein GAN approach of Arjovsky and Bottou [2], and the energy based approach of Zhao et al .", "startOffset": 98, "endOffset": 101}, {"referenceID": 35, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "An interesting approach using GANs for unsupervised image representation learning is proposed in [5], where the generative model is trained along with an inference model that maps images to their latent representations.", "startOffset": 97, "endOffset": 100}, {"referenceID": 28, "context": "[30] showed that GANs can be applied to video generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] use similar ideas to develop an iterative image generation model where objects are sequentially pasted on the image canvas using a recurrent GAN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] predict future video frames from a single given frame using a VAE approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] perform forecasting with a VAE, predicting feature point trajectories from still images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "To circumvent the need for datasets with per-frame annotations, we use the state-of-the-art multi-scale Dilated-10 semantic image segmentation network [36] to provide target semantic segmentations for all frames in each video.", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "This is motivated by recent observations in network distillation that the log-probabilities carry more information when training one network on the outputs of another network [3, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 10, "context": "This is motivated by recent observations in network distillation that the log-probabilities carry more information when training one network on the outputs of another network [3, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 21, "context": "[22], with two spatial scales.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Following [22], for all models the loss function between the model output \u0176 and the target output Y is the sum of an `1 loss and a gradient difference loss:", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "The gradient difference loss [22], instead, penalizes errors in the gradients of the target image and the predicted one.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "[22] in the context of raw images, introducing an adversarial loss term allows the model to disambiguate between modes corresponding to different turns of events, and reduces blur associated with this uncertainty.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] has demonstrated the positive influence of the adversarial training for semantic image segmentation, and its effectiveness in detecting higher-order spatial inconsistencies in the produced outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Our formulation of the adversarial loss term is based on the principles of recently introduced Wasserstein GAN [2], with some modifications for the semantic segmentation application.", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Following [2], we employ clipping of the discriminator weights \u0398 to the range [\u22120.", "startOffset": 10, "endOffset": 13}, {"referenceID": 31, "context": "We then back-propagate the gradients through time [33], where the gradients w.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "The Cityscapes dataset [4] contains 2,975 training, 500 validation and 500 testing video sequences of 1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 34, "context": "the segmentation produced using the Dilated-10 network [36] for the 20-th frame (IoU SEG).", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "To evaluate the quality of the frame RGB predictions, we compute the Peak Signal to Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) measures [32].", "startOffset": 163, "endOffset": 167}, {"referenceID": 7, "context": "We performed patch-wise training with 64\u00d7 64 patches for the largest scale resolution, enabling equal class frequency sampling as in [8], using mini-batches of four patches and a learning rate pf 0.", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "For the second baseline we use FlowNet [6] to estimate the optical flow between the last two inputs, and warp the last input using the estimated flow.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "This is in line with observations made in network distillation [3, 11].", "startOffset": 63, "endOffset": 70}, {"referenceID": 10, "context": "This is in line with observations made in network distillation [3, 11].", "startOffset": 63, "endOffset": 70}], "year": 2017, "abstractText": "The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we focus on predicting semantic segmentations of future frames. More precisely, given a sequence of semantically segmented video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Our models predict trajectories of cars and pedestrians much more accurately (25%) than baselines that copy the most recent semantic segmentation or warp it using optical flow. Prediction results up to half a second in the future are visually convincing, the mean IoU of predicted segmentations reaching two thirds of the real future segmentations.", "creator": "LaTeX with hyperref package"}}}