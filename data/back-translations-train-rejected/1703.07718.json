{"id": "1703.07718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Independently Controllable Features", "abstract": "Finding features that disentangle the different causes of variation in real data is a difficult task, that has nonetheless received considerable attention in static domains like natural images. Interactive environments, in which an agent can deliberately take actions, offer an opportunity to tackle this task better, because the agent can experiment with different actions and observe their effects. We introduce the idea that in interactive environments, latent factors that control the variation in observed data can be identified by figuring out what the agent can control. We propose a naive method to find factors that explain or measure the effect of the actions of a learner, and test it in illustrative experiments.", "histories": [["v1", "Wed, 22 Mar 2017 15:54:18 GMT  (123kb,D)", "http://arxiv.org/abs/1703.07718v1", "RLDM submission"]], "COMMENTS": "RLDM submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emmanuel bengio", "valentin thomas", "joelle pineau", "doina precup", "yoshua bengio"], "accepted": false, "id": "1703.07718"}, "pdf": {"name": "1703.07718.pdf", "metadata": {"source": "CRF", "title": "Independently Controllable Features", "authors": ["Emmanuel Bengio", "Valentin Thomas"], "emails": ["ebengi@cs.mcgill.ca", "valentin.thomas@epfl.ch", "jpineau@cs.mcgill.ca", "dprecup@cs.mcgill.ca", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": null, "text": "Keywords: presentation learning, controllable characteristics"}, {"heading": "Acknowledgements", "text": "The authors thank the Samsung Advanced Institute of Technology (SAIT), the Canadian Institute For Advanced Research (CIFAR) and the Natural Sciences and Engineering Research Council of Canada (NSERC) for their financial support of this work. Xiv: 170 3.07 718v 1 [cs.L G] 22 March 2"}, {"heading": "1 Introduction", "text": "Whether in static or dynamic environments, decision-making for real-world problems often faces the difficult challenge of finding a \"good\" representation of the problem. In the context of supervised or semi-supervised learning, it is argued (Bengio, 2009) that good representations separate the underlying explanatory factors that may be the causes of the observed data. In such problems, feature learning often involves mechanisms such as autoencoders (Hinton & Salakhutdinov, 2006) that find latent features that explain the observed data. In interactive environments, the dependence of time between successive observations creates a new opportunity to see structure in data that is not only evident from observational studies. The need to experiment in order to discover causal structures is already well explored in psychology (e.g. Gopnik & Wellman (in the press))."}, {"heading": "2 Independently controllable features", "text": "In order to concretize the above intuitions, we assume that the observations from an interactive environment are based on variation factors that are \"independently controllable,\" that is, for each of these variation factors there is a policy that changes only this factor, not the others. For example, the object behind a series of pixels could be viewed independently of other objects, which would explain the variations in its pose and size when we move it. In this case, the object is a \"variation factor.\" What makes the discovery and attribution of such factors to characteristics difficult is that the factors are not explicitly observed. Our goal is to learn these factors, which we call independently controllable characteristics, along with measures that control them. Although these may seem strong assumptions about the nature of the environment, our view is that they are similar to regulators designed to better limit a difficult learning problem. There are many possible ways to express the desire to visualize independently controllable characteristics as an objective part of this scenario."}, {"heading": "2.1 Autoencoders", "text": "Our approach builds on the familiar framework of autoencoders (Hinton & Salakhutdinov, 2006), which are defined as a pair of functional approximation factors f, g with parameters \u03b8, so that f: X \u2192 H maps the entrance space to a latent room H, and g: H \u2192 X to the entrance space X-Rd. Autoencoders are trained to minimize the discrepancy between x and g (f (x)), a.k.a. the reconstruction error, e.g.: min \u03b81 2 x-g (f (x)). 2 2We call f (x) = h-H-Rn the latent feature representation of x, with n characteristics. It is common to assume that f and g perform the dimensional reduction of X, i.e. the compression, since there is a dimensional bottleneck through which information about the input data must pass."}, {"heading": "2.2 Policy Selectivity", "text": "Consider the following simple scenario: We train an autoencoder f, g that generates n latent characteristics, fk, k = 1,.. n. In parallel to these characteristics, we train n policies, which are called \u03c0k. Autoencoders can learn relatively arbitrary representations of characteristics, but we want these characteristics to correspond to controllable factors in the environment of the learner. Specifically, we want policies to cause a change only in fk and not in other characteristics. We think of fk and \u03c0k as directional characteristics. To quantify the change in fk when actions are taken according to \u03c0k, we define the selectivity of a characteristic as: sel (s, a, k) = It is a problem (s) \u2212 fk (s) \u2212 fk (s) | fk \"fk\" (s \") that the change (s\" fk \") \u2212 fk\" (s \") \u2212 be selected, whereby selectivity can be defined by a characteristic."}, {"heading": "2.3 A first toy problem", "text": "Consider the simple environment described in Figure 1 (a): The agent sees a 2 x 2 square of adjacent cells in the environment and has 4 actions that move it up, down, left, or right. An auto encoder with directed selectivity (see Figure 1 (c, d))) learns latent characteristics that correspond to the (x, y) position of the square in the entrance space without ever being able to explicitly access these values, and reconstructs the input properly. An auto encoder without selectivity also correctly reconstructs the input, but without explicitly learning these two latent (x, y) characteristics. In this setting, f, g, and \u03c0 share some of their parameters. We use the following architecture: f has two 16 x 3 x 3 x 3 ReLU layers, followed by a fully connected ReLU layer with 32 units and a tanh layer with n = 4 characteristics; g is the transposed architecture from max to max, a softness policy that is complete."}, {"heading": "2.4 A slightly harder toy problem", "text": "In the next experiments, we aim to generalize the above model in a slightly more complex environment. Instead of parameterizing f and \u03c0k by the same parameters, we now introduce a number of different parameters for each policy and for the encoder, so that each policy can be learned separately. In addition, we use a richer action space, in which we add a \"move down and to the right\" and a \"increase / decrease the color of the square action.\" Also, note that the first two actions (\"move down\") are redundant. In this context, f, g and \u03c0k are all parameterized by different variables, such as: \u2022 f (s) = f (s; Wf) a neural net with a tanh output activation \u2022 g (h; Wg) a neural network with a ReLU output activation \u2022 (h); (h) a neural network with an h: \u2022 h)."}, {"heading": "3 Scaling to general environments: controllability and the binding problem", "text": "In this case, if the goal postulated in Section 2.2 is understood correctly, we can assume that the attribute k of the representation can clearly refer to a certain property of a certain object in the environment. For example, the agent's world could contain only a red circle and a green rectangle, which are influenced only by the actions of the agent (they do not move on their own), and we only change the positions and colors of these objects from one attempt to the next. Therefore, a certain attribute fk can learn to point unequivocally to the position or color of one of these two objects. In reality, environments are stochastical, and the set of objects in a particular scene is drawn by a distribution."}, {"heading": "4 Discussion", "text": "In Jaderberg et al. (2016), agents propose models that learn to predict the future, due to action sequences that induce the agent to capture temporal characteristics (Bacon et al., 2016). There are many other works that go in this direction, such as (deep) sequel characteristics representations (Dayan, 1993; Kulkarni et al., 2016) or the Options Framework (Sutton et al., 1999; Precup, 2000) when used in conjunction with neural networks (Bacon et al., 2016). Our approach is similar in the spirit of Horde architecture (Sutton, 2011). In this scenario, agents learn strategies that maximize specific inputs. Predictions for all these strategies then become characteristics for the agent. Our goal is to learn in contrast to David, the work-focused world (cost targets)."}], "references": [{"title": "The option-critic architecture", "author": ["Bacon", "Pierre-Luc", "Harb", "Jean", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1609.05140,", "citeRegEx": "Bacon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2016}, {"title": "Learning deep architectures for AI", "author": ["Bengio", "Yoshua"], "venue": "Now Publishers,", "citeRegEx": "Bengio and Yoshua.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["Dayan", "Peter"], "venue": "Neural Computation,", "citeRegEx": "Dayan and Peter.,? \\Q1993\\E", "shortCiteRegEx": "Dayan and Peter.", "year": 1993}, {"title": "Tagger: Deep unsupervised perceptual grouping", "author": ["Greff", "Klaus", "Rasmus", "Antti", "Berglund", "Mathias", "Hao", "Tele", "Valpola", "Harri", "Schmidhuber", "Juergen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks. science", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Deep successor reinforcement learning", "author": ["Kulkarni", "Tejas D", "Saeedi", "Ardavan", "Gautam", "Simanta", "Gershman", "Samuel J"], "venue": "arXiv preprint arXiv:1606.02396,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Complex valued artificial recurrent neural network as a novel approach to model the perceptual binding problem", "author": ["Minin", "Alexey", "Knoll", "Alois", "Zimmermann", "Hans-Georg", "AG Siemens", "Siemens", "LLC"], "venue": "In ESANN. Citeseer,", "citeRegEx": "Minin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Minin et al\\.", "year": 2012}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Temporal abstraction in reinforcement learning", "author": ["Precup", "Doina"], "venue": null, "citeRegEx": "Precup and Doina.,? \\Q2000\\E", "shortCiteRegEx": "Precup and Doina.", "year": 2000}, {"title": "Using mdp characteristics to guide exploration in reinforcement learning", "author": ["B. Ratitch", "D. Precup"], "venue": "In ECML, pp", "citeRegEx": "Ratitch and Precup,? \\Q2003\\E", "shortCiteRegEx": "Ratitch and Precup", "year": 2003}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton"], "venue": "In AAMAS,", "citeRegEx": "Sutton and Precup.D.,? \\Q2011\\E", "shortCiteRegEx": "Sutton and Precup.D.", "year": 2011}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 7, "context": "The binding problem has seen some attention in the representation learning literature (Minin et al., 2012; Greff et al., 2016), but still remains mostly unsolved.", "startOffset": 86, "endOffset": 126}, {"referenceID": 3, "context": "The binding problem has seen some attention in the representation learning literature (Minin et al., 2012; Greff et al., 2016), but still remains mostly unsolved.", "startOffset": 86, "endOffset": 126}, {"referenceID": 3, "context": ", 2012; Greff et al., 2016), but still remains mostly unsolved. Jointly considering this problem and larning controllable features may prove fruitful. These ideas may also lead to interesting ways of performing exploration. How do humans choose with which object to play? We are attracted to objects which we do not know yet (i.e., if and how we can control them). The RL exploration process could be driven by a notion of controllability, predicting the interestingness of objects in a scene and choosing features and associated policies with which to attempt control them. Such ideas have only been explored briefly in the literature, e.g. Ratitch & Precup (2003)", "startOffset": 8, "endOffset": 666}], "year": 2017, "abstractText": "Finding features that disentangle the different causes of variation in real data is a difficult task, that has nonetheless received considerable attention in static domains like natural images. Interactive environments, in which an agent can deliberately take actions, offer an opportunity to tackle this task better, because the agent can experiment with different actions and observe their effects. We introduce the idea that in interactive environments, latent factors that control the variation in observed data can be identified by figuring out what the agent can control. We propose a naive method to find factors that explain or measure the effect of the actions of a learner, and test it in illustrative experiments.", "creator": "LaTeX with hyperref package"}}}