{"id": "1703.07805", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Supervised Typing of Big Graphs using Semantic Embeddings", "abstract": "We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.", "histories": [["v1", "Wed, 22 Mar 2017 18:20:07 GMT  (1504kb,D)", "http://arxiv.org/abs/1703.07805v1", "6 pages, to be published in Semantic Big Data Workshop at ACM, SIGMOD 2017; extended version in preparation for Open Journal of Semantic Web (OJSW)"]], "COMMENTS": "6 pages, to be published in Semantic Big Data Workshop at ACM, SIGMOD 2017; extended version in preparation for Open Journal of Semantic Web (OJSW)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mayank kejriwal", "pedro szekely"], "accepted": false, "id": "1703.07805"}, "pdf": {"name": "1703.07805.pdf", "metadata": {"source": "META", "title": "Supervised Typing of Big Graphs using Semantic Embeddings", "authors": ["Mayank Kejriwal", "Marina Del Rey", "Pedro Szekely"], "emails": ["kejriwal@isi.edu", "pszekely@isi.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "In fact, the fact is that most of them are able to reform themselves, and that they are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...)"}, {"heading": "1 RELATEDWORK", "text": "In fact, most people who are able are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Framework", "text": "The fundamentals of work in this field are defined by the formal definition of a typical knowledge base (t-KB) and the associated conceptual principles in terms of the way in which the individual knowledge in terms of the way in which they are related to the way in which the individual knowledge in terms of the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in terms of the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related in which they are related to the way in which they are related in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related to the way in which they are related in which they are related to the way in which"}, {"heading": "2.2 Generating Type Embeddings", "text": "Algorithm 1 provides the pseudocode for our solution. Before describing the pseudocode, we describe the intuition as follows. Algorithm 1 is based on two assumptions: first, a type is ultimately described by its entities, which means that all things are equal, a type should be near as many entities that have that type as possible. Second, a type should give more priority to entities that exclusively describe it. Suppose an entity s1 has more than ten (explicitly declared) type assertions {t1, t2, t10} while entity s2 has only two type assertions, t2}. Algorithm 1 is designed so that s2 will contribute to the derivation of t1 and t2 entities."}, {"heading": "2.3 Applications", "text": "Type Recommendation. Given the type embeddings generated in the previous section, we can recommend types (with values) for an untyped entity by recommending the point product between the embeddings \u2212 \u2192 s of the entity and each of the type embeddings derived in Algorithm 1 and normalizing them via the | T | results (if a valid probability distribution is desired).The probability distribution can also be used to classify a number of types, with the highest type being the most relevant suggestion for the entity. Apart from the fact that knowledge databases are \"completed\" with many untyped entities, such rankings can also be used for tasks such as the semantic search.Online clusters. Unlike other update-based machine learning algorithms such as Stochastic Gradient Descent, we note that once-seen data can be discarded, making Algorithm 1 less likely to be used for streaming instances of a set of instances > full Bpedia results."}, {"heading": "3 PRELIMINARY EXPERIMENTS", "text": "The aim of this section is to illustrate the promising approach based on some preliminary results. In section 4 we discuss future directions based on the preliminary results."}, {"heading": "3.1 Preliminaries", "text": "Datasets: We construct evaluation datasets by performing random strated partitioning on the full set of DBpedia triples. We used the publicly available type-only KB2 for our experiments from the October 2015 release of the English-language DBpedia. le contains all kinds of assertions that are derived only from the mapping-based extractions, without transitive closure using ontology. Details of the ground truth datasets are provided in Table 1. About the full partition of Mac datasets, there are 3,963,983 unique instances and 415 unique types. E ve datasets are broadly consistent in their representation of the entire dataset, and are not subject to splitting or dataset bias, due to random strati ed partitioning. In Section 3.3, we also compare our type of embedding with the DBpedia domain ontology3.Entity embedding algorithms: the algorithms presented in this paper."}, {"heading": "1. (a) plots the averageRecall@k across all ve experimental runs, while (b) illustrates average number of recommended types per entity for each of the datasets and baselines.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 Probabilistic Type Recommendation", "text": "This year it is more than ever before."}, {"heading": "3.3 Visualizing the Extensional Model", "text": "Note that our methods have never relied on the ontology of DBpedia to derive embeddings and generative model parameters. However, an interesting experiment is the use of the intensive semantics of types (e.g. subclass relationships in ontology) to visualize embeddings derived from extensive assertions (mapping-based type assertion extractions). We perform two visualization experiments using the unattended t-SNE algorithm [10], a tool for visualizing high-dimensional points on a 2-D plan. Visualization 1: We apply t-SNE to a matrix containing the embedding of vectors of all direct subtypes of the ontology of DBpedia, namely Animal, Politican, MusicalWork and Plant."}, {"heading": "3.4 Online Type Clustering of DBpedia", "text": "The results of Experiment 2 in Section 3.2 illustrated that there are many types in the ontology of DBpedia that are clearly related to an entity of interest, even if they are not supertypes (or even sub-types) of the entity type. Ideally, given an entity, we would like to assign a probability to each type. Such clustering has a lot of potential, including topic modeling of entities and blurred thinking. To achieve a complete fuzzy clustering via DBpedia, we used the unification of all ve records in Table 1 to calculate our type embeddings, and then executed the fuzzy clustering algorithm across all DBpedia entities in the unification of all ve datasets. e-algorithm scaled almost linearly and was able to complete the execution of nearly 4 million entities and 415 clusters (types) in about 50 hours, with over 100 GB of future data being collected on a public server close to it."}, {"heading": "4 CONCLUSION", "text": "We developed a scalable data-driven algorithm to deduce type embedding and applied the algorithm to a probable type recommendation task on ve DBpedia partitions. Compared to a kNN baseline, the algorithm delivers better results based on various relevance criteria and is significantly faster. Visualizations also show that clusters exist intentionally via type embedding. Future workers. Here, there are many opportunities for future work that we have already explored. First, we use the methods in this paper to embed entire ontologies (collections of types and properties) in vector spaces to enable a combination of distributed and ontological semantic thinking. Second, we are investigating more applications of embedded types, such as an improved version of semantic search and epimanditions."}], "references": [{"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in neural information processing systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Entity Typing using Distributional Semantics and DBpedia", "author": ["Marieke van Erp", "Piek Vossen"], "venue": "In Under Review", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Applying Universal Schemas for Domain Speci\u0080c Ontology Expansion", "author": ["Paul Groth", "Sujit Pal", "Darin McBeath", "Brad Allen", "Ron Daniel"], "venue": "Proceedings of AKBC", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Semantically Smooth Knowledge Graph Embedding", "author": ["Shu Guo", "\u008ban Wang", "Bin Wang", "Lihong Wang", "Li Guo"], "venue": "In ACL", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "PSDVec: A toolbox for incremental and scalable word embedding", "author": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In AAAI", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Graph kernels for RDF data", "author": ["Uta L\u00f6sch", "Stephan Bloehdorn", "Achim Re\u008ainger"], "venue": "In Extended Semantic Web Conference", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Typi\u0080er: Inferring the type semantics of structured data", "author": ["Yongtao Ma", "\u008canh Tran", "Veli Bicer"], "venue": "In Data Engineering (ICDE),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geo\u0082rey Hinton"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Type prediction in RDF knowledge bases using hierarchical multilabel classi\u0080cation", "author": ["Andr\u00e9 Melo", "Heiko Paulheim", "Johanna V\u00f6lker"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Je\u0082 Dean"], "venue": "In Advances in neural information processing systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Type inference on noisy rdf data", "author": ["Heiko Paulheim", "Christian Bizer"], "venue": "In International Semantic Web Conference", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Je\u0082rey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Graph classi\u0080cation and clustering based on vector space embedding. World Scienti\u0080c Publishing Co., Inc", "author": ["Kaspar Riesen", "Horst Bunke"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Rdf2vec: Rdf graph embeddings for data mining", "author": ["Petar Ristoski", "Heiko Paulheim"], "venue": "In International Semantic Web Conference", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "RDF graph embeddings for content-based recommender systems", "author": ["Jessica Rosati", "Petar Ristoski", "Tommaso Di Noia", "Renato de Leone", "Heiko Paulheim"], "venue": "In CEUR workshop proceedings,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "\u008ce distributional hypothesis", "author": ["Magnus Sahlgren"], "venue": "Italian Journal of Linguistics 20,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Knowledge Base Completion Using Embeddings and Rules", "author": ["\u008ban Wang", "Bin Wang", "Li Guo"], "venue": "In IJCAI", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "InAAAI. Citeseer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "In recent years, the distributional semantics paradigm has been used to great e\u0082ect in natural language processing (NLP) for embedding words in vector spaces [20].", "startOffset": 158, "endOffset": 162}, {"referenceID": 18, "context": "\u008ce distributional hypothesis (also known as Firth\u2019s axiom) states that the meaning of a word is determined by its context [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "Algorithms like word2vec use neural networks on large corpora of text to embed words in semantic vector spaces such that contextually similar words are close to each other in the vector space [12].", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "Recent work has extended such neural embedding techniques, traditionally introduced only for natural language word sequences, to alternate kinds of data, including entities in large knowledge graphs like DBpedia [16, 17].", "startOffset": 212, "endOffset": 220}, {"referenceID": 16, "context": "Recent work has extended such neural embedding techniques, traditionally introduced only for natural language word sequences, to alternate kinds of data, including entities in large knowledge graphs like DBpedia [16, 17].", "startOffset": 212, "endOffset": 220}, {"referenceID": 7, "context": "\u008ce basic approach is to convert an instance-rich knowledge graph into sets of sequences of graph nodes by performing random walks or using graph kernels [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 16, "context": "NLP algorithms like word2vec are applied on the sequences to embed entities, just like words in natural language sentences [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "word2vec [12] and glove [14]) achieving state-of-the-art performance on a number of NLP tasks (e.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "word2vec [12] and glove [14]) achieving state-of-the-art performance on a number of NLP tasks (e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 1, "context": "dependency parsing) [2].", "startOffset": 20, "endOffset": 23}, {"referenceID": 14, "context": "A famous example is DeepWalk, which applies word embedding techniques on random walk sequences on a graph to embed nodes in the graph to vectors [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 16, "context": "In the Semantic Web, variants of this strategy were recently applied to DBpedia and Wikidata, and the embedded entities were used in several important problems, include content-based recommendation and node classi\u0080cation [17],[18].", "startOffset": 221, "endOffset": 225}, {"referenceID": 17, "context": "In the Semantic Web, variants of this strategy were recently applied to DBpedia and Wikidata, and the embedded entities were used in several important problems, include content-based recommendation and node classi\u0080cation [17],[18].", "startOffset": 226, "endOffset": 230}, {"referenceID": 6, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 21, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 20, "context": "An important aspect of this research is automatic knowledge base construction and completion (AKBC), to which this work is related [21], [4].", "startOffset": 131, "endOffset": 135}, {"referenceID": 3, "context": "An important aspect of this research is automatic knowledge base construction and completion (AKBC), to which this work is related [21], [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "entities), we can a\u0082ord to infer types without incrementally training the model such as in [6] or any other details of how the entity embeddings were derived.", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "We also do not rely on natural language context of any kind [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "in prior work, a good example being Typi\u0080er [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "data and pseudo-schema features [9]), our approach is feature-agnostic.", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "ommenders are the systems in [11], [13].", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "ommenders are the systems in [11], [13].", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "Due to the di\u0081culty in automating feature construction, feature-agnostic systems are still quite rare; for example, in the Semantic Web, only a recent work achieved competitive performance at scale for feature-agnostic node classi\u0080cation [17].", "startOffset": 238, "endOffset": 242}, {"referenceID": 16, "context": "Concerning the actual learning of entity embeddings, we noted in Section 1 that recent work has successfully managed to learn embeddings (in spaces with only a few hundred dimensions) on large datasets like DBpedia by applying neural embedding algorithms like word2vec on graph node sequences [17].", "startOffset": 293, "endOffset": 297}, {"referenceID": 16, "context": "Recently, [17] publicly made available 500-dimensional embeddings for DBpedia entities by using the word2vec algorithm on graph node sequences4.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "We perform two visualization experiments using the unsupervised t-SNE algorithm [10], a stateof-the-art tool for visualizing high-dimensional points on a 2-D plot.", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. \u008ce algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.", "creator": "LaTeX with hyperref package"}}}