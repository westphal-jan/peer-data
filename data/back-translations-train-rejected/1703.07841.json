{"id": "1703.07841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Classification-based RNN machine translation using GRUs", "abstract": "We report the results of our classification-based machine translation model, built upon the framework of a recurrent neural network using gated recurrent units. Unlike other RNN models that attempt to maximize the overall conditional log probability of sentences against sentences, our model focuses a classification approach of estimating the conditional probability of the next word given the input sequence. This simpler approach using GRUs was hoped to be comparable with more complicated RNN models, but achievements in this implementation were modest and there remains a lot of room for improving this classification approach.", "histories": [["v1", "Wed, 22 Mar 2017 20:31:47 GMT  (36kb,D)", "http://arxiv.org/abs/1703.07841v1", "7 pages, 1 figure; graduate course research project"]], "COMMENTS": "7 pages, 1 figure; graduate course research project", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["ri wang", "maysum panju", "mahmood gohari"], "accepted": false, "id": "1703.07841"}, "pdf": {"name": "1703.07841.pdf", "metadata": {"source": "CRF", "title": "Classification-based RNN machine translation using GRUs", "authors": ["Ri Wang", "Maysum Panju", "Mahmood Gohari"], "emails": ["rtwang@uwaterloo.ca", "mhpanju@uwaterloo.ca", "mgohari@uwaterloo.ca"], "sections": [{"heading": "1 Introduction", "text": "Machine translation is an increasingly important area of study as the need for natural language translation grows rapidly in a world that is increasingly multilingual and networked. At the same time, machine translation is an incredibly difficult task, the problems of which span a range of linguistic fields and are difficult even for human translators. Previous efforts have focused on statistical machine translation, but the continuous and exponential increase in computing power and advances in neural network formation have enabled researchers to approach this problem with recurring neural networks. Currently, neural networks such as LSTMs and bi-directional RNs are being trained for this purpose. However, the methods described in much of this research are relatively complex and often not intuitive. However, the approach of forward-working neural network researchers involves using the neural network as a classifier to predict probabilities, and is a simpler and more attractive framework on which we can build the neural network we have to attempt to build on the relative length of the work."}, {"heading": "2 Background and Related Work", "text": "The approach to translation using recursive neural networks was mainly based on the model of recursive input to input given by Sutskever et. al. [12] to produce a translation. In their work, they trained pairs of long short memory models (LSTMs): one LSTM served as a feature extractor to convert a variable length sentence into a fixed length vector so that sentences with similar meanings are mapped close together, and the other LSTM records the word vectors as hidden states and prints words one by one, gradually building up the translation. It was not entirely clear how this pair of LSTM training was carried out in the context of reverse propagation, and we decided to focus on their alternative model, to have a single RNN to perform both feature extraction and translation. The model we used was also very closely related to that of Cho al et al et al coal as the RNN coal and the GRN coal [1] coding machine."}, {"heading": "3 Basic Framework of the Learning Models", "text": "The key to training a machine is a large proportion of the sentences that have been correctly translated into the target language. For a high-performance translator, this training must be very large and the translations must be perfect."}, {"heading": "4 Model", "text": "In this section, we describe how the deep learning model is used to perform machine translations once the pre-processing and setup from Section 3 is implemented."}, {"heading": "4.1 Automatic Machine Translation", "text": "After the input text in the source language is presented as sequences of words containing fixed-length vectors, our translation model is treated as follows: For a given sequence of input word vectors (x1,.., xF) encoding a sentence in the source language, and for the sequence of output word vectors (y1,.., yk) encoding the tokens in the translated sentence in the target language so far, the neural network estimates the conditional probability of word vector representation of the next word in the translated word sequence yk + 1. Thus, part of the input sequence into the neural network is a complete sequence of word vectors and the rest of the input word is a sequence in progress consisting of word vectors containing the next word yk. The output vector of this neural network is a vector of length xy."}, {"heading": "4.2 The neural network model: GRUs", "text": "The main contribution of deep learning to the task of automatic machine translation, in the approach that this paper takes, is the calculation of conditional probabilities for the next translated word given the progress of translation to date. \u2212 This approach has been applied using various specializations of neural networks for the task, including long-term short-term memory networks [5] and bi-directional recursive neural networks [9]. In this paper, we explore the possibility of using gated recursive units in the model. \u2212 A gated recurrent unit (GRU) was originally proposed by Cho et. al. [1] for its translation task. The idea behind this approach is to maintain the current content at each step in the neural network \u2212 and to add the new content to it while forgetting all information from the past that does not add additional information to the current state. Unlike the LSTM approach, which defines a separate memory cell, the Uxxt resets and stores only the relevant content of the STM and STM precursor, the two precursor phrases are deleted."}, {"heading": "5 Training Methods and Implementation", "text": "For this work, we used the publicly available Europarl1 corpus [6], with French as the source language and English as the target language. This data set contains approximately two million examples of English and French sentence pairs over a variety of sentence lengths. Translation was made from French into English because this report is intended for an English-speaking audience, and an English-language model allows us to assess the quality of the translation directly and independently. Furthermore, this approach distinguishes our work from that of many other work in the field in which English is often used as the source language. However, for English word embedding, we did not provide a pre-embedded model with GloVe as part of this work, provided by researchers for GloVe on their website at http: / / nlp.stanford.edu / projects / glove /."}, {"heading": "5.1 Method 1", "text": "With this training method, the focus is solely on building a strong classifier assuming that everything is perfectly aligned with the current position in the translation. Given a sequence of French input word vectors (x0,.., xF) and the corresponding sequence of correct English translation word vectors (y0,.., yE), this method generates forecasts1 The European Parliament dataset, which is largely limited to political topics and specific phrases, is not representative of the speech as a whole, meaning that the model may not be able to generalize well under other circumstances. However, it is not easy to obtain high-quality translation data, and we need to get along with what is available. Translation words y-k individually, but uses the basic truth translations instead of its own predictions each time it looks for the next word in order: 1. Input: (x1,., xF), (); Edition: 2. Expectation (y); Y (): (1)."}, {"heading": "5.2 Method 2", "text": "The focus of the second training method is on building a classifier capable of correcting itself robustly from intermediate translation errors. As in the previous approach, this method generates predicted translation words y-k individually, but now uses its own predicted translation of the model and not the basic truth to predict the next translated word in the output sequence: 1. Input: (((x1,.., xF), ())); Output: y-1; Output: y-1,.); Output: y-1,.); Output: y-E; Output: (.,., xF); Output: y2; Output: y2... 3. Input: (((x1,.., xF), (y-1,.); Output: y-1); Output: y-1; Output: (.,., xF); Output: y-1; (y-1); (y-E); (.), y-buster; (.), y-1): This is said in a similar way to the previous one; and it is said in a similar way to the previous one, y-E), and (1): it is much more practical."}, {"heading": "5.3 Technical Details", "text": "Some of the specific decisions made regarding the implementation of model training for this work are described below. 1. Intimate weights were determined using glorot initialization [4] with weights sampled from the uniform distribution, designed to determine weights that are neither too small nor too large to avoid vanishing or exploding signals.2. We did not anticipate many problems with disappearing gradients, since the GRU-based model is generally able to handle this problem safely. In order to avoid the unlikely risk of exploding gradients, a hard clip for gradient values was kept at 100.3. The entire collection of gradient values is mixed at the beginning of each training epoch. Training then proceeds pochastically through the sequence of stacks. In total, there were about 6,000 stacks and a single epoch took 5-6 hours using a single Nvidia error to predict each Ti 4."}, {"heading": "6 Results", "text": "In general, the neural network trained for this work has not performed as well as other state-of-the-art models, but has had modest success given the limited time and resources available.2 Of the two methods described in Section 5, the second method is believed to provide a stronger and more robust translation model. However, in practice, the training process has been prohibitively slow, as the model has not been able to quickly identify useful patterns in the translation from the randomly generated original translations. As a result, only the first method has been successfully implemented and tested, and the results of the translation model can be evaluated qualitatively by looking at some of the automatically translated sentences and comparing them with the correct English translations. A sample of some translations can be seen in Table 1. For comparison, the translation offered by Google Translate is also subject to review. The current model quickly deteriorates the translation quality for longer sentences, even for those within the training, but the results on some of the shorter sentences are reasonable, and sometimes the translation is not complete enough even if the word is translated."}, {"heading": "7 Recommendations and Conclusions", "text": "Although the resulting translations were not always of the highest quality, there is almost always a visible link to the intended translation, and there is often some potential for a powerful automatic translator. In fact, the current success can be seen as quite promising as a starting attempt at a new approach with tight time and resource constraints. In future work, it would be interesting to use a larger training set, more nodes per hidden layer in the neural network, and a larger dataset for the training of French word vectors. These adjustments would certainly increase the time required for training, but it is possible that they will lead to a more capable translator. Based on the solitary model examined in this work, it is not clear what impact many of the parameters, such as the number of nodes and the size of the vocabulary, would have on the ability of the resulting translator."}, {"heading": "8 Acknowledgments", "text": "The authors would like to thank Dr. Ghodsi for his teaching and for creating an environment of enthusiasm for learning, both for the students and for our algorithms. Thank you also for taking the time to highlight our latest projects during the winter holiday.2It is regrettable that, despite repeated and serious consultation with Sharcnet support services, technical problems prevented the research team from taking full advantage of Sharcnet resources in time to do this work. 3Perhaps 40% is not particularly good, but machine translation is a difficult problem, and it is reassuring to note that even Google Translate has problems with the translation of some sentences."}], "references": [{"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk"], "venue": "In COLING (Posters),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "[12] of recursively inputting output into input to generate a translation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1], which introduced the GRU and applied it as an RNN encoder-decoder machine translation model as an alternative to LSTM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "However, the classification approach taken here is closer related to previous work done by Schwenk [10], with feedforward neural networks that take in a fixed input sentence and output the probability of a sequence of words occurring next, as well as the work of Devlin et al.", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": ", who only predicted the next word [3].", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Prior to use in the model, words were converted into word vectors using a standard continuous word-to-vector embedding [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "Furthermore, we expect higher quality word embedding from specialized representation models, such as GloVe [8], than any word embedding built into the translation neural network such as the one used by Cho et al.", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "[1], because they are trained on a much larger dataset and their linguistic capabilities have been proven to be very strong.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "This approach has been applied using various specializations of neural networks for the task, including long shortterm memory networks [5] and bidirectional recurrent neural networks [9].", "startOffset": 135, "endOffset": 138}, {"referenceID": 8, "context": "This approach has been applied using various specializations of neural networks for the task, including long shortterm memory networks [5] and bidirectional recurrent neural networks [9].", "startOffset": 183, "endOffset": 186}, {"referenceID": 0, "context": "[1] for their translation task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": ": Empirical evaluation of gated recurrent neural networks on sequence modeling [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "For this work, we made use of the publicly available Europarl1 corpus [6], with French as the source language and English as the target language.", "startOffset": 70, "endOffset": 73}, {"referenceID": 11, "context": "al [12]; however, the number of nodes in the present model is set at 500 per layer, in contrast with the 1000 used by the earlier work, in order to speed up training and reduce potential overfitting.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "Intial weights were set using the Glorot initialization [4] with weights sampled from the uniform distribution.", "startOffset": 56, "endOffset": 59}, {"referenceID": 10, "context": "Nesterov momentum [11] was used during the gradient descent process in an attempt to speed up the training procedure.", "startOffset": 18, "endOffset": 22}], "year": 2017, "abstractText": "We report the results of our classification-based machine translation model, built upon the framework of a recurrent neural network using gated recurrent units. Unlike other RNN models that attempt to maximize the overall conditional log probability of sentences against sentences, our model focuses a classification approach of estimating the conditional probability of the next word given the input sequence. This simpler approach using GRUs was hoped to be comparable with more complicated RNN models, but achievements in this implementation were modest and there remains a lot of room for improving this classification approach.", "creator": "LaTeX with hyperref package"}}}