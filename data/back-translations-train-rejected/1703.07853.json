{"id": "1703.07853", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Faster Reinforcement Learning Using Active Simulators", "abstract": "In this work, we propose several online methods to build a \\emph{learning curriculum} from a given set of target-task-specific training tasks in order to speed up reinforcement learning (RL). These methods can decrease the total training time needed by an RL agent compared to training on the target task from scratch. Unlike traditional transfer learning, we consider creating a sequence from several training tasks in order to provide the most benefit in terms of reducing the total time to train.", "histories": [["v1", "Wed, 22 Mar 2017 21:07:35 GMT  (352kb,D)", "http://arxiv.org/abs/1703.07853v1", "12 pages and 4 figures"]], "COMMENTS": "12 pages and 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vikas jain", "theja tulabandhula"], "accepted": false, "id": "1703.07853"}, "pdf": {"name": "1703.07853.pdf", "metadata": {"source": "CRF", "title": "Faster Reinforcement Learning Using Active Simulators", "authors": ["Vikas Jain"], "emails": ["vksjn18@gmail.com", "tt@theja.org"], "sections": [{"heading": null, "text": "Our methods use the agent's learning path on the previously seen curriculum tasks to decide which tasks to train next. An attractive feature of our methods is that they are weakly linked to the choice of the RL algorithm and the transfer learning method. Furthermore, when domain information is available, our methods can incorporate this knowledge to further accelerate learning. We have experimentally demonstrated that these methods can be used to obtain suitable curricula that accelerate the entire training time in two different areas."}, {"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves if they are not able to play by the rules. In fact, it is so that they are not able to play by the rules. In fact, it is so that they are able to play by the rules themselves. In fact, it is so that they are able to play by the rules. In fact, it is so that they are not able to play by the rules. In fact, it is so that they are able to play by the rules themselves."}, {"heading": "2 Related Work", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "3 Preliminaries", "text": "We present a few key components that will be useful in defining our problem and solutions. Transfer to RL: Transfer to RL: In transfer amplification Learn = v = learn on a task M directly, an agent first learns on a suitable source task Msub. The knowledge learned (for example Q values) from Msub is then used for warm-start learning on the target task M, which makes the latter task faster. In our thesis, we demonstrate the performance of our online curriculum learning methods using Q function transfers. This includes initializing the Q function for the agent working with M, using the environment model [4] and the value function [16]. In our thesis, we demonstrate the performance of our online curriculum learning methods using Q function transfers."}, {"heading": "4 Problem Setup", "text": "Make the MDP count as Mtarget according to the target task. \u2192 Allow the set of training tasks Tsub = {Msubk: 1 \u2264 k \u2264 K}, where each msubk is an MDP similar to Mtarget with possibly different states and spaces of action. Allow the total number of steps (each corresponding to a state transition) to train the RL Agent T. The goal of learning the curriculum is to design an algorithm that, when given an RL Agent as input, will perform a (partial) sequence of tasks by Tsub and train the agent on them first and then on theMtarget tasks to achieve a desired level of performance. In particular, the total number of steps he takes to design the curriculum, train the agent on the curriculum and then on the Mtarget is smaller than the steps necessary to train the agent directly on Mtarget. In other words, the algorithm (the simulator who selects and trains the target) on the task."}, {"heading": "5 Active Simulators for Online Curriculum Learning", "text": "In the domain agnostic setting, online curriculum building is based on the task specific rewards accumulated and their transformations. In the domain aware setting, curricula are received using features that describe each of the tasks, and (e) Active ReMaximizing Selector, (d) Active ReMaximizing Selector, and (e) Active Local Transfer Maximizing Selector. Among them are the first three domain agnostic and the two are domain aware Methods.5.0.1 Binary Curriculum SelectorIn this method (see Algorithm 1), the agent is assigned tasks on the past is less."}, {"heading": "6 Experiments", "text": "We have reviewed the effectiveness of our methods in two areas: (a) a labyrinth environment [1], and (b) a network world environment (see Figure 1). We have found that the curricula chosen by our approaches do indeed outperform the alternative (training on the target) in all cases. The reward for achieving the target is + 1. We have placed the discount factor on the target task to increase by 0.9 and the learning rate (\u03b1) to reach the agent's starting position. The reward for achieving the target is + 1."}, {"heading": "7 Conclusion", "text": "In this paper, we have defined the problem of online learning of curricula in order to accelerate the learning of learning tools to reinforce a target task. We have developed several algorithms for two broad areas: firstly, no task information is available, and secondly, task-dependent functions are available. Our algorithms have been able to select curricula and train learning tools to reinforce more efficiently than training agents directly at the target tasks for two different areas. Statistical and computational guarantees for the proposed algorithms remain for future work."}], "references": [{"title": "Vision-based behavior acquisition for a shooting robot by using a reinforcement learning", "author": ["M. Asada", "S. Noda", "S. Tawaratsumida", "K. Hosoda"], "venue": "Proc. of IAPR/IEEE Workshop on Visual Behaviors, pages 112\u2013118. Citeseer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Successor features for transfer in reinforcement learning", "author": ["A. Barreto", "R. Munos", "T. Schaul", "D. Silver"], "venue": "arXiv preprint arXiv:1606.05312", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 41\u201348. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Transferring task models in reinforcement learning agents", "author": ["A. Fachantidis", "I. Partalas", "G. Tsoumakas", "I. Vlahavas"], "venue": "Neurocomputing, 107:23\u201332", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic policy reuse in a reinforcement learning agent", "author": ["F. Fern\u00e1ndez", "M. Veloso"], "venue": "Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 720\u2013727. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "course materials for 6.867 machine learning, fall 2006. mit opencourseware", "author": ["T. Jaakkola"], "venue": "(http://ocw.mit.edu/) Massachusetts Institute of Technology. Downloaded on", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Transfer in reinforcement learning: a framework and a survey", "author": ["A. Lazaric"], "venue": "Reinforcement Learning, pages 143\u2013173. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer of samples in batch reinforcement learning", "author": ["A. Lazaric", "M. Restelli", "A. Bonarini"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 544\u2013551. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "The teaching dimension of linear learners", "author": ["J. Liu", "X. Zhu"], "venue": "Journal of Machine Learning Research, 17(162):1\u201325", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Source task creation for curriculum learning", "author": ["S. Narvekar", "J. Sinapov", "M. Leonetti", "P. Stone"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 566\u2013574. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Smart task orderings for active online multitask learning", "author": ["P. Pang", "J. An", "J. Zhao", "X. Li", "T. Ban", "D. Inoue", "H. Sarrafzadeh"], "venue": "Proceedings of SIAM International Conference on Data Mining", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Curriculum learning of multiple tasks", "author": ["A. Pentina", "V. Sharmanska", "C.H. Lampert"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5492\u2013 5500", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Active task selection for lifelong machine learning", "author": ["P. Ruvolo", "E. Eaton"], "venue": "AAAI", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning inter-task transferability in the absence of target task samples", "author": ["J. Sinapov", "S. Narvekar", "M. Leonetti", "P. Stone"], "venue": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 725\u2013733. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Autonomous transfer for reinforcement learning", "author": ["M.E. Taylor", "G. Kuhlmann", "P. Stone"], "venue": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 1, pages 283\u2013290. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Behavior transfer for value-function-based reinforcement learning", "author": ["M.E. Taylor", "P. Stone"], "venue": "Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 53\u201359. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research, 10(Jul):1633\u20131685", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "An introduction to intertask transfer for reinforcement learning", "author": ["M.E. Taylor", "P. Stone"], "venue": "AI Magazine, 32(1):15", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Transfer via inter-task mappings in policy search reinforcement learning", "author": ["M.E. Taylor", "S. Whiteson", "P. Stone"], "venue": "Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems, page 37. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-task reinforcement learning: a hierarchical bayesian approach", "author": ["A. Wilson", "A. Fern", "S. Ray", "P. Tadepalli"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 1015\u20131022. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 16, "context": "In transfer reinforcement learning (RL), the knowledge obtained from training on a source task can be leveraged to learn a target task more efficiently [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 9, "context": "[10] present preliminary attempts in direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Though in [10], the authors", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "In the latter case, such domain information can be encoded in various ways (for instance, in [10], the authors are able to design a parametric model for tasks using domain knowledge).", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Thus our methods can work with existing RL algorithms and transfer methods introduced in the literature [4, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 6, "context": "Thus our methods can work with existing RL algorithms and transfer methods introduced in the literature [4, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 2, "context": "In [3], the authors showed that learning using a curriculum has an effect on the rate of convergence of the prediction model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "In [12], the authors proposed an approach that processes multiple supervised learning tasks in a sequence and finds the best curriculum of tasks to be learned.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "This is analogous to our reinforcement learning setting, although the emphasis in [12] is on empirical risk minimization rather than training time.", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "Active learning methods have also been used to find a curriculum of supervised learning tasks in the lifelong machine learning setting [13, 11].", "startOffset": 135, "endOffset": 143}, {"referenceID": 10, "context": "Active learning methods have also been used to find a curriculum of supervised learning tasks in the lifelong machine learning setting [13, 11].", "startOffset": 135, "endOffset": 143}, {"referenceID": 8, "context": "Analogous to our work is that of [9], wherein the authors define a student-teacher setting for supervised learning.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Transfer in reinforcement learning addresses the problem of designing schemes for efficient knowledge transfer from a given source task to a given target task [2, 18, 15].", "startOffset": 159, "endOffset": 170}, {"referenceID": 17, "context": "Transfer in reinforcement learning addresses the problem of designing schemes for efficient knowledge transfer from a given source task to a given target task [2, 18, 15].", "startOffset": 159, "endOffset": 170}, {"referenceID": 14, "context": "Transfer in reinforcement learning addresses the problem of designing schemes for efficient knowledge transfer from a given source task to a given target task [2, 18, 15].", "startOffset": 159, "endOffset": 170}, {"referenceID": 19, "context": "Certain recent works in multi-task reinforcement learning look at the problem of task selection where the agent is presented with a set of tasks to learn from [20].", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "In [14], the authors presented an offline method to find a curriculum by computing transfer measure for each pair of training tasks to form an inter-task transferability model.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "This method of ours builds a model similar to that in [14] while benefiting from reduced", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "Our work complements the work in [10], which present methods to create a set of training tasks which are related to a target task.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 170, "endOffset": 173}, {"referenceID": 15, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 197, "endOffset": 201}, {"referenceID": 18, "context": "In the literature, there are ways to facilitate such a transfer even if the state and action spaces of the two tasks are distinct, as long as there is a cross-task mapping between them [19].", "startOffset": 185, "endOffset": 189}, {"referenceID": 5, "context": "In the active learning variant of this problem [6], we sequentially select training examples to minimize the estimation error.", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "This method is in principle an offline method similar to [14] but without taking domain knowledge in consideration.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "We verified the effectiveness of our methods on two domains: (a) a maze environment [1], and (b) a grid world environment (see Figure 1).", "startOffset": 84, "endOffset": 87}, {"referenceID": 9, "context": "Training Tasks: We construct training tasks for the above two target tasks using methods similar to [10] (see Figure 2).", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Domain Aware: It can be seen in Figure 3 that the Local Transfer Maximizing Selector [14]", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "In this work, we propose several online methods to build a learning curriculum from a given set of target-task-specific training tasks in order to speed up reinforcement learning (RL). These methods can decrease the total training time needed by an RL agent compared to training on the target task from scratch. Unlike traditional transfer learning, we consider creating a sequence from several training tasks in order to provide the most benefit in terms of reducing the total time to train. Our methods utilize the learning trajectory of the agent on the curriculum tasks seen so far to decide which tasks to train on next. An attractive feature of our methods is that they are weakly coupled to the choice of the RL algorithm as well as the transfer learning method. Further, when there is domain information available, our methods can incorporate such knowledge to further speed up the learning. We experimentally show that these methods can be used to obtain suitable learning curricula that speed up the overall training time on two different domains.", "creator": "LaTeX with hyperref package"}}}