{"id": "1703.07872", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Random Features for Compositional Kernels", "abstract": "We describe and analyze a simple random feature scheme (RFS) from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of convolutional neural networks and kernels. The resulting scheme yields sparse and efficiently computable features. Each random feature can be represented as an algebraic expression over a small number of (random) paths in a composition tree. Thus, compositional random features can be stored compactly. The discrete nature of the generation process enables de-duplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. Our approach complements and can be combined with previous random feature schemes.", "histories": [["v1", "Wed, 22 Mar 2017 22:05:04 GMT  (92kb,D)", "http://arxiv.org/abs/1703.07872v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "roy frostig", "vineet gupta", "yoram singer"], "accepted": false, "id": "1703.07872"}, "pdf": {"name": "1703.07872.pdf", "metadata": {"source": "CRF", "title": "Random Features for Compositional Kernels", "authors": ["Amit Daniely", "Roy Frostig", "Vineet Gupta", "Yoram Singer"], "emails": ["singer}@google.com"], "sections": [{"heading": null, "text": "* Google Brain, {amitdaniely, frosty, vineet, singer} @ google.comar Xiv: 170 3.07 872v 1 [cs.L G] 22 Mar 2"}, {"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he said the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"the\" New York Times, the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\""}, {"heading": "2 Background and notation", "text": "We start with a few notational conventions that are used throughout the work. < z, z, z \u00b2 > = Re (zz, z \u00b2) = aa, z \u00b2 (instead of the more common < z, z \u00b2 > = zz \u00b2) Likewise, for z, z \u00b2, Cq < z, z \u2032 > = q qi = 1 < zi, z \u00b2 i >. For a measurement space (x, \u00b5), L2 denotes the space of square integrable functions f:."}, {"heading": "3 Random feature schemes", "text": "Let X be a measurable space and let k: X \u00b7 X \u00b7 R a normed q = q functions. A random feature (RFS) for k is a pair (1) in which \u00b5 is a probability size in a measurable space. < < / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "4 Random feature schemes for basic spaces", "text": "To apply theorem 5, we need to control the limitation of the generated characteristics =. Consider the RFS generation method given in algorithm 1, which uses multiplications of characteristics generated by basic RFSs. If every basic RFS characteristic is C-limited, then every characteristic that is a multiplication of the basic characteristics is Ct-bounded. Against this background, we would first like to describe such RFSs for the basic spaces whose norm is as small as possible. Indeed, the best we can hope for RFSs is RFSs with norm of 1. We first describe such RFSs for multiple kernels, including the Gaussian kernel on Rd, and the inner standard product on S0 and S1. Then we discuss the inner standard product on Sd \u2212 1 for d-3. In this case, we show that the smallest possible norm is for an RFS."}, {"heading": "5 Compositional random feature schemes", "text": "Therefore it will be useful, RFSs for multiplications and average values of cores to have (cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf... cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf."}, {"heading": "6 Empirical Evaluation", "text": "It is a question of whether we are able to generate some practical advantages of our design, which are illustrated in the following experiments. We use the CIFAR-10 dataset for visual object recognition [11].We looked at two core structures, one flat and one deep. Figure 2: For visual clarity it simplifies the constellation and the original image to onedimensional object recognition. We looked at two core structures, one flat and one deep."}], "references": [{"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv:1412.8690,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "On the equivalence between quadrature rules and random features", "author": ["F.R. Bach"], "venue": "CoRR, abs/1502.06800,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Object recognition with hierarchical kernel descriptors", "author": ["L. Bo", "K. Lai", "X. Ren", "D. Fox"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729\u20131736. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel methods for deep learning", "author": ["Y. Cho", "L.K. Saul"], "venue": "Advances in neural information processing systems, pages 342\u2013350,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Sgd learns the conjugate kernel class of the network", "author": ["Amit Daniely"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The pyramid match kernel: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "Tenth IEEE International Conference on Computer Vision, volume 2, pages 1458\u20131465,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "arXiv:1201.6530,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks", "author": ["J. Mairal"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "Cordelia Schmid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Spherical random features for polynomial kernels", "author": ["J. Pennington", "F. Yu", "S. Kumar"], "venue": "Advances in Neural Information Processing Systems, pages 1837\u20131845,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, pages 1177\u20131184,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1089\u20131096,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Advances in Kernel Methods - Support Vector Learning", "author": ["B. Sch\u00f6lkopf", "C. Burges", "A. Smola", "editors"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Prior knowledge in support vector kernels", "author": ["B. Sch\u00f6lkopf", "P. Simard", "A. Smola", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems 10, pages 640\u2013646. MIT Press,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 21, "context": "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.", "startOffset": 60, "endOffset": 72}, {"referenceID": 20, "context": "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.", "startOffset": 60, "endOffset": 72}, {"referenceID": 17, "context": "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.", "startOffset": 60, "endOffset": 72}, {"referenceID": 18, "context": "A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9].", "startOffset": 144, "endOffset": 151}, {"referenceID": 8, "context": "A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9].", "startOffset": 144, "endOffset": 151}, {"referenceID": 3, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 2, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 13, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 12, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 15, "context": "Rahimi and Recht [16] described and analyzed an elegant and computationally effective way that mitigates this problem by generating random features that approximate certain kernels.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 14, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 1, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 0, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 5, "context": "The kernels\u2019 definition and the connection to networks was developed in [6, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 4, "context": "The kernels\u2019 definition and the connection to networks was developed in [6, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 15, "context": "Indeed, the base kernels of a compositional kernel can be non-elementary such as the Gaussian kernel, and hence our RFS can be used in conjunction with the well-studied RFS of [16] for Gaussian kernels.", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "8 in [20]) that for every f \u2208 H k, E S LD(f\u0302) \u2264 LD (f) + \u03bb\u2016f?\u20162Ht k + 2\u03c1 \u03bbm .", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Then, \u03c8 is a norm-efficient RFS, as implied by [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "An internal node v is associated with a PSD function (called a conjugate activation function [6][Sec.", "startOffset": 93, "endOffset": 96}, {"referenceID": 15, "context": "This is in contrast to the Rahimi and Recht scheme [16], in which the cost is linear in n.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "We use the CIFAR-10 dataset for visual object recognition [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "[6], for a scalar function \u03c3 : R \u2192 R termed an activation, let \u03c3\u0302(\u03c1) = E(X,Y )\u223cN\u03c1 [\u03c3(X)\u03c3(Y )] be its conjugate activation (shown in the original to be a PSD function).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6], it corresponds to a singlelayer fully-connected skeleton, having a single internal node v to which all input nodes point.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In each setting, we compare to a natural baseline built (in part, where possible) on the scheme of Rahimi and Recht [16] for Gaussian kernels.", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "Each such node corresponds to a Gaussian kernel, and so we apply the scheme Rahimi and Recht [16] to approximate the kernel of that node.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "[6] establish between a compositional kernel and neural networks", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In the shallow setting, the baseline (RR) is the scheme of Rahimi and Recht [16] for Gaussian kernels.", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "We again use the CIFAR-10 dataset, with a standard data augmentation pipeline [12]: random 24x24 pixel crop, random horizontal flip, random brightness, saturation, and contrast delta, per-image whitening, and per-patch PCA.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "We generated 10 random features, and trained for 120 epochs with AdaGrad [7] and a manually tuned initial learning rate among {25, 50, 100, 200}.", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "We use the typical random Gaussian initialization [8], 200 epochs of AdaGrad with a manually tuned initial learning rate among {.", "startOffset": 50, "endOffset": 53}, {"referenceID": 16, "context": "[17], who final-layer training of a randomly initialized network to rank fully-trained networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], we know that the random network approach is an alternative random feature map for the compositional kernel.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We describe and analyze a simple random feature scheme (RFS) from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of convolutional neural networks and kernels. The resulting scheme yields sparse and efficiently computable features. Each random feature can be represented as an algebraic expression over a small number of (random) paths in a composition tree. Thus, compositional random features can be stored compactly. The discrete nature of the generation process enables de-duplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. Our approach complements and can be combined with previous random feature schemes. \u2217Google Brain, {amitdaniely, frostig, vineet, singer}@google.com ar X iv :1 70 3. 07 87 2v 1 [ cs .L G ] 2 2 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}