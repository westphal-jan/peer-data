{"id": "1703.07980", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Discriminatively Boosted Image Clustering with Fully Convolutional Auto-Encoders", "abstract": "Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft $k$-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.", "histories": [["v1", "Thu, 23 Mar 2017 09:49:37 GMT  (2983kb,D)", "http://arxiv.org/abs/1703.07980v1", "27 pages"]], "COMMENTS": "27 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["fengfu li", "hong qiao", "bo zhang", "xuanyang xi"], "accepted": false, "id": "1703.07980"}, "pdf": {"name": "1703.07980.pdf", "metadata": {"source": "CRF", "title": "Discriminatively Boosted Image Clustering with Fully Convolutional Auto-Encoders", "authors": ["Fengfu Li", "Hong Qiao", "Bo Zhang", "Xuanyang Xi"], "emails": [], "sections": [{"heading": null, "text": "Traditional image cluster methods take a two-step approach, where learning and clustering follow each other. However, recent research has shown that combining the separate phases in a single framework and joint training can achieve better performance. In this paper, we first propose fully revolutionary auto-encoders for learning image characteristics and then propose a single cluster framework to jointly learn image representations and cluster centers based on a fully revolutionary auto-encoder and soft k-mean values. In the early stages of the learning process, the representations extracted from the auto-encoder may not be particularly discriminatory for the latter cluster formation. We address this problem by applying an enhanced discriminatory distribution, highlighting high scores and low scores."}, {"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Related work", "text": "In recent years, the number of those capable of skyrocketing has multiplied. Most of these methods involve a two-step training process [26], one is layer-by-layer pre-training and the other is general fine-tuning. One of the main shortcomings of this learning process is that layer-by-layer pre-training is time-consuming and lengthy, especially if the base layer is a layer-by-layer pre-training (RBM), and the other is that the entire network is deeply entrenched."}, {"heading": "3. Proposed methods", "text": "In this section, we propose a unified image clustering framework with fully revolutionary auto-encoders and a soft k-means clustering model (see Figure 1). The framework consists of two parts: Part I is a fully revolutionary auto-encoder (FCAE) for fast and rough image attribute extraction, and Part II is a discriminatively enhanced clustering (DBC) procedure consisting of a fully revolutionary encoder and a soft k-means categorizer. The DBC takes an image as input and exports soft assignments as output. It can be trained together with a discriminatively enhanced distribution assumption, making the deep representations learned more suitable for the top categorizer. Our idea is very similar to self-step learning [9], where simplest instances are focused first and more complex objects are gradually expanded. In the following sections, we will explain the detailed implementation of the idea."}, {"heading": "3.1. Fully convolutional auto-encoder for image feature extraction", "text": "This could be lengthy and time consuming when dealing with very deep neural networks. To address this problem, we propose a fully conventional auto-encoder architecture that can be trained in an end-to-end manner. Part I of Figure 1 shows an example of FCAE on the MNIST dataset. It has the following features: Fully conventional As in [27], the max-pooling layers are very important for learning biologically plausible features in conventional architectures. Thus, we adopt confrontation layers along with the max-pooling layers to make a fully conventional encoder (FCE). Since the down-sampling operations in the FCE reduce the size of the output feature cards, we use a non-uniform layer to restore the functional layers."}, {"heading": "3.2. Discriminatively boosted clustering", "text": "This strategy is used in many clustering methods based on auto-encoders, such as GraphEncoder [18], deep embedding networks [35], and auto-encoder-based clustering [36]. These approaches treat the auto-encoder as a pre-processing step designed separately from the latter clustering step. However, the representations learned in this way could be amphibolous for clustering, and the clusters may be unclear (see the initial phase in Fig. 2). To address this problem, we propose a self-determined approach to make feature learning and clustering within a unified framework (see Part II in Fig. 1). We discard the decoder of FACE and add a soft k-mean model at the feature level."}, {"heading": "3.2.2. Boosting easiness with discriminative target distribution", "text": "We transform the more difficult examples into the simpler ones by increasing the higher scores and, in the meantime, bringing down those with lower scores. This can be achieved by constructing an underlying target distribution in such a way that it runs as follows: rij, s\u03b1ij, \u03b1 > 1 (2) s.t. k, j, j = 1 rij = 1Suppose, ideally, we can learn from the soft values (referred to as S) to the assumed distribution (referred to as R) each time. Then, we can create a learning chain as follows: S (0) \u2192 R (0) = S (1) \u2192 R (1) = S (2) = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7.The following two properties can be observed from the chain: Property 1 If s (0) ij = s we have for all j and j, then s (t) ij, ij, then ij that we are equal for all j."}, {"heading": "3.2.3. Learning with the Kullback-Leibler divergence loss", "text": "In the last subsection, it was assumed that we could learn from sij = the increased target distribution rij. This goal can be achieved with a common Kullback-Leibler (KL) divergence loss. (3) Figure 3 gives an example of the common loss if k = 2, where Lij = rij log (rij / sij) is the loss generated by the xi sample in relation to the jten cluster (j = 1 or 2). Registers marked in Fig. 3 roughly correspond to the regions shown in Fig. 2. Intuitively, the loss has the following main characteristics: \u2022 For an ambiguous (or hard) sample (i.e., sij \u2012 sil, l, l), its loss will be zi = 1 rij sij."}, {"heading": "3.2.4. Training algorithm", "text": "In this section, we summarize the general training procedure of the proposed method in Algorithm 1 and Algorithm 2. They implement the framework presented in Figure 1. Here, T is the maximum learning poche, B is the maximum updating iteration in each epoch, and mb is the minibatch size. The encoder part of FCAE is f: x \u00b7 \u2192 z, which is parametrized by \u03b8e, and the decoder part of FCAE is g: z \u2192 x, which is parametrized by Success 1 Discriminally Boosted Clustering (DBC) Required: X, T, B, mb, \u03b1, k Secure: \u03b8 and \u00b5 / / / Stage I: Train a FCAE \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 and Clustering with its features 1: Train a deep full convolutional auto-encoderxi \u0445e \u2212 \u2192 zi (features) \u0445d \u2212 \u2192 x \u00b7 \u00b7 \u00b7 \u00b7 i (M1) with the euclidean loss (conditional conditional conditional conditional e) and clustering with its features \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1: Train a deep, complete encoded car."}, {"heading": "4. Experiments", "text": "In this section, we will present experimental results on several real data sets to evaluate the proposed methods by comparing them with several state-of-the-art methods. To this end, we will first present several evaluation benchmarks and then present visualization results of the internal characteristics, the learned FCAE weights, the frequency of soft assignments during the learning process, and the characteristics embedded in a low-dimensional space. In addition, we will perform some ablation studies on the boosting factor \u03b1, the normalization factor nj, and the FCAE initializations."}, {"heading": "4.1. Evaluation benchmarks", "text": "The size of the records, 1http: / / yann.lecun.com / exdb / mnist / 2http: / / www.cs.columbia.edu / roweis / data.html 3http: / / www.cs.columbia.edu / software / softlib / coil-20.php 4http: / / www.cs.columbia.edu / software / softlib / coil-algorithm 2 DBC (Continued) / / Stage II: Jointly learn the FCE and cluster centers 4: Construct a unified clustering model with encoder ameters."}, {"heading": "4.2. Visualization", "text": "One of the advantages of fully revolutionary neural networks is that we can visualize the inner activations (or features) and the trained weights (or filters) in a two-dimensional space in a natural way [27]. We can also monitor the learning process of DBC by plotting frequency histories of assignment values. In addition, t-SNE can be applied to the embedded features to visualize the multiple structures in a low-dimensional space. Finally, we show some typical miscategorized samples generated by our algorithm."}, {"heading": "4.2.1. Visualization of the inner activations and learned filters", "text": "In Figure 4, we illustrate the inner activations of FCAE on the three-digit MNIST dataset: 1, 5, and 9. As shown in the figure, the activations at the feature level are very sparse. Furthermore, the deconvolution layer gradually restores details of the pooled feature maps and finally provides a rough description of the original image. This indicates that FCAE can learn cluster-friendly properties and retain key information for image reconstruction. Figure 5 visualizes the learned FCAE filters on the MNIST dataset. However, in [27] it is observed that the stacked Convolutional Auto encoders trained on noisy inputs (30% binary noise) and a max pooling layer can learn localized biologically plausible filters."}, {"heading": "4.2.2. Monitoring the learning process", "text": "Fig. 6 shows the results of the MNIST test data set (a subset of the MNIST data set with 10,000 samples), which are assigned to the first cluster at different learning epochs. In early epochs (t \u2264 4), most values are close to 0.1. This is a random guess probability, since there are 10 clusters. As the learning process progresses, some samples with higher results are increased discriminatively and their values become larger than others. As a result, the cluster tends to \"believe\" in these samples with higher values and thus reduce the values of the others (approximately zero). Finally, the values assigned to the cluster are polarized on two sides. Samples with very high values (sij \u2248 0.8) are counted as definitely belonging to the first cluster and the others with very small values (sij \u2248 0.02) should belong to other clusters."}, {"heading": "4.2.3. Embedding learned features in a low dimensional space", "text": "Figure 7 shows the distribution of learned characteristics in a two-dimensional space with t-SNE [37]. Figure 7 shows the embedded characteristics of the MNIST test data set at different epochs. In the initial epoch, the characteristics learned with FCAE are not particularly discriminatory for clustering. As shown in Figure 7 (a), the characteristics of the numbers 3, 5, and 8 are closely related. The same happened with the numbers 4, 7, and 9. In the second epoch, the distribution of learned characteristics becomes much more compact locally. Furthermore, the characteristics of the number 7 are far removed from those of the numbers 4 and 9. Similarly, the characteristics of the numbers 8 are far removed from those of the numbers 3 and 5. As the learning process progresses, the hardest digits (4 v.s. 9, 3 v.s. 5) for categorization are mostly classified according to sufficiently discriminatory boosting categories."}, {"heading": "4.2.4. Visualization of falsely categorized examples", "text": "In Fig. 8, we show the top 100 incorrectly categorized examples whose maximum soft-assignment values are above 0.6. It can be observed that it is very difficult to distinguish between some ground truth numbers 4, 7 and 9, even without human experience. Many numbers 7 are written with horizontal slashes in their center space and would be considered ambiguous for the clustering algorithm. In addition, some ground truth images themselves are confusing, such as those with a gray background."}, {"heading": "4.3. Discussions", "text": "In this section, we perform some ablation studies on the learning process with respect to different boosting factors (\u03b1), different normalization methods (nj), and different initialization models generated by FCAE."}, {"heading": "4.3.1. Impact of the boosting factor \u03b1", "text": "Fig. 9 (a) shows the ACC and NMI curves, where \u03b1 1.5, 2, 4 correspond. At a small \u03b1 (\u03b1 = 1.5), the learning process is very slow and takes a very long time to complete. On the contrary, if the factor is to be very large (\u03b1 = 4), the learning process is very fast in the initial phase. However, this could lead to some values of the ambiguous samples being incorrectly raised. As a result, the model has learned too much from some incorrect information, so the performance is not as satisfactory. At a moderate increase factor (\u03b1 = 2), the ACC and NMI curves grow reasonably and progressively."}, {"heading": "4.3.2. Impact of the balance normalization", "text": "In DEC [19], the authors pointed out that the normalization of equilibrium plays an important role in preventing large clusters from distorting the hidden attribute space. To solve this problem, we compare three normalization strategies: 1) constant normalization for comparison, i.e. nj = 1, 2) normalization by dividing the sum of the original Soft Assignment Score per cluster, i.e. nj = \u2211 i sij adopted in DEC, and 3) normalization by dividing the sum of the increased Soft Assignment Score per cluster, i.e. nj = \u2211 i s \u03b1 ij. Figure 9 (b) shows the value curves of ACC and NMI against the epoch with these settings. Originally, normalization did not have much influence on ACC and NMI. However, constant normalization can easily get stuck in early stages."}, {"heading": "4.3.3. Impact of the FCAE initialization", "text": "To investigate the effects of FCAE initialization on DBC, we compare the performance of DBC with three different initialization models: 1) random initialization, 2) initialization with a semi-trained FCAE model, and 3) initialization with a sufficiently trained FCAE model. Comparison results are shown in Figure 9 (c). As shown in the figure, DBC performs very well based on all models even if the initialization model is randomly distributed. However, if the FCAE model is not sufficiently trained, the resulting DBC model is suboptimal."}, {"heading": "5. Conclusions and future works", "text": "In this paper, we proposed FCAE and DBC, which deal with image learning and image clustering, respectively. Benchmarks on multiple visual datasets show that our methods can perform better than analog methods. In addition, the visualization shows that the proposed learning algorithm can implement the idea suggested in Section 3.2. Measures to be considered in the future include: 1) adding appropriate constraints for FCAE to deal with natural images, and 2) scaling the algorithm to deal with large-scale datasets such as the ImageNet dataset."}, {"heading": "Acknowledgement", "text": "This work was partially supported by the NNSF of China under grants 61379093, 61602483 and 61603389. We thank Shuguang Ding, Xuanyang Xi, Lu Qi and Yanfeng Lu for valuable discussions."}, {"heading": "A. Derivation of (4).", "text": "We use the chain rule for deduction. (0,2) Now follows setqij = u \u2212 1 + v 2 ij, (0,3) so \u2202 qij \u2202 zi = \u2202 qij \u2202 zi = (\u2212 1 + v 2) u \u2212 3 + v 2 ij \u00b7 2 v (zi \u2212 \u00b5j) = (\u2212 1 + v) u \u2212 1ij qij \u00b7 (zi \u2212 \u00b5j). (0,4) Furthermore, we have letsij = qij \u00b2 qij \u2032 qij \u2032. (0,5) Then we have taj sij zi = qij \u00b2 qij \u00b2 qij \u00b2 (1 + 1) qij \u00b2 (1 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (1)."}, {"heading": "B. Derivation of (5).", "text": "(5) can similarly be derived by replacing \u00b5 and z in the above derivatives of (4)."}], "references": [{"title": "Data Mining: Concepts and Techniques", "author": ["J. Han", "J. Pei", "M. Kamber"], "venue": "Elsevier,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of clustering data mining techniques", "author": ["P. Berkhin"], "venue": "In: Grouping Multidimensional Data, pp. 25-71,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Randomized dimensionality reduction for k-means clustering", "author": ["C. Boutsidis", "A. Zouzias", "M. Mahaoney", "P. Drineas"], "venue": "IEEE Transactions on Information Theory, vol. 61, no. 2, pp. 1045-1062,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888-905,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.P. Kriegel", "J. Sander", "X. Xu"], "venue": "KDD, vol. 96, no. 34, pp. 226-231,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Clustering by fast search and find of density peaks", "author": ["A. Rodriguez", "A. Laio"], "venue": "Science, vol. 344, no. 6191, pp. 1492-1496,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective deterministic initialization for kmeans-like methods via local density peaks searching", "author": ["F. Li", "H. Qiao", "B. Zhang"], "venue": "arXiv:1611.06777,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Recent review on image clustering", "author": ["N. Ahmed"], "venue": "IET Image Processing, vol. 9, no. 11, pp. 1020-1032,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning the easy things first: Self-paced visual category discovery", "author": ["Y.J. Lee", "K. Grauman"], "venue": "IEEE Conference on CVPR, pp. 1721-1728,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive dimension reduction using discriminant analysis and k-means clustering", "author": ["C. Ding", "T. Li"], "venue": "Proc. 24th International Conference on Machine learning, pp. 521-528,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Object recognition from local scale-invariant features", "author": ["D. Lowe"], "venue": "Proc. 7th International Conference on Computer Vision, vol. 2, pp. 1150-1157,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. Computer Vision and Pattern Recognition, vol. 1, pp. 886-893,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint image clustering and labeling by matrix factorization", "author": ["S. Hong", "J. Choi", "J. Feyereisl", "B. Han", "L.S. Davis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 7, pp. 1411-1424,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Complex wavelet structural similarity: A new image similarity index", "author": ["M. Sampat", "Z. Wang", "S. Gupta", "A. Bovik", "M. Markey"], "venue": "IEEE Transactions on Image Processing, vol. 18, no. 1, pp. 2385-2401,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A new manifold distance for visual object categorization", "author": ["F. Li", "X. Huang", "H. Qiao", "B. Zhang"], "venue": "The 12th World Congress on Intelligent Control and Automation, pp. 2232-2236,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advanced Neural Information Processing Systems, vol. 24, pp. 1097-1105,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527-1554,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deep representations for graph clustering", "author": ["F. Tian", "B. Gao", "Q. Cui", "E. Chen", "T. Liu"], "venue": "AAAI, pp. 1293-1299,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised deep embedding for clustering analysis", "author": ["J. Xie", "R. Girshick", "A. Farhadi"], "venue": "Proc. 33rd International Conference on Machine Learning, pp. 478-487,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint unsupervised learning of deep representations and image clusters", "author": ["J. Yang", "D. Parikh", "D. Batra"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Infinite ensemble for image clustering", "author": ["H. Liu", "M. Shao", "S. Li", "Y. Fu"], "venue": "Proc. 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 1745-1754,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked denoising auto-encoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371-3408, Dec.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["P. Baldi"], "venue": "ICML Workshop on Unsupervised and Transfer Learning, vol. 27, pp. 37-50,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1789-1828,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504-507,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, vol. 19, pp. 153,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. ciresan", "J. Schmidhuber"], "venue": "International Conference on Artificial Neural Networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A. Ng"], "venue": "Proc. 26th Annual International Conference on Machine Learning, pp. 609-616,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proc. IEEE International Conference on Computer Vision, pp. 1520-1528,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision, pp. 818-833,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep deconvolutional networks for scene parsing", "author": ["R. Mohan"], "venue": "arXiv:1411.4101,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M. Zeiler", "G. Taylor", "R. Fergus"], "venue": "2011 International Conference on Computer Vision, pp. 2018-2025,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep embedding network for clustering", "author": ["P. Huang", "Y. Huang", "W. Wang", "L. Wang"], "venue": "ICPR, pp. 1532-1537,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoder based data clustering", "author": ["C. Song", "F. Liu", "Y. Huang"], "venue": "Iberoamerican Congress on Pattern Recognition,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Visualizing data using t-SNE", "author": ["L. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2579-2605,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "The Hungarian method for the assignment problem", "author": ["H. Kuhn"], "venue": "50 Years of Integer Programming 1958-2008, pp. 29-47,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Locally consistent concept factorization for document clustering", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 23, no. 6, pp. 902-913,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards kmeans-friendly spaces: simultaneous deep learning and clustering", "author": ["B. Yang", "X. Fu", "ND Sidiropoulos", "M Hong"], "venue": "arXiv:1610.04794,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Clustering methods are very important techniques for exploratory data analysis with wide applications ranging from data mining [1, 2], dimension reduction [3], segmentation [4] and so on.", "startOffset": 127, "endOffset": 133}, {"referenceID": 1, "context": "Clustering methods are very important techniques for exploratory data analysis with wide applications ranging from data mining [1, 2], dimension reduction [3], segmentation [4] and so on.", "startOffset": 127, "endOffset": 133}, {"referenceID": 2, "context": "Clustering methods are very important techniques for exploratory data analysis with wide applications ranging from data mining [1, 2], dimension reduction [3], segmentation [4] and so on.", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "Clustering methods are very important techniques for exploratory data analysis with wide applications ranging from data mining [1, 2], dimension reduction [3], segmentation [4] and so on.", "startOffset": 173, "endOffset": 176}, {"referenceID": 4, "context": "Approaches to achieve this aim include partitional methods such as k-means and k-medoids, hierarchical methods like agglomerative clustering and divisive clustering, methods based on density estimation such as DBSCAN [5], and recent methods based on finding density peaks such as CFSFDP [6] and LDPS [7].", "startOffset": 217, "endOffset": 220}, {"referenceID": 5, "context": "Approaches to achieve this aim include partitional methods such as k-means and k-medoids, hierarchical methods like agglomerative clustering and divisive clustering, methods based on density estimation such as DBSCAN [5], and recent methods based on finding density peaks such as CFSFDP [6] and LDPS [7].", "startOffset": 287, "endOffset": 290}, {"referenceID": 6, "context": "Approaches to achieve this aim include partitional methods such as k-means and k-medoids, hierarchical methods like agglomerative clustering and divisive clustering, methods based on density estimation such as DBSCAN [5], and recent methods based on finding density peaks such as CFSFDP [6] and LDPS [7].", "startOffset": 300, "endOffset": 303}, {"referenceID": 7, "context": "Image clustering [8] is a special case of clustering analysis that seeks to find compact, object-level models from many unlabeled images.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "Its applications include automatic visual concept discovery [9], content-based image retrieval and image annotation.", "startOffset": 60, "endOffset": 63}, {"referenceID": 9, "context": "However, image clustering is a hard task mainly owning to the following two reasons: 1) images often are of high dimensionality, which will significantly affect the performance of clustering methods such as k-means [10], and 2) objects in images usually have twodimensional or three-dimensional local structures which should not be ignored when exploring the local structure information of the images.", "startOffset": 215, "endOffset": 219}, {"referenceID": 10, "context": "Traditionally, various hand-crafted features such as SIFT [11], HOG [12], NMF [13], and (geometric) CW-SSIM similarity [14, 15] have been used to encode the visual information.", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "Traditionally, various hand-crafted features such as SIFT [11], HOG [12], NMF [13], and (geometric) CW-SSIM similarity [14, 15] have been used to encode the visual information.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Traditionally, various hand-crafted features such as SIFT [11], HOG [12], NMF [13], and (geometric) CW-SSIM similarity [14, 15] have been used to encode the visual information.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "Traditionally, various hand-crafted features such as SIFT [11], HOG [12], NMF [13], and (geometric) CW-SSIM similarity [14, 15] have been used to encode the visual information.", "startOffset": 119, "endOffset": 127}, {"referenceID": 14, "context": "Traditionally, various hand-crafted features such as SIFT [11], HOG [12], NMF [13], and (geometric) CW-SSIM similarity [14, 15] have been used to encode the visual information.", "startOffset": 119, "endOffset": 127}, {"referenceID": 15, "context": "Recently, many approaches have been proposed to combine clustering methods with deep neural networks (DNN), which have shown a remarkable performance improvement over hand-crafted features [16].", "startOffset": 189, "endOffset": 193}, {"referenceID": 16, "context": "In the first group, a kind of deep (convolutional) neural networks, such as deep belief network (DBN) [17] and stacked auto-encoders [18], is first trained in an unsupervised manner to approximate the non-linear feature embedding from the raw image space to the embedded feature space (usually being low-dimensional).", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "In the first group, a kind of deep (convolutional) neural networks, such as deep belief network (DBN) [17] and stacked auto-encoders [18], is first trained in an unsupervised manner to approximate the non-linear feature embedding from the raw image space to the embedded feature space (usually being low-dimensional).", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "In [19], the authors proposed deep embedded clustering that simultaneously learns feature representations with stacked auto-encoders and cluster assignments with soft k-means by minimizing a joint loss function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "In [20], joint unsupervised learning was proposed to learn deep convolutional representations and agglomerative clustering jointly using a recurrent framework.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [21], the authors proposed an infinite ensemble clustering framework that integrates deep representation learning and ensemble clustering.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Related work Stacked auto-encoders [22, 23, 24, 17, 25, 26] have been studied in the past years for unsupervised deep feature extraction and nonlinear dimension reduction.", "startOffset": 35, "endOffset": 59}, {"referenceID": 22, "context": "Related work Stacked auto-encoders [22, 23, 24, 17, 25, 26] have been studied in the past years for unsupervised deep feature extraction and nonlinear dimension reduction.", "startOffset": 35, "endOffset": 59}, {"referenceID": 23, "context": "Related work Stacked auto-encoders [22, 23, 24, 17, 25, 26] have been studied in the past years for unsupervised deep feature extraction and nonlinear dimension reduction.", "startOffset": 35, "endOffset": 59}, {"referenceID": 16, "context": "Related work Stacked auto-encoders [22, 23, 24, 17, 25, 26] have been studied in the past years for unsupervised deep feature extraction and nonlinear dimension reduction.", "startOffset": 35, "endOffset": 59}, {"referenceID": 24, "context": "Related work Stacked auto-encoders [22, 23, 24, 17, 25, 26] have been studied in the past years for unsupervised deep feature extraction and nonlinear dimension reduction.", "startOffset": 35, "endOffset": 59}, {"referenceID": 25, "context": "Related work Stacked auto-encoders [22, 23, 24, 17, 25, 26] have been studied in the past years for unsupervised deep feature extraction and nonlinear dimension reduction.", "startOffset": 35, "endOffset": 59}, {"referenceID": 26, "context": "Their extensions for dealing with images are convolutional stacked auto-encoders [27, 28].", "startOffset": 81, "endOffset": 89}, {"referenceID": 27, "context": "Their extensions for dealing with images are convolutional stacked auto-encoders [27, 28].", "startOffset": 81, "endOffset": 89}, {"referenceID": 25, "context": "Most of these methods contain a two-stage training procedure [26]: one is layer-wise pre-training and the other is overall finetuning.", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "In [29], a deep deconvolution network is learned for image segmentation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "The network achieves the state-of-the-art performance compared with analogous methods thanks to three factors: 1) introducing a deconvolution layer and a unpooling layer [30, 31, 32] to recover the original image size of the segmentation mask, 2) applying the batch normalization [33] to each convolution layer and each deconvolution layer to reduce the internal covariate", "startOffset": 170, "endOffset": 182}, {"referenceID": 30, "context": "The network achieves the state-of-the-art performance compared with analogous methods thanks to three factors: 1) introducing a deconvolution layer and a unpooling layer [30, 31, 32] to recover the original image size of the segmentation mask, 2) applying the batch normalization [33] to each convolution layer and each deconvolution layer to reduce the internal covariate", "startOffset": 170, "endOffset": 182}, {"referenceID": 31, "context": "The network achieves the state-of-the-art performance compared with analogous methods thanks to three factors: 1) introducing a deconvolution layer and a unpooling layer [30, 31, 32] to recover the original image size of the segmentation mask, 2) applying the batch normalization [33] to each convolution layer and each deconvolution layer to reduce the internal covariate", "startOffset": 170, "endOffset": 182}, {"referenceID": 32, "context": "The network achieves the state-of-the-art performance compared with analogous methods thanks to three factors: 1) introducing a deconvolution layer and a unpooling layer [30, 31, 32] to recover the original image size of the segmentation mask, 2) applying the batch normalization [33] to each convolution layer and each deconvolution layer to reduce the internal covariate", "startOffset": 280, "endOffset": 284}, {"referenceID": 33, "context": "shifts, which not only makes an end-to-end training procedure possible but also speeds up the process, and 3) adopting a pre-trained encoder on largescale datasets such as VGG-16 model [34].", "startOffset": 185, "endOffset": 189}, {"referenceID": 9, "context": "[10, 18, 35, 36]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 17, "context": "[10, 18, 35, 36]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 34, "context": "[10, 18, 35, 36]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 35, "context": "[10, 18, 35, 36]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 18, "context": "Recently, there are attempts to combine the auto-encoders and clustering in a unified framework [19, 40].", "startOffset": 96, "endOffset": 104}, {"referenceID": 39, "context": "Recently, there are attempts to combine the auto-encoders and clustering in a unified framework [19, 40].", "startOffset": 96, "endOffset": 104}, {"referenceID": 18, "context": "In [19], the authors proposed Deep Embedded Clustering (DEC) that learns deep representations and cluster assignments jointly.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [40], the authors proposed Deep Clustering Network (DCN), a joint dimensional reduction and k-means clustering framework.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "Our idea is very similar to self-paces learning [9], where easiest instances are first focused and more complex objects are expanded progressively.", "startOffset": 48, "endOffset": 51}, {"referenceID": 26, "context": "Fully Convolutional As pointed out in [27], the max-pooling layers are very crucial for learning biologically plausible features in the convolutional architectures.", "startOffset": 38, "endOffset": 42}, {"referenceID": 28, "context": "Since the down-sampling operations in the FCE reduce the size of the output feature maps, we use an unpooling layer introduced in [29] to recover the feature maps.", "startOffset": 130, "endOffset": 134}, {"referenceID": 28, "context": "As a result, the unpooling layers along with deconvolution layers (see [29]) are adopted to make a fully convolutional decoder (FCD).", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "To overcome this problem, we adopt the batch normalization (BN) [33] strategy for reducing the internal covariate shift and speeding up the training.", "startOffset": 64, "endOffset": 68}, {"referenceID": 28, "context": "As pointed out in [29], BN is critical to optimize the fully convolutional neural networks.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "This strategy is used in many clustering methods based on auto-encoders, such as GraphEncoder [18], deep embedding networks [35], and auto-encoder based clustering [36].", "startOffset": 94, "endOffset": 98}, {"referenceID": 34, "context": "This strategy is used in many clustering methods based on auto-encoders, such as GraphEncoder [18], deep embedding networks [35], and auto-encoder based clustering [36].", "startOffset": 124, "endOffset": 128}, {"referenceID": 35, "context": "This strategy is used in many clustering methods based on auto-encoders, such as GraphEncoder [18], deep embedding networks [35], and auto-encoder based clustering [36].", "startOffset": 164, "endOffset": 168}, {"referenceID": 18, "context": "Easiness measurement with the soft k-means scores We follow DEC [19] to adopt the t-distribution-based soft assignment to measure the easiness of a sample.", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "The t-distribution is investigated in [37] to deal with the crowding problem of low-dimensional data distributions.", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "Thus, we do not need to pay much attention to the parameter estimation (see [37]), which is a hard task in unsupervised learning.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "In [19], the authors proposed to normalize the boosted assignments to prevent large clusters from distorting the hidden feature space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "\u2022 Accuracy (ACC) [19].", "startOffset": 17, "endOffset": 21}, {"referenceID": 37, "context": "The optimal mapping can be efficiently computed using the Hungarian algorithm [38].", "startOffset": 78, "endOffset": 82}, {"referenceID": 38, "context": "\u2022 Normalized mutual information (NMI) [39].", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "\u2022 DAE-KMS [19] uses deep auto-encoders for feature extraction and then applies k-means for later clustering.", "startOffset": 10, "endOffset": 14}, {"referenceID": 35, "context": "\u2022 AEC [36] is a variant of DAE-KMS that simultaneously optimizes the data reconstruction error and representation compactness.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "\u2022 IEC [21] incorporates the deep representation learning and ensemble clustering.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "\u2022 DEC [19] simultaneously learns the feature representations and cluster centers using deep auto-encoders and soft k-means, respectively.", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "\u2022 DEN [35] learns the clustering-oriented representations by utilizing deep auto-encoders and manifold constraints.", "startOffset": 6, "endOffset": 10}, {"referenceID": 39, "context": "\u2022 DCN [40] jointly applies dimensionality reduction and k-means clustering.", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "Visualization One of the advantages of fully convolutional neural networks is that we can naturally visualize the inner activations (or features) and the trained weights (or filters) in a two-dimensional space [27].", "startOffset": 210, "endOffset": 214}, {"referenceID": 26, "context": "It is observed in [27] that the stacked convolutional auto-encoders trained on noisy inputs (30% binomial noise) and a max-pooling layer can learn localized biologically plausible filters.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "As discussed in [27], the max-pooling layers are elegant way of enforcing sparse codes which are required to deal with the over-complete representations of convolutional architectures.", "startOffset": 16, "endOffset": 20}, {"referenceID": 36, "context": "Embedding learned features in a low dimensional space We visualize the distribution of the learned features in a two-dimensional space with t-SNE [37].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "Impact of the balance normalization In DEC [19], the authors pointed out that the balance normalization plays an important role in preventing large clusters from distorting the hidden feature space.", "startOffset": 43, "endOffset": 47}], "year": 2017, "abstractText": "Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft k-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}