{"id": "1703.08002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "A network of deep neural networks for distant speech recognition", "abstract": "Despite the remarkable progress recently made in distant speech recognition, state-of-the-art technology still suffers from a lack of robustness, especially when adverse acoustic conditions characterized by non-stationary noises and reverberation are met. A prominent limitation of current systems lies in the lack of matching and communication between the various technologies involved in the distant speech recognition process. The speech enhancement and speech recognition modules are, for instance, often trained independently. Moreover, the speech enhancement normally helps the speech recognizer, but the output of the latter is not commonly used, in turn, to improve the speech enhancement. To address both concerns, we propose a novel architecture based on a network of deep neural networks, where all the components are jointly trained and better cooperate with each other thanks to a full communication scheme between them. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework can overtake other competitive solutions, including recent joint training approaches.", "histories": [["v1", "Thu, 23 Mar 2017 11:02:47 GMT  (168kb,D)", "http://arxiv.org/abs/1703.08002v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["mirco ravanelli", "philemon brakel", "maurizio omologo", "yoshua bengio"], "accepted": false, "id": "1703.08002"}, "pdf": {"name": "1703.08002.pdf", "metadata": {"source": "CRF", "title": "A NETWORK OF DEEP NEURAL NETWORKS FOR DISTANT SPEECH RECOGNITION", "authors": ["Mirco Ravanelli", "Philemon Brakel", "Maurizio Omologo", "Yoshua Bengio", "Bruno Kessler"], "emails": [], "sections": [{"heading": null, "text": "For example, speech enhancement and speech recognition modules are often trained independently of each other. Moreover, speech enhancement usually helps the speech recognition system, but the output of the speech recognition module is not often used to improve speech enhancement. To address these two concerns, we propose a novel architecture based on a network of deep neural networks, where all components are trained together and cooperate better with each other thanks to a complete communication program. Experiments conducted using different data sets, tasks and acoustic conditions have shown that the proposed framework can overtake other competitive solutions, including more recent joint training approaches."}, {"heading": "1. INTRODUCTION", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. A NETWORK OF DEEP NEURAL NETWORKS", "text": "The upper part of the figure emphasizes the complete communication between the speech enhancement and the speech recognition of DNNs. This is caused by the fact that the speech recognition function is powered by the speech enhancement, which in turn requires the speech recognition power itself to generate the advanced language. We can circumvent this problem by deploying the proposed architecture to an arbitrary number of levels L as shown in Figure 1. The resulting computing power is built up by linking several speech enhancements and speech recognition levels of DNNs that form different levels of interaction. On the first level ('= 0), the two networks are independent, but full communication is established and continuously evolved."}, {"heading": "3. RELATED WORK", "text": "The idea of using more than one neural network in the speech recognition process has long been explored. Examples of several DNN systems include the so-called hierarchical bottleneck DNNs [27-29], which looked at a cascade between a short-term and a long-term DNN to embed longer-term information in the speech recognition process. More recently, the common training methods outlined in paragraph 1 have received considerable attention. This work can be seen as an evolution of such approaches, in which we use a more advanced architecture based on full communication between DNNs. Similar to this work is an iterative pipeline based on feeding speech recognition output into a speech enhancement DNN."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Corpora and tasks", "text": "To ensure accurate evaluation of the proposed technique, experimental validation was performed using different training data sets, different tasks and different environmental conditions. Specifically, a series of experiments were conducted with TIMIT to test the proposed paradigm under resource-poor conditions. In order to validate a more realistic task, the proposed technique was also evaluated on a WSJ task. Experiments with TIMIT are based on a phoneme recognition task (in accordance with the Kaldi-s5 recipe).The original training data set was contaminated with a series of impulse responses measured in a real dwelling.The reverberation time (T60) of the room under consideration is approximately 0.7 seconds. Development and test data were simulated using the same approach, but using a different set of impulse responses. WSJ experiments are based on the popular wsj5k task (aligned to the HiME-3 task) and are performed under two different acoustic conditions."}, {"heading": "4.2. System details", "text": "The features taken into account in this paper are standard 39 Mel-Cepstral coefficients (MFCCs), calculated every 10 ms with a frame length of 25 ms. DNNs speech enhancement are fed with a context of 21 consecutive frames and forecast (every 10 ms) 11 consecutive frames of improved MFCC functions. DNNs speech enhancement are fed through such 11 speech enhancement frames and forecast both context-dependent and monophonic targets at their output. All used layers are Rectified Linear Units (ReLU), except for the output of DNNs speech enhancement (linear) and the output of speech recognition modules (softmax). Batch normalization of language [26] and dropouts [35] are used for all hidden layers. Labels for speech enhancement DNNs (linear) and output of speech enhancement DNN (linear) and output of speech recognition modules (max)."}, {"heading": "5. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Close-talking baselines", "text": "The phoneme error rate (PER%) achieved by decoding the original TIMIT test sets is 19.5% (when using DNN models trained with the original data set).The word error rate (WER%) achieved by decoding the dense DIRHA English WSJ sets is 3.3%.It is worth noting that the DNN model results in very accurate sentence transcription under such favorable acoustic conditions, especially in conjunction with a speech model."}, {"heading": "5.2. Network of DNNs performance", "text": "The first line shows the performance achieved when the individual DNN is coupled with traditional multi-task learning, in which a language enhancement and a speech recognition task are considered at the same time. This multi-task architecture splits the first half of the hidden layers across the tasks, while the second half of the architecture is task-dependent. This approach aims to discover (within the common layers) more general and robust features that can be used to solve both correlated tasks; the third line reports on the performance achieved with the common training approach, while the second half of the architecture is task-dependent. This approach aims to discover (within the common layers) more general and robust features that can be exploited."}, {"heading": "5.3. Alternative architectures", "text": "The architecture shown in Fig. 1 is only one of the possible ways to implement the proposed network of the DNN paradigm. An alternative solution would be to share the parameters across the various speech enhancement and speech recognition DNNs. However, the results (which are not reported here) do not show any benefit from this approach. Another possible modification is presented through pre-training of each DNN before the back-propagation is carried out through the network. As noted in [24], no advantages from pre-training methods were observed when introducing batch normalization. Although a more detailed exploration of alternative solutions is in the works, in this section we report on some preliminary results obtained through the further development of the proposed method with an architecture partially inspired by residual networks (ResNets). [39] ResNets have recently achieved state-of-the-art performance in computer vision and are based on the idea that instead of learning distribution directly, we can learn the residual functions more easily with reference to the input layer."}, {"heading": "6. CONCLUSION AND FUTURE WORKS", "text": "The experiments, which were carried out taking into account different tasks, data sets and acoustic conditions, show that our method is effectively able to exploit full communication between a speech enhancement and a speech recognition device, resulting in significantly better performance than with more conventional DSR pipelines. However, this work is only a first step towards more advanced architectures that can better cooperate and communicate with each other to achieve a common goal. Therefore, our future research efforts will focus on improving the current paradigm. In this paper, for example, we have considered only speech enhancement and speech recognition of DNNs. Nevertheless, the proposed approach is a general framework that can easily be expanded by incorporating other modules and finally includes DNNs for acoustic scene classification, speech activity recognition and speech recognition. Furthermore, a more detailed exploration of alternative architectures and the natural extension of this paradigm to RNNs will also be considered as research in the future."}, {"heading": "7. REFERENCES", "text": "[1] I. Goodfellow, Y. Yoshioka et al., \"Deep learning,\" book in preparation for MIT Press, 2016. [2] D. Yu and L. Deng, Automatic Speech Recognition - A Deep Learning Approach, 2015. [3] J. Barker, R. Marxer, E. Vincent, and S. Watanabe. \"The Third CHiME Speech Separation and Recognition Challenge: Dataset, task and baselines,\" in Proc. of ASRU, 2015. K. Kinoshita et al., The reverb challenge: A Common Evaluation Framework for Dereverberation and Recognition Challenge of Reverberant Speech of Reverant Speech, in Proc. 1-4."}], "references": [{"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic Speech Recognition - A Deep Learning", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The third CHiME Speech Separation and Recognition Challenge: Dataset, task and baselines", "author": ["J. Barker", "R. Marxer", "E. Vincent", "S. Watanabe"], "venue": "Proc. of ASRU, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The reverb challenge: A Common Evaluation Framework for Dereverberation and Recognition of Reverberant Speech", "author": ["K. Kinoshita"], "venue": "Proc. of WASPAA 2013, pp. 1\u20134.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "The NTT CHiME-3 system: Advances in speech enhancement and recognition for mobile multimicrophone devices", "author": ["T. Yoshioka"], "venue": "Proc. ASRU, 2015, pp. 436\u2013443.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic event detection and classification", "author": ["A. Temko", "C. Nadeu", "D. Macho", "R. Malkin", "C. Zieger", "M. Omologo"], "venue": "Computers in the Human Interaction Loop, pp. 61\u201373. Springer London, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio concept classification with hierarchical deep neural networks", "author": ["M. Ravanelli", "B. Elizalde", "K. Ni", "G. Friedland"], "venue": "Proc. of EUSIPCO, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Proc. of ASRU, 2013, pp. 285\u2013290.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Using neural network frontends on far field multiple microphones based speech recognition", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "Proc. of ICASSP, 2014, pp. 5542\u20135546.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Contaminated speech training methods for robust DNN-HMM distant speech recognition", "author": ["M. Ravanelli", "M. Omologo"], "venue": "Proc. of Interspeech, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.R. Dai", "C.H. Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, Jan 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement with LSTM recurrent neural networks and its application to noiserobust ASR", "author": ["F. Weninger", "H. Erdogan", "S. Watanabe", "E. Vincent", "J. Le Roux", "J.R. Hershey", "B.W. Schuller"], "venue": "Proc. of LVA/ICA, 2015, pp. 91\u201399.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint noise adaptive training for robust automatic speech recognition", "author": ["A. Narayanan", "D. Wang"], "venue": "Proc. of ICASSP, 2014, pp. 4380\u20134384.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint training of front-end and back-end deep neural networks for robust speech recognition", "author": ["T. Gao", "J. Du", "L.R. Dai", "C.H. Lee"], "venue": "Proc. of ICASSP, 2015, pp. 4375\u20134379.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks", "author": ["Z. Chen", "S. Watanabe", "H. Erdogan", "J. Hershey"], "venue": "Proc. of Interspeech, 2015, pp. 3274\u20133278.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker localization and microphone spacing invariant acoustic modeling from raw multichannel waveforms", "author": ["T.N. Sainath", "R.J. Weiss", "K.W. Wilson", "A. Narayanan", "M. Bacchiani", "A. Senior"], "venue": "Proc. of ASRU, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint optimization of denoising autoencoder and dnn acoustic model based on multitarget learning for noisy speech recognition", "author": ["M. Mimura", "S. Sakai", "T. Kawahara"], "venue": "Proc. of Interspeech, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimization of speech enhancement front-end with speech recognition-level criterion", "author": ["T. Higuchi", "T. Yoshioka", "T. Nakatani"], "venue": "Proc. of Interspeech, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep beamforming networks for multi-channel speech recognition", "author": ["X. Xiao"], "venue": "Proc. of ICASSP, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Batchnormalized joint training for dnn-based distant speech recognition", "author": ["M. Ravanelli", "P. Brakel", "M. Omologo", "Y. Bengio"], "venue": "Proc. of SLT, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-task learning in deep neural networks for improved phoneme recognition", "author": ["M. Seltzer", "J. Droppo"], "venue": "Proc. of ICASSP, 2014, pp. 6965\u20136969.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proc. of ICML, 2015, pp. 448\u2013456.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical Neural Net Architectures for Feature Extraction in ASR", "author": ["F. Gr\u00e9zl", "M. Karafi\u00e1t"], "venue": "Proc. of Interspeech, 2010, pp. 1201\u20131204.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical Bottle-Neck Features for LVCSR", "author": ["C. Plahl", "R. Schl\u00fcter", "H. Ney"], "venue": "Proc. of Interspeech, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "TANDEM-Bottleneck Feature Combination using Hierarchical Deep Neural Networks", "author": ["M. Ravanelli", "V.H. Do", "A. Janin"], "venue": "Proc. of ISCSLP 2014, 2014, pp. 113\u2013117.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe", "J. Le Roux"], "venue": "Proc. of ICASSP, 2015, pp. 708\u2013712.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-Task Learning for Speech Recognition: An Overview", "author": ["G. Pironkov", "S. Dupont", "T. Dutoit"], "venue": "Proc. of ESANN, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["C. Goller", "A. Kuchler"], "venue": "Proc. of ICNN, 1996, pp. 347\u2013352.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments", "author": ["M. Ravanelli", "L. Cristoforetti", "R. Gretter", "M. Pellin", "A. Sosi", "M. Omologo"], "venue": "Proc. of ASRU, 2015, pp. 275\u2013282.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Realistic multimicrophone data simulation for distant speech recognition", "author": ["M. Ravanelli", "P. Svaizer", "M. Omologo"], "venue": "Proc. of Interspeech, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1929}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["D. Povey"], "venue": "Proc. of ASRU, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. of AISTATS, 2010, pp. 249\u2013256.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv eprints, vol. abs/1605.02688, May 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "Jian Sun"], "venue": "Proc. of CVPR, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "A crucial role in improving current solutions is being played by deep learning [1], which has recently contributed to outperforming previous HMM-GMM speech recognizers [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "A crucial role in improving current solutions is being played by deep learning [1], which has recently contributed to outperforming previous HMM-GMM speech recognizers [2].", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "The progress in the field was also fostered by the considerable success of some international challenges such as CHiME [3] and REVERB [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "The progress in the field was also fostered by the considerable success of some international challenges such as CHiME [3] and REVERB [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "To counteract such adversities, most DSR systems must rely on a combination of several interconnected technologies [6], including methods for speech enhancement [7], speech separation [8], acoustic event classification [9, 10], speaker identification [11], just to name a few.", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "To counteract such adversities, most DSR systems must rely on a combination of several interconnected technologies [6], including methods for speech enhancement [7], speech separation [8], acoustic event classification [9, 10], speaker identification [11], just to name a few.", "startOffset": 219, "endOffset": 226}, {"referenceID": 6, "context": "To counteract such adversities, most DSR systems must rely on a combination of several interconnected technologies [6], including methods for speech enhancement [7], speech separation [8], acoustic event classification [9, 10], speaker identification [11], just to name a few.", "startOffset": 219, "endOffset": 226}, {"referenceID": 7, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [12\u201314], but has also enabled the development of competitive DNN-based speech enhancement solutions [15, 16], making an effective integration between such modules easier.", "startOffset": 161, "endOffset": 168}, {"referenceID": 8, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [12\u201314], but has also enabled the development of competitive DNN-based speech enhancement solutions [15, 16], making an effective integration between such modules easier.", "startOffset": 161, "endOffset": 168}, {"referenceID": 9, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [12\u201314], but has also enabled the development of competitive DNN-based speech enhancement solutions [15, 16], making an effective integration between such modules easier.", "startOffset": 161, "endOffset": 168}, {"referenceID": 10, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [12\u201314], but has also enabled the development of competitive DNN-based speech enhancement solutions [15, 16], making an effective integration between such modules easier.", "startOffset": 261, "endOffset": 269}, {"referenceID": 11, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [12\u201314], but has also enabled the development of competitive DNN-based speech enhancement solutions [15, 16], making an effective integration between such modules easier.", "startOffset": 261, "endOffset": 269}, {"referenceID": 12, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 13, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 14, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 15, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 16, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 17, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 18, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 19, "context": "Within the DNN framework, the adoption of a joint training approach between speech enhancement and speech recognition DNNs has recently been proposed to mitigate the lack-of-matching issue [17\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 20, "context": "Similarly to [25], this is realized by adding an additional softmax classifier on the top of the last hidden layer of each speech recognition DNN.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "In general, as we discussed in [24], the integration of different gradients coming from the higher levels produces a regularization effect, which could significantly help the training of the system.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "Batch normalization [26], which has been recently proposed in the machine learning community, addresses this concern (known as internal covariate shift) by normalizing the mean and the variance of each layer for each training mini-batch, and back-propagating through the normalization step.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "Similarly to what we observed in [24], batch normalization resulted crucial to significantly achieve better performance, to improve convergence of the proposed training algorithm, and to avoid any time-consuming pretraining steps.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "Examples of multi-DNN systems were, for instance, the so-called hierarchical bottleneck DNNs [27\u201329], which considered a cascade between a short-term and a long-term DNN to embedding longer-term information in the speech recognition process.", "startOffset": 93, "endOffset": 100}, {"referenceID": 23, "context": "Examples of multi-DNN systems were, for instance, the so-called hierarchical bottleneck DNNs [27\u201329], which considered a cascade between a short-term and a long-term DNN to embedding longer-term information in the speech recognition process.", "startOffset": 93, "endOffset": 100}, {"referenceID": 24, "context": "Examples of multi-DNN systems were, for instance, the so-called hierarchical bottleneck DNNs [27\u201329], which considered a cascade between a short-term and a long-term DNN to embedding longer-term information in the speech recognition process.", "startOffset": 93, "endOffset": 100}, {"referenceID": 14, "context": "Similarly to this work, an iterative pipeline based on feeding the speech recognition output into a speech enhancement DNN has recently been proposed in [19, 30].", "startOffset": 153, "endOffset": 161}, {"referenceID": 25, "context": "Similarly to this work, an iterative pipeline based on feeding the speech recognition output into a speech enhancement DNN has recently been proposed in [19, 30].", "startOffset": 153, "endOffset": 161}, {"referenceID": 26, "context": "Our paradigm has also some similarities with traditional multitasking techniques [31].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Finally, the proposed training algorithm has some aspects in common with the back-propagation through structure originally proposed for the parsing problem [32].", "startOffset": 156, "endOffset": 160}, {"referenceID": 2, "context": "The WSJ experiments are based on the popular wsj5k task (aligned with the CHiME 3 [3] task) and are conducted under two different acoustic conditions.", "startOffset": 82, "endOffset": 85}, {"referenceID": 28, "context": "More details on this corpus and on the impulse responses adopted in this work can be found in [33, 34].", "startOffset": 94, "endOffset": 102}, {"referenceID": 29, "context": "More details on this corpus and on the impulse responses adopted in this work can be found in [33, 34].", "startOffset": 94, "endOffset": 102}, {"referenceID": 21, "context": "Batch normalization [26] and dropout [35] are employed for all the hidden layers.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "Batch normalization [26] and dropout [35] are employed for all the hidden layers.", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "See the standard s5 recipe of Kaldi for more details [36].", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "The weights of the networks are initialized according to the Glorot initialization [37], while biases are initialized to zero.", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "Similarly to [24], \u03bb is fixed to 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 33, "context": "The proposed system, which has been implemented with Theano [38], has been coupled with the Kaldi toolkit [36] to form a contextdependent DNN-HMM speech recognizer.", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "The proposed system, which has been implemented with Theano [38], has been coupled with the Kaldi toolkit [36] to form a contextdependent DNN-HMM speech recognizer.", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "The third line reports the performance achieved with the joint training approach recently proposed in [24].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "As observed in [24], when batch normalization is adopted, no benefits from any pre-training methods have been observed.", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "Although a more detailed exploration of alternative solutions is under study, in this section we report some preliminary results obtained by evolving the proposed method with an architecture partly-inspired by residual networks (ResNets) [39].", "startOffset": 238, "endOffset": 242}], "year": 2017, "abstractText": "Despite the remarkable progress recently made in distant speech recognition, state-of-the-art technology still suffers from a lack of robustness, especially when adverse acoustic conditions characterized by non-stationary noises and reverberation are met. A prominent limitation of current systems lies in the lack of matching and communication between the various technologies involved in the distant speech recognition process. The speech enhancement and speech recognition modules are, for instance, often trained independently. Moreover, the speech enhancement normally helps the speech recognizer, but the output of the latter is not commonly used, in turn, to improve the speech enhancement. To address both concerns, we propose a novel architecture based on a network of deep neural networks, where all the components are jointly trained and better cooperate with each other thanks to a full communication scheme between them. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework can overtake other competitive solutions, including recent joint training approaches.", "creator": "LaTeX with hyperref package"}}}