{"id": "1703.08068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Sequential Recurrent Neural Networks for Language Modeling", "abstract": "Feedforward Neural Network (FNN)-based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks (RNN) perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.", "histories": [["v1", "Thu, 23 Mar 2017 13:48:45 GMT  (168kb,D)", "http://arxiv.org/abs/1703.08068v1", "published (INTERSPEECH 2016), 5 pages, 3 figures, 4 tables"]], "COMMENTS": "published (INTERSPEECH 2016), 5 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["youssef oualil", "clayton greenberg", "mittul singh", "dietrich klakow"], "accepted": false, "id": "1703.08068"}, "pdf": {"name": "1703.08068.pdf", "metadata": {"source": "CRF", "title": "Sequential recurrent neural networks for language modeling", "authors": ["Youssef Oualil", "Clayton Greenberg", "Mittul Singh", "Dietrich Klakow"], "emails": ["firstname.lastname@lsv.uni-saarland.de"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Neural Network Language Models", "text": "The object of a language model is to estimate the probability distribution p (wT1) of the word sequences wT1 = w1, \u00b7 \u00b7 \u00b7, wT. Using the chain rule, this distribution can be expressed as asp (wT1) = T \u0442t = 1 p (wt | wt \u2212 11) (1). The rest of this section shows how FNN and RNN are used to approximate this probability distribution."}, {"heading": "2.1. Feedforward Neural Networks", "text": "Similar to N-gram models, FNN uses the Markov assumption of the order N-1 to approximate (1) according to the uppermost (wT1) T-value T-value = 1p (wt | wt \u2212 1t \u2212 N + 1). (2) Subsequently, each of the terms involved in this product, i.e. p (wt | wt \u2212 1t \u2212 N + 1), in a single bottom-up evaluation of the network according to Pt \u2212 i = Xt \u2212 i \u00b7 U, i = N \u2212 1, \u00b7, 1 (3) Ht = f (N \u2212 1 \u2211 i = 1 Pt \u2212 i \u00b7 Vi) (4) Ot = g (Ht \u00b7 W) (5) Xt \u2212 i encodes the word wt \u2212 i once, while the rows of U encode the continuous word representations (i.e, embedding)."}, {"heading": "2.2. Recurrent Neural Networks", "text": "An RNN attempts to capture the complete history in a context vector ht, which represents the state of the network and evolves over time. Therefore, it approaches (1) upwards (wT1) \u2248 T-t = 1p (wt | wt \u2212 1, ht \u2212 1) = T-t = 1p (wt | ht) (6) RNN rates this distribution similarly to FNN. The main difference occurs in the equations (3) and (4), which are combined into Ht = f (Xt \u2212 1 \u00b7 U + Ht \u2212 1 \u00b7 V) (7). Figure (1b) shows an example of a standard RNN. The next section shows how an RNN can be extended to explicitly modelled short-distance dependencies by additional sequential connections."}, {"heading": "3. Sequential Recurrent Neural Network", "text": "The main difference between an RNN and an FNN is the context representation. Specifically, the context layer Ht of an FNN is estimated on the basis of a fixed context size, i.e. the last N \u2212 1 words, whereas in an RNN Ht is constantly updated (for each iteration), using only the last word and the context at the time t \u2212 1."}, {"heading": "3.1. The proposed Neural Architecture", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "3.2. SRNN Training", "text": "The parameters to train for an SRNN are the word embeddings U, the project hidden connection weights V = [V1, \u00b7 \u00b7, VN \u2212 1], the hidden connection weights W and the context weight vector C for the WI model or C = [C1, \u00b7, C K] (K is the vocabulary size) for the WD model. In this case, each word w in the vocabulary is characterized by two learnable vectors, namely the continuous representation (embedding) Uw and the context weight Cw. Similar to RNN, the parameter learning of an SRNN architecture follows the standard Back Propagation Through Time (BPTT) algorithm. The main difference occurs on the projection layer, where the additional error vectors resulting from the sequential connections should be taken into account (see example or error spread in Figure 3) before the network unfolds in time."}, {"heading": "4. Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setup", "text": "We evaluated the proposed architecture using two different benchmark tasks. The first set of experiments was conducted on the Penn Treebank (PTB) corpus using standard division, e.g. [9, 11]: Sections 0-20 are used for training, while Sections 21-22 and 23-24 are used for validation and testing. Vocabulary was limited to most 10k common words, while the remaining words were all mapped onto the tokens. To assess how the proposed approach scales to large corpora, we conducted a series of experiments on the Large Text Compression Benchmark (LTCB) [12]. This corpus is based on the enwik9 dataset, which contains the first 109 bytes enwiki20060303-page articles. xml We used the same trainingtest-validation data split and preprocessing from [11]."}, {"heading": "4.2. PTB Experiments", "text": "For the PTB experiments, the FNN, FOFE and SRNN architectures, similar configurations have been proposed, i.e., the hidden layer (s) is 400 with all the hidden units that use the Rectified Linear Unit (ReLu), i.e., f (x) as an activation function, while the word representation (embedding) has the size of 200 for FNN, FOFE and LSTM and 100 for SRNN. The latter uses fs = tanh () as a sequential activation function. The hidden layer size of RNN and LSTM has been set to 400 and follows the original configuration proposed in [9] and [10], respectively, the same learning situation that in [11] we use the stochastic descent of algorithms with a minibatch size of 200, the learning rate is set to 0.4."}, {"heading": "4.3. LTCB Experiments", "text": "The results shown in Table 3 follow the same experimental setup used in [11]. Specifically, these results were obtained without the use of impulses or weight degradation, while the mini-margin size was set to 400. FNN and FOFE architectures contain 2 hidden layers of size 600 (or 400), whereas RNN and SRNN have a single hidden layer of size 600. To compare with [11], the forgetfulness factor C of the WI-SRNN model is fixed at 0.6. The LTCB results shown in Table 3 generally confirm the conclusions of PTB. In particular, we can see that SRNN models outperform all other models while requiring comparable or fewer model parameters. In addition, the WI-SRNN model with a single hidden layer shows slight differences (2 hidden layers). However, these results show a more significant improvement for the WRNN model and lower model parameters."}, {"heading": "5. Conclusion and Future Work", "text": "We have presented a sequential recursive neural network that captures short-term dependencies through short history windows and models long-term contexts through recurrent connections. Experiments on PTB and LTCB corpora have shown that this architecture significantly outperforms many state-of-the-art neural systems due to its successful combination of the motivating properties of its feedforward and recurrent predecessors. Further benefits could be achieved by better controlling the amount of information that develops in the network, as happens in LSTM, and by treating long-term dependencies more thoroughly, which will be investigated in future work."}, {"heading": "6. References", "text": "[1] S. Katz, \"Estimation of probabilities from spare data off for the lan-guage networks in International Conference,\" IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400-401, March 1987. [2] P. F. Brown, J. Cocke, S. A. D. Pietra, V. J. Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin \"A statistical approach to machine translation,\" Comput. Linguist., vol. 16, no. D. Pietra, V. J. Pietra-85, June 1990. [3] R. Rosenfeld, \"Two decades of statistical language modeling: Where do we go from here?\" in Proceedings of the IEEE, vol. 88, 2000, pp. 1270-1278. [4] R. Kneser and H. Ney. \""}], "references": [{"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400\u2013401, Mar. 1987.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "A statistical approach to machine translation", "author": ["P.F. Brown", "J. Cocke", "S.A.D. Pietra", "V.J.D. Pietra", "F. Jelinek", "J.D. Lafferty", "R.L. Mercer", "P.S. Roossin"], "venue": "Comput. Linguist., vol. 16, no. 2, pp. 79\u201385, Jun. 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Two decades of statistical language modeling: Where do we go from here?", "author": ["R. Rosenfeld"], "venue": "Proceedings of the IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Detroit, Michigan, USA, May 1995, pp. 181\u2013184.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1137\u20131155, Mar. 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "A bit of progress in language modeling, extended version", "author": ["J. Goodman"], "venue": "Microsoft Research, Tech. Rep. MSR-TR-2001- 72, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Training neural network language models on very large corpora", "author": ["H. Schwenk", "J. Gauvain"], "venue": "Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP), Oct. 2005, pp. 201\u2013208.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "11th Annual Conference of the International Speech Communication Association (INTERSPEECH), Makuhari, Chiba, Japan, Sep. 2010, pp. 1045\u20131048.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. ernock", "S. Khudanpur"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 2011, pp. 5528\u20135531.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "13th Annual Conference of the International Speech Communication Association (INTER- SPEECH), Portland, OR, USA, Sep. 2012, pp. 194\u2013197.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "The fixedsize ordinally-forgetting encoding method for neural network language models", "author": ["S. Zhang", "H. Jiang", "M. Xu", "J. Hou", "L. Dai"], "venue": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL, vol. 2, July 2015, pp. 495\u2013 500.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Large text compression benchmark", "author": ["M. Mahoney"], "venue": "2011. [Online]. Available: http://mattmahoney.net/dc/textdata.html", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1312.6026, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), Chia Laguna Resort, Sardinia, Italy, May 2010, pp. 249\u2013256.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. Cernock\u00fd"], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), Waikoloa, HI, USA, Dec. 11-15, 2011, pp. 196\u2013201.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Random forests and the data sparseness problem in language modeling", "author": ["P. Xu", "F. Jelinek"], "venue": "Computer Speech & Language, vol. 21, no. 1, pp. 105\u2013152, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A joint language model with fine-grain syntactic tags", "author": ["D. Filimonov", "M.P. Harper"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), A meeting of SIGDAT, a Special Interest Group of the ACL, Singapore, Aug. 2009, pp. 1114\u20131123.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact training of a neural syntactic language model", "author": ["A. Emami", "F. Jelinek"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Montreal, Quebec, Canada, May 2004, pp. 245\u2013248.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernock\u00fd"], "venue": "12th Annual Conference of the International Speech Communication Association (INTERSPEECH), Florence, Italy, Aug. 27-31, 2011, pp. 605\u2013608.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "A high quality Language Model (LM) is considered to be an integral component of many systems for language technology applications, such as speech recognition [1], machine translation [2], etc.", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "A high quality Language Model (LM) is considered to be an integral component of many systems for language technology applications, such as speech recognition [1], machine translation [2], etc.", "startOffset": 183, "endOffset": 186}, {"referenceID": 2, "context": "The most common approach to build such models is the word count-based method, which is commonly known as N gram language modeling [3, 4].", "startOffset": 130, "endOffset": 136}, {"referenceID": 3, "context": "The most common approach to build such models is the word count-based method, which is commonly known as N gram language modeling [3, 4].", "startOffset": 130, "endOffset": 136}, {"referenceID": 4, "context": "[5] proposed a Feedforward Neural Network (FNN) for language modeling, as an alternative to", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "This approach was very successful and has been shown to outperform a mixture of different other models [6], and to significantly improve speech recognition performance [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "This approach was very successful and has been shown to outperform a mixture of different other models [6], and to significantly improve speech recognition performance [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 7, "context": "[8, 9] proposed a Recurrent Neural Network (RNN) which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[8, 9] proposed a Recurrent Neural Network (RNN) which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "Another recurrence-based network architecture, Long-Short Term Memory (LSTM) [10], addresses some learning issues from the original RNN and explicitly controls the longevity of context information in the network.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "Moreover, setting C to a fixed value in [0, 1] and fs(x) = x leads to the Fixed-size Ordinally-Forgetting Encoding (FOFE) [11] architecture, which was proposed to uniquely encode word sequences.", "startOffset": 40, "endOffset": 46}, {"referenceID": 10, "context": "Moreover, setting C to a fixed value in [0, 1] and fs(x) = x leads to the Fixed-size Ordinally-Forgetting Encoding (FOFE) [11] architecture, which was proposed to uniquely encode word sequences.", "startOffset": 122, "endOffset": 126}, {"referenceID": 8, "context": "[9, 11]: sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[9, 11]: sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "In order to evaluate how the proposed approach scales to large corpora, we run a set of experiments on the Large Text Compression Benchmark (LTCB) [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "We adopted the same trainingtest-validation data split and preprocessing from [11].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "For feedforward networks, the baseline systems include 1) the FNN-based LM [5] as well as the 2) Fixed-size Ordinally Forgetting Encoding (FOFE) approach, which was implemented as a feedforward sentence-based model [11].", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "For feedforward networks, the baseline systems include 1) the FNN-based LM [5] as well as the 2) Fixed-size Ordinally Forgetting Encoding (FOFE) approach, which was implemented as a feedforward sentence-based model [11].", "startOffset": 215, "endOffset": 219}, {"referenceID": 10, "context": "The FOFE results were obtained using the FOFE toolkit [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "Regarding recurrent models, we compare the proposed approach to 3) the full RNN (without classes) [9], 4) to a deep RNN [13], which investigates different ways of adding hidden layers to RNN, and finally 5) to the LSTM architecture [10], which explicitly regulates the amount of information that propagates in the network.", "startOffset": 98, "endOffset": 101}, {"referenceID": 12, "context": "Regarding recurrent models, we compare the proposed approach to 3) the full RNN (without classes) [9], 4) to a deep RNN [13], which investigates different ways of adding hidden layers to RNN, and finally 5) to the LSTM architecture [10], which explicitly regulates the amount of information that propagates in the network.", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Regarding recurrent models, we compare the proposed approach to 3) the full RNN (without classes) [9], 4) to a deep RNN [13], which investigates different ways of adding hidden layers to RNN, and finally 5) to the LSTM architecture [10], which explicitly regulates the amount of information that propagates in the network.", "startOffset": 232, "endOffset": 236}, {"referenceID": 8, "context": "The hidden layer size of RNN and LSTM were set to 400 and follow the original configuration proposed in [9] and [10], respectively.", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": "The hidden layer size of RNN and LSTM were set to 400 and follow the original configuration proposed in [9] and [10], respectively.", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "We also use the same learning setup adopted in [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "The weights initialization follows the normalized initialization proposed in [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Similarly to [8], the learning rate is halved when no significant improvement in the log-likelihood of the validation data is observed.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "For both models, the context connection weights, C, were randomly initialized in [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 14, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 183, "endOffset": 187}, {"referenceID": 18, "context": "More particularly, SRNN with two hidden layers achieves a comparable performance to a mixture of RNNs [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "The results shown in Table 3 follow the same experimental setup used in [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "In order to compare to [11], the forgetting factor C of WI-SRNN\u2217 is fixed at 0.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "Feedforward Neural Network (FNN)-based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks (RNN) perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.", "creator": "LaTeX with hyperref package"}}}