{"id": "1703.08084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "histories": [["v1", "Thu, 23 Mar 2017 14:20:52 GMT  (135kb,D)", "http://arxiv.org/abs/1703.08084v1", "Submitted to ICLR Workshop 2017"]], "COMMENTS": "Submitted to ICLR Workshop 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jean-benoit delbrouck", "stephane dupont"], "accepted": false, "id": "1703.08084"}, "pdf": {"name": "1703.08084.pdf", "metadata": {"source": "CRF", "title": "MULTIMODAL NEURAL MACHINE TRANSLATION", "authors": ["Jean-Benoit Delbrouck", "Stephane Dupont"], "emails": ["Jean-Benoit.DELBROUCK@umons.ac.be", "Stephane.DUPONT@umons.ac.be"], "sections": [{"heading": "1 INTRODUCTION", "text": "In machine translation, neural networks have attracted a lot of research attention. Recently, the attention-based encoder decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been widely adopted. In this approach, recursive neural networks (RNNNs) map source sequences from words to target sequences. The attention mechanism learns to focus on different parts of the input sentence while it is decrypted. Attention mechanisms have also been shown to work with other modalities, such as images, where they are able to take care of prominent parts of an image, for example when generating text labels (Xu et al., 2015). For such applications, Convolutionary neural Networks (CNNs) have proven to be the best way to represent images (He et al., 2016). Multimodal models of texts and images enable applications such as visual model translation or multimodal translation."}, {"heading": "2 MODEL", "text": "(2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (). (). (2014). (). (). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). ((2014). (). (2014). (2014). ((2014). (). (2014). (2014). (2014). (2014). (2014). (). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014). (2014. (2014). (2014). (2014). (2014). (2014). (2014). (2014. (2014). (2014). (2014). (2014). (2014. (2014). (2014). (2014). (2014). (2014). (2014). (2014. (2014. (2014). (2014. (2014). (2014). (2014. (2014). (2014. (2014). (2014). (2014. (2014. (2014). (2014). (2014. (2014). (2014). (2014."}, {"heading": "3 SETTINGS", "text": "We use the Adam Optimizer (Kingma & Ba, 2014) with an l.r. of \u03b1 = 0.0007 and the L2 regularization of \u03b4 = 10 \u2212 5. layer size L and word embedding size E is 512. Embedding is trained with the model. We use mini batch size of 32 and Xavier weight initialization (Glorot & Bengio, 2010). For these experiments we used the Multi30K dataset (Elliott et al., 2016), an enhanced version of the Patchr30K units. For each image, one of the English descriptions was selected and manually translated into German by a professional translator (task 1). Training and development data are 29,000 and 1,014 triples respectively. A test set of size 1000 is used for BLEU and METEOR. Vocabulary sizes are 11,180 (en) and 19,154 (de)."}, {"heading": "4 RESULTS", "text": "To our knowledge, there is currently no multimodal translation architecture that convincingly exceeds a monomodal NMT baseline. However, our work shows a small but encouraging improvement. In the \"MM Attention Model,\" where both attention vectors are merged, we do not see any improvement with MCB compared to an elementary product. We suspect that the merged attention vector must be linked to cell output and then transformed linearly by projolage into a size 512 vector. This sharp reduction in dimensionality by the vector may have led to a consistent loss of information, i.e. to poor results. This motivated us to implement the second attention mechanism, the \"MM Attention.\" Here, the attention model can enjoy the full use of the combined vector dimension, which varies from 1024 to 16,000 EU. We believe that a multimodal + 62 improvement over BLW + 18 could be done here."}, {"heading": "5 ACKNOWLEDGEMENTS", "text": "This work was partially supported by the IGLU Chist Era project with a contribution from the Belgian Fonds de la Recherche Scientique (FNRS), Contract No. R.50.11.15.F, and by the FSO project VCYCLE with a contribution from the Belgian Walloon Region, Contract No 1510501."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": "arXiv preprint arXiv:1605.09186,", "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Doubly-attentive decoder for multi-modal neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell"], "venue": "arXiv preprint arXiv:1702.01287,", "citeRegEx": "Calixto et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Multi30k: Multilingual english-german image descriptions", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": "arXiv preprint arXiv:1606.01847,", "citeRegEx": "Fukui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "Compact bilinear pooling", "author": ["Yang Gao", "Oscar Beijbom", "Ning Zhang", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Attention-based multimodal neural machine translation", "author": ["Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer"], "venue": "In Proceedings of the First Conference on Machine Translation, Berlin,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Separating style and content", "author": ["Joshua B Tenenbaum", "William T Freeman"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Tenenbaum and Freeman.,? \\Q1997\\E", "shortCiteRegEx": "Tenenbaum and Freeman.", "year": 1997}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Recently, the attention-based encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been largely adopted.", "startOffset": 56, "endOffset": 103}, {"referenceID": 1, "context": "Recently, the attention-based encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been largely adopted.", "startOffset": 56, "endOffset": 103}, {"referenceID": 14, "context": "Attention mechanisms have been shown to work with other modalities too, like images, where their are able to learn to attend to salient parts of an image, for instance when generating text captions (Xu et al., 2015).", "startOffset": 198, "endOffset": 215}, {"referenceID": 8, "context": "For such applications, Convolutional neural networks (CNNs) have shown to work best to represent images (He et al., 2016).", "startOffset": 104, "endOffset": 121}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014) has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mechanism is learned to focus on different parts of the input sentence while decoding. Attention mechanisms have been shown to work with other modalities too, like images, where their are able to learn to attend to salient parts of an image, for instance when generating text captions (Xu et al., 2015). For such applications, Convolutional neural networks (CNNs) have shown to work best to represent images (He et al., 2016). Multimodal models of texts and images enable applications such as visual question answering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications. The efficient integration of multimodal information still remains a challenging task though. Both Huang et al. (2016) and Caglayan et al.", "startOffset": 8, "endOffset": 1046}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014) has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mechanism is learned to focus on different parts of the input sentence while decoding. Attention mechanisms have been shown to work with other modalities too, like images, where their are able to learn to attend to salient parts of an image, for instance when generating text captions (Xu et al., 2015). For such applications, Convolutional neural networks (CNNs) have shown to work best to represent images (He et al., 2016). Multimodal models of texts and images enable applications such as visual question answering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications. The efficient integration of multimodal information still remains a challenging task though. Both Huang et al. (2016) and Caglayan et al. (2016) made a first attempt in multimodal neural machine translation.", "startOffset": 8, "endOffset": 1073}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014) has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mechanism is learned to focus on different parts of the input sentence while decoding. Attention mechanisms have been shown to work with other modalities too, like images, where their are able to learn to attend to salient parts of an image, for instance when generating text captions (Xu et al., 2015). For such applications, Convolutional neural networks (CNNs) have shown to work best to represent images (He et al., 2016). Multimodal models of texts and images enable applications such as visual question answering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications. The efficient integration of multimodal information still remains a challenging task though. Both Huang et al. (2016) and Caglayan et al. (2016) made a first attempt in multimodal neural machine translation. Recently, Calixto et al. (2017) showed an improved architecture that significantly surpassed the monomodal baseline.", "startOffset": 8, "endOffset": 1168}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014) has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mechanism is learned to focus on different parts of the input sentence while decoding. Attention mechanisms have been shown to work with other modalities too, like images, where their are able to learn to attend to salient parts of an image, for instance when generating text captions (Xu et al., 2015). For such applications, Convolutional neural networks (CNNs) have shown to work best to represent images (He et al., 2016). Multimodal models of texts and images enable applications such as visual question answering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications. The efficient integration of multimodal information still remains a challenging task though. Both Huang et al. (2016) and Caglayan et al. (2016) made a first attempt in multimodal neural machine translation. Recently, Calixto et al. (2017) showed an improved architecture that significantly surpassed the monomodal baseline. Multimodal tasks require combining diverse modality vector representations with each other. Bilinear pooling models Tenenbaum & Freeman (1997), which computes the outer product of two vectors (such as the visual and textual representations), may be more expressive than basic combination methods such as element-wise sum or product.", "startOffset": 8, "endOffset": 1396}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014) has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mechanism is learned to focus on different parts of the input sentence while decoding. Attention mechanisms have been shown to work with other modalities too, like images, where their are able to learn to attend to salient parts of an image, for instance when generating text captions (Xu et al., 2015). For such applications, Convolutional neural networks (CNNs) have shown to work best to represent images (He et al., 2016). Multimodal models of texts and images enable applications such as visual question answering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications. The efficient integration of multimodal information still remains a challenging task though. Both Huang et al. (2016) and Caglayan et al. (2016) made a first attempt in multimodal neural machine translation. Recently, Calixto et al. (2017) showed an improved architecture that significantly surpassed the monomodal baseline. Multimodal tasks require combining diverse modality vector representations with each other. Bilinear pooling models Tenenbaum & Freeman (1997), which computes the outer product of two vectors (such as the visual and textual representations), may be more expressive than basic combination methods such as element-wise sum or product. Because of its high and intractable dimensionality (n), Gao et al. (2016) proposed a method that relies on Multimodal Compact Bilinear pooling (MCB) to efficiently compute a joint and expressive representation combining both modalities, in a visual question answering tasks.", "startOffset": 8, "endOffset": 1660}, {"referenceID": 0, "context": "(2014) implemented in TensorFlow (Abadi et al., 2016).", "startOffset": 33, "endOffset": 53}, {"referenceID": 9, "context": "We detail our model build from the attention-based encoder-decoder neural network described by Sutskever et al. (2014) and Bahdanau et al.", "startOffset": 95, "endOffset": 119}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) implemented in TensorFlow (Abadi et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 13, "context": "We use the same attention model for both modalities described by Vinyals et al. (2015). We first compute modality specific attention weights \u03b1 t = softmax(v T tanh(W1A mod + W2st + b)).", "startOffset": 65, "endOffset": 87}, {"referenceID": 2, "context": "The projection layer W2 is applied to the decoder state st and is thus shared (Caglayan et al., 2016).", "startOffset": 78, "endOffset": 101}, {"referenceID": 5, "context": "This model, referred as the \u201dMM Attention\u201d in the results section, is illustrated in Figure 1 (top right) We try a second model inspired by the work of (Fukui et al., 2016).", "startOffset": 152, "endOffset": 172}, {"referenceID": 5, "context": "We use the compact method proposed by Gao et al. (2016), based on the tensor sketch algorithm (see Algorithm 1), to make bilinear models feasible.", "startOffset": 38, "endOffset": 56}, {"referenceID": 4, "context": "For this experiments, we used the Multi30K dataset (Elliott et al., 2016) which is an extended version of the Flickr30K Entities.", "startOffset": 51, "endOffset": 73}, {"referenceID": 5, "context": "We believe a step further could be to investigate different experimental settings or layer architectures as we felt MCB could perform much better as seen in similar previous work (Fukui et al., 2016).", "startOffset": 179, "endOffset": 199}], "year": 2017, "abstractText": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "creator": "LaTeX with hyperref package"}}}