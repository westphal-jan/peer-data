{"id": "1703.08088", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Rapid-Rate: A Framework for Semi-supervised Real-time Sentiment Trend Detection in Unstructured Big Data", "abstract": "Commercial establishments like restaurants, service centres and retailers have several sources of customer feedback about products and services, most of which need not be as structured as rated reviews provided by services like Yelp, or Amazon, in terms of sentiment conveyed.", "histories": [["v1", "Thu, 23 Mar 2017 14:37:15 GMT  (715kb,D)", "https://arxiv.org/abs/1703.08088v1", null], ["v2", "Sat, 25 Mar 2017 00:00:00 GMT  (715kb,D)", "http://arxiv.org/abs/1703.08088v2", "11 pages, 6 figures"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vineet john"], "accepted": false, "id": "1703.08088"}, "pdf": {"name": "1703.08088.pdf", "metadata": {"source": "CRF", "title": "Rapid-Rate: A Framework for Semi-supervised Real-time Sentiment Trend Detection in Unstructured Big Data", "authors": ["Vineet John", "David R. Cheriton"], "emails": ["vineet.john@uwaterloo.ca"], "sections": [{"heading": null, "text": "This text could be integrated into a system with a built-in prediction model in which the real-time graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph"}, {"heading": "II. SIMILAR WORK", "text": "Most similar work in the field of learning wordembeddings refer to the classification of documents in a given set of labels or topics. Purpose of this study is to extend the use of document vector representation to the evidence of Xiv: 170 3.08 088v 2 [cs.C L] 25 Mar 2its use when the response variable (the output of a learning task) a continuous function (document scoring) rather than to discrete values (document classification). This work can be considered as a generalization of previous approaches, since this study aims to predict fine-grained document values, rather than rough document classifications. Most of the current work in the application of regression techniques to the text content refer to metadata or feature extraction [24] [25]. Similarly, there were several studies to identify document markings using classification techniques [12] [23]. There is, however, a relative lack of studies in which the effectiveness of documents presuppose a rating of effectiveness of models, an assessment of accuracy, an assessment of a rating."}, {"heading": "III. PROBLEM STATEMENT", "text": "The problem can be formulated as an evaluation of Paragraph Vectors [20] on a Document Score regression task to evaluate the accuracy of the predictive model, which can be built using the flat neural network language learning algorithm previously described by Mikolov et al. [21], which was primarily used to train word embedding in an uncontrolled manner. Predictive values are evaluated using the coefficient determination metric, also known as R2 metric [15], the hypothesis being to prove that the evaluations of the documents have a positive correlation with the vectorized versions of the text content of the document, since the semantics of positive and negative evaluations are expected to be captured by the flat neural network trained on the corpus. The expected R2 score above 0 will confirm the hypothesis, and a negative value will contradict it."}, {"heading": "IV. MOTIVATION", "text": "The motivation for the project is to prove that a significant portion of the semantic signal about sentiment about a product or service, as extracted from a snippet or text document, should prove to be a good indicator of the values of the labeled document. If this proves to be true, the goal includes building a real-time streaming framework that analyzes invisible ratings from the corpus of unlabeled data and is able to provide a score prediction with minimal processing delays. This framework can be set up on raw material hardware, and a particular company can simply inject its unlabeled ratings or articles into the system and expect a graphical representation of sentiment on a particular topic to be updated in real-time. It also allows the setting of rules for warnings, so that, for example, if the approval rate of a particular company falls below a user-defined threshold, an e-mail alert to the business decision makers could trigger a more rapid response to the negative sentiment that would otherwise."}, {"heading": "A. Semi-supervised Learning", "text": "The statistical learning formulation of the problem in this study can be divided into two distinct segments, namely: \u2022 Learning a projection of the document into vector space. This projection acts as a vector-space proxy for the feeling the document is trying to convey. This segment is completely unattended and does not require any tuning or pre-processing to calculate it. \u2022 Learning a relationship between the aforementioned projection of each document referred to above and the associated score label. This segment of the learning pipeline is monitored on the basis of previously evaluated or rated documents."}, {"heading": "B. Continuous function learning", "text": "Suppose that a given output variable y depends on a variable x such as f (x) = \u03b2x + c + (1) y = f (x) (2), where x represents a vector of independent variables, y represents the dependent or response variable, \u03b2 and c are model vector space functions applied to x to map them to y, and the error is concept.Suppose that the variable x increases by a small amount \u2206 x and x0 was the initial value, so that the value of x = x \u2212 x0 (3) is updated. A continuous function is defined as one where an infinitely small change in the input variable x results in a corresponding, infinitely small change in the response variable y [3], which can be expressed as \u0445y = f (x) \u2212 f (x0) (4)."}, {"heading": "V. NATURAL LANGUAGE UNDERSTANDING DOMAIN CHALLENGES", "text": "The section discusses the domain-specific challenges NLP has faced in implementing this research project, and the proposed strategies to address these challenges or minimize their impact on the final outcome of the experiment."}, {"heading": "A. Language Model Learning", "text": "Language modeling learning is complicated because the most common modeling approaches currently used have inherent limitations when it comes to reliably using language across the board. A few such cases are described in this paragraph.1) Dictionary-based model learning: Dictionary-based models are reliable for one-time tasks, but require manual annotation of the polarity of words and an over-reliance on the fact that the lexicon needs to be updated regularly when it encounters new words. This takes time and effort and is quite lengthy. This challenge is addressed in the research project by using an uncontrolled method to learn the vector representations of the document, rather than relying on any lexicon for details about word or document semantics. 2) Statistical model-based learning: TF-IDF and similar document-scoring approaches are based on the word-of-bag model, not on a specific domain."}, {"heading": "B. Sarcasm/Irony Detection in Text", "text": "One of the biggest hurdles to accurately gauging the mood of a document, even though it has a well-trained model or even a lexicon, is the presence of sarcasm and irony, which is highly likely in unmoderated comments and articles, especially in forums and social media. Currently, there is no clear best approach to detecting sarcasm and irony, and often the best characteristics for detecting such feelings are text metadata such as emoticons and keywords, rather than the text itself, as Gonza \u00d3lez-Iba \u00d3nez et al. [17]."}, {"heading": "C. Deep Learning", "text": "The challenge in using deep learning for model training is that deep learning models typically take hours to days to fully train. If a system needs to be rebuilt from scratch, deep learning extends the time to market because it is notoriously difficult to train. This challenge is solved by incurring only one-time costs for training the labeled document corpus. Future iterations of the training will not block the current execution of the real-time prediction component."}, {"heading": "VI. ENGINEERING DESIGN & IMPLEMENTATION CHALLENGES", "text": "The previous section described the challenges of natural language comprehension. This section aims to describe the technical design and implementation challenges that had to be addressed during the implementation phase."}, {"heading": "A. Unstructured Big Data", "text": "Unstructured data is defined as data, in this case large amounts of data that do not adhere to a strict scheme [14], if the process of generating data itself is not moderated and does not impose too many constraints on the type of data allowed. It may also be that the data contains soft links or references to other entities within the data. Therefore, it is up to the application to process large amounts of unstructured data and try to extract some meaning and relationships from it. This is addressed by a paragraph vector approach to convert variable-length text into fixed-length vectors, which are described in detail in Section VII-B."}, {"heading": "B. Distributed Document Vector Learning", "text": "The preference for the most computationally intensive tasks is to distribute them by horizontal scaling, which allows a driver to transmit multiple parts of the same job to a number of workers, who can then transmit the output to the driver once they have completed their respective tasks, and the driver can aggregate these results. However, the Gensim library used for this study does not include a distributed setup for learning document models by default. Instead, it is up to the implementation to integrate Gensim with a distributed processing frame such as Spark or Hadoop. Current implementation of Rapid Rate uses a low-memory iterator technique to reduce system memory usage to a single node, for the offline task of document vector learning described in Section IX-A, which could be scaled to train even larger volumes of documents when integrated with a distributed processing frame."}, {"heading": "C. Optimal Setup for Production Execution", "text": "Hence the decision to separate the business logic of the component document vector training (offline module) and the component of the real-time prediction (online module) as a modular set. Thus, the document vector training can be performed asynchronously in the background without knowing the current state of the real-time component, and can further improve the currently learned document vector model and the modules of machine learning. Likewise, the component of the real-time prediction does not need to be blocked after the first run on the component of the document vector learning. It can simply use the older version of the trained document vector model and the machine learning model while the planned cron job of the offline component runs in the background. This allows easy maintenance when the offline module needs to be temporarily stopped for maintenance. Likewise, if the ever-running daemon job for the online module needs to run the documents temporarily out of the queue."}, {"heading": "VII. BACKGROUND", "text": "The sections touch on the background information that needs to be known before one can delve deeply into the details of the research study and implementation conducted for this project."}, {"heading": "A. Word2Vec", "text": "Word vectors are a step away from the traditional dictionary bag + n gram model. Ultimately, the latter loses the context of words because the words that follow it are not taken into account and only the preceding words are taken into account. The Word2Vec model [21] can take both preceding and subsequent words into account as a sliding text window to calculate the probability of occurrence of the current word in the context of the Continuous Bag-of-words (CBOW) model. Alternatively, Word2Vec also offers a skip-gram model [22] that conversely uses the context probability of the current word to predict the most likely words that precede and follow it. These word embeddings can be used to derive both syntactical and semantic relationships in the vector space of the language model."}, {"heading": "B. Paragraph Vectors", "text": "Paragraph vectors are a similar concept to Word2Vec, but are based on the calculation of a fixed-length vector representation for a group of variable words that can be a sentence, a paragraph, or even an entire document. Since vector representations of documents seem to outperform some of the key weaknesses of word-bag models in terms of semantic summary and document classification tasks, it follows that a vector-based approach should also be used to evaluate the performance of a regression task. Paragraph vectors also address some of the main weaknesses of word-bag models. First, they inherit an important property of the word vector: the semantics of words. The second advantage of paragraph vectors is that they consider the word sequence, at least in a small context, in the same way as an n-gram model with a large n-aggregator. This is important because the n-gram model represents a lot of paragraph information."}, {"heading": "C. Statistical Learning Models", "text": "Once the units through which we want to learn a representation are projected into the vector space, there is a wealth of statistical learning methods that can be used to learn a continuous function, including linear regression, polynomial regression, burr regression, gradual regression, lasso regression, etc. This study uses linear regression and support vector regression to verify the correctness of the variables under consideration, both methods described in the following subparagraph.1) Linear regression: The problem of regression in mathematical statistics is characterized by the fact that there is insufficient information about the distributions of the variables under consideration [8]. In this study, once the Doc2Vec implementation generates the vector space representation of each of the documents, each of the learned dimensions represents a different input variable for the regression problem."}, {"heading": "D. Distributed Messaging with Kafka", "text": "This project uses Apache Kafka to process huge amounts of data from real-time streams. Like a messaging system, Kafka uses a pull-based consumption model that allows an application to consume data at its own speed and rewind consumption when needed. By focusing on log processing applications, Kafka achieves much higher throughput than traditional messaging systems, and it also provides integrated distributed support and can be scaled. Kafka has been used successfully on LinkedIn for both offline and online applications. [19]"}, {"heading": "E. Horizontal Scaling with Spark Streaming", "text": "Discretized Streams (D-Streams), a stream programming model for large clusters that provides consistency, efficient troubleshooting, and powerful integration with batch systems, is the core offering of Spark Streaming. The key idea is to treat streaming as a series of short batch jobs and reduce the latency of these jobs as much as possible, bringing many of the benefits of batch processing models into stream processing, including clear consistency semantics and a new parallel recovery technique that is a truly cost-effective recovery technique for processing streams in large clusters [26]."}, {"heading": "VIII. SYSTEM ARCHITECTURE", "text": "This section describes the proposed architectural design, including the associated components, and the flow of control and data within the architecture. The system architecture is described in Fig. 3The individual components and their interactions are described in the following subsections VIII-A and VIII-B."}, {"heading": "A. Components", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "B. Component Interactions", "text": "A simple processor reads the labeled documents and feeds them into the Document Embedding Tool, which in turn sends a representation of each of the labeled documents as a vector. Document vector representations are now associated with their corresponding results and vector representations, and the results are fed together into the ML modeling tool to generate linear regression and support vector regression models. These models are then \"inserted\" or flushed to the hard disk to be used by the Real-Time Prediction Module. Along with these models, the trained Document Embedding Model is also flushed to the hard disk. Once these models are flushed to the hard disk, the system can start its real-time prediction frame. The unlabeled documents are sent to the news agent expectation queue to which the daemon component of the architecture is attached. It processes the data from lists of documents into batches and sends the results into a micro-database."}, {"heading": "A. Off-line learning module", "text": "The offline learning module consists of the flat implementation of neural networks using the Gensim library (Section IX-C2). The algorithm of this module is represented in algorithm 1."}, {"heading": "B. On-line real-time prediction engine", "text": "The online prediction module is the real-time component presented to Spark (Section IX-C5) workers. Each Spark worker listens to a distributed queue in Kafka (Section IX-C4). As soon as the blank documents are streamed, the engine reads the transferred data and predicts a score before entering it in OpenTSDB (Section IX-C6). The general algorithm for handling the streaming data is described in Algorithm 2. A subsection of this module, the algorithm used to derive mood, is described in Algorithm 3.Data: rdd, docvec model, ml model Result: zero initialize opentsdb connection for (key, message): rdd.collect () dotokenized document = message.tokenize () document vector = docvec model.infer vector (tokenized document) predicted score = idgorize (.db) now predict (.db) document (.db)"}, {"heading": "C. Tech stack", "text": "The subsection described the composition of the tech stack used to implement the Rapid Rate Framework1) Python: Document Processing & Purification, Pipelining, Real-Time Prediction [6].2) Gensim: Python Library. The Doc2Vec class in Gensim is used to learn document vector representations [4].3) Scikit-Learn: Python Library. Activate the calculation of linear regression and support vector regression models [9].4) Apache Kafka: Distributed Messaging Queue. Used to obtain new unlabeled documents, and is also a potential sink for dumping the results of unlabeled documents [19].5) Apache Spark Streaming: The real-time prediction component written in Python is submitted as Spark Streaming Job [26]. This database predicts each corresponding time base."}, {"heading": "X. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Testbed", "text": "The tests were conducted on an Amazon document corpus of 600,000 unique product ratings, and the full Amazon ratings dataset is available for download [1]."}, {"heading": "B. Metrics", "text": "The coefficient of determination transmits the quantified signal quantity of the input vector captured by the trained model by validating it with a test set. The value of the coefficient of determination is derived from the following equations: SStot = \u2211 i (yi \u2212 y) 2 (5) SSres = \u2211 i (f (x) i \u2212 y \u0441) 2 (6) R2 = 1 \u2212 SSres SStot (7) The R2 score derived from Eq.7 is the coefficient of determination use to predict the quality of fit for each of the learned machine learning models."}, {"heading": "C. Experimental Results", "text": "The accuracy values of the experimental results are shown in Table I. The value shown in this table is the determination coefficient or R2 value. Positive values of the two experimental analyses indicate that the model captures a good part of the mood in each of the marked documents and ensures an appropriate level of accuracy for predicting the unmarked documents in real time. As shown in Figure 5, the current trend seems to suggest at least an improvement in the accuracy of the polynomial function in terms of increasing the number of documents to be trained.The initial experiments were conducted with a corpus of 100,000 marked documents, the subsequent experiments used corpora with 300,000 documents and finally 600,000 documents. In addition, the code profiling information shown in Figure 4 shows that the unattended learning segment of obtaining the document vectors of the marked documents is the longest running part of the offline learning module. In comparison, the processing of documents and statistical model training is negligible."}, {"heading": "XI. CONCLUSIONS", "text": "In summary, this research project summarizes the following objectives: \u2022 Evaluating the use of document embedding for a regression task \u2022 Establishing a framework for predicting real-time sentiment trends. By attempting to derive a feeling from unstructured data, the result of a positive R2 value is promising (Table I). It shows that the model captures a significant amount of the signal present in the independent variable. Therefore, it can be used trustworthy to a certain extent to provide close enough value for the actual intended document sentiment.In addition, this work has led to a generic framework that can be used to build a sentiment analytics dashboard on a user-friendly interface. This framework can be expanded to process any kind of document content both for vector training and for the real-time sentiment prediction module. The general framework was used to build a state-of-the-art product framework for the part of the document validation in the waiting architecture, which is offered as a component of the waiting architecture."}, {"heading": "XII. FUTURE WORK", "text": "With respect to the statistical models studied, the current approach supports only the simplest regression models, i.e. linear regression and supportive vector regression (SVR), which could be expanded to provide additional training mechanisms that can integrate more complex regression algorithms for a higher level of accuracy. Additional channels could be chosen as the culmination point of the newly evaluated, unlabeled documents. Currently, the data is stored in a time series database, but this can easily be converted into something more general, such as publication to another Kafka queue. Multiple consumer applications can therefore benefit from the stream of findings published to the stream. Similarly, following the challenge of natural language comprehension described in Section V-A3, a proposed solution could be to simply obfuscate the units spoken through the document for which the initial sentiment analysis model is trained. Similarly, the logic of the prediction of the pre-seasonal data must also be carried out completely."}, {"heading": "XIII. ACKNOWLEDGMENTS", "text": "The author thanks Dr. Paulo Alencar for his guidance, feedback and suggestions in the conception of this research project. The author also thanks Dr. Olga Vechtomova for her feedback and ideas on document evaluation and the tasks of mood polarity."}], "references": [{"title": "Support vector regression", "author": ["D. Basak", "S. Pal", "D.C. Patranabis"], "venue": "Neural Information Processing-Letters and Reviews,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Sentiment classification based on supervised latent n-gram analysis", "author": ["D. Bespalov", "B. Bai", "Y. Qi", "A. Shokoufandeh"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A query language and optimization techniques for unstructured data", "author": ["P. Buneman", "S. Davidson", "G. Hillebrand", "D. Suciu"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "An r-squared measure of goodness of fit for some common nonlinear regression models", "author": ["A.C. Cameron", "F.A. Windmeijer"], "venue": "Journal of Econometrics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Sentiment regression: Using real-valued scores to summarize overall document sentiment", "author": ["A. Drake", "E.K. Ringger", "D.A. Ventura"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["R. Gonz\u00e1lez-Ib\u00e1nez", "S. Muresan", "N. Wacholder"], "venue": "Association for Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Statistics: A spectator sport, volume", "author": ["R.M. Jaeger"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}, {"title": "Kafka: A distributed messaging system for log processing", "author": ["J. Kreps", "N. Narkhede", "J. Rao"], "venue": "In Proceedings of the NetDB,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the ACL- 02 conference on Empirical methods in natural language processing- Volume", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Genetic algorithm based feature selection and parameter optimization for support vector regression applied to semantic textual similarity", "author": ["B.-h. Su", "Y.-l. Wang"], "venue": "Journal of Shanghai Jiaotong University (Science),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Natural language processing to assess documentation of features of critical illness in discharge documents of acute respiratory distress syndrome survivors", "author": ["G.E. Weissman", "M.O. Harhay", "R.M. Lugo", "B.D. Fuchs", "S.D. Halpern", "M.E. Mikkelsen"], "venue": "Annals of the American Thoracic Society,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Discretized streams: an efficient and fault-tolerant model for stream processing on large clusters", "author": ["M. Zaharia", "T. Das", "H. Li", "S. Shenker", "I. Stoica"], "venue": "In Presented as part of the,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Most of the current work in usage of regression techniques on text content are related to meta-data or features extraction [24][25].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "Most of the current work in usage of regression techniques on text content are related to meta-data or features extraction [24][25].", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "Similarly there have been several studies to identify document tags using classification techniques[12][23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Similarly there have been several studies to identify document tags using classification techniques[12][23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "The problem statement can be formulated as an evaluation of Paragraph Vectors[20] on a document score regression task to evaluate the accuracy of the prediction model that can be built using the shallow neural network language model learning algorithm previously described by Mikolov et al.", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "[21], which was primarily used to train word embeddings in an unsupervised manner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The prediction scores will be evaluated using the co-efficient of determination metric, also known as the R2 metric[15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 11, "context": "Paragraph Vector Learning Framework[22]", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "Since sentiment as fine-grained values offer more depth in terms of sentiment insight [16], the study in this paper is formulated as a regression problem.", "startOffset": 86, "endOffset": 90}, {"referenceID": 6, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Unstructured data is defined as data, large amounts of it, in this case, which do not abide by a strict schema[14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "The Word2Vec model[21] can take into consideration both preceding and following words, as a sliding window of text, to compute the probability of occurrence of the current word, as part of the Continuous Bag-of-words (CBOW) model.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "Alternatively, Word2Vec also provides a skip-gram model[22], which conversely, uses the context probabilitie of the current word, to predict the most likely words to precede and follow it.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "This is important, because the n-gram model preserves a lot of information of the paragraph, including the word order[20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "SVR has been applied in various fields time series and financial (noisy and risky) prediction, approximation of complex engineering analyses, convex quadratic programming and choices of loss functions[11].", "startOffset": 200, "endOffset": 204}, {"referenceID": 8, "context": "[19]", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "This brings many of the benefits of batch processing models to stream processing, including clear consistency semantics and a new parallel recovery technique that is a truly cost-efficient recovery technique for stream processing in large clusters[26].", "startOffset": 247, "endOffset": 251}, {"referenceID": 2, "context": "It uses algorithms such as stochastic gradient descent in an attempt to converge to a lowest possible value of learning error[13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "Used to receive new unlabeled documents, and is also a potential sink for dumping the results of the result ratings of unlabeled documents[19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "5) Apache Spark Streaming: The real-time prediction component written in Python is submitted as a Spark Streaming job[26].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "The experimental results are being evaluated for goodness of model fit using the co-efficient of determination metric [18].", "startOffset": 118, "endOffset": 122}], "year": 2017, "abstractText": "Commercial establishments like restaurants, service centers and retailers have several sources of customer feedback about products and services, most of which need not be as structured as rated reviews provided by services like Yelp, or Amazon, in terms of sentiment conveyed. For instance, Amazon provides a fine-grained score on a numeric scale for product reviews. Some sources, however, like social media (Twitter, Facebook), mailing lists (Google Groups) and forums (Quora) contain text data that is much more voluminous, but unstructured and unlabeled. It might be in the best interests of a business establishment to assess the general sentiment towards their brand on these platforms as well. This text could be pipelined into a system with a builtin prediction model, with the objective of generating real-time graphs on opinion and sentiment trends. Although such tasks like the one described about have been explored with respect to document classification problems in the past, the implementation described in this paper, by virtue of learning a continuous function rather than a discrete one, offers a lot more depth of insight as compared to document classification approaches. This study aims to explore the validity of such a continuous function predicting model to quantify sentiment about an entity, without the additional overhead of manual labeling, and computational preprocessing & feature extraction. This research project also aims to design and implement a re-usable document regression pipeline as a framework, Rapid-Rate[7], that can be used to predict document scores in real-time. Keywords\u2014 natural language processing, big data, word embedding, regression, streaming data processing, time-series analytics", "creator": "LaTeX with hyperref package"}}}