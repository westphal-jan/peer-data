{"id": "1703.08120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Recurrent and Contextual Models for Visual Question Answering", "abstract": "We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to $64.6\\%$, while the ensemble of all models achieves the best accuracy of $66.67\\%$, within $0.5\\%$ of the current state-of-the-art for Visual7W.", "histories": [["v1", "Thu, 23 Mar 2017 15:57:23 GMT  (563kb,D)", "http://arxiv.org/abs/1703.08120v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["abhijit sharang", "eric lau"], "accepted": false, "id": "1703.08120"}, "pdf": {"name": "1703.08120.pdf", "metadata": {"source": "CRF", "title": "Recurrent and Contextual Models for Visual Question Answering", "authors": ["Abhijit Sharang", "Eric Lau"], "emails": ["abhisg@stanford.edu", "eclau@stanford.edu"], "sections": [{"heading": null, "text": "Motivated by divergent trends in model complexity in the literature, we examine the balance between the expressiveness and simplicity of the model by examining incrementally more complex architectures; we begin with the LSTM encoding of input questions and answers; build on this by generating context through LSTM encoding of neural image and question presentations and attention to images; and evaluate the diversity and predictability of our models and the ensemble thereof. All models are evaluated using a simple state-of-the-art baseline, consisting of simple concatenation of word bags and CNN representations for the text or images. Generally, we observe significant variations in image foundation performance between our models that are not evident from their overall performance, as well as evidence of data set bias. Our standalone models achieve accuracy up to 64.6%, while the best models are achieved within the 70.5% W accuracy of all current models."}, {"heading": "1 Introduction", "text": "A high-performance QA system should demonstrate a wide range of skills, such as semantic thinking, sentiment analysis, and contextual deduction of language. Visual question-and-answer (vQA) is an extension of text-based QA that requires understanding of both images and questions about them. Common methods include the use of revolutionary and recursive neural networks to map image-text pairs into a vector space that is representative of the image's interaction with text. Several compositional models for combining these multimodal representations have been researched [1]. In the literature, there are two diverging trends in state-of-the-art vQA systems. One is toward increasingly complex recurring models that often have complex attention mechanisms over the images and text. In these models, textual content is often associated to determine which areas of the image are more important."}, {"heading": "2 Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Standard and Visual Question Answering", "text": "Although answering textual questions is an established standard task in natural language processing, relatively new improvements in recurrent neural network (RNN) models and Convolutionary Neural Network (CNN) image recognition models have been effectively applied in combination with vQA [2]. Several vQA datasets have been published, such as VQA 1.0, Visual Genome, and Visual7W. However, recent studies have shown that some datasets, such as VQA 1.0, exhibit strong speech distortions [1], and surprisingly few questions require non-trivial thinking and abstraction to arrive at the answer. Visual7W [3] consists of 327,939 QA pairs and claims to have a greater complexity and diversity of questions and answers in order to assess vQA models more rigorously. VQA 2.0 is under development and claims to have improvements along the same lines."}, {"heading": "2.2 Trend 1: Toward Complex Neural Architectures", "text": "A predictable trend of newer vQA systems in the literature is toward more complex neural architectures that incorporate frequently recurring models and attention mechanisms. Thus, for example, Shih et al. [4] attempted to map both the question and text and images into a latent space in which inner products of textual features produced attention regions for the associated image. Moreover, Lu et al. [5] seek to create a co-attention model that conducts common reflections on question and image."}, {"heading": "2.3 Trend 2: Toward Simple Baselines", "text": "Recent work on very simple base systems has led to basic systems whose performance is comparable to or exceeds that of more complex recurring systems on VQA 1.0 and Visual7W. Thus, Zhou et al. [6] demonstrated a simple, non-recurring model in which the input query is transformed using na\u00efve pouches into a word characteristic that is simply linked to the depth image characteristics obtained with GoogLeNet and fed to a Softmax level to predict the response class. Similarly, Jabri et al. [7] developed a system that instead uses the full question-picture triple number (q, a, I) as an example input. In their system, 300-dimensional word bag characteristics word2vec for the question and multiple-choice answer are combined with 2048-dimensional depth image characteristics from Resnet-101 and with 8192 hidden units are fed into your current ML1% accuracy model."}, {"heading": "3 Approach", "text": "We study model architectures of incrementally increasing complexity, starting with simple, non-recurring baselines and then progressively more complex, recurring, contextual, and attention-oriented models that combine some or all of the (q, a, I) triplets for a given question q, choice a (of which there are four per q), and CNN image characteristics I to create new intermediate neural character representations. A diagram of our models is shown in Figure 1. For our individual experimental models, we describe relevant modifications that only generate the trait vector; the rest of the model can be considered identical, minus minor hyperparameter adjustments elsewhere."}, {"heading": "3.1 Baseline: Simple Non-Recurrent Bag-Of-Words", "text": "The simplest model for the task is to adopt the BOW model in [7]. QA text is mapped onto word embedding and images onto representations obtained from a pre-trained CNN after removing the last dense layer. We use Glove-300 for word embedding and ResNet-50 for image embedding. Each word is 300 dimensional and each image is 7 x 7 x 2048 dimensional. We then obtain the BOW representations of the text sequences by calculating the word embedding across all words in the sequence and the bag-of-images representation by averaging over each 7 x 7 stack on 2048 stacks. These are concatenated and used as input on an MLP that consists of a fully connected hidden layer and output layer that yields a single score for each (q, a, I) triplet. These values are then summarized as a size 4 vector and used as a cross-loss function."}, {"heading": "3.2 Version [1]: LSTM-Encoding of Question and Image Text", "text": "Version [1] generates recurring representations of the question and answer sequences. The question and answer sequences each go through their own LSTMs and the last hidden state of each is the vector representation of each sequence. However, we also intend to remain as agnostic as possible in the order of the words. For example, \"a sweet dog\" and \"a sweet dog\" should have close associations in transformed space. Therefore, we use bidirectional LSTMs where the sequences are read forwards and backwards and the concatenation serves as representation."}, {"heading": "3.3 Version [2]: Augmenting With Context From Question and Image", "text": "The baseline and V [1] have minimal interaction between the image and the text, and therefore may not capture the deeper semantic relationships required for correct reasoning. To obtain the context vector C, we transform the sequence of questions into a sequence {(qw, I)}, where qw is the word that represents a question w and I is the bag-of-image representation shown above. As before, the transformed sequence is passed by a bidirectional LSTM, and the final hidden state is called C. We experiment with three architectures that use C to achieve deeper interaction between text and image: \u2022 The most basic architecture receives the representation from the bidirectional LSTM and associates it with C. This vector is then used as an image to give the answer to the MP variation \u2022 In the complete linkage of LSTM LM, the STM variation consists of the answer."}, {"heading": "3.4 Version [3]: Augmenting With Attention Over Images", "text": "While the contextual model captures some interaction between the image and the text, it exerts a unified effect on our models. In fact, at any time frame in the sequence, we may want the image representation to be influenced by the word representation. Instead of simply cutting the 7 \u00d7 7 values in the 2048-dimensional stack for each word, we want the 2048-dimensional image representation to depend on the word with which it interacts. To achieve this, we introduce an attention module. This module transforms the 7 \u00d7 7 \u00d7 2048-dimensional image embedding into 1 \u00d7 2048-dimensional embedding in the following way. Suppose w is embedding the word under interaction and {I1,.., I2048} is the raw image embedding. Then Aj = MLP ([Ij, w]), where MLP consists of two completely interconnected layers, the light and the activation of the second layer (the I)."}, {"heading": "4 Experiments", "text": "We use the telling part of the Visual7W dataset, consisting of 69,817 training samples, 28,020 validation samples, and 42,031 test samples. Each sample consists of an image, a question string, and four option strings, of which only one choice is correct. To prevent data contamination, each image occurs in exactly one partition of the dataset. Question types are broken down into who, what, when, why, and how. In each of the models described above, the architecture is the same after generating the feature vector. The last two layers consist of a completely hidden layer with a large number of hidden units and ReLU activations, and a fully connected output layer with a single unit with sigmoid activations. This copy of the network is used for (q, a, I) triplets over all four response choices for a given question q, and the results are then experimented with by a soft-max layer of different normoid units."}, {"heading": "4.1 Baseline", "text": "We examine three variants of the BOW baseline: the first version uses the image, the question, and the answers; the second version does not use the image; and the third version is trained only on the answers, both of which are useful to investigate the bias of datasets. In all three cases, a convergence in 50-60 iterations of the training with the standard Adam optimizer is achieved."}, {"heading": "4.2 Version [1]: LSTM QA-Encoding", "text": "Here too, the three variants described above are used. We adjust the hyperparameters for regularization and dropout to the matrices U and W. Convergence is achieved in 90-100 iterations with the standard Adam optimizer."}, {"heading": "4.3 Versions [2] and [3]: Context and Attention Augmentations", "text": "Because the more complicated context and attention models have a higher susceptibility to overadjustment to the training set, the drop-out and regularization parameters used in these models are more severe. Furthermore, these models have a more unpredictable optimization landscape, so the Adam optimizer used for these models has a learning rate of 10 \u2212 4, 0.1 times the learning rate of the standard optimizer. We generally observe that these models take much longer to align and have a lower convergence validation accuracy than the simple models."}, {"heading": "4.4 Ensemble Model", "text": "Ensembling is an effective technique to get a stronger model from a range of weaker models. It works best when models have diversity, meaning that different models focus on different aspects of the data and collectively create an ensemble model that can deliver robust results on every aspect. We assume that our selection of models is sufficiently varied with increasing complexity to produce a strong ensemble model. To this end, we run each test example through each of the ten models and select the option chosen by the majority of models as an answer to the question, severing ties arbitrarily. We do not train on the ensemble model and do not weigh the models, so the voice of each model counts equally."}, {"heading": "5 Results", "text": "We use our gradual progression in the additional model complexity to examine its diversity in terms of general and specific performance increases and limitations. Results for the multitude of models are shown in Table 1. A comparison of the ensemble of our models with other models in the literature for the Visual7W dataset is shown in Table 2."}, {"heading": "5.1 Effect of Additional Complexity: Context and Attention", "text": "From the questions for which these models are the \"experts,\" we find that most seem to focus on aspects of the image in terms of color and spatial positioning. Figures 2 and 3 illustrate some questions that only the model can answer with attention, or the models with a correct answer. To answer these questions, we need to focus on specific sections of the image and understand the underlying semantics, but that is the goal of these improved models. For example, the question \"How many white blocks are shown on the plate between sinks?\" requires consideration of the position of the sink, the recognition of the blocks, and then the filtering out of only the white blocks. The attention model is able to capture this relationship. Likewise, the question \"When was the image taken?\" requires context generation by associating fading light with the evening, which the contextual models can do. Interestingly, we find that simply the attention of the viewer and the viewer's function increases with the question M."}, {"heading": "5.2 Effect of Ensembling", "text": "Our qualitative and quantitative observations of the diversity of our models are supported by the improved performance of our ensemble, which successfully exploits the diversity of its constituent classifiers. In each question category, with the exception of the \"when\" category, there is a marked improvement in the accuracy of the ensemble by about 2-3% compared to the accuracy of the best singleton model, which indicates that different models specialize in different aspects of the data and their combination is strong in all aspects. The \"when\" category does not show a marked improvement due to higher homogeneity of the questions in the category, which is also demonstrated by our subsequent bias analysis of the data sets. Examples of questions correctly answered by the ensemble are shown in Figure 4."}, {"heading": "5.3 Evidence of Dataset Bias", "text": "Predictably, these \"simple\" questions can be answered without knowing the image at all, while \"hard\" questions, which all classifiers do not answer correctly, require more in-depth thinking about the image. Table 3 shows a breakdown into hard questions (which are correctly answered by fewer than 3 models), simple questions (which are correctly answered by more than 7 models), and fair questions (which are correctly answered by 3-7 models) The \"who,\" \"when,\" \"where,\" and \"why\" has a significant number of simple questions, with the \"when\" category having a percentage of up to 71.3%, indicating the homogeneity of questions and biases in the text that can be picked up by models that do not include images. On the other hand, 33.5% and 25.1% of the questions in the \"how\" and \"what\" categories have a correct answer to these questions, while Figure 6 shows that the distribution of questions is very strong enough to answer certain questions correctly (the questions are very tied to the answers)."}, {"heading": "6 Conclusion", "text": "In summary, we have proposed a series of recurring, context-dependent, and attentive models to answer visual questions that examine the impact of the balance between the expressiveness of the model and simplicity on performance. While we observe that our simplest experimental model achieves the best individual model performance with simple LSTM encodings of the QA text, confirming a trend toward simpler architectures found in the literature, we also see that our more complex context-dependent and attention-oriented models show significant improvements in image viewing, exploited by the improved performance of our ensemble model, which achieves a test accuracy within 0.5% of the state of the art on Visual7W. In evaluating the performance of our model series, we find evidence of dataset bias in Visual7W, in the form of significant percentages of both universally simple and hard questions that answer all or none of our models correctly."}, {"heading": "Acknowledgments", "text": "The authors thank Danqi Chen for her advice and help with this project and Dr. Manning and Dr. Socher for their guidance."}], "references": [{"title": "Visual question answering: A survey of methods and datasets", "author": ["Qi Wu", "Damien Teney", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Revisiting visual question answering baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "TensorFlow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi"], "venue": "on heterogeneous systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Several compositional models for combining these multimodal representations have been explored [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Though textual question answering is a well-established standard task in natural language processing, relatively recent improvements in recurrent neural network (RNN) models, as well as convolutional neural network (CNN) models for image recognition, have been effectively applied in combination to vQA [2].", "startOffset": 303, "endOffset": 306}, {"referenceID": 0, "context": "0, exhibit strong language bias [1], and surprisingly few questions require non-trivial reasoning and abstraction to arrive at the answer.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "Visual7W [3] consists of 327,939 QA pairs and claims to have greater question-answer complexity and diversity to more rigorously evaluate vQA models.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "[4] attempted to map both the question and answer text and images into a latent space where inner products of textual features yielded attention regions for the associated image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] attempt to create a co-attention model that performs joint reasoning over the question and image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] demonstrated a simple non-recurrent model where the input question is transformed into a word feature using naive bag-of-words, simply concatenated with deep image features obtained using GoogLeNet, and fed to a softmax layer to predict the answer class.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] created a system that instead took the full question-image-answer triplet (q, a, I) as the sample input.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The simplest model for the task is an adoption of the bag-of-words (BOW) model in [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "In [2] and [3], we replace the dotted portion of [1] with minor adjustments elsewhere.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [2] and [3], we replace the dotted portion of [1] with minor adjustments elsewhere.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "In [2] and [3], we replace the dotted portion of [1] with minor adjustments elsewhere.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "2 Version [1]: LSTM-Encoding of Question and Image Text", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "Version [1] creates recurrent representations of the question and answer sequences.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "3 Version [2]: Augmenting With Context From Question and Image", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "The baseline and V[1] have minimal interaction between the question image and text and hence may not capture deeper semantic relationships required for proper reasoning.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "The augmented answer representation is concatenated with the LSTMencoding of the question only, as in V[1], and is passed as the feature vector to the MLP.", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "4 Version [3]: Augmenting With Attention Over Images", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "We concatenate its last hidden layer with the answer LSTM representation and the transformed image representation as in V[2] as the feature vector to the aforementioned MLP for score generation.", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "Finally, the network is tuned using Adam optimizer [8] for a certain number of iterations with the learning rate, minibatch size, and number of iterations tuned with other hyperparameters.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "The entire experimental framework is run with a Tensorflow back-end [9] with front-end code written in Keras [10].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "[2] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The other results are from [7].", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "2 Version [1]: LSTM QA-Encoding", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "3 Versions [2] and [3]: Context and Attention Augmentations", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "3 Versions [2] and [3]: Context and Attention Augmentations", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "Interestingly, we find that simply augmenting the answer and question LSTM-encoded feature inputs to the MLP for the contextual and attentional models, respectively, and keeping the default image embeddings from V[1] is less effective than removing the image embedding and using the augmentations alone with the original LSTM-encodings of the question and answer, respectively.", "startOffset": 213, "endOffset": 216}, {"referenceID": 2, "context": "While we observe increased image reasoning for V[3], it performs worse overall across all question types.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "[3] in their more complicated implementation of attention.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Generally, however, the best individual model was V[1], which is evidence of trend (2) seen in the literature towards simpler architectures with high performance, alluding to strong dataset bias as we explore subsequently.", "startOffset": 51, "endOffset": 54}], "year": 2017, "abstractText": "We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to 64.6%, while the ensemble of all models achieves the best accuracy of 66.67%, within 0.5% of the current state-of-the-art for Visual7W.", "creator": "LaTeX with hyperref package"}}}