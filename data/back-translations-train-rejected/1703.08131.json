{"id": "1703.08131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Online Distributed Learning Over Networks in RKH Spaces Using Random Fourier Features", "abstract": "We present a novel diffusion scheme for online kernel-based learning over networks. So far, a major drawback of any online learning algorithm, operating in a reproducing kernel Hilbert space (RKHS), is the need for updating a growing number of parameters as time iterations evolve. Besides complexity, this leads to an increased need of communication resources, in a distributed setting. In contrast, the proposed method approximates the solution as a fixed-size vector (of larger dimension than the input space) using Random Fourier Features. This paves the way to use standard linear combine-then-adapt techniques. To the best of our knowledge, this is the first time that a complete protocol for distributed online learning in RKHS is presented. Conditions for asymptotic convergence and boundness of the networkwise regret are also provided. The simulated tests illustrate the performance of the proposed scheme.", "histories": [["v1", "Thu, 23 Mar 2017 16:27:40 GMT  (1157kb)", "https://arxiv.org/abs/1703.08131v1", "submitted paper"], ["v2", "Fri, 24 Mar 2017 09:35:30 GMT  (1157kb)", "http://arxiv.org/abs/1703.08131v2", null]], "COMMENTS": "submitted paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pantelis bouboulis", "symeon chouvardas", "sergios theodoridis"], "accepted": false, "id": "1703.08131"}, "pdf": {"name": "1703.08131.pdf", "metadata": {"source": "CRF", "title": "Online Distributed Learning Over Networks in RKH Spaces Using Random Fourier Features", "authors": ["Pantelis Bouboulis"], "emails": ["bouboulis@gmail.com,", "stheodor@di.uoa.gr.", "symeon.chouvardas@huawei.com"], "sections": [{"heading": null, "text": "In recent years, the number of people who are able to live in a country where people are able to integrate, where they are able to integrate, and where they are unable to integrate, has increased. In most of these countries, the data cannot be processed in a single way as they do (due to memory and computer resources), and the respective learning and inferencing problems have to be shared. In most cases, data has been accessed not in a single place, but at different levels."}, {"heading": "II. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. RKHS", "text": "The reproduction of Kernel Hilbert Spaces (RKHS) are internal product spaces of functions defined on X whose respective point evaluation is functional, i.e., Tx: H \u2192 X: Tx (f) = f (x), is linear and continuous for each x-X. This is usually represented by the reproducing property [18], [25], the internal products in H with a specific (semi-) positive definitive core function, defined on X \u00b7 X (associated with space H). As a result (\u00b7, x) lies in H for all x-X, the reproducing property explains that < p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p."}, {"heading": "B. Kernel Online Learning", "text": "The above features have made RKHS a popular tool for solving non-linear tasks in both batch and online settings. In addition to the widespread use of SVMs, there has been increased interest in non-linear online tasks around the square error loss function in recent years. In addition, there have been core-based implementations of LMS [27], [28], RLS [29], APSM [31], [32] and other related methods [33] as well as online implementations of SVMs [34] that focus on the original formulation of the task. Henceforth, we will consider online learning tasks based on the training sequences of the form D = (xn, yn) and other related methods [33], as well as online implementations of SVMs [34] that focus on the original formulation of the task."}, {"heading": "C. Approximating the Kernel with random Fourier Features", "text": "This means, for example, that a large kernel matrix must be calculated in order to significantly increase the calculation costs of the method. To reduce the computing load, a general approach must be used, which includes a kind of approximation of the kernel evaluation. The most popular techniques in this category are the method [41], [42] and the random Fourier approach [24], [43]; the latter, of course, fits the online setting. Instead, we rely on the implicit increase provided by the implicit trick of the kernel, Rahimi and Law in the input data mapping of an end-dimensional euclidean space (with a smaller dimension than H but larger than the input space) using a randomized function card e.g."}, {"heading": "III. DISTRIBUTED KERNEL-BASED LEARNING", "text": "In this section we discuss the problem of online learning in RKHS via the distributed networks."}, {"heading": "A. Consensus and regret bound", "text": "(1), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (, (2), (2), (2), (2), (2), (2), (, (2), (2), ("}, {"heading": "B. Diffusion SVM (Pegasos) Algorithm", "text": "The fact is that most of us will be able to hide in the position, in the way that they are able to behave as they have done in the past: in the way that they have done it, in the way that they have done it, in the way that they have done it, in the way that they have done it, in the way that they have done it, in the way that they have done it, in the way that they have done it, in the way that they have done it. \""}, {"heading": "C. Diffusion KLMS", "text": "It is not possible to apply Proposition 1 because it requires a decreasing step size, but we can derive sufficient conditions for consensus based on the results of the standard diffusion LMS [8]. We assume that the data pairs are generated by the data pairs."}, {"heading": "IV. REVISITING ONLINE KERNEL BASED LEARNING", "text": "In this section we examine the use of random Fourier features as a general framework for online-based learning processes. (1) Example 12000 4000 3000 3500 4000 4500 5000 \u2212 20 \u2212 18 \u2212 16 \u2212 14 \u2212 12 \u2212 10 300 500 600 800 800 800 900 \u2212 28 \u2212 24 \u2212 22 \u2212 20 300 400 500 500 \u2212 35 \u2212 30 \u2212 30 25 \u2212 20 \u2212 15 10noncooperative diffusion (c) Example 3100 200 300 500 800 800 900 \u2212 20 \u2212 16noncooperative diffusion (d) Example 4Fig. 1. Comparison of the performance of RFF Diffusion KLMS against the non-cooperative strategy. Algorithm 2 Random Fourier features online effusion (d) Example 4Fig."}, {"heading": "V. CONCLUSION", "text": "We have presented a complete fixed-budget framework for non-linear online distributed learning in the context of the RKHS. The proposed scheme achieves a symptotic consensus under reasonable assumptions. Furthermore, we have shown that the respective remorse limit grows sublinear over time. In the case of a network comprising only one node, the proposed method can be considered as an alternative to a fixed budget for online kernel-based learning. The simulations presented confirm the theoretical results and demonstrate the effectiveness of the proposed schema.10"}, {"heading": "APPENDIX A PROOF OF PROPOSITION 2", "text": "In the following, we will use the notation Lk, n (Lk, n): = L (xk, n, yk, n, \u03b8) to abbreviate the respective equations. \u2212 n (n, n, n, n), (n, n), (n), (n, n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n, (n), (n, (n), (, (,), (, (,), (, (, (,), (, (, (,), (, (,, (,), (, (,, (,), (, (,), (, (,), (, (,,), (, (, (,), (, (,), (, (, (,), (, (,), (, (,), (, (,), (, (, (,), (, (,), (, (,), (, (,), (, (,,), (, (,), (, (,), (, (,), (, (,), (, (, (,), (, (,), (,, (, (,), (, (, (,), (,,,,, (, (,,), (, (, (,,), (, (,), (,, (,, (, (,,), (, (, (, (,), (,,, (,, (,), (,,, (, (,), (, (, (,,,, (, (,), (, (, (,,,,,,,,), (, (,), (, (,,"}, {"heading": "APPENDIX B PROOF OF PROPOSITION 3", "text": "For the entire network, the step-by-step update of RFF-DKLMS can be defined as \"Un\" = \"Un\" \u2212 \"Un\" \u2212 \"Un\" \u2212 \"We\" = \"We\" (We \"we\" we \"we\" we \"we\" we. \"(We\" we \"we\" we \"we\" we \"we\" we \"we\" we \"we.\" (We \"we\" we \"we\" we. \") For the entire network, the step-by-step update of RFF-DKLMS can be defined as\" Un \"\u2212\" Un \"and\" we, \"n\" we, \"n\" we, \"we\" we \"we.\" (we \"we\" we \"we\" we. \") as\" We. \"\u2212 n\" We \"We,\" we \"we\" we. \"(we\" we \"we\" we \"we.\")"}, {"heading": "APPENDIX C PROOF OF PROPOSITION 4", "text": "Leave Bn = E [UnU T n], where Un = AUn \u2212 1 \u2212 \u00b5VnV T n AUn \u2212 1 + \u00b5Vn\u03b7n. Taking into account the fact that the noise is i.i.e independent of Un and Vn and that it is close to zero (if D is sufficiently large), we can assume that Bn = ABn \u2212 1A \u2212 \u00b5ABn \u2212 1ATR \u2212 \u00b5RABn \u2212 1AT + \u00b52E + \u00b5 2E [VnV T n AUn \u2212 1U T. For sufficiently small step sizes, the most correct term can be neglected [53], [49], so we can use the simplified form Bn = ABn \u2212 1A T \u2212 \u00b5ABn \u2212 1ATR \u2212 \u00b5RABn \u2212 1AT \u2212 \u00b52\u0441."}], "references": [{"title": "Modeling and optimization for big data analytics:(statistical) learning tools for our era of data deluge", "author": ["K. Slavakis", "G. Giannakis", "G. Mateos"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 5, pp. 18\u201331, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Map-reduce for machine learning on multicore", "author": ["C. Chu", "S.K. Kim", "Y.-A. Lin", "Y. Yu", "G. Bradski", "A.Y. Ng", "K. Olukotun"], "venue": "Advances in neural information processing systems, vol. 19, p. 281, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Big data and cloud computing: current state and future opportunities", "author": ["D. Agrawal", "S. Das", "A. El Abbadi"], "venue": "Proceedings of the 14th International Conference on Extending Database Technology. ACM, 2011, pp. 530\u2013533.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.-Q. Wu", "W. Ding"], "venue": "IEEE transactions on knowledge and data engineering, vol. 26, no. 1, pp. 97\u2013107, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Diffusion adaptation over networks", "author": ["A.H. Sayed"], "venue": "Academic Press Library in Signal Processing, vol. 3, pp. 323\u2013454, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine Learning: A Bayesian and Optimization Perspective", "author": ["S. Theodoridis"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Adaptive robust distributed learning in diffusion sensor networks", "author": ["S. Chouvardas", "K. Slavakis", "S. Theodoridis"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 10, pp. 4692\u20134707, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis", "author": ["C.G. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122\u20133136, July 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "An adaptive projected subgradient approach to learning in diffusion networks", "author": ["R.L. Cavalcante", "I. Yamada", "B. Mulgrew"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 7, pp. 2762\u20132774, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed LMS for consensus-based in-network adaptive processing", "author": ["I.D. Schizas", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 6, pp. 2365\u20132382, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed recursive least-squares for consensus-based in-network adaptive estimation", "author": ["G. Mateos", "I.D. Schizas", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 11, pp. 4583\u20134588, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Parallel and distributed computation: Numerical Methods, 2nd ed", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena-Scientific,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Gossip algorithms for distributed signal processing", "author": ["A.G. Dimakis", "S. Kar", "J.M. Moura", "M.G. Rabbat", "A. Scaglione"], "venue": "Proceedings of the IEEE, vol. 98, no. 11, pp. 1847\u20131864, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1847}, {"title": "Multitask diffusion adaptation over networks", "author": ["J. Chen", "C. Richard", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 16, pp. 4129\u20134144, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed diffusion-based LMS for node-specific adaptive parameter estimation", "author": ["J. Plata-Chaves", "N. Bogdanovic", "K. Berberidis"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 13, pp. 3448\u20133460, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Consensus-based distributed support vector machines", "author": ["P.A. Forero", "A. Cano", "G.B. Giannakis"], "venue": "Journal of Machine Learning Research, vol. 11, no. May, pp. 1663\u20131707, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "On distributed online classification in the midst of concept drifts", "author": ["Z.J. Towfic", "J. Chen", "A.H. Sayed"], "venue": "Neurocomputing, vol. 112, pp. 138\u2013152, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond", "author": ["B. Scholkopf", "A. Smola"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Pattern Recognition, 4 ed", "author": ["S. Theodoridis", "K. Koutroumbas"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "The diffusion-KLMS algorithm", "author": ["R. Mitra", "V. Bhatia"], "venue": "ICIT, 2014, Dec 2014, pp. 256\u2013259.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Diffusion adaptation over networks with kernel least-mean-square", "author": ["W. Gao", "J. Chen", "C. Richard", "J. Huang"], "venue": "CAMSAP, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A diffusion kernel LMS algorithm for nonlinear adaptive networks", "author": ["C. Symeon", "D. Moez"], "venue": "ICASSP, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Random features for large scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, vol. 20, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to support vector machines and other kernel-based learning methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Spline Models for Observational Data, volume 59 of CBMS- NSF Regional Conference Series in Applied Mathematics", "author": ["G. Wahba"], "venue": "Philadelphia: SIAM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "The kernel Least-Mean-Square algorithm", "author": ["W. Liu", "P. Pokharel", "J.C. Principe"], "venue": "IEEE Transanctions on Signal Processing, vol. 56, no. 2, pp. 543\u2013554, Feb. 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Extension of Wirtinger\u2019s Calculus to Reproducing Kernel Hilbert spaces and the complex kernel LMS", "author": ["P. Bouboulis", "S. Theodoridis"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 3, pp. 964\u2013978, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "A sliding-window kernel rls algorithm and its application to nonlinear channel identification", "author": ["S. Van Vaerenbergh", "J. Via", "I. Santamana"], "venue": "ICASSP, vol. 5, may 2006, p. V.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "The kernel recursive least-squares algorithm", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "IEEE Transanctions on Signal Processing, vol. 52, no. 8, pp. 2275\u20132285, Aug. 2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Sliding window generalized kernel affine projection algorithm using projection mappings", "author": ["K. Slavakis", "S. Theodoridis"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 19, p. 183, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Adaptive multiregression in reproducing kernel Hilbert spaces: the multiaccess MIMO channel case", "author": ["K. Slavakis", "P. Bouboulis", "S. Theodoridis"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23(2), pp. 260\u2013276, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "On line kernel-based classification using adaptive projection algorithms", "author": ["K. Slavakis", "S. Theodoridis", "I. Yamada"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 2781\u20132796, Jul. 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Pegasos: primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming, vol. 127, no. 1, pp. 3\u201330, 2011. [Online]. Available: http://dx.doi.org/10.1007/s10107-010-0420-4", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning in reproducing kernel Hilbert spaces", "author": ["K. Slavakis", "P. Bouboulis", "S. Theodoridis"], "venue": "Signal Processing Theory and Machine Learning, ser. Academic Press Library in Signal Processing, R. Chellappa and S. Theodoridis, Eds. Academic Press, 2014, pp. 883\u2013987.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Online prediction of time series data with kernels", "author": ["C. Richard", "J. Bermudez", "P. Honeine"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 3, pp. 1058 \u20131067, march 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Online dictionary learning for kernel LMS", "author": ["W. Gao", "J. Chen", "C. Richard", "J. Huang"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 11, pp. 2765 \u2013 2777, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantized kernel least mean square algorithm", "author": ["B. Chen", "S. Zhao", "P. Zhu", "J. Principe"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 1, pp. 22 \u201332, jan. 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Self-organizing kernel adaptive filtering", "author": ["S. Zhao", "B. Chen", "C. Zheng", "P. Zhu", "J. Principe"], "venue": "EURASIP Journal on Advances in Signal Processing, (to appear).", "citeRegEx": "40", "shortCiteRegEx": null, "year": 0}, {"title": "Using the Nystrom method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "NIPS, vol. 14, 2001, pp. 682 \u2013 688.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2001}, {"title": "On the Nystrom method for approximating a gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "JMLR, vol. 6, pp. 2153 \u2013 2175, 2005.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "Weighted sums of random kitchen sinks: replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, vol. 22, 2009, pp. 1313 \u2013 1320.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "On the error of random Fourier features", "author": ["D.J. Sutherland", "J. Schneider"], "venue": "UAI, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Nystr\u00f6m method vs random Fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y.-F. Li", "M. Mahdavi", "J. Rong", "Z.-H. Zhou"], "venue": "NIPS, vol. 25, 2012, pp. 476\u2013484.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis", "author": ["C.G. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122\u20133136, 2008.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion LMS strategies for distributed estimation", "author": ["F.S. Cattiveli", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1035\u20131048, 2010.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic behavior analysis of the gaussian kernel least-mean-square algorithm", "author": ["W. Parreira", "J. Bermudez", "C. Richard", "J.-Y. Tourneret"], "venue": "Signal Processing, IEEE Transactions on, vol. 60, no. 5, pp. 2208\u20132222, May 2012.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "Online learning with kernels: Overcoming the growing sum problem", "author": ["A. Singh", "N. Ahuja", "P. Moulin"], "venue": "MLSP, September 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient KLMS and KRLS algorithms: A random Fourier feature perspective", "author": ["P. Bouboulis", "S. Pougkakiotis", "S. Theodoridis"], "venue": "SSP, 2016.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Digital Signal Processing Fundamentals", "author": ["S.C. Douglas", "M. Rupp"], "venue": "ch. Convergence Issues in the LMS Adaptive Filter,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": ", [1].", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": ", [2], [3], [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": ", [2], [3], [4].", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": ", [2], [3], [4].", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "Finally, the task of interest is considered to be common across the nodes and, thus, cooperation among each other is meaningful and beneficial, [5], [6].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Finally, the task of interest is considered to be common across the nodes and, thus, cooperation among each other is meaningful and beneficial, [5], [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": ", [7], [8], [9], ADMM-based schemes, e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [7], [8], [9], ADMM-based schemes, e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": ", [7], [8], [9], ADMM-based schemes, e.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": ", [10], [11], as well as consencus-based ones, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [10], [11], as well as consencus-based ones, e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": ", [12], [13].", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": ", [12], [13].", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": ", [14], [15].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [14], [15].", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "The literature on online distributed classification is more limited; in [16], a batch distributed SVM algorithm is presented, whereas in [17], a diffusion based scheme suitable for classification is proposed.", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "The literature on online distributed classification is more limited; in [16], a batch distributed SVM algorithm is presented, whereas in [17], a diffusion based scheme suitable for classification is proposed.", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].", "startOffset": 213, "endOffset": 217}, {"referenceID": 19, "context": "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].", "startOffset": 219, "endOffset": 223}, {"referenceID": 5, "context": "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].", "startOffset": 225, "endOffset": 228}, {"referenceID": 20, "context": ", [21], [22], [23], these have major drawbacks.", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": ", [21], [22], [23], these have major drawbacks.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": ", [21], [22], [23], these have major drawbacks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "In [21] and [23], the estimation of f , at each node, is given as an increasingly growing sum of kernel functions centered at the observed data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [21] and [23], the estimation of f , at each node, is given as an increasingly growing sum of kernel functions centered at the observed data.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "In contrast, the method of [22] assumes that these growing sums are limited by a sparsification strategy; how this can be achieved is left for the future.", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "This is achieved through an efficient approximation of the growing sum using the random Fourier features rationale [24].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "This is usually portrayed by the reproducing property [18], [6], [25], which links inner products in H with a specific (semi-)positive definite kernel function \u03ba defined on X \u00d7X (associated with the space H).", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "This is usually portrayed by the reproducing property [18], [6], [25], which links inner products in H with a specific (semi-)positive definite kernel function \u03ba defined on X \u00d7X (associated with the space H).", "startOffset": 60, "endOffset": 63}, {"referenceID": 24, "context": "This is usually portrayed by the reproducing property [18], [6], [25], which links inner products in H with a specific (semi-)positive definite kernel function \u03ba defined on X \u00d7X (associated with the space H).", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": ", N, xn \u2208 X, yn \u2208 R}, the representer theorem [26], [18], states that the unique", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": ", N, xn \u2208 X, yn \u2208 R}, the representer theorem [26], [18], states that the unique", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 129, "endOffset": 133}, {"referenceID": 33, "context": "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.", "startOffset": 177, "endOffset": 181}, {"referenceID": 5, "context": ", [6]), the estimate at the next iteration, employing the gradient descent method, becomes fn = fn\u22121+\u03bc\u01ebn\u03ba(xn, \u00b7), where \u01ebn = yn\u2212fn\u22121(xn) and \u03bc is the step-size (see, e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [6], [35], [36] for more).", "startOffset": 2, "endOffset": 5}, {"referenceID": 34, "context": ", [6], [35], [36] for more).", "startOffset": 7, "endOffset": 11}, {"referenceID": 35, "context": "Typically, a sparsification strategy is adopted to bound the size of the expansion [37], [38], [39].", "startOffset": 83, "endOffset": 87}, {"referenceID": 36, "context": "Typically, a sparsification strategy is adopted to bound the size of the expansion [37], [38], [39].", "startOffset": 89, "endOffset": 93}, {"referenceID": 37, "context": "Typically, a sparsification strategy is adopted to bound the size of the expansion [37], [38], [39].", "startOffset": 95, "endOffset": 99}, {"referenceID": 38, "context": "There are also methods that can remove specific points from the expansion, if their information becomes obsolete, in order to increase the tracking ability of the algorithm [40].", "startOffset": 173, "endOffset": 177}, {"referenceID": 39, "context": "The most popular techniques of this category are the Nystr\u00f6m method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.", "startOffset": 68, "endOffset": 72}, {"referenceID": 40, "context": "The most popular techniques of this category are the Nystr\u00f6m method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "The most popular techniques of this category are the Nystr\u00f6m method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.", "startOffset": 120, "endOffset": 124}, {"referenceID": 41, "context": "The most popular techniques of this category are the Nystr\u00f6m method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.", "startOffset": 126, "endOffset": 130}, {"referenceID": 23, "context": "Instead of relying on the implicit lifting, \u03a6, provided by the kernel trick, Rahimi and Recht in [24] proposed to map the input data to a finite-dimensional Euclidean space (with dimension lower than H but larger than the input space) using a randomized feature map z\u03a9 : R d \u2192 R, so that the kernel evaluations can be approximated as \u03ba(xn,xm) \u2248 z\u03a9(xn) z\u03a9(xm).", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "Details on the quality of this approximation can be found in [24], [43], [44], [45].", "startOffset": 61, "endOffset": 65}, {"referenceID": 41, "context": "Details on the quality of this approximation can be found in [24], [43], [44], [45].", "startOffset": 67, "endOffset": 71}, {"referenceID": 42, "context": "Details on the quality of this approximation can be found in [24], [43], [44], [45].", "startOffset": 73, "endOffset": 77}, {"referenceID": 43, "context": "Details on the quality of this approximation can be found in [24], [43], [44], [45].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "This is the rationale adopted in [21], [22], [23] for the case of KLMS.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "This is the rationale adopted in [21], [22], [23] for the case of KLMS.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "This is the rationale adopted in [21], [22], [23] for the case of KLMS.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "As A is doubly stochastic, we have the following [9]:", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "In [9], it has been proved that, the algorithmic scheme achieves asymptotic consensus, i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 33, "context": ", L(x, y, \u03b8) = \u03bb2 \u2016\u03b8\u20162 +max{0, 1\u2212 y\u03b8z\u03a9(x)}, for a specific value of the regularization parameter \u03bb > 0, generates the Distributed Pegasos (see [34]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 33, "context": "where, following [34], we have used a decreasing step size, \u03bcn = 1 \u03bbn .", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "decreasing step-size \u03bcn), which has been suggested that improves the classification accuracy of Pegasos (see [34]).", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "Although proposition 1 cannot be applied here (as it requires a decreasing step-size), we can derive sufficient conditions for consensus following the results of the standard Diffusion LMS [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 44, "context": ", [48], [49]) cannot be applied directly (in these works the inputs are assumed to be zero mean Gaussian to simplify the formulas related to stability).", "startOffset": 2, "endOffset": 6}, {"referenceID": 45, "context": ", [48], [49]) cannot be applied directly (in these works the inputs are assumed to be zero mean Gaussian to simplify the formulas related to stability).", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "3) Example 3: Here we adopt the following chaotic series model [50]: dk,n = dk,n\u22121 1+d2k,n\u22121 + uk,n\u22121, yk,n = dk,n + \u03b7k,n, where \u03b7n is zero-mean i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 46, "context": "4) Example 4: For the final example, we use another chaotic series model [50]: dk,n = uk,n+0.", "startOffset": 73, "endOffset": 77}, {"referenceID": 47, "context": "We call this scheme the Random Fourier Features KLMS (RFF-KLMS) [51], [52].", "startOffset": 64, "endOffset": 68}, {"referenceID": 48, "context": "We call this scheme the Random Fourier Features KLMS (RFF-KLMS) [51], [52].", "startOffset": 70, "endOffset": 74}, {"referenceID": 37, "context": "We choose the QKLMS [39] as a reference, since this is one of the most effective and fast KLMS pruning methods.", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "has a stabilizing effect on the network [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 49, "context": "For sufficiently small step-sizes, the rightmost term can be neglected [53], [49], hence we can take the simplified form", "startOffset": 71, "endOffset": 75}, {"referenceID": 45, "context": "For sufficiently small step-sizes, the rightmost term can be neglected [53], [49], hence we can take the simplified form", "startOffset": 77, "endOffset": 81}], "year": 2017, "abstractText": "We present a novel diffusion scheme for online kernel-based learning over networks. So far, a major drawback of any online learning algorithm, operating in a reproducing kernel Hilbert space (RKHS), is the need for updating a growing number of parameters as time iterations evolve. Besides complexity, this leads to an increased need of communication resources, in a distributed setting. In contrast, the proposed method approximates the solution as a fixed-size vector (of larger dimension than the input space) using Random Fourier Features. This paves the way to use standard linear combine-then-adapt techniques. To the best of our knowledge, this is the first time that a complete protocol for distributed online learning in RKHS is presented. Conditions for asymptotic convergence and boundness of the networkwise regret are also provided. The simulated tests illustrate the performance of the proposed scheme.", "creator": "LaTeX with hyperref package"}}}