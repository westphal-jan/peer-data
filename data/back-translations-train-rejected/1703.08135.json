{"id": "1703.08135", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "An embedded segmental K-means model for unsupervised segmentation and clustering of speech", "abstract": "Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most competitive approaches lie at methodological extremes: some follow a Bayesian approach, defining probabilistic models with convergence guarantees, while others opt for more efficient heuristic techniques. Here we introduce an approximation to a segmental Bayesian model that falls in between, with a clear objective function but using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental k-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. On English and Xitsonga data, ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being five times faster with fewer hyperparameters. However, there is a trade-off in cluster purity, with the Bayesian model's purer clusters yielding about 10% better unsupervised word error rates.", "histories": [["v1", "Thu, 23 Mar 2017 16:45:22 GMT  (527kb,D)", "http://arxiv.org/abs/1703.08135v1", "5 pages, 3 figures, 2 tables"], ["v2", "Tue, 5 Sep 2017 14:14:11 GMT  (272kb,D)", "http://arxiv.org/abs/1703.08135v2", "8 pages, 3 figures, 3 tables; accepted to ASRU 2017"]], "COMMENTS": "5 pages, 3 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["herman kamper", "karen livescu", "sharon goldwater"], "accepted": false, "id": "1703.08135"}, "pdf": {"name": "1703.08135.pdf", "metadata": {"source": "CRF", "title": "An embedded segmental k-means model for unsupervised segmentation and clustering of speech", "authors": ["Herman Kamper", "Karen Livescu", "Sharon Goldwater"], "emails": ["kamperh@ttic.edu,", "klivescu@ttic.edu,", "sgwater@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2.2. Segmentation", "text": "Under a fixed cluster of z, the target (1) is the minimum Q \u00b2 x \u00b2 (Q) len (x) | | x \u2212 \u00b5 \u0445 x | | 2 = min Q \u00b2 x \u00b2 (Q) d (x) (2), where \u00b5 \u00b2 x is the mean of the cluster currently assigned to x (according to z), and d (x), len (x) | x \u2212 \u03bc x | | 2 is the \"value\" of the embedding x (lower d is better). Equation (2) can be optimized separately for each expression, so we want to find the segmentation q for each expression that yields the minimum sum of the values of the embedding values under this segmentation. Exactly, the problem addressed by the shortest path algorithm (Viterbi) that uses dynamic programming to efficiently solve this problem [33, \u00a7 21.7].Let qt define the number of frames in the hypothesized segment (j), which is the box that ends with: t = t."}, {"heading": "2.3. Cluster assignments and mean updates", "text": "For a fixed segmentationQ, the target (1) means: min z K \u00b2 c = 1 x x x Xc \u00b2 X (Q) len (x) | x \u2212 \u00b5c | | 2 (4) If the means {\u00b5c} K = 1 are fixed, the optimal reallocations (5) follow the standard k averages and are guaranteed to improve (1), since the distance between the embedding and the assigned cluster never increases: zi = argmin c (xi) | xi \u2212 \u00b5c | 2} = argmin c | | xi \u2212 \u00b5c | 2 (5) Finally, we fix the allocations z \u2212 and update the means: \u00b5c = 1 \u0445Xc \u2212 len (x)."}, {"heading": "2.4. The Bayesian embedded segmental GMM", "text": "In previous work [20, 22] we proposed a very similar model, but instead of k-means we used a Bayesian GMM as a full-word cluster component (Figure 1). This Bayesian embedded segmental GMM (BES-GMM) served as inspiration for ES-KMeans; we briefly discuss their relationship here. A Bayesian GMM treats its mixture weights and components as random variables and not as point counters as is done in a regular GMM. We use conjugation priors: a dirichlet before over \u03c0 and a spherical covariance Gaussian before over \u00b5c. All components share the same fixed spherical covariant matrix 2I. The model is then formally defined as: \u03c0-Dir (a / K1) (7) zi-ceptional components referring to individual segments (8) \u00b5c-covariants Gaussian all share the same coherical I."}, {"heading": "2.5. Heuristic recurring syllable-unit word segmentation", "text": "Their system, which we call SylSeg, is based on a novel, cognitively motivated, unsupervised method that predicts the boundaries of syllable-like units and then clusters these units per loudspeaker. By means of a greedy bottom-up mapping, recurring syllable cluster sequences are then predicted as words. SylSeg has the advantage that it is much simpler in terms of computational complexity and implementation than ES-KMeans or BES-GMM. But unlike the heuristic methodology followed in SylSeg, both ES-KMeans and BES-GMM have clear objective functions that they optimize, one by hard clustering, the other by Bayesian inference."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experimental setup and evaluation", "text": "The evaluation is based on the two ZRSC datasets: an English corpus of about 5 hours of speech by 12 speakers and a Xitsonga corpus of about 2.5 hours by 24 speakers [37]. We also use a separate English corpus of about 6 hours for development. As in [20, 22, 38], we use several metrics to evaluate the forced alignment of the basic truth. By assigning each discovered word to the truth sign with which it most overlaps, and then assigning each cluster to its most common word, the average cluster purity and the unattended word error rate (WHO) can be calculated. 2 By assigning each symbol to the true phoneme sequence with which it most overlaps, the normalized edit distance (NED) between all segments in the same cluster can be calculated accurately."}, {"heading": "3.2. Results and analysis", "text": "The results of SylSeg are unknown as they were not part of the ZRSC evaluation. However, compared to BES-GMM, ES-KMeans2We achieves more than one cluster that can be assigned to the same word. Table 1: (a) Performance of models on the two test corporations. Lower NED is better. Runtimes for SylSeg are rough estimates derived from personal communication with the authors. (b) English development assumes the performance of BES-GMM as a variance. (a) English (%) Xitsonga (%) SylSeg ES-KMeans BES-GMMM is different."}], "references": [{"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Commun., vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner", "author": ["E. Dupoux"], "venue": "arXiv preprint arXiv:1607.08723, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of acoustic sub-word units", "author": ["B. Varadarajan", "S. Khudanpur", "E. Dupoux"], "venue": "Proc. ACL, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian phonotactic language model for acoustic unit discovery", "author": ["L. Ondel", "L. Burget", "J. Cernocky", "S. Kesiraju"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "An auto-encoder based approach to unsupervised learning of subword units", "author": ["L. Badino", "C. Canevari", "L. Fadiga", "G. Metta"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint learning of speaker and phonetic similarities with Siamese networks", "author": ["N. Zeghidour", "G. Synnaeve", "N. Usunier", "E. Dupoux"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised training of an HMM-based speech recognizer for topic classification", "author": ["H. Gish", "M.-H. Siu", "A. Chan", "B. Belfield"], "venue": "Proc. Interspeech, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Topic identification of spoken documents using unsupervised acoustic unit discovery", "author": ["S. Kesiraju", "R. Pappagari", "L. Ondel", "L. Burget"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "A parallelized dynamic programming approach to zero resource spoken term discovery", "author": ["B. Oosterveld", "R. Veale", "M. Scheutz"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "The Zero Resource Speech Challenge 2015: Proposed approaches and results", "author": ["M. Versteegh", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "A segmental framework for fully-unsupervised large-vocabulary speech recognition", "author": ["H. Kamper", "A. Jansen", "S. Goldwater"], "venue": "arXiv preprint arXiv:1606.06950, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllablelike units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A modified K-means clustering algorithm for use in isolated work recognition", "author": ["J.G. Wilpon", "L.R. Rabiner"], "venue": "IEEE Trans. Acoust. Speech Signal Process., vol. 33, no. 3, pp. 587\u2013594, 1985.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1985}, {"title": "A segmental kmeans training procedure for connected word recognition", "author": ["L.R. Rabiner", "J.G. Wilpon", "B.-H. Juang"], "venue": "AT&T Tech. J., vol. 65, no. 3, pp. 21\u201331, 1986.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}, {"title": "The segmental K-means algorithm for estimating parameters of hidden Markov models", "author": ["B.-H. Juang", "L.R. Rabiner"], "venue": "IEEE Trans. Acoust. Speech Signal Process., vol. 38, no. 9, pp. 1639\u20131641, 1990.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "On the use of acoustic unit discovery for language recognition", "author": ["S.H. Shum", "D.F. Harwath", "N. Dehak", "J.R. Glass"], "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. 24, no. 9, pp. 1665\u20131676, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of audio segment representations using sequence-tosequence recurrent neural networks", "author": ["Y.-A. Chung", "C.-C. Wu", "C.-H. Shen", "H.-Y. Lee"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative acoustic word embeddings: Recurrent neural network-based approaches", "author": ["S. Settle", "K. Livescu"], "venue": "Proc. SLT, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-view recurrent neural acoustic word embeddings", "author": ["W. He", "W. Wang", "K. Livescu"], "venue": "Proc. ICLR, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end ASR-free keyword search from speech", "author": ["K. Audhkhasi", "A. Rosenberg", "A. Sethy", "B. Ramabhadran", "B. Kingsbury"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Algorithms, etc.", "author": ["J. Erickson"], "venue": "UIUC Lecture Notes,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Gibbs sampling for the uninitiated", "author": ["P. Resnik", "E. Hardisty"], "venue": "University of Maryland, College Park, MD, Tech. Rep., 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Revisiting k-means: New algorithms via Bayesian nonparametrics", "author": ["B. Kulis", "M.I. Jordan"], "venue": "Proc. ICML, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "The Zero Resource Speech Challenge 2015", "author": ["M. Versteegh", "R. Thiolli\u00e8re", "T. Schatz", "X.N. Cao", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Bridging the gap between speech technology and natural language processing: An evaluation toolbox for term discovery systems", "author": ["B. Ludusan", "M. Versteegh", "A. Jansen", "G. Gravier", "X.-N. Cao", "M. Johnson", "E. Dupoux"], "venue": "Proc. LREC, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Conjugate Bayesian analysis of the Gaussian distribution", "author": ["K.P. Murphy"], "venue": "2007. [Online]. Available: http://www.cs.ubc.ca/ \u223cmurphyk/mypapers.html", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": ", unwritten or endangered languages [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "In addition, such methods may shed light on how human infants acquire language [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "In addition, such methods may shed light on how human infants acquire language [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 3, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 81, "endOffset": 86}, {"referenceID": 5, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 81, "endOffset": 86}, {"referenceID": 6, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 125, "endOffset": 130}, {"referenceID": 7, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 125, "endOffset": 130}, {"referenceID": 8, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 125, "endOffset": 130}, {"referenceID": 9, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 156, "endOffset": 164}, {"referenceID": 10, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 156, "endOffset": 164}, {"referenceID": 11, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 185, "endOffset": 192}, {"referenceID": 12, "context": "Several zero-resource tasks have been studied, including acoustic unit discovery [4\u20136], unsupervised representation learning [7\u20139], query-by-example search [10, 11] and topic modelling [12,13].", "startOffset": 185, "endOffset": 192}, {"referenceID": 13, "context": "Early work mainly focused on unsupervised term discovery, where the aim is to automatically find repeated wordor phrase-like patterns in a collection of speech [14\u201316].", "startOffset": 160, "endOffset": 167}, {"referenceID": 14, "context": "Early work mainly focused on unsupervised term discovery, where the aim is to automatically find repeated wordor phrase-like patterns in a collection of speech [14\u201316].", "startOffset": 160, "endOffset": 167}, {"referenceID": 15, "context": "Early work mainly focused on unsupervised term discovery, where the aim is to automatically find repeated wordor phrase-like patterns in a collection of speech [14\u201316].", "startOffset": 160, "endOffset": 167}, {"referenceID": 16, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 17, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 18, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 19, "context": "This has prompted several studies on full-coverage approaches, where the entire speech input is segmented and clustered into word-like units [17\u201320].", "startOffset": 141, "endOffset": 148}, {"referenceID": 20, "context": "Two such full-coverage approaches have recently been applied to the data of the Zero Resource Speech Challenge 2015 (ZRSC), giving a useful basis for comparison [21].", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "The first is the Bayesian embedded segmental Gaussian mixture model (BES-GMM) [22]: a probabilistic model that represents potential word segments as fixed-dimensional acoustic word embeddings, and then builds a whole-word acoustic model in this space while jointly doing segmentation.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The second is the recurring syllableunit segmenter (SylSeg) [23], which is a cognitively motivated, fast, heuristic method that applies unsupervised syllable segmentation and clustering and then predicts recurring syllable sequences as words.", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 198, "endOffset": 209}, {"referenceID": 18, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 198, "endOffset": 209}, {"referenceID": 21, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 198, "endOffset": 209}, {"referenceID": 17, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 267, "endOffset": 275}, {"referenceID": 22, "context": "These two models are representative of two methodological extremes often used in zero-resource systems: either a Bayesian approach is used, defining probabilistic models with convergence guarantees [6, 19, 22], or heuristic techniques are used in pipeline approaches [18, 23].", "startOffset": 267, "endOffset": 275}, {"referenceID": 23, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 111, "endOffset": 118}, {"referenceID": 24, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 111, "endOffset": 118}, {"referenceID": 25, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 111, "endOffset": 118}, {"referenceID": 26, "context": "Hard approximations have been used since the start of probabilistic modelling in supervised speech recognition [24\u201326], and also in more recent work to improve the efficiency of an unsupervised Bayesian model [27].", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 27, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 28, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 29, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 30, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 31, "context": "There is a growing focus on such acoustic word embedding methods [11, 28\u201332], since they make it possible to easily and efficiently compare variable-duration speech segments in a fixed-dimensional space.", "startOffset": 65, "endOffset": 76}, {"referenceID": 10, "context": "Here we instead follow an acoustic word embedding approach [11, 28]: an embedding function fe is used to map a variable length speech segment to a single embedding vector x \u2208 R in a fixed-dimensional space, i.", "startOffset": 59, "endOffset": 67}, {"referenceID": 27, "context": "Here we instead follow an acoustic word embedding approach [11, 28]: an embedding function fe is used to map a variable length speech segment to a single embedding vector x \u2208 R in a fixed-dimensional space, i.", "startOffset": 59, "endOffset": 67}, {"referenceID": 19, "context": "In previous work [20, 22], we proposed a very similar model, but instead of k-means, we used a Bayesian GMM as wholeword clustering component (top of Figure 1).", "startOffset": 17, "endOffset": 25}, {"referenceID": 21, "context": "In previous work [20, 22], we proposed a very similar model, but instead of k-means, we used a Bayesian GMM as wholeword clustering component (top of Figure 1).", "startOffset": 17, "endOffset": 25}, {"referenceID": 33, "context": "Under this model, component assignments and a segmentation can be inferred jointly using a collapsed Gibbs sampler [34].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "Full details are given in [20], but the Gibbs sampler looks very similar to Algorithm 1: the Bayesian GMM gives likelihood terms (\u201cscores\u201d) in order to find an optimal segmentation, while the segmentation hypothesizes the boundaries for the word segments which are then clustered using the GMM.", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "5], [36].", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "In a similar way it can be shown that the Gibbs sampling equations for segmentation and component assignments for BES-GMM (as given in [20]) approach (3) and (5), respectively, in the limit \u03c3 \u2192 0, when all other hyperparameters are fixed.", "startOffset": 135, "endOffset": 139}, {"referenceID": 26, "context": "Parallelizing BES-GMM is also possible, but the guarantee of converging to the true posterior distribution is lost [27].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "5 hours from 24 speakers [37].", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "As in [20, 22, 38], we use several metrics to evaluate against ground truth forced alignments.", "startOffset": 6, "endOffset": 18}, {"referenceID": 21, "context": "As in [20, 22, 38], we use several metrics to evaluate against ground truth forced alignments.", "startOffset": 6, "endOffset": 18}, {"referenceID": 37, "context": "As in [20, 22, 38], we use several metrics to evaluate against ground truth forced alignments.", "startOffset": 6, "endOffset": 18}, {"referenceID": 37, "context": "See [38] for full details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "Our implementation of ES-KMeans follows as closely as possible that of BES-GMM in [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 38, "context": "Both use uniform downsampling as embedding function fe: a segment is represented by keeping 10 equally spaced MFCCs and flattening these [39].", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "Both models use unsupervised syllable pre-segmentation [23] to limit allowed word boundaries.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "See [22] for full details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "Some of the SylSeg scores are unknown since these were not part of the ZRSC evaluation [23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "Runtimes for SylSeg\u2217 are rough estimates, obtained from personal communication with the authors [23].", "startOffset": 96, "endOffset": 100}, {"referenceID": 39, "context": "This is because, based on the recommendation in [40], we set the variance of the prior on the component means of BES-GMM as \u03c3 0 = \u03c3/\u03ba0 (see end of \u00a73.", "startOffset": 48, "endOffset": 52}, {"referenceID": 39, "context": "Murphy [40] explains that this coupling is a sensible way to incorporate prior knowledge of the typical spread of data, and here we indeed show how this helps our Bayesian model; this principled way of including priors is not possible in ES-KMeans or SylSeg.", "startOffset": 7, "endOffset": 11}], "year": 2017, "abstractText": "Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most competitive approaches lie at methodological extremes: some follow a Bayesian approach, defining probabilistic models with convergence guarantees, while others opt for more efficient heuristic techniques. Here we introduce an approximation to a segmental Bayesian model that falls in between, with a clear objective function but using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental k-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. On English and Xitsonga data, ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being five times faster with fewer hyperparameters. However, there is a trade-off in cluster purity, with the Bayesian model\u2019s purer clusters yielding about 10% better unsupervised word error rates.", "creator": "LaTeX with hyperref package"}}}