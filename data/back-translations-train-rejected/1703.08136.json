{"id": "1703.08136", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Visually grounded learning of keyword prediction from untranscribed speech", "abstract": "During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance---acting as a spoken bag-of-words classifier---without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. \"man\" and \"person\", making it even more effective as a semantic keyword spotter.", "histories": [["v1", "Thu, 23 Mar 2017 16:46:00 GMT  (569kb,D)", "http://arxiv.org/abs/1703.08136v1", "5 pages, 3 figures, 5 tables"], ["v2", "Thu, 25 May 2017 20:49:15 GMT  (343kb,D)", "http://arxiv.org/abs/1703.08136v2", "5 pages, 3 figures, 5 tables; small updates, added link to code; accepted to Interspeech 2017"]], "COMMENTS": "5 pages, 3 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["herman kamper", "shane settle", "gregory shakhnarovich", "karen livescu"], "accepted": false, "id": "1703.08136"}, "pdf": {"name": "1703.08136.pdf", "metadata": {"source": "CRF", "title": "Visually grounded learning of keyword prediction from untranscribed speech", "authors": ["Herman Kamper", "Shane Settle", "Gregory Shakhnarovich", "Karen Livescu"], "emails": ["klivescu}@ttic.edu"], "sections": [{"heading": null, "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2. Related work", "text": "Our work overlaps with several other research directions. Recent studies have shown that the use of additional visual features from the scene in which speech takes place can improve conventional ASR [20, 21]. These systems continue to rely on marked speech data, while our goal is to ground non-transcribed speech using vision. There is also great interest in the development of language models that can learn in place of exact transcriptions of very loud captions [22-26]. The study of [26] has particularly influenced our approach as they build a language system that uses textual BoW captions (\u00a7 3.1). In the visual community, captions have recently received a lot of attention, with the goal of creating a fluent and informative natural language description for a visual scene [27-30.] In the processing of natural language, images have also been used to capture aspects of meaning (semantics) of written language; [36, 36, 36, 32, and 33 have multimodal modeling for images]."}, {"heading": "3. Word prediction from images and speech", "text": "Faced with a body of parallel images and spoken captions that are not labeled with text, we propose a method to train a predictive model for spoken words based on labels derived from visual modality."}, {"heading": "3.1. Model overview", "text": "We use a vision system toar Xiv: 170 3.08 136v 1 [cs.C L] 23 Mar 201 7day I with soft textual labels that give goals to train the language network f (X) to predict which words are present in X. Therefore, the network f (X) acts like a spoken bag-of-words (BoW) classifier (without taking into account the order and quantity of words). No transcriptions are used during the training. When using the trained f (X), only language input is used (and no picture).The approach is illustrated in Figure 1, and below we give full detail.If we knew which words occur in the training X, we could construct a multi-hot vector ybow system."}, {"heading": "3.2. Two convolutional architectures over speech", "text": "We look at two different convolutionary architectures for f (X), both of which deal with the variable number of frames in X by summarizing the entire output of their final convolution. As an input layer, both use a one-dimensional convolution only over time, covering a number of frames and the entire frequency axis, the first architecture is shown schematically in Figure 1. It is a CNN based on [16, 37], consisting of several revolutionary and maximum pooling layers (final pooling covering the entire output), followed by fully connected layers. A sigmoid activation is used for the final output f (X), and intermediate ReLUs. The second architecture is that of Palaz, Synnaeve and Collobert [26], which is referred to as PSC. It was originally developed for the ideal configuration of BoW (\u00a7 3.1), with the aim of not only making spoken configuration, but also to locate where words are spoken."}, {"heading": "3.3. The vision system", "text": "In the caption, the goal is to create a natural language description of a scene [27-30] that accurately describes aspects of the scene (Figure 1, left), a multi-level binary classification task in which we have to predict for each word whether it is suitable for the image. We train our vision tagging system on the Flickr30k dataset [41], which contains 30k images, each with a set of 5 captions, which we convert into a BoW after removing stopwords. Given a limited set of task-specific training data, such as Flickr30k, a common approach is to start with a visual representation learned as part of an end-to-end training, on a larger dataset (possibly for a different task), and then adapt it to the task."}, {"heading": "4.1. Experimental setup", "text": "We train our Word prediction model on the basis of the data set of parallel images and spoken captions of [44], which contains 8000 images with 5 spoken captions each; the audio consists of approximately 37 hours of active speech; the data comes with train, development and test fragments, each containing 30,000, 5000 and 5000 expressions; the language is parameterised as MFCCs with first and second order derivatives, creating 39-dimensional input factors; 2 Utterances longer than 8 s are truncated (99.5% of the expressions are shorter than 8 s); the training images are passed on through the vision system (\u00a7 3.3), which applies the word prediction model f (X) to the unlabelled language. We consider two architectures for f (X), which are referred to as VisionSpeech models."}, {"heading": "4.2. Spoken bag-of-words prediction", "text": "We first consider the task of predicting which words are present in a given test statement. Given the input X, our model gives a score fw [0, 1] for each word w in its vocabulary, and2We have therefore also tried to use filter banks; MFCCs have always worked similarly or better. A child in a black shirt who can remember a boy, a child, a small dog running through a field dog, field, grass, running, runs a snowboarder who can be used for spoken BoW prediction with a person riding a ski lift in the background air, snow, snowboardera white dog with black spots jumping, jumping, jumping, jumping, while the man in a blue sweater climbs down the rocks, climbing, rockthis can be used for spoken BoW prediction. To make a hard prediction, we put a threshold for phases and output labels for everything where fw (X) > \u03b1 is used. We compare the predicted BoW predictions with the prediction of true EW labels in the prediction system."}, {"heading": "4.3. Keyword spotting", "text": "Our model can, of course, also be used as a keyword spotter: Although the goal of a text query is to retrieve all the utterances in the test sentence with spoken instances of that query, we randomly select 20 textual keywords from the vocabulary of the VisionSpeech output as queries. To evaluate, we use three metrics [47, 48]: P @ 10 is the average precision (across keywords, in%) of the 10 suggestions with the highest score; P @ N is the average precision of the top N suggestions with N the number of true occurrences of the keyword; and the same error rate (EER) is the average error rate where false acceptance and false rejection rates are the same. Table 3 shows keyword spotting results. The trend in relative performance is similar to that of Table 1: the un-rambled baseline leads the worst, the VisionSpeech models deliver reasonable values, and the Oracle models perform best."}, {"heading": "4.4. Semantic keyword spotting", "text": "To quantify this problem, we looked at the 10 best suggested expressions for each keyword for each model and called them correct those expressions that either contained keyword variants or were semantically related, allowing us to report P @ 10 for the task of semantic keyword spotting, as shown in Table 5 (the other metrics would require us to label all test expressions semantically) Compared to Table 3, the semantic keyword spotting performance is better than exact keyword spotting values for all models. However, the VisionSpeech models in Table 4 are: Examples of incorrectly retrieved expressions when using VisionSpeechCNN for keyword spotting. Keyword spotting. Example for mismatched expressions TypeModel P @ 10Unigram Baseline 10.0 VisionSpeechCNN 82.5 VisionSpeechPSC 73.0OracleSpeechCNN 99.5 OracleSpeechCNN 100PSC demonstrate the greatest benefit of all, with VisionSpeechCNN 100.030% improved."}, {"heading": "5. Conclusion", "text": "We have introduced a new way of using images to learn from untranscribed language. \"By using a visual image-to-word classifier to provide soft labels for speech, we are able to learn a neural speech-to-keyword prediction system.\" Further analysis shows that the model's errors are often semantic in nature, such as confusing \"boys\" and \"children.\" To quantify this, we rated our model as a semantic keyword spotter, where the task is to find all expressions in a corpus that are semantically related to the textual keyword query. In this setting, our model achieves a semantic P @ 10 of more than 80%."}], "references": [{"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition", "author": ["A. Jansen"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The Zero Resource Speech Challenge 2015: Proposed approaches and results", "author": ["M. Versteegh", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Commun., vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Representations of language in a model of visually grounded speech signal", "author": ["G. Chrupa\u0142a", "L. Gelderloos", "A. Alishahi"], "venue": "arXiv preprint arXiv:1702.01991, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Object category detection using audio-visual cues", "author": ["J. Luo", "B. Caputo", "A. Zweig", "J.-H. Bach", "J. Anem\u00fcller"], "venue": "Proc. ICVS, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Symbol emergence in robotics: A survey", "author": ["T. Taniguchi", "T. Nagai", "T. Nakamura", "N. Iwahashi", "T. Ogata", "H. Asoh"], "venue": "Adv. Robotics, vol. 30, no. 11-12, pp. 706\u2013728, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Infants rapidly learn word-referent mappings via cross-situational statistics", "author": ["L. Smith", "C. Yu"], "venue": "Cognition, vol. 106, no. 3, pp. 1558\u2013 1568, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Effects of visual information on adults and infants auditory statistical learning", "author": ["E.D. Thiessen"], "venue": "Cognitive Sci., vol. 34, no. 6, pp. 1093\u20131106, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Weak semantic context helps phonetic learning in a model of infant language acquisition", "author": ["S. Frank", "N.H. Feldman", "S.J. Goldwater"], "venue": "Proc. ACL, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning words from images and speech", "author": ["G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "NIPS Workshop Learn. Semantics, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["D. Harwath", "A. Torralba", "J.R. Glass"], "venue": "Proc. NIPS, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech-based annotation and retrieval of digital photographs", "author": ["T.J. Hazen", "B. Sherry", "M. Adler"], "venue": "Proc. Interspeech, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Multimodal photo annotation and retrieval on a mobile phone", "author": ["X. Anguera", "J. Xu", "N. Oliver"], "venue": "Proc. ICMIR, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning word-like units from joint audio-visual analysis", "author": ["D. Harwath", "J.R. Glass"], "venue": "arXiv preprint arXiv:1701.07481, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Look, listen, and decode: Multimodal speech recognition with images", "author": ["F. Sun", "D. Harwath", "J.R. Glass"], "venue": "Proc. SLT, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual features for context-aware speech recognition", "author": ["A. Gupta", "Y. Miao", "L. Neves", "F. Metze"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Discovering an optimal set of minimally contrasting acoustic speech units: A point of focus for whole-word pattern matching", "author": ["G. Aimetti", "R.K. Moore", "L. ten Bosch"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Mutually exclusive grounding for weakly supervised non-negative matrix factorisation", "author": ["V. Renkens", "H. Van hamme"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "An attentional model for speech translation without transcription", "author": ["L. Duong", "A. Anastasopoulos", "D. Chiang", "S. Bird", "T. Cohn"], "venue": "Proc. NAACL, 2016, pp. 949\u2013959.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards speech-to-text translation without speech recognition", "author": ["S. Bansal", "H. Kamper", "A. Lopez", "S.J. Goldwater"], "venue": "Proc. EACL, 2017.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Jointly learning to locate and classify words using convolutional networks", "author": ["D. Palaz", "G. Synnaeve", "R. Collobert"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning visually grounded meaning representations", "author": ["C.H. Silberer"], "venue": "Ph.D. dissertation, The University of Edinburgh, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic description generation from images: A survey of models, datasets, and evaluation measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler- Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "J. Artif. Intell. Res., vol. 55, pp. 409\u2013442, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba"], "venue": "Proc. ECCV, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Soundnet: Learning sound representations from unlabeled video", "author": ["Y. Aytar", "C. Vondrick", "A. Torralba"], "venue": "Proc. NIPS, 2016, pp. 892\u2013900.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Sound- Word2Vec: Learning word representations grounded in sounds", "author": ["A.K. Vijayakumar", "R. Vedantam", "D. Parikh"], "venue": "arXiv preprint arXiv:1703.01720, 2017.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "author": ["L. Gelderloos", "G. Chrupa\u0142a"], "venue": "Proc. COLING, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1107\u20131135, 2003.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "Proc. ICCV, 2009.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Trans. ACL, vol. 2, pp. 67\u201378, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. CVPR, 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multimodal semantic embeddings for speech and images", "author": ["D. Harwath", "J. Glass"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi"], "venue": "2015. [Online]. Available: http: //tensorflow.org/", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example spoken term detection using phonetic posteriorgram templates", "author": ["T.J. Hazen", "W. Shen", "C. White"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 1, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 2, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 3, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 4, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 5, "context": "for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 7, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 116, "endOffset": 122}, {"referenceID": 8, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 116, "endOffset": 122}, {"referenceID": 9, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 116, "endOffset": 122}, {"referenceID": 10, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 11, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 12, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 13, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 14, "context": "Specifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled.", "startOffset": 61, "endOffset": 69}, {"referenceID": 15, "context": "Specifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled.", "startOffset": 61, "endOffset": 69}, {"referenceID": 14, "context": "Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18].", "startOffset": 106, "endOffset": 114}, {"referenceID": 17, "context": "The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18].", "startOffset": 106, "endOffset": 114}, {"referenceID": 6, "context": "Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.", "startOffset": 78, "endOffset": 85}, {"referenceID": 18, "context": "Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.", "startOffset": 78, "endOffset": 85}, {"referenceID": 6, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 14, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 15, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 18, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 19, "context": "Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21].", "startOffset": 130, "endOffset": 138}, {"referenceID": 20, "context": "Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21].", "startOffset": 130, "endOffset": 138}, {"referenceID": 21, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 22, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 23, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 24, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 25, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 25, "context": "The study of [26] particularly influenced our approach, since they build a speech system using textual BoW labels (\u00a73.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 27, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 28, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 29, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 30, "context": "In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews.", "startOffset": 126, "endOffset": 134}, {"referenceID": 31, "context": "In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews.", "startOffset": 126, "endOffset": 134}, {"referenceID": 32, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 95, "endOffset": 102}, {"referenceID": 33, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 95, "endOffset": 102}, {"referenceID": 34, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 95, "endOffset": 102}, {"referenceID": 35, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 129, "endOffset": 133}, {"referenceID": 25, "context": "In [26], transcriptions were used to obtain this type of ideal BoW supervision.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "We use a multi-label visual classifier (with parameters \u03b3) which, instead of binary indicators, produces soft targets yvis \u2208 [0, 1] , with yvis,w = P (w|I,\u03b3) the probability of wordw being present given image I .", "startOffset": 125, "endOffset": 131}, {"referenceID": 0, "context": "Note that f(X) is not a distribution over the vocabulary, since any number of terms in the vocabulary can be present in an utterance; rather, each dimension fw(X) can have any value in [0, 1].", "startOffset": 185, "endOffset": 191}, {"referenceID": 15, "context": "It is a CNN based on [16, 37], consisting of several convolution X logsumexp", "startOffset": 21, "endOffset": 29}, {"referenceID": 36, "context": "It is a CNN based on [16, 37], consisting of several convolution X logsumexp", "startOffset": 21, "endOffset": 29}, {"referenceID": 25, "context": "Figure 2: A two-layer Palaz, Synnaeve and Collobert (PSC) network [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "The second architecture is the one from Palaz, Synnaeve and Collobert [26], referred to as PSC.", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "note that this intermediate method improves PSC\u2019s location prediction capability (refer to [26]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 27, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 28, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 29, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 37, "context": "In contrast, rather than a fluent sentence, here we want a vision tagging system [38\u201340] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left).", "startOffset": 81, "endOffset": 88}, {"referenceID": 38, "context": "In contrast, rather than a fluent sentence, here we want a vision tagging system [38\u201340] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left).", "startOffset": 81, "endOffset": 88}, {"referenceID": 39, "context": "We train our vision tagging system on the Flickr30k data set [41], which contains 30k images, each with a set of 5 captions, which we convert into a BoW after removing stop words.", "startOffset": 61, "endOffset": 65}, {"referenceID": 40, "context": "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].", "startOffset": 134, "endOffset": 141}, {"referenceID": 15, "context": "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].", "startOffset": 134, "endOffset": 141}, {"referenceID": 41, "context": "Specifically, we use VGG-16 [43], but replace the final", "startOffset": 28, "endOffset": 32}, {"referenceID": 42, "context": "We train our word prediction model on the data set of parallel images and spoken captions of [44], containing 8000 images with 5 spoken captions each.", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.", "startOffset": 67, "endOffset": 71}, {"referenceID": 43, "context": "All models were implemented in TensorFlow [45].", "startOffset": 42, "endOffset": 46}, {"referenceID": 44, "context": "Based on development tuning, they are trained using Adam [46] for 25 epochs with a minibatch of 128 and a learning rate of 0.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Given input X , our model gives a score fw(X) \u2208 [0, 1] for every word w in its vocabulary, and", "startOffset": 48, "endOffset": 54}, {"referenceID": 45, "context": "For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.", "startOffset": 37, "endOffset": 45}, {"referenceID": 46, "context": "For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.", "startOffset": 37, "endOffset": 45}], "year": 2017, "abstractText": "During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance\u2014acting as a spoken bag-of-words classifier\u2014without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. \u201cman\u201d and \u201cperson\u201d, making it even more effective as a semantic keyword spotter.", "creator": "LaTeX with hyperref package"}}}