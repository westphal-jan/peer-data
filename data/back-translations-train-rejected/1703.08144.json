{"id": "1703.08144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Note Value Recognition for Piano Transcription Using Markov Random Fields", "abstract": "This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.", "histories": [["v1", "Thu, 23 Mar 2017 17:07:14 GMT  (548kb,D)", "http://arxiv.org/abs/1703.08144v1", "12 pages, 15 figures, version submitted to IEEE/ACM TASLP"], ["v2", "Tue, 27 Jun 2017 22:26:42 GMT  (873kb,D)", "http://arxiv.org/abs/1703.08144v2", "13 pages, 16 figures, version accepted to IEEE/ACM TASLP"], ["v3", "Fri, 7 Jul 2017 13:10:15 GMT  (873kb,D)", "http://arxiv.org/abs/1703.08144v3", "13 pages, 16 figures, version accepted to IEEE/ACM TASLP, minor revision"]], "COMMENTS": "12 pages, 15 figures, version submitted to IEEE/ACM TASLP", "reviews": [], "SUBJECTS": "cs.AI cs.SD", "authors": ["eita nakamura", "kazuyoshi yoshii", "simon dixon"], "accepted": false, "id": "1703.08144"}, "pdf": {"name": "1703.08144.pdf", "metadata": {"source": "CRF", "title": "Note Value Recognition for Rhythm Transcription Using a Markov Random Field Model for Musical Scores and Performances of Piano Music", "authors": ["Eita Nakamura", "Simon Dixon"], "emails": ["enakamura@sap.ist.i.kyoto-u.ac.jp."], "sections": [{"heading": null, "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they are moving."}, {"heading": "II. RELATED WORK", "text": "Before starting the main discussion, let's review previous studies related to this paper.There have been many studies on converting MIDI performance signals into a form of score. Previous studies [7], [8] used rule-based methods and networks in attempts to model the process of human perception of musical rhythm.Since about 2000, various statistical models have been proposed to combine the statistical nature of note sequences in scores and that of time fluctuations in music performation.The most popular approach is to use hidden Markov models (HMMs).9] - [12], [16] The score is either described as a Markov process on beat positions (metric Markov model) [11], [12] or a Markov model of notes (Markov model).10], and the performance model is often constructed as a state that describes latent variables describing locally defined tempi."}, {"heading": "III. PRELIMINARY OBSERVATIONS AND ANALYSES", "text": "Below, we explain basic facts about the structure of polyphonic piano scores and discuss how important and non-trivial it is to detect note values for such music based on observations and statistical analysis of music data, which provides ideas for the architecture of our model. Some terms and concepts used in this paper are also presented."}, {"heading": "A. Structure of Polyphonic Musical Scores", "text": "To discuss the recognition of note values in polyphonic piano music, we first explain the structure of polyphonic scores. The left and right voices are usually written in separate staves, and each staff can contain multiple voices or note currents (fig. 2 (a)). In piano scores, each voice can contain chords, and the number of voices can vary locally. We use the word chords to indicate those within a voice. Apart from rare cases of score bindings in chords, notes in a chord follow a rest. Such pauses are rare [14], and therefore the configuration of note values and voice structure may lie between chords. The significance of the vocal structure in the description of note values can also be confirmed by comparing a polyphonic score with a reduced score in a large score, correcting those note values and voice structure in a single score."}, {"heading": "B. Distribution of Durations in Music Performances", "text": "A natural approach to restoring note values from MIDI performances is to find those note values that best correspond to the actual note values in the performances. In this essay, duration always means the time duration measured in physical time, as opposed to a note value that is described in score time / measure units. To correlate durations with note values, you need the (local) tempo that provides the conversion ratio. Although estimating tempi from MIDI performances is not a trivial problem (see Fig. IV), here, for simplicity reasons, we assume they are given. Given a local tempo and a note value, you can calculate an expected duration, and vice versa, you can estimate a note value that indicates a local tempo and an actual duration. Fig. 3 shows distributions of the ratios of actual durations in performances and the durations expected by note values estimated by note values and tempo estimated by fixed times (performance data are described in Deviation Model IV)."}, {"heading": "C. Hints for Constructing a Score Model", "text": "The simplest solution is for it to be able to put itself at the top, and for it to be able to put itself at the top."}, {"heading": "D. Summary of the Section", "text": "Here we summarize the results in this section: \u2022 The vocal structure and configuration of note values are related to each other, and the logical constraints on scores result in an interdependence between note values. \u2022 Note durations listed are very different from those implied by the score, and a score model is critical for accurately estimating note values from performance signals. \u2022 Information on incipient score times provides an efficient search space for note values through the use of IONVs. In particular, the probability that a note value falls into one of the first ten IONVs is quite high. \u2022 The distribution of note values is highly dependent on the pitch context, which would be useful to improve its predictability. For the rest of this paper, we construct a mathematical model to incorporate these results and conduct numerical experiments to examine how they quantitatively affect the accuracy of note value recognition."}, {"heading": "IV. PROPOSED METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem Statement", "text": "In rhythm transcription, input is a MIDI power signal, represented as a sequence of pitches, starting times, and balancing times (pn, tn, toffn, t, off n) n = 1, where n is an index of notes, and N is the number of notes. As explained in Section III-B, we can define two balancing times for each note, the triggering time and the damping cooldown time, denoted by toffn and t, off. The associated duration of holding and raising the dampers is denoted by dn = toffn \u2212 tn and d = t, off \u2212 tn. The goal is to recognize the triggering times of note inflections and deflections, which are typically denoted by (\u03c4n, v.) N n = 1. Generally, we assume that n and prevoffn values in the series of rational numbers in units of the abeat unit, say, the whole note length are recorded."}, {"heading": "B. Estimation of Onset Score Times and Local Tempos", "text": "To reduce the probability of insertion of P (s) = P (sn) Nn = 2 P (sn) Nn = 1 with sn {1,.. G} (G) (G) (G) (n) (G) (n)) (G) (n) (G) (n) (G) (G) (G) (G) (G) (G) (G) (n) (G) (n) (G) (n) (G) (n) (G) (G) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (G) (n) (n) (n) (n) (n) (n) (n) (G) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (p) (n) (n) (n) (n) (n) (p) (n) (n) (n) (n) (n) (n) (p) (n) (n) (n) (n) (p) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (g) (n (n) (n) (n) (n) (n) (n) (n) (n (n) (g) (n) (n) (n (n) (n (n) (n) (n) (n) (n) (g) (n (g) (n) (n) (n (n (n) (n) (n) (n) (n) (n) (n) (n (n (n) (n) (g) (n (g) (n) (n) (n) (n) (n) (n (g) (n) (n) (n) (n) (n (n) (n) (n) (n (n) (n) (n) (n) ("}, {"heading": "C. Markov Random Field Model", "text": "Here we describe our main model, which depends on the respective scores and heights of marks. As explained in Sec. III, it is essential to combine a score model that allows the prediction of mark values (3)."}, {"heading": "D. Model Learning", "text": "In much the same way as the speech model and the acoustic model for a speech recognition system are generally trained separately with different data, our three component models can be trained separately and then combined to determine the optimal weights (\u03b2 s).The context model and the interdependence model can be learned from score data and we used a data set of 148 classical piano pieces (with 3.4 \u00b7 106 annotations) by different composers and different performers1The lists of pieces used for score data and performance data are available on the accompanying website. [27] The data used consists of 180 performances (60 sets \u00b7 3 different players) by different composers and different performers 1The list of pieces used for score data is available on the accompanying website."}, {"heading": "V. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Evaluation Measures", "text": "First, we define weighting measures used in our study. For each note n = 1.,., N, we let rcn and r e n be the correct and estimated note values. Then, the error rate E is defined as E = 1 N \u00b2 n = 1 I (ren 6 = rcn) (26), where I (C) is 1 if condition C is true and 0 otherwise. This measure does not take into account how close the estimate is to the correct value if they are not exactly the same. Alternatively, we can consider the averaged \"distance\" between the estimated and correct note values. As such, we define the following yardstick error S: S = exp [1N \u00b2 n | ln (ren / rcn) |]. (27) The difference and the average is defined in the logarithmic domain to avoid distortion for larger note values. S is unit if all note values are correctly estimated, and for example S = 2 if all estimates are doubled or halved."}, {"heading": "B. Comparative Evaluations", "text": "In this section, we evaluate the proposed method, a previously investigated method [14] and a simple model discussed in Sec. III on our data set and compare them in terms of accuracy of note value identification.1) Setup: To examine the contribution of the component models of our MRF model, we include the complete model, a model without an interdependence model (\u03b22 = 0), a model without the power model (\u03b231 = \u03b232 = 0) and an MRF model with a context model that has no (or a trivial) context tree, all applied to the result of the transcription of the rhythm used by the metric HMM. Furthermore, we evaluate a method based on a simple prior distribution of note values (Fig. 5 (a) combined with an initial probability P (dn; rn, vn) in Eq. (16), which does not use information about the set point number. For comparison, we evaluated the Melisma (version 2)."}, {"heading": "C. Examining the Proposed Model", "text": "This is just one example of the fact that the distribution for the basic truth is essentially the same as that in Figure 5 (b), but a little different because the data is different and the initial annotations here are defined with the result of the one-sided ratio transcription by the metric HMM. Firstly, the model without context tree indicates the first IONV to be highly probable (> 98%), indicating that the estimated results of the model are almost the same as for the monolingual representation in Figure 2. This is consistent with the results in Fig. 11 that this model tends to estimate note values shorter than the correct values. Secondly, it can be noted that the model without the performance model has a higher probability for the first IONV and lower probabilities for most of the later IONVs."}, {"heading": "VI. CONCLUSION AND DISCUSSION", "text": "As suggested in the discussion in Section III and confirmed by evaluation results, listed note values may differ greatly from the specified note lengths, and therefore the Aline performance model has little predictive power. Constructing the score model is then the key to solving the problem. We formulated a context-tree model that can learn highly predictable distributions of note values from the data, using specified score times and pitch context. It has been shown that this score model brings significant improvements in detection accuracy. Refinement of the score model is possible in many respects, and the use of additional features for the context-tree model could improve the results. Similarly, the use of other function-based learning schemes, such as deeper neural networks, is possible. Refinement and expansion of the search space for note values is another problem, as the set of the first ten IONVs used in this study is a certain proportion of solutions lost. The result is that the tuning model is learned to construct the tone tree structure, which is a basis for learning the pitch structure."}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank David Temperley for providing the source code for the Melisma Analyzer. E. Nakamura thanks Shinji Takaki for useful discussions about context tree clustering. This work is partially supported by the projects JSPS KAKENHI no. 24220006, 26280089, 26700020, 15K16054, 16H01744 and 16J05486, JST OngaCREST and OngaACCEL as well as the long-term research fund of the Telecommunications Advancement Foundation."}], "references": [{"title": "Automatic Music Transcription: Challenges and Future Directions,", "author": ["E. Benetos", "S. Dixon", "D. Giannoulis", "H. Kirchhoff", "A. Klapuri"], "venue": "J. Intelligent Information Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Adaptive Harmonic Spectral Decomposition for Multiple Pitch Estimation,", "author": ["E. Vincent", "N. Bertin", "R. Badeau"], "venue": "IEEE TASLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Polyphonic Piano Transcription Using Non-Negative Matrix Factorisation with Group Sparsity,", "author": ["K. O\u2019Hanlon", "M.D. Plumbley"], "venue": "Proc. ICASSP, pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Infinite Superimposed Discrete All-Pole Modeling for Source-Filter Decomposition of Wavelet Spectrograms,", "author": ["K. Yoshii", "K. Itoyama", "M. Goto"], "venue": "Proc. ISMIR, pp. 86\u201392,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "An End-to-End Neural Network for Polyphonic Piano Music Transcription,", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "IEEE/ACM TASLP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Mental Processes: Studies in Cognitive Science", "author": ["H. Longuet-Higgins"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "The Quantization of Musical Time: A Connectionist Approach,", "author": ["P. Desain", "H. Honing"], "venue": "Comp. Mus. J.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "A Hybrid Graphical Model for Rhythmic Parsing,", "author": ["C. Raphael"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Hidden Markov Model for Automatic Transcription of MIDI Signals,", "author": ["H. Takeda", "T. Otsuki", "N. Saito", "M. Nakai", "H. Shimodaira", "S. Sagayama"], "venue": "Proc. MMSP, pp", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "A Learning-Based Quantization: Unsupervised Estimation of the Model Parameters,", "author": ["M. Hamanaka", "M. Goto", "H. Asoh", "N. Otsu"], "venue": "Proc. ICMC,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization,", "author": ["A. Cemgil", "B. Kappen"], "venue": "J. Artificial Intelligence Res.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Signal-to-Score Music Transcription Using Graphical Models,", "author": ["E. Kapanci", "A. Pfeffer"], "venue": "Proc. IJCAI, pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "A Unified Probabilistic Model for Polyphonic Music Analysis,", "author": ["D. Temperley"], "venue": "J. New Music Res.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Probabilistic Model of Two-Dimensional Rhythm Tree Structure Representation for Automatic Transcription of Polyphonic MIDI Signals,", "author": ["M. Tsuchiya", "K. Ochiai", "H. Kameoka", "S. Sagayama"], "venue": "Proc. APSIPA, pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices,", "author": ["E. Nakamura", "K. Yoshii", "S. Sagayama"], "venue": "IEEE/ACM TASLP, to appear,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Automatic Extraction of Tempo and Beat from Expressive Performances,", "author": ["S. Dixon"], "venue": "J. New Music Res.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Simultaneous Beat and Downbeat- Tracking Using a Probabilistic Framework: Theory and Large-Scale Evaluation,", "author": ["G. Peeters", "H. Papadopoulos"], "venue": "IEEE TASLP, vol. 19,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Inferring Metrical Structure in Music Using Particle Filters,", "author": ["F. Krebs", "A. Holzapfel", "A.T. Cemgil", "G. Widmer"], "venue": "IEEE/ACM TASLP, vol. 23,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Downbeat Tracking with Multiple Features and Deep Neural Networks,", "author": ["S. Durand", "J.P. Bello", "B. David", "G. Richard"], "venue": "Proc. ICASSP,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Context-Free 2D Tree Structure Model of Musical Notes for Bayesian Modeling of Polyphonic Spectrograms,", "author": ["H. Kameoka", "K. Ochiai", "M. Nakano", "M. Tsuchiya", "S. Sagayama"], "venue": "Proc. ISMIR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling,", "author": ["S. Raczynski", "E. Vincent", "S. Sagayama"], "venue": "IEEE TASLP, vol. 21,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Tree-Based State Tying for High Accuracy Acoustic Modelling", "author": ["S. Young", "J.J. Odell", "P. Woodland"], "venue": "Proc. Human Lang. Techno.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Contextual Additive Structure for HMM-Based Speech Synthesis,", "author": ["S. Takaki", "Y. Nankaku", "K. Tokuda"], "venue": "IEEE J. Selected Topics in Signal Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "MDL-Based Context-Dependent Subword Modeling for Speech Recognition", "author": ["K. Shinoda", "T. Watanabe"], "venue": "J. Acoustical Soc. Japan (E),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Note Value Recognition for Polyphonic Music", "author": ["E. Nakamura", "K. Yoshii", "S. Dixon"], "venue": "[Online]. Available: http://anonymous574868. github.io/demo.html, Accessed on: Mar", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "An Introduction to the Application of the Theory of Probabilistic Functions of a Markov Process to Automatic Speech Recognition,", "author": ["S. Levinson", "L. Rabiner", "M. Sondhi"], "venue": "The Bell Sys. Tech. J., vol. 62,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1983}, {"title": "Voice and Stream: Perceptual and Computational Modeling of Voice Separation,", "author": ["E. Cambouropoulos"], "venue": "Music Perception,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "HMM-Based Voice Separation of MIDI Performance,", "author": ["A. McLeod", "M. Steedman"], "venue": "J. New Music Res.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Merged-Output HMM for Piano Fingering of Both Hands,", "author": ["E. Nakamura", "N. Ono", "S. Sagayama"], "venue": "Proc. ISMIR, pp", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Rhythm Transcription of MIDI Performances Based on Hierarchical Bayesian Modelling of Repetition and Modification of Musical Note Patterns,", "author": ["E. Nakamura", "K. Itoyama", "K. Yoshii"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1946}, {"title": "Universal Coding, Information, Prediction, and Estimation", "author": ["J. Rissanen"], "venue": "IEEE TIT,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1984}, {"title": "On Tempo Tracking: Tempogram Representation and Kalman Filtering,", "author": ["A.T. Cemgil", "B. Kappen", "P. Desain", "H. Honing"], "venue": "J. New Music Res.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Music transcription is one of the most fundamental and challenging problems in music information processing [1], [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Pitch analysis aims to convert the audio signals into the form of a piano roll, which can be represented as a MIDI signal, and multi-pitch analysis methods for polyphonic music have been extensively studied [3]\u2013[6].", "startOffset": 207, "endOffset": 210}, {"referenceID": 4, "context": "Pitch analysis aims to convert the audio signals into the form of a piano roll, which can be represented as a MIDI signal, and multi-pitch analysis methods for polyphonic music have been extensively studied [3]\u2013[6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "Rhythm transcription, on the other hand, aims to convert a MIDI signal into a musical score by locating note onsets and offsets in a musical time direction (score time) [7]\u2013[16].", "startOffset": 169, "endOffset": 172}, {"referenceID": 14, "context": "Rhythm transcription, on the other hand, aims to convert a MIDI signal into a musical score by locating note onsets and offsets in a musical time direction (score time) [7]\u2013[16].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "In order to track time-varying tempo, beat tracking is employed to locate beat positions in music audio signals [17]\u2013[21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "Note value recognition has been addressed only in a few studies [10], [14] and the results of this study reveal that it is a non-trivial problem.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Note value recognition has been addressed only in a few studies [10], [14] and the results of this study reveal that it is a non-trivial problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 180, "endOffset": 183}, {"referenceID": 12, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 185, "endOffset": 189}, {"referenceID": 14, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 19, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 197, "endOffset": 201}, {"referenceID": 20, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 203, "endOffset": 207}, {"referenceID": 21, "context": "To determine an optimal set of contexts/features for the score model from data, we develop a statistical learning method based on context-tree clustering [24]\u2013[26], which is an adaptation of statistical decision tree analysis.", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "To determine an optimal set of contexts/features for the score model from data, we develop a statistical learning method based on context-tree clustering [24]\u2013[26], which is an adaptation of statistical decision tree analysis.", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "Lastly, source code of our algorithms and evaluation tools is available from the accompanying web page [27] to facilitate future comparisons and applications.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "Older studies [7], [8] used rule-based methods and networks in attempts to model the process of human perception of musical rhythm.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Older studies [7], [8] used rule-based methods and networks in attempts to model the process of human perception of musical rhythm.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "The most popular approach is to use hidden Markov models (HMMs) [9]\u2013[12], [16].", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "The most popular approach is to use hidden Markov models (HMMs) [9]\u2013[12], [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "The most popular approach is to use hidden Markov models (HMMs) [9]\u2013[12], [16].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": "Recently a merged-output HMM incorporating the multiple-voice structure has been proposed [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "Temperley [14] proposed a score model similar to the metrical Markov model in which the hierarchical metrical structure is explicitly described.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "There are also studies that investigated probabilistic context-free grammar models [15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "A recent study [16] reported results of systematic evaluation of (onset) rhythm transcription methods.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "[10] applied a similar method of estimating onset score times to estimating note values of monophonic performances and reported that the recognition accuracy dropped from 97.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Temperley\u2019s Melisma Analyzer [14], based on a statistical model, outputs estimated onset and offset beat positions together with voice information for polyphonic music.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "Such rests are rare [14] and thus the configuration of note values and the voice structure are inter-related.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "As mentioned previously [12], [14], this makes note value recognition a difficult problem and it has often been avoided in previous studies.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "As mentioned previously [12], [14], this makes note value recognition a difficult problem and it has often been avoided in previous studies.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "Similarly to using a prior language model, which was the key to improve the accuracy of speech recognition [28], a prior model for musical scores (score model) would be a key to solving our problem, which we seek in this paper.", "startOffset": 107, "endOffset": 111}, {"referenceID": 26, "context": "This is because neighbouring notes (either horizontally or vertically) in a voice tend to have close pitches, as discussed in studies on voice separation [29]\u2013[31].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "This is because neighbouring notes (either horizontally or vertically) in a voice tend to have close pitches, as discussed in studies on voice separation [29]\u2013[31].", "startOffset": 159, "endOffset": 163}, {"referenceID": 7, "context": "To estimate onset score times \u03c4 and local tempos v from a MIDI performance (p, t, t , t\u0304), we use a metrical HMM [9], which is one of the most accurate onset rhythm transcription methods (Sec.", "startOffset": 113, "endOffset": 116}, {"referenceID": 29, "context": "An extension is possible to allow note onset intervals larger than G [32].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "Although a direct application of the Viterbi algorithm is impossible due to the presence of continuous and discrete latent variables, discretisation of the tempo variables (typically into \u223c50 steps) yields a good approximation as discussed previously [20], [32].", "startOffset": 251, "endOffset": 255}, {"referenceID": 29, "context": "Although a direct application of the Viterbi algorithm is impossible due to the presence of continuous and discrete latent variables, discretisation of the tempo variables (typically into \u223c50 steps) yields a good approximation as discussed previously [20], [32].", "startOffset": 257, "endOffset": 261}, {"referenceID": 21, "context": ") To solve this problem, we use a context-tree model [24], [25], in which contexts are categorised according to a set of criteria that are represented as a tree (as in decision tree analysis) and all contexts in one category have the same probability distribution.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": ") To solve this problem, we use a context-tree model [24], [25], in which contexts are categorised according to a set of criteria that are represented as a tree (as in decision tree analysis) and all contexts in one category have the same probability distribution.", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "1The lists of used pieces for the score data and the performance data are available at the accompanying web page [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "This is usually done by recursively splitting a node that minimises the likelihood [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "[2] 141317 (41.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "47%) [3] 72191 (21.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "18%) [4] 127228 (37.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[5] 67978 (19.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] 43510 (12.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "76%) [7] 24468 (7.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "18%) [8] 59250 (17.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "[9] 17238 (5.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] 8860 (2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "6%) [11] 8378 (2.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "45%) [12] 42012 (12.", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "according to the data, the minimal description length (MDL) criterion for model selection can be used [26], [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "according to the data, the minimal description length (MDL) criterion for model selection can be used [26], [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 30, "context": "The MDL criterion is justified by an information-theoretic argument [34].", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "Our implementation of the MRF model and the metrical HMM for onset rhythm transcription and tempo estimation is available [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "Because of the ambiguity of defining the beat unit, score times estimated by rhythm transcription methods often have doubled, halved or other scaled values [16], [35], which should not be treated as complete errors.", "startOffset": 156, "endOffset": 160}, {"referenceID": 31, "context": "Because of the ambiguity of defining the beat unit, score times estimated by rhythm transcription methods often have doubled, halved or other scaled values [16], [35], which should not be treated as complete errors.", "startOffset": 162, "endOffset": 166}, {"referenceID": 12, "context": "In this section, we evaluate the proposed method, a previously studied method [14] and a simple model discussed in Sec.", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "For comparison, we evaluated the Melisma Analyzer (version 2) [14], which is to our knowledge the only major method that can estimate onset and offset score times, and we also applied post-processing by the proposed method on the onset score times obtained by the Melisma Analyzer.", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "mod el No co ntext tree Melis ma + MRF Melis ma [1 4] No o nset info.", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "Melis ma [1 4] No o nset info.", "startOffset": 9, "endOffset": 14}, {"referenceID": 12, "context": "mod el No co ntext tree Melis ma + MRF M elism a [14 ]", "startOffset": 49, "endOffset": 54}, {"referenceID": 2, "context": "Melis ma [1 4] No o nset info.", "startOffset": 9, "endOffset": 14}, {"referenceID": 2, "context": "mod el No co ntext tree Melis ma + MRF Melis ma [1 4] No o nset info.", "startOffset": 48, "endOffset": 53}, {"referenceID": 24, "context": "2Sound files are available at the accompanying web page [27].", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "The framework developed in this study can be combined with existing multi-pitch analysers [3]\u2013[6] for this purpose.", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "The framework developed in this study can be combined with existing multi-pitch analysers [3]\u2013[6] for this purpose.", "startOffset": 94, "endOffset": 97}], "year": 2017, "abstractText": "This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.", "creator": "LaTeX with hyperref package"}}}