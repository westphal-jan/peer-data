{"id": "1703.08244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "TokTrack: A Complete Token Provenance and Change Tracking Dataset for the English Wikipedia", "abstract": "We present a dataset that contains every instance of all tokens (~ words) ever written in undeleted, non-redirect English Wikipedia articles until October 2016, in total 13,545,349,787 instances. Each token is annotated with (i) the article revision it was originally created in, and (ii) lists with all the revisions in which the token was ever deleted and (potentially) re-added and re-deleted from its article, enabling a complete and straightforward tracking of its history. This data would be exceedingly hard to create by an average potential user as it is (i) very expensive to compute and as (ii) accurately tracking the history of each token in revisioned documents is a non-trivial task. Adapting a state-of-the-art algorithm, we have produced a dataset that allows for a range of analyses and metrics, already popular in research and going beyond, to be generated on complete-Wikipedia scale; ensuring quality and allowing researchers to forego expensive text-comparison computation, which so far has hindered scalable usage. We show how this data enables, on token-level, computation of provenance, measuring survival of content over time, very detailed conflict metrics, and fine-grained interactions of editors like partial reverts, re-additions and other metrics, in the process gaining several novel insights.", "histories": [["v1", "Thu, 23 Mar 2017 22:20:45 GMT  (779kb,D)", "http://arxiv.org/abs/1703.08244v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fabian fl\\\"ock", "kenan erdogan", "maribel acosta"], "accepted": false, "id": "1703.08244"}, "pdf": {"name": "1703.08244.pdf", "metadata": {"source": "CRF", "title": "TokTrack: A Complete Token Provenance and Change Tracking Dataset for the English Wikipedia", "authors": ["Fabian Fl\u00f6ck", "Kenan Erdogan", "Maribel Acosta"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "2 The TokTrack Dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Origin", "text": "The TokTrack record was created from the full data dump in Wikipedia revision history dated November 1, 2016, as published by Wikimedia, and thus covers all full months from January 2001 to October 2016.5. This XML dump contains the complete content (in the wiki markup) of the revisions of all pages that were not deleted. 6 Uncompressed, the XML dump measures about 12 TB. We extracted the article pages (namespace = 0) that were not redirected in the XML dump according to their last revision, resulting in 5, 275, 388 articles. For each article, we received the corresponding revisions. In total, the TokTrack record contains 451,350,901 revisions."}, {"heading": "2.2 Dataset Creation", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2.3 Dataset Size and Structure", "text": "In fact, most of them are able to retaliate, but they are not yet able to retaliate."}, {"heading": "3 Research Using Data Derivable from the TokTrack Dataset", "text": "There are a number of strands of research that either rely on data that can be derived much more efficiently from the TokTrack dataset or that could otherwise benefit from it. We also argue that a canonical content tracking dataset for Wikipedia, created using a quality-verified algorithm, would allow for the conduct and reproducibility of many future studies. Below, we analyze publications that address related topics based on the type of data they extract and use to achieve their goals, and discuss the benefits that future studies could similarly derive from our dataset."}, {"heading": "3.1 Authorship, Persistence, and Reputation", "text": "The first set of approaches can be roughly identified by their common goal of (i) recognizing the revision of the original introduction of a token (and by extension, the original author, timestamps, etc.) and (ii) recognizing the subsequent survival or persistence of that token in the article, either measured in time or the number of revisions. This requires tracking potential deletions and readditions of a token after the original addition, since the same tokens can be deleted and reinserted multiple times during the article-writing process while they were originally written by the same author. [24] Use a persistence metric to determine which editors \"have how much content in each revision and to determine whether the content of a publisher gets a certain amount of views, while Halfaker et al. [15,16] compares the quality of an edit and the productivity of editors with an equivalent approach."}, {"heading": "3.2 Identifying Conflicted Content", "text": "A recurring theme in Wikipedia research is the measurement and characterization of conflict or controversy specific content subject to.12 cf., e.g. http: / / fogonwater.com / blog / 2015 / 11 / wikipedia-edit-history-stratigraphy or http: / / iphylo.blogspot. en / 2009 / 09 / visualization-edit-history-of-wikipedia.htmlFor a part, studies have focused on the particular problem of identifying controversial articles as a whole, either by calculating mutual (identity) reversal levels between editors [28,33,32]; or by building more complex models that include a number of metrics, e.g. conversation pages and anonymous editors, edit comments and removed words [20,31,25]."}, {"heading": "3.3 Interactions Between Editors", "text": "In fact, most of them are able to survive on their own."}, {"heading": "4 Use Cases and Analyses", "text": "In this section, we show how metrics related to the research strands in Section 3 can be extracted from our dataset, and present the results of several analyses conducted across the entire English Wikipedia content as of the end of October 2016.13 We will also publish the auxiliary datasets that we have created for each of the following analyses in addition to the main dataset, which include calculated token survival information, conflict values, and return data to further facilitate exploration of the TokTrek corpus."}, {"heading": "4.1 Token Survival and Authorship", "text": "The survival of the tokens over time can provide important insights into the resistance faced by their introduction, possibly due to various reasons, such as their quality or the characteristics of the contributing editor. [15] Therefore, we first analyzed, based on all 13,545,349,787 content change stories registered, (i) the total number of tokens originally added in each month for the entire data set (visible as the total height of the stacked bar charts in Figure 2a). Also, since added tokens are often deleted, we measured (ii) the number of tokens originally added in each month as a measure of their \"fitness,\" which is represented by the sum of the blue and green bars in Figure 2a. 14 as well as the tokens added in each month could still be read in Wikipedia, at the end of October 2016, represented by the green bars in Figure 2a, we see the rapid growth of the tokens in Figure 2a, which we see in Figure 2a."}, {"heading": "4.2 Conflict", "text": "In order to obtain the first answers to the question of which parts of the content have come into conflict with each other, we have presented two measures which we have repeatedly questioned in recent years. (...) The following measures have been calculated: -) Most of them have occurred in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the past in the future in the past in the future in the past in the future in the past in the future in the past in the future in the past in the past in the past in the past in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in the future in"}, {"heading": "4.3 Interactions Between Users", "text": "To show another use case for the dataset, we examined how many \"undo actions\" similar to the publisher's interactions defined in [4] can first of all be extracted from two different types of interactions (4). We use the term \"undo action\" to refer to three different ways: adding an entirely new token, Del: deleting a single token, Re: performing the reinsertion of a single token. The last two types, Del and Re: revisions, are always considered \"undo actions,\" as they can either undo a previous addition to a token (Del) or the deletion of a token (Re). Thus, an editing (creating a new revision) may possibly contain several backward revisions, e.g. removing n tokens (n \u00d7 Del actions) and adding completely new (m \u00d7 Add) in their place, which would amount to n + edit actions, which would amount to m + edit actions, m n n."}, {"heading": "5 Conclusions", "text": "With the data set presented here, we hope to remove some of the larger barriers to calculating content-based, fine-grained metrics in addition to Wikipedia's revamped content, which provide insights into the dynamics of collaboration and the way content evolves under the influence of these social patterns. We have shown how the token-level data can help better understand existing insights and open new avenues for exploring Wikipedia content."}], "references": [{"title": "A content-driven reputation system for the Wikipedia", "author": ["T. Adler", "L. De Alfaro"], "venue": "In Proc. WWW, pages 261\u2013270,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Measuring author contributions to the Wikipedia", "author": ["T. Adler", "L. de Alfaro", "I. Pye", "V. Raman"], "venue": "In Proc. WikiSym,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A platform for visually exploring the development of wikipedia articles", "author": ["E. Borra", "D. Laniado", "E. Weltevrede", "M. Mauri", "G. Magni", "T. Venturini", "P. Ciuccarelli", "R. Rogers", "A. Kaltenbrunner"], "venue": "In Proc. ICWSM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Network analysis of collaboration structure in Wikipedia", "author": ["U. Brandes", "P. Kenis", "J. Lerner", "D. van Raaij"], "venue": "In Proc. WWW,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Fine-grained controversy detection in Wikipedia", "author": ["S. Bykau", "F. Korn", "D. Srivastava", "Y. Velegrakis"], "venue": "In Proc. ICDE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Attributing authorship of revisioned content", "author": ["L. De Alfaro", "M. Shavlovsky"], "venue": "In Proc. WWW,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Conflict and computation on wikipedia: a finite-state machine analysis of editor interactions", "author": ["S. DeDeo"], "venue": "CoRR, abs/1512.04177,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "WikiWho: Precise and efficient attribution of authorship of revisioned content", "author": ["F. Fl\u00f6ck", "M. Acosta"], "venue": "In Proc. WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "whoVIS: visualizing editor interactions and dynamics in collaborative writing over time", "author": ["F. Fl\u00f6ck", "M. Acosta"], "venue": "In Proc. WWW - Companion Volume,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "TokTrack: A complete token provenance and change tracking dataset for the english Wikipedia. Zenodo data sharing service, DOI: https: //doi.org/10.5281/zenodo.345571, Feb 2017", "author": ["F. Fl\u00f6ck", "K. Erdogan", "M. Acosta"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Towards better visual tools for exploring wikipedia article development\u2013the use case of \u201dgamergate controversy", "author": ["F. Fl\u00f6ck", "D. Laniado", "F. Stadthaus", "M. Acosta"], "venue": "In Wikipedia, a Social Pedia - Workshop at the Ninth International AAAI Conference on Web and Social Media,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Whose article is it anyway?\u2013Detecting authorship distribution in Wikipedia articles over time with WIKIGINI", "author": ["F. Fl\u00f6ck", "A. Rodchenko"], "venue": "In Online proceedings of the Wikipedia Academy", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Revisiting reverts: Accurate revert detection in wikipedia", "author": ["F. Fl\u00f6ck", "D. Vrande\u010di\u0107", "E. Simperl"], "venue": "In Proceedings of the 23rd ACM Conference on Hypertext and Social Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "The rise and decline of an open collaboration system: How Wikipedia\u2019s reaction to popularity is causing its decline", "author": ["A. Halfaker", "R.S. Geiger", "J.T. Morgan", "J. Riedl"], "venue": "American Behavioral Scientist,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A jury of your peers: quality, experience and ownership in Wikipedia", "author": ["A. Halfaker", "A. Kittur", "R. Kraut", "J. Riedl"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Don\u2019t bite the newbies: How reverts affect the quantity and quality of Wikipedia work", "author": ["A. Halfaker", "A. Kittur", "J. Riedl"], "venue": "In Proc. WikiSym,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Analyzing the creative editing behavior of Wikipedia editors: Through dynamic social network analysis", "author": ["T. Iba", "K. Nemoto", "B. Peters", "P.A. Gloor"], "venue": "Procedia-Social and Behavioral Sciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Modeling user reputation in wikis", "author": ["S. Javanmardi", "C. Lopes", "P. Baldi"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Analyzing organizational routines in online knowledge collaborations: A case for sequence analysis in CSCW", "author": ["B.C. Keegan", "S. Lev", "O. Arazy"], "venue": "In Proc. CSCW,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "He says, she says: Conflict and coordination in Wikipedia", "author": ["A. Kittur", "B. Suh", "B.A. Pendleton", "E.H. Chi"], "venue": "In Proc. CHI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "WP:Clubhouse? an exploration of Wikipedia\u2019s gender imbalance", "author": ["S.K. Lam", "A. Uduwage", "Z. Dong", "S. Sen", "D.R. Musicant", "L. Terveen", "J. Riedl"], "venue": "In WikiSym,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Dominance, deference, and hierarchy formation in Wikipedia editnetworks", "author": ["J. Lerner", "A. Lomi"], "venue": "In Proc. COMPLEX NETWORKS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Building a signed network from interactions in Wikipedia", "author": ["S. Maniu", "B. Cautis", "T. Abdessalem"], "venue": "In Databases and Social Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Creating, destroying, and restoring value in Wikipedia", "author": ["R. Priedhorsky", "J. Chen", "S.T.K. Lam", "K. Panciera", "L. Terveen", "J. Riedl"], "venue": "In ACM GROUP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Leveraging editor collaboration patterns in Wikipedia", "author": ["H. Sepehri Rad", "A. Makazhanov", "D. Rafiei", "D. Barbosa"], "venue": "In ACM Hypertext,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Us vs. them: Understanding social dynamics in Wikipedia with revert graph visualizations", "author": ["B. Suh", "E.H. Chi", "B.A. Pendleton", "A. Kittur"], "venue": "In IEEE VAST,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "The singularity is not near: slowing growth of Wikipedia", "author": ["B. Suh", "G. Convertino", "E.H. Chi", "P. Pirolli"], "venue": "In Proc. WikiSym,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Dynamics of disagreement: Large-scale temporal network analysis reveals negative interactions in online collaboration", "author": ["M. Tsvetkova", "R. Garc\u0131\u0301a-Gavilanes", "T. Yasseri"], "venue": "Scientific reports,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Studying cooperation and conflict between authors with history flow visualizations", "author": ["F.B. Vi\u00e9gas", "M. Wattenberg", "K. Dave"], "venue": "In Proc. CHI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "On ranking controversies in wikipedia: models and evaluation", "author": ["B. Vuong", "E. Lim", "A. Sun", "M. Le", "H.W. Lauw"], "venue": "In Proc. WSDM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "The most controversial topics in Wikipedia: A multilingual and geographical analysis", "author": ["T. Yasseri", "A. Spoerri", "M. Graham", "J. Kert\u00e9sz"], "venue": "Global Wikipedia: International and Cross-Cultural Issues in Online Collaboration.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Dynamics of conflicts in Wikipedia", "author": ["T. Yasseri", "R. Sumi", "A. Rung", "A. Kornai", "J. Kert\u00e9sz", "A. Szolnoki"], "venue": "PLoS ONE,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "However, to detect the changes to these tokens and their original provenance in the revision history necessitates the application of computationally expensive text-comparison algorithms, which on top can often be inaccurate when tracking alterations to all tokens in each edit [6].", "startOffset": 277, "endOffset": 280}, {"referenceID": 17, "context": "This poses the first challenge, as solely applying a text comparison algorithm from each revision to the previous one cannot detect such reinsertions [18].", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "One technique applied by some works to compute provenance and survival of tokens over revisions is to, on one hand, compute the changes from one revision to the immediately previous one using an out-of-the-box text-difference algorithm [24,15].", "startOffset": 236, "endOffset": 243}, {"referenceID": 14, "context": "One technique applied by some works to compute provenance and survival of tokens over revisions is to, on one hand, compute the changes from one revision to the immediately previous one using an out-of-the-box text-difference algorithm [24,15].", "startOffset": 236, "endOffset": 243}, {"referenceID": 19, "context": "These identity reverts seem to be the most common type of reverts [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "[26] mention: \u201cthe disadvantage of this method is that it does not pick up partial reverts, in which only some of the text in an article is reverted\u201d and an identical revision is not created; see for example Figure 1, where we did not include any revision\u2019s content reoccurring identically in another revision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] remark that using only identity reverts does lead to inaccuracies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1,2] detects provenance by searching for longest matches for all word sequences of the current revision in selected preceding revisions and their previously existing (but now deleted) word-chunks.", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1,2] detects provenance by searching for longest matches for all word sequences of the current revision in selected preceding revisions and their previously existing (but now deleted) word-chunks.", "startOffset": 0, "endOffset": 5}, {"referenceID": 5, "context": "As De Alfaro and Shavlovsky [6] later argue, it is not well-suited for the task of authorship or provenance detection, as the process depends on several factors of its \u201ccomputationally involved\u201d editor reputation calculation \u2013 a suspicion supported by an evaluation on a small sample of authorship data generated with Wikitrust, yielding only around 50% correctly attributed authors for tokens [12].", "startOffset": 28, "endOffset": 31}, {"referenceID": 11, "context": "As De Alfaro and Shavlovsky [6] later argue, it is not well-suited for the task of authorship or provenance detection, as the process depends on several factors of its \u201ccomputationally involved\u201d editor reputation calculation \u2013 a suspicion supported by an evaluation on a small sample of authorship data generated with Wikitrust, yielding only around 50% correctly attributed authors for tokens [12].", "startOffset": 394, "endOffset": 398}, {"referenceID": 5, "context": "De Alfaro and Shavlovsky [6] in their recent work propose a more advanced technique for attributing original authorship to the tokens in a target revision of an article which can be also used to infer changes applied to a token over its lifetime.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "More recently, we have proposed another approach, Wikiwho, aimed at calculating token provenance, and also able to perform change detection [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "In comparison with the approach by [6], Wikiwho achieves distinctly better runtimes [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "In comparison with the approach by [6], Wikiwho achieves distinctly better runtimes [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "In regard to precision of attributing original contributors to tokens, experimental results indicate that Wikiwho outperforms the algorithm by [6] and reaches 95% correct attributions.", "startOffset": 143, "endOffset": 146}, {"referenceID": 9, "context": "The TokTrack dataset is available for download [10].", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "[24] use a content persistence metric to identify which editors \u201cowned\u201d how much content in every revision and to determine whether an editor\u2019s content gets a certain amount of views, while Halfaker et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15,16] approximate the quality of an edit and the productivity of editors with an equivalent approach.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[15,16] approximate the quality of an edit and the productivity of editors with an equivalent approach.", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[1,2] detects provenance and persistence and also features an interface for coloring more or less trusted content sequences.", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1,2] detects provenance and persistence and also features an interface for coloring more or less trusted content sequences.", "startOffset": 0, "endOffset": 5}, {"referenceID": 17, "context": "[18] follow a similar strategy for computing user reputation, but use only identity reverts aside from text-difference computation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "The HistoryFlow visualization [30] also makes use of tracking the authors and positions in each revision of sample articles, although on the (coarse-grained) sentencelevel, drawing them in a so-called stratigraph.", "startOffset": 30, "endOffset": 34}, {"referenceID": 31, "context": "Either by computing the mutual (identity) revert levels between editors [28,33,32]; or by building more complex models, involving a range of metrics beside reverts, e.", "startOffset": 72, "endOffset": 82}, {"referenceID": 30, "context": "Either by computing the mutual (identity) revert levels between editors [28,33,32]; or by building more complex models, involving a range of metrics beside reverts, e.", "startOffset": 72, "endOffset": 82}, {"referenceID": 19, "context": ", talk page and anonymous edits, edit comments and removed words [20,31,25].", "startOffset": 65, "endOffset": 75}, {"referenceID": 29, "context": ", talk page and anonymous edits, edit comments and removed words [20,31,25].", "startOffset": 65, "endOffset": 75}, {"referenceID": 24, "context": ", talk page and anonymous edits, edit comments and removed words [20,31,25].", "startOffset": 65, "endOffset": 75}, {"referenceID": 2, "context": "[3] developed the web platform Contropedia to visualize controversies related to the internal Wikipedia-links present in an article, by coloring them in different shades representing their conflict levels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] proposed a novel method to cluster disagreements of editors over specific tokens into larger controversies, delimited not only by the content involved, but also determining a timeframe for when a controversy occurs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": ", [17,19]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 18, "context": ", [17,19]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 19, "context": "Some works that use identity reverts as a central tool provide a general overview of the evolution of editing dynamics and the changing state of Wikipedia\u2019s editor community [20,27].", "startOffset": 174, "endOffset": 181}, {"referenceID": 26, "context": "Some works that use identity reverts as a central tool provide a general overview of the evolution of editing dynamics and the changing state of Wikipedia\u2019s editor community [20,27].", "startOffset": 174, "endOffset": 181}, {"referenceID": 14, "context": "Other works focus on reverts in regard to their indicator role of a reverted edit\u2019s (insufficient) quality [15] as well as their possible damaging effects and the harmful barriers faced by new editors [16,14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "Other works focus on reverts in regard to their indicator role of a reverted edit\u2019s (insufficient) quality [15] as well as their possible damaging effects and the harmful barriers faced by new editors [16,14].", "startOffset": 201, "endOffset": 208}, {"referenceID": 13, "context": "Other works focus on reverts in regard to their indicator role of a reverted edit\u2019s (insufficient) quality [15] as well as their possible damaging effects and the harmful barriers faced by new editors [16,14].", "startOffset": 201, "endOffset": 208}, {"referenceID": 20, "context": "Gender imbalances in how editor\u2019s work is received [21] have been in-", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "vestigated with the help of reverts as well as more intricate motifs of editor interaction [7,29].", "startOffset": 91, "endOffset": 97}, {"referenceID": 27, "context": "vestigated with the help of reverts as well as more intricate motifs of editor interaction [7,29].", "startOffset": 91, "endOffset": 97}, {"referenceID": 3, "context": "[4] make the argument for extracting interactions based on all token-level changes between editors (and thereby, revisions) when they state that identity reverts do \u201cnot consider who deletes how much of whose edits\u201d and which exact parts are reinstated later (cf.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Lerner and Lomi [22] extend this method to infer detailed disagreements and restorations between editors to study emerging hierarchies and we have earlier implemented an interactive visualization based on the technique proposed by Brandes et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "[4] and comparable data [9,11].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[4] and comparable data [9,11].", "startOffset": 24, "endOffset": 30}, {"referenceID": 10, "context": "[4] and comparable data [9,11].", "startOffset": 24, "endOffset": 30}, {"referenceID": 22, "context": "[23] infer a very similar signed editor network, including deletion, revert and restore actions and tracking authorship of words, using text difference computation and edit comments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Contropedia [3] also employs interaction network visualization based on token changes.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "While not needed in every research setting, the minute interactions as used by [4] and others promise however to enable more precise explorations of these exchanges between editors, which can benefit future research.", "startOffset": 79, "endOffset": 82}, {"referenceID": 14, "context": "The survival of tokens over time can give important insights about the resistance their introduction has faced, which might be due to different reasons, for instance their quality or features of the contributing editor [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 4, "context": "more intricate token-based measures or adopt such as proposed by [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 29, "context": "One could also for instance imagine a weighting of undo actions by the experience of the editor in the article as per [28], to more effectively down-weigh vandalism, or the editor\u2019s overall proneness to conflict [31].", "startOffset": 212, "endOffset": 216}, {"referenceID": 30, "context": "The topics feature some \u201cevergreens\u201d, as also included in the top 100 most controversial articles of [32],16 such as Evolution, Quebec, and Wikipedia, but naturally have a strong recency bias, as only currently still present tokens were used.", "startOffset": 101, "endOffset": 105}, {"referenceID": 30, "context": "txt, as referenced in [32], who did not limit their analysis to current content or reverts.", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "To show another use case for the dataset, we studied how many \u201cundo actions\u201d similar to the editor interactions defined in [4] can be extracted.", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": "This hints at the strong possibility of misidentifying type as well as the targets of reverts when using the identity revert method, as we have argued previously [13].", "startOffset": 162, "endOffset": 166}], "year": 2017, "abstractText": "We present a dataset that contains every instance of all tokens (\u2248 words) ever written in undeleted, non-redirect English Wikipedia articles until October 2016, in total 13, 545, 349, 787 instances. Each token is annotated with (i) the article revision it was originally created in, and (ii) lists with all the revisions in which the token was ever deleted and (potentially) re-added and redeleted from its article, enabling a complete and straightforward tracking of its history. This data would be exceedingly hard to create by an average potential user as it is (i) very expensive to compute and as (ii) accurately tracking the history of each token in revisioned documents is a non-trivial task. Adapting a stateof-the-art algorithm, we have produced a dataset that allows for a range of analyses and metrics, already popular in research and going beyond, to be generated on complete-Wikipedia scale; ensuring quality and allowing researchers to forego expensive text-comparison computation, which so far has hindered scalable usage. We show how this data enables, on token-level, computation of provenance, measuring survival of content over time, very detailed conflict metrics, and finegrained interactions of editors like partial reverts, re-additions and other metrics, in the process gaining several novel insights.", "creator": "LaTeX with hyperref package"}}}