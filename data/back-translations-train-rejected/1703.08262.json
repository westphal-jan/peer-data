{"id": "1703.08262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Supervisor Synthesis of POMDP based on Automata Learning", "abstract": "As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an $L^*$ learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.", "histories": [["v1", "Fri, 24 Mar 2017 01:59:11 GMT  (140kb,D)", "http://arxiv.org/abs/1703.08262v1", null]], "reviews": [], "SUBJECTS": "cs.SY cs.AI cs.FL", "authors": ["xiaobin zhang", "bo wu", "hai lin"], "accepted": false, "id": "1703.08262"}, "pdf": {"name": "1703.08262.pdf", "metadata": {"source": "CRF", "title": "Supervisor Synthesis of POMDP based on Automata Learning", "authors": ["Xiaobin Zhang", "Bo Wu", "Hai Lin"], "emails": ["(xzhang11@nd.edu;", "bwu3@nd.edu;", "hlin1@nd.edu)."], "sections": [{"heading": null, "text": "In fact, it is not that this is a way in which the various systems have become a hot field of research in recent years, ranging from navigation and navigation to communication protocol design. [1] The different system models have been considered to capture uncertainties, and partially observable Markov decisions (POMDP) have been considered one of the most general and thus most popular models of system software and hardware status with discrete states. Between different states, probable transitions are triggered by different system actions to describe uncertainties in system behavior."}, {"heading": "A. Related Work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "B. Our Contributions", "text": "This paper is an extended and revised version of our preliminary conference paper [39]. Compared to [39], this paper provides the following new contributions: First, we formally build the control framework for POMDP supervisors by examining the adequacy of using za-DFA as a controller form, defining the probability space for POMDP, and then defining PCTL satisfaction for POMDP. Second, the model verification of POMDP against observation-based adversaries is intensively studied, and a modified POMCP algorithm is given to achieve computational complexity. Third, we develop new oracles for the L-learning algorithm to guarantee complexity and permissiveness of the supervisor. Building on this, a new example is given to illustrate the learning process in detailed steps."}, {"heading": "C. Outline of the Paper", "text": "The remainder of this paper is structured as follows: Section II presents MDP-related preparatory work with definitions and notations; Section III proposes the supervisory control framework for POMDP; Section IV introduces the learning-based supervisor synthesis algorithm; Section V discusses analysis and discussions; Section VI provides an example of the learning process; and Section VII concludes this paper with future work."}, {"heading": "II. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. MDP Modeling, Paths and Adversaries", "text": "MDPs are probabilistic models for systems with separate state spaces. With non-determinisms from decision-making and probabilistic behavior during system transitions, MDPs are widely used to model system uncertainties. Definition 1. [43] An MDP is a tuple M = (S, s, A, T) in which \u2022 S is a finite series of states; \u2022 s \"S\" S is the initial state; \u2022 A is a finite series of actions; \u2022 T: S \"A\" S \"S \u2192 [0, 1] is a transitional function. Here, T (s, a, s\") describes the likelihood of a transition from one state s \"S\" to another state s \"S\" after taking an action A. \"In MDPs, there are multiple actions defined for each state. If we limit the number of actions defined for each state to 1, we limit a discrete time Markov chain (TMC) Dup.C = DupS 2. (A DupS is a TMC)."}, {"heading": "B. PCTL and PCTL Model Checking over MDPs", "text": "For a model called MDP, we can use PCTL [43] to represent the requirements for system design. PCTL is the probable extension of the Computation Tree Logic (CTL) [44].Definition 3. [43] The syntax of PCTL is defined as \u2022 State formula \u03c6:: = true | \u03b1 | \u00ac \u03c6 | P / p [\u043a], \u2022 Path formula \u0432: = X\u03c6 | U \u2264 k\u03c6 | \u03c6 U \u03c6, where \u03b1 AP,. / \u0432, < \u2265, >}, p [0, 1] and k \u00b2 N.Here \u00ac stands for \"negation,\" \u0456 for \"conjunction,\" X for \"next,\" U \u2264 k for \"bounded until\" and U for \"until.\" Specifically, P. / p [\u0142] takes a path finity formula [0, 1] as a parameter and describes the probabilistic constraint.Given the syntax of POMDP, we can perceive PC's satisfaction relationship to MDP and PfU as \"to a formula [0, 1]."}, {"heading": "III. POMDP MODELING AND SUPERVISORY CONTROL FRAMEWORK", "text": "In this section, we propose a supervisory control framework to regulate the closed-loop behavior of POMDP in order to meet the finite-horizon PCTL specifications."}, {"heading": "A. POMDP Modeling, Paths and Adversaries", "text": "In this case it is a pure problem where the observable information for each state is a probability distribution over Z (s, z) stands for the probability of observation z (s, z) stands for the probability of observation z (s, z).0 Then the MDP can also be regarded as a special case of POMDP, where its observation function for each state s (s, s, z) stands for the probability distribution over Z (s, z) stands for the probability of observation z (s, z).0 Then the MDP can be considered a special case of POMDP, where its observation function for each s (s, s) S is defined as a dirac delta function without (s, z) = 0, otherwise.Remark: Since the states in POMDP are not directly observable, it may happen that we have this observation function for each s (s, s).S is a dirac delta function within O (s, z) = s, otherwise.0, Remark"}, {"heading": "B. Supervisory Control with za-DFA", "text": "& & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & &"}, {"heading": "C. Probability Space and PCTL Satisfaction over POMDP", "text": "To formally address PCTL satisfaction via POMDP, we first define the probability space in POMDP = 43. With an observation-based adversary, the behavior of POMDP is purely probabilistic. In view of a finite path \u03c1fin and its corresponding observation sequence obs (\u03c1fin), with an observation-based adversary, we can define the basic cylinder in POMDP as follows: C (\u03c1fin): = {\u03c1 PathP | \u03c1fin is a prefix of \u03c1 and obs (\u03c1fin) (i), with an observation-based adversary set in POMDP, prefixing the set of all infinite paths with the prefix \u03c1fin and the observation probability obs (\u03c1fin). Let Cyl contain all propositions C (\u03c1fin) in which the spectral extends over all possible observation sequences. Then, the cell algebra on the paths of Cyl generated paths and the corresponding probability magnitude can be defined."}, {"heading": "D. POMDP Model Checking", "text": "To verify the satisfaction relationship with the regulated behavior, we need to check the PCTL model verification problem for MF = =, where most operators are treated in the same way as in the MDP model verification, but for the state formula P. / p, we then need to check if the probability is limited. / p is satisfied with observation-based adversaries rather than all adversaries. We can solve this by calculating either the minimum or maximum probability, depending on whether a lower or upper limit is defined. / [43] This problem can be solved with EXPTIME-complete complexity for finite horizon specifications, but with the size of POMDP and the increasing planning horizon, this problem becomes much more promising when the verification problem is transformed into an equivalent optimal policy verification problem. Following this method, we can handle the recently developed POMDP solutions with high efficiency."}, {"heading": "IV. LEARNING BASED SUPERVISOR SYNTHESIS", "text": "Within the supervisory control framework that za-DFA uses, our task of finding a supervisor for POMDP is converted to find a DFA that represents an equivalent representation of the regular set [49], inspiring us to use the L algorithm to learn a supervisor."}, {"heading": "A. L\u2217 Learning Algorithm", "text": "Based on a fixed known quantity of the number of alphabets, learning defines an observation table (Y, E, G) to organize the knowledge acquired by the learning algorithm. The row index of the table contains two parts: Y and Y, where Y is a non-empty finite number table. G forms a string y, which specifies a closed number table y. The number of the finite number table is limited to {0, 1}, which contains all finite number tables containing symbols from the table Iof Strings. A string y (Y, Y, Y, Y, Y, Y, E, E) is a closed number table. the function G (y) = 1, if y, U."}, {"heading": "B. Learn za-DFA as the Supervisor", "text": "In view of a POMDP P = {S, S, T, O} and a finite horizon PCTL-Specification f / f, we use L * learning to learn a za-DFA F as the supervisor. To obtain a practicable za-DFA, the POMDP can regulate the learning process to satisfy the learning process, we develop algorithms to reply to membership inquiries and assumptions. To simplify the analysis, take we?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "V. ANALYSIS AND DISCUSSIONS", "text": "In this section, we analyze the algorithm of the learning-based supervisor synthesis in terms of completion, solidity and complexity, as well as computational complexity. Our analysis focuses on cases where the algorithm is not terminated during the pre-processing phase, as trivial statements can otherwise be followed."}, {"heading": "A. Termination", "text": "Since we are looking at finite POMDP with a specification of the finite horizon, the number of all possible observation sequences is finite, so we only have a finite number of strings that must be labeled true in the observation table for the L-E algorithm. Our algorithm requires a refinement process for the observation table if the returned negative counter-examples and their sufferings have been answered true by member requests in previous iterations. However, with a10 string y (Y-Y-Z) \u00b7 E, it will never happen that G (y) is changed from 0 to 1. Consider a string y with G (y) = 0. Then either the cumulative probability from y is violated by the threshold from the specification, or Pref (y) from racles."}, {"heading": "B. Soundness and Completeness", "text": "If a Za-DFA is returned as a supervisor based on the definition of OracleP, OracleB, and OracleS, this Za-DFA is not blocking, and the model verification of the regulated behavior of the POMDP confirms satisfaction with the specification. This shows the solidity of the algorithm. However, we cannot guarantee completeness if OracleS returns \"good\" observation and action sequences as negative counter-examples. While OracleS will never incorrectly identify a single string that has a sufficient probability mass of violation when a number of paths are needed to observe the violation, how to select a suitable counter-example from this set, and it is possible that some newly applied examples are returned by selecting the desired maximum probability, while we can use the negative methods as counter-examples."}, {"heading": "C. Complexity", "text": "Define the size of the POMDP SP as a product of the size of the underlying MDP SM and | Z |: SP = SM \u0445 | Z | and label the planning horizon of the specification as k. Following the final analysis, the number of iterations is at most O (| \u03a3 | k), where \u03a3 is the alphabet. In each iteration, label the size of the current acceptor DFA as SF. OracleP tries to find the difference between the current acceptor DFA and \u03c3min. This can be achieved with the time complexity O (SF) by performing complementarity and interaction between two DFAs, and then first checking whether the initial state in k steps can be reached from the recognized state or not. OracleB mainly applies the depth analysis to the product MDP, so that the time complexity is O (SP)."}, {"heading": "VI. EXAMPLE", "text": "Consider a POMDP P = {S, S, Z, T, O} where \u2022 S = {s0, s2, s3, s4}; \u2022 s = s0, a1, a3, a3}; \u2022 Z = {z1, z2}.The chances of transition among various actions are in the order of a1, a2, a3 in the square brackets in Fig. 3. The observation matrix is given in Table II. Under S, the state s4 represents a failure and is in the order of a1, a2, a3 in the square brackets in Fig. 3. The specification is specified by a finite horizon PCTL = P \u2264 0.28 [s] with real U \u2264 3fail, which requires the probability of failure within 3 steps. < Remark: This POMDP P is specially designed to solve the model verification problem."}, {"heading": "VII. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, a learning-based supervisor synthesis framework is proposed for POMDP to meet formal specifications. As PCTL with a limited horizon is considered a system specification, we design the supervisor control framework with za-DFA. By modifying queries and assumptions about membership in the L-DFA algorithm, our learning process can automatically synthesize a za-DFA with a termination guarantee. The proposed algorithm is also solid and complete. However, due to the challenges posed by the probabilistic system counter-sample selection, OracleS can incorrectly identify good control guidelines as counter-examples, where the supervisor's admissibility cannot be guaranteed in these cases. In the future, we will investigate various probabilistic system selection algorithms to reduce the possibility of falsely identifying OracleS. While each part of the learning process is currently running separately on the basis of different packages, for example COMPOS AlPOS for full-TML counter-sample 57 [COMPOS algorithms]."}], "references": [{"title": "Exploration and mapping with autonomous robot teams", "author": ["E. Olson", "J. Strom", "R. Goeddel", "R. Morton", "P. Ranganathan", "A. Richardson"], "venue": "Communications of the ACM, vol. 56, no. 3, pp. 62\u201370, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Cooperative air and ground surveillance", "author": ["B. Grocholsky", "J. Keller", "V. Kumar", "G. Pappas"], "venue": "Robotics & Automation Magazine, IEEE, vol. 13, no. 3, pp. 16\u201325, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Decentralized cognitive mac for opportunistic spectrum access in ad hoc networks: A pomdp framework", "author": ["Q. Zhao", "L. Tong", "A. Swami", "Y. Chen"], "venue": "Selected Areas in Communications, IEEE Journal on, vol. 25, no. 3, pp. 589\u2013600, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Datadriven probabilistic modeling and verification of human driver behavior", "author": ["D. Sadigh", "K. Driggs-Campbell", "A. Puggelli", "W. Li", "V. Shia", "R. Bajcsy", "A.L. Sangiovanni-Vincentelli", "S.S. Sastry", "S.A. Seshia"], "venue": "AAAI Spring Symposium-Technical Report, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A pomdp framework for human-in-theloop system", "author": ["C.-P. Lam", "S.S. Sastry"], "venue": "Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on. IEEE, 2014, pp. 6031\u20136036.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling and solving human-robot collaborative tasks using pomdps", "author": ["N. Gopalan", "S. Tellex"], "venue": "Proc. Robot., Sci. Syst.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 0}, {"title": "Human and robot behavior modeling for probabilistic cognition of an autonomous service robot", "author": ["S.R. Schmidt-Rohr", "M. Losch", "R. Dillmann"], "venue": "Robot and Human Interactive Communication, 2008. RO-MAN 2008. The 17th IEEE International Symposium on. IEEE, 2008, pp. 635\u2013640.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Performance guaranteed human-robot collaboration through correct-by-design", "author": ["X. Zhang", "Y. Zhu", "H. Lin"], "venue": "American Control Conference (ACC), 2016. American Automatic Control Council (AACC), 2016, pp. 6183\u20136188.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Pomdp-based long-term user intention prediction for wheelchair navigation", "author": ["T. Taha", "J.V. Mir\u00f3", "G. Dissanayake"], "venue": "Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on. IEEE, 2008, pp. 3920\u20133925.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimal control of markov processes with incomplete state information", "author": ["K.J. Astrom"], "venue": "Journal of mathematical analysis and applications, vol. 10, no. 1, pp. 174\u2013205, 1965.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1965}, {"title": "The optimal control of partially observable markov processes over the infinite horizon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research, vol. 26, no. 2, pp. 282\u2013304, 1978.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1978}, {"title": "Formal methods for control synthesis in partially observed environments: application to autonomous robotic manipulation", "author": ["H.-T. Cheng"], "venue": "Ph.D. dissertation, University of British Columbia, 1988.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1988}, {"title": "Speeding up the convergence of value iteration in partially observable markov decision processes", "author": ["N.L. Zhang", "W. Zhang"], "venue": "Journal of Artificial Intelligence Research, vol. 14, no. 1, pp. 29\u201351, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Anytime point-based approximations for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research, pp. 335\u2013380, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Sarsop: Efficient point-based pomdp planning by approximating optimally reachable belief spaces.", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "in Robotics: Science and systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Finite-state controllers based on mealy machines for centralized and decentralized pomdps.", "author": ["C. Amato", "B. Bonet", "S. Zilberstein"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Bounded finite state controllers.", "author": ["P. Poupart", "C. Boutilier"], "venue": "in NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Introduction to automata theory, languages, and computation", "author": ["III I. Hill"], "venue": "1979.  13", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1979}, {"title": "Scaling internal-state policy-gradient methods for pomdps", "author": ["D. Aberdeen", "J. Baxter"], "venue": "MACHINE LEARNING-INTERNATIONAL WORK- SHOP THEN CONFERENCE-, 2002, pp. 3\u201310.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Solving pomdps by searching the space of finite policies", "author": ["N. Meuleau", "K.-E. Kim", "L.P. Kaelbling", "A.R. Cassandra"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1999, pp. 417\u2013426.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Isomorph-free branch and bound search for finite state controllers.", "author": ["M. Grzes", "P. Poupart", "J. Hoey"], "venue": "in IJCAI. Citeseer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Hybrid controllers for path planning: A temporal logic approach", "author": ["G.E. Fainekos", "H. Kress-Gazit", "G.J. Pappas"], "venue": "Decision and Control, 2005 and 2005 European Control Conference. CDC-ECC\u201905. 44th IEEE Conference on. IEEE, 2005, pp. 4885\u20134890.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Temporal-logic-based reactive mission and motion planning", "author": ["H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas"], "venue": "Robotics, IEEE Transactions on, vol. 25, no. 6, pp. 1370\u20131381, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Temporal logic motion planning for dynamic robots", "author": ["G.E. Fainekos", "A. Girard", "H. Kress-Gazit", "G.J. Pappas"], "venue": "Automatica, vol. 45, no. 2, pp. 343\u2013352, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Synthesis of control protocols for autonomous systems", "author": ["T. Wongpiromsarn", "U. Topcu", "R.M. Murray"], "venue": "Unmanned Systems, vol. 1, no. 01, pp. 21\u201339, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "The temporal logic of reactive and concurrent systems: Specification", "author": ["Z. Manna", "A. Pnueli"], "venue": "Springer Science & Business Media,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "A probabilistic approach for control of a stochastic system from ltl specifications", "author": ["M. Lahijanian", "S.B. Andersson", "C. Belta"], "venue": "Decision and Control, 2009 held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009. Proceedings of the 48th IEEE Conference on. IEEE, 2009, pp. 2236\u20132241.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Formal methods for control synthesis in partially observed environments: application to autonomous robotic manipulation", "author": ["R. Sharan"], "venue": "Ph.D. dissertation, California Institute of Technology, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "A symbolic sat-based algorithm for almost-sure reachability with small strategies in pomdps", "author": ["K. Chatterjee", "M. Chmelik", "J. Davies"], "venue": "arXiv preprint arXiv:1511.08456, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Safetyconstrained reinforcement learning for mdps", "author": ["S. Junges", "N. Jansen", "C. Dehnert", "U. Topcu", "J.-P. Katoen"], "venue": "International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, 2016, pp. 130\u2013146.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Control in belief space with temporal logic specifications", "author": ["C.-I. Vasile", "K. Leahy", "E. Cristofalo", "A. Jones", "M. Schwager", "C. Belta"], "venue": "Decision and Control (CDC), 2016 IEEE 55th Conference on. IEEE, 2016, pp. 7419\u20137424.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Replanning in domains with partial information and sensing actions", "author": ["G. Shani", "R.I. Brafman"], "venue": "IJCAI, vol. 2011, 2011, pp. 2021\u2013 2026.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Heuristics for planning under partial observability with sensing actions", "author": ["G. Shani", "R. Brafman", "S. Maliah", "E. Karpas"], "venue": "Proceedings of the 23rd ICAPS Workshop on Heuristics and Search for Domainindependent Planning (HSDIP\u0105\u015513), 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Synthesis of joint control and active sensing strategies under temporal logic constraints", "author": ["J. Fu", "U. Topcu"], "venue": "IEEE Transactions on Automatic Control, vol. 61, no. 11, pp. 3464\u20133476, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "What is decidable about partially observable markov decision processes with \u03c9-regular objectives", "author": ["K. Chatterjee", "M. Chmel\u00edk", "M. Tracol"], "venue": "Journal of Computer and System Sciences, vol. 82, no. 5, pp. 878\u2013911, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Bounded model checking", "author": ["A. Biere", "A. Cimatti", "E.M. Clarke", "O. Strichman", "Y. Zhu"], "venue": "Advances in computers, vol. 58, pp. 117\u2013148, 2003.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Provably-correct stochastic motion planning with safety constraints", "author": ["C. Yoo", "R. Fitch", "S. Sukkarieh"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2013, pp. 981\u2013986.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Monte-carlo planning in large pomdps", "author": ["D. Silver", "J. Veness"], "venue": "Advances in neural information processing systems, 2010, pp. 2164\u2013 2172.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning based supervisor synthesis of pomdp for pctl specifications", "author": ["X. Zhang", "B. Wu", "H. Lin"], "venue": "2015 54th IEEE Conference on Decision and Control (CDC). IEEE, 2015, pp. 7470\u20137475.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Permissive controller synthesis for probabilistic systems", "author": ["K. Dr\u00e4ger", "V. Forejt", "M. Kwiatkowska", "D. Parker", "M. Ujma"], "venue": "International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, 2014, pp. 531\u2013546.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "A tutorial guide to mixed-integer programming models and solution techniques", "author": ["J.C. Smith", "Z.C. Taskin"], "venue": "Optimization in Medicine and Biology, pp. 521\u2013548, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Counterexample-guided permissive supervisor synthesis for probabilistic systems through learning", "author": ["B. Wu", "H. Lin"], "venue": "American Control Conference (ACC), 2015. IEEE, 2015, pp. 2894\u20132899.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Mathematical techniques for analyzing concurrent and probabilistic systems", "author": ["J.J. Rutten", "M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "American Mathematical Soc.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Design and synthesis of synchronization skeletons using branching time temporal logic", "author": ["E.M. Clarke", "E.A. Emerson"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1982}, {"title": "Principles of Model Checking", "author": ["C. Baier", "J. Katoen"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2008}, {"title": "Prism 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Computer aided verification. Springer, 2011, pp. 585\u2013591.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "A storm is coming: A modern probabilistic model checker", "author": ["C. Dehnert", "S. Junges", "J.-P. Katoen", "M. Volk"], "venue": "arXiv preprint arXiv:1702.04311, 2017.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2017}, {"title": "Randomness for free", "author": ["K. Chatterjee", "L. Doyen", "H. Gimbert", "T.A. Henzinger"], "venue": "International Symposium on Mathematical Foundations of Computer Science. Springer, 2010, pp. 246\u2013257.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling and control of logical discrete event systems", "author": ["R. Kumar", "V.K. Garg"], "venue": "Springer Science & Business Media,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Logic and model checking for hidden markov models", "author": ["L. Zhang", "H. Hermanns", "D.N. Jansen"], "venue": "International Conference on Formal Techniques for Networked and Distributed Systems. Springer, 2005, pp. 98\u2013112.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2005}, {"title": "Counterexample generation in probabilistic model checking", "author": ["T. Han", "J.-P. Katoen", "D. Berteun"], "venue": "IEEE Transactions on Software Engineering, vol. 35, no. 2, pp. 241\u2013257, 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Counterexample-guided abstraction refinement for pomdps", "author": ["X. Zhang", "B. Wu", "H. Lin"], "venue": "arXiv preprint arXiv:1701.06209, 2017.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2017}, {"title": "Efficient selectivity and backup operators in monte-carlo tree search", "author": ["R. Coulom"], "venue": "International Conference on Computers and Games. Springer, 2006, pp. 72\u201383.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2002}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "European conference on machine learning. Springer, 2006, pp. 282\u2013 293.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning regular sets from queries and counterexamples", "author": ["D. Angluin"], "venue": "Information and computation, vol. 75, no. 2, pp. 87\u2013106, 1987.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1987}, {"title": "The comics tool\u2013computing minimal counterexamples for dtmcs", "author": ["N. Jansen", "E. \u00c1brah\u00e1m", "M. Volk", "R. Wimmer", "J.-P. Katoen", "B. Becker"], "venue": "International Symposium on Automated Technology for Verification and Analysis. Springer, 2012, pp. 349\u2013353.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "libalf: The automata learning framework", "author": ["B. Bollig", "J.-P. Katoen", "C. Kern", "M. Leucker", "D. Neider", "D.R. Piegdon"], "venue": "Computer Aided Verification. Springer, 2010, pp. 360\u2013364.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],", "startOffset": 177, "endOffset": 180}, {"referenceID": 3, "context": "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],", "startOffset": 201, "endOffset": 204}, {"referenceID": 4, "context": "[5], and human-robot collaboration [6]\u2013[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], and human-robot collaboration [6]\u2013[8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "[5], and human-robot collaboration [6]\u2013[8].", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "This property is very useful in modeling autonomous systems with hidden states, such as advanced driver assistant system (ADAS) [5] and human-robot collaboration [7], [9] where human intention can not be directly observed.", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "This property is very useful in modeling autonomous systems with hidden states, such as advanced driver assistant system (ADAS) [5] and human-robot collaboration [7], [9] where human intention can not be directly observed.", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "This property is very useful in modeling autonomous systems with hidden states, such as advanced driver assistant system (ADAS) [5] and human-robot collaboration [7], [9] where human intention can not be directly observed.", "startOffset": 167, "endOffset": 170}, {"referenceID": 34, "context": "While most of the \u03c9\u2212regular properties are undecidable for POMDPs [35], we consider PCTL specifications with finite horizons which can bound the searching space with finite memory in POMDP model checking following the philosophy of Bounded Model Checking [36].", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "While most of the \u03c9\u2212regular properties are undecidable for POMDPs [35], we consider PCTL specifications with finite horizons which can bound the searching space with finite memory in POMDP model checking following the philosophy of Bounded Model Checking [36].", "startOffset": 255, "endOffset": 259}, {"referenceID": 36, "context": "Meanwhile, a lot of robotics applications require task completion with finite time, such as motion planning [37], which also makes PCTL specification with finite horizons suitable to describe our control tasks.", "startOffset": 108, "endOffset": 112}, {"referenceID": 37, "context": "To check the satisfaction relation efficiently, we show the connection between the model checking and the optimal policy computation, then modify a state-ofart POMDP solving algorithm, Partially Observable MonteCarlo Planning (POMCP) [38], to reduce the computational complexity for POMDP model checking.", "startOffset": 234, "endOffset": 238}, {"referenceID": 9, "context": "statistics for history [10], POMDP can be viewed as MDP with a continuous state space formed by belief states.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "Exact planning of POMDP [11] can be intractable with the size of state space and planning horizon exploding quickly.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "As one of the most popular approaches, pointbased value iteration (PBVI) optimizes the value function only over a selected finite set of belief states and provides the optimization result with a bounded error [12]\u2013[15].", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "As one of the most popular approaches, pointbased value iteration (PBVI) optimizes the value function only over a selected finite set of belief states and provides the optimization result with a bounded error [12]\u2013[15].", "startOffset": 214, "endOffset": 218}, {"referenceID": 15, "context": "Compared to the point-based approach that solves POMDP on the continuous state space of belief states, the controllerbased approach [16] finds the optimal policy represented by a finite state controller (FSC) with finite memory.", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "Each node has one outward edge per observation, and a policy can be executed by taking action associated with the node at current time instance and updating the current node by following the edge labeled by the observation made [17].", "startOffset": 228, "endOffset": 232}, {"referenceID": 17, "context": "This representation is equivalent to Moore machine [18] from automata theory [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "This representation is equivalent to Moore machine [18] from automata theory [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The gradient search usually leads to a suboptimal solution that often traps in local optimum [19], [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "The gradient search usually leads to a suboptimal solution that often traps in local optimum [19], [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "To combine the advantages from gradient ascent and policy iteration, bounded policy iteration (BPI) is proposed in [17] to limit the size of the controller and provide evidence to help escape local optimum.", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "Besides direct graph as the controller form for FSC, DFA and Mealy machine have also been considered in [21] and [16], respectively.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "Besides direct graph as the controller form for FSC, DFA and Mealy machine have also been considered in [21] and [16], respectively.", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "used to generate controllers that can guarantee the system performance to satisfy high-level mission requirements [22]\u2013 [25].", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "used to generate controllers that can guarantee the system performance to satisfy high-level mission requirements [22]\u2013 [25].", "startOffset": 120, "endOffset": 124}, {"referenceID": 25, "context": "For complicated missions, temporal logic [26] is an efficient tool to describe requirements for system tasks due to its expressiveness and similarity to natural languages.", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "Compared to extensive studies in reward-based planning, very few results on formal methods based planning have been established for POMDP, which makes it an open problem [27].", "startOffset": 170, "endOffset": 174}, {"referenceID": 27, "context": "In [28], the controller synthesis of POMDP with Linear Temporal Logic (LTL) specifications over infinite horizon is discussed and solved based on gradient", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "However, this method suffers from local maxima, and the initial choice of the FSC\u2019s structure does not have a systematic guideline [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 28, "context": "In [29], the authors use observation-stationary (memoryless) controller to regulate POMDP to satisfy almost-sure reachability properties.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Since the action is selected only depends on current observation, the satisfiability modulo theories (SMT) method is applied with similar idea shows in [30] where a state-based controller for MDP is learned.", "startOffset": 152, "endOffset": 156}, {"referenceID": 30, "context": "In [31], a linear time invariant system with linear observation model for states is considered, which is equivalent to a discrete time continuous space POMDP.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Similar to POMDPs, the deterministic systems with partial information have also been studied for synthesis problems in [32]\u2013[34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 33, "context": "Similar to POMDPs, the deterministic systems with partial information have also been studied for synthesis problems in [32]\u2013[34].", "startOffset": 124, "endOffset": 128}, {"referenceID": 39, "context": "Especially for permissive controller design, in [40], state-based controllers without memory are proposed for infinite horizon planning and Mixed Integer Linear Programming (MILP) [41] is applied to find a permissive controller.", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "Especially for permissive controller design, in [40], state-based controllers without memory are proposed for infinite horizon planning and Mixed Integer Linear Programming (MILP) [41] is applied to find a permissive controller.", "startOffset": 180, "endOffset": 184}, {"referenceID": 29, "context": "Similarly, in [30], SMT is combined with reinforcement learning to learn a state-based controller.", "startOffset": 14, "endOffset": 18}, {"referenceID": 41, "context": "Besides these works, using the L\u2217 algorithm to learn system supervisor has also been considered in our previous work for MDPs [42].", "startOffset": 126, "endOffset": 130}, {"referenceID": 38, "context": "preliminary conference paper [39].", "startOffset": 29, "endOffset": 33}, {"referenceID": 38, "context": "Compared to [39], this paper makes the following new contributions.", "startOffset": 12, "endOffset": 16}, {"referenceID": 42, "context": "[43] An MDP is a tuple M = (S, s\u0304, A, T ) where", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2022 S is a finite set of states; \u2022 s\u0304 \u2208 S is the initial state; \u2022 A is a finite set of actions; \u2022 T : S \u00d7A\u00d7 S \u2192 [0, 1] is a transition function.", "startOffset": 110, "endOffset": 116}, {"referenceID": 42, "context": "[43] A DTMC is a tuple M = (S, s\u0304, T ) where", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2022 S is a finite set of states; \u2022 s\u0304 \u2208 S is the initial state; \u2022 T : S \u00d7 S \u2192 [0, 1] is a transition function.", "startOffset": 76, "endOffset": 82}, {"referenceID": 42, "context": "where s0 = s\u0304, si \u2208 S, ai \u2208 A and T (si, ai, si+1) \u2265 0 for all i \u2265 0 [43].", "startOffset": 69, "endOffset": 73}, {"referenceID": 42, "context": "For a labeled MDP, we can use PCTL [43] to represent the system design requirements.", "startOffset": 35, "endOffset": 39}, {"referenceID": 43, "context": "PCTL is the probabilistic extension of the Computation Tree Logic (CTL) [44].", "startOffset": 72, "endOffset": 76}, {"referenceID": 42, "context": "[43] The syntax of PCTL is defined as", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "/\u2208 {\u2264, <,\u2265, >}, p \u2208 [0, 1] and k \u2208 N.", "startOffset": 20, "endOffset": 26}, {"referenceID": 42, "context": "[43] For an labeled MDPM = (S, s\u0304, A, T, L), the satisfaction relation for any states s \u2208 S is defined inductively:", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "The model checking of PCTL specification has been extensively studied for MDPs [43].", "startOffset": 79, "endOffset": 83}, {"referenceID": 42, "context": "/ in the specification gives lower or upper bound, PCTL model checking of MDPs solves an optimization problem by computing either the minimum or maximum probability over all adversaries [43].", "startOffset": 186, "endOffset": 190}, {"referenceID": 44, "context": "Since the states are fully observable, the model checking for MDPs can be solved following dynamic programming techniques with polynomial time complexity [45].", "startOffset": 154, "endOffset": 158}, {"referenceID": 45, "context": "Different software tools for MDP model checking are available, such as PRISM [46] and recently developed model checker Storm [47].", "startOffset": 77, "endOffset": 81}, {"referenceID": 46, "context": "Different software tools for MDP model checking are available, such as PRISM [46] and recently developed model checker Storm [47].", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "\u2022 M is an MDP; \u2022 Z is a finite set of observations; \u2022 O : S \u00d7 Z \u2192 [0, 1] is an observation function.", "startOffset": 66, "endOffset": 72}, {"referenceID": 38, "context": "If the initial status of POMDP is given as a probability distribution over S, we can add a dummy initial state then define its transitions to other s \u2208 S based on the initial probability distribution [39].", "startOffset": 200, "endOffset": 204}, {"referenceID": 47, "context": "But for the finite horizon PCTL specifications considered in our work, the pure adversaries and randomized adversaries have the same power in the sense that restricting the set of adversaries to pure strategies will not change the satisfaction relation of the considered PCTL fragments [48].", "startOffset": 286, "endOffset": 290}, {"referenceID": 47, "context": "While the detailed analysis follows the fact that POMDP is a one-and-a-half player game [48], the intuitive justification for this claim is that if we are just interested in upper and lower bounds to the probability of some events to happen, any probabilistic combination of these events stays within the bounds.", "startOffset": 88, "endOffset": 92}, {"referenceID": 47, "context": "Moreover, pure adversaries are sufficient to observe the bounds [48].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "Since the control objective is given by a finite horizon specification, history-dependent controller outperforms history-independent (memoryless or observation-stationary) one and its justification can be directly inherited from MDP cases [40].", "startOffset": 239, "endOffset": 243}, {"referenceID": 38, "context": "[39] A supervisor for POMDP P={S, s\u0304, A, Z, T,O} is a za-DFA F={Q, q\u0304,\u03a3, \u03b4, Qm}, where", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Since DFA is an equivalent representation of regular language [49], za-DFA represents a regular set of strings with the set of the observation-action pairs in POMDP as its alphabet.", "startOffset": 62, "endOffset": 66}, {"referenceID": 48, "context": ", Pref(L(F)) = Lm(F) where Pref(L(F)) denotes all prefixes of the language of F [49].", "startOffset": 80, "endOffset": 84}, {"referenceID": 48, "context": "By applying the subset construction on NFA F , we can get a DFA whose accepted language is U [49].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "Remark: Compared to the global Markov chain defined in [28] describing the regulated behavior of POMDP under an FSC, the product MDP defined in Definition 7 is more general", "startOffset": 55, "endOffset": 59}, {"referenceID": 38, "context": "Compared to the feasibility constraint defined in our previous work [39], here we allow multiple actions being enabled given Algorithm 1: Simulation run of POMDP P regulated by za-DFA F up to time k", "startOffset": 68, "endOffset": 72}, {"referenceID": 49, "context": "These results are modified based on [50] where the probability space for Hidden Markov Model (HMM) is defined.", "startOffset": 36, "endOffset": 40}, {"referenceID": 42, "context": "Then based on the general definition of the probability space on MDP [43], it is not hard to see that the probability spaces on MDPMF and POMDP P are equivalent.", "startOffset": 69, "endOffset": 73}, {"referenceID": 50, "context": "But for finite horizon PCTL, the generality is not lost since lots of finite horizon PCTL specifications can be transformed to bounded until form and the model checking mechanism is similar as shown in [51].", "startOffset": 202, "endOffset": 206}, {"referenceID": 42, "context": "/ [43].", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "Following this method, we can leverage recently developed POMDP solvers that can handle a larger problem size with high-efficiency [13]\u2013[15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Following this method, we can leverage recently developed POMDP solvers that can handle a larger problem size with high-efficiency [13]\u2013[15].", "startOffset": 136, "endOffset": 140}, {"referenceID": 27, "context": "computation problem by modifying the transition structure of POMDP to make all states s |= \u00ac\u03c61 and states s |= \u03c62 absorbing, and designing the reward scheme that assigns 0 to intermediate transitions and 1 to the final transitions on s |= \u03c62 when the planning depth k is reached [28], [38], [52].", "startOffset": 279, "endOffset": 283}, {"referenceID": 37, "context": "computation problem by modifying the transition structure of POMDP to make all states s |= \u00ac\u03c61 and states s |= \u03c62 absorbing, and designing the reward scheme that assigns 0 to intermediate transitions and 1 to the final transitions on s |= \u03c62 when the planning depth k is reached [28], [38], [52].", "startOffset": 285, "endOffset": 289}, {"referenceID": 51, "context": "computation problem by modifying the transition structure of POMDP to make all states s |= \u00ac\u03c61 and states s |= \u03c62 absorbing, and designing the reward scheme that assigns 0 to intermediate transitions and 1 to the final transitions on s |= \u03c62 when the planning depth k is reached [28], [38], [52].", "startOffset": 291, "endOffset": 295}, {"referenceID": 37, "context": "POMDP optimal policy computation algorithm, Partially Observable Monte-Carlo Planning (POMCP) [38], that can well fit with our supervisory control framework.", "startOffset": 94, "endOffset": 98}, {"referenceID": 52, "context": "stead of explicitly solving a POMDP, POMCP applies MonteCarlo tree search [53] by running Monte-Carlo simulations to maintain a search tree of histories.", "startOffset": 74, "endOffset": 78}, {"referenceID": 53, "context": "1 (UCB1) [54] algorithm to maximize V (ha) + c \u221a logN(h)", "startOffset": 9, "endOffset": 13}, {"referenceID": 51, "context": "Then by initializing the current history h0 to empty, we can estimate the optimal value V (h0), which is equal to the maximum satisfaction probability V (h0) [52].", "startOffset": 158, "endOffset": 162}, {"referenceID": 37, "context": "With the convergence guarantee in probability, the bias of the value function E[V (h0) \u2212 V (h0)] is O(log(N(h0))/N(h0) [38].", "startOffset": 119, "endOffset": 123}, {"referenceID": 54, "context": "Given a fixed \u03b4 > 0, the probability of V (h) in the range of [V \u2217(h) \u2212 \u2206n/n, V \u2217(h) + \u2206n/n] is less or equal to \u03b4 with \u2206n = 9 \u221a 2n ln(2/\u03b4) for a sufficiently large number of simulations n [55].", "startOffset": 189, "endOffset": 193}, {"referenceID": 37, "context": "in [38].", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "Within the supervisory control framework using za-DFA, our task of finding a supervisor for POMDP is converted to find a DFA, which is an equivalent representation of regular set [49].", "startOffset": 179, "endOffset": 183}, {"referenceID": 55, "context": "L\u2217 Learning Algorithm The L\u2217 learning algorithm [56] is proposed to learn an unknown regular set [49].", "startOffset": 48, "endOffset": 52}, {"referenceID": 48, "context": "L\u2217 Learning Algorithm The L\u2217 learning algorithm [56] is proposed to learn an unknown regular set [49].", "startOffset": 97, "endOffset": 101}, {"referenceID": 55, "context": "row(y1) = row(y2); for consistence, whenever y1, y2 \u2208 Y with row(y1) = row(y2), it requires that \u2200\u03b1 \u2208 \u03a3, row(y1 \u00b7 \u03b1) = row(y2 \u00b7 \u03b1) [56].", "startOffset": 131, "endOffset": 135}, {"referenceID": 55, "context": ", row(y1) = row(y2) but G(y1 \u00b7\u03b1 \u00b7e) 6= G(y2 \u00b7\u03b1 \u00b7e), then adds \u03b1 \u00b7 e to E and extends the table [56].", "startOffset": 95, "endOffset": 99}, {"referenceID": 55, "context": "With a Teacher being able to answer membership queries and conjectures, L\u2217 algorithm is proved to converge to the minimum DFA accepting U in polynomial time [56].", "startOffset": 157, "endOffset": 161}, {"referenceID": 50, "context": "Then we apply the DTMC counterexample generation algorithm in [51] to get the strongest evidence as a finite path with the maximum probability of violate.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "1] [0, 0, 1]", "startOffset": 3, "endOffset": 12}, {"referenceID": 0, "context": "[1, 1, 1] [0 .", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[1, 1, 1] [0 .", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[1, 1, 1] [0 .", "startOffset": 0, "endOffset": 9}, {"referenceID": 50, "context": "After that, the counterexample selection algorithm will take polynomial time with k and the number of transitions in the derived DTMC [51].", "startOffset": 134, "endOffset": 138}, {"referenceID": 55, "context": "Then combining with the time analysis of L\u2217 in [56], we can see that our algorithm has a complexity exponential with k, polynomial with SP , and Lf .", "startOffset": 47, "endOffset": 51}, {"referenceID": 56, "context": "While currently each part of the learning process is running separately based on different packages, for example, COMICS for DTMC counterexample selection [57], libalf for L\u2217 algorithm [58], POMCP for POMDP solving [38], we will glue every part together to deliver a whole software package for the automatic synthesis purpose.", "startOffset": 155, "endOffset": 159}, {"referenceID": 57, "context": "While currently each part of the learning process is running separately based on different packages, for example, COMICS for DTMC counterexample selection [57], libalf for L\u2217 algorithm [58], POMCP for POMDP solving [38], we will glue every part together to deliver a whole software package for the automatic synthesis purpose.", "startOffset": 185, "endOffset": 189}, {"referenceID": 37, "context": "While currently each part of the learning process is running separately based on different packages, for example, COMICS for DTMC counterexample selection [57], libalf for L\u2217 algorithm [58], POMCP for POMDP solving [38], we will glue every part together to deliver a whole software package for the automatic synthesis purpose.", "startOffset": 215, "endOffset": 219}], "year": 2017, "abstractText": "As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an L\u2217 learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.", "creator": "LaTeX with hyperref package"}}}