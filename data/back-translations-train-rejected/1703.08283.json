{"id": "1703.08283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Experimental Identification of Hard Data Sets for Classification and Feature Selection Methods with Insights on Method Selection", "abstract": "The paper first reports an experimentally identified list of benchmark data sets that are hard for representative classification and feature selection methods. This was done after systematically evaluating a total of 54 combinations of methods, involving nine state-of-the-art classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository (some data sets with known high classification accuracy were excluded). In this paper, a data set for classification is called hard if none of the 54 combinations can achieve an AUC over 0.8 and none of them can achieve an F-Measure value over 0.8; it is called easy otherwise. A total of 17 out of the 129 data sets were found to be hard in that sense. This paper also compares the performance of different methods, and it produces rankings of classification methods, separately on the hard data sets and on the easy data sets. This paper is the first to rank methods separately for hard data sets and for easy data sets. It turns out that the classifier rankings resulting from our experiments are somehow different from those in the literature and hence they offer new insights on method selection.", "histories": [["v1", "Fri, 24 Mar 2017 04:26:22 GMT  (300kb)", "http://arxiv.org/abs/1703.08283v1", "19 pages, 3 figures, 12 tables"]], "COMMENTS": "19 pages, 3 figures, 12 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cuiju luan", "guozhu dong"], "accepted": false, "id": "1703.08283"}, "pdf": {"name": "1703.08283.pdf", "metadata": {"source": "CRF", "title": "Experimental Identification of Hard Data Sets for Classification and Feature Selection Methods with Insights on Method Selection", "authors": ["Cuiju Luan", "Guozhu Dong"], "emails": ["cjluan@shmtu.edu.cn", "guozhu.dong@wright.edu"], "sections": [{"heading": null, "text": "For representative classification and feature selection methods, a total of 54 method combinations have been systematically evaluated (excluding some data sets with notoriously high classification accuracy).In this paper, a data set for classification is described as difficult if none of the 54 combinations can achieve an AUC above 0.8 and none of them can achieve an F-measure value above 0.8; otherwise, it is described as simple. A total of 17 of the 129 data sets have been classified as difficult in this sense.This paper also compares the performance of different methods and produces rankings of classification methods, separated by the hard data sets and by the easydata sets. This paper is the first to classify methods separately for hard data sets and for simple data sets. It turns out that the classification rankings resulting from our experiments differ from those of the main data sets and easydata sets."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to outdo themselves, and they see themselves in a position to outdo themselves."}, {"heading": "2 Algorithms Used in the Study", "text": "The classification algorithms we use are Boosting, Decision Tree, Random Forest, Nearest Neighbor, Logistic Regression, and Support Vector Machine (SVM).The methods we use for feature selection are correlation-based methods, information gain-based methods, and the relief f method (all are filter-based methods).All classification algorithms and feature selection methods we use are implemented in Weka 3.8.0 (Hall et al., 2009) (see Table X in the appendix) (in practice it is rarely used for the same reason (Li et al., 2016)."}, {"heading": "2.1 Classification Algorithms and Parameter Settings", "text": "We selected representative classification algorithms, some of which are based on multiple papers, which have a systematic evaluation of classification algorithms, some of which are based on shared knowledge, and in particular, these algorithms are often better than the others (SVN et al. 2015). We chose a list of common classification algorithms and their implementations in Weka, Decision Tree, Random Forest, Nearest Neighbor, Logistic Regression and SVM, as representatives of the existing classification algorithms. Table 1 shows the correspondence of classification algorithms and their implementations in Weka. Some of the classification algorithms have multiple versions due to different parameters, Logistic Regression and SVM, as representatives of the existing classification algorithms."}, {"heading": "2.2 Feature Selection Methods", "text": "In fact, most of them will be able to play by the rules they have given themselves. (7) Most of them are able to play by the rules. (7) Most of them are unable to play by the rules. (7) Most of them are unable to play by the rules. (7) Most of them are unable to play by the rules. (7) Most of them are able to play by the rules. (7) Most of them are unable to play by the rules. (7) Most of them are unable to play by the rules. (7) Most of them are able to play by the rules themselves. (7)"}, {"heading": "3 Data Sets Included in the Study", "text": "The aforementioned suggestions from the period around 1900 to 1900 caught the public's attention in the years around 1900 to 1900, in the way in which they were able to change and change the world, in the way in which they were able to change, to change and to change the world, in the way in which they were able to change the world, in the way in which they had turned the world upside down, and in the way in which they were able to change and to change the world, in the way in which they were able to change and to change the world, in the way in which they were able to change the world."}, {"heading": "4 Experimental Settings and Evaluation Measures", "text": "For each fold of each dataset, a classification model is created from the other 9 folds, using each of the 54 combinations of nine classifiers and six methods for selecting characteristics. As is widely noted in the literature, simple accuracy measurement may not be sufficient for unbalanced datasets. Because some of the datasets used in our experiments are not balanced, we have not used accuracy measurement; we have the AUC and F measurements unstead.AUC is the probability that the underlying classifier values a randomly selected positive instance higher than a randomly selected negative instance (Fawcett, 2006). It is also referred to as the ROC range in Weka. F measurement is the harmonic mean of precision and recall. AUC has several desirable properties as a classification performance measure, such as decision threshold independent and invariant in terms of a priori class probabilities."}, {"heading": "5 Experimental Results and Discussion", "text": "In this section, we will first present the 17 hard data sets identified, and then analyze the performance of the classification and the selection methods under various conditions, presenting for each hard data set the best or worst combinations for the data set, building on this the most common best classification algorithm, the most common method for selecting the best characteristics, etc. Similarly, the worst combinations are presented, and the most common operative methods are identified. Finally, rankings of classification algorithms are determined based solely on the results of the 98 data sets for which complete results were obtained for all 54 combinations. To facilitate the discussion, we will introduce a few terms and notations for each data set. It should be noted that the rankings are based solely on the results of the 98 data sets for which complete results were obtained for all 54 combinations."}, {"heading": "5.2 Best and Worst Method Combinations for the Hard Data Sets", "text": "For each of them we have the best selection methods (the best selection methods); the best combinations for ten (10) of the best records we have all; the best are the use of feature methods (58.8%); the best combinations for three (17.6%); for the remaining four (4) records we have the best (5)."}, {"heading": "5.3 Maximum and Minimum AUC by All Combinations for the Hard/Easy Data Sets", "text": "Let min54AUC be defined for each dataset similar to max54AUC and let span54AUC Figure 1 be max54AUC or min54AUC for the fixed datasets. We note that the maximum voltage 54AUC, minimum voltage54AUC and average voltage54AUC are 0.277, 0.083 or 0.182, respectively. Therefore, the choice of classification and feature selection methods often has a major impact on the classification accuracy of the fixed datasets. Figure 2 shows max54AUC or min54AUC for all simple datasets. We note that the maximum voltage54AUC, minimum voltage54AUC and average voltage54AUC are 0.603, 0.056 or 0.351, respectively. In addition, span54AUC is greater than 0.3 for 66.7% of the simple datasets. While several simple datasets can be well classified by all 54 combinations, the difference in classification performance by different combinations is large."}, {"heading": "5.4 Average AUC and Average F-Measure for the Hard/Easy Data Sets", "text": "In fact, it is the case that most people who are able to determine for themselves what they want and what they want to do, and that they are not willing to abide by the rules. (...) It is not the case that they feel able to understand the rules that they have imposed on themselves. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that the rules of the market adhere. (...) It is not the case that they adhere to the rules. (...) It is not the case that they adhere to the rules. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is not the case. (...) It is. (...) It is. () It is. () It is. () It is. () It is. (... It is. () It is. () It is not. () It is. () It is. () It is. (... It is. It is. () It is. It is. () It is. It is. () It is. It is. (. () It is. () It is. It is. It is. (. () It is. It is. It is. It is. (). It is. It is."}, {"heading": "5.5 Summary of Classifier Rankings on Hard Data Sets and on Easy Data Sets", "text": "Tables 9 and 10 summarize the rankings of the 9 classification algorithms for the hard and simple records, respectively. Each of these algorithms gives two classification classifications, one based on the number of records for which the classifier is the best, and the other based on the average AUC. # DatasetsBest is the number of records for which a particular algorithm receives the maximum AUC. There are several classifiers that receive the maximum AUC for some simple records, so the sum of # DatasetsBest in Table 10 (left field) is greater than the number of simple datasets.Classifier # DatasetsBest Classifier AVG _ AUC"}, {"heading": "J48 0 SVM-LN 0.580", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Conclusions", "text": "This paper reported on a systematic evaluation of classification performance by representative, state-of-the-art classification algorithms and feature selection methods based on 129 UCI data sets. It identified a list of benchmark data sets that are difficult for representative classification and feature selection methods. It ranked the classification algorithms based on their performance on the hard data sets and their performance on the simple data sets. It also compared the effectiveness of feature selection methods. To the best of knowledge, this study is the first to present a list of hard benchmark data sets in the machine learning literature and classify classification algorithms by taking their performance on the hard data sets into account. This list of hard benchmark data sets can be useful in motivating the development of new classification and feature selection algorithms and in evaluating such algorithms."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Shanghai Natural Science Foundation (grant number. The authors would like to thank Dr. Huan Liu and Dr. Lei Yu for encouraging us to conduct this study. Part of the work of the first author was done during a visit to the Data Mining Research Lab at Wright State University. Appendix X indicates the runtime of the wrapper method based on several sets of data, showing that the method is very time consuming.Table XI lists the 31 sets of data for which our experiments with the 54 classification algorithm and feature selection method combinations took more than 120 hours.For each of the 31 sets of data, calculations for at least one of the 54 combinations were not completed (i.e. for some of the combinations, the 10-fold cross validation took more than 120 hours), \"abnormal program termination,\" etc. For each of the 31 sets, the calculation for at least one of the 54 data sets was not completed, as the 54 combinations took too much time to complete (i.e. the AUC)."}], "references": [{"title": "Do we need hundreds of classifiers to solve real world classification problems", "author": ["Manuel Fern\u00e1ndez-Delgado", "Eva Cernadas", "Sen\u00e9n Barro", "Dinani Amorim"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fern\u00e1ndez.Delgado et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fern\u00e1ndez.Delgado et al\\.", "year": 2014}, {"title": "Experimental perspectives on learning from imbalanced data", "author": ["Jason Van Hulse", "Taghi M. Khoshgoftaar", "Amri Napolitano"], "venue": "In Proceedings of the 24th Annual International Conference on Machine Learning,", "citeRegEx": "Hulse et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hulse et al\\.", "year": 2007}, {"title": "Feature selection: A data perspective", "author": ["Jundong Li", "Kewei Cheng", "Suhang Wang", "Fred Morstatter", "Robert P Trevino", "Jiliang Tang", "Huan Liu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "The WEKA Data Mining Software: An Update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers", "author": ["Abraham J. Wyner", "Matthew Olson", "Justin Bleich", "David Mease"], "venue": null, "citeRegEx": "Wyner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wyner et al\\.", "year": 2015}, {"title": "Toward Integrating Feature Selection Algorithms for Classification and Clustering", "author": ["Huan Liu", "Lei Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Liu and Yu.,? \\Q2005\\E", "shortCiteRegEx": "Liu and Yu.", "year": 2005}, {"title": "Feature Selection for Classification", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Dash and Liu.,? \\Q1997\\E", "shortCiteRegEx": "Dash and Liu.", "year": 1997}, {"title": "Theoretical and empirical analysis of ReliefF and RReliefF", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine Learning,", "citeRegEx": "Robnik.\u0160ikonja and Kononenko.,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja and Kononenko.", "year": 2003}, {"title": "UCI Machine Learning Repository [http://archive.ics.uci.edu/ml", "author": ["M. Lichman"], "venue": "University of California, School of Information and Computer Science,", "citeRegEx": "Lichman.,? \\Q2013\\E", "shortCiteRegEx": "Lichman.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Reference (Fern\u00e1ndez-Delgado et al., 2014) is a main representative of such studies, which used 121 data sets to evaluate 179 classifiers.", "startOffset": 10, "endOffset": 42}, {"referenceID": 0, "context": "Moreover, the study will use the area under the ROC (AUC) and F-Measure, instead of the accuracy measure (as was done in Fern\u00e1ndez-Delgado et al., 2014), to evaluate the performance of classification models. These measures were chosen based on the recent consensus that the accuracy measure has significant shortcomings when compared with the above two measures, especially AUC. To identify a list of benchmark data sets that are hard for representative classification and feature selection methods, we perform a systematical evaluation of 54 combinations, involving nine representative classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository. We note that some data sets with known high classification accuracy based on results of Fern\u00e1ndez-Delgado et al. (2014) were excluded in our experiments.", "startOffset": 121, "endOffset": 824}, {"referenceID": 2, "context": "3 experiments, we also considered the wrapper based method, but we decided to exclude it due to its computational expensiveness (see Table X in Appendix) (it is seldom used in practice (Li et al., 2016) due to the same reason).", "startOffset": 185, "endOffset": 202}, {"referenceID": 3, "context": "0 (Hall et al., 2009).", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": "In particular, reference (Fern\u00e1ndez-Delgado et al., 2014) showed Random Forest and SVM are often better than the others, and reference (Wyner et al.", "startOffset": 25, "endOffset": 57}, {"referenceID": 4, "context": ", 2014) showed Random Forest and SVM are often better than the others, and reference (Wyner et al. 2015) gave a list of common-used successful classification algorithms.", "startOffset": 85, "endOffset": 104}, {"referenceID": 5, "context": "Feature selection methods are used to remove irrelevant, redundant, or noisy attributes, with the aim of speeding up the computation and improving the accuracy (Li et al., 2009; Liu and Yu, 2005; Dash and Liu, 1997).", "startOffset": 160, "endOffset": 215}, {"referenceID": 6, "context": "Feature selection methods are used to remove irrelevant, redundant, or noisy attributes, with the aim of speeding up the computation and improving the accuracy (Li et al., 2009; Liu and Yu, 2005; Dash and Liu, 1997).", "startOffset": 160, "endOffset": 215}, {"referenceID": 5, "context": "We selected the following filter methods because they are commonly used (Liu and Yu, 2005; Robnik-\u0160ikonja and Kononenko, 2003): the correlation based method, the information gain based method and the relief-f method.", "startOffset": 72, "endOffset": 126}, {"referenceID": 7, "context": "We selected the following filter methods because they are commonly used (Liu and Yu, 2005; Robnik-\u0160ikonja and Kononenko, 2003): the correlation based method, the information gain based method and the relief-f method.", "startOffset": 72, "endOffset": 126}, {"referenceID": 8, "context": "Our experiments used 129 data sets, all from the UCI repository (Lichman, 2013).", "startOffset": 64, "endOffset": 79}, {"referenceID": 0, "context": "(a) From the data sets studied in (Fern\u00e1ndez-Delgado et al., 2014), we first selected 34 data sets such that the maximum reported accuracy of (Fern\u00e1ndez-Delgado et al.", "startOffset": 34, "endOffset": 66}, {"referenceID": 0, "context": ", 2014), we first selected 34 data sets such that the maximum reported accuracy of (Fern\u00e1ndez-Delgado et al., 2014) is below 0.", "startOffset": 83, "endOffset": 115}, {"referenceID": 0, "context": "(b) Because (Fern\u00e1ndez-Delgado et al., 2014) only dealt with UCI data sets dated before March 2013, we examined the 91 UCI data sets whose dates are between January 2013 and June 2016.", "startOffset": 12, "endOffset": 44}, {"referenceID": 0, "context": "We note that RF is the best or the second best for both easy and hard data sets, which is in strong agreement with the ranking of algorithms provided by (Fern\u00e1ndez-Delgado et al., 2014).", "startOffset": 153, "endOffset": 185}, {"referenceID": 0, "context": "However, Figure 3 shows that SVM-PN is the last one in the rank for both easy and hard data sets, which is very different from the ranking give by (Fern\u00e1ndez-Delgado et al., 2014) (which found SVM to be the second best classification algorithm).", "startOffset": 147, "endOffset": 179}, {"referenceID": 0, "context": "There are at least three potential reasons for the disagreement: We used AUC and F-Measure whereas (Fern\u00e1ndez-Delgado et al., 2014) used accuracy.", "startOffset": 99, "endOffset": 131}, {"referenceID": 0, "context": "15 data sets for which (Fern\u00e1ndez-Delgado et al., 2014) reported high classification accuracies (by any of the classification algorithms) and we included some data sets not studied in (Fern\u00e1ndez-Delgado et al.", "startOffset": 23, "endOffset": 55}, {"referenceID": 0, "context": ", 2014) reported high classification accuracies (by any of the classification algorithms) and we included some data sets not studied in (Fern\u00e1ndez-Delgado et al., 2014).", "startOffset": 136, "endOffset": 168}, {"referenceID": 0, "context": "(3) In our ranking, we separated data sets into a hard pool and an easy pool, whereas (Fern\u00e1ndez-Delgado et al., 2014) considered all data sets in one pool.", "startOffset": 86, "endOffset": 118}], "year": 2017, "abstractText": "The paper first reports an experimentally identified list of benchmark data sets that are hard for representative classification and feature selection methods. This was done after systematically evaluating a total of 54 combinations of methods, involving nine state-of-the-art classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository (some data sets with known high classification accuracy were excluded). In this paper, a data set for classification is called hard if none of the 54 combinations can achieve an AUC over 0.8 and none of them can achieve an F-Measure value over 0.8; it is called easy otherwise. A total of 17 out of the 129 data sets were found to be hard in that sense. This paper also compares the performance of different methods, and it produces rankings of classification methods, separately on the hard data sets and on the easy data sets. This paper is the first to rank methods separately for hard data sets and for easy data sets. It turns out that the classifier rankings resulting from our experiments are somehow different from those in the literature and hence they offer new insights on method selection.", "creator": "PDFCreator 2.5.1.5"}}}