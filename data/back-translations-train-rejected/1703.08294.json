{"id": "1703.08294", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Multi-Level Discovery of Deep Options", "abstract": "Augmenting an agent's control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72% accuracy.", "histories": [["v1", "Fri, 24 Mar 2017 06:35:46 GMT  (1163kb,D)", "http://arxiv.org/abs/1703.08294v1", null], ["v2", "Thu, 5 Oct 2017 07:33:58 GMT  (1122kb,D)", "http://arxiv.org/abs/1703.08294v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roy fox", "sanjay krishnan", "ion stoica", "ken goldberg"], "accepted": false, "id": "1703.08294"}, "pdf": {"name": "1703.08294.pdf", "metadata": {"source": "CRF", "title": "Multi-Level Discovery of Deep Options", "authors": ["Roy Fox", "Sanjay Krishnan", "Ion Stoica", "Ken Goldberg"], "emails": ["ROYF@BERKELEY.EDU", "SANJAYKRISHNAN@BERKELEY.EDU", "ISTOICA@BERKELEY.EDU", "GOLDBERG@BERKELEY.EDU"], "sections": [{"heading": null, "text": "Extending an agent's control with useful higher-value behaviors called options can greatly reduce the sample complexity of amplification learning, but the manual design of options is not feasible in high-dimensional and abstract states. While recent work has suggested several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy gradient algorithm that detects parameterized options from a series of demonstration paths and can be used recursively to detect additional levels of hierarchy. We show that the scalability of our approach to multi-level hierarchies stems from the decoupling of low option discovery from the high level of metaction learning, facilitated by sub-parameterizing the high level, and that the use of discovered options to enhance the unified space can not be guided by action-learning actions in Deep-Q."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "2. Related Work", "text": "The field of hierarchical reinforcement has a long history (Barto & Mahadevan, 2003; Daniel, 1997; Parr, 1998; Sutton et al., 1999) and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet al., 2016) and in the analysis of hierarchical structures (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011) Early work in hierarchical control demonstrated the advantages of hierarchical structures through manual hierarchy (Brooks, 1986) and by learning various manual specifications. State abstractions (Dayan & Hinton, 1992; Hengst, 2002; Kolter et al., 2007; Konidaris & Barto, 2007), a series of waypoints (Kaelbling, 1993), low-level skills."}, {"heading": "3. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Markov Decision Processes", "text": "We consider a discretely time-discounted Markov decision-making process (MDP), which is described by a 6-fold xS, A, p0, p, r, \u03b3y, where S denotes the state space, A the action space, p0ps0q the initial state distribution, ppst '1 | st, atq the state transitional distribution, rpst, atq P R the reward function, and \u03b3 P r0, 1q the discount factor. A policy \u03c0pat | stq defines a conditional probability distribution over actions given to the state. A path is defined as a sequence of states and actions, such as ps0, a0, s1,...., sT q of a given length T. In a given MDP, a policy induces the distribution over the trajectories: P\u03c0pB \"p0ps0q T\" 1\u017at \"0 \u03c0pat | stqppst '1 | st, atq.The return of a policy is its expected total discounting over the trajectories: P0ctT 1psut.\""}, {"heading": "3.2. Reinforcement Learning", "text": "A successful approach to solving this problem is Q-Learning (Sutton & Barto, 1998; Watkins & Dayan, 1992), a reinforcement learning algorithm to estimate the maximum return Qps, aq, which is the expected discounted total remuneration achieved by taking measures a in the states and then following the optimal policy. Q-Learning is apolitical in that it scans a state si, a measure ai, a reward ri \"rpsi, aiq and the next state s1i\" pp \"| si, aiq using a sub-optimal exploration policy. On the basis of these samples, Q-Learning then updates its estimate of Qpsi, aiq greedily towards the one-step backward estimate ri\" ri \"ri\" max a1 Qps1i, a1q."}, {"heading": "3.3. Imitation Learning", "text": "Imitation learning (IL) is an alternative to the reinforcement learning environment, where policy is learned from demonstrations of expert behavior, not from rewarding an environment. In the context of behavioral cloning (BC), the goal is to get as close to expert policy as possible. We can formalize this setting as an estimate of the parameters of a generative model. Suppose that a demonstration curve of \"ps0, a0, s1,.., sT q\" is generated by an unknown policy approach according to the following generative model: Initiate t \"0, s0\" p0 for t \"0,.., T '1 doDraw for\" \u03c0 \"p\" | stq Draw st \"1\" pp., \"atqend forWe can define a parametrized set of policies preventabilize and find the parameters that maximize the log-likelihoodLrdging;\" log \"p0ps0q\" T \"ettoq4\" makes this dynamics factor vulnerable. \""}, {"heading": "3.4. The Options Framework", "text": "Control measures that perform long and complex tasks often require a very high-dimensional parameterization of options, which can take many ways to learn. The Options Framework is a hierarchical political structure that can mitigate this example complexity by dividing politics into simpler skills called options (Sutton et al., 1999). One option is a subordinate control primitive that can be invoked by meta-control policies at a higher level of the hierarchy to perform a certain sub-task. Formally, an option h is described by a triplet xIh, \u03c0h, \u0441hy, where Ih \u0441hy denotes the initiation set, \u03c0hpat | stq the control policy, and \u0441hpstq P r0, 1s the termination policy. When the process reaches the P Ih of a state, option h can be invoked to implement the policies."}, {"heading": "4. Discovery of Deep Options", "text": "In this section, we present the Discovery of Deep Options (DDO), a policy-gradient algorithm that determines parameterized options from a series of demonstration paths (sequences of states and actions).The option parameters are derived by adapting a generative model to the trajectories observed. These demonstrations do not need to be given by an optimal agent - the demonstrator may be a person who is not an expert, or a partially trained algorithmic controller. Our approach only assumes that the trajectories are meaningful, which actions should be performed in each visited state, and that these preferences can be represented in a hierarchical structure (see Section C)."}, {"heading": "4.1. Imitation Learning for Option Discovery", "text": "We generalize the default IL to hierarchical control by introducing hierarchical behavioral cloning (HBC). In HBC, the meta-control signals that form the hierarchy are unobservable, latent variables of the generative model that must be derived. Consider a trajectory generated by a two-step hierarchy: \"ps0, a0, s1,..., sT q.\" The lower level implements a set of options x \"0, s0\" p0, hPH. \"The upper level implements a meta meta control policy \u03b7pht | stq that repeatedly selects an option h considering the current state and executes it until termination. Our hierarchical generative model is:\" Initialize t \"0, s0\" p0, hPH, \"h0\" \u03b7p \"| s0q for t\" 0,"}, {"heading": "4.2. Expectation-Gradient Algorithm", "text": "\"We have the possibility that the probability of such an evolution depends on the latent sequence,\" hT q of the meta actions and the termination of the indicators, in order to use a given dataset method based on a given dataset method. \"The probability of such a development depends on the latent sequence.,\" hT q of the meta actions and the termination of the indicators, and to use a secured optimization method, we rewrite the gradients. \"We have the possibility of rewriting the gradients.,\" hT q of the meta actions and termination indicators, to use a gradient-based optimization method that we apply as gradients. \""}, {"heading": "4.3. Deeper Hierarchies", "text": "A D-level hierarchy can be considered a 2-level hierarchy in which the \"high level\" has a hierarchical structure at the pD '1q level, while the value of a meta-control policy depends on what options are available, potentially leading to an exponential growth in the size of the latent variables needed for an inference, and the available data may not be sufficient to learn a policy so expressive.We can avoid this problem by using a simplified parameterization for the medium meta-control level that applies a policy at the hypothesis level."}, {"heading": "5. Experiments", "text": "We present an empirical study of DDO. Our results suggest that DDO can find options that accelerate the learning of reinforcement measures. We examine two different scenarios: (supervision) with a supervisor who demonstrates a few times how to perform a task, show that the options discovered are useful to accelerate the learning of reinforcement measures for the same task; (exploration) we apply reinforcement measures for T episodes, sample histories from current best policy, and expand the scope for action by the discovered options for the remaining T-1 episodes. The first set of experiments illustrates DDO on a number of GridWorld domains. We then show how the expectation gradient can be scaled to more difficult Atari-RAM domains. Finally, we show that DDO can be used to identify visual primitives in surgical data."}, {"heading": "5.1. Four Rooms GridWorld", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "5.2. Atari RAM Games", "text": "The RAM variant of the popular Atari Deep Reinforcement Learning Domain looks at a game developer who dictates not the screen but the RAM state of the Atari machine. This RAM state is a 128-byte vector that fully determines the state of the game and can be encoded in a uniform representation as s P R128\u0445 256. RAM state space illustrates the power of an automated options discovery framework, as it would be impossible to encode options manually without carefully understanding the memory structure of the game. With a discovery algorithm, we have a universal approach to learning in this environment. All strategies are parameterized with a deep network. There are three dense layers, each with tangible activations, and the output distribution is a softmax of the last layer, which has five episodes of game character."}, {"heading": "5.3. Segmentation of Robotic-Assisted Surgery", "text": "In this section, we demonstrate the broad applicability of the DDO framework by applying it to human demonstrations in a robotic area. We apply DDO to long robot trajectories (e.g. 3 minutes) demonstrating a complicated task, and discover options for useful sub-tasks, as well as segmentation of demonstrations into semantic units. The JIGSAWS dataset consists of robotic surgical trajectories of human surgeons performing training procedures (Gao, 2014).The dataset was captured with the da Vinci surgical system by eight surgeons of different skill levels, each performing five repetitions of needle delivery, sewing, and tying. This dataset consists of videos and kinematic data from the robotic arms and is commented by experts who identify the activity occurring in each frame."}, {"heading": "6. Discussion", "text": "In this paper, we introduced the DDO algorithm, which can detect parameterized options from a number of demonstration paths and use them recursively to detect multi-level hierarchies. Our results show that the discovered options accelerate learning in RL problems. However, an important point of discussion is the definition of the hyperparameters of DDO, namely (1) the number of options at each level of the hierarchy, (2) how many RL episodes wait before applying DDO, and (3) how the options parameterization is designed. In our experiments, we matched the number of options based on the observation of the RL agent with these options."}, {"heading": "Acknowledgements", "text": "This research was conducted at the AUTOLAB of the University of Berkeley in collaboration with the Berkeley AI Research (BAIR) Lab, the Real-Time Intelligent Secure Execution (RISE) Lab and the CITRIS \"People and Robots\" (CPAR) Initiative. Partially supported by the U.S. National Science Foundation under the NRI Award IIS1227536: Multilateral Manipulation by Human-Robot Collaborative Systems and the Berkeley Deep Drive (BDD) Program, the DHS Award HSHQDC-16-3-00083, the NSF CISE Expeditions Award CCF-1139158, as well as donations and gifts from Siemens, Google, Cisco, Autodesk, IBM, Ant Financial, Amazon Web Services, CapitalOne, Ericsson, GE, Huawei, Intel, Microsoft and VMwar."}, {"heading": "A. Forward-Backward Algorithm", "text": "\"Despite the exponential domain size of the latent variables,\" Expectation-Gradient for trajectq \"allows us to decompose the posterior variable,\" Expectation-Gradient for trajectq, \"and deal only separately with each posterior marginal variable. Thus, these marginal posteriors can be calculated by a forward-backward dynamic programming algorithm, similar to Baum-Welch (Baum, 1972).\" By first calculating the current parameters and trajectors derived from our notation, we calculate the probability of a trajector prefix \"Pps0, a0,., ht\" hq, \"using the forward recurrent parameters,,,,,,, p0ps0qtqtqtph,\" s0qtph, 1ph1qtqtqtqt1, \".\""}, {"heading": "B. Supplemental Experiments", "text": "This year it is more than ever before."}, {"heading": "C. Additional Considerations", "text": "The attempt to adapt the hierarchical generative model to demonstrations implicitly presupposes that there is a useful structure to be discovered at all. Hierarchy assumes that for each option h, the meta-control policy can identify a number of states where h is advantageous by setting the ph | sq high; that the advantage of applying the policy \u03c0h, if applied in such a state, is likely to spread to the next state; and that the termination policy can identify states where h is no longer advantageous by setting the ph | sq high. Under these assumptions, meta-control policy benefits from the fact that it only takes care of slowly changing state characteristics, while each option benefits from taking care only of local state characteristics. An important aspect is the expressiveness of the parameterization of \u03c0h, \u0445h and \u03b7 and its effects on the \"life expectancy\" of options, i.e. the expected time to their end."}], "references": [{"title": "Learning with options: Just deliberate and relax", "author": ["Bacon", "Pierre-Luc", "Precup", "Doina"], "venue": "In NIPS Bounded Optimality and Rational Metareasoning Workshop,", "citeRegEx": "Bacon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2015}, {"title": "The option-critic architecture", "author": ["Bacon", "Pierre-Luc", "Harb", "Jean", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1609.05140,", "citeRegEx": "Bacon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2016}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Barto", "Andrew G", "Mahadevan", "Sridhar"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barto et al\\.", "year": 2003}, {"title": "An equality and associated maximization technique in statistical estimation for probabilistic functions of markov processes", "author": ["Baum", "Leonard E"], "venue": "Inequalities, 3:1\u20138,", "citeRegEx": "Baum and E.,? \\Q1972\\E", "shortCiteRegEx": "Baum and E.", "year": 1972}, {"title": "Hierarchical models of behavior and prefrontal function", "author": ["Botvinick", "Matthew M"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Botvinick and M.,? \\Q2008\\E", "shortCiteRegEx": "Botvinick and M.", "year": 2008}, {"title": "Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective", "author": ["Botvinick", "Matthew M", "Niv", "Yael", "Barto", "Andrew C"], "venue": null, "citeRegEx": "Botvinick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Botvinick et al\\.", "year": 2009}, {"title": "A robust layered control system for a mobile robot", "author": ["Brooks", "Rodney"], "venue": "IEEE journal on robotics and automation,", "citeRegEx": "Brooks and Rodney.,? \\Q1986\\E", "shortCiteRegEx": "Brooks and Rodney.", "year": 1986}, {"title": "Policy recognition in the abstract hidden Markov model", "author": ["Bui", "Hung Hai", "Venkatesh", "Svetha", "West", "Geoff"], "venue": null, "citeRegEx": "Bui et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2002}, {"title": "Hierarchical relative entropy policy search", "author": ["Daniel", "Christian", "Neumann", "Gerhard", "Peters", "Jan"], "venue": "In AISTATS, pp", "citeRegEx": "Daniel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2012}, {"title": "Probabilistic inference for determining options in reinforcement learning", "author": ["Daniel", "Christian", "Van Hoof", "Herke", "Peters", "Jan", "Neumann", "Gerhard"], "venue": "Machine Learning,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Feudal reinforcement learning", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Dayan et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1992}, {"title": "Hierarchical reinforcement learning with the MAXQ value function", "author": ["Dietterich", "Thomas G"], "venue": "decomposition. JAIR,", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Cstochastic neural networks for hierarchical reinforcement learning", "author": ["Florensa", "Carlos", "Duan", "Yan", "Abbeel", "Pieter"], "venue": "In ICLR,", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Principled option learning in Markov decision processes", "author": ["Fox", "Roy", "Moshkovitz", "Michal", "Tishby", "Naftali"], "venue": "arXiv preprint arXiv:1609.05524,", "citeRegEx": "Fox et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2016}, {"title": "The JHU-ISI gesture and skill assessment dataset (jigsaws): A surgical activity working set for human motion modeling", "author": ["Gao", "Yixin"], "venue": "In Medical Image Computing and Computer-Assisted Intervention (MICCAI),", "citeRegEx": "Gao and Yixin,? \\Q2014\\E", "shortCiteRegEx": "Gao and Yixin", "year": 2014}, {"title": "Bounded rationality, abstraction, and hierarchical decision-making: An informationtheoretic optimality principle", "author": ["Genewein", "Tim", "Leibfried", "Felix", "Grau-Moya", "Jordi", "Braun", "Daniel Alexander"], "venue": "Frontiers in Robotics and AI,", "citeRegEx": "Genewein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Genewein et al\\.", "year": 2015}, {"title": "Active imitation learning of hierarchical policies", "author": ["Hamidi", "Mandana", "Tadepalli", "Prasad", "Goetschalckx", "Robby", "Fern", "Alan"], "venue": "In IJCAI, pp", "citeRegEx": "Hamidi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hamidi et al\\.", "year": 2015}, {"title": "Learning and transfer of modulated locomotor controllers", "author": ["Heess", "Nicolas", "Wayne", "Greg", "Tassa", "Yuval", "Lillicrap", "Timothy", "Riedmiller", "Martin", "Silver", "David"], "venue": "arXiv preprint arXiv:1610.05182,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Discovering hierarchy in reinforcement learning with HEXQ", "author": ["Hengst", "Bernhard"], "venue": "In ICML,", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "A feedback control structure for on-line learning tasks", "author": ["Huber", "Manfred", "Grupen", "Roderic A"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Huber et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Huber et al\\.", "year": 1997}, {"title": "Hierarchical linearlysolvable Markov decision problems", "author": ["Jonsson", "Anders", "G\u00f3mez", "Vicen\u00e7"], "venue": "arXiv preprint arXiv:1603.03267,", "citeRegEx": "Jonsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jonsson et al\\.", "year": 2016}, {"title": "Hierarchical learning in stochastic domains: Preliminary results", "author": ["Kaelbling", "Leslie Pack"], "venue": "In ICML, pp", "citeRegEx": "Kaelbling and Pack.,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling and Pack.", "year": 1993}, {"title": "Hierarchical apprenticeship learning with application to quadruped locomotion", "author": ["Kolter", "J Zico", "Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Kolter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2007}, {"title": "Building portable options: Skill transfer in reinforcement learning", "author": ["Konidaris", "George", "Barto", "Andrew G"], "venue": "In IJCAI,", "citeRegEx": "Konidaris et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2007}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["Konidaris", "George", "Barto", "Andrew G"], "venue": "In NIPS, pp", "citeRegEx": "Konidaris et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2009}, {"title": "Robot learning from demonstration by constructing skill trees", "author": ["Konidaris", "George", "Kuindersma", "Scott", "Grupen", "Roderic A", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Konidaris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2012}, {"title": "Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning", "author": ["Krishnan", "Sanjay", "Garg", "Animesh", "Patil", "Sachin", "Lea", "Colin", "Hager", "Gregory", "Abbeel", "Pieter", "Goldberg", "Ken"], "venue": "ISRR,", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards", "author": ["Krishnan", "Sanjay", "Garg", "Animesh", "Liaw", "Richard", "Thananjeyan", "Brijen", "Miller", "Lauren", "Pokorny", "Florian T", "Goldberg", "Ken"], "venue": null, "citeRegEx": "Krishnan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik", "Saeedi", "Ardavan", "Tenenbaum", "Josh"], "venue": "In NIPS,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Option discovery in hierarchical reinforcement learning using spatiotemporal clustering", "author": ["Lakshminarayanan", "Aravind S", "Krishnamurthy", "Ramnandan", "Kumar", "Peeyush", "Ravindran", "Balaraman"], "venue": "arXiv preprint arXiv:1605.05359,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Unified inter and intra options learning using policy gradient methods", "author": ["Levy", "Kfir Y", "Shimkin", "Nahum"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "Levy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2011}, {"title": "Composing meta-policies for autonomous driving using hierarchical deep reinforcement", "author": ["Liaw", "Richard", "Krishnan", "Sanjay", "Garg", "Animesh", "Crankshaw", "Daniel", "Gonzalez", "Joseph E", "Goldberg", "Ken"], "venue": null, "citeRegEx": "Liaw et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liaw et al\\.", "year": 2017}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "In ICML, pp", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "The EM algorithm and extensions, volume 382", "author": ["McLachlan", "Geoffrey", "Krishnan", "Thriyambakam"], "venue": null, "citeRegEx": "McLachlan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McLachlan et al\\.", "year": 2007}, {"title": "Q-cut\u2014 dynamic discovery of sub-goals in reinforcement learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In ECML,", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Parr", "Ronald", "Russell", "Stuart J"], "venue": "In NIPS, pp", "citeRegEx": "Parr et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Parr et al\\.", "year": 1997}, {"title": "Hierarchical control and learning for Markov decision processes", "author": ["Parr", "Ronald Edward"], "venue": "PhD thesis, UNIVERSITY of CALIFORNIA at BERKELEY,", "citeRegEx": "Parr and Edward.,? \\Q1998\\E", "shortCiteRegEx": "Parr and Edward.", "year": 1998}, {"title": "Optimization with EM and expectation-conjugategradient", "author": ["Salakhutdinov", "Ruslan", "Roweis", "Sam", "Ghahramani", "Zoubin"], "venue": "In ICML, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Universal value function approximators", "author": ["Schaul", "Tom", "Horgan", "Daniel", "Gregor", "Karol", "Silver", "David"], "venue": "In ICML, pp", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Unsupervised perceptual rewards for imitation learning", "author": ["Sermanet", "Pierre", "Xu", "Kelvin", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1612.06699,", "citeRegEx": "Sermanet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2016}, {"title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning", "author": ["Sharma", "Sahil", "Lakshminarayanan", "Aravind S", "Ravindran", "Balaraman"], "venue": "In ICLR,", "citeRegEx": "Sharma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2017}, {"title": "Using relative novelty to identify useful temporal abstractions in reinforcement learning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In ICML,", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2004\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2004}, {"title": "Optimal behavioral hierarchy", "author": ["Solway", "Alec", "Diuk", "Carlos", "C\u00f3rdova", "Natalia", "Yee", "Debbie", "Barto", "Andrew G", "Niv", "Yael", "Botvinick", "Matthew M"], "venue": "PLOS Comput Biol,", "citeRegEx": "Solway et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solway et al\\.", "year": 2014}, {"title": "Automated discovery of options in reinforcement learning", "author": ["Stolle", "Martin"], "venue": "PhD thesis, McGill University,", "citeRegEx": "Stolle and Martin.,? \\Q2004\\E", "shortCiteRegEx": "Stolle and Martin.", "year": 2004}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder P"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Learning from the memory of Atari 2600", "author": ["Sygnowski", "Jakub", "Michalewski", "Henryk"], "venue": "arXiv preprint arXiv:1605.01335,", "citeRegEx": "Sygnowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sygnowski et al\\.", "year": 2016}, {"title": "Finding structure in reinforcement learning", "author": ["Thrun", "Sebastian", "Schwartz", "Anton"], "venue": "In NIPS, pp", "citeRegEx": "Thrun et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1994}, {"title": "Imitation of hierarchical action structure by young children", "author": ["Whiten", "Andrew", "Flynn", "Emma", "Brown", "Katy", "Lee", "Tanya"], "venue": "Developmental science,", "citeRegEx": "Whiten et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Whiten et al\\.", "year": 2006}, {"title": "Prediction error associated with the perceptual segmentation of naturalistic events", "author": ["Zacks", "Jeffrey M", "Kurby", "Christopher A", "Eisenberg", "Michelle L", "Haroutunian", "Nayiri"], "venue": "Journal of Cognitive Neuroscience,", "citeRegEx": "Zacks et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zacks et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 46, "context": "One approach is to augment the agent\u2019s controls with useful higher-level behaviors called options (Sutton et al., 1999), each consisting of a control policy for one region of the state space, and a termination condition recognizing leaving that region.", "startOffset": 98, "endOffset": 119}, {"referenceID": 1, "context": "Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies (Bacon et al., 2016; Heess et al., 2016; Kulkarni et al., 2016), while others are inefficient for learning expressive representations (Bui et al.", "startOffset": 119, "endOffset": 182}, {"referenceID": 17, "context": "Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies (Bacon et al., 2016; Heess et al., 2016; Kulkarni et al., 2016), while others are inefficient for learning expressive representations (Bui et al.", "startOffset": 119, "endOffset": 182}, {"referenceID": 28, "context": "Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies (Bacon et al., 2016; Heess et al., 2016; Kulkarni et al., 2016), while others are inefficient for learning expressive representations (Bui et al.", "startOffset": 119, "endOffset": 182}, {"referenceID": 7, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 8, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 16, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 29, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 46, "context": "The field of hierarchical reinforcement learning has a long history (Barto & Mahadevan, 2003; Parr, 1998; Sutton et al., 1999), and has also been applied in robotics (Konidaris et al.", "startOffset": 68, "endOffset": 126}, {"referenceID": 25, "context": ", 1999), and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet et al., 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al.", "startOffset": 47, "endOffset": 117}, {"referenceID": 27, "context": ", 1999), and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet et al., 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al.", "startOffset": 47, "endOffset": 117}, {"referenceID": 40, "context": ", 1999), and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet et al., 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al.", "startOffset": 47, "endOffset": 117}, {"referenceID": 5, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 43, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 49, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 50, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 22, "context": "Early work in hierarchical control demonstrated the advantages of hierarchical structures by handcrafting hierarchical policies (Brooks, 1986) and by learning them given various manual specifications: state abstractions (Dayan & Hinton, 1992; Hengst, 2002; Kolter et al., 2007; Konidaris & Barto, 2007), a set of waypoints (Kaelbling, 1993), low-level skills (Bacon & Precup, 2015; Huber & Grupen, 1997; Liaw et al.", "startOffset": 220, "endOffset": 302}, {"referenceID": 31, "context": ", 2007; Konidaris & Barto, 2007), a set of waypoints (Kaelbling, 1993), low-level skills (Bacon & Precup, 2015; Huber & Grupen, 1997; Liaw et al., 2017), a set of finite-state meta-controllers (Parr & Russell, 1997), a set of subgoals (Dietterich, 2000; Sutton et al.", "startOffset": 89, "endOffset": 152}, {"referenceID": 46, "context": ", 2017), a set of finite-state meta-controllers (Parr & Russell, 1997), a set of subgoals (Dietterich, 2000; Sutton et al., 1999), or intrinsic reward (Kulkarni et al.", "startOffset": 90, "endOffset": 129}, {"referenceID": 28, "context": ", 1999), or intrinsic reward (Kulkarni et al., 2016).", "startOffset": 29, "endOffset": 52}, {"referenceID": 29, "context": "Since then, the focus of research has shifted towards discovery of the hierarchical structure itself, by: trading off value with description length (Thrun & Schwartz, 1994), identifying transitional states (Lakshminarayanan et al., 2016; McGovern & Barto, 2001; Menache et al., 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al.", "startOffset": 206, "endOffset": 319}, {"referenceID": 34, "context": "Since then, the focus of research has shifted towards discovery of the hierarchical structure itself, by: trading off value with description length (Thrun & Schwartz, 1994), identifying transitional states (Lakshminarayanan et al., 2016; McGovern & Barto, 2001; Menache et al., 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al.", "startOffset": 206, "endOffset": 319}, {"referenceID": 7, "context": ", 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al., 2002; Daniel et al., 2012; Krishnan et al., 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 8, "context": ", 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al., 2002; Daniel et al., 2012; Krishnan et al., 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 26, "context": ", 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al., 2002; Daniel et al., 2012; Krishnan et al., 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 25, "context": ", 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al., 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al.", "startOffset": 72, "endOffset": 121}, {"referenceID": 12, "context": ", 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al., 2017; Fox et al., 2016; Genewein et al., 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al.", "startOffset": 98, "endOffset": 185}, {"referenceID": 13, "context": ", 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al., 2017; Fox et al., 2016; Genewein et al., 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al.", "startOffset": 98, "endOffset": 185}, {"referenceID": 15, "context": ", 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al., 2017; Fox et al., 2016; Genewein et al., 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al.", "startOffset": 98, "endOffset": 185}, {"referenceID": 16, "context": ", 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al., 2015), or recently value-function approximation (Bacon et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": ", 2015), or recently value-function approximation (Bacon et al., 2016; Heess et al., 2016; Sharma et al., 2017).", "startOffset": 50, "endOffset": 111}, {"referenceID": 17, "context": ", 2015), or recently value-function approximation (Bacon et al., 2016; Heess et al., 2016; Sharma et al., 2017).", "startOffset": 50, "endOffset": 111}, {"referenceID": 41, "context": ", 2015), or recently value-function approximation (Bacon et al., 2016; Heess et al., 2016; Sharma et al., 2017).", "startOffset": 50, "endOffset": 111}, {"referenceID": 7, "context": "Discovery of multi-level hierarchies was shown to be possible via particle filters (Bui et al., 2002), however we are seeking more expressive representations.", "startOffset": 83, "endOffset": 101}, {"referenceID": 9, "context": "Our work is most related to (Daniel et al., 2016), who use a similar generative model, originally introduced by (Bui et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 7, "context": ", 2016), who use a similar generative model, originally introduced by (Bui et al., 2002) as an Abstract Hidden Markov Model, and learn its parameters via the Expectation-Maximization (EM) algorithm.", "startOffset": 70, "endOffset": 88}, {"referenceID": 17, "context": "Gradient-descent algorithms for value-function approximation with deep networks has been used to train hierarchical policies (Heess et al., 2016; Kulkarni et al., 2016), using a Universal Value Function Approximator (Schaul et al.", "startOffset": 125, "endOffset": 168}, {"referenceID": 28, "context": "Gradient-descent algorithms for value-function approximation with deep networks has been used to train hierarchical policies (Heess et al., 2016; Kulkarni et al., 2016), using a Universal Value Function Approximator (Schaul et al.", "startOffset": 125, "endOffset": 168}, {"referenceID": 39, "context": ", 2016), using a Universal Value Function Approximator (Schaul et al., 2015).", "startOffset": 55, "endOffset": 76}, {"referenceID": 35, "context": "When Q\u03b8 is represented by a Deep Q-Network (DQN) (Mnih et al., 2015), the update is a gradient step to reduce the mean square Bellman error over a batch of samples Lp\u03b8q \u201c  \u0301Erpyi  \u0301Q\u03b8psi, aiqqs \u03b8 \u00d0 \u03b8 ` \u03b1\u2207\u03b8Lp\u03b8q, with learning rate \u03b1, and yi based on the fixed current parameter \u03b8 \u0301, independent of \u03b8.", "startOffset": 49, "endOffset": 68}, {"referenceID": 46, "context": "The options framework is a hierarchical policy structure that can mitigate this sample complexity by breaking down the policy into simpler skills called options (Sutton et al., 1999).", "startOffset": 161, "endOffset": 182}, {"referenceID": 39, "context": "This generic notation allows us the flexibility of a completely separate network for each option, \u03b8 \u201c p\u03b8hqhPH, or the efficiency of sharing some of the parameters between options, similarly to a Universal Value Function Approximator (Schaul et al., 2015).", "startOffset": 233, "endOffset": 254}, {"referenceID": 38, "context": "which is the so-called Expectation-Gradient method (McLachlan & Krishnan, 2007; Salakhutdinov et al., 2003).", "startOffset": 51, "endOffset": 107}, {"referenceID": 13, "context": "The challenge is the coupling between the levels; namely, the value of a set of options is determined by its usefulness for meta-control (Fox et al., 2016), while the value of a meta-control policy depends on which options are available.", "startOffset": 137, "endOffset": 155}], "year": 2017, "abstractText": "Augmenting an agent\u2019s control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72% accuracy. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.", "creator": "LaTeX with hyperref package"}}}