{"id": "1703.08434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Linear classifier design under heteroscedasticity in Linear Discriminant Analysis", "abstract": "Under normality and homoscedasticity assumptions, Linear Discriminant Analysis (LDA) is known to be optimal in terms of minimising the Bayes error for binary classification. In the heteroscedastic case, LDA is not guaranteed to minimise this error. Assuming heteroscedasticity, we derive a linear classifier, the Gaussian Linear Discriminant (GLD), that directly minimises the Bayes error for binary classification. In addition, we also propose a local neighbourhood search (LNS) algorithm to obtain a more robust classifier if the data is known to have a non-normal distribution. We evaluate the proposed classifiers on two artificial and ten real-world datasets that cut across a wide range of application areas including handwriting recognition, medical diagnosis and remote sensing, and then compare our algorithm against existing LDA approaches and other linear classifiers. The GLD is shown to outperform the original LDA procedure in terms of the classification accuracy under heteroscedasticity. While it compares favourably with other existing heteroscedastic LDA approaches, the GLD requires as much as 60 times lower training time on some datasets. Our comparison with the support vector machine (SVM) also shows that, the GLD, together with the LNS, requires as much as 150 times lower training time to achieve an equivalent classification accuracy on some of the datasets. Thus, our algorithms can provide a cheap and reliable option for classification in a lot of expert systems.", "histories": [["v1", "Fri, 24 Mar 2017 14:45:12 GMT  (20kb)", "http://arxiv.org/abs/1703.08434v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kojo sarfo gyamfi", "james brusey", "rew hunt", "elena gaura"], "accepted": false, "id": "1703.08434"}, "pdf": {"name": "1703.08434.pdf", "metadata": {"source": "CRF", "title": "Linear classifier design under heteroscedasticity in Linear Discriminant Analysis", "authors": ["Kojo Sarfo Gyamfi", "James Brusey", "Andrew Hunt", "Elena Gaura"], "emails": ["gyamfik@uni.coventry.ac.uk", "j.brusey@coventry.ac.uk", "ab8187@coventry.ac.uk", "csx216@coventry.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.08 434v 1 [cs.L G] 24 MUnter Normality and homoscedasticity assumptions, Linear Discriminant Analysis (LDA) is known to be optimal in terms to minimize the Bayes error for binary classification. In the heteroscedastic case, LDA is not guaranteed to minimize this error. In addition, we also propose a local neighborhood search algorithm (LNS) to obtain a more robust classifier when the data is known to have an abnormal distribution. We evaluate the proposed classifiers on two artificial and ten real world datasets covering a wide range of applications, including handwriting recognition, medical diagnosis and remote sensing."}, {"heading": "1. Introduction", "text": "A typical example is the task of classifying a machine part among a number of health conditions (Ng & Jordan, 2002) Other applications that include classification include facial recognition, medical diagnosis, credit card fraud and error diagnosis. A common treatment for such classification problems is the conditional density function model of the trait vector (Ng & Jordan, 2002), in which case the most likely class to which a trait vector belongs can be selected as the class that maximizes the trait vector, known as the maximum posteriori decision. Let K be the number of classes, Ck be the kth class, x be a feature vector and Dk be training samples to the kth class (kth class, 2, K)."}, {"heading": "2. Related Work", "text": "Within the framework of heteroscedasticity assumption, many LDA approaches have been proposed, among which we mention (Fukunaga, 2013, Chapter 4), (Zhang & Liu, 2008; McLachlan, 2004; Duin & Loog, 2004; Decell Jr & Marani, 1976; Decell & Mayekar, 1977; Malina, 1981; Loog & Duin, 2002) Since it is known that Fisher's criterion (the maximization of which corresponds to the LDA derivative described in the Introduction section) only takes into account the difference in projected class means, existing heteroscedastic LDA approaches tend to generalize Fisher's criterion. In the work of (Loog & Duin, 2002), a directed distance matrix (DDM) is known as the Chernoff distance, which takes into account the difference in covariance matrices between the two classes as well as the projected class means."}, {"heading": "3. Gaussian Linear Discriminant", "text": "s choose w & # 246; Rd is a vector of weight differences, and w0 & # 34; R & # 34;, a threshold so that: K & # 246; physical normal distribution in classes C1 and C2 is assumed, y has an average of p & # 246; 1 and a deviation of p & # 34; 21 for class C1 and a mean of p & # 246; 2 and a deviation of p & # 246; 2 for class C2 and a deviation of p & # 246; 2 and a deviation of p & # 246; 2 and a deviation of p & # 246; 1). & # 34; W & # 34; W # 34; W # 34 of probabilities k & # 246; nn are expressed as follows: p & # 246; (w & # 246; 1). & # 34; W & # 34; W # 34; W # 34 of probabilities k & # 246; and W # 246; W # 246; (W & # 246; 246; W & # 246; 1)."}, {"heading": "3.1. Stopping Criteria", "text": "The GLD algorithm can be terminated under the following conditions: 1. If the modification of the objective function pe remains within a certain tolerance limit for a number of consecutive iterations; 2. If the modification of the standard w remains within a certain tolerance; 2. If the gradient of pe remains within a certain tolerance according to (21); 3. If the gradient of pe remains for a number of consecutive iterations; 4. After a fixed number of iterations I, if the convergence is slow. At the end of the algorithm, either the final solution to which the iterations converge or the solution corresponding to the minimum of pe found in the iterative updates can be selected."}, {"heading": "3.2. Multiclass Classification", "text": "If we now assume that there are K > 2 classes in record D, then the classification problem could be reduced to a number of binary classification problems, and the two main approaches normally used for this reduction are the Onevs-All (OvA) and One-vs-One (OvO) strategies (Bishop, 2006; Hsu & Lin, 2002)."}, {"heading": "3.2.1. One-vs-All (OvA)", "text": "In OvA, you train a classifier to distinguish between a class and all other classes. So, there are K different classifiers. An unknown vector x is then tested on all K classifiers, so that the class that corresponds to the classifier with the highest discriminatory value is selected. However, with respect to the proposed GLD algorithm, this is an inappropriate approach, because capturing all the other classes on one side of the discriminant will not necessarily have a normal distribution and could actually be multimodal if the means are well separated. Since our algorithm is based on strong normalcy assumptions of the data on each side of the discriminant, the GLD is expected to perform poorly as formulated."}, {"heading": "3.2.2. One-vs-One", "text": "In OvO, a classifier is trained to distinguish between each class pair in the data set, ignoring the other K \u2212 2 classes. Thus, there are K (K \u2212 1) / 2 unique classifiers that can be constructed. Again, an unknown vector x is tested for allK (K \u2212 1) / 2 classifiers. In such a case, the predicted classes for all classifiers are then randomly selected to select the most common class, which is a majority decision. However, in many cases, there is no clear winner, as more than one class can have the highest number of votes. In such a case, the most likely class is often randomly selected among the most common classes. GLD provides a more appropriate means to break such ties by using the minimized Bayes error for each classifier. Specifically, one can instead use a weighted voting system in which the number of each predicted class is weighted by 1 \u2212 weighted, because a problem is identified between the individual classifier."}, {"heading": "4. Non-normal Distributions", "text": "The basic assumption used to derive the GLD is that the data in each class has a normal distribution. Consequently, for an unknown non-normal distribution of linear classifiers that we have received, the Bayes error for this unknown distribution cannot be minimized. However, we argue that this unknown distribution is almost normal (Mudholkar & Hutson, 2000), then a more robust linear classifier can be found in any neighborhood of the GLD. Therefore, we use a local search algorithm to explore the region in Rd + 1 around the GLD to obtain the classifier that minimizes the number of misclassifiers in the training data sets. We do this by disrupting each of the d + 1 vector elements in the optimal w range."}, {"heading": "5. Experimental Validation", "text": "It is up to us that the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the European Commission, the European Commission, the EU Commission, the EU Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, of course, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the"}, {"heading": "6. Results and Discussion", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "7. Conclusion", "text": "In this paper, we have introduced the Gaussian Linear Discriminant (GLD), a novel and computationally efficient method for obtaining a linear discriminant for heteroscedastic linear Discriminant Analysis (LDA) for binary classification purposes. Our algorithm minimizes the Bayes error through an iterative optimization process that uses Fisher's linear Discriminant as the starting point. Furthermore, the GLD does not require parameter adjustments. We have also proposed a local search method to achieve a robust linear classifier for non-normal distributions. Our experimental results on two artificial and ten real applications show that the covariance matrices of the classes are unequal, LDA is unable to minimize the Bayes error. Thus, under heteroscedasticity, our proposed algorithm requires superior classification."}, {"heading": "Appendix A.", "text": "Theorem 1. If w + 0 and w \u2212 0 leave the two different solutions of (34), then w + 0 and w \u2212 0 cannot satisfy both (38), because \u03c31 6 = \u03c32. Proof. Let\u03b2 = \u221a (\u00b51 \u2212 \u00b52) 2 + 2 (\u03c321 \u2212 \u03c322) ln (\u03c31 \u03c32) (A.1) and letw + 0 = \u00b52\u03c32 1 \u2212 \u00b51\u03c322 + \u03c31\u03c32\u03b2\u03c321 \u2212 \u03c322 (A.2) Thenz2 \u03c32 = (\u00b52 \u2212 \u00b51) \u03c32 + \u03b2\u03c31 \u03c32 (\u03c321 \u2212 \u03c322), z1 \u03c31 = (\u00b52 \u2212 \u00b51) \u03c31 + \u03b22 \u03c31 \u2212 \u2212 \u03c321 \u2212 \u03c322) (A.3) Provided that w + 0 satisfies (38), then (\u00b52 \u2212 \u00b51) \u03c32 + \u03b22 \u03c32 \u03c32 \u2212 \u03c31 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 scan. In this case w + 0 is satisfactory (38), then \u00b52 \u2212 \u03bc1."}], "references": [{"title": "Classification into two multivariate normal distributions with different covariance matrices. The annals of mathematical statistics", "author": ["T.W. Anderson", "R. Bahadur"], "venue": null, "citeRegEx": "Anderson and Bahadur,? \\Q1962\\E", "shortCiteRegEx": "Anderson and Bahadur", "year": 1962}, {"title": "Bayesian reasoning and machine learning", "author": ["D. Barber"], "venue": "Cambridge University Press.", "citeRegEx": "Barber,? 2012", "shortCiteRegEx": "Barber", "year": 2012}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning , 128 , 1\u201358.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Toward Bayes-optimal linear dimension reduction", "author": ["L.J. Buturovic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 16 , 420\u2013424.", "citeRegEx": "Buturovic,? 1994", "shortCiteRegEx": "Buturovic", "year": 1994}, {"title": "Locality sensitive discriminant analysis", "author": ["D. Cai", "X. He", "K. Zhou", "J. Han", "H. Bao"], "venue": "In Proceedings of the 20th international joint conference on Artifical intelligence (pp", "citeRegEx": "Cai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2007}, {"title": "A new LDA-based face recognition system which can solve the small sample size problem", "author": ["Chen", "L.-F", "Liao", "H.-Y. M", "Ko", "M.-T", "Lin", "J.-C", "Yu", "G.-J"], "venue": "Pattern recognition,", "citeRegEx": "Chen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2000}, {"title": "The application of linear discriminant analysis in the diagnosis of thyroid diseases", "author": ["D. Coomans", "M. Jonckheer", "D.L. Massart", "I. Broeckaert", "P. Blockx"], "venue": "Analytica chimica acta,", "citeRegEx": "Coomans et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Coomans et al\\.", "year": 1978}, {"title": "Feature combinations and the divergence criterion", "author": ["H.P. Decell", "S.M. Mayekar"], "venue": "Computers & Mathematics with Applications ,", "citeRegEx": "Decell and Mayekar,? \\Q1977\\E", "shortCiteRegEx": "Decell and Mayekar", "year": 1977}, {"title": "Feature combinations and the Bhattacharyya criterion", "author": ["H.P. Decell Jr.", "S.K. Marani"], "venue": "Communications in Statistics-Theory and Methods ,", "citeRegEx": "Jr and Marani,? \\Q1976\\E", "shortCiteRegEx": "Jr and Marani", "year": 1976}, {"title": "Linear dimensionality reduction via a heteroscedastic extension of LDA: the Chernoff criterion", "author": ["R. Duin", "M. Loog"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Duin and Loog,? \\Q2004\\E", "shortCiteRegEx": "Duin and Loog", "year": 2004}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of eugenics , 7 , 179\u2013188. 23", "citeRegEx": "Fisher,? 1936", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Regularized discriminant analysis", "author": ["J.H. Friedman"], "venue": "Journal of the American statistical association, 84 , 165\u2013175.", "citeRegEx": "Friedman,? 1989", "shortCiteRegEx": "Friedman", "year": 1989}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": "Academic press.", "citeRegEx": "Fukunaga,? 2013", "shortCiteRegEx": "Fukunaga", "year": 2013}, {"title": "Nonparametric discriminant analysis", "author": ["K. Fukunaga", "J. Mantock"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Fukunaga and Mantock,? \\Q1983\\E", "shortCiteRegEx": "Fukunaga and Mantock", "year": 1983}, {"title": "Bayes optimality in linear discriminant analysis", "author": ["O.C. Hamsici", "A.M. Martinez"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Hamsici and Martinez,? \\Q2008\\E", "shortCiteRegEx": "Hamsici and Martinez", "year": 2008}, {"title": "Discriminative decorrelation for clustering and classification", "author": ["B. Hariharan", "J. Malik", "D. Ramanan"], "venue": "In European Conference on Computer Vision (pp", "citeRegEx": "Hariharan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2012}, {"title": "Discriminant analysis by Gaussian mixtures", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Hastie and Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1996}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["Hsu", "C.-W", "Lin", "C.-J"], "venue": "IEEE transactions on Neural Networks ,", "citeRegEx": "Hsu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2002}, {"title": "Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning", "author": ["A.J. Izenman"], "venue": "Springer Science & Business Media.", "citeRegEx": "Izenman,? 2009", "shortCiteRegEx": "Izenman", "year": 2009}, {"title": "Gaussian mixture discriminant analysis and sub-pixel land cover characterization in remote sensing", "author": ["J. Ju", "E.D. Kolaczyk", "S. Gopal"], "venue": "Remote Sensing of Environment ,", "citeRegEx": "Ju et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ju et al\\.", "year": 2003}, {"title": "Nonparametric discriminant analysis for face recognition", "author": ["Z. Li", "D. Lin", "X. Tang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Efficient pseudoinverse linear discriminant analysis and its nonlinear form for face recognition", "author": ["J. Liu", "S. Chen", "X. Tan", "D. Zhang"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Non-iterative heteroscedastic linear dimension reduction for two-class data. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR) (pp. 508\u2013517)", "author": ["M. Loog", "R.P. Duin"], "venue": null, "citeRegEx": "Loog and Duin,? \\Q2002\\E", "shortCiteRegEx": "Loog and Duin", "year": 2002}, {"title": "Regularized discriminant analysis for the small sample size problem in face recognition", "author": ["J. Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Pattern Recognition Letters ,", "citeRegEx": "Lu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2003}, {"title": "Detecting credit card fraud by modified Fisher discriminant analysis", "author": ["N. Mahmoudi", "E. Duman"], "venue": "Expert Systems with Applications ,", "citeRegEx": "Mahmoudi and Duman,? \\Q2015\\E", "shortCiteRegEx": "Mahmoudi and Duman", "year": 2015}, {"title": "On an extended Fisher criterion for feature selection", "author": ["W. Malina"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, (pp. 611\u2013614).", "citeRegEx": "Malina,? 1981", "shortCiteRegEx": "Malina", "year": 1981}, {"title": "Discriminant functions when covariance matrices are unequal", "author": ["S. Marks", "O.J. Dunn"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Marks and Dunn,? \\Q1974\\E", "shortCiteRegEx": "Marks and Dunn", "year": 1974}, {"title": "Discriminant analysis and statistical pattern recognition volume 544", "author": ["G. McLachlan"], "venue": "John Wiley & Sons.", "citeRegEx": "McLachlan,? 2004", "shortCiteRegEx": "McLachlan", "year": 2004}, {"title": "Fisher discriminant analysis with kernels", "author": ["S. Mika", "G. Ratsch", "J. Weston", "B. Scholkopf", "Mullers", "K.-R"], "venue": "In Neural Networks for Signal Processing IX,", "citeRegEx": "Mika et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mika et al\\.", "year": 1999}, {"title": "The epsilon\u2013skew\u2013normal distribution for analyzing near-normal data", "author": ["G.S. Mudholkar", "A.D. Hutson"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Mudholkar and Hutson,? \\Q2000\\E", "shortCiteRegEx": "Mudholkar and Hutson", "year": 2000}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "Advances in neural information processing systems ,", "citeRegEx": "Ng and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2002}, {"title": "Data set for fall events and daily activities from inertial sensors", "author": ["O. Ojetola", "E. Gaura", "J. Brusey"], "venue": "In Proceedings of the 6th ACM Multimedia Systems Conference (pp. 243\u2013248)", "citeRegEx": "Ojetola et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ojetola et al\\.", "year": 2015}, {"title": "Improved pseudoinverse linear discriminant analysis method for dimensionality reduction", "author": ["K.K. Paliwal", "A. Sharma"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Paliwal and Sharma,? \\Q2012\\E", "shortCiteRegEx": "Paliwal and Sharma", "year": 2012}, {"title": "A method of finding linear discriminant functions for a class of performance criteria", "author": ["D. Peterson", "R. Mattson"], "venue": "IEEE Transactions on Information Theory ,", "citeRegEx": "Peterson and Mattson,? \\Q1966\\E", "shortCiteRegEx": "Peterson and Mattson", "year": 1966}, {"title": "A cascade learning system for classification of diabetes disease: Generalized discriminant analysis and least square support vector machine. Expert systems with applications", "author": ["K. Polat", "S. G\u00fcne\u015f", "A. Arslan"], "venue": null, "citeRegEx": "Polat et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Polat et al\\.", "year": 2008}, {"title": "An expert system based on linear discriminant analysis and adaptive neuro-fuzzy inference system to diagnosis heart valve diseases", "author": ["A. Sengur"], "venue": "Expert Systems with Applications , 35 , 214\u2013222.", "citeRegEx": "Sengur,? 2008", "shortCiteRegEx": "Sengur", "year": 2008}, {"title": "Cancer classification by gradient LDA technique using microarray gene expression data", "author": ["A. Sharma", "K.K. Paliwal"], "venue": "Data & Knowledge Engineering ,", "citeRegEx": "Sharma and Paliwal,? \\Q2008\\E", "shortCiteRegEx": "Sharma and Paliwal", "year": 2008}, {"title": "Linear discriminant analysis for the small sample size problem: an overview", "author": ["A. Sharma", "K.K. Paliwal"], "venue": "International Journal of Machine Learning and Cybernetics ,", "citeRegEx": "Sharma and Paliwal,? \\Q2015\\E", "shortCiteRegEx": "Sharma and Paliwal", "year": 2015}, {"title": "A parameterized direct LDA and its application to face recognition", "author": ["F. Song", "D. Zhang", "J. Wang", "H. Liu", "Q. Tao"], "venue": "Neurocomputing ,", "citeRegEx": "Song et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Song et al\\.", "year": 2007}, {"title": "Do unbalanced data have a negative effect on LDA", "author": ["Xue", "J.-H", "D.M. Titterington"], "venue": "Pattern Recognition,", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "A direct LDA algorithm for high-dimensional datawith application to face recognition", "author": ["H. Yu", "J. Yang"], "venue": "Pattern recognition,", "citeRegEx": "Yu and Yang,? \\Q2001\\E", "shortCiteRegEx": "Yu and Yang", "year": 2001}, {"title": "Recent advances of large-scale linear classification", "author": ["Yuan", "G.-X", "Ho", "C.-H", "Lin", "C.-J"], "venue": "Proceedings of the IEEE ,", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "An equalized heteroscedastic linear discriminant analysis algorithm", "author": ["Zhang", "W.-Q", "J. Liu"], "venue": "IEEE Signal Processing Letters ,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Multiclass probabilistic kernel discriminant analysis", "author": ["Z. Zhao", "L. Sun", "S. Yu", "H. Liu", "J. Ye"], "venue": "In Proceedings of the 21st international jont conference on Artifical intelligence (pp", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 28, "context": "A linear classifier is robust against noise since it tends not to overfit (Mika et al., 1999).", "startOffset": 74, "endOffset": 93}, {"referenceID": 41, "context": "A linear classifier has relatively shorter training and testing times (Yuan et al., 2012).", "startOffset": 70, "endOffset": 89}, {"referenceID": 10, "context": "If only the weight vector w is required for dimensionality reduction, w may be obtained by maximising Fishers criterion (Fisher, 1936), given by:", "startOffset": 120, "endOffset": 134}, {"referenceID": 3, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al.", "startOffset": 154, "endOffset": 204}, {"referenceID": 35, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al.", "startOffset": 154, "endOffset": 204}, {"referenceID": 18, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al., 1999).", "startOffset": 291, "endOffset": 325}, {"referenceID": 28, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al., 1999).", "startOffset": 291, "endOffset": 325}, {"referenceID": 6, "context": "(Sharma & Paliwal, 2008; Coomans et al., 1978; Sengur, 2008; Polat et al., 2008), face and object recognition e.", "startOffset": 0, "endOffset": 80}, {"referenceID": 35, "context": "(Sharma & Paliwal, 2008; Coomans et al., 1978; Sengur, 2008; Polat et al., 2008), face and object recognition e.", "startOffset": 0, "endOffset": 80}, {"referenceID": 34, "context": "(Sharma & Paliwal, 2008; Coomans et al., 1978; Sengur, 2008; Polat et al., 2008), face and object recognition e.", "startOffset": 0, "endOffset": 80}, {"referenceID": 38, "context": "(Song et al., 2007; Chen et al., 2000; Liu et al., 2007; Yu & Yang, 2001) and credit card fraud prediction e.", "startOffset": 0, "endOffset": 73}, {"referenceID": 5, "context": "(Song et al., 2007; Chen et al., 2000; Liu et al., 2007; Yu & Yang, 2001) and credit card fraud prediction e.", "startOffset": 0, "endOffset": 73}, {"referenceID": 21, "context": "(Song et al., 2007; Chen et al., 2000; Liu et al., 2007; Yu & Yang, 2001) and credit card fraud prediction e.", "startOffset": 0, "endOffset": 73}, {"referenceID": 28, "context": "The widespread use of LDA in these areas is not because the datasets necessarily satisfy the normality and homoscedasticity assumptions, but mainly due to the robustness of LDA against noise, being a linear model (Mika et al., 1999).", "startOffset": 213, "endOffset": 232}, {"referenceID": 15, "context": "Since the linear Support Vector Machine (SVM) can be quite expensive to train, especially for large values of K or n (n = n1 + n2), LDA is often relied upon (Hariharan et al., 2012).", "startOffset": 157, "endOffset": 181}, {"referenceID": 21, "context": "In works by, for example, (Liu et al., 2007; Paliwal & Sharma, 2012), this problem is overcome by taking the Moore-Penrose pseudo-inverse of the scatter matrix, rather than the ordinary matrix inverse.", "startOffset": 26, "endOffset": 68}, {"referenceID": 11, "context": "Another approach to solving the SSS problem involves adding a scalar multiple of the identity matrix to the scatter matrix to make the resulting matrix nonsingular, a method known as regularised discriminant analysis (Friedman, 1989; Lu et al., 2003).", "startOffset": 217, "endOffset": 250}, {"referenceID": 23, "context": "Another approach to solving the SSS problem involves adding a scalar multiple of the identity matrix to the scatter matrix to make the resulting matrix nonsingular, a method known as regularised discriminant analysis (Friedman, 1989; Lu et al., 2003).", "startOffset": 217, "endOffset": 250}, {"referenceID": 27, "context": "One such modification, in the case of a non-normal distribution, is the mixture discriminant analysis (Hastie & Tibshirani, 1996; McLachlan, 2004; Ju et al., 2003) in which a non-normal distribution is modelled as a mixture of Gaussians.", "startOffset": 102, "endOffset": 163}, {"referenceID": 19, "context": "One such modification, in the case of a non-normal distribution, is the mixture discriminant analysis (Hastie & Tibshirani, 1996; McLachlan, 2004; Ju et al., 2003) in which a non-normal distribution is modelled as a mixture of Gaussians.", "startOffset": 102, "endOffset": 163}, {"referenceID": 4, "context": "Other non-parametric approaches to LDA that remove the normality assumption involve using local neighbourhood structures (Cai et al., 2007; Fukunaga & Mantock, 1983; Li et al., 2009) to construct a similarity matrix instead of the scatter matrix \u03a3x used in LDA.", "startOffset": 121, "endOffset": 182}, {"referenceID": 20, "context": "Other non-parametric approaches to LDA that remove the normality assumption involve using local neighbourhood structures (Cai et al., 2007; Fukunaga & Mantock, 1983; Li et al., 2009) to construct a similarity matrix instead of the scatter matrix \u03a3x used in LDA.", "startOffset": 121, "endOffset": 182}, {"referenceID": 28, "context": "Another modification, in the case of a non-linear decision boundary between D1 and D2, is the Kernel Fisher Discriminant (KFD) (Mika et al., 1999; Zhao et al., 2009; Polat et al., 2008).", "startOffset": 127, "endOffset": 185}, {"referenceID": 43, "context": "Another modification, in the case of a non-linear decision boundary between D1 and D2, is the Kernel Fisher Discriminant (KFD) (Mika et al., 1999; Zhao et al., 2009; Polat et al., 2008).", "startOffset": 127, "endOffset": 185}, {"referenceID": 34, "context": "Another modification, in the case of a non-linear decision boundary between D1 and D2, is the Kernel Fisher Discriminant (KFD) (Mika et al., 1999; Zhao et al., 2009; Polat et al., 2008).", "startOffset": 127, "endOffset": 185}, {"referenceID": 28, "context": "KFD maps the original feature space X into some other space Y (usually higher dimensional) via the kernel trick (Mika et al., 1999).", "startOffset": 112, "endOffset": 131}, {"referenceID": 15, "context": "In works by, for example, (Liu et al., 2007; Paliwal & Sharma, 2012), this problem is overcome by taking the Moore-Penrose pseudo-inverse of the scatter matrix, rather than the ordinary matrix inverse. Sharma & Paliwal (2008) use a gradient descent approach where one starts from an initial solution of w and moves in the negative direction of the gradient of Fisher\u2019s criterion (8).", "startOffset": 27, "endOffset": 226}, {"referenceID": 27, "context": "Under the heteroscedasticity assumption, many LDA approaches have been proposed among which we mention (Fukunaga, 2013, Chapter 4),(Zhang & Liu, 2008; McLachlan, 2004; Duin & Loog, 2004; Decell Jr & Marani, 1976; Decell & Mayekar, 1977; Malina, 1981; Loog & Duin, 2002).", "startOffset": 131, "endOffset": 269}, {"referenceID": 25, "context": "Under the heteroscedasticity assumption, many LDA approaches have been proposed among which we mention (Fukunaga, 2013, Chapter 4),(Zhang & Liu, 2008; McLachlan, 2004; Duin & Loog, 2004; Decell Jr & Marani, 1976; Decell & Mayekar, 1977; Malina, 1981; Loog & Duin, 2002).", "startOffset": 131, "endOffset": 269}, {"referenceID": 12, "context": "As far as we know, the closest work to ours in this regard are the works by (Marks & Dunn, 1974; Anderson & Bahadur, 1962; Peterson & Mattson, 1966; Fukunaga, 2013)", "startOffset": 76, "endOffset": 164}, {"referenceID": 12, "context": "Our proposed algorithm, which is described in the next section, unlike the trial and error approach by (Marks & Dunn, 1974; Anderson & Bahadur, 1962), has a principled optimisation procedure, and unlike (Fukunaga, 2013; Peterson & Mattson, 1966) does not encounter the problem of choosing an inappropriate \u2206s, nor restricts s to the interval [0, 1].", "startOffset": 203, "endOffset": 245}, {"referenceID": 12, "context": "This is where our approach most significantly differs from (Fukunaga, 2013).", "startOffset": 59, "endOffset": 75}, {"referenceID": 12, "context": "However, unlike (Anderson & Bahadur, 1962; Marks & Dunn, 1974), s is not chosen by systematic trial and error, and unlike (Fukunaga, 2013), s is not varied between 0 and 1 at small step increments.", "startOffset": 122, "endOffset": 138}, {"referenceID": 2, "context": "The two main approaches usually taken for this reduction are the Onevs-All (OvA) and One-vs-One (OvO) strategies (Bishop, 2006; Hsu & Lin, 2002).", "startOffset": 113, "endOffset": 144}, {"referenceID": 12, "context": "75, \u03a31 = I (43) The above Gaussian parameters are slightly modified from the two class data used by (Fukunaga, 2013) and (Xue & Titterington, 2008) in order to make the sample means less separated.", "startOffset": 100, "endOffset": 116}, {"referenceID": 12, "context": "As we are interested only in linear classification, we compare the performance of the GLD with the original LDA as well as the heteroscedastic LDA procedures by (Fukunaga, 2013),(Anderson & Bahadur, 1962) and (Marks & Dunn, 1974) as described in Section 2 in terms of the Bayes error (9).", "startOffset": 161, "endOffset": 177}, {"referenceID": 31, "context": "in machine fault diagnosis, or accelerometer-based human activity recognition (Ojetola et al., 2015), as it also requires far less training time than the existing heteroscedastic LDA approaches.", "startOffset": 78, "endOffset": 100}, {"referenceID": 31, "context": "in machine fault diagnosis, or accelerometer-based human activity recognition (Ojetola et al., 2015), as it also requires far less training time than the existing heteroscedastic LDA approaches. However, for datasets (c) through to (l), the classes do not have any known normal distribution. Therefore, minimising the Bayes error under the normality assumption would not necessarily result in a classifier that has the best classification accuracy, even if the difference in covariance matrices has been accounted for. For this reason, it is not surprising that LDA achieves a superior classification accuracy than C-HLD, R-HLD-1, R-HLD-2 and the GLD on datasets (c) and (h) as can be seen in Table 3. However, by searching around the neighbourhood of the GLD, the Local Neighbourhood Search (LNS) algorithm is able to account for the non-normality and obtain a more robust classifier. Thus, the GLD, together with the LNS procedure, achieves a higher classification accuracy than all the LDA approaches on all the realworld datasets (i.e. (c) to (l)) with the exception of dataset (f) which has the GLD showing superior classification accuracy. While the SVM outperforms the LDA approaches on half of the datasets, its training time can be rather long for large datasets. For instance, for dataset (d) which has 58000 elements, the SVM takes about 1.3 hours to train whereas the GLD with LNS, which achieves the best classification accuracy on this dataset, takes 43 seconds to train, representing over 100 fold savings in computational time over the SVM. Similar patterns can be seen in other datasets like (i), where the GLD with LNS achieves a superior classification accuracy with over 150 times shorter training time than the SVM. This suggests that for such large datasets, the GLD with Local Neighbourhood Search is a low-complexity alternative to the SVM, as it requires far less computational time than the SVM. We, however, make note of two weaknesses our proposed algorithms have. For the GLD, the procedure as described in Algorithm 1, may converge to a saddle point, instead of a local minimum. Even if it were to converge to a local minimum, there is no guarantee that is the global optimum solution due to the fact that the objective function pe is known to be non-convex Anderson & Bahadur (1962). Also, since the Local Neighbourhood Search involves evaluating the misclassification rate on the training set for every perturbation, the procedure does not scale well with large amounts of training", "startOffset": 79, "endOffset": 2314}], "year": 2017, "abstractText": "Under normality and homoscedasticity assumptions, Linear Discriminant Analysis (LDA) is known to be optimal in terms of minimising the Bayes error for binary classification. In the heteroscedastic case, LDA is not guaranteed to minimise this error. Assuming heteroscedasticity, we derive a linear classifier, the Gaussian Linear Discriminant (GLD), that directly minimises the Bayes error for binary classification. In addition, we also propose a local neighbourhood search (LNS) algorithm to obtain a more robust classifier if the data is known to have a non-normal distribution. We evaluate the proposed classifiers on two artificial and ten real-world datasets that cut across a wide range of application areas including handwriting recognition, medical diagnosis and remote sensing, and then compare our algorithm against existing LDA approaches and other linear classifiers. The GLD is shown to outperform the original LDA procedure in terms of the classification accuracy under heteroscedasticity. While it compares favourably with other existing heteroscedastic LDA approaches, the GLD requires as much as 60 times lower training time on some datasets. Our comparison with the support vector machine (SVM) also shows that, the GLD, together with the LNS, requires as much as 150 times lower training time to achieve an equivalent classification accuracy on some of the datasets. Thus, our algorithms can provide a cheap and reliable option for classification in a lot of expert systems.", "creator": "LaTeX with hyperref package"}}}