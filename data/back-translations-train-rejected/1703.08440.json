{"id": "1703.08440", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "K-Means Clustering using Tabu Search with Quantized Means", "abstract": "The Tabu Search (TS) metaheuristic has been proposed for K-Means clustering as an alternative to Lloyd's algorithm, which for all its ease of implementation and fast runtime, has the major drawback of being trapped at local optima. While the TS approach can yield superior performance, it involves a high computational complexity. Moreover, the difficulty in parameter selection in the existing TS approach does not make it any more attractive. This paper presents an alternative, low-complexity formulation of the TS optimization procedure for K-Means clustering. This approach does not require many parameter settings. We initially constrain the centers to points in the dataset. We then aim at evolving these centers using a unique neighborhood structure that makes use of gradient information of the objective function. This results in an efficient exploration of the search space, after which the means are refined. The proposed scheme is implemented in MATLAB and tested on four real-world datasets, and it achieves a significant improvement over the existing TS approach in terms of the intra cluster sum of squares and computational time.", "histories": [["v1", "Fri, 24 Mar 2017 14:59:06 GMT  (78kb,D)", "http://arxiv.org/abs/1703.08440v1", "World Conference on Engineering and Computer Science"]], "COMMENTS": "World Conference on Engineering and Computer Science", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kojo sarfo gyamfi", "james brusey", "andrew hunt"], "accepted": false, "id": "1703.08440"}, "pdf": {"name": "1703.08440.pdf", "metadata": {"source": "CRF", "title": "K-Means Clustering using Tabu Search with Quantized Means", "authors": ["Kojo Sarfo Gyamfi", "James Brusey", "Andrew Hunt"], "emails": ["gyamfik@uni.coventry.ac.uk).", "aa3172@coventry.ac.uk).", "ab8187@coventry.ac.uk)."], "sections": [{"heading": null, "text": "This is called a clustering problem, where the number of traits commonly referred to as D. Clustering is summarized in one sentence. More generally, d denotes the dimensionality of the sentence D. In addition, d denotes the collection of trait vectors of all organisms based on their traits, the set D, while the clusters referred to as Ck are represented by the various kingdoms. In machine learning, clustering falls within the domain of unattended learning, as it does not have class labels for the objects in D. Nevertheless, it can also be performed as an advance or to some superior learning techniques."}, {"heading": "II. PROBLEM DESCRIPTION", "text": "The question of how people in each country of the world should behave in each country of the world is not only a question of how they should behave in each country of the world, but also a question of how they should behave in each country of the world. Therefore, the question of how they should behave in each country of the world is only a question of how they should behave in each country of the world. Therefore, the question of how they should behave in each country of the world is only a question of how they should behave in each country of the world. Therefore, the question of how they should behave in each country of the world is only a problem that the search for the clusters to which an object most likely belongs can be described mathematically as a maximization of the following probabilities for the different models S: p (xn | S) = K = K = K = K = 1wkp (1), S) where S includes the cluster memberships in which the objects are summarized in the elns."}, {"heading": "III. TABU SEARCH", "text": "Taboo search is a metaheuristic technique for combinatorial optimization. It does not require the optimization problem to be convex; the algorithm uses neighborhood structures to explore the search space; it also uses a short-term memory structure called Taboo, which is essentially a list of forbidden movements or solutions; taboos prevent the back-and-forth between solutions that have already been considered in the search, a phenomenon known as cycling. In addition, TS allows movements to solutions that do not result in improvement of the objective function; it does so with the view that the bad solution may lead to a better one at a later stage in the search; therefore, it can escape local minima. TS remembers the best solution found at any point in the search and returns this solution when the algorithm is finished. In its most basic form, it follows the procedure outlined below: 1) Choose an initial solution M (0) This solution can be achieved randomly or by means of generating."}, {"heading": "IV. QUANTIZED MEANS TS CLUSTERING", "text": "In this section we discuss the proposed algorithm. As with any TS implementation, the Quantized Means TS clustering follows the skeleton of the description of the TS algorithm in Section III with the following changes and features."}, {"heading": "A. Search Space", "text": "In this formulation, a vector M (0) is defined as M (0) = [\u00b5T1,..., \u00b5 T K] T exclusively as the initial solution, where \u00b51,..., \u00b5K randomly selected observations from the data set D. M (0) is then Mc.In order to navigate the search space, neighbors of Mc must be found. Neighboring solutions are typically drawn from a finite set that includes the current solution itself. Alternatively, they can be obtained by a simple transformation of the current solution. It is worth noting that in the context of TS, neighboring solutions are not necessarily those closest to the current solution. To obtain neighbors of Mc, we modify their individual components, i.e. \u00b5k (k = 1,..., K) by replacing them with some new means or centers."}, {"heading": "B. Neighborhood Construction", "text": "Due to the large size of W, it is only necessary to select R (R) J (J) points from the W (J) series, which are necessary through a simple transformation of the solution Mc = 2, and consider them as neighbors of Mc in each individual iteration of the TS algorithm. However, the difficulty lies in choosing what R (J) neighbors will randomly select and thus will probably be slow to approximate the optimal solution. However, this seems to be a poor choice intuitively, since in many cases there will be no change in clustering and where there might be no change in the right direction. Analytical inclinbors: We therefore use the gradient information of the objective function to perform the neighbor selection. In each TS iteration, we select R points in W, which result in the steepest descent of the objective function. We consider the selection of a single neighbor, i.e R = 1 in this work."}, {"heading": "C. Tabu", "text": "The taboo structure used in this formulation is a list of all means considered in the search. As there are K means, the considered taboo is an array of K rows whose column length increases with the progression of the TS algorithm. If for some k, x-k from the minimization (13) is included in the taboo list, it is discarded and the next point x-Ck is chosen in increasing order of K-J. If for each k all the points in the kest cluster are in the taboo list, the last entry in the kest line of the taboo list is deleted so that at least one solution is valid."}, {"heading": "D. Termination Criterion", "text": "The termination criterion used in the proposed algorithm is twofold: firstly, the algorithm is terminated after reaching the maximum number of ITmax TS iterations; secondly, there is an early termination criterion, in which the algorithm is truncated after a predefined number of iterations (the so-called cut-out parameter rmax), within which there is no improvement of the best-found solution Mb. This is an indicator of the convergence of the algorithm. Early termination is based on the assumption that the global minimum may have already been reached. In order for this assumption to be largely valid, the neighboring solutions generated in an iteration must not be accidental; otherwise, there is a good chance that the global optimal solution would be found in any TS iteration. Therefore, the process of generating R random neighbors of Mc from W would not provide good results in terms of early termination."}, {"heading": "E. Refinement", "text": "The essence of the initial limitation of microk's affiliation to the finite set D is to make the optimization problem combinatorial and allow efficient exploration of the search space. If this is achieved at the end of the TS algorithm, the means can then be used without limitation. As a result, the components of the best-found solution Mb are recalculated as the centering of the clusters obtained at the end of the TS algorithm. Alternatively, Mb can be used as the initial solution to the K-Means algorithm to achieve a more refined solution.The proposed TS scheme (i.e. using analytical neighborhoods) is illustrated in the flow chart of Fig. 2."}, {"heading": "V. SIMULATIONS AND RESULTS", "text": "For our simulations we use four real datasets: the Bavaria Postal Code Dataset [16] (for two different values of K), the Fisher's Iris Dataset, the Glass Identification Dataset, and the normalized Cloud Dataset [17] (also for two different values of K). These datasets are selected to compare the performance of this scheme with the TSC-Means + [18], and the K-Means processor with the following parameters: ITmax = 400 and rmax = 0.25ITmax. We compare the performance of this scheme with the K-Means + + [18], and the K-Means algorithm with the objective parameters."}, {"heading": "VI. RELATED WORK", "text": "The TS algorithm has been applied to the K-Means cluster problem with a different formulation of Al-Sultan [7], where a candidate solution in the form of a series of lengths N is used. This array is called Ac and consists of the cluster indices of all N objects. However, in order to obtain a propensity to belong to a cluster, the cluster indices in Ac can be modified according to some criteria, which can lead to poor cluster membership."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we have presented an efficient taboo search method to solve the K-Means cluster problem, including limiting K-Means to objects in the dataset and optimizing these means through a number of neighbors determined by the object's gradient information. We have compared the proposed scheme to an existing TS algorithm and the K-Means and K-Means + + algorithms. We have shown that this approach works well with these well-known algorithms and does not require too many parameter settings, which is a promising result for many machine learning applications that use K-Means clustering. However, we note that the type of taboo structure used in our TS implementation may require a large amount of memory, especially for large datasets where the maximum number of TS iterations is correspondingly large. Therefore, ongoing work is focused on identifying a more compact representation of tabu entries in the algorithm structure and thus shortening the runtime."}], "references": [{"title": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers,", "author": ["B. Schlkopf"], "venue": "IEEE Transactions on Signal Processing vol. 45,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Testing for Uniformity in Multidimensional Data,", "author": ["S.P. Smith", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence vol. 6,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1984}, {"title": "Data Clustering: 50 Years Beyond K-Means,", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Refining Initial Points for K-Means Clustering,", "author": ["P.S. Bradley", "U.M. Fayyad"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning vol. 11,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Computational Experience on Four Algorithms for the Hard Clustering Problem,", "author": ["K.S. Al-Sultan", "M.M. Khan"], "venue": "Pattern Recognition Letters vol. 17,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Clustering Large Graphs via the Singular Value Decomposition,", "author": ["P. Drineas"], "venue": "Machine Learning vol. 56,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A Tabu Search Approach to the Clustering Problem,", "author": ["K.S. Al-Sultan"], "venue": "Pattern Recognition vol. 28,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Tabu Search - Part 1,", "author": ["F. Glover"], "venue": "ORSA Journal of Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "A Parallel Tabu Search Algorithm for Large Travelling Salesman Problems,", "author": ["C.-N. Fiechter"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Tabu Search Detection for MIMO Systems,", "author": ["H. Zhao", "H. Long", "W. Wang"], "venue": "IEEE 18th International Symposium on Personal, Indoor and Mobile Radio Communications", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Learning the K in K-Means,", "author": ["G. Hamerly", "C. Elkan"], "venue": "Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Evolution-Based Tabu Search Approach to Automatic Clustering,", "author": ["S.-M. Pan", "K.S. Cheng"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics vol. 37,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "A Self-Organizing Network for Hyperellipsoidal Clustering,", "author": ["J. Mao", "A.K. Jain"], "venue": "IEEE Transactions on Neural Networks vol. 7,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Clustering with Bregman Divergences,", "author": ["A. Banerjee"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "K-Means Clustering of Proportional Data using L1 Distance,", "author": ["H. Kashima"], "venue": "19th International Conference on Pattern Recognition ICPR", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "K-Means++: The Advantages of Careful Seeding,", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A Modified Tabu Search Approach for the Clustering Problem,", "author": ["A Kharrousheh", "S. Abdullah", "M. Nazri"], "venue": "Journal of Applied Sciences vol. 11,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Fedjki, \u201dA Tabu Search-Based Algorithm for the Fuzzy Clustering Problem,", "author": ["C.A.K.S. Al-Sultan"], "venue": "Pattern Recognition vol. 30,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "A Tabu-Search-Based Heuristic for Clustering,", "author": ["C.S. Sung", "H.W. Jin"], "venue": "Pattern Recognition vol. 33,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "example of this latter application is in the implementation of the radial basis function (RBF) with K centers [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "It is assumed that the objects in D lend themselves to some natural grouping [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Clustering is, however, an ill-posed problem [3] for the following reasons.", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "The value of K which minimizes some predefined criterion like the Akaike Information Criterion (AIC) or the Bayes Information Criterion [3] is then chosen.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Clustering algorithms may yield poor results if the K chosen is inappropriate [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "For this reason, several other methods have been applied to solving the clustering problem [4]-[7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "For this reason, several other methods have been applied to solving the clustering problem [4]-[7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "Notable among these is the approach of Al-Sultan [7] which is based on the Tabu Search", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "(TS) algorithm developed by Glover [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "[7] (our reference work) as the Tabu Search Clustering (TSC) algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "traveling salesman problem (TSP) [9] and signal detection in multiple input multiple output (MIMO) antenna systems [10].", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "traveling salesman problem (TSP) [9] and signal detection in multiple input multiple output (MIMO) antenna systems [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "[11] and Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] for a detailed treatment on how to choose K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "In general, none of these is known, and so the following set of simplifying assumptions [4] are", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "Indeed, [13]-[15] explore the use of other distance measures for clustering.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Indeed, [13]-[15] explore the use of other distance measures for clustering.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "parameter J known as the intra cluster sum of squares (ICSS) or the distortion [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "problem is computationally difficult, and is NP-hard [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "Since the K-Means algorithm is a special case of the Expectation-Maximization (EM) algorithm [4], it proceeds in two stages namely, the expectation and the maximization stages.", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Our approach is motivated by the above limitation, and is based on the TS algorithm in [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 15, "context": "We compare the performance of this scheme to the TSC, the K-Means++ [18], and the K-Means algorithms in terms of the objective function of (2) and the time taken for completion.", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "95 as suggested by Al-Sultan [5].", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "The TS algorithm has been applied to the K-Means clustering problem with a different formulation by Al-Sultan [7] where a candidate solution in the form of an array of length N is used.", "startOffset": 110, "endOffset": 113}, {"referenceID": 16, "context": "The TS clustering algorithm in [19] discusses essentially the same procedure as the TSC algorithm with two additional neighborhood structures presented.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The TS algorithm has also been applied to the Fuzzy C-Means clustering problem [20], where an object in the dataset D can belong to more than one cluster to varying degrees.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "Other TS approaches for clustering includes the packingreleasing algorithm [21] which is also based on [7], but with the following fundamental difference: a pair of objects in the dataset that are close to each other are packed together and treated as one object.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Other TS approaches for clustering includes the packingreleasing algorithm [21] which is also based on [7], but with the following fundamental difference: a pair of objects in the dataset that are close to each other are packed together and treated as one object.", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "clusters K is known beforehand, the evolution-based tabu search algorithm [12] uses TS for the determination of the number of clusters in the dataset, by considering K as another variable to be optimized in the TS procedure.", "startOffset": 74, "endOffset": 78}], "year": 2017, "abstractText": "The Tabu Search (TS) metaheuristic has been proposed for K-Means clustering as an alternative to Lloyd\u2019s algorithm, which for all its ease of implementation and fast runtime, has the major drawback of being trapped at local optima. While the TS approach can yield superior performance, it involves a high computational complexity. Moreover, the difficulty in parameter selection in the existing TS approach does not make it any more attractive. This paper presents an alternative, low-complexity formulation of the TS optimization procedure for K-Means clustering. This approach does not require many parameter settings. We initially constrain the centers to points in the dataset. We then aim at evolving these centers using a unique neighborhood structure that makes use of gradient information of the objective function. This results in an efficient exploration of the search space, after which the means are refined. The proposed scheme is implemented in MATLAB and tested on four real-world datasets, and it achieves a significant improvement over the existing TS approach in terms of the intra cluster sum of squares and computational time.", "creator": "LaTeX with hyperref package"}}}