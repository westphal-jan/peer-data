{"id": "1703.08471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Batch-normalized joint training for DNN-based distant speech recognition", "abstract": "Improving distant speech recognition is a crucial step towards flexible human-machine interfaces. Current technology, however, still exhibits a lack of robustness, especially when adverse acoustic conditions are met. Despite the significant progress made in the last years on both speech enhancement and speech recognition, one potential limitation of state-of-the-art technology lies in composing modules that are not well matched because they are not trained jointly. To address this concern, a promising approach consists in concatenating a speech enhancement and a speech recognition deep neural network and to jointly update their parameters as if they were within a single bigger network. Unfortunately, joint training can be difficult because the output distribution of the speech enhancement system may change substantially during the optimization procedure. The speech recognition module would have to deal with an input distribution that is non-stationary and unnormalized. To mitigate this issue, we propose a joint training approach based on a fully batch-normalized architecture. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework significantly overtakes other competitive solutions, especially in challenging environments.", "histories": [["v1", "Fri, 24 Mar 2017 15:40:19 GMT  (187kb,D)", "http://arxiv.org/abs/1703.08471v1", "arXiv admin note: text overlap witharXiv:1703.08002"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1703.08002", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["mirco ravanelli", "philemon brakel", "maurizio omologo", "yoshua bengio"], "accepted": false, "id": "1703.08471"}, "pdf": {"name": "1703.08471.pdf", "metadata": {"source": "CRF", "title": "BATCH-NORMALIZED JOINT TRAINING FOR DNN-BASED DISTANT SPEECH RECOGNITION", "authors": ["Mirco Ravanelli", "Philemon Brakel", "Maurizio Omologo", "Yoshua Bengio", "Bruno Kessler"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2.1. Batch normalization", "text": "The formation of DNNs is complicated by the fact that the distribution of input factors at each level changes during training, as the parameters of the previous layers change. This problem, known as internal covariate shift, slows down the formation of deep neural networks. Batch normalization [36], recently proposed in the machine learning community, addresses this problem by normalizing the mean and variance of each layer for each layer of training and spreading backwards through the normalization step. However, it has long been known that network education converges faster when its input factors are properly normalized [39], and in this way, batch normalization expands normalization to all layers of the architecture. However, since normalization per layer can affect model capacity, a traable scaling parameter and a traable shift of the parameter \u03b2 should be introduced in each layer to restore the representation power of the network."}, {"heading": "2.2. System details", "text": "The features considered in this paper are 39 MelCepstral coefficients (MFCCs), which are calculated every 10 ms with a frame length of 25 ms. Language enhancement DNN is fed with a context of 21 consecutive images and predicts (every 10 ms) 11 consecutive images of improved MFCC features. The idea of predicting multiple improved images has also been explored in [31]. All layers used are Rectified Linear Units (ReLU), except for the output of speech enhancement (linear) and the output of speech recognition (softmax). Batch normalization [36] is used for all hidden layers, while the dropout [41] is adopted in all parts of the architecture, except the output layers. The data sets used for joint training are characterized by contamination of clean corpora (i.e., TIMIT and WSJ) data sets with noise and reverberation. The MFCs are referred to as the language enhancement labels for the DNN (2)."}, {"heading": "2.3. Relation to prior work", "text": "A major difference from previous work is that we propose to combine joint training with batch normalization. In [30, 31], for example, the joint training was actually carried out as fine-tuning, which was done independently of each other only after the training of the two networks. A critical aspect of such an approach is that the learning rate adopted in the fine-tuning step must be properly selected in order to really take advantage of the pre-training. With batch normalization, we are able not only to significantly improve the performance of the system, but also to perform a joint training from the ground up, using each pre-training phase.Another interesting aspect of this work is a deeper study of the role played by the gradient weight factor. 3. CORPORA AND TASKSTo ensure an accurate evaluation of the proposed technique, experimental validation using different training data was carried out."}, {"heading": "4.1. Close-talking baselines", "text": "The phoneme error rate (PER%) achieved by decoding the original TIMIT test sets is 19.5% (using DNN models trained with the original data set).To ensure the reproducibility of the results reported in this thesis, the code of our common training system is available at https: / / github.com / mravanelli. In the same repository, all the scripts required for data contamination are available.The public dissemination of the DIRHA English data set is currently under discussion with the Linguistic Data Consortium (LDC). (WHO%) achieved by decoding the narrowly speaking WSJ sets is 3.3%. It is worth noting that the DNN model results in very accurate typesetting transcription under such favorable acoustic conditions, especially in connection with a language model."}, {"heading": "4.2. Joint training performance", "text": "In Table 1, the proposed common training approach is compared with other competitive strategies, in particular, the first line reports on the results achieved with a single neural network; the second line shows the performance achieved when the neural network for speech enhancement (4 hidden layers of 1024 neurons for TIMIT, 6 hidden layers of 2048 neurons for WSJ) is independently trained and later coupled with the nearby speaking DNN of 4.1. These results are particularly critical because the speech enhancement model exhibits significant distortions, especially under adverse acoustic conditions, that a normally trained DNN cannot handle. To partially restore such a critical imbalance, one approach is to train the language enhancement first, then pass all the training characteristics, although language enhancement DNN and speech enhancement, which are trained in the usual way, are best."}, {"heading": "4.3. Role of batch normalization", "text": "In Table 2, the impact of batch normalization on the common training framework is shown; the first two columns report on the results achieved with and without batch normalization when no pre-training techniques are used; in the last two columns, the effects of the pre-training strategy are analyzed; the pre-training strategy considered here consists of initializing the two DNNs with the appropriate training system discussed in Section 4.2 and conducting a fine-tuning phase with a reduced learning rate; the column corresponding to the pre-training without batch normalization represents a system that comes closest to the approaches pursued in [30, 31]; Table 2 clearly shows that batch normalization is particularly helpful; for example, a relative improvement of about 23% is achieved when the batch normalization for the WSJ task is adopted in a loud and reverberating scenario; the central importance of batch normalization is also emphasized in Figure 2, where the data normalization during the batch normalization (if the normalization architecture for the batch) is better represented (if the normalization architecture for the normalization)."}, {"heading": "4.4. Role of the gradient weighting", "text": "In fact, the situation is that most people are able to survive on their own and that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}], "references": [{"title": "Automatic Speech Recognition - A Deep Learning", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Interpretation of Multiparty Meetings the AMI and Amida Projects", "author": ["S. Renals", "T. Hain", "H. Bourlard"], "venue": "Proc. of HSCMA, 2008, pp. 115\u2013118.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "A prototype of distant-talking interface for control of interactive TV", "author": ["M. Omologo"], "venue": "Proc. of ASILOMAR 2010, pp. 1711\u20131715.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "The PASCAL CHiME speech separation and recognition challenge", "author": ["J. Barker", "E. Vincent", "N. Ma", "H. Christensen", "P. Green"], "venue": "Computer Speech and Language, vol. 27, no. 3, pp. 621\u2013633, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "The third CHiME Speech Separation and Recognition Challenge: Dataset, task and baselines", "author": ["J. Barker", "R. Marxer", "E. Vincent", "S. Watanabe"], "venue": "Proc. of ASRU, 2015, pp. 504\u2013511.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "The reverb challenge: A Common Evaluation Framework for Dereverberation and Recognition of Reverberant Speech", "author": ["K. Kinoshita", "M. Delcroix", "T. Yoshioka", "T. Nakatani", "E. Habets", "R. Haeb-Umbach", "V. Leutnant", "A. Sehr", "W. Kellermann", "R. Maas", "S. Gannot", "B. Raj"], "venue": "Proc. of WASPAA 2013, pp. 1\u20134.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "The NTT CHiME-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices", "author": ["T. Yoshioka", "N. Ito", "M. Delcroix", "A. Ogawa", "K. Kinoshita", "M. Fujimoto", "C. Yu", "W.J. Fabian", "M. Espi", "T. Higuchi", "S. Araki", "T. Nakatani"], "venue": "Proc. ASRU 2015, pp. 436\u2013443.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic event detection and classification", "author": ["A. Temko", "C. Nadeu", "D. Macho", "R. Malkin", "C. Zieger", "M. Omologo"], "venue": "Computers in the Human Interaction Loop, pp. 61\u201373. Springer London, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio Concept Classification with Hierarchical Deep Neural Networks", "author": ["M. Ravanelli", "B. Elizalde", "K. Ni", "G. Friedland"], "venue": "Proc. of EUSIPCO, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic event localization using a crosspower-spectrum phase based technique", "author": ["M. Omologo", "P. Svaizer"], "venue": "Proc. of ICASSP, 1994, pp. 273\u2013276.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "A speech event detection/localization task for multiroom environments", "author": ["A. Brutti", "M. Ravanelli", "P. Svaizer", "M. Omologo"], "venue": "Proc. of HSCMA 2014, pp. 157\u2013161.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Likelihoodmaximizing beamforming for robust hands-free speech recognition", "author": ["M.L. Seltzer", "Raj B.", "R.M. Stern"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 12, no. 3, pp. 489\u2013498, September 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Joint discriminative front end and back end training for improved speech recognition accuracy", "author": ["J. Droppo", "A. Acero"], "venue": "Proc. of ICASSP, 2006, pp. 281\u2013284.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Proc. of ASRU 2013, pp. 285\u2013290.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "Proc. of ICASSP 2014, pp. 5542\u20135546.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "The MERL/MELCO/TUM System for the REVERB Challenge Using Deep Recurrent Neural Network Feature Enhancement", "author": ["F. Weninger", "S. Watanabe", "J. Le Roux", "J.R. Hershey", "Y. Tachioka", "J. Geiger", "B. Schuller", "G. Rigoll"], "venue": "IEEE REVERB Workshop, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Reverberant speech recognition combining deep neural networks and deep autoencoders", "author": ["S. Sakai M. Mimura", "T. Kawahara"], "venue": "IEEE REVERB Workshop, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy and Reverberant Environments", "author": ["A. Schwarz", "C. Huemmer", "R. Maas", "W. Kellermann"], "venue": "Proc. of ICASSP 2015, pp. 4380\u20134384.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "On the selection of the impulse responses for distant-speech recognition based on contaminated speech training", "author": ["M. Ravanelli", "M. Omologo"], "venue": "Proc. of Interspeech 2014, pp. 1028\u20131032.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Contaminated speech training methods for robust DNN-HMM distant speech recognition", "author": ["M. Ravanelli", "M. Omologo"], "venue": "Proc. of Interspeech 2015, pp. 756\u2013760.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L. Dai", "C. Lee"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u2013 68, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.R. Dai", "C.H. Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, Jan 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR", "author": ["F. Weninger", "H. Erdogan", "S. Watanabe", "E. Vincent", "J. Le Roux", "J.R. Hershey", "B.W. Schuller"], "venue": "Proc. of LVA/ICA, 2015, pp. 91\u201399.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint noise adaptive training for robust automatic speech recognition", "author": ["A. Narayanan", "D. Wang"], "venue": "Proc. of ICASSP, 2014, pp. 4380\u20134384.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A joint training framework for robust automatic speech recognition", "author": ["Z.Q. Wang", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing, vol. 24, no. 4, pp. 796\u2013806, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint training of front-end and back-end deep neural networks for robust speech recognition", "author": ["T. Gao", "J. Du", "L.R. Dai", "C.H. Lee"], "venue": "Proc. of ICASSP, 2015, pp. 4375\u20134379.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks", "author": ["Z. Chen", "S. Watanabe", "H. Erdogan", "J. Hershey"], "venue": "Proc. of Interspeech, 2015, pp. 3274\u2013 3278.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker localization and microphone spacing invariant acoustic modeling from raw multichannel waveforms", "author": ["T.N. Sainath", "R.J. Weiss", "K.W. Wilson", "A. Narayanan", "M. Bacchiani", "A. Senior"], "venue": "Proc. of ASRU, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint optimization of denoising autoencoder and dnn acoustic model based on multi-target learning for noisy speech recognition", "author": ["M. Mimura", "S. Sakai", "T. Kawahara"], "venue": "Proc. of Interspeech, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimization of speech enhancement front-end with speech recognition-level criterion", "author": ["T. Higuchi", "T. Yoshioka", "T. Nakatani"], "venue": "Proc. of Interspeech, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proc. of ICML, 2015, pp. 448\u2013456.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["\u00c7. G\u00fcl\u00e7ehre", "Y. Bengio"], "venue": "Journal of Machine Learning Research, vol. 17, no. 8, pp. 1\u201332, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "Samira Ebrahimi K.", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "Proc. of ICLR, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. M\u00fcller"], "venue": "Neural networks: Tricks of the trade, pp. 9\u201350. Springer Berlin Heidelberg, 1998.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1998}, {"title": "Recurrent batch normalization", "author": ["T. Cooijmans", "N. Ballas", "C. Laurent", "\u00c7. G\u00fcl\u00e7ehre", "A. Courville"], "venue": "arXiv preprint arXiv:1603.09025, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1929}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Proc. of ASRU, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. of AISTATS, 2010, pp. 249\u2013256.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Impulse response estimation for robust speech recognition in a reverberant environment", "author": ["M. Ravanelli", "A. Sosi", "P. Svaizer", "M. Omologo"], "venue": "Proc. of EUSIPCO 2012, pp. 1668\u20131672.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "The DIRHA simulated corpus", "author": ["L. Cristoforetti", "M. Ravanelli", "M. Omologo", "A. Sosi", "A. Abad", "M. Hagmueller", "P. Maragos"], "venue": "Proc. of LREC 2014, pp. 2629\u2013 2634.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Realistic multi-microphone data simulation for distant speech recognition", "author": ["M. Ravanelli", "P. Svaizer", "M. Omologo"], "venue": "Proc. of Interspeech, 2016.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "The DIRHA-ENGLISH corpus and related tasks for distant-speech recognition in domestic environments", "author": ["M. Ravanelli", "L. Cristoforetti", "R. Gretter", "M. Pellin", "A. Sosi", "M. Omologo"], "venue": "Proc. of ASRU 2015, pp. 275\u2013282.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Automatic Speech Recognition (ASR) [1], thanks to the substantial performance improvement achieved with modern deep learning technologies [2], has recently been applied in several fields, and it is currently used by millions of users worldwide.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Automatic Speech Recognition (ASR) [1], thanks to the substantial performance improvement achieved with modern deep learning technologies [2], has recently been applied in several fields, and it is currently used by millions of users worldwide.", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Valuable examples include the AMI/AMIDA projects [3], who were focused on automatic meeting transcription, DICIT [4] which investigated voice-enabled TVs and, more recently, DIRHA which addressed speech-based domestic control.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Valuable examples include the AMI/AMIDA projects [3], who were focused on automatic meeting transcription, DICIT [4] which investigated voice-enabled TVs and, more recently, DIRHA which addressed speech-based domestic control.", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "The progress in the field was also fostered by the considerable success of some international challenges such as CHiME [5, 6] and REVERB [7].", "startOffset": 119, "endOffset": 125}, {"referenceID": 5, "context": "The progress in the field was also fostered by the considerable success of some international challenges such as CHiME [5, 6] and REVERB [7].", "startOffset": 119, "endOffset": 125}, {"referenceID": 6, "context": "The progress in the field was also fostered by the considerable success of some international challenges such as CHiME [5, 6] and REVERB [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "To counteract such adversities, even the most recent DSR systems [9] must rely on a combination of several interconnected technologies, including for instance speech enhancement [10], speech separation [11], acoustic event detection and classification [12, 13], speaker identification [14], speaker localization [15, 16], just to name a few.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "To counteract such adversities, even the most recent DSR systems [9] must rely on a combination of several interconnected technologies, including for instance speech enhancement [10], speech separation [11], acoustic event detection and classification [12, 13], speaker identification [14], speaker localization [15, 16], just to name a few.", "startOffset": 252, "endOffset": 260}, {"referenceID": 9, "context": "To counteract such adversities, even the most recent DSR systems [9] must rely on a combination of several interconnected technologies, including for instance speech enhancement [10], speech separation [11], acoustic event detection and classification [12, 13], speaker identification [14], speaker localization [15, 16], just to name a few.", "startOffset": 252, "endOffset": 260}, {"referenceID": 10, "context": "To counteract such adversities, even the most recent DSR systems [9] must rely on a combination of several interconnected technologies, including for instance speech enhancement [10], speech separation [11], acoustic event detection and classification [12, 13], speaker identification [14], speaker localization [15, 16], just to name a few.", "startOffset": 312, "endOffset": 320}, {"referenceID": 11, "context": "To counteract such adversities, even the most recent DSR systems [9] must rely on a combination of several interconnected technologies, including for instance speech enhancement [10], speech separation [11], acoustic event detection and classification [12, 13], speaker identification [14], speaker localization [15, 16], just to name a few.", "startOffset": 312, "endOffset": 320}, {"referenceID": 12, "context": "An early attempt to mitigate this issue was published in [17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Another approach was proposed in [18], where a front-end for feature extraction and a GMM-HMM back-end were jointly trained using maximum mutual information.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [19, 20, 21, 22, 23, 24, 25], but has also enabled the dear X iv :1 70 3.", "startOffset": 161, "endOffset": 189}, {"referenceID": 15, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [19, 20, 21, 22, 23, 24, 25], but has also enabled the dear X iv :1 70 3.", "startOffset": 161, "endOffset": 189}, {"referenceID": 16, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [19, 20, 21, 22, 23, 24, 25], but has also enabled the dear X iv :1 70 3.", "startOffset": 161, "endOffset": 189}, {"referenceID": 17, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [19, 20, 21, 22, 23, 24, 25], but has also enabled the dear X iv :1 70 3.", "startOffset": 161, "endOffset": 189}, {"referenceID": 18, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [19, 20, 21, 22, 23, 24, 25], but has also enabled the dear X iv :1 70 3.", "startOffset": 161, "endOffset": 189}, {"referenceID": 19, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [19, 20, 21, 22, 23, 24, 25], but has also enabled the dear X iv :1 70 3.", "startOffset": 161, "endOffset": 189}, {"referenceID": 20, "context": "Nevertheless, the recent success of deep learning has not only largely contributed to the substantial improvement of the speech recognition part of a DSR system [19, 20, 21, 22, 23, 24, 25], but has also enabled the dear X iv :1 70 3.", "startOffset": 161, "endOffset": 189}, {"referenceID": 21, "context": "velopment of competitive DNN-based speech enhancement solutions [26, 27, 28].", "startOffset": 64, "endOffset": 76}, {"referenceID": 22, "context": "velopment of competitive DNN-based speech enhancement solutions [26, 27, 28].", "startOffset": 64, "endOffset": 76}, {"referenceID": 23, "context": "velopment of competitive DNN-based speech enhancement solutions [26, 27, 28].", "startOffset": 64, "endOffset": 76}, {"referenceID": 24, "context": "Although joint training for speech recognition is still an under-explored research direction, such a paradigm is progressively gaining more attention and some interesting works in the field have been recently published [29, 30, 31, 32, 33, 34, 35].", "startOffset": 219, "endOffset": 247}, {"referenceID": 25, "context": "Although joint training for speech recognition is still an under-explored research direction, such a paradigm is progressively gaining more attention and some interesting works in the field have been recently published [29, 30, 31, 32, 33, 34, 35].", "startOffset": 219, "endOffset": 247}, {"referenceID": 26, "context": "Although joint training for speech recognition is still an under-explored research direction, such a paradigm is progressively gaining more attention and some interesting works in the field have been recently published [29, 30, 31, 32, 33, 34, 35].", "startOffset": 219, "endOffset": 247}, {"referenceID": 27, "context": "Although joint training for speech recognition is still an under-explored research direction, such a paradigm is progressively gaining more attention and some interesting works in the field have been recently published [29, 30, 31, 32, 33, 34, 35].", "startOffset": 219, "endOffset": 247}, {"referenceID": 28, "context": "Although joint training for speech recognition is still an under-explored research direction, such a paradigm is progressively gaining more attention and some interesting works in the field have been recently published [29, 30, 31, 32, 33, 34, 35].", "startOffset": 219, "endOffset": 247}, {"referenceID": 29, "context": "Although joint training for speech recognition is still an under-explored research direction, such a paradigm is progressively gaining more attention and some interesting works in the field have been recently published [29, 30, 31, 32, 33, 34, 35].", "startOffset": 219, "endOffset": 247}, {"referenceID": 30, "context": "Although joint training for speech recognition is still an under-explored research direction, such a paradigm is progressively gaining more attention and some interesting works in the field have been recently published [29, 30, 31, 32, 33, 34, 35].", "startOffset": 219, "endOffset": 247}, {"referenceID": 31, "context": "Batch normalization [36], which has recently been proposed in the machine learning community, has been shown crucial to significantly improve both the convergence and the performance of the proposed joint training algorithm.", "startOffset": 20, "endOffset": 24}, {"referenceID": 25, "context": "Differently to previous works [30, 31], thanks to batch normalization, we are able to effectively train the joint architecture even without any pre-training steps.", "startOffset": 30, "endOffset": 38}, {"referenceID": 26, "context": "Differently to previous works [30, 31], thanks to batch normalization, we are able to effectively train the joint architecture even without any pre-training steps.", "startOffset": 30, "endOffset": 38}, {"referenceID": 32, "context": "On the other hand, it is well known that training deep architectures is easier when some hints are given about the targeted function [37].", "startOffset": 133, "endOffset": 137}, {"referenceID": 32, "context": "As shown previously [37], such prior knowledge becomes progressively more precious as the complexity of the problem increases and can thus be very helpful for a distant speech recognition task.", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "Similarly to the current work, in [37, 38] a task-specific prior knowledge has been injected into an intermediate layer of a DNN for better addressing an image classification problem.", "startOffset": 34, "endOffset": 42}, {"referenceID": 33, "context": "Similarly to the current work, in [37, 38] a task-specific prior knowledge has been injected into an intermediate layer of a DNN for better addressing an image classification problem.", "startOffset": 34, "endOffset": 42}, {"referenceID": 31, "context": "Batch normalization [36], which has been recently proposed in the machine learning community, addresses this issue by normalizing the mean and the variance of each layer for each training mini-batch, and back-propagating through the normalization step.", "startOffset": 20, "endOffset": 24}, {"referenceID": 34, "context": "It has been long known that the network training converges faster if its inputs are properly normalized [39] and, in such a way, batch normalization extends the normalization to all the layers of the architecture.", "startOffset": 104, "endOffset": 108}, {"referenceID": 31, "context": "Contrary to [36], where it was initialized to unit variance (\u03b3 = 1), in this work we have observed better performance and convergence properties with a smaller variance initialization (\u03b3 = 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 35, "context": "A similar outcome has been found in [40], where fewer vanishing gradient problems are empirically observed with small values of \u03b3 in the case of recurrent neural networks.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "The idea of predicting multiple enhanced frames was also explored in [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 31, "context": "Batch normalization [36] is employed for all the hidden layers, while dropout [41] is adopted in all part of the architecture, except for the output layers.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "Batch normalization [36] is employed for all the hidden layers, while dropout [41] is adopted in all part of the architecture, except for the output layers.", "startOffset": 78, "endOffset": 82}, {"referenceID": 37, "context": "See the standard s5 recipe of Kaldi for more details [42].", "startOffset": 53, "endOffset": 57}, {"referenceID": 38, "context": "The weights of the network are initialized according to the Glorot initialization [43], while biases are initialized to zero.", "startOffset": 82, "endOffset": 86}, {"referenceID": 39, "context": "The proposed system, which has been implemented with Theano [44], has been coupled with the Kaldi toolkit [42] to form a context-dependent DNN-HMM speech recognizer.", "startOffset": 60, "endOffset": 64}, {"referenceID": 37, "context": "The proposed system, which has been implemented with Theano [44], has been coupled with the Kaldi toolkit [42] to form a context-dependent DNN-HMM speech recognizer.", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "Similarly to this paper, a joint training framework has been explored in [29, 30, 31, 32, 33, 34, 35].", "startOffset": 73, "endOffset": 101}, {"referenceID": 25, "context": "Similarly to this paper, a joint training framework has been explored in [29, 30, 31, 32, 33, 34, 35].", "startOffset": 73, "endOffset": 101}, {"referenceID": 26, "context": "Similarly to this paper, a joint training framework has been explored in [29, 30, 31, 32, 33, 34, 35].", "startOffset": 73, "endOffset": 101}, {"referenceID": 27, "context": "Similarly to this paper, a joint training framework has been explored in [29, 30, 31, 32, 33, 34, 35].", "startOffset": 73, "endOffset": 101}, {"referenceID": 28, "context": "Similarly to this paper, a joint training framework has been explored in [29, 30, 31, 32, 33, 34, 35].", "startOffset": 73, "endOffset": 101}, {"referenceID": 29, "context": "Similarly to this paper, a joint training framework has been explored in [29, 30, 31, 32, 33, 34, 35].", "startOffset": 73, "endOffset": 101}, {"referenceID": 30, "context": "Similarly to this paper, a joint training framework has been explored in [29, 30, 31, 32, 33, 34, 35].", "startOffset": 73, "endOffset": 101}, {"referenceID": 25, "context": "In [30, 31], for instance, the joint training was actually performed as a fine-tuning procedure, which was carried out only after training the two networks independently.", "startOffset": 3, "endOffset": 11}, {"referenceID": 26, "context": "In [30, 31], for instance, the joint training was actually performed as a fine-tuning procedure, which was carried out only after training the two networks independently.", "startOffset": 3, "endOffset": 11}, {"referenceID": 40, "context": "More details about the data contamination approach can be found in [45, 46, 47].", "startOffset": 67, "endOffset": 79}, {"referenceID": 41, "context": "More details about the data contamination approach can be found in [45, 46, 47].", "startOffset": 67, "endOffset": 79}, {"referenceID": 42, "context": "More details about the data contamination approach can be found in [45, 46, 47].", "startOffset": 67, "endOffset": 79}, {"referenceID": 5, "context": "The WSJ experiments are based on the popular wsj5k task (aligned with the CHiME 3 [6] task) and are conducted under two different acoustic conditions.", "startOffset": 82, "endOffset": 85}, {"referenceID": 43, "context": "For more details see [48, 47].", "startOffset": 21, "endOffset": 29}, {"referenceID": 42, "context": "For more details see [48, 47].", "startOffset": 21, "endOffset": 29}, {"referenceID": 25, "context": "The column corresponding to the pre-training without batch normalization represents a system that most closely matches the approaches followed in [30, 31].", "startOffset": 146, "endOffset": 154}, {"referenceID": 26, "context": "The column corresponding to the pre-training without batch normalization represents a system that most closely matches the approaches followed in [30, 31].", "startOffset": 146, "endOffset": 154}, {"referenceID": 25, "context": "Note that these values are smaller than that considered in [30, 29], where a pure gradient summation (\u03bb = 1) was adopted.", "startOffset": 59, "endOffset": 67}, {"referenceID": 24, "context": "Note that these values are smaller than that considered in [30, 29], where a pure gradient summation (\u03bb = 1) was adopted.", "startOffset": 59, "endOffset": 67}, {"referenceID": 35, "context": "We argue that this result is due to the fact that, as observed in [40], the norm of the gradient decays very slowly when adopting batch normalization with a proper initialization of \u03b3, even after the gradient has passed through many hidden layers.", "startOffset": 66, "endOffset": 70}], "year": 2017, "abstractText": "Improving distant speech recognition is a crucial step towards flexible human-machine interfaces. Current technology, however, still exhibits a lack of robustness, especially when adverse acoustic conditions are met. Despite the significant progress made in the last years on both speech enhancement and speech recognition, one potential limitation of state-ofthe-art technology lies in composing modules that are not well matched because they are not trained jointly. To address this concern, a promising approach consists in concatenating a speech enhancement and a speech recognition deep neural network and to jointly update their parameters as if they were within a single bigger network. Unfortunately, joint training can be difficult because the output distribution of the speech enhancement system may change substantially during the optimization procedure. The speech recognition module would have to deal with an input distribution that is non-stationary and unnormalized. To mitigate this issue, we propose a joint training approach based on a fully batch-normalized architecture. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework significantly overtakes other competitive solutions, especially in challenging environments.", "creator": "LaTeX with hyperref package"}}}