{"id": "1703.08524", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Joint Modeling of Event Sequence and Time Series with Attentional Twin Recurrent Neural Networks", "abstract": "A variety of real-world processes (over networks) produce sequences of data whose complex temporal dynamics need to be studied. More especially, the event timestamps can carry important information about the underlying network dynamics, which otherwise are not available from the time-series evenly sampled from continuous signals. Moreover, in most complex processes, event sequences and evenly-sampled times series data can interact with each other, which renders joint modeling of those two sources of data necessary. To tackle the above problems, in this paper, we utilize the rich framework of (temporal) point processes to model event data and timely update its intensity function by the synergic twin Recurrent Neural Networks (RNNs). In the proposed architecture, the intensity function is synergistically modulated by one RNN with asynchronous events as input and another RNN with time series as input. Furthermore, to enhance the interpretability of the model, the attention mechanism for the neural point process is introduced. The whole model with event type and timestamp prediction output layers can be trained end-to-end and allows a black-box treatment for modeling the intensity. We substantiate the superiority of our model in synthetic data and three real-world benchmark datasets.", "histories": [["v1", "Fri, 24 Mar 2017 17:29:14 GMT  (2457kb,D)", "http://arxiv.org/abs/1703.08524v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuai xiao", "junchi yan", "mehrdad farajtabar", "le song", "xiaokang yang", "hongyuan zha"], "accepted": false, "id": "1703.08524"}, "pdf": {"name": "1703.08524.pdf", "metadata": {"source": "CRF", "title": "Joint Modeling of Event Sequence and Time Series with Attentional Twin Recurrent Neural Networks", "authors": ["Shuai Xiao", "Junchi Yan", "Mehrdad Farajtabar", "Le Song", "Xiaokang Yang", "Hongyuan Zha"], "emails": ["benjaminforever@sjtu.edu.cn,", "xkyang@sjtu.edu.cn", "yanesta13@163.com"], "sections": [{"heading": null, "text": "Index terms - recurring neural networks, temporal point processes, relational mining, interpretable attention models.F"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that it is a matter of a way in which people place themselves and their environment in the center. (...) In fact, it is such that people are able to understand themselves. (...) It is not so that they feel able to understand the world. (...) It is as if they see themselves in the world, to change the world. (...) It is as if they are able to change the world. (...) It is so that they feel able to understand the world. (...) It is as if they see themselves in the world, to change the world. (...) It is as if they are able to change the world. (...) It is as if they feel able to change the world. (...) It is as if they see themselves in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world, in the world, in the world, in the world in the world in the, in the world in the, in the world, in the world, in the world in the, in the world, in the world in the, in the world in the, in the, in the world in the, in the world in the, in the world in the, in the world, in the, in the world, in the, in the world in the, in the world in the, in the, in the world in the, in the world, in the, in the world in the, in the world, in the, in the world in the, in the, in the, in the world, in the, in the world in the, in the, in the world in the, in the, in the, in the world in the, in the, in the world, in the, in the, in the, in the"}, {"heading": "2 RELATED WORK AND MOTIVATION", "text": "It is about the question to what extent it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way in which it is about a way, in which it is about a way in which it is about a way, in which it is about a way and in which it is about a way in which it is about a way, in which it is about which it is about a way and in which it is about a way, in which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is"}, {"heading": "3 NETWORK STRUCTURE AND LEARNING", "text": "In this section we present the proposed network structure together with the learning algorithm for modeling the behavior of dynamic events."}, {"heading": "3.1 Brief on RNN as building block", "text": "Taking a sequence {x} Tt = 1 as input, the RNN generates the hidden states {h} Tt = 1, also known as high-level representation of the inputs [24], [25]: ht = f (Wxt + Hht \u2212 1 + b), where xt is the profile associated with each event, and f is a nonlinear function, and W, H, b are parameters to be learned. A common choice for the nonlinear function f is sigmoid or tanh, who suffers from the problem of disappearance gradients [47] and is poor long-range dependency modeling ability. In contrast, we implement our RNN with Long Short Term Memory (LSTM) [27] for its popularity \u00a9 2013 IBM Research-China22, te jj te, eh1 eh 2 e jhzvz jcz z1 zz j11, te22, tz jtz, tzig."}, {"heading": "3.2 Infectivity matrix based attention mechanism", "text": "rE \"s rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "3.3 Network structure", "text": "For time series data, these signals are expected to reflect the states of each dimension and influence the predictive intensity of events. Therefore, we have: hyt = LSTMy (yt, h y t \u2212 1) For the event sequence {zi, ti} Ni = 1, we can create hidden states by LSTM that can capture the dependence of events over a long period of time. First, we project the dimension zi to a deep-dimensional embedding of vector space. Then, the embedded vector is fed to LSTM in combination with time stamps. We use the following equation to represent the process: ei = Wemzi, hei = LSTei (ti)."}, {"heading": "3.4 End-to-end learning", "text": "The probability of observing a sequence {zi, ti} Ni = 1 together with time series signals {yt} Tt = 1 can be expressed as follows: L ({zi, ti} Ni = 1) = N \u2212 1 \u2211 j = 1 {bzj + 1 log (u zj + 1 j + 1) + log (f (tj + 1 | Htj))) (9), whereby the weight parameters b are set as a reversal of the sample number in this dimension against the total size of the samples in order to weigh more on these dimensions with fewer training samples. This is in line with the importance policy for distorted data in machine learning [49]. For the second term, the underlying consideration is that we not only promote the correct prediction of the coming event dimension, but also require the corresponding time stamp of the event in order to be close to the ground truth."}, {"heading": "4 EXPERIMENTS AND DISCUSSION", "text": "We evaluate the proposed approach on both synthetic and real data sets, covering three popular application scenarios: social networking analysis, mining electronic health records (EHR), and proactive machine maintenance; the first two scenarios include publicly available benchmark records: MemeTracker and MIMIC, while the last one includes a private ATM maintenance record from a North American-based commercial bank; the code is based on Theano running on a 32 G Linux server; 2 6-core CPUs: Intel (R) Xeon (R) CPU E5-2603 v3 @ 1.60GHz; and we also use 4 GPU: GeForce GTX TITAN X with 12G memory supported by CUDA and MKL."}, {"heading": "4.1 Baselines and evaluation metrics", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2 Experiments on synthetic data", "text": "Synthetic data generation. We use simulated data with known ground truth to quantitatively verify our model using the above metrics. Specifically, we generate cascades of multidimensional falcons processed via the thinning algorithm. [52] We select Z = 20 for the number of event dimensions. The term background intensity is randomly set uniformly: \u00b5d \u0445 U (0, 0.01). Mutual influence is set similar to that of aij \u0445 U (0, 0.1). Half of the elements in the infectivity matrix are randomly set to 0. https: / en.wikipedia.org / wiki / Kendall ranks the correlation coefficient 8the deficiency of the influence between dimensions in many real-world problems. For the stability of the simulation process, the matrix is scaled so that it does not spectrally larger than a radius."}, {"heading": "4.3 Predictive machine maintenance", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4.4 Social network analysis", "text": "In line with previous work on tracking information [14], [42], [53], the public database MemeTracker4 is4. http: / / memetracker.org10used in this paper, which contains more than 172 million news articles or blog posts from various online media. Information, such as ideas, products, and user behavior, spreads through websites in subtle ways. For example, when Mark Zuckerberg posted, \"I just killed a pig and a goat,\" the memes appeared in theguardian, Fortune, Business Insider one by one, and it went viral. This cascade can be seen as a realization of information diffusion across the network. By looking at the observed diffusion history, we want to know by which side and when a particular meme is likely to spread. We also want to uncover the hidden diffusion structure from these meme cascades that is useful."}, {"heading": "4.5 Electronic health records mining", "text": "This year, it has reached the point where it is able to retaliate."}, {"heading": "5 CONCLUSION", "text": "We conclude this work with Fig. 11 and identify our proposed method as a recursive point-process model. To elaborate, the Hawkes process uses a fully explicit parametric model and RMTPP misses the dense time series characteristics to model time-varying base intensity, assuming a partial parametric form and modelling the pseudo-multidimensional \u00a9 2013 IBM Research-China13point process. We take another step by proposing an interpretative model that is simple and general and can be trained end-to-end. Most importantly, our model can uncover the subtle network structure and provide interpretable evidence for predicting the outcome. The extensive experiments in this paper have clearly suggested its superior performance in synthetic and real data, even if we do not have domain knowledge of the problem. This is in contrast to existing point-process models where an assumption about dynamics must be specified frequently beforehand."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Robert Chen of Emory University School of Medicine and for helpful discussions and suggestions for exploring the experimental calculation results of the MIMIC dataset. We are also grateful to Changsheng Li for providing us with IBM's ATM log data so that we can perform the predictive study on real data."}], "references": [{"title": "Survival and event history analysis: a process point of view", "author": ["O. Aalen", "O. Borgan", "H. Gjessing"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Identifying and labeling search tasks via query-based hawkes processes", "author": ["L. Li", "H. Deng", "A. Dong", "Y. Chang", "H. Zha"], "venue": "KDD, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics", "author": ["L. Yu", "P. Cui", "F. Wang", "C. Song", "S. Yang"], "venue": "2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Coevolve: A joint point process model for information diffusion and network co-evolution", "author": ["M. Farajtabar", "Y. Wang", "M.G. Rodriguez", "S. Li", "H. Zha", "L. Song"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1954\u20131962.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent marked temporal point processes: Embedding event history to vectore", "author": ["N. Du", "H. Dai", "R. Trivedi", "U. Upadhyay", "M. Gomez-Rodriguez", "L. Song"], "venue": "KDD, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Shaping social activity by incentivizing users", "author": ["M. Farajtabar", "N. Du", "M.G. Rodriguez", "I. Valera", "H. Zha", "L. Song"], "venue": "Advances in neural information processing systems, 2014, pp. 2474\u20132482.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The analysis of time series: an introduction", "author": ["C. Chatfield"], "venue": "CRC press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Event detection from time series data", "author": ["V. Guralnik", "J. Srivastava"], "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 1999, pp. 33\u201342.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Hawkes processes in finance", "author": ["E. Bacry", "I. Mastromatteo", "J.-F. Muzy"], "venue": "Market Microstructure and Liquidity, vol. 1, no. 01, p. 1550005, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A nonparametric em algorithm for multiscale hawkes processes", "author": ["E. Lewis", "E. Mohler"], "venue": "Journal of Nonparametric Statistics, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning triggering kernels for multi-dimensional hawkes processes.", "author": ["K. Zhou", "H. Zha", "L. Song"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Modeling and predicting popularity dynamics via reinforced poisson processes", "author": ["H. Shen", "D. Wang", "C. Song", "A. Barab\u00e1si"], "venue": "AAAI, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Reactive point processes: A new approach to predicting power failures in underground electrical systems", "author": ["S. Ertekin", "C. Rudin", "T.H. McCormick"], "venue": "The Annals of Applied Statistics, vol. 9, no. 1, pp. 122\u2013144, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Constructing disease network and temporal progression model via context-sensitive hawkes process", "author": ["E. Choi", "N. Du", "R. Chen", "L. Song", "J. Sun"], "venue": "ICDM. IEEE, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Patient flow prediction via discriminative learning of mutually-correcting processes", "author": ["H. Xu", "W. Wu", "S. Nemati", "H. Zha"], "venue": "TKDE, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectra of some self-exciting and mutually exciting point processes", "author": ["A.G. Hawkes"], "venue": "Biometrika, 1971.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1971}, {"title": "Modeling the intensity function of point process via recurrent neural networks", "author": ["X. Shuai", "J. Yan", "X. Yang", "H. Zha", "S. Chu"], "venue": "AAAI, 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Multivariate hawkes processes", "author": ["T.J. Liniger"], "venue": "PhD thesis, Swiss Federal Institute Of Technology, Zurich, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards effective prioritizing water pipe replacement and rehabilitation", "author": ["J. Yan", "Y. Wang", "K. Zhou", "J. Huang", "C.H. Tian", "H.Y. Zha", "W.S. Dong"], "venue": "IJCAI, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning granger causality for hawkes processes", "author": ["H. Xu", "M. Farajtabar", "H. Zha"], "venue": "ICML, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, vol. 14, pp. 179\u2013211, 1990.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv:1412.3555, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Rezende", "D. Wierstra"], "venue": "ICML, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards end-toend speech recognition with recurrent neural networks", "author": ["A. Graves", "A. rahman Mohamed", "G. Hinton"], "venue": "ICML, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "NIPS, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural networks and robust time series prediction", "author": ["J.T. Connor", "R.D. Martin", "L.E. Atlas"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 240\u2013254, 1994.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1994}, {"title": "Cooperative coevolution of elman recurrent neural networks for chaotic time series prediction", "author": ["R. Chandra", "M. Zhang"], "venue": "Neurocomputing, vol. 86, pp. 116\u2013123, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural networks for driver activity anticipation via sensory-fusion architecture", "author": ["A. Jain", "A. Singh", "H.S. Koppula", "S. Soh", "A. Saxena"], "venue": "ICRA, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Retain: An interpretable predictive model for healthcare using reverse time attention mechanism", "author": ["E. Choi", "M.T. Bahadori", "J. Sun", "J. Kulas", "A. Schuetz", "W. Stewart"], "venue": "NIPS, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville"], "venue": "ICML, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["K. Cho", "A. Courville", "Y. Bengio"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 1875\u20131886, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1875}, {"title": "A survey of random processes with reinforcement", "author": ["R. Pemantle"], "venue": "Probability Survey, vol. 4, no. 0, pp. 1\u201379, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Netcodec: Community detection from individual activities", "author": ["L. Tran", "M. Farajtabar", "L. Song", "H. Zha"], "venue": "Proceedings of the 2015 SIAM International Conference on Data Mining. SIAM, 2015, pp. 91\u201399.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Selfexciting point process models of insurgency in iraq", "author": ["E. Lewis", "G. Mohler", "P.J. Brantingham", "A. Bertozzi"], "venue": "UCLA CAM Reports 10, vol. 38, 2010.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Multistage campaigning in social networks", "author": ["M. Farajtabar", "X. Ye", "S. Harati", "L. Song", "H. Zha"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 4718\u20134726.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent poisson factorization for temporal recommendation", "author": ["S.A. Hosseini", "K. Alizadeh", "A. Khodadadi", "A. Arabzadeh", "M. Farajtabar", "H. Zha", "H.R. Rabiee"], "venue": "arXiv preprint arXiv:1703.01442, 2017.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "Distilling information reliability and source trustworthiness from digital traces", "author": ["B. Tabibian", "I. Valera", "M. Farajtabar", "L. Song", "B. Sch\u00f6lkopf", "M. Gomez-Rodriguez"], "venue": "arXiv preprint arXiv:1610.07472, 2016.  14", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Statistical models for earthquake occurrences and residual analysis for point processes", "author": ["Y. Ogata"], "venue": "J. Amer. Statist. Assoc., vol. 83, no. 401, pp. 9\u201327, 1988.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1988}, {"title": "A self-correcting pint process", "author": ["V. Isham", "M. Westcott"], "venue": "Advances in Applied Probability, vol. 37, pp. 629\u2013646, 1979.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1979}, {"title": "On the difficulty of training recurrent neural networks.", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML (3),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "A cluster process representation of a self-exciting process", "author": ["A.G. Hawkes", "D. Oakes"], "venue": "Journal of Applied Probability, 1974.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1974}, {"title": "Classifying skewed data: Importance weighting to optimize average recall.", "author": ["A. Rosenberg"], "venue": "INTERSPEECH,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y.N. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "arXiv:1502.04390, 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards effective prioritizing water pipe replacement and rehabilitation", "author": ["J. Yan", "Y. Wang", "K. Zhou", "J. Huang", "C.H. Tian", "H.Y. Zha", "W.S. Dong"], "venue": "IJCAI, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "On lewis\u2019 simulation method for point processes", "author": ["Y. Ogata"], "venue": "IEEE Transactions on Information Theory, vol. 27, no. 1, pp. 23\u201331, 1981.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1981}, {"title": "Uncovering the structure and temporal dynamics of information propagation", "author": ["M.G. Rodriguez", "J. Leskovec", "D. Balduzzi", "B. Sch\u00f6lkopf"], "venue": "Network Science, vol. 2, no. 01, pp. 26\u201365, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Influence maximization in dynamic social networks", "author": ["H. Zhuang", "Y. Sun", "J. Tang", "J. Zhang", "X. Sun"], "venue": "2013.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring networks of diffusion and influence", "author": ["M. Gomez Rodriguez", "J. Leskovec", "A. Krause"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2010, pp. 1019\u20131028.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Uncovering the temporal dynamics of diffusion networks", "author": ["M. Gomez Rodriguez", "D. Balduzzi", "B. Sch\u00f6lkopf", "G.T. Scheffer"], "venue": "28th International Conference on Machine Learning (ICML 2011). International Machine Learning Society, 2011, pp. 561\u2013568.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of statistical mechanics: theory and experiment, vol. 2008, no. 10, p. P10008, 2008.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2008}, {"title": "Laplacian dynamics and multiscale modular structure in networks", "author": ["R. Lambiotte", "J.-C. Delvenne", "M. Barahona"], "venue": "arXiv preprint arXiv:0812.1770, 2008.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "A major line of research [1] has been devoted to studying event sequence, especially exploring the timestamp information to model the underlying dynamics of the system, whereby point process has been a powerful and elegant framework", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": ", based on thresholding the stock price series [11]) from the series data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": ", [13] start to turn to non-parametric form to fit the structure of a point process, but their method is under the Hawkes process formulation, which runs the risk of unknown model complexity and can be inappropriate for point processes that disobey the self or mutual-exciting rule assumed by the Hawkes model [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [13] start to turn to non-parametric form to fit the structure of a point process, but their method is under the Hawkes process formulation, which runs the risk of unknown model complexity and can be inappropriate for point processes that disobey the self or mutual-exciting rule assumed by the Hawkes model [19].", "startOffset": 310, "endOffset": 314}, {"referenceID": 4, "context": "In another recent work [6], the authors proposed a semi-parametric pesudo point process model, which assumes a time-decaying influence between events and the background of intensity is constant.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "As an extension to the conference version [20]1, the overall highlights of this paper are:", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "[22] that prohibiting the wide use for practitioners.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Previous work [6] use the RNN to model so-called pseudo multi-dimensional point process [21].", "startOffset": 14, "endOffset": 17}, {"referenceID": 17, "context": "Previous work [6] use the RNN to model so-called pseudo multi-dimensional point process [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "The main extensions include: i) in contrast to [20] where the socalled pseudo multi-dimensional point process [21] is adopted which involves only a single intensity function for all types of event sequence, here we separately model the intensity functions for each event type leading to the so-called genuine multi-dimensional point process via recurrent neural networks; ii) based on the resulting multi-dimensional point process model, we incorporate a new attention mechanism to improve the interpretability of the prediction model.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "The main extensions include: i) in contrast to [20] where the socalled pseudo multi-dimensional point process [21] is adopted which involves only a single intensity function for all types of event sequence, here we separately model the intensity functions for each event type leading to the so-called genuine multi-dimensional point process via recurrent neural networks; ii) based on the resulting multi-dimensional point process model, we incorporate a new attention mechanism to improve the interpretability of the prediction model.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "While the conference paper [20] only deals with event prediction rather than relation mining.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "One typical resulting utility involves decision support and causality analysis [23].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "The building block of our model is the Recurrent Neural Networks (RNNs) [24], [25] and its modern variants e.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "The building block of our model is the Recurrent Neural Networks (RNNs) [24], [25] and its modern variants e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": ", Long Short-Term Memory (LSTM) units [26], [27] and Gated Recurrent Units (GRU) [28].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": ", Long Short-Term Memory (LSTM) units [26], [27] and Gated Recurrent Units (GRU) [28].", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": ", Long Short-Term Memory (LSTM) units [26], [27] and Gated Recurrent Units (GRU) [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 25, "context": "Recently, RNNs and LSTMs have been successfully applied in large-scale vision [29], speech [30] and language [31] problems.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "Recently, RNNs and LSTMs have been successfully applied in large-scale vision [29], speech [30] and language [31] problems.", "startOffset": 91, "endOffset": 95}, {"referenceID": 27, "context": "Recently, RNNs and LSTMs have been successfully applied in large-scale vision [29], speech [30] and language [31] problems.", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "i) Synchronized series: RNNs have been a long time a natural tool for standard time series modeling and prediction [32], [33], whereby the indexed series data point", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "i) Synchronized series: RNNs have been a long time a natural tool for standard time series modeling and prediction [32], [33], whereby the indexed series data point", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "In a broader sense, video frames can also be treated as time series and RNN are widely used in recent visual analytics works [34] and so for speech [30].", "startOffset": 125, "endOffset": 129}, {"referenceID": 26, "context": "In a broader sense, video frames can also be treated as time series and RNN are widely used in recent visual analytics works [34] and so for speech [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "RNNs are also intensively adopted for sequence modeling tasks [28] when only order information is considered.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "ii) Asynchronous event: In contrast, event sequence with timestamp about their occurrence, which are asynchronously and randomly distributed over the continuous time space, is another typical input type for RNNs [6], [35] (despite its title for \u2019time series\u2019).", "startOffset": 212, "endOffset": 215}, {"referenceID": 31, "context": "ii) Asynchronous event: In contrast, event sequence with timestamp about their occurrence, which are asynchronously and randomly distributed over the continuous time space, is another typical input type for RNNs [6], [35] (despite its title for \u2019time series\u2019).", "startOffset": 217, "endOffset": 221}, {"referenceID": 31, "context": "There have been several attempts to interpret RNNs [35], [36], [37].", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "There have been several attempts to interpret RNNs [35], [36], [37].", "startOffset": 57, "endOffset": 61}, {"referenceID": 33, "context": "There have been several attempts to interpret RNNs [35], [36], [37].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "However, they either compute the attention score by the same function regardless of the affected point\u2019s dimension [35], or only consider the hidden state of the decoder for sequence prediction [36], [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 32, "context": "However, they either compute the attention score by the same function regardless of the affected point\u2019s dimension [35], or only consider the hidden state of the decoder for sequence prediction [36], [37].", "startOffset": 194, "endOffset": 198}, {"referenceID": 33, "context": "However, they either compute the attention score by the same function regardless of the affected point\u2019s dimension [35], or only consider the hidden state of the decoder for sequence prediction [36], [37].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "Point process is a mathematically rich and principled framework for modeling event data [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "2) Reinforced Poisson processes [15], [39]: the model cap-", "startOffset": 32, "endOffset": 36}, {"referenceID": 34, "context": "2) Reinforced Poisson processes [15], [39]: the model cap-", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "tures the \u2018rich-get-richer\u2019 mechanism characterized by a compact intensity function, which is recently used for popularity prediction [15].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "3) Hawkes process [19]: Recently, Hawkes process has received a wide attention in network cascades modeling [5],", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "3) Hawkes process [19]: Recently, Hawkes process has received a wide attention in network cascades modeling [5],", "startOffset": 108, "endOffset": 111}, {"referenceID": 35, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 69, "endOffset": 72}, {"referenceID": 36, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 141, "endOffset": 145}, {"referenceID": 38, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 170, "endOffset": 174}, {"referenceID": 39, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 217, "endOffset": 221}, {"referenceID": 40, "context": "The model is originally motivated to analyze the earthquake and its aftershocks [45].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "4) Reactive point process [16]: it can be regarded as a generalization of the Hawkes process by adding a selfinhibiting term to account for the inhibiting effects from history events.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "5) Self-correcting process [46]: its background part increases steadily, while it is decreased by a constant e\u2212\u03b1 < 1 every time a new event appears.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Taking a sequence {x}t=1 as input, the RNN generates the hidden states {h}t=1, also known high-level representation of inputs [24], [25]:", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "Taking a sequence {x}t=1 as input, the RNN generates the hidden states {h}t=1, also known high-level representation of inputs [24], [25]:", "startOffset": 132, "endOffset": 136}, {"referenceID": 42, "context": "problem [47] and poor long-range dependency modeling capability.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "In contrast, we implement our RNN with Long Short Term Memory (LSTM) [26], [27] for its popularity \u00a9 2013 IBM Research-China 2 2 , t e j j t e , e h1 e h 2 e j h z v z j c", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "In contrast, we implement our RNN with Long Short Term Memory (LSTM) [26], [27] for its popularity \u00a9 2013 IBM Research-China 2 2 , t e j j t e , e h1 e h 2 e j h z v z j c", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "Gated Recurrent Units (GRU) [28] can also be alternative choices, while the analysis of the consequence of this particular choice is not the focus of our paper.", "startOffset": 28, "endOffset": 32}, {"referenceID": 43, "context": "The former can timely affect the transient occurrence intensity of events and the latter can often abruptly cause jump and transition of states of agents and capture longrange event dependency [48].", "startOffset": 193, "endOffset": 197}, {"referenceID": 30, "context": "Empirically, it has been shown in some recent study [34] that using separate RNN for each time series data e.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "For example in the problem of multi-dimensional Hawkes process learning, one important goal is to uncover the hidden network structure, infectivity matrix A, from real-world event sequences, such as the influence strength between users in social network [5], [14], or progression relationship between event types [17].", "startOffset": 254, "endOffset": 257}, {"referenceID": 13, "context": "For example in the problem of multi-dimensional Hawkes process learning, one important goal is to uncover the hidden network structure, infectivity matrix A, from real-world event sequences, such as the influence strength between users in social network [5], [14], or progression relationship between event types [17].", "startOffset": 313, "endOffset": 317}, {"referenceID": 19, "context": "Uncovering the hidden structure is also stressed in causal analysis [23], which gives much evidence for prediction result.", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "Here we choose the widely used soft attention mechanism [36], whose influence from former events is in an additive form [5]:", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "Here we choose the widely used soft attention mechanism [36], whose influence from former events is in an additive form [5]:", "startOffset": 120, "endOffset": 123}, {"referenceID": 33, "context": "Note that hard attention [37] only assigns 0 or 1 to the influence strength \u03b1 zi , which is too rough to capture fine-grained influence.", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "work [35], [36], [37] in that they only consider one-way effect over the sequence.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "work [35], [36], [37] in that they only consider one-way effect over the sequence.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "work [35], [36], [37] in that they only consider one-way effect over the sequence.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "4 is also used in [36], [37] to model the one-way attention weight \u03b1i = fatt(hi,v).", "startOffset": 18, "endOffset": 22}, {"referenceID": 33, "context": "4 is also used in [36], [37] to model the one-way attention weight \u03b1i = fatt(hi,v).", "startOffset": 24, "endOffset": 28}, {"referenceID": 44, "context": "This is in line with the importance weighting policy for skewed data in machine learning [49].", "startOffset": 89, "endOffset": 93}, {"referenceID": 45, "context": "We adopt RMSprop gradients [50] which have been shown", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": ", Majorization-Minimization techniques [5], [51]) used in generative Point process models.", "startOffset": 39, "endOffset": 42}, {"referenceID": 46, "context": ", Majorization-Minimization techniques [5], [51]) used in generative Point process models.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "recent work [18], another limitation for the generative point process model is that they are aimed to maximize the joint", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "2) Hawkes Process: To enable multi-type event prediction, we use a Multi-dimensional Hawkes process [7], [14].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "3) Recurrent Marked Temporal Point Processes (RMTPP): [6] uses a neural network to model the event dependency flexibly.", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "This method is the one presented in the conference version of this paper [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "Poisson Processes [15], [39].", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "Poisson Processes [15], [39].", "startOffset": 24, "endOffset": 28}, {"referenceID": 47, "context": "from multi-dimensional hawkes process via the Thinning algorithm [52].", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "In line with the previous works for information diffusion tracking [14], [42], [53], the public dataset MemeTracker4 is", "startOffset": 73, "endOffset": 77}, {"referenceID": 48, "context": "In line with the previous works for information diffusion tracking [14], [42], [53], the public dataset MemeTracker4 is", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": ", marketing by information maximization [54].", "startOffset": 40, "endOffset": 44}, {"referenceID": 37, "context": "For the experiments here, we use the top 500 media sites with the largest number of documents and select the meme cascades diffuse over them as done in previous works [14], [42].", "startOffset": 173, "endOffset": 177}, {"referenceID": 50, "context": "As the ground truth of network is unknown, we proceed by following the protocol as designed and adopted in [14], [55], [56].", "startOffset": 113, "endOffset": 117}, {"referenceID": 51, "context": "As the ground truth of network is unknown, we proceed by following the protocol as designed and adopted in [14], [55], [56].", "startOffset": 119, "endOffset": 123}, {"referenceID": 52, "context": "In order to visualize the learned network, we use community detection algorithm [57] with resolution 0.", "startOffset": 80, "endOffset": 84}, {"referenceID": 53, "context": "9 [58] over learned directed network of ATRPP, which renders 18 communities.", "startOffset": 2, "endOffset": 6}, {"referenceID": 52, "context": "Similar to MemeTracker, community detection algorithm [57] with resolution 0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "bodiments of the first two blocks can be referred to [14] and [6] respectively.", "startOffset": 62, "endOffset": 65}, {"referenceID": 16, "context": "our conference version [20] and this extended journal paper (from left to right).", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "A variety of real-world processes (over networks) produce sequences of data whose complex temporal dynamics need to be studied. More especially, the event timestamps can carry important information about the underlying network dynamics, which otherwise are not available from the time-series evenly sampled from continuous signals. Moreover, in most complex processes, event sequences and evenly-sampled times series data can interact with each other, which renders joint modeling of those two sources of data necessary. To tackle the above problems, in this paper, we utilize the rich framework of (temporal) point processes to model event data and timely update its intensity function by the synergic twin Recurrent Neural Networks (RNNs). In the proposed architecture, the intensity function is synergistically modulated by one RNN with asynchronous events as input and another RNN with time series as input. Furthermore, to enhance the interpretability of the model, the attention mechanism for the neural point process is introduced. The whole model with event type and timestamp prediction output layers can be trained end-to-end and allows a black-box treatment for modeling the intensity. We substantiate the superiority of our model in synthetic data and three real-world benchmark datasets.", "creator": "LaTeX with hyperref package"}}}