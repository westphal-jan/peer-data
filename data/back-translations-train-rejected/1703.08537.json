{"id": "1703.08537", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Crowdsourcing Universal Part-Of-Speech Tags for Code-Switching", "abstract": "Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for codeswitching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task.", "histories": [["v1", "Fri, 24 Mar 2017 17:55:33 GMT  (18kb)", "http://arxiv.org/abs/1703.08537v1", "Submitted to Interspeech 2017"]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["victor soto", "julia hirschberg"], "accepted": false, "id": "1703.08537"}, "pdf": {"name": "1703.08537.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vsoto@cs.columbia.edu,", "julia@cs.columbia.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.08 537v 1 [cs.C L] 24 Mar 201 7ers switch while communicating between multiple languages. The importance of developing language technologies for encoding data is immense given the large populations that routinely perform code switching. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, few such data exist for code switching. In this essay, we describe crowd sourcing universal part-of-of-speech tags for the Miami Bangor Corpus Spanish-English codeSwitched idiom. We divide the annotation task into three subtasks: one in which a subset of tokens is automatically labeled, one in which questions are specifically designed to disambiguate a subset of high-frequency words for the Miami Bangor Corpus Spanish codeSwitched idiom, and a more general cascaded pre-task is expanded to include a subset of tokens for each remaining worker's decision-making structure according to the three linguistic references and a subset of the remaining worker's decision-decision-making structure."}, {"heading": "1. Introduction & Previous Work", "text": "In fact, most of them will be able to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2. The Miami Bangor Corpus", "text": "The Miami Bangor corpus is a conversation corpus recorded by bilingual Spanish-English speakers residing in Miami, Florida. It comprises 56 files of conversation speeches by 84 speakers. The corpus consists of 242,475 words (transcribed) and 35 hours of recorded conversation. 63% of the transcribed words are English, 34% Spanish and 3% are indeterminate. Manual transcripts include start and end times of utterances and identification per word. The original Bangor Miami corpus has been automatically embellished and tagged with POS tags using the Bangor autoglosser [17, 18]. The autoglosser finds the closest English gloss for each token in the corpus and assigns the tag or group of tags most commonly used for that word in the annotated language."}, {"heading": "3. Annotation Scheme", "text": "The annotation scheme we have developed consists of several tasks: each token is assigned to a marker task depending on the word identity, language, and whether it is present in one of three fragmented word lists. It combines a) manual annotations by computer linguists, b) automatic annotations based on knowledge distilled from Penn TreeBank guidelines and the Universal Tagset Guidelines, and c) and d) two language-specific crowdsourcing tasks, one for English and one for Spanish. The pseudo-code of the annotation scheme is represented in Algorithm 1. Table 1 shows the number and percentage of tokens tagged in each annotation task (second and third column), and the percentage of tokens commented by experts in the lab, either because it was the manual task or because there was a discrepancy in the crowdsourcing task. In the next sections, we explain each of the anchor blocks - all of the following, and not the answer blocks - in detail."}, {"heading": "3.1. Automatically tagged tokens", "text": "In English, the PTB annotation policy [16] instructs annotators to mark a certain subset of words with a specific POS tag. We follow these instructions by mapping the fixed PTB tag to a universal tag. In addition, we extend this list of words by a) English words, which we always mark with the same universal tag in the Universal Dependencies Dataset, and b) low-frequency words, which we only found with a unique tag in the Bangor Corpus. Similarly, in Spanish, we automatically mark all words marked with a unique tag in the Universal Dependencies Dataset (e.g. conjunctions such as \"aunque,\" \"e,\" \"o,\" y \"etc.; adpositions such as\" a, \"con\" de, \"etc.; and some adverbs, pronouns, and digits) and low-frequency words, which occur with only one tag in the Bangor Corpus (e.g.\" aquella, \"de,\" pronouns, \"and adverbs), and addencations, etc."}, {"heading": "3.2. Manually tagged tokens", "text": "We identified a number of English and Spanish words that we found particularly difficult for na\u00efve workers to mark, and which occurred so frequently in the data set that we could have them marked by computer linguists in the laboratory. Note that a question specific to each of these tokens could have been designed for crowd-sourced annotations as it was done for the words in Section 3.3.1. Most of these tokens are tokens that could be clearly distinguished between adposition and adverb in English (e.g. \"up,\" \"down,\" \"between\") and between terminants and pronouns in Spanish (e.g. \"algunos / as,\" cua'ntos / as, \"muchos / as\")."}, {"heading": "3.3. Crowdsourcing Universal Tags", "text": "We used crowdsourcing to obtain new gold labels for each word that was not manually or automatically labeled. We started with the two basic approaches discussed in [15] for disambiguating POS tags using crowdsourcing, which we modified for a multilingual corpus. In the first task, a question and a series of answers were designed to decipher the POS tag of a particular token. In the second task, we defined two questionnaires (one for English and one for Spanish) that ask non-technical questions of the workers one after the other until the POS tag is clear. These questions were designed to require the worker to have minimal language skills. All required knowledge, including definitions, is given as a guide or as an example in each set of questions and answers. Most answers contain examples that illustrate the potential benefits of the token in this answer. Two judgments were gathered from the relevant crowdsourcing task and a third was calculated from a cardboard application of a universe."}, {"heading": "3.3.1. Token-specific questions (TSQ)", "text": "In this task, we designed a question and several answers specifically for specific word marks. The worker was then asked to select the answer that he thought was truest. Following is the question we asked the workers for the token \"can\" (note that users cannot see the POS tags if they select one of the answers): In the context of the sentence, \"can\" is a verb that takes on the meaning of \"to be able\" or \"to know\"? \u2022 Yes. For example: \"I can speak Spanish.\" (AUX) \u2022 No, it refers to a cylindrical container. For example: \"Pass me a can of beer.\" (NOUN) We started with the original list of English words and the questions that were developed in [15] for English. However, we added additional token-specific questions for words that we thought might be particularly challenging for labeling (e.g. \"as,\" \"\" off, \"\" \"and\" b)."}, {"heading": "3.3.2. Annotations Following a Question Tree", "text": "In this task, the worker is confronted with a sequence of questions that follow a tree structure. Each answer chosen by the user leads to the next question until a leaf node is reached when the token is assigned a POS tag. We followed the basic tree structure proposed in [15], but the trees can effectively be part of a multilingual context. If the worker reacts negatively, they are asked to follow the rest of the tree. This is very important, since every verb, adjective or nouns seems to be effective."}, {"heading": "3.3.3. Mapping Stage", "text": "To obtain this, we first cleaned the corpus of ambiguous tags and then defined a mapping from the Bangor tagset to the universal tagset. This mapping procedure was first published in [21]."}, {"heading": "4. Results", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able, that they are able to assert themselves, and that they are able to achieve their goals."}, {"heading": "5. Conclusions", "text": "We have introduced a new crowdsourcing scheme for the universal POS marking of Spanish-English code-swapped data from a monolingual process that also uses a different tagset. Our scheme consists of four different tasks (one automatic, one manual and two crowdsourcing tasks), each word in the corpus being sent to only one task based on curated word lists. In the case of crowdsourcing tokens, we have shown that we get highly accurate predictions by majority voting of one unguarded tag and two crowdsourcing judgments. We have also shown high consistency in predictions: between 95% and 99% of the tokens received two or more votes for the same day. Looking at the performance of each POS tag, our predictions remember an average of between 0.88 and 0.93 depending on the task."}, {"heading": "6. References", "text": "In fact, it is so that most people are able to survive themselves if they do not feel able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. \""}], "references": [{"title": "A smorgasbord of features for statistical machine translation.", "author": ["F.J. Och", "D. Gildea", "S. Khudanpur", "A. Sarkar", "K. Yamada", "A.M. Fraser", "S. Kumar", "L. Shen", "D. Smith", "K. Eng"], "venue": "in HLT- NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "The architecture of the festival speech synthesis system", "author": ["P. Taylor", "A.W. Black", "R. Caley"], "venue": "1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Assigning phrase breaks from partof-speech sequences", "author": ["P. Taylor", "A.W. Black"], "venue": "1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Accent and discourse context: Assigning pitch accent in synthetic speech.", "author": ["J. Hirschberg"], "venue": "in AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Unsupervised continuousvalued word features for phrase-break prediction without a partof-speech tagger.", "author": ["O. Watts", "J. Yamagishi", "S. King"], "venue": "INTERSPEECH,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Creating speech and language data with amazon\u2019s mechanical turk", "author": ["C. Callison-Burch", "M. Dredze"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computational Linguistics, 2010, pp. 1\u201312.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2008, pp. 254\u2013263.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast, cheap, and creative: evaluating translation quality using amazon\u2019s mechanical turk", "author": ["C. Callison-Burch"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association for Computational Linguistics, 2009, pp. 286\u2013295.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Data quality from crowdsourcing: a study of annotation selection criteria", "author": ["P.-Y. Hsueh", "P. Melville", "V. Sindhwani"], "venue": "Proceedings of the NAACL HLT 2009 workshop on active learning for natural language processing. Association for Computational Linguistics, 2009, pp. 27\u201335.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Annotating named entities in twitter data with crowdsourcing", "author": ["T. Finin", "W. Murnane", "A. Karandikar", "N. Keller", "J. Martineau", "M. Dredze"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computational Linguistics, 2010, pp. 80\u2013 88.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Corpus creation for new genres: A crowdsourced approach to pp attachment", "author": ["M. Jha", "J. Andreas", "K. Thadani", "S. Rosenthal", "K. McKeown"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computational Linguistics, 2010, pp. 13\u201320.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning whom to trust with mace.", "author": ["D. Hovy", "T. Berg-Kirkpatrick", "A. Vaswani", "E.H. Hovy"], "venue": "Proceedings of the NAACL HLT", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Labeling parts of speech using untrained annotators on mechanical turk", "author": ["J.E. Mainzer"], "venue": "Master\u2019s thesis, The Ohio State University, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision)", "author": ["B. Santorini"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "The bangor autoglosser: a multilingual tagger for conversational text", "author": ["K. Donnelly", "M. Deuchar"], "venue": "ITA11, Wrexham, Wales, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Using constraint grammar in the bangor autoglosser to disambiguate multilingual spoken text", "author": ["\u2014\u2014"], "venue": "Constraint Grammar Applications: Proceedings of the NODALIDA 2011 Workshop, Riga, Latvia, 2011, pp. 17\u201325.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "arXiv preprint arXiv:1104.2086, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, vol. 19, no. 2, pp. 313\u2013330, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Part of speech tagging for code switched data", "author": ["F. AlGhamdi", "G. Molina", "M. Diab", "T. Solorio", "A. Hawwari", "V. Soto", "J. Hirschberg"], "venue": "EMNLP 2016, p. 98, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "and machine translation [1].", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 2, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 3, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 4, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 5, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 6, "context": "It also raises some important questions about the validity and quality of the annotations, mainly: a) are aggregated labels by non-experts as good as labels by experts? b) what steps are necessary to ensure quality? and c) how do you explain complex tasks to non-experts to maximize output quality? [7].", "startOffset": 299, "endOffset": 302}, {"referenceID": 7, "context": "In [8] the authors crowdsourced annotations in five different NLP tasks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "[9] crowdsourced translation quality evaluations and found that by aggregating non-expert judgments it was possible to achieve the quality expected from experts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In [10] crowdsourcing was used to annotate sentiment in political snippets using multiple noisy labels.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[11] described a crowdsourced approach to obtaining Named Entity labels for Twitter data from a set of four labels using both AMT and CF.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposes a two-step disambiguation task to extract prepositional phrase attachments from noisy blog data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Using MACE [14] they obtained 82.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "thesis, Mainzer [15] proposed an interactive approach to crowdsourcing POS tags, where workers are assisted through a sequence of questions to help disambiguate the tags with minimal knowledge of linguistics.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Workers following this approach for the Penn Treebank (PTB) Tagset [16] achieved 90% accuracy.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "In this work, we propose to adapt the monolingual annotation scheme from [15] to crowdsource Universal POS tags in a code-switching setting for the Miami Bangor Corpus.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "The original Bangor Miami corpus was automatically glossed and tagged with POS tags using the Bangor Autoglosser [17, 18].", "startOffset": 113, "endOffset": 121}, {"referenceID": 16, "context": "The original Bangor Miami corpus was automatically glossed and tagged with POS tags using the Bangor Autoglosser [17, 18].", "startOffset": 113, "endOffset": 121}, {"referenceID": 17, "context": "To overcome these problems we decided to a) obtain new part-of-speech tags through in-lab annotation and crowdsourcing and b) to use the Universal Part-of-Speech Tagset [19].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Comparing it to the PTB POS tagset [16, 20], which has a total of 45 tags, the Universal POS tagset has only 17: Adjective, Adposition, Adverb, Auxiliary Verb, Coordinating and Subordinating Conjunction, Determiner, Interjection, Noun, Numeral, Proper Noun, Pronoun, Particles, Punctuation, Symbol, Verb and Other.", "startOffset": 35, "endOffset": 43}, {"referenceID": 18, "context": "Comparing it to the PTB POS tagset [16, 20], which has a total of 45 tags, the Universal POS tagset has only 17: Adjective, Adposition, Adverb, Auxiliary Verb, Coordinating and Subordinating Conjunction, Determiner, Interjection, Noun, Numeral, Proper Noun, Pronoun, Particles, Punctuation, Symbol, Verb and Other.", "startOffset": 35, "endOffset": 43}, {"referenceID": 14, "context": "For English, the PTB Annotation guidelines [16] instructs annotators to tag a certain subset of words with a given POS tag.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "We started with the two basic approaches discussed in [15] for disambiguating POS tags using crowdsourcing which we modified for a multilingual corpus.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "We began with the initial list of English words and the questions developed in [15] for English.", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "\u2018ese/a\u2019, \u2018este/a\u2019, \u2018la/lo\u2019) We modified many of the questions proposed in [15], to adapt them to a code-switching setting and to the universal POS tagset.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "We followed the basic tree structure proposed in [15], but needed to modify the trees considerably due again to the multilingual context.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "The resulting tree is slightly simpler than the one in [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "This mapping process was first published in [21].", "startOffset": 44, "endOffset": 48}], "year": 2017, "abstractText": "Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for codeswitching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task.", "creator": "LaTeX with hyperref package"}}}