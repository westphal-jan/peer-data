{"id": "1703.08595", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Low Precision Neural Networks using Subband Decomposition", "abstract": "Large-scale deep neural networks (DNN) have been successfully used in a number of tasks from image recognition to natural language processing. They are trained using large training sets on large models, making them computationally and memory intensive. As such, there is much interest in research development for faster training and test time. In this paper, we present a unique approach using lower precision weights for more efficient and faster training phase. We separate imagery into different frequency bands (e.g. with different information content) such that the neural net can better learn using less bits. We present this approach as a complement existing methods such as pruning network connections and encoding learning weights. We show results where this approach supports more stable learning with 2-4X reduction in precision with 17X reduction in DNN parameters.", "histories": [["v1", "Fri, 24 Mar 2017 20:59:50 GMT  (798kb)", "http://arxiv.org/abs/1703.08595v1", "Presented at CogArch Workshop, Atlanta, GA, April 2016"]], "COMMENTS": "Presented at CogArch Workshop, Atlanta, GA, April 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sek chai", "aswin raghavan", "david zhang", "mohamed amer", "tim shields"], "accepted": false, "id": "1703.08595"}, "pdf": {"name": "1703.08595.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. RELATED RESEARCH", "text": "In recent years, it has become clear that the number of unemployed people who are able to do their jobs is increasing. In recent years, the number of unemployed people who are able to do their jobs has doubled. In the last ten years, the number of unemployed people has doubled. In the last ten years, the number of unemployed people who are slipping into unemployment has doubled."}, {"heading": "III. APPROACH", "text": "In this section, we present details of our subband decomposition approach to low precision DNN. Our main goal is to study its properties to determine its effectiveness. To achieve this, we perform analyses on a baseline DNN without the various variants and methods that researchers have used to gain in algorithmic performance. We choose the LeNet-5 CNN, as shown in Figure 1, with five layers consisting of two conventional layers, and ReLU activation functions. A pooling / subsampling layer, which provides the maximum pooling function over non-overlapping windows of size 2x2, follows each layer with 5x5 filters. The second pooling layer leads to another layer consisting of a different layer."}, {"heading": "IV. RESULTS AND EVALUATION", "text": "In fact, most people who have lived in the US in recent years are not in a position to help themselves, but to help themselves."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "This feature helps to achieve better classification results (per MNIST result) or more stable learning (per CIFAR-10 result). We also show that our approach works orthogonally with less precision. We expect to be able to combine other approaches such as network bounce to further improve performance and approximate learning weights. We show that our approach works orthogonally with stochastic rounding approach to reduce precision. We expect to be able to combine other approaches such as network pruning to further improve performance. Costs associated with our proposed approach include processing the subjects during training phases. In this short study, we offer a simple fusion approach to combine the subband results."}, {"heading": "VI. ACKNOWLEDGEMENTS", "text": "This research was developed with the support of the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL). The views, opinions and / or results expressed are those of the authors and should not be interpreted to represent the official views or policies of the Department of Defense or the U.S. government."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "et. al"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5):602\u2013610, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert"], "venue": "JMLR, 12:2493\u20132537,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun"], "venue": "Proceedings of the IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Y. Taigman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep learning with COTS HPC systems", "author": ["A. Coates", "et. al"], "venue": "In 30th ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Learning both Weights and Connections for Efficient Neural Network.", "author": ["S. Han"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Subband and wavelet transforms: design and applications", "author": ["A. Ali", "MJT Smith", "eds"], "venue": "Vol. 340. Springer Science & Business Media,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Improving the speed of neural networks on CPUs", "author": ["V. Vanhoucke", "A. Senior", "M. Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Deep learning with limited numerical precision.", "author": ["S. Gupta"], "venue": "arXiv preprint arXiv:1502.02551", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["D. Soudry"], "venue": "In NIPS\u20192014,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Training binary multilayer neural networks for image classification using expectation backpropgation", "author": ["Z. Cheng"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights+", "author": ["K. Hwang", "W. Sung"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks", "author": ["J. Kim"], "venue": "IEEE ICASSP, pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Binary Connect: Training Deep Neural Networks with binary weights during propagations.", "author": ["M. Courbariaux", "Y. Bengio", "J-P David"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "On the Impact of Energy-Accuracy Tradeoff in a Digital Cellular Neural Network for Image Processing.\" Computer-Aided Design of Integrated Circuits and Systems", "author": ["J. Kung"], "venue": "IEEE Transactions on 34.7", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A Pyramid Framework for Real-Time Computer Vision", "author": ["P. Burt"], "venue": "Foundations of Image Understanding, ser. The Springer International Series in Engineering and Computer Science. Springer US, 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": ", in speech, vision and natural language) comes in part from the ability to train much larger models on much larger datasets than was previously possible [1-3].", "startOffset": 154, "endOffset": 159}, {"referenceID": 1, "context": ", in speech, vision and natural language) comes in part from the ability to train much larger models on much larger datasets than was previously possible [1-3].", "startOffset": 154, "endOffset": 159}, {"referenceID": 2, "context": ", in speech, vision and natural language) comes in part from the ability to train much larger models on much larger datasets than was previously possible [1-3].", "startOffset": 154, "endOffset": 159}, {"referenceID": 3, "context": "Net (CNN) [4] uses 1M parameters to classify handwritten digits.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "The AlexNet CNN [1] uses 60M parameters to win the ImageNet image classification competition in 2012.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "More recently, new DNNs such as Deepface uses 120M parameters for human face verification [5], and there are even networks with 10B parameters [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "More recently, new DNNs such as Deepface uses 120M parameters for human face verification [5], and there are even networks with 10B parameters [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "[7] estimates 12.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Our particular approach is inspired by image compression using wavelet decomposition [8].", "startOffset": 85, "endOffset": 88}, {"referenceID": 8, "context": "[9] converted 32-bit floating point activations to 8-bit integer fixed point implementations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] compressed", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] explore stochastic rounding techniques to implement DNN from 32-bit floating point to 16-bit fixed point integers on", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Another approach is focused on training the DNNs with defined target weights [12-15].", "startOffset": 77, "endOffset": 84}, {"referenceID": 12, "context": "Another approach is focused on training the DNNs with defined target weights [12-15].", "startOffset": 77, "endOffset": 84}, {"referenceID": 13, "context": "Another approach is focused on training the DNNs with defined target weights [12-15].", "startOffset": 77, "endOffset": 84}, {"referenceID": 14, "context": "Another approach is focused on training the DNNs with defined target weights [12-15].", "startOffset": 77, "endOffset": 84}, {"referenceID": 15, "context": "[16] shows a training method using binary weights to avoid the", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] proposes a multi-step training", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "al [17] studied the energy and accuracy tradeoffs by removing neural connections based on error propagation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "For this study, we show two subbands, high and low, generated by Laplacian and Gaussian filters [18].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Much like the pyramid structure in [18], we choose a subsampled representation of the Gaussian at half resolution.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "We achieve this using stochastic rounding [11] to trim the weights.", "startOffset": 42, "endOffset": 46}], "year": 2016, "abstractText": "Large-scale deep neural networks (DNN) have been successfully used in a number of tasks from image recognition to natural language processing. They are trained using large training sets on large models, making them computationally and memory intensive. As such, there is much interest in research development for faster training and test time. In this paper, we present a unique approach using lower precision weights for more efficient and faster training phase. We separate imagery into different frequency bands (e.g. with different information content) such that the neural net can better learn using less bits. We present this approach as a complement existing methods such as pruning network connections and encoding learning weights. We show results where this approach supports more stable learning with 2-4X reduction in precision with 17X reduction in DNN parameters.", "creator": "Word"}}}