{"id": "1703.08769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Open Vocabulary Scene Parsing", "abstract": "Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.", "histories": [["v1", "Sun, 26 Mar 2017 05:44:56 GMT  (5445kb,D)", "https://arxiv.org/abs/1703.08769v1", null], ["v2", "Tue, 4 Apr 2017 18:28:20 GMT  (8759kb,D)", "http://arxiv.org/abs/1703.08769v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hang zhao", "xavier puig", "bolei zhou", "sanja fidler", "antonio torralba"], "accepted": false, "id": "1703.08769"}, "pdf": {"name": "1703.08769.pdf", "metadata": {"source": "CRF", "title": "Open Vocabulary Scene Parsing", "authors": ["Hang Zhao", "Xavier Puig", "Bolei Zhou", "Sanja Fidler", "Antonio Torralba"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "1.1. Related work", "text": "Our work relates to various topics in the literature, which we briefly review below.Semantic segmentation and scene analysis. Due to the amazing power of deep learning, especially CNNs [14], pixel-by-pixel dense labeling has attracted a lot of attention. Existing work includes fully revolutionary neural networks (FCN) [17], deconvolutionary neural networks [20], encoder-decoder SegNet [2], dilated neural networks [3, 32], etc. These networks work well on datasets such as PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes and a recently published benchmark SceneParse150 [34], which cover 150 most common daily objects. However, these models are not easily adaptable to new objects."}, {"heading": "2. Learning joint embeddings for pixel features", "text": "We treat parsing an open scene as a retrieval problem for each pixel, according to the ideas of retrieval work with captions. [29] Our goal is to embed pixel characteristics and word concepts in a common high-dimensional positive vector space RN +, as shown in Figure 2. The guiding principle in the construction of the common embedding space is that image characteristics should be close to their concept designations, and word concepts should maintain their semantic hypernyme / hyponym relationships. In this embedding space, vectors near the origin are general concepts, and vectors with larger standards represent a higher specificity; (2) Hypernyme / hyponym relationship is defined as whether a vector is smaller / larger than another vector in all N dimensions. A hypernyme scoring function is crucial for the construction of this embedding space, which in Section 2.1.3 provides an overview of our suggested concept system that is embedded in two:"}, {"heading": "2.1. Scoring functions", "text": "For each of the streams, the goal is to maximize the number of points of matching pairs and to minimize the number of points of non-matching pairs. Therefore, the choice of the scoring functions S (x, y) becomes important. There are symmetrical scoring functions such as Lp distance and cosinal similarity that are widely used in the embedding tasks: SLp (x, y) = \u2212 x \u2212 y, p, Scos (x, y) = x \u00b7 y. (1) To reveal the asymmetrical hypernym / hyponym relationships between word concepts, the hypernym scoring function [29] is indispensable, Shyper (x, y) = \u2212 x max (0, x \u2212 y), p. (2) If x is a hypernym of y (x y), then ideally all coordinates are smaller than Shy (x), thus totally different from Sy (x)."}, {"heading": "2.2. Concept stream", "text": "The purpose of the concept stream is to build semantic relationships in the embedding space. In our case, the semantic structure arises from WordNet hypernym / hyponym relationships. Consider all vocabulary concepts as directed acyclic graphs (DAG) H = (V, E), which share a common root of V \"entity,\" any node in graph V \"V\" can be an abstract concept, such as the union of its children nodes or a specific class as sheet. A visualization of a part of the DAG, which we have created on the basis of Wordnet and ADE20K labels, can be found in supplementary materials. Intern, the concept stream includes parallel layers of a common portable reference table, which includes the word concepts u, v to f (u), f (v), f (v), f (v), f (v), v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v (), v (), u, () (), u, (), u, (), (), v, v, v, v, v, v, v, v, v, v (), v, v, v, v, v, v, v, v, v, v, v (), v, v, v, v, v (), v, v, v, v, v, v, v, v, v (), v, v, v, v (), v (), v, v, v, v ()."}, {"heading": "2.3. Image stream", "text": "The image stream consists of a fully revolutionary network, commonly used in image segmentation tasks, and a lookup layer that is shared with the word concept stream. Let's look at an image pixel at position (i, j) with the caption xi, j, whose feature yi, j is the output of the top layer of the Convolutionary Network. Our mapping function g (yi, j) embeds the pixel features in the same space as their labels f (xi, j) and then rates them with a scoring function Simage (f (xi, j), g (yi, j)). Since retrieving labels per se is a ranking problem, negative labels x-i, j are introduced in training. A maximum ranking loss is often used [9] to make the scores of true labels larger than negative labels per border, Limage (yi, j) = x-labels, jmax {0, sim \u2212 g (etxel), j (yxel), j (etx), (etxel)."}, {"heading": "2.4. Joint model", "text": "Our common model combines the two streams via a common loss function to maintain the conceptual hierarchy and visual feature similarities. In particular, we simply weighted the losses of two streams L = Limage + \u03bbLconcept (\u03bb = 5) during training. We set the embedding space dimension to N = 300, which is commonly used in Word embedding. Training and model details are described in Section 4.2."}, {"heading": "3. Evaluation Criteria", "text": "In order to better evaluate our models, this section examines metrics for various parsing tasks."}, {"heading": "3.1. Baseline flat metrics", "text": "While we are working on a limited number of classes, four traditional criteria are good metrics for the performance of the scene analysis model: (1) pixel-by-pixel accuracy: the proportion of correctly classified pixels; (2) average accuracy: the proportion of correctly classified pixels averaged across all classes; (3) average IoU: the overlap between predictions and basic truth averaged across all classes; (4) weighted IoU: the IoU, weighted by the pixel ratio of each class."}, {"heading": "3.2. Open vocabulary metrics", "text": "Given the nature of open vocabulary recognition, choosing a good evaluation criterion is not trivial. First, it should use the graphical structure of the concepts to detect the distance of the predicted class from the basic truth. Second, the evaluation should accurately reflect the highly unbalanced distribution of the dataset classes, which are also common in the objects seen in nature. To do this, a score s (l, p) is used for each sample / pixel to measure the similarity between the basic truth label s and the prediction p. Overall accuracy is the mean of all samples / pixels."}, {"heading": "3.2.1 Hierarchical precision, recall and F-score", "text": "Hierarchical precision, memory and F score were also known as the Wu-Palmer similarity, which was originally used for lexical selection [30]. For two given concepts l and p, we define the lowest common ancestor LCA as the most specific concept (i.e. farthest from the root), which is a hypernym of both. Then, hierarchical precision and memory are defined by the number of common hypernyms that have prediction and designation above the vocabulary hierarchy H. formally: sHP (l, p) = dLCA dp, sHR (l, p) = dLCA dl. (5), with the depth of the lowest common ancestor node dLCA being the number of hypernyms in the community. Combining hierarchical precision and hierarchical memory, one gets hierarchical F score sHF (l, p). \""}, {"heading": "3.2.2 Information content ratio", "text": "As already mentioned, an unbalanced distribution of the data points could allow performance to be dominated by frequent classes. The information content ratio, which was also used in the lexical search, effectively addresses these problems. According to the information theory and statistics, the information content of a message is the reverse logarithm of its frequency I (c) = \u2212 logP (c). We inherit this idea and prepare our image data to obtain the pixel frequency of each concept v \u0394H. Specifically, the frequency of a concept is the sum of its own frequency and the frequency of all its descendants in the image record. It is expected that the root \"entity\" has frequency 1.0 and information content. During the evaluations, we measure for each test sample how much information our prediction receives from the total amount of information in the label. Thus, the end result is determined by the information of the eco-balance and the basic truth and predicted concepts."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Image label and concept association", "text": "To learn how to embed them together, we associate each class in the ADE20K dataset with a synset in WordNet, which is a unique concept; the process of linking data requires semantic understanding, so we resort to Amazon Mechanical Turk (AMT). We develop a rigorous annotation protocol that is incorporated into supplementary materials.After associating, we get 3019 classes in the dataset with synset matches, of which there will be unique synsets that form a DAG in 2019. All synsets combined have entity.n.01 as the top hypernym and there are an average of 8.2 synsets in between. The depth of ADE20K dataset annotations ranges from 4 to 19."}, {"heading": "4.2. Network implementations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Concept stream", "text": "The data layer of the concept stream feeds the network with positive and negative vocabulary concept pairs. The positive training pairs are found by traversing diagram H and find all transitive closure hypernym pairs, e.g. \"neck dress\" and \"tie,\" \"clothing\" and \"tie,\" \"unit\" and \"tie\"; negative samples are randomly generated before each training epoch by excluding these positive samples."}, {"heading": "4.2.2 Image stream", "text": "Considering the characteristics of an image pixel from the last layer of the fully twisted network fc7 with dimension 4096, we add a 1 x 1 twisted layer g (\u00b7) with weight dimension 4096 x 300 to embed the pixel feature. To ensure the positivity, we add a ReLU layer. One technique we use to improve the training is to set the standards for embedding image pixels to 30, where a wide range of values will work. This technique stabilizes the training numerically and accelerates convergence. Intuitively, fixing the image to a large standard makes sense in the hierarchical embedding of space: image pixels are the most specific descriptions of concepts, while words are more general and closer to the source."}, {"heading": "4.2.3 Training and inference", "text": "In all experiments, we first train the concept stream to get the word embedding, and then use it as initializations in joint training. Pre-trained weights from VGG-ImageNet [27] are used as initializations for the image flow.Adam Optimizer [13] with learning rate 1e-3 is used to update weights throughout the model.The loss margin is by default \u03b1 = 1.0.In the inference phase, there are two cases: (1) While we test the 150 training classes, the pixel embedding is compared with the embedding of all 150 applicant labels based on the scoring function, with the class with the highest score as a prediction; (2) While we make zero-hot predictions on the other side, we use a threshold on the values to decide the cutoff score, concepts with values above the cutoff score are used as a prediction. This best threshold is found by a set of 100 validators."}, {"heading": "4.3. Results on SceneParse150", "text": "In this section we report on the performance of our model in the scene parsing task. Training is conducted on the most common 150 classes of substances and objects in the ADE20K dataset, SceneParse150, where each of the classes has at least 0.02% of the total pixels in the datasets. We have trained some models in the references and several variants of our proposed model, all of which share the same architecture of the revolutionary networks to make fair comparisons. Softmax is the base model that makes classic multi-class classification. Conditional Softmax is a hierarchical classification model proposed in [24]. It builds a tree on the label relationships, and softmax is performed only between the nodes of a common parent, so that only conditional probabilities for each node are compressed. To obtain absolute probabilities during testing, the conditional probabilities following the paths to root.Word2Vec is a model that simply makes the image pixelated."}, {"heading": "4.3.1 On the training classes", "text": "Based on the 150 training classes, our proposed models provide competitive results. To compare performance, flat baseline metrics are used, as shown in Table 1. The best performance is unsurprisingly achieved by the Softmax baseline, which agrees with the observation from [9], classification formulations usually achieve greater accuracy than regression formulations. At the same time, our proposed models Joint-Cosine and Word2Vec + lag only about 1% behind Softmax, which is an affordable sacrifice given the zero-shot predictability and interpretability discussed later. Visual results of the best-proposed model Joint-Cosine are shown in Figure 4."}, {"heading": "4.3.2 Zero-shot predictions", "text": "We move to zero-shot prediction tasks to fully exploit the hierarchical predictive power of our models, which are evaluated on 500 less common object classes in the ADE20K dataset. Predictions can be in the 500 classes, or their hyperactivity, based on our open vocabulary."}, {"heading": "4.4. Diversity test", "text": "To answer this question, we run a diversity test in this section. In contrast to the previous experiments, we do not take the most common classes for training, but we consistently take samples for training and test classes from the histogram of pixel numbers. For better comparison, we set the number of zero-shot test set classes to 500, and the training classes range from 50 to 1500. In the training process, we compensate for the imbalance of pixel numbers by weighting the loss of the training class with the corresponding information content, so that the rarer classes contribute to higher losses. We are only experimenting with our best model Joint-Hyper for this diversity test. The results in Figure 6 suggest that performance after training may be saturated with more than 500 classes. We suspect that training with many classes with few instances could lead to sample noise. To further improve performance, higher quality data is required."}, {"heading": "5. Interpreting the embedding space", "text": "In this section, we perform three tests to explore them. In our framework, joint training does not require all the concepts to have the corresponding image data. Heatmaps are the Query Image Mapmax (\"Play Equipment,\" \"Table\") and the search for concepts that are not equipped with images. In Figure 7, we show some pixelated concept results. The heat maps are the Query Image Mapmax (\"Play Equipment,\" \"Canopy\")."}, {"heading": "6. Conclusion", "text": "We presented a new challenging task: parsing scenes with open vocabulary, which aims to analyze images in the wild; and we proposed a framework to solve this problem by embedding concepts from a knowledge diagram and image pixel features in a common vector space in which the semantic hierarchy is maintained; we demonstrated that our model works well when parsing open vocabulary; and we continued to examine the semantics learned in the embedding space. Credit: This work was supported by Samsung and the NSF grant number 1524817 to AT. SF. BZ is supported by the Facebook Fellowship."}, {"heading": "1. Data association protocol", "text": "To learn how to embed images and word concepts together, we need to expand the ADE20K dataset by adding information about how the label classes (> 3000) are semantically related to each other. We associate each class in the ADE20K dataset with a synset in WordNet, which is a unique concept.The data mapping process requires semantic understanding, so we resort to Amazon Mechanical Turk (AMT).The annotation protocol is detailed as follows, and screenshots of our AMT interface are shown in Figure 10. We look for each class in the dataset, for all synsets with the same name. We find 3 different cases: (1) a single synset is found for the given class; (2) multiple synsets are found due to polysemia; (3) no synets are found, either because the correct synthesis has a different name or because this concept is not in WordNet.In the first case, we automatically match a class in the dataset with the synset held."}, {"heading": "2. Concept graph", "text": "After the data association we get 3019 classes in the dataset with synset matches. 2019 are unique synsets that form a DAG. All matching synsets have entity.n.01 as the uppermost hypernym and there are an average of 8.2 synsets in between. The depth of the ADE20K annotations of the dataset ranges from 4 to 19. A detailed visualization of the created conceptual graph is shown in Figure 11. The node radii show the class frequencies in the ADE20K dataset. The figure shows only a part of the complete graph, nodes with 5 descendants or less have been hidden."}, {"heading": "3. Full zero-shot prediction lists", "text": "Our model gives each sample a list of predictions in hierarchical order. Due to the side constraints, the full prediction lists are not shown in the main work. In Figure 12, we give details of the zero-shot predictions, both the basic truth and the prediction lists are shown in the texts below the images. Correct predictions are marked in green, inconsistent points are marked in orange. It is evident that for hard examples, such as \"domes\" (row 1, column 3), a general and conservative prediction is made; if the sample is simple and resembles training samples, such as \"wagons\" (row 1, column 1), our model gives specific and aggressive predictions."}], "references": [{"title": "Labelembedding for attribute-based classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv:1511.00561,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv:1606.00915,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "The cityscapes dataset", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Scharw\u00e4chter", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR Workshop on The Future of Datasets in Vision,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale object classification using label relation graphs", "author": ["J. Deng", "N. Ding", "Y. Jia", "A. Frome", "K. Murphy", "S. Bengio", "Y. Li", "H. Neven", "H. Adam"], "venue": "European Conference on Computer Vision, pages 48\u201364. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition", "author": ["J. Deng", "J. Krause", "A.C. Berg", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3450\u20133457. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "Int\u2019l Journal of Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Large-scale knowledge transfer for object localization in imagenet", "author": ["M. Guillaumin", "V. Ferrari"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3202\u20133209. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep metric learning using triplet network", "author": ["E. Hoffer", "N. Ailon"], "venue": "International Workshop on Similarity-Based Pattern Recognition, pages 84\u201392. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u2013 3137,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(3):453\u2013465,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions", "author": ["J. Lei Ba", "K. Swersky", "S. Fidler"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM, 38(11):39\u201341,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proc. ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1312.5650,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "From large scale image categorization to entry-level categories", "author": ["V. Ordonez", "J. Deng", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2768\u20132775,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 503\u2013 510. IEEE,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Yolo9000: Better, faster, stronger", "author": ["J. Redmon", "A. Farhadi"], "venue": "arXiv preprint arXiv:1612.08242,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1641\u20131648. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "What helps where\u2013and why? semantic relatedness for knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "I. Gurevych", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 910\u2013917. IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "Advances in neural information processing systems, pages 935\u2013943,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Order-embeddings of images and language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133\u2013138. Association for Computational Linguistics,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1994}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 3485\u20133492. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in neural information processing systems, pages 487\u2013495,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic understanding of scenes through the ade20k dataset", "author": ["B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba"], "venue": "arXiv preprint arXiv:1608.05442,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting object hierarchy: Combining models from different category levels", "author": ["A. Zweig", "D. Weinshall"], "venue": "2007 IEEE 11th International Conference on Computer Vision, pages 1\u20138. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 145, "endOffset": 156}, {"referenceID": 30, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 145, "endOffset": 156}, {"referenceID": 32, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 145, "endOffset": 156}, {"referenceID": 11, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "Our framework incorporates hypernym/hyponym relations from WordNet [19] to help parsing.", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "The open vocabulary parsing ability of the proposed framework is evaluated on the recent ADE20K dataset [34].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "Due to astonishing performance of deep learning, in particular CNNs [14], pixel-wise dense labeling has received significant amount of attention.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 157, "endOffset": 164}, {"referenceID": 31, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 157, "endOffset": 164}, {"referenceID": 7, "context": "These networks perform well on datasets like PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes, and a recently released benchmark SceneParse150 [34] covering 150 most frequent daily objects.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "These networks perform well on datasets like PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes, and a recently released benchmark SceneParse150 [34] covering 150 most frequent daily objects.", "startOffset": 98, "endOffset": 101}, {"referenceID": 33, "context": "These networks perform well on datasets like PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes, and a recently released benchmark SceneParse150 [34] covering 150 most frequent daily objects.", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "Zero-shot learning addresses knowledge transfer and generalization [25, 10].", "startOffset": 67, "endOffset": 75}, {"referenceID": 9, "context": "Zero-shot learning addresses knowledge transfer and generalization [25, 10].", "startOffset": 67, "endOffset": 75}, {"referenceID": 25, "context": "Rohrbach [26] introduced the idea to transfer large-scale linguistic knowledge into vision tasks.", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "[28] and Frome et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] directly embedded visual features into the word vector space so that visual similarities are connected to semantic similarities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21] used a convex combination of visual features of training classes to represent new categories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 0, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 15, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 14, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 34, "context": "[35] combined classifiers on different levels to help improve classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] achieved hierarchical imagelevel classification by trading off accuracy and gain as an optimization problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22], on the other hand, proposed to make entry-level predictions when dealEntity Physical entity Surface", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] formulated a label relation graph that could be directly integrated with deep neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "Our approach on hierarchical parsing is inspired by the order-embeddings work [29], we attempt to construct an asymmetric embedding space, so that both image features and hierarchical information in the knowledge graph are effectively and implicitly encoded by the deep neural networks.", "startOffset": 78, "endOffset": 82}, {"referenceID": 28, "context": "We treat open-ended scene parsing as a retrieval problem for each pixel, following the ideas of image-caption retrieval work [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "In order to reveal the asymmetric hypernym/hyponym relations between word concepts, hypernym scoring function [29] is indispensable,", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "A max-margin ranking loss is commonly used [9] to encourage the scores of true labels be larger than negative labels by a margin,", "startOffset": 43, "endOffset": 46}, {"referenceID": 10, "context": "(4) This loss function is a variation of triplet ranking loss proposed in [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "Hierarchical precision, recall and F-score were also known as Wu-Palmer similarity, which was originally used for lexical selection [30].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Our core CNN in the image stream is adapted from VGG-16 by taking away pool4 and pool5 and then making all the following convolution layers dilated (or Atrous) [3, 32].", "startOffset": 160, "endOffset": 167}, {"referenceID": 31, "context": "Our core CNN in the image stream is adapted from VGG-16 by taking away pool4 and pool5 and then making all the following convolution layers dilated (or Atrous) [3, 32].", "startOffset": 160, "endOffset": 167}, {"referenceID": 33, "context": "Softmax [34] 73.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "6014 Conditional Softmax [24] 72.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "5942 Word2Vec [9] 71.", "startOffset": 14, "endOffset": 17}, {"referenceID": 26, "context": "VGG-ImageNet [27] are used as initializations for the image stream.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "Adam optimizer [13] with learning rate 1e-3 is used to update weights across the model.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "Conditional Softmax is a hierarchical classification model proposed in [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "This model is a direct counterpart of DeViSe[9] in our scene parsing settings.", "startOffset": 44, "endOffset": 47}, {"referenceID": 17, "context": "Then we finetune the pre-trained word vectors with skip-gram model [18] for 5 epochs, and these word vectors are finally fixed for regression like Word2Vec.", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "Without surprise, the best performance is achieved by the Softmax baseline, which agrees with the observation from [9], classification formulations usually achieves higher accuracy than regression formulations.", "startOffset": 115, "endOffset": 118}, {"referenceID": 20, "context": "Convex Combination [21] is another baseline model for comparison: we take the probability output from Softmax within the 150 classes, to form new embeddings in the word", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "Softmax [34] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "1632 Conditional Softmax [24] 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "1657 Word2Vec [9] 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 20, "context": "1794 Convex Combination [21] 0.", "startOffset": 24, "endOffset": 28}], "year": 2017, "abstractText": "Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.", "creator": "LaTeX with hyperref package"}}}