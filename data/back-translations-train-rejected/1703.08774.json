{"id": "1703.08774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Who Said What: Modeling Individual Labelers Improves Classification", "abstract": "Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.", "histories": [["v1", "Sun, 26 Mar 2017 06:34:45 GMT  (3896kb,D)", "http://arxiv.org/abs/1703.08774v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["melody y guan", "varun gulshan", "rew m dai", "geoffrey e hinton"], "accepted": false, "id": "1703.08774"}, "pdf": {"name": "1703.08774.pdf", "metadata": {"source": "META", "title": "Who Said What: Modeling Individual Labelers Improves Classification", "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geo\u0082rey E. Hinton"], "emails": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geointon@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, deep revolutionary neural networks have led to rapid improvements in computers \"ability to classify objects in images, and they are now comparable to human performance in several areas. As computers become faster and researchers develop even better techniques, neural networks will continue to improve, especially for tasks where it is possible to obtain a very large number of accurately labeled training examples. In the near future, we can expect neural networks to serve as alternatives to human experts. In fact, we would like neural networks to function much better than the human experts used to provide the training labels because these training labels are unreliable, such as the poor match between experienced experts (55.4% for the data sets we are looking at) or even between an expert and the same expert viewing the same image some time later (70.7%).1 Intuitively, we would expect the quality of the training labels to work as part of the Google Brain Residency program (residency) to be intuitive."}, {"heading": "2 MOTIVATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Beating the teacher", "text": "To show that a trained neural network can be far better than its teacher, we use the well-known MNIST benchmark, for which the true names are known, and we create unreliable training labels by corrupting the true names. Corruption is done only once per experiment before training begins, so the noise caused by corruption cannot be averaged by training on the same example. MNIST has 60,000 training images and 10,000 test images of isolated, standardized, hand-wri en-digits and the task is to classify the image into one of ten classes. Each image has 28 x 28 pixels. For the purposes of this demonstration, we used a very simple neural network containing two hidden revolutionary layers, each with 1,024 reflected linear units and 64 patches followed by a fully connected hidden layer of 32 reflected linear units."}, {"heading": "2.2 Making better use of noisy labels", "text": "We are interested in data sets of medical images in which many different doctors have provided labels, but each image has only been labeled by a few doctors and most doctors have labeled only a relatively small fraction of the images. We expect that some doctors will be more reliable than others, and we want to give more weight to their opinions. We also expect that doctors have received proper training and that they have experienced distributions of images, so the relative reliability of two doctors will depend on both the class of image and the characteristics of the image with which it was taken. In this paper we focus on data sets of images used for screening, diabetic retinopathy, because neural networks have recently achieved human performance on such images."}, {"heading": "2.3 Diabetic retinopathy classi cation", "text": "In recent years, it has been shown that the number of patients who are able to beat their disease is very high. (...) In recent years, the number of patients who are able to beat the disease has multiplied. (...) The number of patients who are able to beat the disease has multiplied. (...) The number of patients who are able to beat the disease has multiplied. (...) The number of patients who are able to beat the disease is very high. (...) The number of patients who are able to beat the disease is very high. (...) The number of patients who are able to detect the disease is very high. (...) The number of patients who are able to detect the disease is very high. (...) The number of patients who are able to detect the disease is very high. (...)"}, {"heading": "3 METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model Architecture", "text": "We have developed a series of models of increasing complexity for the development of diabetic retinopathy, which we have chosen (Figure 5)."}, {"heading": "3.2 Estimating doctor reliability with EM", "text": "Since the basic work of Dawid and Skene [3], who model annotator accuracies with expectation maximization (EM), and Smyth et al. [21], which incorporate the opinions of many experts to derive the truth of the soil, there has been a great deal of work with EM approaches to estimate precise labels for data sets commented on by several experts [18, 19, 25]. Substitute for this, Welinder and Perona [24] use an online EM algorithm to estimate the capabilities of several noisy annotators and determine the most likely value of the labels. We calculated updated labels by executing Welinder and Perona [24] on the basis of our human physicians and using these updated labels to train BN as a competing algorithm for our DN method. Welinder and Perona [24] also actively select which images we label and how many labels we request based on these desired values, and based on their uncertainty for the degree of veracity they will all be used."}, {"heading": "3.3 Modeling label noise", "text": "Mnih and Hinton [15] describe a deep neural network that learns to mark road pixels in aerial photographs so that the target markings are unreliable. To deal with this label noise, Mnih and Hinton [15] propose a robust loss function, the asymmetric outlet noise.ey assume that a true, unobserved label m is generated from a wm \u00b7 wm field of view s according to some distribution points p (m | s), and the corrupted, observed label m is then generated according to an asymmetric outlet noise.ey assume that a true, unobserved label m is generated from a wm \u00b7 wm field of view rst (m \u00b7 wm). e authors assume an asymmetric binary sound distribution p (m \u00b2)."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Neural network training", "text": "In fact, it is as if most of us are able to play by the rules that they have made in the past, and as if they were able to play by the rules that they have made in the past. (...) It is not as if they play by the rules. (...) It is not as if they play by the rules. (...) It is as if they play by the rules. (...) It is not as if they play by the rules. (...) It is not as if they play by the rules. (...) It is as if they play by the rules. (...) It is as if they play by the rules. (...) It is not as if they play by the rules. (...) It is as if they play by the rules. (...) It is as if they are able to play by the rules. (...) It is as if they play by the rules. (...) It is as if they play by the rules."}, {"heading": "4.2 Datasets", "text": "The training dataset consists of 126,522 images of patients screening for diabetic retinopathy at sites managed by 4 different clinical partners: EyePACS, Aravind Eye Care, Sankara Nethralaya and Narayana Nethralaya. e Validation dataset consists of 7,804 images from EyePACS clinics. Our test dataset consists of 3,547 images from the EyePACS-1 and Messidor-2 datasets. Further details on image acquisition can be found in Appendix E. Each of the images in the training and validation datasets has been evaluated by at least one of 54 US licensed ophthalmologists or ophthalmologists who were trained in their last year of residency (postgraduate year 4). To train the physician models, we used the datasets of the 30 ophthalmologists who graded at least 1,000 images, and we placed the remaining physicians as a single composite doctor to avoid a pot."}, {"heading": "4.3 Our baseline vs published baseline", "text": "The section \"Diagnostics\" describes several ways in which our basic data should be used from those of Gulshan et al. [10] For these reasons, the results from the own BN should be used for model comparisons with DN, WN, IWDN and BIWDN and not numbers from Gulshan et al. [10]. \u2022 Unlike in Gulshan et al. [10], we remove grades from doctors who have graded test cartridge images from training and validation kits to reduce the likelihood that the model will be overtaken by certain experts."}, {"heading": "5 SUMMARY OF RESULTS", "text": "We performed 10 replicas of each model and calculated the resulting ratios, which are shown in Table 5. For full comparability of the models, we used the same 10 replicas reported for DN to serve as a fixed part of the model for training the WDN-, IWDN- and BIWDN-replicates."}, {"heading": "5.1 Training with ve-class loss beats training with binary loss even on binary metrics", "text": "We found that training BN with a 5-class loss improves the binary AUC in the test compared to training with a binary loss, as Gulshan et al. [10], even if the former is validated for 5-class training errors instead of binary AUC (Table 4). The binary AUC in the test was increased by 1.53% from 95.58% from the use of multi-class loss. Intuitively, this coincides with our thesis that generalization is improved by increasing the amount of information in the desired results. All results obtained in Table 5 and subsequent sections are from training with 5-class loss."}, {"heading": "2 24.75 17.62", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 51.03 72.69", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2 Averaging modeled doctors beats modeling the average doctor", "text": "We observed a 1.97% decrease in the 5-grade classification test from 23.83% (8.27% relative decrease) as the average value of the modeled physician (DN) was determined instead of the modeling of the average physician (BN). In comparison, the use of labels calculated with EM to train BN only reduced the 5-grade test class error by 0.09% (0.38% relative decrease). In comparison, DN increased the binary AUC by 0.17% from 97.11%, the error of the binary classes by 0.17% from 9.92%, and the sensitivity of the specifications by 97% (spec @ 97% sens) by 2.21% from 79.60%. Meanwhile, the use of labels calculated with EM on BN only increased the specification @ 97% by 0.37% compared to vanilla BN and actually resulted in slightly worse performance in binary AUC (+ 0.20%) and 0.11% (metric)."}, {"heading": "5.3 Learning averaging weights helps", "text": "We saw a further 1.28% reduction in 5-class error compared to BN through the use of WDN compared to DN (5.37% additional relative decrease); binary AUC increased by a further 0.17%; binary classisi cation error decreased by a further 0.68%; and the 97% sens specification increased by an additional 0.88%, all on test data; the results of IWDN and BIWDN were slightly worse than those of WDN. We would expect a greater improvement in WDN and potentially further improvements through training for image averaging of logits if we had physicians with more diverse capabilities and greater environmental impacts, but for the dataset we did not use image-specific c averaging of logits. Our enhancement of Mnih and Hinton [15] s competing algorithm actually resulted in DN performing 0.90% worse on 5-class test errors (3.78% less technical and also relative)."}, {"heading": "6 CONCLUSIONS", "text": "We are introducing a method to leverage noisy labels more effectively when each example is labeled by a subset of a larger pool of experts. Our method learns from the identity of several noisy annotators by individually modeling them with a common neural network that provides separate outputs for each expert, and then learning averages for combining their modeled predictions. We evaluate our method of diagnosing the severity of diabetic retinopathy on the 5-point scale from images of the retina. Compared with our baseline model of training based on average physician's opinion, a strategy that delivered cutting-edge results in the automated diagnosis of DR, our method can lower the 5-class classification error from 23.83% to 20.58%, corresponding to a relative reduction of 13.6%. We also found that training with a 5-class loss signal is significantly better than training with binary loss, as was the case in the published baseline."}, {"heading": "A CODE", "text": "The TensorFlow code used in this paper will be made publicly available shortly."}, {"heading": "B MUTUAL INFORMATION FOR NOISY LABELS", "text": "Here we calculate the mutual information between a loud MNIST label and the truth, using random noise to estimate the number of noisy labeled training cases that correspond to a case that is known to be correctly labeled. Empirically, N perfectly labeled training cases result in roughly the same test error as NIperfect / Inoisy training cases with loud labels, where Inoisy is the mutual information per case between a loud label and the truth, and Iperfect is the corresponding mutual information for perfect labels. For ten classes, the mutual information (in mosquitoes) is Iperfect = 2.3 = \u2212 noisy (0.1), but if the loud label is on average 20% correct, the mutual information is: Inoisy = 0.044 = \u2212 noisy (0.1) \u2212 10 x 0.02 \u00d7 noisy (0.1 0.02) \u2212 90 \u00d7 0.1 \u00d7 0.89 noisy (0.1 \u00d7 0.8 / 9), so if we need the similar information in 0000, we can use the mutual information in the 000 000 000 000 labels, which is 0.0000."}, {"heading": "C OTHER IDEAS TESTED C.1 Mean Class Balancing", "text": "In addition to the previous correction of class distributions, we also have an empirical mean distribution, where examples from less common classes are upweighted and more common classes are downweighted in cross-sectional entropy loss, in inverse proportion to their prevalence in terms of even distribution between classes. We explicitly weighted each example of class c by: \u03b1c = q q q (c) = 1 | c | q (c), Eigen and Fergus. [5] We use a similar method for computer vision tasks, although they use averages. In our case, the middle class alignment is lowered, possibly because it has made too many assumptions about the hidden test distribution, and has not been employed.C.2 Alternative target distribution for training mediocre logits, we took each training case and used the opinions of the physicians who actually designated this case to decipher target performance distribution."}, {"heading": "D HYPERPARAMETER TUNING", "text": "For the MNIST experiment, we used the standard parameters of the Adam optimizer \u03b21 = 0.9, \u03b22 = 0.999, and \u0430 = 1 \u00d7 10 \u2212 8. We examined the learning rates in the set {0.00003, 0.0000001, 0.00003,.., 0.00003} and the standard deviations of the initial random normal weights in the set {0.0001, 0.0003,., 0.01} and found optimal values of 0.00003 for the former and 0.0001 for the latter. For the computer-assisted diagnosis of DR, we performed a grid search on the following hyperparameter spaces: failure rates for the conception spine, {0.5, 0.55, 0.0001}."}, {"heading": "E DATASET DETAILS", "text": "119,589 of our training datasets correspond to those used in the training dataset of Gulshan et al. [10] (which consists of 128,175 images). E images removed from the training dataset used by Gulshan et al. [10] are listed in detail here: (i) 4,204 of the 128,175 images were removed to create a separate validation dataset for experiments within the research group. (ii) 4,265 of the 128,175 images were excluded because they were deemed unusable by any ophthalmologist who graded them. In contrast to Gulshan et al. [10] we do not predict the gradability of the images in this work and therefore exclude these images. (iii) 117 of the 128, 175 images fail at our image scale preprocessing stage and have also been excluded. We also acquired 6,933 additional labeled images since the creation of the training dataset in Gulshan et al. [10] and added them."}, {"heading": "F MORE DIFFERENCES FROM PUBLISHED BASELINE", "text": "Here we list differences between BN and the model in Gulshan et al. [10] that are not mentioned in Section 4.3. Gulshan et al. [10] interpret referable diabetic retinopathy as the presence of moderate and worse diabetic retinopathy or referable diabetic macular edema, while ignoring information about la era. ey also reported only binary (referable / non-referable) classisi cation measurements, while we reported both binary and 5-classed classisi cation measurements. Finally, we did not group replicas Gulshan et al. [10] because we focused on comparing different methods for using the labels, rather than squeezing the final performance drop of a method."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Dale Webster, Lily Peng, Jonathan Krause, Arunachalam Narayanaswamy, oc Le, Alexey Kurakin, Anelia Angelova, Nathan Silberman, George Dahl, Brian Cheung, Anna Goldie, David Ha, Ma Ho man, Olga Wichrowska, Justin Gilmer, Denny Britz, Mohammad Norouzi and Luke Metz for helpful discussions and feedback. Is work was supported by the Google Brain Residency Program, for which we thank Leslie Phillips, Samy Bengio and Je Dean in particular."}], "references": [{"title": "TensorFlow: A System for Large-Scale Machine Learning", "author": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard", "M. Kudlur", "J. Levenberg", "R. Monga", "S. Moore", "D.G. Murray", "B. Steiner", "P. Tucker", "V. Vasudevan", "P. Warden", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Automated analysis of retinal images for detection of referable diabetic retinopathy", "author": ["M.D. Abr\u00e0mo", "J.C. Folk", "D.P. Han", "J.D. Walker", "D.F. Williams", "S.R. Russell", "P. Massin", "B. Cochener", "L. Tang P. Gain", "M. Lamard", "D.C. Moga", "G. \u008bellec", "M. Niemeijer"], "venue": "JAMA Ophthalmol. 131,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied Statistics 28,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1979}, {"title": "Feedback on a Publicly Distributed Image Database: \u008ce", "author": ["E. Decenci\u00e8re", "Z. Xiwei", "G. Cazuguel", "B. Lay", "B. Cochener", "C. Trone", "P. Gain", "J.-R. Ord\u00f3 nez Varela", "P. Massin", "A. Erginay", "B. Charton", "J.-C"], "venue": "Kleain", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "ICCV 11,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Onega, A .N", "author": ["J.G. Elmore", "G.M. Longton", "P.A. Carney", "T.B.M. Geller"], "venue": "JAMA 313,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Variability in radiologists\u2019 interpretations of mammograms", "author": ["J.G. Elmore", "C.K. Wells", "C.H. Lee", "D.H. Howard", "A.R. Feinstein"], "venue": "NEJM 331,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs", "author": ["V. Gulshan", "L. Peng", "M. Coram", "M.C. Stumpe", "A. Narayanaswamy D. Wu", "S. Venugopalan", "T K. Widner", "Madams", "J. Cuadros", "R. Kim", "R. Raman", "P.C. Nelson", "J.L. Mega", "D.R. Webster"], "venue": "JAMA 316,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shi\u0089", "author": ["S. Io\u0082e", "C. Szegedy"], "venue": "ICML 37", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "ADAM: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "ICLR (July", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning to Label Aerial Images from Noisy Data", "author": ["V. Mnih", "G.E. Hinton"], "venue": "ICML (July", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Regularizing Neural Networks by Penalizing Con\u0080dent Output Distributions", "author": ["G. Pereyra", "G. Tucker", "L. Kaiser", "G.E. Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Supervised Learning from Multiple Experts: Whom to trust when everyone lies a bit", "author": ["V. Raykar", "S. Yu", "L. Zhao", "A. Jerebko", "C. Florin", "G. Valadez", "L. Bogoni", "L. Moy"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Eliminating spammers and ranking annotators for crowdsourced labeling", "author": ["V.C. Raykar", "S. Yu"], "venue": "tasks. JMLR", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Inferring ground truth from subjective labelling of Venus", "author": ["P. Smyth", "U. Fayyad", "M. Burl", "P. Perona", "P. Baldi"], "venue": "images. NIPS", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "EyeArt: Automated, High-throughput, Image Analysis for Diabetic Retinopathy Screening", "author": ["K. Solanki", "C. Ramachandra", "S. Bhat", "M. Bhaskaranand", "M.G. Ni\u008aala", "S.R. Sadda"], "venue": "Invest Ophthalmol Vis Sci 56,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Re-thinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Io\u0082e", "J. Shlens", "Z. Wojna"], "venue": "CVPR (June", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Online crowdsourcing: rating annotators and obtaining cost-e\u0082ective labels", "author": ["P. Welinder", "P. Perona"], "venue": "CVPR Workshop (June", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["J. Whitehill", "P. Ruvolo", "T. Wu", "J. Bergsma", "J. Movellan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "1Inter-grader variability is a well-known issue in many se\u008aings in which human interpretation is used as a proxy for ground truth, such as radiology [7] or pathology [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "1Inter-grader variability is a well-known issue in many se\u008aings in which human interpretation is used as a proxy for ground truth, such as radiology [7] or pathology [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 10, "context": "using stochastic gradient descent on mini-batches of size 200 with the Adam optimizer [14] and we used the remaining 10,000 training images as a validation set to select good values for the learning rate and the magnitude of the initial random weights.", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "In this paper we focus on datasets of images used for screening diabetic retinopathy because neural networks have recently achieved human-level performance on such images [10] and if we can produce even a relatively small improvement in the state-of-the-art system it will be of great value.", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "For a K-way classi\u0080cation task, we can replace the single so\u0089max [9] that is normally used by as many di\u0082erent K-way so\u0089maxes as we have doctors.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "[10] who show a high sensitivity (97.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "On the Messidor-2 dataset [4], Abr\u00e0mo\u0082 et al.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "[2] report a sensitivity of 96.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[22] report a sensitivity of 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "\u008cese were calculated from Welinder and Perona [24]\u2019s expectation-maximization algorithm on our training data.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] (see Section 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Since the foundational work of Dawid and Skene [3], who model annotator accuracies with expectation-maximization (EM), and Smyth et al.", "startOffset": 47, "endOffset": 50}, {"referenceID": 15, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 194, "endOffset": 206}, {"referenceID": 14, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 194, "endOffset": 206}, {"referenceID": 19, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 194, "endOffset": 206}, {"referenceID": 18, "context": "Representatively, Welinder and Perona [24] use an online EM algorithm to estimate abilities of multiple noisy annotators and to determine the most likely value of the labels.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "We calculated updated labels by executing Welinder and Perona [24]\u2019s method on our human doctors and used these updated labels to train BN, as a competing algorithm for our DN method.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Welinder and Perona [24] also actively select which images to label and how many labels to request based on the uncertainty of their estimated ground truth values and the desired level of con\u0080dence, and they select and prioritize which annotators to use when requesting labels.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Mnih and Hinton [15] describe a deep neural network that learns to label road pixels in aerial images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "To handle this label noise, Mnih and Hinton [15] propose a robust loss function that models asymmetric omission noise.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Our variant of Mnih and Hinton [15] is an alternative way to improve upon DN to our proposed approach of learning averaging weights.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "\u008ce optimization algorithm used to train the network weights was distributed stochastic gradient descent (SGD) [1] with the Adam optimizer on mini-batches of size 8.", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "To speed up the training, we used batch normalization [13], pre-initialization of our Inception network using weights from the network trained to classify objects in the ImageNet dataset [20], and the following trick: we set the learning rate on the weight matrix producing prediction logits to one-tenth of the learning rate for", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "We prevented over\u0080\u008aing using a combination of L1 and L2 penalties, dropout, and a con\u0080dence penalty [17], which penalizes a model for having an output distribution with low entropy.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10], we remove grades of doctors who graded test set images from training and validation sets to reduce the chance that the model is over\u0080\u008aing on certain experts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] treats these as independent diagnoses while we collapse these multiple diagnoses into a single diagnosis which may be a distribution over classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] (section 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10], even when validating the former on 5-class training error instead of binary AUC (Table 4).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Our extension of Mnih and Hinton [15]\u2019s competing algorithm actually caused DN to perform worse by 0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "[5] employ a similar method for computer vision tasks although they use medians instead of means.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Because the multi-class extension of Mnih and Hinton [15] we tried showed poor results, which we postulated may have been because it was sensitive to di\u0082erences in class distributions between datasets, we considered a di\u0082erent noise model that made less assumptions on the class distribution of the data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "with the real doctor reliability score calculated from the Welinder and Perona [24] algorithm.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "\u008cis method performed slightly worse than the 5-class variant of Mnih and Hinton [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "[10] (which consists of 128,175 images).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] are detailed here: (i) 4,204 out of the 128,175 were removed to create a separate validation dataset for experiments within the research group.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10], we do not predict image gradeability in this work and hence exclude those images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] and added them to this training set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "\u008ce test set consists of 1,748 images of the Messidor-2 dataset [4] and the remaining 2,000 out of the 9,963 images of the EyePACS-1 test dataset used in Gulshan et al.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] that are not mentioned in Section 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10]", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] did because we focused on comparing di\u0082erent methods of using the labels rather than squeezing the last drop of performance from one method.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Data are o\u0089en labeled by many di\u0082erent experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. \u008cis reduces the workload on individual experts and also gives a be\u008aer estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. \u008cese approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-speci\u0080c ways. \u008cis allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs be\u008aer than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work o\u0082ers an innovative approach for dealing with the myriad real-world se\u008aings that use expert opinions to de\u0080ne labels for training.", "creator": "LaTeX with hyperref package"}}}