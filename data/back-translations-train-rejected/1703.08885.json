{"id": "1703.08885", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Question Answering from Unstructured Text by Retrieval and Comprehension", "abstract": "Open domain Question Answering (QA) systems must interact with external knowledge sources, such as web pages, to find relevant information. Information sources like Wikipedia, however, are not well structured and difficult to utilize in comparison with Knowledge Bases (KBs). In this work we present a two-step approach to question answering from unstructured text, consisting of a retrieval step and a comprehension step. For comprehension, we present an RNN based attention model with a novel mixture mechanism for selecting answers from either retrieved articles or a fixed vocabulary. For retrieval we introduce a hand-crafted model and a neural model for ranking relevant articles. We achieve state-of-the-art performance on W IKI M OVIES dataset, reducing the error by 40%. Our experimental results further demonstrate the importance of each of the introduced components.", "histories": [["v1", "Sun, 26 Mar 2017 23:48:06 GMT  (249kb,D)", "http://arxiv.org/abs/1703.08885v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yusuke watanabe", "bhuwan dhingra", "ruslan salakhutdinov"], "accepted": false, "id": "1703.08885"}, "pdf": {"name": "1703.08885.pdf", "metadata": {"source": "CRF", "title": "Question Answering from Unstructured Text by Retrieval and Comprehension", "authors": ["Yusuke Watanabe", "Bhuwan Dhingra", "Ruslan Salakhutdinov"], "emails": ["rsalakhu}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why? Why?.? Why? Why? Why?? Why? Why? Why? Why? Why? Why? Why? Why?? Why? Why? Why???? Why? Why?? Why?? Why?? Why??? Why? Why???????? Why? Why? Why?? Why?.??????? Why? Why? Why? Why???????????? Why? Why???????????????????..? Why? Why? Why?., \"........,\".., \"......,\"............, \".....................................\" The question........ \""}, {"heading": "2 WIKIMOVIES Dataset", "text": "We focus on the WIKIMOVIES1 dataset, proposed by (Miller et al., 2016), which consists of pairs of questions and answers about movies. Some examples are given in Table 1. As a source of knowledge, about 18K articles are also offered from Wikipedia, where each article is about a movie. Since film articles can be very long, we only use the first paragraph of the article, which typically provides a summary of the movie. Formally, the dataset consists of question-answer pairs {(qj, Aj)} Yy = 1 and movie articles {dk} Kk = 1. In addition, the dataset contains a list of units: movie titles, actor names, genres, etc. Answers to all questions are in the entity list. The questions are created by human annotators using these simple questions (Bordes et al., 2015), an existing open domain question that answers the data sets, and the commented answers come from two structured QBQs: B2 and B3 respectively, which are divided into two questions."}, {"heading": "3 Comprehension Model", "text": "Our QA system answers questions in two steps, as shown in Figure 1. The first step is retrieval, 1 http: / / fb.ai / babi 2 http: / / beforethecode.com / projects / omdb / download.aspx 3 http: / / grouplens.org / datasets / movielens / 4Category labels are only for dev / test datasetwhere articles relevant to the question are retrieved. The second step is understanding where the question and retrieved articles are processed to derive answers. In this section, we focus on the understanding model, assuming that relevant articles have already been retrieved and merged into a context document. In the next section, we will explore approaches to retrieving the articles. Miller et al. (2016), which were introduced WIKIMOVIES dataset, uses an improved variant of memory networks called Key-Value Memory Networks."}, {"heading": "3.1 Comprehension model detail", "text": "The question we have asked ourselves is whether we will be able to understand the world, in which we are able to understand the world, in which we are able to understand the world, in which we are able to understand the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, in which we see the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the"}, {"heading": "4 Retrieval Model", "text": "Our QA system answers questions in two steps as shown in Figure 1. A basic approach to searching for relevant articles is to select articles that contain at least one unit that is also included in the question. We identify maximum intervals of words that match entities in questions and articles. Capitalization of words is ignored in this step because some words in the questions are not properly capitalized. We can randomly select M from these (say N) articles. We call this approach (r0). However, for some movie titles, this method retrieves too many articles that actually have nothing to do with questions. For example, there is a movie called \"Love Story\" that accidentally picks up the words \"Love Story.\" This reduces the performance of the comprehension step. Therefore, we describe two other retrieval models - (1) a data-specific, hand-made approach, and (2) a general approach."}, {"heading": "4.1 Hand-Crafted Model (r1)", "text": "If the film title matches an entity in the question, the article will receive a high score as it is most likely relevant. A similar heuristic was used in (Miller et al., 2016). Additionally, the number of matching entities will also be used to rate each article. Top M articles based on these results will be selected for understanding. This handcrafted approach already provides strong performance for the WIKIMOVIES dataset, however, the heuristics for matching article titles may not be suitable for other QA tasks."}, {"heading": "4.2 Learning Model (R2)", "text": "The learning model for retrieving documents is trained by an oracle constructed using remote monitoring. By looking at the labels on the answers in the training set, we can find suitable articles that contain the information requested in the question. For example, the articles in the answer film are the right articles to retrieve. Figure 5 provides an overview of the model that uses a word level attention mechanism (SLA). After collecting the labels, we train a retrieval model to classify a question and a pair of articles as relevant or irrelevant. Figure 5 provides an overview of the model that uses a word level attention mechanism (SLA). Firstly, the question and the article are embedded in vector sequences using the same method as the understanding model. We do not use anonymization here to maintain simplicity. Otherwise, the anonymization process for a potentially large collection of documents would have to be repeated several times."}, {"heading": "5 Experiments", "text": "We evaluate the understanding model on both WIKIMOVIES-FL- and WIKIMOVIES-WE-datasets. Performance is evaluated on the basis of the accuracy of the top hit (single answer) across all possible answers (all entities). This is referred to as hits @ 1-Metric. For the understanding model, we use the embedding dimension 100 and the GRU-Dimension 128. We use up to M = 10 found articles as context. The order of articles is randomly shuffled for each training instance to avoid overfitting. The size of the anonymized entity samples is 600, as in most cases the number of entities in a question and context pair is less than 600. For the understanding model, the Adam (Kingma and Ba, 2015) optimization rule with batch size 32 is applied. We stop the optimization on the basis of Dev-Set Performance, the training and the IWFL respectively takes about 10 hours (for each question)."}, {"heading": "5.1 Performance of Retrieval Models", "text": "We evaluate the retrieval models based on the precision and recall of oracle articles. The evaluation is based on the test kit. R @ k is the ratio of cases in which the highest-ranking oracle articles are retrieved in the top k. P @ k is the ratio of oracle articles that are retrieved in the top k. These figures are summarized in Table 3. We can see that the articles do not have specific types of entities.We also have simpler models based on question and article vectors. We emphasize that (R2) no domain specific knowledge is used, and can be easily applied to other datasets where articles do not have specific types of entities.We also have tested simpler models on the question and article vectors. In these models, the question qj and article dk are converted to vectors."}, {"heading": "5.2 Benefit of training methods", "text": "Table 6 shows the effects of anonymizing entities and mixing training articles prior to the understanding step described in Section 3. Mixing the contextual article prior to its concatenation works as a technique for enlarging data. Anonymizing entities helps because without it each entity has an embedding. As most entities occur only a few times in the articles, these embedding may not be properly formed. Instead, the anonymous embedding vectors are trained to distinguish different entities. This technique is motivated by a similar process used in the construction of CNN / Daily Mail (Hermann et al., 2015) and discussed in detail in (Wang et al., 2016)."}, {"heading": "5.3 Visualization", "text": "Figure 6 shows a test example from the WIKIMOVIES-FL test data. In this case, the answers \"Hindi\" and \"English\" are not in context, but correctly assessed from the vocabulary. Note the high value of g in this case. Figure 7 shows another example of how the blending model works. Here, the answer is successfully selected from the document rather than from the vocabulary. In this case, note the low value of g."}, {"heading": "5.4 Performance in each category", "text": "Table 7 shows the comparison for each category of questions between our model and the KV-MemNN for the WIKIMOVIES-WE dataset 6. We see that the performance improvements in the Movie to x are relatively large. The KV-MemNN model has a record-specific \"Title encoding\" function that helps the Model x to film question types. However, without this feature, performance is poor in other categories."}, {"heading": "5.5 Analysis of the mixture gate", "text": "The advantage of the mixing model comes from the fact that ppointer is omitted for some questions6 categories \"Movie to IMDb Votes\" and \"Movie to IMDb Rating\" in this table, as there is only 0.5% test data for these categories and most answers are \"famous\" or \"good,\" while pvocab works well for others. Table 8 shows how often pvocab is used for each category in the AsV model (g > 0.5). For question types \"Movie to Language\" and \"Movie to Genre\" (the so-called \"Choice Questions\"), the number of possible answers is low. In this case, even if the answer can be found in context, it is easier for the model to select the answer from an external vocabulary that encodes global statistics about the entities. For other \"free questions\" one approach is better than the other, depending on the question type. Our model is able to successfully estimate the latent category and to change the model type of coefficient."}, {"heading": "6 Related Work", "text": "Choi et al. (2016) solved the QA problem by selecting a sentence in the document. They show that joint training of selection and understanding slightly improves performance. In our case, joint training is much more difficult due to the large number of film articles. Therefore, we are adopting a two-step retrieval and understanding approach. Recently, Zoph and Le (2016) proposed a framework to use performance on a downstream task (e.g. understanding) as a signal to guide the learning of the neural network that determines the input for the downstream task (e.g. retrieval). This motivates us to introduce a neural network-based approach to both query and understanding, as in this case the retrieval step can be directly trained to maximize downstream performance. In the context of speech modeling, however, the idea of combining two initial probabilities in (Merity et al., 2016) is used in our equation of the mixing coefficient."}, {"heading": "7 Conclusion and Future Work", "text": "We have developed a QA system that uses a two-step retrieval and understanding approach, using a blending model to achieve state-of-the-art technology based on the WIKIMOVIES dataset, which greatly improves previous work. We would like to emphasize that our approach uses minimal heuristics and does not use dataset-specific feature engineering. Efficient retrieval while maintaining variation in representation is a challenging problem. Although much research has been done on understanding, little emphasis has been placed on developing neural network-based retrieval models. We present a simple model of this kind and emphasize the importance of this research direction."}], "references": [{"title": "A neural knowledge language model", "author": ["Sungjin Ahn", "Heeyoul Choi", "Tanel P\u00e4rnamaa", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1608.00318 .", "citeRegEx": "Ahn et al\\.,? 2016", "shortCiteRegEx": "Ahn et al\\.", "year": 2016}, {"title": "Clustering is efficient for approximate maximum inner product searrch", "author": ["Alex Auvolat", "Sarath Chandar", "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio."], "venue": "arXiv:1507.05910 .", "citeRegEx": "Auvolat et al\\.,? 2015", "shortCiteRegEx": "Auvolat et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1533\u20131544.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "ACM SIGMOD international conference on Management of data. ACM, pages", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Hierarchical memory networks", "author": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio."], "venue": "arXiv:1605.07427 .", "citeRegEx": "Chandar et al\\.,? 2016", "shortCiteRegEx": "Chandar et al\\.", "year": 2016}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP. pages", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Hierarchical question answering for long documents", "author": ["Eunsol Choi", "Daniel Hewlett", "Alexandre Lacoste", "Illia Polosukhin", "Jakob Uszkoreit", "Jonathan Berant."], "venue": "arXiv: 1611.01839 .", "citeRegEx": "Choi et al\\.,? 2016", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv: 1606.01549 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 1156\u20131165.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems 28 (NIPS),", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst."], "venue": "Proceedings of the", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Xiong Caiming", "Bradbury James", "Richard Socher."], "venue": "arXiv: 1609.07843 .", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Building a question answering test collection", "author": ["Ellen M Voorhees", "Dawn M Tice."], "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, pages 200\u2013207.", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch."], "venue": "Communications of the ACM 57(10):78\u201385.", "citeRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.,? 2014", "shortCiteRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.", "year": 2014}, {"title": "Emergent logical structure in vector representations of neural readers", "author": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester."], "venue": "arXiv: 1611.07954 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Knowledge base completion via search-based question answering", "author": ["Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin."], "venue": "Proceedings of the 23rd international conference on World wide web. ACM, pages", "citeRegEx": "West et al\\.,? 2014", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["Barret Zoph", "Quoc V. Le."], "venue": "arXiv: 1611.01578 .", "citeRegEx": "Zoph and Le.,? 2016", "shortCiteRegEx": "Zoph and Le.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Most of the current approaches for Question Answering (QA) are based on structured Knowledge Bases (KB) such as Freebase (Bollacker et al., 2008) and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014).", "startOffset": 121, "endOffset": 145}, {"referenceID": 19, "context": ", 2008) and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014).", "startOffset": 21, "endOffset": 51}, {"referenceID": 11, "context": "In this setting the question is converted to a logical form using semantic parsing, which is queried against the KB to obtain the answer (Fader et al., 2014; Berant et al., 2013).", "startOffset": 137, "endOffset": 178}, {"referenceID": 3, "context": "In this setting the question is converted to a logical form using semantic parsing, which is queried against the KB to obtain the answer (Fader et al., 2014; Berant et al., 2013).", "startOffset": 137, "endOffset": 178}, {"referenceID": 21, "context": "However, recent studies have shown that even large curated KBs, such as Freebase, are incomplete (West et al., 2014).", "startOffset": 97, "endOffset": 116}, {"referenceID": 18, "context": "This retrieval based approach has a longer history than the KB based approach (Voorhees and Tice, 2000).", "startOffset": 78, "endOffset": 103}, {"referenceID": 16, "context": "However, there are still gaps in its performance compared to the KB-based approach (Miller et al., 2016).", "startOffset": 83, "endOffset": 104}, {"referenceID": 17, "context": "Several large-scale datasets introduced recently (Rajpurkar et al., 2016; Hermann et al., 2015) have ar X iv :1 70 3.", "startOffset": 49, "endOffset": 95}, {"referenceID": 12, "context": "Several large-scale datasets introduced recently (Rajpurkar et al., 2016; Hermann et al., 2015) have ar X iv :1 70 3.", "startOffset": 49, "endOffset": 95}, {"referenceID": 10, "context": "These models fall into one of two categories: (1) those which extract answers as a span of text from the document (Dhingra et al., 2016; Kadlec et al., 2016; Xiong et al., 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al.", "startOffset": 114, "endOffset": 177}, {"referenceID": 13, "context": "These models fall into one of two categories: (1) those which extract answers as a span of text from the document (Dhingra et al., 2016; Kadlec et al., 2016; Xiong et al., 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al.", "startOffset": 114, "endOffset": 177}, {"referenceID": 22, "context": "These models fall into one of two categories: (1) those which extract answers as a span of text from the document (Dhingra et al., 2016; Kadlec et al., 2016; Xiong et al., 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al.", "startOffset": 114, "endOffset": 177}, {"referenceID": 7, "context": ", 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al., 2016; Miller et al., 2016) (Figure 2 bottom).", "startOffset": 82, "endOffset": 122}, {"referenceID": 16, "context": ", 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al., 2016; Miller et al., 2016) (Figure 2 bottom).", "startOffset": 82, "endOffset": 122}, {"referenceID": 2, "context": "We incorporate the above mixture model in a simple Recurrent Neural Network (RNN) architecture with an attention mechanism (Bahdanau et al., 2015) for comprehension.", "startOffset": 123, "endOffset": 146}, {"referenceID": 2, "context": "We incorporate the above mixture model in a simple Recurrent Neural Network (RNN) architecture with an attention mechanism (Bahdanau et al., 2015) for comprehension. In the second part of the paper we focus on the retrieval step for the QA system, and introduce a neural network based ranking model to select the articles to feed the comprehension model. We evaluate our model on WIKIMOVIES dataset, which consists of 200K questions about movies, along with 18K Wikipedia articles for extracting the answers. Miller et al. (2016) applied Key-Value Memory Neural Networks (KV-MemNN) to the dataset, achieving 76.", "startOffset": 124, "endOffset": 530}, {"referenceID": 16, "context": "We focus on the WIKIMOVIES1 dataset, proposed by (Miller et al., 2016).", "startOffset": 49, "endOffset": 70}, {"referenceID": 5, "context": "The questions are created by human annotators using SimpleQuestions (Bordes et al., 2015), an existing open-domain question answering dataset, and the annotated answers come from facts in two structured KBs: OMDb2 and MovieLens3.", "startOffset": 68, "endOffset": 89}, {"referenceID": 13, "context": "Instead, we use RNN based network, which has been successfully used in many reading comprehension tasks (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 104, "endOffset": 166}, {"referenceID": 10, "context": "Instead, we use RNN based network, which has been successfully used in many reading comprehension tasks (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 104, "endOffset": 166}, {"referenceID": 7, "context": "Instead, we use RNN based network, which has been successfully used in many reading comprehension tasks (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 104, "endOffset": 166}, {"referenceID": 13, "context": "WIKIMOVIES dataset has two notable differences from many of the existing comprehension datasets, such as CNN and SQuAD (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 119, "endOffset": 181}, {"referenceID": 10, "context": "WIKIMOVIES dataset has two notable differences from many of the existing comprehension datasets, such as CNN and SQuAD (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 119, "endOffset": 181}, {"referenceID": 7, "context": "WIKIMOVIES dataset has two notable differences from many of the existing comprehension datasets, such as CNN and SQuAD (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 119, "endOffset": 181}, {"referenceID": 13, "context": "We also use attention sum architecture proposed by Kadlec et al. (2016), which has been shown to give high performance for comprehension tasks.", "startOffset": 51, "endOffset": 72}, {"referenceID": 12, "context": "The method is similar to the anonymization method used in CNN / Daily Mail datasets (Hermann et al., 2015).", "startOffset": 84, "endOffset": 106}, {"referenceID": 8, "context": "Next, the question embedding sequence (xi) is fed into a bidirectional GRU (BiGRU) (Cho et al., 2014) to obtain a fixed length vector v", "startOffset": 83, "endOffset": 101}, {"referenceID": 11, "context": "The method is similar to the anonymization method used in CNN / Daily Mail datasets (Hermann et al., 2015). Wang et al. (2016) showed that such a procedure actually helps readers since it adds coreference information to the system.", "startOffset": 85, "endOffset": 127}, {"referenceID": 16, "context": "We note that KV-MemNN (Miller et al., 2016) employs \u201cTitle encoding\u201d technique, which uses the prior knowledge that movie titles are often in answers.", "startOffset": 22, "endOffset": 43}, {"referenceID": 16, "context": "We note that KV-MemNN (Miller et al., 2016) employs \u201cTitle encoding\u201d technique, which uses the prior knowledge that movie titles are often in answers. Miller et al. (2016) showed that this technique substantially improves model performance by over 7% for WIKIMOVIES-WE dataset.", "startOffset": 23, "endOffset": 172}, {"referenceID": 16, "context": "A similar heuristic was also employed in (Miller et al., 2016).", "startOffset": 41, "endOffset": 62}, {"referenceID": 14, "context": "For training the comprehension model, the Adam (Kingma and Ba, 2015) optimization rule is used with batch size 32.", "startOffset": 47, "endOffset": 68}, {"referenceID": 6, "context": "Maximum Inner Product Search algorithms may also be utilized here (Chandar et al., 2016; Auvolat et al., 2015).", "startOffset": 66, "endOffset": 110}, {"referenceID": 1, "context": "Maximum Inner Product Search algorithms may also be utilized here (Chandar et al., 2016; Auvolat et al., 2015).", "startOffset": 66, "endOffset": 110}, {"referenceID": 12, "context": "This technique is motivated by a similar procedure used in the construction of CNN / Daily Mail (Hermann et al., 2015), and discussed in detail in (Wang et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 20, "context": ", 2015), and discussed in detail in (Wang et al., 2016).", "startOffset": 36, "endOffset": 55}, {"referenceID": 23, "context": "Recently Zoph and Le (2016) proposed a framework to use the performance on a downstream task (e.", "startOffset": 9, "endOffset": 28}, {"referenceID": 15, "context": "In the context of language modeling, the idea of combining of two output probabilities is given in (Merity et al., 2016), however, our equation to compute the mixture coefficient is slightly different.", "startOffset": 99, "endOffset": 120}, {"referenceID": 0, "context": "More recently, Ahn et al. (2016) used a mixture model to predict the next word from either the entire vocabulary, or a set of Knowledge Base facts associated with the text.", "startOffset": 15, "endOffset": 33}], "year": 2017, "abstractText": "Open domain Question Answering (QA) systems must interact with external knowledge sources, such as web pages, to find relevant information. Information sources like Wikipedia, however, are not well structured and difficult to utilize in comparison with Knowledge Bases (KBs). In this work we present a two-step approach to question answering from unstructured text, consisting of a retrieval step and a comprehension step. For comprehension, we present an RNN based attention model with a novel mixture mechanism for selecting answers from either retrieved articles or a fixed vocabulary. For retrieval we introduce a hand-crafted model and a neural model for ranking relevant articles. We achieve state-of-the-art performance on WIKIMOVIES dataset, reducing the error by 40%. Our experimental results further demonstrate the importance of each of the introduced components.", "creator": "LaTeX with hyperref package"}}}