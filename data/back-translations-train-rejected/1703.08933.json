{"id": "1703.08933", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Multiple Instance Learning with the Optimal Sub-Pattern Assignment Metric", "abstract": "Multiple instance data are sets or multi-sets of unordered elements. Using metrics or distances for sets, we propose an approach to several multiple instance learning tasks, such as clustering (unsupervised learning), classification (supervised learning), and novelty detection (semi-supervised learning). In particular, we introduce the Optimal Sub-Pattern Assignment metric to multiple instance learning so as to provide versatile design choices. Numerical experiments on both simulated and real data are presented to illustrate the versatility of the proposed solution.", "histories": [["v1", "Mon, 27 Mar 2017 05:23:32 GMT  (6304kb,D)", "http://arxiv.org/abs/1703.08933v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["quang n tran", "ba-ngu vo", "dinh phung", "ba-tuong vo", "thuong nguyen"], "accepted": false, "id": "1703.08933"}, "pdf": {"name": "1703.08933.pdf", "metadata": {"source": "CRF", "title": "Multiple Instance Learning with the Optimal Sub-Pattern Assignment Metric", "authors": ["Quang N. Tran", "Ba-Ngu Vo", "Dinh Phung", "Ba-Tuong Vo", "Thuong Nguyen"], "emails": [], "sections": [{"heading": null, "text": "This year it is so far that it is only a matter of time before it will be so far, until it is so far."}, {"heading": "II. SET DISTANCES", "text": "Machine learning tasks such as clustering, classification and novelty detection mainly relate to grouping / separating data based on their similarities / similarities. Distance is a basic measure of dissimilarity between two objects. Therefore, the concept of distance or metric is important for learning approaches without models [30], [3], [31]. In MI learning, several fixed distances were introduced for PP data, namely for Hausdorff [20] and Chamfer [21] distances. In this section, we introduce Hausdorff [20], Wasserstein [29] and OSPA distances [24]. In particular, we discuss their properties and implications in the context of design decisions for MI learning algorithms. Choosing the specified distance in MI learning directly influences performance and therefore it is important to select deviations that are compatible with the application areas."}, {"heading": "A. Hausdorff distance", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "B. Wasserstein distance", "text": "The Waterstone distance (also known as Optimal Mass Transfer Distance [24]) of the order p \u2265 1 between two sets X and Y is defined by [29] d (P) W (X, Y) = minC m \u2211 i = 1 n, J = 1, D = 1, D = 1, D = 1, D = 1, D = 1, D = 1, D = 1, D = 1, D = 1, D = 1, D = 1, D = 1."}, {"heading": "C. OSPA distance", "text": "It is not, therefore, as if it were a matter of a way and a way in which it is a matter of a way and a way in which it is a matter of a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about"}, {"heading": "III. CLUSTERING OF POINT PATTERNS", "text": "Clustering is generally an unattended learning problem, since class (or cluster) names are not available [36], [37]. The aim of clustering is to divide the data into groups so that the members of a group are similar to each other but differ from the observations of other groups [38]. Clustering is a fundamental problem in data analysis with a long history in psychology dating back to the 1930s [39]. Comprehensive surveys of clustering can be found in [36], [30], [40]."}, {"heading": "A. Problem Formulation", "text": "In an MI clustering context, the overall goal is to partition a specific PP dataset D = {X1,..., XN}, X into fragmented clusters that minimize the sum of (specified) distances between PPs and their cluster centers. Furthermore, the automatic punishment of the trivial partition P = {X1},..., {XN}, (i.e., each observation is a cluster that yields the zero sum of distances. More precisely: D \u2192 X is a mapping that assigns a cluster center to each5 PP in D, i.e., \u00b5 (X) is the center of the cluster to which X belongs, then the clustering problem can be specified as asmin \u00b5."}, {"heading": "B. AP clustering with set distances", "text": "The AP algorithm uses the similarity values between all observation pairs in the data set D = {X1,..., XN} and custom copy preferences, as input and return of the \"best\" examples. The similarity values in this work are the negatives of the OSPA distances between the PPs in D. The preference value for a date Xn is the negative value of the penalty, i.e., \u2212 bn (Xn), which corresponds more to their preference, the more likely it is that Xn is a copy. In AP, which could be Xn itself or another observation, is represented by a variable cn, where cn = k means that Xn.Note is the copy of a configuration (c1, cN)."}, {"heading": "C. Experiments", "text": "In this section, we evaluate the performance of the proposed AP-based clustering algorithm on both simulated and real PP data. (In particular, we compare the clustering performance between Hausdorff, Wasserstein and OSPA distances. Note that BAMIC (which uses the K-Medoid algorithm instead of AP) as AP clustering with the Hausdorff distance. Since the outcome of the AP algorithm depends on the choice of copy preferences, we first empirically select the copy preferences that provide the best performance in terms of the number of clusters for each distance, and then benchmark the best performance of one distance against the other. Therrelevant performance indicators are: Purity (Pu), Normalized Mutual Information (NMI), Edge Index (RI), F1 Score (F1) [44].1) Clustering with Simulated Data: In this experiment, we look at three simulated data sets."}, {"heading": "IV. CLASSIFICATION OF POINT PATTERNS", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "B. Experiments", "text": "In the following experiments, we will perform the classification of the OSPA distance to the two simulated and real data. As the performance depends on the choice of the closest neighbors, we have performed our experiments for each region."}, {"heading": "V. NOVELTY DETECTION FOR POINT PATTERNS", "text": "Novelty detection is the task of identifying new or strange data that differ significantly from \"normal\" training data [54], [31]. Note that novelty detection is not a specific case of classification, as no anomalous or novel training data is available [17]. There are typically two phases of novelty detection: training and detection. Since the training phase only requires normal data, novelty detection is considered semi-supervised learning [55], [17]. Novelty detection, however, is a fundamental problem in data analysis with a wide range of 10 applications ranging from intrusion detection [56], fraud detection [57], structural health monitoring [58] to tumour detection from MRI images [55]."}, {"heading": "A. Novelty detection with set distances", "text": "If the distance (e.g. Hausdorff, Wasserstein or OSPA) between candidate PP and its closest normal neighbour (NNN) 9 is greater than a given threshold, then the candidate is considered novelty, otherwise it is normal. An appropriate threshold can be chosen experimentally [54]. An appropriate threshold is the 95th percentile of the distances between classes (between normal observations and their NNNNs). However, no single threshold is guaranteed to work well for all cases.Similar to the classification with OSPA (section IV-A), training data can be used to determine a suitable balance between differences in characteristics and differences in cardinality p."}, {"heading": "B. Experiments", "text": "In this case, we are in a position to go in search of a new partner who is able to go to another world."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, algorithms for clustering, classification and novelty detection were presented with point pattern data using the OSPA distance. In clustering, AP is combined with the OSPA distance (or others such as Wasserstein and Hausdorff) as a measure of dissimilarity. In classification, the OSPA distance is incorporated into the k-closest neighbor (k-NN) algorithm. MI novelty detection has developed a solution that provides the specified distances between candidate PP and his closest normal neighbor in the training set.Numerical experiments with simulated and real data showed that the OSPA distance offers more flexibility in design choice for all applications, as well as the ability to better detect differences between sets compared to other distances. We repeat that while the OSPA distance offers some advantages over other distances, there is not a single distance that works for all applications. In practice, it is important to determine which distance (and parameters) is better suited for which application."}], "references": [{"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial intelligence, vol. 89, no. 1, pp. 31\u201371, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple instance learning of calmodulin binding sites", "author": ["F. Minhas", "A. Ben-Hur"], "venue": "Bioinformatics (Oxford, England), vol. 28, no. 18, pp. i416\u2013i422, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["J. Amores"], "venue": "Artificial Intelligence, vol. 201, pp. 81\u2013105, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A review of multi-instance learning assumptions", "author": ["J. Foulds", "E. Frank"], "venue": "The Knowledge Engineering Review, vol. 25, no. 01, pp. 1\u201325, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical inference and simulation for spatial point processes", "author": ["J. Moller", "R.P. Waagepetersen"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "A probabilistic analysis of the rocchio algorithm with tfidf for text categorization.", "author": ["T. Joachims"], "venue": "DTIC Document, Tech. Rep.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "A comparison of event models for naive Bayes text classification", "author": ["A. McCallum", "K. Nigam"], "venue": "AAAI-98 Workshop learning for text categorization, vol. 752. Citeseer, 1998, pp. 41\u201348.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "Workshop statistical learning in computer vision, ECCV, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recognition (CVPR), 2005, vol. 2. IEEE, 2005, pp. 524\u2013 531.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "3d is here: Point cloud library (pcl)", "author": ["R.B. Rusu", "S. Cousins"], "venue": "IEEE Int. Conf. Robotics and Automation (ICRA), 2011. IEEE, 2011, pp. 1\u20134.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Tomographic reconstruction using an adaptive tetrahedral mesh defined by a point cloud", "author": ["A. Sitek", "R.H. Huesman", "G.T. Gullberg"], "venue": "IEEE Trans. Medical Imaging, vol. 25, no. 9, pp. 1172\u20131179, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "A new segmentation method for point cloud data", "author": ["H. Woo", "E. Kang", "S. Wang", "K.H. Lee"], "venue": "Int. Journal Machine Tools and Manufacture, vol. 42, no. 2, pp. 167\u2013178, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Rock: A robust clustering algorithm for categorical attributes", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "Proc. 15th Int. Conf. Data Eng., 1999. IEEE, 1999, pp. 512\u2013521.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Clope: a fast and effective clustering algorithm for transactional data", "author": ["Y. Yang", "X. Guan", "J. You"], "venue": "Proc. 8th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining. ACM, 2002, pp. 682\u2013687.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "An efficient clustering algorithm for market basket data based on small large ratios", "author": ["C.-H. Yun", "K.-T. Chuang", "M.-S. Chen"], "venue": "Comput. Softw. and Appl. Conf., 2001. COMPSAC 2001. 25th Annual Int. IEEE, 2001, pp. 505\u2013510.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "A general probabilistic framework for clustering individuals and objects", "author": ["I.V. Cadez", "S. Gaffney", "P. Smyth"], "venue": "Proc. 6th ACM SIGKDD Int. Conf. knowledge discovery and data mining. ACM, 2000, pp. 140\u2013 149.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey of outlier detection methodologies", "author": ["V.J. Hodge", "J. Austin"], "venue": "Artificial Intelligence Review, vol. 22, no. 2, pp. 85\u2013126, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-instance clustering with applications to multi-instance prediction", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Appl. Intell., vol. 31, no. 1, pp. 47\u201368, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "M3ic: Maximum margin multiple instance clustering", "author": ["D. Zhang", "F. Wang", "L. Si", "T. Li"], "venue": "IJCAI, vol. 9, 2009, pp. 1339\u20131344.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Tracking non-rigid objects in complex scenes", "author": ["D.P. Huttenlocher", "J.J. Noh", "W.J. Rucklidge"], "venue": "Proc. 4th Int. Conf. Comput. Vision, 1993. IEEE, 1993, pp. 93\u2013101.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Real-time object detection for \u201dsmart\u201d vehicles", "author": ["D.M. Gavrila", "V. Philomin"], "venue": "Proc. 7th Int. Conf. Comput. Vision, 1999, vol. 1. IEEE, 1999, pp. 87\u201393.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Local features and kernels for classification of texture and object categories: A comprehensive study", "author": ["J. Zhang", "M. Marsza\u0142ek", "S. Lazebnik", "C. Schmid"], "venue": "Int. J. Comput. Vision, vol. 73, no. 2, pp. 213\u2013238, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "A metric for distributions with applications to image databases", "author": ["Y. Rubner", "C. Tomasi", "L.J. Guibas"], "venue": "6th Int. Conf. Comput. Vision, 1998. IEEE, 1998, pp. 59\u201366.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "A consistent metric for performance evaluation of multi-object filters", "author": ["D. Schuhmacher", "B.-T. Vo", "B.-N. Vo"], "venue": "IEEE Trans. Signal Process., vol. 56, no. 8, pp. 3447\u20133457, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Sci., vol. 315, no. 5814, pp. 972\u2013976, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Clustering for point pattern data", "author": ["Q.N. Tran", "B.-N. Vo", "D. Phung", "B.-T. Vo"], "venue": "23rd Intl. Conf. Pattern Recognition (ICPR), Dec. 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Trans. Inf. Theory, vol. 13, no. 1, pp. 21\u201327, 1967.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1967}, {"title": "A fuzzy k-nearest neighbor algorithm", "author": ["J.M. Keller", "M.R. Gray", "J.A. Givens"], "venue": "IEEE Trans. Systems, Man and Cybernetics, no. 4, pp. 580\u2013 585, 1985.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1985}, {"title": "Multitarget miss distance via optimal assignment", "author": ["J.R. Hoffman", "R.P. Mahler"], "venue": "IEEE Trans. Systems, Man and Cybernetics, Part A: Systems and Humans, vol. 34, no. 3, pp. 327\u2013336, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern recognition letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A review of novelty detection", "author": ["M.A. Pimentel", "D.A. Clifton", "L. Clifton", "L. Tarassenko"], "venue": "Signal Process., vol. 99, pp. 215\u2013249, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparing images using the hausdorff distance", "author": ["D.P. Huttenlocher", "G.A. Klanderman", "W.J. Rucklidge"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 15, no. 9, pp. 850\u2013863, 1993.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1993}, {"title": "Locating objects using the hausdorff distance", "author": ["W.J. Rucklidge"], "venue": "Proc. 5th Int. Conf. Comput. Vision, 1995. IEEE, 1995, pp. 457\u2013464.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1995}, {"title": "Metro: measuring error on simplified surfaces", "author": ["P. Cignoni", "C. Rocchini", "R. Scopigno"], "venue": "Comput. Graphics Forum, vol. 17, no. 2. Wiley Online Library, 1998, pp. 167\u2013174.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}, {"title": "Errors in binary images and an lp version of the hausdorff metric", "author": ["A. Baddeley"], "venue": "Nieuw Archief voor Wiskunde, vol. 10, no. 4, pp. 157\u2013183, 1992.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1992}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Comput. surveys (CSUR), vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Tryon, Cluster analysis: correlation profile and orthometric (factor) analysis for the isolation of unities in mind and personality", "author": ["C. R"], "venue": "Edwards brothers,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1939}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Trans. Neural Networks, vol. 16, no. 3, pp. 645\u2013678, 2005.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Ospa barycenters for clustering set-valued data", "author": ["M. Baum", "B. Balasingam", "P. Willett", "U.D. Hanebeck"], "venue": "18th Int. Conf. Inf. Fusion (Fusion). IEEE, 2015, pp. 1375\u20131381.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-metric affinity propagation for unsupervised image categorization", "author": ["D. Dueck", "B.J. Frey"], "venue": "11th Int. Conf. Comput. Vision (ICCV). IEEE, 2007, pp. 1\u20138.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "A binary variable model for affinity propagation", "author": ["I.E. Givoni", "B.J. Frey"], "venue": "Neural computation, vol. 21, no. 6, pp. 1589\u20131600, 2009.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Model-based classification and novelty detection for point pattern data", "author": ["B.-N. Vo", "Q.N. Tran", "D. Phung", "B.-T. Vo"], "venue": "23rd Intl. Conf. Pattern Recognition (ICPR), Dec. 2016.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "A sparse texture representation using local affine regions", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 8, pp. 1265\u20131278, 2005.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Vlfeat: An open and portable library of comput. vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org/, 2008.  13", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "StudentLife: assessing mental health, academic performance and behavioral trends of college students using smartphones", "author": ["R. Wang", "F. Chen", "Z. Chen", "T. Li", "G. Harari", "S. Tignor", "X. Zhou", "D. Ben-Zeev", "A.T. Campbell"], "venue": "Proc. 2014 ACM Int. Joint Conf. Pervasive and Ubiquitous Comput. ACM, 2014, pp. 3\u201314.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern recognition and machine", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2005}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "Proc. 5th Annual Workshop Computational Learning Theory. ACM, 1992, pp. 144\u2013152.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1992}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1995}, {"title": "Pattern classification (2nd edition)", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2001}, {"title": "Novelty detection: a review \u2013 part 1: statistical approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Process., vol. 83, no. 12, pp. 2481\u20132497, 2003.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2003}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Comput. Surveys (CSUR), vol. 41, no. 3, p. 15, 2009.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "A virtual machine introspection based architecture for intrusion detection", "author": ["T. Garfinkel", "M. Rosenblum"], "venue": "NDSS, vol. 3, 2003, pp. 191\u2013206.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2003}, {"title": "Statistical fraud detection: A review", "author": ["R.J. Bolton", "D.J. Hand"], "venue": "Statistical Sci., pp. 235\u2013249, 2002.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2002}, {"title": "An introduction to structural health monitoring", "author": ["C.R. Farrar", "K. Worden"], "venue": "Philosophical Trans. Royal Soc. London A: Mathematical, Physical and Engineering Sci., vol. 365, no. 1851, pp. 303\u2013315, 2007.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1851}, {"title": "Outlier detection using k-nearest neighbour graph", "author": ["V. Hautam\u00e4ki", "I. K\u00e4rkk\u00e4inen", "P. Fr\u00e4nti"], "venue": "ICPR, 2004, pp. 430\u2013433.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Multiple instance (MI) data, more commonly known as \u2018bags\u2019 [1], [2], [3], [4], are mathematical objects called point patterns.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "Multiple instance (MI) data, more commonly known as \u2018bags\u2019 [1], [2], [3], [4], are mathematical objects called point patterns.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "Multiple instance (MI) data, more commonly known as \u2018bags\u2019 [1], [2], [3], [4], are mathematical objects called point patterns.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "Multiple instance (MI) data, more commonly known as \u2018bags\u2019 [1], [2], [3], [4], are mathematical objects called point patterns.", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "A point pattern (PP) is a set or multi-set of unordered points (or elements) [5], in which each point represents the state or features of the object of study.", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "In natural language processing and information retrieval, the \u2018bag-of-words\u2019 representation treats each document as a collection or set of words [6], [7].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "In natural language processing and information retrieval, the \u2018bag-of-words\u2019 representation treats each document as a collection or set of words [6], [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "In image and scene categorization, the \u2018bag-of-visualwords\u2019 representation\u2014the analogue of the \u2018bag-of-words\u2019 in text analysis\u2014treats each image as a set of its key patches [8], [9].", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "In image and scene categorization, the \u2018bag-of-visualwords\u2019 representation\u2014the analogue of the \u2018bag-of-words\u2019 in text analysis\u2014treats each image as a set of its key patches [8], [9].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "In applications involving three-dimensional (3D) images such as computer tomography scan, and magnetic resonance imaging, point cloud data are actually sets of points in some coordinate system [10], [11], [12].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "In applications involving three-dimensional (3D) images such as computer tomography scan, and magnetic resonance imaging, point cloud data are actually sets of points in some coordinate system [10], [11], [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 11, "context": "In applications involving three-dimensional (3D) images such as computer tomography scan, and magnetic resonance imaging, point cloud data are actually sets of points in some coordinate system [10], [11], [12].", "startOffset": 205, "endOffset": 209}, {"referenceID": 12, "context": "In data analysis for the retail industry as well as web management systems, transaction records such as market-basket data [13], [14], [15] and web log data [16] are sets of transaction items.", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "In data analysis for the retail industry as well as web management systems, transaction records such as market-basket data [13], [14], [15] and web log data [16] are sets of transaction items.", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "In data analysis for the retail industry as well as web management systems, transaction records such as market-basket data [13], [14], [15] and web log data [16] are sets of transaction items.", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "In data analysis for the retail industry as well as web management systems, transaction records such as market-basket data [13], [14], [15] and web log data [16] are sets of transaction items.", "startOffset": 157, "endOffset": 161}, {"referenceID": 2, "context": "While PP data are abundant, fundamental MI learning tasks such as clustering (unsupervised learning), classification (supervised learning), and novelty detection1 (semi-supervised learning), have received limited attention [3], [4].", "startOffset": 223, "endOffset": 226}, {"referenceID": 3, "context": "While PP data are abundant, fundamental MI learning tasks such as clustering (unsupervised learning), classification (supervised learning), and novelty detection1 (semi-supervised learning), have received limited attention [3], [4].", "startOffset": 228, "endOffset": 231}, {"referenceID": 17, "context": "In MI clustering, two algorithms have been developed for PP data: Bag-level Multi-instance Clustering (BAMIC) [18]; and Maximum Margin Multiple Instance Clustering (MIC) [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "In MI clustering, two algorithms have been developed for PP data: Bag-level Multi-instance Clustering (BAMIC) [18]; and Maximum Margin Multiple Instance Clustering (MIC) [19].", "startOffset": 170, "endOffset": 174}, {"referenceID": 17, "context": "BAMIC adapts the k-medoids algorithm with the Hausdorff distance as a measure of dissimilarity between PPs [18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "1Novelty detection is not a special case of classification because anomalous or novel training data is not available [17].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "on the other hand, poses the PP clustering problem as a non-convex optimization problem which is then relaxed and solved via a combination of the Constrained Concave-Convex Procedure and Cutting Plane methods [19].", "startOffset": 209, "endOffset": 213}, {"referenceID": 2, "context": "In MI classification, there are three paradigms: InstanceSpace; Embedded-Space; and Bag-Space [3], [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "In MI classification, there are three paradigms: InstanceSpace; Embedded-Space; and Bag-Space [3], [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 19, "context": "Existing methods in the Bag-Space paradigm uses the Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] distances.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Existing methods in the Bag-Space paradigm uses the Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] distances.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "Existing methods in the Bag-Space paradigm uses the Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] distances.", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "Existing methods in the Bag-Space paradigm uses the Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] distances.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "In this paper, we propose the use of the Optimal SubPattern Assignment (OSPA) distance [24] in MI clustering, classification and novelty detection.", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Our specific contributions are: \u2022 In MI clustering, we combine the Affinity Propagation (AP) clustering algorithm [25] with set distances as dissimilarity measures2.", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "Compared to existing k-medoids based techniques [18], AP can find clusters faster with much lower error, and does not require the number of clusters to be specified [25].", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "Compared to existing k-medoids based techniques [18], AP can find clusters faster with much lower error, and does not require the number of clusters to be specified [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "In addition, the OSPA distance is more versatile than the Hausdorff distance used in [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 26, "context": "\u2022 In MI classification, we use the OSPA distance in the k-nearest neighbour (k-NN) algorithm [27], [28], and examine the performance against the Hausdorff-based technique [20] and the Wasserstein-based technique (the Earth Mover\u2019s distance adapted for PPs [29]).", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "\u2022 In MI classification, we use the OSPA distance in the k-nearest neighbour (k-NN) algorithm [27], [28], and examine the performance against the Hausdorff-based technique [20] and the Wasserstein-based technique (the Earth Mover\u2019s distance adapted for PPs [29]).", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "\u2022 In MI classification, we use the OSPA distance in the k-nearest neighbour (k-NN) algorithm [27], [28], and examine the performance against the Hausdorff-based technique [20] and the Wasserstein-based technique (the Earth Mover\u2019s distance adapted for PPs [29]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 28, "context": "\u2022 In MI classification, we use the OSPA distance in the k-nearest neighbour (k-NN) algorithm [27], [28], and examine the performance against the Hausdorff-based technique [20] and the Wasserstein-based technique (the Earth Mover\u2019s distance adapted for PPs [29]).", "startOffset": 256, "endOffset": 260}, {"referenceID": 25, "context": "2Preliminary results have been presented in the conference paper [26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "Bag-Space approaches lies in the versatility of the OSPA distance over the Hausdorff [20], Chamfer [21] and Earth mover\u2019s [22] distances.", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "Bag-Space approaches lies in the versatility of the OSPA distance over the Hausdorff [20], Chamfer [21] and Earth mover\u2019s [22] distances.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Bag-Space approaches lies in the versatility of the OSPA distance over the Hausdorff [20], Chamfer [21] and Earth mover\u2019s [22] distances.", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "Hence, the notion of distance or metric is important to learning approaches without models [30], [3], [31].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Hence, the notion of distance or metric is important to learning approaches without models [30], [3], [31].", "startOffset": 97, "endOffset": 100}, {"referenceID": 30, "context": "Hence, the notion of distance or metric is important to learning approaches without models [30], [3], [31].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "In MI learning, several set distances have been introduced for PP data3, namely the Hausdorff [20], and Chamfer [21] distances.", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "In MI learning, several set distances have been introduced for PP data3, namely the Hausdorff [20], and Chamfer [21] distances.", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "In this section, we present the Hausdorff [20], Wasserstein [29], and OSPA distances [24].", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "In this section, we present the Hausdorff [20], Wasserstein [29], and OSPA distances [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "In this section, we present the Hausdorff [20], Wasserstein [29], and OSPA distances [24].", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "Hausdorff distance has been successfully applied in applications dealing with PP data, such as detecting objects from binary images [32], [33], or measuring the dissimilarities between 3D surfaces\u2014sets of coordinates of points [34].", "startOffset": 132, "endOffset": 136}, {"referenceID": 32, "context": "Hausdorff distance has been successfully applied in applications dealing with PP data, such as detecting objects from binary images [32], [33], or measuring the dissimilarities between 3D surfaces\u2014sets of coordinates of points [34].", "startOffset": 138, "endOffset": 142}, {"referenceID": 33, "context": "Hausdorff distance has been successfully applied in applications dealing with PP data, such as detecting objects from binary images [32], [33], or measuring the dissimilarities between 3D surfaces\u2014sets of coordinates of points [34].", "startOffset": 227, "endOffset": 231}, {"referenceID": 2, "context": "In MI learning it has been applied in classification [3] and clustering [18].", "startOffset": 53, "endOffset": 56}, {"referenceID": 17, "context": "In MI learning it has been applied in classification [3] and clustering [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "\u2022 The Hausdorff distance is relatively insensitive to dissimilarities in cardinality [24].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "\u2022 The Hausdorff distance penalizes heavily outliers\u2014elements in one set which are far from every element of the other set [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 34, "context": "Note that there are also generalizations of the Hausdorff distance that avoid the undesirable outlier penalty [35].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "The Chamfer \u201cdistance\u201d [21] is a variation of the Hausdorff construction, but does not satisfy the metric axioms.", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "In terms of measuring dissimilarity, it is very similar to the Hausdorff distance, and has been used to construct a Support Vector Machine kernel for MI classification in [3].", "startOffset": 171, "endOffset": 174}, {"referenceID": 23, "context": "Wasserstein distance The Wasserstein distance (also known as Optimal Mass Transfer distance [24]) of order p \u2265 1 between two sets X and Y is defined by [29]", "startOffset": 92, "endOffset": 96}, {"referenceID": 28, "context": "Wasserstein distance The Wasserstein distance (also known as Optimal Mass Transfer distance [24]) of order p \u2265 1 between two sets X and Y is defined by [29]", "startOffset": 152, "endOffset": 156}, {"referenceID": 28, "context": "Note that similar to the Hausdorff distance the Wasserstein distance is a metric [29] and is not defined when either X or Y is empty.", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "The Wasserstein distance can be considered as the Earth Mover\u2019s distance [23] adapted for PPs [29].", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "The Wasserstein distance can be considered as the Earth Mover\u2019s distance [23] adapted for PPs [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "Indeed the Earth Mover\u2019s distance has been used to construct a Support Vector Machine kernel for MI classification in [22], [3].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "Indeed the Earth Mover\u2019s distance has been used to construct a Support Vector Machine kernel for MI classification in [22], [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 23, "context": "The Wasserstein distance partially addresses the cardinality insensitivity and reduces the undesirable penalty on the outliers of the Hausdorff distance [24], see for example Fig.", "startOffset": 153, "endOffset": 157}, {"referenceID": 23, "context": "OSPA distance The Optimal SubPattern Assignment (OSPA) [24] distance of order p \u2265 1, and cutoff c > 0, is defined by", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Thus, the OSPA distance has a physically intuitive interpretation as the \u201cper element\u201d dissimilarity that incorporates both features and cardinality [24].", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "The OSPA distance is a metric with several salient properties that can address some of the undesirable effects of the Hausdorff and Wasserstein distances [24].", "startOffset": 154, "endOffset": 158}, {"referenceID": 35, "context": "In general, clustering is an unsupervised learning problem since the class (or cluster) labels are not provided [36], [37].", "startOffset": 112, "endOffset": 116}, {"referenceID": 36, "context": "The aim of clustering is to partition the data into groups so that members in a group are similar to each other whilst dissimilar to observations from other groups [38].", "startOffset": 164, "endOffset": 168}, {"referenceID": 37, "context": "Clustering is a fundamental problem in data analysis with a long history dated back to the 1930s in psychology [39].", "startOffset": 111, "endOffset": 115}, {"referenceID": 35, "context": "Comprehensive surveys on clustering can be found in [36], [30], [40].", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "Comprehensive surveys on clustering can be found in [36], [30], [40].", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "Comprehensive surveys on clustering can be found in [36], [30], [40].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "In general, the Fr\u00e9chet mean of a collection of PPs is computationally intractable [41] and a better strategy is to select the centroids from the dataset.", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Such centroids, also known as \u2018exemplars\u2019 [25], can be efficiently computed as well as serving as real prototypes for the data.", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "To the best of our knowledge, BAMIC [18] is the only exemplar-based clustering algorithm for PPs using a set distance (Hausdorff) as a measure of dissimilarity.", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "Determining the correct number of clusters is one of the most challenging aspects of clustering [30].", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "In this work, we propose a versatile MI clustering algorithm using the AP algorithm [25] with the OSPA distance as a dissimilarity measure.", "startOffset": 84, "endOffset": 88}, {"referenceID": 40, "context": "Using message passing, AP provides good approximate solutions to problem (5)-(6) [42], [25], thereby determining the number of clusters automatically from the data (see details in section III-B).", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "Using message passing, AP provides good approximate solutions to problem (5)-(6) [42], [25], thereby determining the number of clusters automatically from the data (see details in section III-B).", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Compared to k-medoids (used in BAMIC), AP can find clusters faster with considerably lower error [25] and does not require random initialization of cluster centers (since AP first considers all observations as exemplars).", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "AP is an efficient approximate max-sum message-passing algorithm using a protocol originally derived from loopy propagation on factor graphs [25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "5 Further details on the AP algorithm can be found in [25], [42].", "startOffset": 54, "endOffset": 58}, {"referenceID": 40, "context": "5 Further details on the AP algorithm can be found in [25], [42].", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": ", the median of the similarities (which results in a moderate number of clusters) or the minimum of the similarities (which results in a small number of clusters) [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 41, "context": "5An equivalent binary graphical model representation for AP was later proposed in [43].", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Instead of creating a latent node for each individual observation as in [25], a binary node bn,k is created for each pair (Xn, Xk) and bn,k = 1 if Xk is an exemplar for Xn.", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "The loopy propagation is usually terminated when changes in the messages fall below a threshold (see Algorithm 1), or when the cluster assignments stay constant for some iterations, or when number of iterations reaches a given value [25].", "startOffset": 233, "endOffset": 237}, {"referenceID": 24, "context": "The cluster label cn is the value of k that maximizes the sum r(n, k) + a(n, k) [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "2) Clustering with the Texture dataset: This experiment involves clustering images from the classes \u201cT14 brick1\u201d, \u201cT15 brick2\u201d, and \u201cT20 upholstery\u201d of the Texture images dataset [46].", "startOffset": 179, "endOffset": 183}, {"referenceID": 44, "context": "Each image is compressed into a PP of 2-D features by first applying the SIFT algorithm (using the VLFeat library [47]) to produce a PP of 128-D SIFT features, which is then further compressed into a 2-D PP by Principal", "startOffset": 114, "endOffset": 118}, {"referenceID": 42, "context": "6This dataset is similar to that of [45].", "startOffset": 36, "endOffset": 40}, {"referenceID": 45, "context": "3) Clustering with the StudentLife dataset: This experiment involves WiFi scan data from the StudentLife dataset [48] collected from smartphones carried by students at Dartmouth College.", "startOffset": 113, "endOffset": 117}, {"referenceID": 46, "context": ", Nclass} to each input observation X [49].", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": ", clustering (section III), classification relies on training data, which are fully-observed input-output pairs Dtrain = {(Xn, `n)} n=1 [38].", "startOffset": 136, "endOffset": 140}, {"referenceID": 36, "context": "Classification is arguably the most widely used form of supervised machine learning, spanning various fields of study [38], [50].", "startOffset": 118, "endOffset": 122}, {"referenceID": 47, "context": "Classification is arguably the most widely used form of supervised machine learning, spanning various fields of study [38], [50].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "The classification problem can be approached with or without knowledge of the underlying data model [27].", "startOffset": 100, "endOffset": 104}, {"referenceID": 48, "context": "Among non-parametric classifiers such as Support Vector Machine (a binary classifier) [51], [52], Parzen window [53], k-Nearest Neighbors (k-NN) [27], [28], k-NN is more suited to PP data classification using set distances.", "startOffset": 86, "endOffset": 90}, {"referenceID": 49, "context": "Among non-parametric classifiers such as Support Vector Machine (a binary classifier) [51], [52], Parzen window [53], k-Nearest Neighbors (k-NN) [27], [28], k-NN is more suited to PP data classification using set distances.", "startOffset": 92, "endOffset": 96}, {"referenceID": 50, "context": "Among non-parametric classifiers such as Support Vector Machine (a binary classifier) [51], [52], Parzen window [53], k-Nearest Neighbors (k-NN) [27], [28], k-NN is more suited to PP data classification using set distances.", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "Among non-parametric classifiers such as Support Vector Machine (a binary classifier) [51], [52], Parzen window [53], k-Nearest Neighbors (k-NN) [27], [28], k-NN is more suited to PP data classification using set distances.", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "Among non-parametric classifiers such as Support Vector Machine (a binary classifier) [51], [52], Parzen window [53], k-Nearest Neighbors (k-NN) [27], [28], k-NN is more suited to PP data classification using set distances.", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "In MI learning, PP classifiers based on the k-NN algorithm using set distances such as Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] have been proposed.", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "In MI learning, PP classifiers based on the k-NN algorithm using set distances such as Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] have been proposed.", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "In MI learning, PP classifiers based on the k-NN algorithm using set distances such as Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] have been proposed.", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "In MI learning, PP classifiers based on the k-NN algorithm using set distances such as Hausdorff [20], Chamfer [21], and Earth Mover\u2019s [22], [23] have been proposed.", "startOffset": 141, "endOffset": 145}, {"referenceID": 51, "context": "Novelty detection is the task of identifying new or strange data that are significantly different from \u2018normal\u2019 training data [54], [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 30, "context": "Novelty detection is the task of identifying new or strange data that are significantly different from \u2018normal\u2019 training data [54], [31].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "Note that novelty detection is not a special case of classification because anomalous or novel training data is not available [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 52, "context": "Since its training phase requires only normal data, novelty detection is considered as semi-supervised learning [55], [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "Since its training phase requires only normal data, novelty detection is considered as semi-supervised learning [55], [17].", "startOffset": 118, "endOffset": 122}, {"referenceID": 53, "context": "application areas ranging from intrusion detection [56], fraud detection [57], structural health monitoring [58], to tumor detection from MRI images [55].", "startOffset": 51, "endOffset": 55}, {"referenceID": 54, "context": "application areas ranging from intrusion detection [56], fraud detection [57], structural health monitoring [58], to tumor detection from MRI images [55].", "startOffset": 73, "endOffset": 77}, {"referenceID": 55, "context": "application areas ranging from intrusion detection [56], fraud detection [57], structural health monitoring [58], to tumor detection from MRI images [55].", "startOffset": 108, "endOffset": 112}, {"referenceID": 52, "context": "application areas ranging from intrusion detection [56], fraud detection [57], structural health monitoring [58], to tumor detection from MRI images [55].", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "The most common non-parametric novelty detection technique is nearest neighbour [31], which is based on the assumption that normal observations are closer to the training data than novelties [59].", "startOffset": 80, "endOffset": 84}, {"referenceID": 56, "context": "The most common non-parametric novelty detection technique is nearest neighbour [31], which is based on the assumption that normal observations are closer to the training data than novelties [59].", "startOffset": 191, "endOffset": 195}, {"referenceID": 30, "context": "This approach requires a suitable notion of distance between observations [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 51, "context": "A suitable threshold can be chosen experimentally [54].", "startOffset": 50, "endOffset": 54}], "year": 2017, "abstractText": "Multiple instance data are sets or multi-sets of unordered elements. Using metrics or distances for sets, we propose an approach to several multiple instance learning tasks, such as clustering (unsupervised learning), classification (supervised learning), and novelty detection (semi-supervised learning). In particular, we introduce the Optimal Sub-Pattern Assignment metric to multiple instance learning so as to provide versatile design choices. Numerical experiments on both simulated and real data are presented to illustrate the versatility of the proposed solution.", "creator": "LaTeX with hyperref package"}}}