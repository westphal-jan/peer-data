{"id": "1703.09068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Automatic Decomposition of Self-Triggering Kernels of Hawkes Processes", "abstract": "Hawkes Processes capture self- and mutual-excitation between events when the arrival of one event makes future ones more likely to happen in time-series data. Identification of the temporal covariance kernel can reveal the underlying structure to better predict future events. In this paper, we present a new framework to represent time-series events with a composition of self-triggering kernels of Hawkes Processes. That is, the input time-series events are decomposed into multiple Hawkes Processes with heterogeneous kernels. Our automatic decomposition procedure is composed of three main steps: (1) discretized kernel estimation through frequency domain inversion equation associated with the covariance density, (2) greedy kernel decomposition through four base kernels and their combinations (addition and multiplication), and (3) automated report generation. We demonstrate that the new automatic decomposition procedure performs better to predict future events than the existing framework in real-world data.", "histories": [["v1", "Mon, 27 Mar 2017 15:25:54 GMT  (673kb,D)", "https://arxiv.org/abs/1703.09068v1", null], ["v2", "Mon, 17 Jul 2017 04:43:25 GMT  (1751kb,D)", "http://arxiv.org/abs/1703.09068v2", null], ["v3", "Tue, 18 Jul 2017 10:01:49 GMT  (1751kb,D)", "http://arxiv.org/abs/1703.09068v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rafael lima", "jaesik choi"], "accepted": false, "id": "1703.09068"}, "pdf": {"name": "1703.09068.pdf", "metadata": {"source": "CRF", "title": "Automatic Decomposition of Self-Triggering Kernels of Hawkes Processes", "authors": ["Rafael Lima", "Jaesik Choi"], "emails": ["rafael_glima@unist.ac.kr", "jaesik@unist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "In fact, we will be able to go in search of a solution that meets the needs of the people, \"he said in an interview with the German Press Agency."}, {"heading": "2 Hawkes Processes", "text": "A point process representation of a sequence of n-dimensional events is expressed by a vector of the form (t1, t2,..., tn). If the real line is treated as a timeline, the vector can be intuitively associated with a so-called counting process N (t), s.t.: \u2022 dN (t) = 1, if there is an event at the time; \u2022 dN (t) = 0, otherwise. An example of the counting process is shown in Figure 1. A point process can be described by its intensity function (t), which can be shown as the instantaneous expected speed of the arrival of events or the expectation of the derived time N (t). A point process can be described by its intensity function (t), which can be understood as the spontaneous expected speed of the arrival of events, or the expectation of the derived time N (t)."}, {"heading": "2.1 Self-Exciting Kernels", "text": "From the definition of the conditional intensity function, the self-excitation of the process is expressed by the core function \u03c6 (t). For the decomposition of the nucleus, four basic nuclei are used to identify and estimate typical triggering behaviors: \u2022 EXP (\u03b1, \u03b2): The decay exponential behavior of the nucleus is parameterized by the amplitude \u03b1 and decay rate \u03b2 and is useful for modelling a rapid influence of decay, such as in financial or web data [8], in which initial transactions / hyperlinks initially have a lot of impact but gradually diminish their impact over time: EXP (\u03b1, \u03b2) = e \u2212 \u03b2t (6) \u2022 PWL (K, c, p): The power law is parameterized by the amplitude K, the exponent p, and the constant c, as in Equation (7) Coil, and was predominant in earthquakes [18, p) and K (K) as a lower-exposed K (K)."}, {"heading": "3 Automatic Kernel Decomposition for HPs", "text": "This section presents two steps for automatic kernel identification: The steps are (1) non-parametric kernel estimation from discretized covariance and (2) parametric kernel search through our new kernel decomposition scheme. In this article, the discretized kernel estimation is optional if a direct optimization of the kernel structure is possible. Unfortunately, discontinuous functions (SQR, SNS) do not allow such optimization (e.g. gradient descent). In this article, we use the discredited kernel estimation as a unified method for continuous (EXP, PWL), discontinuous cores (SQR, SNS) and especially their combinations. Furthermore, another great advantage of this step over conventional sequential methods is that the value of precision for each value of the individual cores can be calculated independently, while the value of the parameters in gradient decreases is most likely to be the values for this step + 1."}, {"heading": "3.1 Discretized Kernel Estimation", "text": "This step is fully described in [1] and essentially consists of making an estimate of the size of h from empirical measurements of [0, T] and making this estimate in discrete time steps (h). In practice, this estimate is made in constant time steps. (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h). (h)."}, {"heading": "3.2 Greedy Kernel Decomposition", "text": "To express the discretized estimate for the four base cores, the following steps are performed: 2A function ft, f, z is given its Laplace transformation, and the \"?\" symbol corresponds to its conjugate EXP (\u03b1, \u03b2) \u03b1\u03b2 PWL (K, c, p) Kc1 \u2212 pp \u2212 1, (p > 1) SQR (B, L) BL SNS (A, \u03c9, SQR, SNS}) 3: K1 = index _ of _ kernel (min _ residue (fit1) 4: MR1 = min _ residue (fit1) 5: fit2 = fit (kest; K1, {+ EXP, PWL, SQR, SNS}) 3: K1 = index _ of _ kernel (min _ residue (fit1) 4: MR1 = min _ residue (fit1) 5: fit2 = fit (kest; K1, & + EXP, + PWL, PWfitfitresifitdue * (SWR) (SWS): SWR * 1, SWR * 1: SWR * 1: SWR * 1: SWR * 1, SWR: SWR: SWR: SWR * 1, SWR: SWR: SWR: SWR * 1: SWR: SWR: SWR: SWR: SWR: SWR: SWR = 1, SWR: SWR: SWR = 1: SWR: SWR: SWR: SWR: SWR = 1, SWR: SWR = 1: SWR: SWR: SWR = 1, SWR = SWR: SWR = 1: SWR: SWR: SWR: SWR = 1: SWR = 1, F = SWR = SWR: SWR: SWR = 1: SWR = 1, F = SWR: SWR: SWR = 1, (SWR: SWR: SWR: SWR = 1"}, {"heading": "3.3 Stationarity Conditions", "text": "In order to quickly evaluate the validity of the estimated models in terms of the stationarity criterion, we developed closed expressions, either in the form of equality or as an upper limit, which are shown in Table 2 for the case of a single nucleus and Table 3 for multiplicative combinations of two nuclei. 3. The conditions for additive combinations can be easily derived from the conditions for Single3 (\u00b7, \u00b7) if the result of the expression calculated with the estimated parameters belongs to the interval [0,1). This can be justified both from the point of view of the Hawkes process and as a branching process, as well as from the immigrant birth representation [15], and from the limit of the spectral radius (the highest absolute value among the eigenvalues of the 4 matrix)."}, {"heading": "4 Analysis of Higher-order Kernel Decomposition", "text": "Sequential additive decomposition of the discredited estimation vector is quite simple, since one can simply use the residual vector from the previous stages as input for the next ones. In the case of multiplicative decomposition, it is not trivial to find the result of decomposition within the class. To our knowledge, no analysis of the multiplicative HP core decomposition is reported. In this paper, we present a new upper limit above an interclass core product of unknown degree, as in: [EXP] k1 \u00b7 [PWL] k2 \u00b7 [SQR] k3 \u00b7 [SNS] k4for ki-Z, where the operator \"[\u00b7] k\" corresponds to the set of functions that can be decomposed into a core product xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}, {"heading": "4.1 EXP", "text": "[EXP] k1 is reduced to the case of a single exponent with \u03b1 = K1 i = 1 \u03b1i and \u03b2 = K1i = 1 \u03b2i and thus still explains its \"quick decay\" behavior: [EXP] k1 [EXP]; 4For the univariate HP case, the excitation matrix has dimension one, since it is only the excitation function \u03c6 (t)."}, {"heading": "4.2 PWL", "text": "[PWL] k2 is lower delimited by a single PWL core with K = K2 i = 1Ki, c = max (c1,..., ck2) and p = K2i = 1 pi, which is still responsible for its \"slowly decaying\" behavior;"}, {"heading": "4.3 SQR", "text": "[SQR] k3 is reduced to a single SQR core with B = K4 i = 1Bi and L = min (L1,..., Lk4), which is still responsible for its \"constantly triggering\" behavior: [SQR] k3 [SQR];"}, {"heading": "4.4 SNS", "text": "[SNS] k4 has A = K4 i = 1Ai and a \"spikier\" aspect (higher bandwidth) and is therefore still responsible for its \"periodicity-inducing\" behavior. Thus, if we deepen the decomposition algorithm by excessively increasing the number of layers above 2, we could actually add little information about the qualitative aspect of the self-engendering behavior analysis of the data, while making it more vulnerable to revising the sound backdrop of the discredited estimate vectors."}, {"heading": "4.5 Upper Bound", "text": "With regard to the limits of the higher-ranking decomposition types, the exact results for EXP and SQR decompositions within the class and the upper limits for the PWL and SNS decomposition classes result in the following: [EXP] k1 \u00b7 [PWL] k2 \u00b7 [SQR] k3 \u00b7 [SNS] k4 \u2264 \u03b1e \u2212 \u03b2x K (x + Cupper) p BAsin (\u03c9x) \u2264 \u03b1BKAe \u2212 \u03b2x (x + Cupper) p = EXP (\u03b1, \u03b2) \u00b7 PWL (K, Cupper, p) k2 \u00b7 SQR (B, L) \u00b7 A, for 0 \u2264 x \u2264 min (L, \u03b5) and 0 otherwise."}, {"heading": "5 Automated Report Generation", "text": "After estimating the probability-maximizing base core combination, a LaTeX file report is prepared, the sample of which is included in the appendix, with a description of the relevant parameters and their most relevant aspects, such as: \u2022 Decay rate associated with decaying cores, EXP, and PWL. An exponential decay rate is described as \"a rapidly decaying trigger effect,\" and performance decay can be described as \"a slowly decaying trigger effect,\" with its respective parameters presented as evidence. \u2022 Steady rate influence corresponding to an offset level of the SQR core type. It is described as \"a continuously triggering influence,\" using parameter B as evidence. \u2022 Triggering horizon associated with discontinuous cores, SNS, and SQR. It is described as \"a horizon of the triggering influence,\" with the parameter L, in the QR case, flowing into the SNS case."}, {"heading": "6 Related Work", "text": "Recently, the spectral method has been extended to non-parametric kernel estimates for symmetrically networked HPs. [1] A likelihood maximization method has been used on a class of parametric nuclei for the excitation matrices in [19], while a number of papers on probability maximization methods for power-oriented cores in seismology and earthquake data modeling have been compiled in [18]. However, in [2] it is argued that these methods, which are designed for univariate Hawkes processes, can hardly be used to process large amounts of data where the core function is not well localized compared to the exogenous time between events. Subsequently, a comprehensive analysis of spectral methods for non-parametric kernel exposures in relation to networked HPs will be performed."}, {"heading": "7 Experimental Results", "text": "To test the validity of the core decomposition framework, we conducted experiments with synthetic, financial, and earthquake data. Synthetic data sequences were used to evaluate the effectiveness of a scale independence strategy to determine the maximum value of \u03c4 in Equation 11. In the financial data experiments, quantitative comparisons were made between the first and second levels of the decomposition framework over a set of time sequences, as well as between our decomposition framework and an estimation method described in [19]. In the earthquake data experiments, we evaluated the overall response to earthquakes based on the relative frequency of each estimated core type outside the four base cores. No prior information on the core (type and parameters) is available for real data sets. Therefore, we use the probability of the core function protocol over time sequence as a quality criterion."}, {"heading": "7.1 Synthetic Data", "text": "As a means of evaluating the overall precision and efficiency of the core decomposition framework, experiments were conducted with synthetic data from the corresponding four basic core types. Hawkes-related simulation algorithms, from which synthetic time sequences can be derived, are divided into two main categories: cluster-based [17] and intensity-based [20]. For the proposed experiments, the \"Thinning\" algorithm, an intensity-based algorithm, was used, mainly because the parametric representations of the core types make it very convenient to accurately calculate the value of the intensity function throughout the simulation horizon. The related algorithm, whose full sequence is \"Ogata's Modified Thinning Algorithm,\" essentially consists of simulating time sequences that use an exceedingly high constant value for the intensity function and then thinning the generators that refer to the actual sampling-locally, using contradictory values."}, {"heading": "7.2 Financial Data", "text": "In fact, the fact is that most of them will be able to be in the position in which they are able to move."}, {"heading": "7.3 Earthquake Data", "text": "In lists regularly published by the seismological services of most countries that have become accustomed to the occurrence of earthquakes, hardly any aspect of the overall behavior of Q01 has been taken into account. Relevant information can be obtained, such as the epicenter of each shock, the depth of focus, the instrumental size and the time of origin of each earthquake. Statistical analyses of earthquake catalogs and the assessment of earthquake risk in a geophysical area can then be performed by using parametric models of the sequences of origin time obtained by largely ignoring the remaining information. Further analysis of these adapted models can then identify and dissect components such as evolutionary trends, periodicity and clustering [18]. Also, the need to insert scale independence in the form of the histogram-related heuristics mentioned above becomes imperative in the decomposition framework, as earthquake events are separated by interval scales or monthly."}, {"heading": "8 Conclusion", "text": "In order to predict future events with HP, a suitable kernel is manually selected beforehand. In this paper, we proposed a new method of covariance-based core decomposition to illustrate various self-exciting behaviors. We also presented a model (structure / parameter) learning algorithm for selecting the best HP kernel in the face of temporally discrete events. Stationary conditions were derived to guarantee the validity of the kernel learning algorithm. In experiments, we demonstrated that the proposed algorithm works better than existing methods for predicting future events by automatically selecting cores."}, {"heading": "A Derivations of Stationarity Criteria for Multiplicative Combinations of Kernels", "text": "This appendix presents the complete derivatives of the stationary criteria for the multiplicative composition of the four second order base nuclei."}, {"heading": "A.1 EXP x EXP", "text": "For the combination \"EXPxEXP\" we have the following to achieve the stationarity: 0 \u2264 0 EXP (\u03b11, \u03b21) EXP (\u03b12, \u03b22) dx < 10 \u2264 0 \u03b11e \u03b11x\u03b12e \u03b22xdx < 1Thus: 0 \u03b11e \u2212 \u03b21x\u03b12e \u2212 \u03b22xdx = \u0438 \u221e 0 (\u03b11\u03b12) e \u2212 (\u03b21 + \u03b22) xdx = \u0432 \u02da 0 \u03b1e \u2212 \u03b2xdx = \u03b1 \u03b2 = \u03b11\u03b12 \u03b21 + \u03b22So this case is reduced to the case of a single exponent."}, {"heading": "A.2 EXP x PWL", "text": "For the combination \"EXPxPWL\" we have this so that stationarity can be achieved: 0 \u2264 0 EXP (\u03b1, \u03b2) PWL (K, c, p) dx < 10 \u2264 0 \u03b1e \u2212 \u03b2x K (x + c) p dx < 1So: 0 \u03b1e \u2212 \u03b2x K (x + c) p dx = \u03b1K \u00b2 0 (x + c) \u2212 pe \u2212 \u03b2xdx = \u03b1Ke\u03b2c \u00b2 0 (x + c) \u2212 pe \u2212 \u03b2 (x + c) dx = \u03b1Ke\u03b2c\u03b2p \u00b2 0 (\u03b2 (x + c) \u2212 pe \u2212 \u03b2 (x + c) dx = \u03b1Ke\u03b2c\u03b2p \u2212 pe \u2212 tdt = \u03b1Ke\u03b2c\u03b2p \u2212 1b (1 \u2212 p, \u03b2c), whereby the known incomplete gamma function is: a (a, y) = \u2012 t \u2212."}, {"heading": "A.3 EXP x SQR", "text": "For the combination \"EXPxSQR\" we have the following to achieve stationarity: 0 \u2264 + 0 EXP (\u03b1, \u03b2) SQR (B, L) dx < 10 \u2264 + L 0 \u03b1Be \u2212 \u03b2xdx < 1Also: \u2022 L 0 \u03b1Be \u2212 \u03b2xdx = [\u03b1Be \u2212 \u03b2x \u03b2] L 0 = \u03b1B (1 \u2212 e \u2212 \u03b2L) \u03b2Thus, in the case of a multiplicative combination, the SQR core acts as truncation horizon."}, {"heading": "A.4 EXP x SNS", "text": "For the combination \"EXPxSNS\" we have this so that stationarity can be achieved: 0 \u2264 \u00b2 0 EXP (\u03b1, \u03b2) SNS (A, \u03c9) dx < 10 \u2264 \u00b2 \u00b2 \u03c90 A\u03b1e \u2212 \u03b2xsin (\u03c9x) dx < 1Wo: \u00b2 \u00b2 \u00b2 0 A\u03b1e \u2212 \u03b2xsin (\u03c9x) dx = \u00b2 \u00b2 \u00b2 0 A\u03b1e \u2212 \u03b2xei\u03c9x \u2212 e \u2212 i\u03c9x2i dx = A\u03b12i [e (\u2212 \u03b2 + i\u03c9) x \u2212 \u03b2 + i\u03c9) x \u00b1 2 \u00b1 2 \u00b2 \u00b2 (\u03b52 \u00b2) x \u2212 \u03b2 \u2212 i\u03c9] GDP 0 = A\u03b12i [(\u2212 \u03b2 \u2212 i\u03c9) e (\u2212 \u03b2 + i\u03c9) e (\u2212 \u03b2 + i\u03c9) x\u03b22 \u00b1 2 \u00b1 2 \u00b2 (\u03b52 \u00b2)"}, {"heading": "A.5 PWL x PWL", "text": "In the case of the combination \"PWLxPWL,\" an upper limit is derived as follows: 0 \u2264 0 PWL (K1, c1, p1) PWL (K2, c2, p2) dx < 10 \u2264 0 K1 (x + c1) p1 K2 (x + c2) p2 dx < 1Then: 0 K1 (x + c1) p1 K2 (x + c2) p2 dx \u2264 0 K1K2 (x + min (c1, c2)) p1 + p2 dx = K1K2 (p1 + p2 \u2212 1) min (c1, c2) (p1 + p2 \u2212 1)"}, {"heading": "A.6 PWL x SQR", "text": "For the combination \"PWLxSQR,\" we have this so that stationarity can be achieved: 0 \u2264 0 PWL (K, c, p) SQR (B, L) dx < 10 \u2264 VP 0 KB (x + c) p dx < 1Wo: A L 0 KB (x + c) p dx = [KB (1 \u2212 p) (x + c) (p \u2212 1)]] L 0 = KB (c \u2212 (p \u2212 1) \u2212 (c + L) \u2212 (p \u2212 1) p \u2212 1Thus, the SQR kernel once again functions as truncation horizon."}, {"heading": "A.7 PWL x SNS", "text": "In the case of the combination \"PWLxSNS,\" an upper limit is derived as follows: 0 \u2264 0 PWL (K, c, p) SNS (A, \u03c9) dx < 10 \u2264 \u0445 \u03c0 \u03c90KAsin (\u03c9x) (x + c) p dx < 1Wo: \u03c0 \u03c90KAsin (\u03c9x) (x + c) p dx \u2264 \u03c0 \u03c90KA (x + c) p dx = [KA (1 \u2212 p) (x + c) (p \u2212 1)] \u03c0 \u03c90 = KA (((((c + \u03c0\u043e) 1 \u2212 p) 1 \u2212 p) 1 \u2212 p"}, {"heading": "A.8 SQR x SQR", "text": "For the combination \"SQRxSQR\" we have the following to achieve the stationarity: 0 \u2264 \u0445 \u221e 0 SQR (B1, L1) SQR (B2, L2) dx < 10 \u2264 \u0445 min (L1, L2) 0 B1B2dx < 1Wo: \u0435min (L1, L2) 0 B1B2dx = B1B2min (L1, L2) = BLThus the multiplicative combination of two SQR cores can be reduced to the case of a single SQR core."}, {"heading": "A.9 SQR x SNS", "text": "In the case of combinations of discontinuous cores (SQR and SNS), we assume that they have the same start and end point, i.e., L = \u03c0\u03c9. For the combination \"SQRxSNS,\" we therefore have this point to achieve stationarity: 0 \u2264 \u0445 \u221e 0 SQR (B, L) SNS (A, \u03c9) dx < 10 \u2264 \u0432\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043dnig (A) (A) combination of \"SQR and SNS), we have the same SQR and SNS combination of SQR and SNS, we have the same SQR (B, L, L, L) SNS, SNS, SNS, SNS, SNS, SNS, SNS, SNS, SNS, SNS, SNS, SNS, SNS, SNS,"}, {"heading": "A.10 SNS x SNS", "text": "In the case of combinations of discontinuous nuclei (SQR and SNS), we assume that they have the same start and end point. Thus, for the combination \"SNSxSNS,\" we have this point so that stationarity can be achieved: 0 \u2264 \u221e \u221e 0 SNS (A1, \u03c9) SNS (A2, \u03c9) dx < 10 http: / / www.SNSxSNs.com / http: / / www.SNS.com / http: / / www.SNSxSNs.com / http: / / www.SNSxSNs.com / http: / www.SNSxSNs.com / http: / / www.SNSxSNs.com / http: / www.SNSxSNs.com / http: / / www.SNSxSNs.com / http: / www.SNSxSNs.com."}, {"heading": "B Derivation of the Log-likelihood formula for HPs", "text": "This derivative follows the steps of [11]. In the face of a realization (t1, t2,..., tk) of a regular point process observed over the interval (0, T), the log probability is expressed as: l = k = 1 log (\u03bb (ti)) \u2212 T 0 \u03bb (u) duProof. If we leave the common probability density of the realization to: L = f (t1, t2,..., tk) = k = k (ti) It can be written in relation to the conditional intensity function. We can then write f in terms such as: \u03bb (t) = f (t) 1 \u2212 F (t) = dF (t) dt 1 \u2212 F (t) = \u2212 d log (1 \u2212 F (t))) dt, with respect to the history until the last arrival u, H (u), F (t), F (t), F (t) which are defined as conditional cumulative probability distribution of the next arrival ti \u2212 T (t) = dF (t): F (t) dt 1 \u2212 F (t) = \u2212 d log (1 \u2212 F (t) (t)))) dt dt (t (t (t))) dt, dt (1 \u2212 F (t) dt) dt, where we have a k (t) dt with respect to the last arrival u (ti), H (u (u), H (t), H (t), F (t), F (t), f (t (t) f (t), ti \u2212 F (t), ti \u2212 F (t (t), f (t) dt (t), f (t (t) dt (t), f (t (t), f (t (t), f (t, f (t) f (t), f (t (t), f (t (t), f (t, f (t), f (t (t), f (t (t) f (t), f (d, f (t), f (d, f (d, f (t), k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Hawkes Processes capture selfand mutual-excitation between events<lb>when the arrival of one event makes future ones more likely to happen in<lb>time-series data. Identification of the temporal covariance kernel can reveal<lb>the underlying structure to better predict future events. In this paper, we<lb>present a new framework to represent time-series events with a composition<lb>of self-triggering kernels of Hawkes Processes. Our automatic decomposition<lb>procedure is composed of three main steps: (1) discretized kernel estimation<lb>through frequency domain inversion equation associated with the covariance<lb>density when a direct parameter optimization is not possible, (2) greedy kernel<lb>decomposition through four base kernels and their combinations (addition<lb>and multiplication), and (3) automated report generation. In addition, we<lb>report the first multiplicative kernel compositions along with stationarity<lb>conditions for Hawkes Processes. We demonstrate that the new automatic<lb>kernel decomposition procedure performs better to predict future events than<lb>the existing framework in real-world data.", "creator": "LaTeX with hyperref package"}}}