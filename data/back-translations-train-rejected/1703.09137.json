{"id": "1703.09137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Where to put the Image in an Image Caption Generator", "abstract": "When a neural language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in a recurrent neural network -- conditioning the language model by injecting image features -- or in a layer following the recurrent neural network -- conditioning the language model by merging the image features. While merging implies that visual features are bound at the end of the caption generation process, injecting can bind the visual features at a variety stages. In this paper we empirically show that late binding is superior to early binding in terms of different evaluation metrics. This suggests that the different modalities (visual and linguistic) for caption generation should not be jointly encoded by the RNN; rather, the multimodal integration should be delayed to a subsequent stage. Furthermore, this suggests that recurrent neural networks should not be viewed as actually generating text, but only as encoding it for prediction in a subsequent layer.", "histories": [["v1", "Mon, 27 Mar 2017 15:13:49 GMT  (116kb,D)", "http://arxiv.org/abs/1703.09137v1", "under review, 29 pages, 5 figures, 6 tables"]], "COMMENTS": "under review, 29 pages, 5 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.CV", "authors": ["marc tanti", "albert gatt", "kenneth p camilleri university of malta)"], "accepted": false, "id": "1703.09137"}, "pdf": {"name": "1703.09137.pdf", "metadata": {"source": "CRF", "title": "Where to put the Image in an Image Caption Generator\u2217", "authors": ["Marc Tanti", "Albert Gatt"], "emails": ["marc.tanti.06@um.edu.mt", "albert.gatt@um.edu.mt", "kenneth.camilleri@um.edu.mt"], "sections": [{"heading": null, "text": "* The research contained in this publication is partly financed by the Endeavour Scholarship Scheme (Malta). Scholarships are partly financed by the European Union - European Social Fund (ESF) - Operational Programme II Cohesion Policy 2014-2020 Investing in human capital to create more opportunities and promote the well-being of society.ar Xiv: 170 3.09 137v 1"}, {"heading": "1 Introduction", "text": "Caption generation1 is the task of generating a natural language description of the content of an image. (Bernardi et al., 2016) One way to do this is to use a neural language model, a neural network, that generates a sentence word by word. This work, using a recursive neural network (RNN) that predicts the next word in a sentence, based on its prefix or its \"history.\" This predicted word can then be appended to the previous prefix to predict the word thereafter, and so on until an entire sentence is generated. A simple neural language model can be expanded into a caption generator by conditioning predictions about image characteristics. In other words, the language model takes as input not only the prefix, but also the image that is put into it. This raises the question at what stage image information should be introduced into the language model we introduce image information."}, {"heading": "2 Background", "text": "In this section, we discuss a number of current caption generation models, focusing on how the image determines the neural language model, based on the distinction between inject and merge architectures shown in Figure 1. Before discussing these models, we will first outline the view of the RNs as used in the language modeling that will be used later in this post, providing a useful background to understand the purpose of the consistently used illustrations."}, {"heading": "2.1 Ways of viewing recurrent neural networks", "text": "Traditionally, neural language models are represented as in Figure 2a, where strings are considered continuously generated. At the end of each time step, a new word is generated, combining the state of the RNN with the last word generated to generate the next word. We are talking here about the \"continuous view.\" In this essay, we take a slightly different - though functionally identical - perspective on RNN and its role in language models, as illustrated in Figure 2b. We propose to view the RNN as a series of discontinuous snapshots over time, each word being generated from the entire prefix of previous words, and the state of the RNN being initialized each time. We speak of the \"discontinuous view.\" The discontinuous perspective brings to light some similarities between RNN as used in language modeling and the coding of RNNN in sequence models."}, {"heading": "2.2 Types of architectures", "text": "In Section 1, we made a high-level distinction between architectures that merge linguistic and image functions in a multimodal layer, and those that inject image functions directly into the caption prefix encoding process. As we noted, merge architectures tend to integrate image functions relatively late after linguistic strings are encoded. In contrast, the injection architecture is illustrated in a number of different ways in the literature that bind image functions at different times, as illustrated in Figure 3 and below: \u2022 Init-Inject: The initial state of the RNN is set to be the image vector (or a vector derived from the image vector). This is an early binding architecture to inject: The first input to the RNN is the image vector (or a vector derived from the image vector)."}, {"heading": "2.2.1 Init-inject architectures", "text": "These include (Devlin et al., 2015) using a gated recurrent unit (GRU) model (Chung et al., 2014) initialized with an image vector, which was compared to a maximum entropy model (ME) that maps a bag of visual words extracted from an image to the most likely complete caption (Ma et al., 2016), extracting in a related vein (Sutskever et al., 2014) a sequence of attributes from an image and then assigning them to a caption using a long-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), in an encoder decoder framework a la (Sutskever et al., 2014), and (Wang et al., 2016) combining a simple RNN and an LSTM Er in an architecture and keeping them independent until their vectors are merged by weighted sum."}, {"heading": "2.2.2 Pre-inject architectures", "text": "The pre-injection models treat the image as if it were the first word in the prefix during the training. (Vinyals et al., 2015) In this way, they use an LSTM language model and find that the pre-injection delivers better results than the par injection (described below). Their hypothesis is that this is because the par injection overwhelms the neural network with the image noise that is always present with every word in the prefix.Instead of injecting a low-level image vector (Wu et al., 2015), they opt for a higher-level approach by extracting attributes from an image and passing them as input to the image signature generator. These attributes vector are passed to the RNN by pre-injection. (Krause et al., 2016) generate attributes at a higher level by extracting attributes from the image and passing them as input to the image signature generator."}, {"heading": "2.2.3 Par-inject architectures", "text": "Par-Injection combines image features and word attributes when passed to the RNN. In two papers, for example, Chen and colleagues (Chen and Zitnick, 2014, 2015) approach caption generation with two simple RNN in parallel. One RNN is trained to predict the image vector based on the caption prefix, while the other RNN is trained to predict the next word based on the caption prefix, the image vector, and the first RNN. The first RNN \"remembers the visual information about the caption prefix; this provides a more stable memory than a simple RNN status. Similarly, (Donahue et al., 2015) describes an image and video signature system that places two LSTMs in a row. However, it was found that injecting the image into the second (later) LSTM works better than injecting the first vector (carpaet) file injection in 2015."}, {"heading": "2.2.4 Merge architectures", "text": "Instead of combining image attributes with linguistic attributes within the RNN, merger architectures delay their combination until after the vectorization of the caption prefix. Exponents of this approach include the work of (Kiros et al., 2014a), which uses a log-bilinear language model (LBL) (Mnih and Hinton, 2007) to convert a caption prefix into a single vector that will later merge with the caption vector to determine the next word in the caption. Later work (Kiros et al., 2014b) describes a caption generator that can be conditioned either on images or on full captions. This is because both image and caption vectors are embedded in a common multimodal space that maximizes the similarity between corresponding images and captions. An image vector or next caption vector is then merged with the 2014L."}, {"heading": "2.2.5 Mixed models", "text": "There is a growing amount of work that has explored a combination of different injection and / or merge strategies, most of which are attention-based models, such as in the work of (Xu et al., 2015), which describes an attention-based caption generator, where an LSTM is initialized with the full image and the visited image is passed to the LSTM by Par injection, creating a new state. This new state is then re-merged with the visited image to determine the next word. Thus, the architecture takes into account aspects of the injection, Par injection and merge strategies. (Lu et al., 2016) describe an attention-based model that also defines the meaning that should be given to an image during generation. If a caption needs to generate the compound word \"stop sign,\" the architecture then only requires access to the image, pair injection and merge strategies."}, {"heading": "2.3 Summary and outlook", "text": "While caption generation literature now offers a rich palette of models and comparative ratings, there has been very little explicit systematic comparison between the performance of the architectures covered above, each of which represents a different way of conditioning the prediction of speech sequences on visual information. Work that has tested both Par-Inject and Pre-Inject, such as (Vinyals et al., 2015), reports that Pre-Inject works better. (Mao et al., 2015a) compares injecting and merging architectures and concludes that merging is better than injection. However, comparison between architectures is a relatively tangential part of their overall assessment and is based only on the BLEU metric (Papineni et al., 2002). Answering the question of which architecture is the best is difficult because different architectures score differently on different rating scales, such as Wang et al, who uses NM's and NM's, although NM's are complex, and NM's are not."}, {"heading": "3 Architectures", "text": "In this section, we will discuss the various architectures evaluated in this essay. A diagram illustrating the three main architectural types is in Figure 4. We begin with a description of a general, schematic architecture in Section 3.1, followed by a description of how each rated architecture modifies this scheme in Section 3.2. Finally, we will give a formal description of the important details of the architectures in Section 3.3."}, {"heading": "3.1 General architecture", "text": "This section describes the basis on which all evaluated architectures are derived. It is a scaled-down version of the architecture described in (Vinyals et al., 2015). This architecture was chosen for its simplicity, while still being the most powerful system in the 2015 MSCOCO caption Challenge.3Word embeddings Word embeddings, that is, the vectors that represent known words before they are fed to the RNN, consist of 256 element vectors randomly initialized. No precompiled vector embeddings such as word2vec (Mikolov et al., 2013) were used. Instead, the embeddings are trained as part of the neural network to learn the best representations of words.Recurring neural network The purpose of the RNN network is a prefix of embedded words (with embedded words 201M) and the architecture creates a single vector representing the vector."}, {"heading": "3.2 Evaluated architectures", "text": "This section describes each architecture evaluated in our experiments. There is a language model architecture (long model), three merge architectures (merge), and four inject architectures (inject), each of which can use either a single RNN (srnn) or an LSTM (lstm) as a recursive neural network. Therefore, we evaluate a total of (8 x 2) 16 different architectures."}, {"heading": "3.2.1 The language model architecture", "text": "Language Model Architecture (langmodel) is a simple, unconditioned, imaginary language model that predicts the next word in a caption with only a prefix. It is used as a basis to measure the effectiveness of various methods of incorporating an image into an architecture."}, {"heading": "3.2.2 The merge architectures", "text": "The merge architectures merge the prefix vector with the image vector before passing the result to the output layer. We consider three ways to merge the image vector with the prefix vector: \u2022 merge-concat: The image vector and prefix vector are merged into a single vector that corresponds to the sum of the length of the two vectors. Although this method is intuitive, we take into account the additional parameters that this architecture must have in order to process a larger layer. \u2022 merge-add: Image vector and prefix vector are merged into a single vector that has a length that corresponds to the original vectors (image vector and prefix vector have the same length). \u2022 merge-mult: Image vector and prefix vector are elementally multiplied into a single vector that has a length (image vector and prefix vector have the same length)."}, {"heading": "3.2.3 The inject architectures", "text": "The injection architecture injects the image vector into the RNN as if it were part of the caption prefix. As mentioned in section 2, there are four ways to inject the image into the RNN, which are realized in our experiments as follows: \u2022 inject-init: The image vector is treated as the initial state for the RNN. After initializing the RNN, the vectors in the caption are then transferred to the RNN as usual. \u2022 inject-init: Any other architecture in our experiments uses the allzeros vector as the initial state. \u2022 inject-pre: The image vector is used as the first \"word\" in the caption, making the image vector the first thing the RNN will see. \u2022 inject-par: The image vector is added (elementary) to each word vector in the caption to make the RNN a mixed word image vector. Each word would have added the exact same image vector."}, {"heading": "3.3 Formal details", "text": "This section gives a more formal description of the weighted architectures. In the sense of notation, we treat vectors as horizontal. The simple RNN, which is the same as in (Mao et al., 2015a), is defined assn = ReLU (xn + sn \u2212 1Wss + bs) (1), where xn is the n th input, sn is the hidden state after n inputs, s0 is the initial state, Wss is the state-to-state weight matrix (which is square), bs is the state bias vector, and ReLU refers to the rectified linear unit function which is defined as ReLU (x) = max (0, x) (2) The LSTM model which is the vector used in (Vinyals et al al, 2015) is the state bias vector, and ReLU refers to the rectified linear unit function which is defined as ReLU (x) = max (0, x) (the vector used in (2015) is \u2212 n = max, n), the state bias vector, and ReLU refers to the rectified linear unit function which is assigned as a (x)."}, {"heading": "4 Experiments", "text": "This section describes the experiments performed to compare the performance of the various architectures described in the previous section. A summary of all parameters and configurations is presented in Table 2."}, {"heading": "4.1 Dataset", "text": "Images The data set used for all the experiments was the version of Flickr30k4 (Young et al., 2014), distributed by (Karpathy and Fei-Fei, 2015) 5, a set of 31,014 images from Flickr combined with five manually written captions per image. The data set provided is divided into a training, validation and test set of 29,000, 1,014 and 1,000 images, respectively. The images are already vectorized into 4,096 element vectors via the activation of the layer \"fc7\" (the penultimate layer) of the VGG OxfordNet 19 layer Convolutionary Neural Networks (Simonyan and Zisserman, 2014), which was trained for object recognition on the ImageNet dataset (Deng et al., 2009). These vectors were normalized to support the unit-length vectors of vectors prior to the training.Vocabulary The known vocabulary consists of all the image words in the captions."}, {"heading": "4.2 Training set", "text": "In Section 2, we have pointed out that it is possible to adopt a continuous or discontinuous perspective on an RNN. For the experiments described here, the latter perspective proves advantageous, as it naturally allows for uniform treatment of all eligible models. Specifically, the formation of a postinject model requires us to inject the image onto a specific prefix at each stage of the training. Therefore, captions are divided into separate prefixes with increasing length during our experiments, and each prefix is treated as a separate entry in the training set. The training goal is to maximize the individual probability of each word in all captions (including the END token), given the image and caption prefix. The training set created from the data set consists of triples < I, C0... n, Cn + 1 >, whereby I am an image described by Capture C, C0... n is the capture-fix with the word START and the following words are all subscribed with the caption."}, {"heading": "4.3 Learning", "text": "We did not focus too much on optimizing hyperparameters, as it is difficult to find a set of hyperparameters that benefit all architectures at the same time, and we did not want one architecture to have an advantage over the other. Therefore, we used default or generally recommended settings were possible. Cost Function As a cost function, the sum of the crossentropy of all < I, C0... n, Cn + 1 > triples (symbols defined in Section 4.2) is used in the Minibatch training set, which means that neural networks are trained to minimize the crossentropy of their probabilities on the training sets."}, {"heading": "4.4 Evaluation metrics", "text": "To evaluate the different architectures, the captions of the test sets (which are shared among all experiments) are used to measure the quality of the architectures using metrics divided into four classes, which are described below."}, {"heading": "4.4.1 Probability metrics", "text": "The predicted helplessness of a sentence is calculated as: helplessness (P, C, I) = 2H (P, C, I) (12) H (P, C, I) = \u2212 1 | C | | \u2211 n = 0 log2 P (Cn + 1 | C0... n, I) (13), where P is the trained neural network that indicates the probability that a particular word is the next word in a caption, C is a caption with | C | words, I is an image described by caption C, and H is the entropy function. Note that Cn is the ninth word in C and C0... n is the first n words in C (with START characters). To aggregate the headings diversity of the entire test sentence in a single number, we mean all values of methometry."}, {"heading": "4.4.2 Generation metrics", "text": "These metrics quantify the quality of the captions generated by measuring the degree of overlap between captions generated and those in the test kit. We use the MSCOCO Rating Code6, which measures the standard rating metrics BLEU- (1,2,3,4) (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDER (Vedantam et al., 2015) and ROUGE-L (Lin and Och, 2004). Captions were generated using beam search with a beam width of 40 and a truncated maximum length of 50 words.6https: / / github.com / tylin / coco-caption"}, {"heading": "4.4.3 Diversity metrics", "text": "In addition to measuring the similarity of captions to the basic truth, we also measure the variety of vocabulary used in the captions generated, to determine the extent to which captions are \"stereotypical,\" that is, the extent to which a model reuses (partial) strings on a case-by-case basis, regardless of the image entered. Let's consider as a limiting case a captions generator that always returns the same captions, which has the least possible variety. To quantify this, we measure the percentage of known vocabulary words used in all captions generated, and the entropy of the unigram and bigram frequencies in all captions generated, which is calculated as: Diversity (P, F) = 1 Pi (F) log2 Pi (F) (14) log2 Pi (F) (F) (F) = Fj (15)."}, {"heading": "4.4.4 Retrieval metrics", "text": "As mentioned in Section 2, we consider image recovery performance to be one of our benchmarks, given the common literature practice of evaluating caption models bidirectionally, both for generation and recovery. In this case, we are primarily interested in whether the various eligible architectures are evaluated equally in the two task areas. Retrieval key indicators are key indicators that quantify how well the architectures are able to retrieve the correct image with a caption. A conditional language model can be used for the query by measuring the degree of relevance that each image has for the specified caption. Relevance is measured as the probability of the entire caption (by multiplying the probability of each word). Different images result in different probabilities for the same caption. The more likely the caption is, the more relevant the image. We use the standard R @ n recall dimensions (Hodosh list, 2013, Recosal, and Recosal list 10.), and the number of images in the caption."}, {"heading": "5 Results", "text": "Three runs of each experiment were performed, and the mean along with the standard deviation (in brackets) of the different yardsticks across the three runs is given. For each run, the initial weight settings, the minibatch selection, and the dropout selection are different, as these are randomly determined; everything else is identical between the runs. By way of comparison, the architectures used in (Mao et al., 2015a) and (Vinyals et al., 2015) are also included in the tables below. Both approaches have been influential in the literature; furthermore, they are explicitly defined and can be easily re-implemented; for the purposes of the present experiments, they have been newly implemented using the same layers and layer sizes as in the original; however, to ensure a fair comparison, we have trained and tested them in the same way as the other architectures considered here, meaning that we have the same initialization, regulation, and training methods as the authors occasionally differ from the original results for this reason reported below."}, {"heading": "5.1 Data", "text": "Table 3 shows the results of the evaluation using probability metrics, which consist of various ways to aggregate helplessness over captions in the test set. Table 4 shows measures that measure the quality of captions generated. Note that the imaginary language model here generates a caption that is the most likely caption overall, and uses this for all captions. Table 5 shows the extent to which models utilize a significant share of the available vocabulary. We estimate the proportion of the available vocabulary (unicab or bigram) used by a model. For comparison, we include the proportions of human captions in the test set, taking into account both an overall share based on all five captions per test picture (human-all) and a ratio based on only the first caption from the five available human captions (human-one).Finally, Table 6 shows the results of the evaluation of the models based on a \"reverse\" of the image captioning process used in the test set."}, {"heading": "5.2 Discussion", "text": "If we take the late binding architectures, merge and subsequently inject, and the early binding architectures, init-inject and pre-inject, as two groups, then there is a clearly discernible pattern for both models that use a simple RNN and those that use an LSTM: Given the same type of RNN, late binding architectures are better than early binding architectures with mixed binding architectures (parinject) floating somewhere in the middle. This is generally true of all valuation metrics, including retrieval. This result is somewhat surprising, since the LSTM is a re-implementation of the one described by (Vinyals et al., 2015) optimized for a pre-injection architecture. In fact, there does not seem to be an evaluation criterion where early binding architectures have an advantage over late binding."}, {"heading": "6 Conclusion", "text": "In fact, the fact is that most of them are able to move to another world, in which they are able, in which they are able to move, in which they are able to move."}], "references": [{"title": "SPICE: Semantic Propositional Image Caption Evaluation, pages 382\u2013398", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "Springer International Publishing, Cham.", "citeRegEx": "Anderson et al\\.,? 2016", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, volume 29, pages 65\u201372.", "citeRegEx": "Banerjee and Lavie,? 2005", "shortCiteRegEx": "Banerjee and Lavie", "year": 2005}, {"title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "Journal of Artificial Intelligence Research, 55:409\u2013442.", "citeRegEx": "Bernardi et al\\.,? 2016", "shortCiteRegEx": "Bernardi et al\\.", "year": 2016}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CoRR, abs/1411.5654.", "citeRegEx": "Chen and Zitnick,? 2014", "shortCiteRegEx": "Chen and Zitnick", "year": 2014}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Chen and Zitnick,? 2015", "shortCiteRegEx": "Chen and Zitnick", "year": 2015}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J.", "K. Li", "L. Fei-Fei"], "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition. Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Language Models for Image Captioning: The Quirks and What Works", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "CoRR, abs/1505.01809.", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Aistats, volume 9, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "The symbol grounding problem", "author": ["S. Harnad"], "venue": null, "citeRegEx": "Harnad,? \\Q1990\\E", "shortCiteRegEx": "Harnad", "year": 1990}, {"title": "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data", "author": ["L.A. Hendricks", "S. Venugopalan", "M. Rohrbach", "R. Mooney", "K. Saenko", "T. Darrell"], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Hendricks et al\\.,? 2016", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Image Representations and New Domains in Neural Image Captioning", "author": ["J. Hessel", "N. Savva", "M.J. Wilber"], "venue": "CoRR, abs/1508.02091.", "citeRegEx": "Hessel et al\\.,? 2015", "shortCiteRegEx": "Hessel et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, 47(1):853\u2013899. Flickr8k.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Karpathy and Fei.Fei,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2015}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Proceedings of The 31st International Conference on Machine Learning, page 595603.", "citeRegEx": "Kiros et al\\.,? 2014a", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014b", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A hierarchical approach for generating descriptive image paragraphs", "author": ["J. Krause", "J. Johnson", "R. Krishna", "L. Fei-Fei"], "venue": "ArXiv.", "citeRegEx": "Krause et al\\.,? 2016", "shortCiteRegEx": "Krause et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Lin", "C.-Y.", "F.J. Och"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics - ACL \u201904. Association for Computational Linguistics (ACL).", "citeRegEx": "Lin et al\\.,? 2004", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin", "T.-Y.", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision \u2013 ECCV 2014, pages 740\u2013755. Springer Nature.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Optimization of image description metrics using policy gradient methods", "author": ["S. Liu", "Z. Zhu", "N. Ye", "S. Guadarrama", "K. Murphy"], "venue": "CoRR, abs/1612.00370.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Knowing when to look: Adaptive attention via A visual sentinel for image captioning", "author": ["J. Lu", "C. Xiong", "D. Parikh", "R. Socher"], "venue": "CoRR, abs/1612.01887. 26", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Describing images by feeding LSTM with structural words", "author": ["S. Ma", "Y. Han"], "venue": "2016 IEEE International Conference on Multimedia and Expo (ICME). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Ma and Han,? 2016", "shortCiteRegEx": "Ma and Han", "year": 2016}, {"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR.", "citeRegEx": "Mao et al\\.,? 2015a", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile. Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Mao et al\\.,? 2015b", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning - ICML \u201907. Association for Computing Machinery (ACM).", "citeRegEx": "Mnih and Hinton,? 2007", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Simplified LSTM unit and search space probability exploration for image description", "author": ["O. Nina", "A. Rodriguez"], "venue": "2015 10th International Conference on Information, Communications and Signal Processing (ICICS). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Nina and Rodriguez,? 2015", "shortCiteRegEx": "Nina and Rodriguez", "year": 2015}, {"title": "Image description through fusion based recurrent multi-modal learning", "author": ["R.M. Oruganti", "S. Sah", "S. Pillai", "R. Ptucha"], "venue": "2016 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Oruganti et al\\.,? 2016", "shortCiteRegEx": "Oruganti et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Understanding the exploding gradient problem", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Computing Research Repository (CoRR) abs/1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Selfcritical sequence training for image captioning", "author": ["S.J. Rennie", "E. Marcheret", "Y. Mroueh", "J. Ross", "V. Goel"], "venue": "CoRR, abs/1612.00563.", "citeRegEx": "Rennie et al\\.,? 2016", "shortCiteRegEx": "Rennie et al\\.", "year": 2016}, {"title": "Semiotic schemas: A framework for grounding language in action and perception", "author": ["D. Roy"], "venue": "Artificial Intelligence, 167(1-2):170\u2013205. 27", "citeRegEx": "Roy,? 2005", "shortCiteRegEx": "Roy", "year": 2005}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman,? 2014", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Multimodal representation: Kneser-ney smoothing/skip-gram based neural language model", "author": ["M. Song", "C.D. Yoo"], "venue": "2016 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Song and Yoo,? 2016", "shortCiteRegEx": "Song and Yoo", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 27, pages 3104\u20133112. Curran Associates, Inc.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, abs/1605.02688.", "citeRegEx": "Team,? 2016", "shortCiteRegEx": "Team", "year": 2016}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A parallel-fusion RNN-LSTM architecture for image caption generation", "author": ["M. Wang", "L. Song", "X. Yang", "C. Luo"], "venue": "2016 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Image captioning with an intermediate attributes layer. CoRR, abs/1506.01144", "author": ["Q. Wu", "C. Shen", "A. van den Hengel", "L. Liu", "A.R. Dick"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, volume abs/1502.03044, page 20482057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Encode, review, and decode: Reviewer module for caption generation", "author": ["Z. Yang", "Y. Yuan", "Y. Wu", "R. Salakhutdinov", "W.W. Cohen"], "venue": "CoRR, abs/1605.07912.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Boosting image captioning with attributes", "author": ["T. Yao", "Y. Pan", "Y. Li", "Z. Qiu", "T. Mei"], "venue": "CoRR, abs/1611.01646. 28", "citeRegEx": "Yao et al\\.,? 2016", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Image caption generation with text-conditional semantic attention", "author": ["L. Zhou", "C. Xu", "P. Koch", "J.J. Corso"], "venue": "CoRR, abs/1606.04621.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Image caption generation is the task of generating a natural language description of the content of an image (Bernardi et al., 2016).", "startOffset": 109, "endOffset": 132}, {"referenceID": 11, "context": "Yet, the question of where image information should feature in captioning is at the heart of a broader set of questions concerning how language can be grounded in perceptual information, questions which have been addressed by philosophers (Harnad, 1990) and AI practitioners (Roy, 2005).", "startOffset": 239, "endOffset": 253}, {"referenceID": 36, "context": "Yet, the question of where image information should feature in captioning is at the heart of a broader set of questions concerning how language can be grounded in perceptual information, questions which have been addressed by philosophers (Harnad, 1990) and AI practitioners (Roy, 2005).", "startOffset": 275, "endOffset": 286}, {"referenceID": 15, "context": "Specifically, the descriptions we talk about are \u2018concrete\u2019 and \u2018conceptual\u2019 image descriptions (Hodosh et al., 2013).", "startOffset": 96, "endOffset": 117}, {"referenceID": 40, "context": "The discontinuous perspective brings to light some similarities between RNNs as used in language modelling and the encoding RNNs in sequence-to-sequence models, for example in neural machine translation systems (Sutskever et al., 2014).", "startOffset": 211, "endOffset": 235}, {"referenceID": 8, "context": "These include (Devlin et al., 2015), who use a gated recurrent unit (GRU) model (Chung et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 6, "context": ", 2015), who use a gated recurrent unit (GRU) model (Chung et al., 2014), that was initialised with an image vector.", "startOffset": 52, "endOffset": 72}, {"referenceID": 24, "context": "In a related vein, (Ma and Han, 2016) first extract a sequence of attributes from an image, then map these to a caption using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), in an encoder-decoder framework \u00e1 la (Sutskever et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 14, "context": "In a related vein, (Ma and Han, 2016) first extract a sequence of attributes from an image, then map these to a caption using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), in an encoder-decoder framework \u00e1 la (Sutskever et al.", "startOffset": 166, "endOffset": 200}, {"referenceID": 40, "context": "In a related vein, (Ma and Han, 2016) first extract a sequence of attributes from an image, then map these to a caption using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), in an encoder-decoder framework \u00e1 la (Sutskever et al., 2014).", "startOffset": 239, "endOffset": 263}, {"referenceID": 44, "context": "(Wang et al., 2016) combine a simple RNN and an LSTM in one architecture, keeping them independent until their vectors are mixed together by weighted sum.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "(Liu et al., 2016) describe two systems, both of which are optimized by discrete optimisation on caption quality metrics directly which are CIDEr (Vedantam et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 42, "context": ", 2016) describe two systems, both of which are optimized by discrete optimisation on caption quality metrics directly which are CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al.", "startOffset": 135, "endOffset": 158}, {"referenceID": 0, "context": ", 2015) and SPICE (Anderson et al., 2016).", "startOffset": 18, "endOffset": 41}, {"referenceID": 4, "context": "(Chen and Zitnick, 2014) X (Chen and Zitnick, 2015) X (Devlin et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "(Chen and Zitnick, 2014) X (Chen and Zitnick, 2015) X (Devlin et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 8, "context": "(Chen and Zitnick, 2014) X (Chen and Zitnick, 2015) X (Devlin et al., 2015) X (Donahue et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": ", 2015) X (Donahue et al., 2015) X (Hendricks et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 12, "context": ", 2015) X (Hendricks et al., 2016) X (Hessel et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 13, "context": ", 2016) X (Hessel et al., 2015) X (Karpathy and Fei-Fei, 2015) X (Kiros et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": ", 2015) X (Karpathy and Fei-Fei, 2015) X (Kiros et al.", "startOffset": 10, "endOffset": 38}, {"referenceID": 17, "context": ", 2015) X (Karpathy and Fei-Fei, 2015) X (Kiros et al., 2014a) X (Kiros et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 18, "context": ", 2014a) X (Kiros et al., 2014b) X (Krause et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 19, "context": ", 2014b) X (Krause et al., 2016) X (Liu et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 22, "context": ", 2016) X (Liu et al., 2016)\u2020 X (Liu et al.", "startOffset": 10, "endOffset": 28}, {"referenceID": 22, "context": ", 2016)\u2020 X (Liu et al., 2016)\u2020 X X (Lu et al.", "startOffset": 11, "endOffset": 29}, {"referenceID": 23, "context": ", 2016)\u2020 X X (Lu et al., 2016) X X (Ma and Han, 2016) X (Mao et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 24, "context": ", 2016) X X (Ma and Han, 2016) X (Mao et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 27, "context": ", 2016) X X (Ma and Han, 2016) X (Mao et al., 2014) X (Mao et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 25, "context": ", 2014) X (Mao et al., 2015a) X (Mao et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 26, "context": ", 2015a) X (Mao et al., 2015b) X (Nina and Rodriguez, 2015) X (Oruganti et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 30, "context": ", 2015b) X (Nina and Rodriguez, 2015) X (Oruganti et al.", "startOffset": 11, "endOffset": 37}, {"referenceID": 31, "context": ", 2015b) X (Nina and Rodriguez, 2015) X (Oruganti et al., 2016) X (Rennie et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 35, "context": ", 2016) X (Rennie et al., 2016)\u2020 X (Rennie et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 35, "context": ", 2016)\u2020 X (Rennie et al., 2016)\u2020 X (Song and Yoo, 2016) X (Vinyals et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 38, "context": ", 2016)\u2020 X (Song and Yoo, 2016) X (Vinyals et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 43, "context": ", 2016)\u2020 X (Song and Yoo, 2016) X (Vinyals et al., 2015) X (Wang et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 44, "context": ", 2015) X (Wang et al., 2016) X (Wu et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 45, "context": ", 2016) X (Wu et al., 2015) X (Xu et al.", "startOffset": 10, "endOffset": 27}, {"referenceID": 46, "context": ", 2015) X (Xu et al., 2015) X X X (Yang et al.", "startOffset": 10, "endOffset": 27}, {"referenceID": 47, "context": ", 2015) X X X (Yang et al., 2016) X X (Yao et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 48, "context": ", 2016) X X (Yao et al., 2016)\u2020 X (Yao et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 48, "context": ", 2016)\u2020 X (Yao et al., 2016)\u2020 X X (You et al.", "startOffset": 11, "endOffset": 29}, {"referenceID": 49, "context": ", 2016)\u2020 X X (You et al., 2016) X X X (Zhou et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 51, "context": ", 2016) X X X (Zhou et al., 2016) X", "startOffset": 14, "endOffset": 33}, {"referenceID": 43, "context": "(Vinyals et al., 2015) use an LSTM language model in this way, finding that pre-injection gives better results than par-injection (described below).", "startOffset": 0, "endOffset": 22}, {"referenceID": 45, "context": "Rather than injecting a low-level image vector, (Wu et al., 2015) opt for a higher-level approach by extracting attributes from an image and passing those as an input to the caption generator.", "startOffset": 48, "endOffset": 65}, {"referenceID": 19, "context": "(Krause et al., 2016) generate paragraph-length captions in two stages.", "startOffset": 0, "endOffset": 21}, {"referenceID": 35, "context": "(Rennie et al., 2016) describe two systems, an attention model and a nonattention model, both of which use discrete optimisation in order to optimize CIDEr directly.", "startOffset": 0, "endOffset": 21}, {"referenceID": 48, "context": "(Yao et al., 2016) describe five systems which mix image vectors and image attributes in different ways.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "This last one gives the best performance of all five in terms of METEOR (Banerjee and Lavie, 2005).", "startOffset": 72, "endOffset": 98}, {"referenceID": 9, "context": "Similarly, (Donahue et al., 2015) describes an image and video captioning system that puts two LSTMs in series after each other.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "(Karpathy and Fei-Fei, 2015) generate captions for multiple subregions within an image.", "startOffset": 0, "endOffset": 28}, {"referenceID": 51, "context": "(Zhou et al., 2016) also use par-injection, but the first word that the image is combined with is an all-zeros vector, that is, they use both pre-injection and par-injection.", "startOffset": 0, "endOffset": 19}, {"referenceID": 35, "context": "One of the two systems of (Rennie et al., 2016) has already been described in Section 2.", "startOffset": 26, "endOffset": 47}, {"referenceID": 17, "context": "Among the exponents of this approach is the work of (Kiros et al., 2014a), who uses a log-bilinear language model (LBL) (Mnih and Hinton, 2007) to convert a caption prefix into a single vector which is later merged with the image vector in order to determine the next word in the caption.", "startOffset": 52, "endOffset": 73}, {"referenceID": 29, "context": ", 2014a), who uses a log-bilinear language model (LBL) (Mnih and Hinton, 2007) to convert a caption prefix into a single vector which is later merged with the image vector in order to determine the next word in the caption.", "startOffset": 55, "endOffset": 78}, {"referenceID": 18, "context": "In later work, (Kiros et al., 2014b) describe a caption generator that can be conditioned either on images or on full captions.", "startOffset": 15, "endOffset": 36}, {"referenceID": 25, "context": "(Mao et al., 2015a) do a basic comparison between inject and merge architectures and find that merging works better than injecting.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "(Mao et al., 2015b) describe a subsequent development, where the vocabulary of a caption generator can be increased after training by only learning the new words\u2019 embeddings whilst leaving the rest of the network intact.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "(Hendricks et al., 2016) also uses a multimodal layer that merges the image vector with the caption prefix vector produced using an LSTM.", "startOffset": 0, "endOffset": 24}, {"referenceID": 46, "context": "This is the case in the work of (Xu et al., 2015), who describe an attention-based caption generator.", "startOffset": 32, "endOffset": 49}, {"referenceID": 23, "context": "(Lu et al., 2016) describe an attention-based model that also determines the importance that should be given to an image during generation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 47, "context": "(Yang et al., 2016) describe a generic attention-based encoder-decoder system that was used for image captioning and source code commenting.", "startOffset": 0, "endOffset": 19}, {"referenceID": 49, "context": "(You et al., 2016) describe an attention-based caption generator that initinjects the full image whilst using attributes from the image to perform attention both at the word level (par-inject) and at the RNN level (merge).", "startOffset": 0, "endOffset": 18}, {"referenceID": 22, "context": "One of the two systems of (Liu et al., 2016) has already been described in Section 2.", "startOffset": 26, "endOffset": 44}, {"referenceID": 48, "context": "Three of the five systems of (Yao et al., 2016) have already been described in Section 2.", "startOffset": 29, "endOffset": 47}, {"referenceID": 43, "context": "Work that has tested both par-inject and pre-inject, such as (Vinyals et al., 2015), reports that pre-inject works better.", "startOffset": 61, "endOffset": 83}, {"referenceID": 25, "context": "The work of (Mao et al., 2015a) compares inject and merge architectures and concludes that merge is better than inject.", "startOffset": 12, "endOffset": 31}, {"referenceID": 33, "context": "\u2019s comparison between architectures is a relatively tangential part of their overall evaluation, and is based only on the BLEU metric (Papineni et al., 2002).", "startOffset": 134, "endOffset": 157}, {"referenceID": 44, "context": "Answering the question of which architecture is best is difficult because different architectures perform differently on different evaluation measures, as shown for example by (Wang et al., 2016), who compared simple RNNs and LSTMs.", "startOffset": 176, "endOffset": 195}, {"referenceID": 43, "context": "It is a scaled-down version of the architecture described in (Vinyals et al., 2015).", "startOffset": 61, "endOffset": 83}, {"referenceID": 28, "context": "No precompiled vector embeddings such as word2vec (Mikolov et al., 2013) were used.", "startOffset": 50, "endOffset": 72}, {"referenceID": 25, "context": "of RNNs were used: the simple RNN described in (Mao et al., 2015a) and the LSTM described in (Vinyals et al.", "startOffset": 47, "endOffset": 66}, {"referenceID": 43, "context": ", 2015a) and the LSTM described in (Vinyals et al., 2015).", "startOffset": 35, "endOffset": 57}, {"referenceID": 25, "context": "Apart from enabling a comparison of different RNN types overall, this strategy accounts for using an RNN that is reported to work well in a merge architecture (Mao et al., 2015a) and another RNN that is reported to work well in an inject architecture (Vinyals et al.", "startOffset": 159, "endOffset": 178}, {"referenceID": 43, "context": ", 2015a) and another RNN that is reported to work well in an inject architecture (Vinyals et al., 2015).", "startOffset": 81, "endOffset": 103}, {"referenceID": 37, "context": "Image Prior to training, all images were vectorised using the activation values of the penultimate layer of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which is trained to perform object recognition and returns a 4,096-element vector.", "startOffset": 164, "endOffset": 194}, {"referenceID": 39, "context": "Regularisation In order to reduce overfitting, dropout (Srivastava et al., 2014) with a dropout rate of 0.", "startOffset": 55, "endOffset": 80}, {"referenceID": 25, "context": "The simple RNN, which is the same one used in (Mao et al., 2015a), is defined as", "startOffset": 46, "endOffset": 65}, {"referenceID": 43, "context": "The LSTM model, which is the one used in (Vinyals et al., 2015), is defined as", "startOffset": 41, "endOffset": 63}, {"referenceID": 50, "context": "6 Data set Flickr30k (Young et al., 2014) Vocabulary all tokens in training set occurring 5 times or more Cost function sum cross-entropy Optimisation Adam (P.", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "5 Stopping criteria early stopping patience of 2 epochs with maximum of 20 epochs Gradient control elementwise gradient clipping of \u00b15 Initialisation biases - zeros; feedforward weights - xavier (Glorot and Bengio, 2010); recurrent weights - orthogonal Generation beam search with beam width being 40 and a maximum length of 50 words", "startOffset": 195, "endOffset": 220}, {"referenceID": 50, "context": "Images The dataset used for all experiments was the version of Flickr30k (Young et al., 2014) distributed by (Karpathy and Fei-Fei, 2015), a set of 31,014 images taken from Flickr combined with five manually written captions per image.", "startOffset": 73, "endOffset": 93}, {"referenceID": 16, "context": ", 2014) distributed by (Karpathy and Fei-Fei, 2015), a set of 31,014 images taken from Flickr combined with five manually written captions per image.", "startOffset": 23, "endOffset": 51}, {"referenceID": 37, "context": "The images are already vectorised into 4,096-element vectors via the activations of layer \u2018fc7\u2019 (the penultimate layer) of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which was trained for object recognition on the ImageNet dataset (Deng et al.", "startOffset": 179, "endOffset": 209}, {"referenceID": 7, "context": "The images are already vectorised into 4,096-element vectors via the activations of layer \u2018fc7\u2019 (the penultimate layer) of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which was trained for object recognition on the ImageNet dataset (Deng et al., 2009).", "startOffset": 276, "endOffset": 295}, {"referenceID": 21, "context": "4We used Flickr30k instead of the larger MSCOCO (Lin et al., 2014) in order to reduce training time as we had to evaluate many different architectures.", "startOffset": 48, "endOffset": 66}, {"referenceID": 34, "context": "Gradient control In order to avoid gradient explosion (Pascanu et al., 2012), all gradients in the gradient descent algorithm were clipped to be within the range \u00b15 as suggested by (Karpathy and Fei-Fei, 2015).", "startOffset": 54, "endOffset": 76}, {"referenceID": 16, "context": ", 2012), all gradients in the gradient descent algorithm were clipped to be within the range \u00b15 as suggested by (Karpathy and Fei-Fei, 2015).", "startOffset": 112, "endOffset": 140}, {"referenceID": 10, "context": "Initialisation For weight initialisation, all biases were set to zero and all feed forward weights were randomly set using xavier initialisation (Glorot and Bengio, 2010), including the word embeddings.", "startOffset": 145, "endOffset": 170}, {"referenceID": 33, "context": "We use the MSCOCO evaluation code which measures the standard evaluation metrics BLEU-(1,2,3,4) (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al.", "startOffset": 96, "endOffset": 119}, {"referenceID": 2, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al.", "startOffset": 16, "endOffset": 42}, {"referenceID": 42, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and ROUGE-L (Lin and Och, 2004).", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "We use the standard R@n recall measures (Hodosh et al., 2013), and report recall at 1, 5, and 10.", "startOffset": 40, "endOffset": 61}, {"referenceID": 25, "context": "For comparison, the architectures used in (Mao et al., 2015a) and (Vinyals et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 43, "context": ", 2015a) and (Vinyals et al., 2015) are also included in the tables below.", "startOffset": 13, "endOffset": 35}, {"referenceID": 43, "context": "This result is somewhat surprising, since the LSTM is a reimplementation of the one described by (Vinyals et al., 2015), which is optimized for a pre-inject architecture.", "startOffset": 97, "endOffset": 119}, {"referenceID": 40, "context": "This would also shed light on the similarities and differences between a range of NLP tasks, as shown by other work on sequence-to-sequence modelling (Sutskever et al., 2014).", "startOffset": 150, "endOffset": 174}], "year": 2017, "abstractText": "When a neural language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in a recurrent neural network \u2013 conditioning the language model by injecting image features \u2013 or in a layer following the recurrent neural network \u2013 conditioning the language model by merging the image features. While merging implies that visual features are bound at the end of the caption generation process, injecting can bind the visual features at a variety stages. In this paper we empirically show that late binding is superior to early binding in terms of different evaluation metrics. This suggests that the different modalities (visual and linguistic) for caption generation should not be jointly encoded by the RNN; rather, the multimodal integration should be delayed to a subsequent stage. Furthermore, this suggests that recurrent neural networks should not be viewed as actually generating text, but only as encoding it for prediction in a subsequent", "creator": "TeX"}}}