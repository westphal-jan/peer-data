{"id": "1703.09179", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Transfer learning for music classification and regression tasks", "abstract": "In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.", "histories": [["v1", "Mon, 27 Mar 2017 16:48:03 GMT  (3787kb,D)", "https://arxiv.org/abs/1703.09179v1", "16 pages, single column, NOT iclr submission"], ["v2", "Thu, 29 Jun 2017 15:58:38 GMT  (130kb,D)", "http://arxiv.org/abs/1703.09179v2", "To appear at 18th International Society of Music Information Retrieval (ISMIR) Conference, 2017"], ["v3", "Sat, 15 Jul 2017 13:36:05 GMT  (130kb,D)", "http://arxiv.org/abs/1703.09179v3", "18th International Society of Music Information Retrieval (ISMIR) Conference, Suzhou, China, 2017"], ["v4", "Wed, 13 Sep 2017 16:20:26 GMT  (130kb,D)", "http://arxiv.org/abs/1703.09179v4", "18th International Society of Music Information Retrieval (ISMIR) Conference, Suzhou, China, 2017"]], "COMMENTS": "16 pages, single column, NOT iclr submission", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM cs.SD", "authors": ["keunwoo choi", "gy\\\"orgy fazekas", "mark sandler", "kyunghyun cho"], "accepted": false, "id": "1703.09179"}, "pdf": {"name": "1703.09179.pdf", "metadata": {"source": "CRF", "title": "TRANSFER LEARNING FOR MUSIC CLASSIFICATION AND REGRESSION TASKS", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In machine learning, transfer learning is often defined as the reuse of parameters that are trained on a source task for a target task and aimed at transferring knowledge between domains. A common motivation for transfer learning is the lack of sufficient training data in the target task. When using a neural network, the number of traceable parameters in the target task model can be significantly reduced, enabling effective learning with smaller datasets. A popular example of transfer learning is semantic image segmentation in computer vision, where the network uses rich information, such as basic shapes or prototypical templates of objects captured when trained for image classification [37]. Another example is traced word embedding in natural language processing. Word embedding, a vector representation of a word, can be transferred to large datasets such as Wikipedia [35] and transferred to other tasks such as mood analysis."}, {"heading": "2. TRANSFER LEARNING FOR MUSIC", "text": "This section describes our proposed transfer learning approach: A Convnet is designed and trained for a source task, and then the weighted network is used as a feature extractor for target tasks. Figure 1 shows the scheme of the proposed approach."}, {"heading": "2.1 Convolutional Neural Networks for Music Tagging", "text": "We choose to label music as the source task because i) large training data is available and ii) its rich set of labels covers various aspects of music, such as genre, mood, era and instrumentation. In the source task, a mel spectrogram (X), a two-dimensional representation of the music signal, is used as input to the conveyor. The mel spectrogram is selected because it is psychologically relevant and computationally efficient. It provides a mel-scaled frequency representation that effectively approximates human auditory perception [36] and typically involves compressing the frequency axis of short-term Fourier transformation representations (e.g. 257 / 513 / 1025 frequency bins to 64 / 96 / 128 Gibson frequency bins). In our study, the number of melbins is set to 96 and the size of the mel spectrogram is mapped to the decibel scale (X)."}, {"heading": "2.2 Representation Transfer", "text": "In this section, we will explain how to extract features from a pre-trained conventional network. In the rest of the work, this function is referred to as a pre-trained Convnet function, or simply Convnet feature. It is already well understood how random convexations learn hierarchical features in visual image classification [58]. Through forward convolution operations, lower features are used to construct features at a higher level. Subsampling layers reduce the size of traits as local inventory is added. At a lower level, the traits become more invariable to (scaling / location) distortions and more relevant to the target task.This kind of hierarchy also exists when a convexnet is trained for a music-related task. Visualization and sonization of convexemplars for music-genre classifications, we have the various levels of hierarchy layers not shown in conventional layers as a motivation [13] for such a proposed transfer."}, {"heading": "2.3 Classifiers and Regressors of Target Tasks", "text": "Variants of support vector machines (SVM) are used as classifiers and regressors [45, 50]. SVM work efficiently on target tasks with small training sets and outperform K-nearest neighbours in all tasks in a preliminary trial. As there is a lot of work using handwritten features and SVMs, the use of SVMs allows us to focus on comparing the performance of features."}, {"heading": "3. PREPARATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Source Task: Music Tagging", "text": "In the source task, 244,224 preview clips of the Million Song Dataset [5] are used (201,680 / 12,605 / 25,940 for training / validation / test kits) with top 50 last.fm tags including genres, epochs, instrumentation and moods. Mel spectrograms are extracted from music signals in real time on the GPU using capre [12]. Binary cross-entropy is used as a loss function during training.2 Because the average is influenced by zero padding applied to signals shorter than 29 seconds, these signals are repeated to generate 29-second signals. This is only done in tasks 5 and 6 of the experiment. ADAM optimization algorithm [25] is used to accelerate stochastic gradient descendence. Convnet reaches 0.849 AUC-ROC score (Area Under Curve - Chareiver Receiving Testacteristic work on our Kerano 14]."}, {"heading": "3.2 Target Tasks", "text": "Six data sets are selected to be used in six target tasks. They are summarized in Table 1. \u2022 Task 1: The extended ballroom dance data set consists of specific ballroom dance subgenres. \u2022 Task 2: The Gtzan genre data set was extremely popular, although some errors were found [48]. \u2022 Task 3: The data set size is smaller than the others of an order of magnitude. \u2022 Task 4: Emotion prediction at the excitation and value level. We evaluate excitation and value separately. We cut and use the first 29 seconds from the 45 second signals. \u2022 Task 5. Excerpts are subsegments of tracks with binary names (\"vowel\" and \"non-vowel\"). Many of them are shorter than 29 seconds. This data set is provided for benchmarking frame-based voice signals, while we use it as a pre-segmented classification task that may be easier than the original task. \u2022 Task 4 seconds are not pieces of music."}, {"heading": "3.3 Baseline Feature and Random Convnet Feature", "text": "The basic feature is the mean and standard deviations of 20 Mel frequency Cepstral Coefficient (MFCCs) and their derivatives of first and second order. In this paper, this basic feature is referred to as MFCCs or MFCC vectors. MFCC is chosen because it has been used in many tasks to retrieve music information and is known to provide robust representation. Librosa [34] is used for MFCC extraction and audio processing.The random Convnet feature is extracted using the identical convnet structure of the source task and after random weight initialization with a normal distribution [21], but without training."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Configurations", "text": "In tasks 1-4, the experiments are carried out with 10-fold cross-validation using layered splits. In tasks 5, predefined training / validation / test sets are used. In task 6, the experiment is performed without replacement with 10-fold cross-validation in order to prevent the sub-segments from the same recordings from being used in training and validation. SVM parameters are optimized by performing the grid search on the basis of the validation results. Kernel type / bandwidth of the radial base function and the penalty parameters are selected from the following ranges: \u2022 Kernel type: [linear, radial] - bandwidth \u03b3 in the radial base function: [1 / 23, 1 / 25, 1 / 27, 1 / 29, 1 / 211, 1 / 213, 1 / Nf] \u2022 Penalty parameters C: [0.1, 2.0, 8.0, 32.0] A radial basic function is exp (\u2212 x \u2212 x, or \u2212 x, respectively, refer to the beam diameter and the Sciff)."}, {"heading": "4.2 Results and Discussion", "text": "Figure 2 shows a summary of the results. The results of the i) best performing Convnet function, ii) concatenates \"12345\" 4 Convnet function and MFCCs, iii) MFCC function, and iv) state-of-the-art algorithms for all tasks. In all six tasks, the majority of Convnet functions exceed the basic functions of Ballnet functions that exceed the basic functions of Ballnet features. Concurrent MFCCs3 means that the properties from the first, third, and fifth levels are concatenated. \"12345\" refers to the Convault function that is concatenated from the 1st to 5th levels. \"135\" means linking the features from the first, third, and fifth levels. With \"12345\" Convnet function normally shows no improvement over a pure Convnet function, except in Task 6, audio event classification. \"Although the State of the Art reported is more musically explicit and almost all of our knowledge is workable."}, {"heading": "5. CONCLUSIONS", "text": "We proposed a transfer-learning approach that used deep learning, and evaluated it based on six retrievals of music information and audio-related tasks. The pre-trained Convnet was first trained to predict music tags, and then transferred aggregated features from the layers to solve the classification of genres, vocal / non-vocal classification, emotion prediction, language / music classification, and acoustic event classification problems. In contrast to the conventional approach to transfer learning, we proposed to use the features from all the evolutionary layers after applying an average pooling to reduce their size on the feature map. In the experiments, the pre-tracked Convnet feature performed well overall. It exceeded the basic MFCC feature for all six tasks by demonstrating the improvement through pre-training on a source task."}, {"heading": "6. REFERENCES", "text": "[1] Arash Foroughmand Arabi and Guojun Lu. Enhanced polyphonic music genre classification using high level features. In Signal and Image Processing Applications (ICSIPA), 2009 IEEE International Conference on, pages 101-106. [3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled video. In Advances in Neural Information Processing Systems, pages 892-900. [4] Babu Kaji Baniya, Joonwhoan Lee, and Ze-Nian Li. Audio feature reduction and analysis for automatic music genre classification. In Systems, Man and Cybernetics (SMC), 2014."}], "references": [{"title": "Enhanced polyphonic music genre classification using high level features", "author": ["Arash Foroughmand Arabi", "Guojun Lu"], "venue": "In Signal and Image Processing Applications (ICSIPA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Look, listen and learn", "author": ["Relja Arandjelovi\u0107", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1705.08168,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Soundnet: Learning sound representations from unlabeled video", "author": ["Yusuf Aytar", "Carl Vondrick", "Antonio Torralba"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Audio feature reduction and analysis for automatic music genre classification", "author": ["Babu Kaji Baniya", "Joonwhoan Lee", "Ze-Nian Li"], "venue": "In Systems, Man and Cybernetics (SMC),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel PW Ellis", "Brian Whitman", "Paul Lamere"], "venue": "In Proceedings of the 12th International Society for Music Information Retrieval Conference, October", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Invariant scattering convolution networks", "author": ["Joan Bruna", "St\u00e9phane Mallat"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The effects of noisy labels on deep convolutional neural networks for music classification", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Kyunghyun Cho", "Mark Sandler"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "In The 17th International Society of Music Information Retrieval Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Explaining deep convolutional neural networks on music classification", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "arXiv preprint arXiv:1607.02444,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Towards playlist generation algorithms using rnns trained on within-track transitions", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "In Workshop on Surprise, Opposition, and Obstruction in Adaptive and Personalized Systems (SOAP), Halifax,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Kyunghyun Cho"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Kapre: On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras", "author": ["Keunwoo Choi", "Deokjin Joo", "Juho Kim"], "venue": "In Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Auralisation of deep convolutional neural networks: Listening to learned features", "author": ["Keunwoo Choi", "Jeonghee Kim", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "In International Society of Music Information Retrieval (ISMIR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Keras: Deep learning library for theano and tensorflow", "author": ["Fran\u00e7ois Chollet"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning feature representations with k-means", "author": ["Adam Coates", "Andrew Y Ng"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Towards multi-purpose spectral rhythm features: An application to dance style, meter and tempo estimation", "author": ["Aggelos Gkiokas", "Vassilis Katsouros", "Gy\u00f6rgy Carayannis"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Evaluating rhythmic descriptors for musical genre classification", "author": ["Fabien Gouyon", "Simon Dixon", "Elias Pampalk", "Gerhard Widmer"], "venue": "In 25th AES International Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Deep image features in music information retrieval", "author": ["Grzegorz Gwardys", "Daniel Grzywczak"], "venue": "International Journal of Electronics and Telecommunications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Transfer learning in mir: Sharing learned latent representations for music audio classification and similarity", "author": ["Philippe Hamel", "Matthew EP Davies", "Kazuyoshi Yoshii", "Masataka Goto"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["Guang-Bin Huang", "Qin-Yu Zhu", "Chee-Kheong Siew"], "venue": "In IEEE International Joint Conference on Neural Networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Music genre classification based on local feature selection using a self-adaptive harmony search algorithm", "author": ["Yin-Fu Huang", "Sheng-Min Lin", "Huan-Yu Wu", "Yu- Siou Li"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Multi-level and multiscale feature aggregation using pre-trained convolutional neural networks for music auto-tagging", "author": ["Jongpil Lee", "Juhan Nam"], "venue": "arXiv preprint arXiv:1703.01793,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "Content-aware collaborative music recommendation using pre-trained neural networks", "author": ["Dawen Liang", "Minshu Zhan", "Daniel PW Ellis"], "venue": "In Conference of the International Society for Music Information Retrieval (ISMIR", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Beat histogram features for rhythm-based musical genre classification using multiple novelty functions", "author": ["Athanasios Lykartsis", "Alexander Lerch"], "venue": "In Proceedings of the 16th ISMIR Conference,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "The extended ballroom dataset. Conference of the International Society for Music Information Retrieval (ISMIR 2016) latebreaking session, 2016", "author": ["Ugo Marchand", "Geoffroy Peeters"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Scale and shift invariant time/frequency representation using auditory statistics: Application to rhythm description", "author": ["Ugo Marchand", "Geoffroy Peeters"], "venue": "In Machine Learning for Signal Processing (MLSP),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "An introduction to the psychology of hearing", "author": ["Brian CJ Moore"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Maxime Oquab", "Leon Bottou", "Ivan Laptev", "Josef Sivic"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Designing efficient architectures for modeling temporal features with convolutional neural networks", "author": ["Jordi Pons", "Xavier Serra"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Vocal detection in music with support vector machines", "author": ["Mathieu Ramona", "Ga\u00ebl Richard", "Bertrand David"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "A dataset and taxonomy for urban sound research", "author": ["J. Salamon", "C. Jacoby", "J.P. Bello"], "venue": "In 22st ACM International Conference on Multimedia (ACM-MM\u201914),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Deep convolutional neural networks and data augmentation for environmental sound classification", "author": ["Justin Salamon", "Juan Pablo Bello"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A tutorial on support vector regression", "author": ["Alex J Smola", "Bernhard Sch\u00f6lkopf"], "venue": "Statistics and computing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2004}, {"title": "1000 songs for emotional analysis of music", "author": ["Mohammad Soleymani", "Micheal N Caro", "Erik M Schmidt", "Cheng-Ya Sha", "Yi-Hsuan Yang"], "venue": "In Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Learning sparse dictionaries for music and speech classification", "author": ["M Srinivas", "Debaditya Roy", "C Krishna Mohan"], "venue": "In Digital Signal Processing (DSP),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "The gtzan dataset: Its contents, its faults, their effects on evaluation, and its future use", "author": ["Bob L Sturm"], "venue": "arXiv preprint arXiv:1306.1461,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Revisiting priorities: Improving mir evaluation practices", "author": ["Bob L Sturm"], "venue": "In Proc. 17th International Society for Music Information Retrieval Conference (IS- MIR\u201916),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Least squares support vector machine classifiers", "author": ["Johan AK Suykens", "Joos Vandewalle"], "venue": "Neural processing letters,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1999}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["The Theano Development Team", "Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Musical genre classification of audio signals", "author": ["George Tzanetakis", "Perry Cook"], "venue": "Speech and Audio Processing, IEEE transactions on,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2002}, {"title": "Transfer learning by supervised pretraining for audio-based music classification", "author": ["A\u00e4ron Van Den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "In Conference of the International Society for Music Information Retrieval", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Self-adaptive harmony search algorithm for optimization", "author": ["Chia-Ming Wang", "Yin-Fu Huang"], "venue": "Expert Systems with Applications,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2010}, {"title": "On-line continuous-time music mood regression with deep recurrent neural networks", "author": ["Felix Weninger", "Florian Eyben", "Bjorn Schuller"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In European conference on computer vision,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Traffic sign recognition using extreme learning classifier with deep convolutional features", "author": ["Yujun Zeng", "Xin Xu", "Yuqiang Fang", "Kun Zhao"], "venue": "In The 2015 international conference on intelligence science and big data engineering (IScIDE", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}], "referenceMentions": [{"referenceID": 34, "context": "A popular example of transfer learning is semantic image segmentation in computer vision, where the network utilises rich information, such as basic shapes or prototypical templates of objects, that were captured when trained for image classification [37].", "startOffset": 251, "endOffset": 255}, {"referenceID": 32, "context": "Word embedding, a vector representation of a word, can be trained on large datasets such as Wikipedia [35] and adopted to other tasks such as sentiment analysis [27].", "startOffset": 102, "endOffset": 106}, {"referenceID": 26, "context": "Word embedding, a vector representation of a word, can be trained on large datasets such as Wikipedia [35] and adopted to other tasks such as sentiment analysis [27].", "startOffset": 161, "endOffset": 165}, {"referenceID": 53, "context": "proposed to directly learn music features using linear embedding [57] of mel-spectrogram representations and genre/similarity/tag labels [20].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "proposed to directly learn music features using linear embedding [57] of mel-spectrogram representations and genre/similarity/tag labels [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "outlines a large-scale transfer learning approach, where a multi-layer perceptron is combined with the spherical K-means algorithm [16] trained on tags and play-count data [54].", "startOffset": 131, "endOffset": 135}, {"referenceID": 50, "context": "outlines a large-scale transfer learning approach, where a multi-layer perceptron is combined with the spherical K-means algorithm [16] trained on tags and play-count data [54].", "startOffset": 172, "endOffset": 176}, {"referenceID": 9, "context": "used the weights of a convolutional neural network for feature extraction in playlist generation [10], while Liang et al.", "startOffset": 97, "endOffset": 101}, {"referenceID": 28, "context": "used a multi-layer perceptron for feature extraction of content-aware collaborative filtering [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 33, "context": "It provides a mel-scaled frequency representation which is an effective approximation of human auditory perception [36] and typically involves compressing the frequency axis of short-time Fourier transform representation (e.", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "In our study, the number of melbins is set to 96 and the magnitude of mel-spectrogram is mapped to decibel scale (log10 X), following [8] since it is also shown to be crucial in [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "In our study, the number of melbins is set to 96 and the magnitude of mel-spectrogram is mapped to decibel scale (log10 X), following [8] since it is also shown to be crucial in [7].", "startOffset": 178, "endOffset": 181}, {"referenceID": 41, "context": "This convnet structure with 2-dimensional 3\u00d73 kernels and 2-dimensional convolution, which is often called Vggnet [44], is expected to learn hierarchical time-frequency ar X iv :1 70 3.", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "Exponential linear unit (ELU) is used as an activation function in all convolutional layers [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 23, "context": "In all the convolutional layers, the kernel sizes are (3, 3), numbers of channels N is 32, and Batch normalisation is used [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "This structure was originally proposed for visual image classification and has been found to be effective and efficient in music classification 1 [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 54, "context": "It is already well understood how deep convnets learn hierarchical features in visual image classification [58].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Visualisation and sonification of convnet features for music genre classification has shown the different levels of hierarchy in convolutional layers [13], [9].", "startOffset": 150, "endOffset": 154}, {"referenceID": 8, "context": "Visualisation and sonification of convnet features for music genre classification has shown the different levels of hierarchy in convolutional layers [13], [9].", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "For the same reason, deep scattering networks [6] and a convnet for mu-", "startOffset": 46, "endOffset": 49}, {"referenceID": 37, "context": "1 For more recent information on kernel shapes for music classification, please see [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 27, "context": "sic tagging introduced in [28] use multi-layer representations.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "Lastly, there have been works suggesting randomweights (deep) neural networks including deep convnet can work well as a feature extractor [22] [59] (Not identical, but a similar approach is transferring knowledge from an irrelevant domain, e.", "startOffset": 138, "endOffset": 142}, {"referenceID": 55, "context": "Lastly, there have been works suggesting randomweights (deep) neural networks including deep convnet can work well as a feature extractor [22] [59] (Not identical, but a similar approach is transferring knowledge from an irrelevant domain, e.", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": ", visual image recognition, to music task [19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 42, "context": "Variants of support vector machines (SVMs) [45, 50] are used as a classifier and regressor.", "startOffset": 43, "endOffset": 51}, {"referenceID": 47, "context": "Variants of support vector machines (SVMs) [45, 50] are used as a classifier and regressor.", "startOffset": 43, "endOffset": 51}, {"referenceID": 4, "context": "In the source task, 244,224 preview clips of the Million Song Dataset [5] are used (201,680/12,605/25,940 for training/validation/test sets respectively) with top-50 last.", "startOffset": 70, "endOffset": 73}, {"referenceID": 11, "context": "Mel-spectrograms are extracted from music signals in real-time on the GPU using Kapre [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "Ballroom dance genre classification Extended ballroom [32] 4,180 Accuracy 13 T2.", "startOffset": 54, "endOffset": 58}, {"referenceID": 49, "context": "Genre classification Gtzan genre [53] 1,000 Accuracy 10 T3.", "startOffset": 33, "endOffset": 37}, {"referenceID": 43, "context": "Emotion prediction EmoMusic (45-second) [46] 744 Coefficient of determination (r) N/A (2-dimensional) T5.", "startOffset": 40, "endOffset": 44}, {"referenceID": 38, "context": "Vocal/non-vocal classification Jamendo [41] 4,086 Accuracy 2 T6.", "startOffset": 39, "endOffset": 43}, {"referenceID": 39, "context": "Audio event classification Urbansound8K [42] 8,732 Accuracy 10", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "The ADAM optimisation algorithm [25] is used for accelerating stochastic gradient descent.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "We use the Keras [14] and Theano [51] frameworks in our implementation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 48, "context": "We use the Keras [14] and Theano [51] frameworks in our implementation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 45, "context": "\u2022 Task 2: The Gtzan genre dataset has been extremely popular, although some flaws have been found [48].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "The random convnet feature is extracted using the identical convnet structure of the source task and after random weights initialisation with a normal distribution [21] but without a training.", "startOffset": 164, "endOffset": 168}, {"referenceID": 35, "context": "We use Scikit-learn [38] for these target tasks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "The ballroom genre labels are closely related to rhythmic patterns and tempo [32] [49].", "startOffset": 77, "endOffset": 81}, {"referenceID": 46, "context": "The ballroom genre labels are closely related to rhythmic patterns and tempo [32] [49].", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "The state-of-the-art algorithm which is also the only algorithm that used the same dataset due to its recent release uses 2D scale transform, an alternative representation of music signals for rhythm-related tasks [33], and 12 34 5 12 34 23 45 12 3 12 4 12 5 13 4 13 5 14 5 23 4 23 5 24 5 34 5 12 13 14 15 23 24 25 34 35 45 1 2 3 4 5 co nc at mf cc So TA .", "startOffset": 214, "endOffset": 218}, {"referenceID": 17, "context": "For additional comparisons, there are several works that use the Ballroom dataset [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "Laykartsis and Lerch [31] combines beat histogram and timbre features to achieve 76.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "[17] respectively shows 88.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "5% as the state-of-the-art score following the dataset analysis in [48], which shows that the perfect score cannot surpass 94.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Arabi and Lu [1] is most similar to the proposed convnet features in a way that it combines low-level and high-level features and shows a similar performance.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "[4] and Huang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[23] report the performances with many low-level features before and after applying feature selection algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2022 Arabi and Lu [1] uses not only low-level features such as {spectral centroid/flatness/roll-off/flux}, but also highlevel musical features such as {beat, chord distribution and chord progressions}.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "[4] uses a particularly rich set of statistics such as {mean, standard deviation, skewness, kurtosis,", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "The feature vector dimensionality is reduced by MRMR (max-relevance and min-redundancy) [39] to obtain the highest classification accuracy of 87.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "[23] adopts another feature selection algorithm, self-adaptive harmony search [55].", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[23] adopts another feature selection algorithm, self-adaptive harmony search [55].", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "\u2022 Reusing AlexNet [26], a pre-trained convnet for visual image recognition achieved 78% of accuracy [19].", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "\u2022 Reusing AlexNet [26], a pre-trained convnet for visual image recognition achieved 78% of accuracy [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 44, "context": "Figure 6 shows the accuracies of convnet features, baseline feature, and state-of-the-art [47] with low-level features including MFCCs and sparse dictionary learning for Gtzan music/speech classification.", "startOffset": 90, "endOffset": 94}, {"referenceID": 52, "context": "500 r scores using music features with a recurrent neural network as a classifier [56] that uses 4,777 audio features including many functionals (such as quantiles, standard deviation, mean, inter peak distances) of 12 chroma features, loudness, RMS Energy, zero crossing rate, 14 MFCCs, spectral energy, spectral roll-off, etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 52, "context": "In order to remove the effect of the choice of a classifier and assess solely the effect of features, we compare our approach to the baseline method of [56] which is based on the same 4,777 features with SVM, not a recurrent neural network.", "startOffset": 152, "endOffset": 156}, {"referenceID": 38, "context": "Figure 8 presents the performances on vocal/non-vocal classification using the Jamendo dataset [41].", "startOffset": 95, "endOffset": 99}, {"referenceID": 39, "context": "Figure 9 shows the results on acoustic event classification using Urbansound8K dataset [42].", "startOffset": 87, "endOffset": 91}, {"referenceID": 40, "context": "The state-of-the-art method is based on a deep convolutional neural network with data augmentation [43].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "The method in [42] with {minimum, maximum, median, mean, variance, skewness, kurtosis} of 25 MFCCs and {mean and variance} of the first and second MFCC derivatives (225-dimensional feature) achieved only 68% accuracy using the SVM classifier.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "6 Transfer learning targeting audio event classification was recently introduced in [2, 3] and achieved a state-of-the-art performance.", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "6 Transfer learning targeting audio event classification was recently introduced in [2, 3] and achieved a state-of-the-art performance.", "startOffset": 84, "endOffset": 90}], "year": 2017, "abstractText": "In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as lowand high-level music features.", "creator": "LaTeX with hyperref package"}}}