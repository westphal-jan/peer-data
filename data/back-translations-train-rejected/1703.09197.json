{"id": "1703.09197", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Deep Architectures for Modulation Recognition", "abstract": "We survey the latest advances in machine learning with deep neural networks by applying them to the task of radio modulation recognition. Results show that radio modulation recognition is not limited by network depth and further work should focus on improving learned synchronization and equalization. Advances in these areas will likely come from novel architectures designed for these tasks or through novel training methods.", "histories": [["v1", "Mon, 27 Mar 2017 17:28:43 GMT  (724kb,D)", "http://arxiv.org/abs/1703.09197v1", "7 pages, 14 figures, to be published in proceedings of IEEE DySPAN 2017"]], "COMMENTS": "7 pages, 14 figures, to be published in proceedings of IEEE DySPAN 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nathan e west", "timothy j o'shea"], "accepted": false, "id": "1703.09197"}, "pdf": {"name": "1703.09197.pdf", "metadata": {"source": "CRF", "title": "Deep Architectures for Modulation Recognition", "authors": ["Nathan E. West", "Timothy J. O\u2019Shea"], "emails": ["nathan.west@nrl.navy.mil", "oshea@vt.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTIONDeep neural networks have pushed the recent limits of performance for a variety of machine learning tasks in areas such as computer vision, natural speech processing and speaker recognition. Recently, researchers in the field of wireless communication have begun to apply deep neural networks to cognitive functional tasks with some success [13], [12], [10]. In particular, it has been shown that relatively simple Convolutionary Neural Networks exceed algorithms with decades of expert searches for radio modulation (13]. This paper provides an introduction to deep neural networks for the cognitive functional task of modulation recognition, compares several state-of-the-art methods in other areas, and experiments with learning techniques. Deep neural networks are large functional approximations consisting of a series of layers in which each layer represents a transform of input to output activations based on a parametric transfer function with some learned weights. Each layer is a parameterizable and each layer is generally applicable to a parameterizable function."}, {"heading": "A. Neural Network Architectures", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "B. Neural Network Training", "text": "Hyperparameters of a network such as learning rate, number of filters / feature maps per layer, filter size, and up to a certain degree number of layers all influence network size and are difficult to optimize. Recent research has attempted to optimize hyperparameters as regular parameters that can be trained with reverse propagation and gradient descent, such as network weights and distortions. For this study, we ignore the training of hyperparameters and use the adam optimizer [6], which provides gradient normalization and dynamics that reduce the importance of hyperparameters such as learning rate. Guided by work showing that depth is more important than the number of feature maps [2], we will establish a baseline convolutionary network similar to the one used in Radio Convolutional Modulation Networks [13]. Our first step is to adjust the number of filters and the number of taps per filter, and consider these unimportant parameters for the rest of the experiments."}, {"heading": "C. Test Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "II. TECHNICAL APPROACH", "text": "We use the RadioML2016.10a dataset [12] as the basis for evaluating the modulation detection task. The goal is to use a 128-sample complex (baseband I / Q) time domain vector to identify the modulation scheme from 11 possible classes. 128 samples are fed into the network in a 2x128 vector, in which the real and imaginary parts of the complex time samples are separated. The dataset uses a power delay profile, frequency selective fading, local oscillator offset, and additive white gaussian noise with details of these effects in [12]. The dataset is labeled with both the modulation type and the SNR basic truth. We use the All-SNR top-1 classification accuracy as a single number benchmark and show top-1 accuracy over SNR to compare the techniques of a LearX kernel with the training library of GTX."}, {"heading": "III. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Baseline Convolution Network", "text": "However, the first hyperparameter optimization is the size of our preventive layers. Each layer will have 1x3 filters, and we will vary the number of filters to find out how many are required. From [1], [2] we expect a large range in the number of filters to perform similarly before any revision takes place. As expected, there is a large window of about 30 to 70 filters per layer where the classification is very similar."}, {"heading": "B. Residual Networks", "text": "While it is not surprising that the addition of additional layers of Convolutionary does not improve classification accuracy, it is surprising that the classification and loss improvements already occur at 2 or 3 layers of Convolutionary. The original Resnet finding is that deeper networks lead to higher training losses, suggesting higher training rather than over-adjustment. Figure 6 shows that our hyperparameter-optimized CNN and 9-layer residual network achieve similar losses, validation losses and accuracy that are not shown, but the residual network learns in fewer epochs. We also experimented with residual networks with 5-9 layers, all of which had similar performance and training times. This, combined with our hyperparameter search for ordinary CNN depth, suggests that we are not limited by the network depth for radio learning tasks, as much as we are limited by purely CNN architectures. 5-9-layer receivers, all of which had similar performance and training times."}, {"heading": "D. LSTM Networks", "text": "As the final architecture, we are testing the addition of recursive network layers, namely those consisting of LSTM units, to model temporal characteristics. This approach is widely used in time series applications, and we expected modulated baseband time series to be similarly applicable. We tested two and three layer waves, followed by recurring layers in a CLDNN architecture with and without the forward / bypass connection before the recursive layer. We found that the forward connection as concatenation of the raw waveform and the conventional output shown in Figure 8 results in better classification accuracy and more stable gradient reduction than other architectures. Using a pooling layer that would create an architecture such as the conventional filter detector described above does not help classification. To further understand what limits classification accuracy, we consider the confusion matrix for a CL10 that shows two areas of confusion."}, {"heading": "IV. DISCUSSION", "text": "Although our experiments focused on modulation detection as a benchmark task, we expect other machine learning tasks to use similar network architectures. Further advances in deep learning for radio tasks will likely come from improved training methods and network architectures that can learn to transmit RF data to remove the effects of radio channels that neural network architectures are not designed for. (a) Time and frequency size representations of a filter in the first revolutionary layer of our trained CLDNN. (b) Random data trained to maximize the activation of the filter that acts like BPSK. These experiments also focused on a dataset that is nominally bandwidth normalized, which is a poor prerequisite for wireless networks to learn real signals for transmitting real networks."}], "references": [{"title": "Do deep nets", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Understanding deep architectures using a recursive convolutional network", "author": ["David Eigen", "Jason Tyler Rolfe", "Rob Fergus", "Yann LeCun"], "venue": "CoRR, abs/1312.1847,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The handbook of brain theory and neural networks. chapter Convolutional Networks for Images, Speech, and Time Series, pages 255\u2013258", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Convolutional networks and applications in vision", "author": ["Yann LeCun", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In ISCAS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Biologically Inspired Radio Signal Feature Extraction with Sparse Denoising Autoencoders", "author": ["B. Migliori", "R. Zeller-Townson", "D. Grady", "D. Gebhardt"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Inceptionism: Going deeper into neural networks. https://research.googleblog.com/2015/06/inceptionism-going-deeperinto-neural.html", "author": ["Alexander Mordvintsev", "Christopher Olah", "Mike Tyka"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Radio machine learning dataset generation with gnu radio", "author": ["Timothy O\u2019Shea", "Nathan West"], "venue": "Proceedings of the GNU Radio Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Convolutional radio modulation recognition", "author": ["Timothy J. O\u2019Shea", "Johnathan Corgan"], "venue": "networks. CoRR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Radio transformer networks: Attention models for learning to synchronize in wireless systems", "author": ["Timothy J. O\u2019Shea", "Latha Pemula", "Dhruv Batra", "T. Charles Clancy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Do deep convolutional nets really need to be deep (or even convolutional)", "author": ["Gregor Urban", "Krzysztof J Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan", "Shengjie Wang", "Rich Caruana", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson"], "venue": "arXiv preprint arXiv:1603.05691,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Recently researchers in the wireless communications field have started to apply deep neural networks to cognitive radio tasks with some success [13], [12], [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "Recently researchers in the wireless communications field have started to apply deep neural networks to cognitive radio tasks with some success [13], [12], [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Recently researchers in the wireless communications field have started to apply deep neural networks to cognitive radio tasks with some success [13], [12], [10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "In particular it has been shown that relatively simple convolutional neural networks outperform algorithms with decades of expert feature searches for radio modulation [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 2, "context": "Each layer is typically a known linear function with adjustable parameters and a non-linear activation function such that the resulting function composition can be highly non-linear [3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "the output of the classifier network which is then converted to a one-hot encoding for classification purposes [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "have been used very successfully for multi-class vision tasks such as object recognition on the Imagenet dataset [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "The use of convolutional layers started for image and hand-writing recognition to provide feature translation invariance [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "The transfer function for a standard convolutional layer [9]", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "A visible trend in neural networks for image processing is building deeper networks to learn more complex functions and hierarchical feature relationships [2], [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "A visible trend in neural networks for image processing is building deeper networks to learn more complex functions and hierarchical feature relationships [2], [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 0, "context": "Deep networks enable more complex functions to be learned more readily from raw data than shallower networks with the same number of parameters [1], [18]; however, depth in neural networks is widely believed to be limited by unstable gradients that either explode or vanish in earlier or later layers in the network.", "startOffset": 144, "endOffset": 147}, {"referenceID": 15, "context": "Deep networks enable more complex functions to be learned more readily from raw data than shallower networks with the same number of parameters [1], [18]; however, depth in neural networks is widely believed to be limited by unstable gradients that either explode or vanish in earlier or later layers in the network.", "startOffset": 149, "endOffset": 153}, {"referenceID": 3, "context": "The best approach so far, which won ImageNet 2015, is residual networks [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "2: Residual network diagram from [4], allowing feature maps combinations from multiple source layers to select optimal architecture paths within a network.", "startOffset": 33, "endOffset": 36}, {"referenceID": 14, "context": "CLDNNs are an approach for voice processing that operate on raw time-domain waveforms rather than expert voice features such as log-mel cepstrums [16], [15].", "startOffset": 152, "endOffset": 156}, {"referenceID": 4, "context": "LSTMs are a common recurrent network architecture consisting of several gates that control how long history is maintained [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 14, "context": "For example, the original CLDNN forwards raw samples with the output of convolutional layers before the LSTM layers [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "For this study we ignore training hyper-parameters and use the adam optimizer [6] which provides gradient normalization and momentum which reduces the importance of hyper-parameters like learning rate.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Guided by work that shows depth being more important than number of feature maps [2] we will establish a baseline convolutional network similar to that used in Radio Convolutional Modulation Networks [13].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Guided by work that shows depth being more important than number of feature maps [2] we will establish a baseline convolutional network similar to that used in Radio Convolutional Modulation Networks [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "10a dataset [12] as a basis for evaluating the modulation recognition task.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Gaussian noise with details of these effects in [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "We start with a network similar to the CNN2 network from [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "This is the chosen baseline because results from [13] show significant improvement upon expert methods; any", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "From [1], [18], [2] we expect that a large range in the number of filters will give similar performance before any overfitting will happen.", "startOffset": 5, "endOffset": 8}, {"referenceID": 15, "context": "From [1], [18], [2] we expect that a large range in the number of filters will give similar performance before any overfitting will happen.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "From [1], [18], [2] we expect that a large range in the number of filters will give similar performance before any overfitting will happen.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "[2] suggests that the size of filters also has minimal impact, but based on expert knowledge of the radio domain and the dataset we expect 8tap filters to be optimal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Another way to visualize these filters is to apply random data to them and perform a gradient ascent for the output of a particular filter which will converge on data that most activates a convolutional neuron [11].", "startOffset": 210, "endOffset": 214}, {"referenceID": 13, "context": "of spatial transforms to equalize and synchronize incoming waveforms [14].", "startOffset": 69, "endOffset": 73}], "year": 2017, "abstractText": "We survey the latest advances in machine learning with deep neural networks by applying them to the task of radio modulation recognition. Results show that radio modulation recognition is not limited by network depth and further work should focus on improving learned synchronization and equalization. Advances in these areas will likely come from novel architectures designed for these tasks or through novel training methods.", "creator": "LaTeX with hyperref package"}}}