{"id": "1703.09260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Goal-Driven Dynamics Learning via Bayesian Optimization", "abstract": "Real-world robots are becoming increasingly complex and commonly act in poorly understood environments where it is extremely challenging to model or learn their true dynamics. Therefore, it might be desirable to take a task-specific approach, wherein the focus is on explicitly learning the dynamics model which achieves the best control performance for the task at hand, rather than learning the true dynamics. In this work, we use Bayesian optimization in an active learning framework where a locally linear dynamics model is learned with the intent of maximizing the control performance, and used in conjunction with optimal control schemes to efficiently design a controller for a given task. This model is updated directly based on the performance observed in experiments on the physical system in an iterative manner until a desired performance is achieved. We demonstrate the efficacy of the proposed approach through simulations and real experiments on a quadrotor testbed.", "histories": [["v1", "Mon, 27 Mar 2017 18:38:06 GMT  (5999kb,D)", "http://arxiv.org/abs/1703.09260v1", null], ["v2", "Fri, 22 Sep 2017 02:06:56 GMT  (2712kb,D)", "http://arxiv.org/abs/1703.09260v2", "This is the extended version of the CDC'17 paper titled \"Goal-Driven Dynamics Learning via Bayesian Optimization.\""]], "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["somil bansal", "roberto calandra", "ted xiao", "sergey levine", "claire j tomlin"], "accepted": false, "id": "1703.09260"}, "pdf": {"name": "1703.09260.pdf", "metadata": {"source": "CRF", "title": "Goal-Driven Dynamics Learning via Bayesian Optimization", "authors": ["Somil Bansal", "Roberto Calandra", "Ted Xiao", "Sergey Levine", "Claire J. Tomlin"], "emails": ["tomlin}@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "This year, it has come to the point where it will be able to put itself at the top, in the way that it is able to put itself at the top."}, {"heading": "II. PROBLEM FORMULATION", "text": "Consider an unknown, stable, discrete system, which is defined in a similar way to the system we know. (zN, uN)..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "III. BACKGROUND", "text": "For optimization we use BO. In this section we briefly introduce Gaussian processes and BO."}, {"heading": "A. Gaussian Process (GP)", "text": "Since the function J (\u03b8) in (5) is unknown from the outset, we use nonparametric GP models to approximate them via their domain. GPs are a popular choice for probabilistic nonparametric regression, where the goal is to find a nonlinear map, J (\u03b8): M \u2192 R, from an input-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-"}, {"heading": "B. Bayesian Optimization (BO)", "text": "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [BO], [16]. BO is particularly suitable for scenarios where the evaluation of the unknown function is expensive, which corresponds to our problem in Sec. II. At each iteration, BO uses the previous observations D to model the objective function, and uses this model to determine informative sample locations. A common model used in BO for the underlying goal, and the one we are looking at, are Gaussian processes (see Sec. III-A. Using the GP's mean and variance predictions of (6), BO calculates the next sample location by optimizing the so-called capture function, (\u00b7). Different capture functions are used in the literature to act between exploration and exploitation during the optimization process [1]. For example, the next evaluation for expected improvements (Ecapture function) is given by us [17]."}, {"heading": "V. NUMERICAL SIMULATIONS", "text": "In this section we present some simulation results on the performance of the proposed control method."}, {"heading": "A. Dubins Car System", "text": "For the first simulation we consider a three-dimensional non-linear Dubins car, whose dynamics BO = BO BO BO = JJ = N cos\u03c6, y = v sin\u03c6, p = \u03c9, (8) where z: = (x, y, \u03c6) is the state of the system, p = (x, y) is the position, \u03c6 is the heading, v is the speed, and \u03c9 is the speed. The input (control) to the system is u: = (v, \u03c9). For simulation purposes we discredit the dynamics at a frequency of 10Hz. Our goal is to design a controller that controls the system to the equilibrium place z."}, {"heading": "B. A Simple 1D Linear System", "text": "For this simulation, we consider a simple 1D linear system zk + 1 = zk + uk, (11) where zk and uk are the state and input of the system at the time k. Although the dynamic model is very simple, it illustrates some key findings about the proposed method. Our goal is to design a controller that minimizes (9) starting from the state z0 = 1. We choose N = 30 and R = Q = Qf = 1. Since the dynamics are assumed to be unknown, we use aDOBO to learn the dynamics. In this case, these are the parameters to be learned. The learning process converges into 45 iterations to true optimal performance (J-0 = 1.61), which is calculated using LQR on the real system. The converged parameters are 1 = 1.69 and 2 = 2.45 parameters, which differ significantly from the true parameters."}, {"heading": "C. Cart-pole System", "text": "Next, we apply aDOBO to a basket pole system (M + m) x \ufffd \u2212 ml\u043d \ufffd cos\u043d = ml\u043d \ufffd cos\u043d = F, l\u043d \u2212 g sin\u043d = x \ufffd cos\u043d, (12) where x denotes the position of the basket with mass M, \u043d denotes the pendulum angle, and F is a force that serves as a control input. The mass pendulum is of length l with mass m at its end. Let's define the system state as z: (x, x, x, and the input as u: = F. Starting from this state (0, 0, 0, \u03c06, 0), the goal is to keep the pendulum straight while keeping the state within the given lower and upper limits. Specifically, we want to minimize the cost J0 (z N 0, u N \u2212 1) = N \u2212 K = 0 (zTk Qzk Qzk + u kRuk) + zNnamifzi that we minimize the dynamics."}, {"heading": "VI. COMPARISON WITH OTHER METHODS", "text": "In this section, we will compare our approach with some other online learning schemes for controllers design.A. Tuning (BO, R) vs aDOBOIn this section, we will consider the case where the cost function J0 is square (see Eq. (9)). Suppose that the actual linearization of the system by z * = 0 and u * = 0 is less known and given in this section than (A *, B *). To overcome this problem, we propose to optimize the controller by using the LQR control for linearized dynamics. However, the resulting controller may be suboptimal for the actual nonlinear system. Authors in [5], [6] seek to optimize the controller by tuning penalty matrices Q and R in (9). In particular, the authors dissolve solvestional connections between authors."}, {"heading": "VII. QUADROTOR POSITION TRACKING EXPERIMENTS", "text": "We present the results of our experiments on Crazyflie 2.0, which is an open source nano quadrotor platform developed by Bitcraze = 23. Its small size, low cost and robustness make it an ideal platform for testing new control paradigms. However, it has recently been extensively used to demonstrate aggressive flights [21], [22].For small greed, the quadrotor system is modeled as a rigid body with a ten-dimensional state vector that encompasses the position p = (x, y, z) in an inertial frame in which the quadrotor system is modeled as a rigid model with a ten-dimensional state vector s. (posture, vz) is represented by Euler angles and angular speeds. The system is controlled via three inputs u: = [u1, u2, u3] where u1 is the thrust along the z-axis, and 3 are rolling moments each."}, {"heading": "VIII. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we present aDOBO, an active learning framework for optimizing system dynamics with the goal of maximizing control performance. Through simulations and real-world experiments, we show that aDOBO achieves optimal control performance even when no prior information about system dynamics is known. In future work, it will be interesting to generalize aDOBO to optimize the dynamics for a class of cost functions. Utilizing state and input-path data along with the observed performance to further increase data efficiency of the learning process is another promising direction. Finally, it will be interesting to see how aDOBO can scale to more complex nonlinear dynamic models."}], "references": [{"title": "Taking the human out of the loop: A review of Bayesian optimization", "author": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"], "venue": "Proceedings of the IEEE, vol. 104, no. 1, pp. 148\u2013175, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Gaussian processes for data-efficient learning in robotics and control", "author": ["M.P. Deisenroth", "D. Fox", "C.E. Rasmussen"], "venue": "Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning with misspecified model classes", "author": ["J. Joseph", "A. Geramifard", "J.W. Roberts", "J.P. How", "N. Roy"], "venue": "International Conference on Robotics and Automation, 2013, pp. 939\u2013946.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Model learning for robot control: a survey", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": "Cognitive Processing, vol. 12, no. 4, pp. 319\u2013340, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic LQR tuning based on Gaussian process global optimization", "author": ["A. Marco", "P. Hennig", "J. Bohg", "S. Schaal", "S. Trimpe"], "venue": "International Conference on Robotics and Automation, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "A selftuning LQR approach demonstrated on an inverted pendulum", "author": ["S. Trimpe", "A. Millane", "S. Doessegger", "R. D\u2019Andrea"], "venue": "IFAC Proceedings Volumes, vol. 47, no. 3, pp. 11 281\u201311 287, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Feedback controller parameterizations for reinforcement learning", "author": ["J.W. Roberts", "I.R. Manchester", "R. Tedrake"], "venue": "Symposium on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2011, pp. 310\u2013317.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian optimization for learning gaits under uncertainty", "author": ["R. Calandra", "A. Seyfarth", "J. Peters", "M.P. Deisenroth"], "venue": "Annals of Mathematics and Artificial Intelligence, vol. 76, no. 1, pp. 5\u201323, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Implicit and explicit LQG self-tuning controllers", "author": ["M. Grimble"], "venue": "Automatica, vol. 20, no. 5, pp. 661\u2013669, 1984.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1984}, {"title": "A generalized LQG approach to self-tuning control part i. aspects of design", "author": ["D. Clarke", "P. Kanjilal", "C. Mohtadi"], "venue": "International Journal of Control, vol. 41, no. 6, pp. 1509\u20131523, 1985.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1985}, {"title": "Nonlinear adaptive control using nonparametric Gaussian process prior models", "author": ["R. Murray-Smith", "D. Sbarbaro"], "venue": "IFAC Proceedings Volumes, vol. 35, no. 1, pp. 325\u2013330, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Adaptive control: stability, convergence and robustness", "author": ["S. Sastry", "M. Bodson"], "venue": "Courier Corporation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["H.J. Kushner"], "venue": "Journal of Basic Engineering, vol. 86, p. 97, 1964.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1964}, {"title": "Gaussian processes for global optimization", "author": ["M.A. Osborne", "R. Garnett", "S.J. Roberts"], "venue": "Learning and Intelligent Optimization (LION3), 2009, pp. 1\u201315.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "On bayesian methods for seeking the extremum", "author": ["J. Mo\u010dkus"], "venue": "Optimization Techniques IFIP Technical Conference, 1975.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1975}, {"title": "The linear-quadratic optimal regulator for descriptor systems: discrete-time case", "author": ["D.J. Bender", "A.J. Laub"], "venue": "Automatica, 1987.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1987}, {"title": "BayesOpt: a bayesian optimization library for nonlinear optimization, experimental design and bandits.", "author": ["R. Martinez-Cantin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "YALMIP: A toolbox for modeling and optimization in MATLAB", "author": ["J. Lofberg"], "venue": "International Symposium on Computer Aided Control Systems Design, 2005, pp. 284\u2013289.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Planning and control for quadrotor flight through cluttered environments", "author": ["B. Landry"], "venue": "Master\u2019s thesis, MIT, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning quadrotor dynamics using neural network for flight control", "author": ["S. Bansal", "A.K. Akametalu", "F.J. Jiang", "F. Laine", "C.J. Tomlin"], "venue": "Conference on Decision and Control, 2016, pp. 4653\u20134660.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Parameter identification of an autonomous quadrotor", "author": ["N. Abas", "A. Legowo", "R. Akmeliawati"], "venue": "International Conference On Mechatronics, 2011, pp. 1\u20138.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Hence, we employ BO, an optimization method often used to optimize a performance criterion while keeping the number of evaluations of the physical system small [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "In online methods, the dynamics model is instead iteratively updated using new data collected by evaluating the controller [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "introduce sufficient inaccuracies to lead to suboptimal control performance [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Using machine learning techniques, such as Gaussian processes, does not alleviate this issue [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Instead, authors in [3] proposed to optimize the dynamics model directly with respect to the controller performance, but since", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "In [5]\u2013[7] authors tuned the penalty", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [5]\u2013[7] authors tuned the penalty", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "Parameters of a linear feedback controller are learned in [8] using BO.", "startOffset": 58, "endOffset": 61}, {"referenceID": 11, "context": "The problem of updating a system model to improve control performance is also related to adaptive control, where the model parameters are identified from sensor data, and subsequently the updated model is used to design a controller (see [9]\u2013[13]).", "startOffset": 242, "endOffset": 246}, {"referenceID": 11, "context": "A linear parameterization is also used in adaptive control for similar reasons [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "variables have a joint Gaussian distribution dependent on the values of \u03b8 [14].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "In the experimental section, we employ the 5/2 Mat\u00e8rn kernel where the hyperparameters are optimized by maximizing the marginal likelihood [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [16].", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "Different acquisition functions are used in literature to trade off between exploration and exploitation during the optimization process [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "For example, the next evaluation for expected improvement (EI) acquisition function [17] is given by \u03b8\u2217 = arg min\u03b8 \u03b1 (\u03b8) where", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "For further details of LQR method, we refer interested readers to [18].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "For BO, we use the MATLAB library BayesOpt [19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 18, "context": "The optimal control problem for a particular linearization is a convex MPC problem and solved using YALMIP [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "To overcome this problem, authors in [5], [6] propose to optimize the controller by tuning penalty matrices Q and R in (9).", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "To overcome this problem, authors in [5], [6] propose to optimize the controller by tuning penalty matrices Q and R in (9).", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "The optimization problem in (14) is solved using BO in a similar fashion as we solve (5) [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "proach is to directly parameterize and optimize the feedback matrix K \u2208 Rxu in (14) as [8]", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "6: Dubins car: Comparison between (Q,R) tuning [5] (dashed curves), and aDOBO (solid curves) for different noise levels in (A\u2217, B\u2217).", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "7: Mean and standard deviation of \u03b7 obtained via directly learning K [8] and aDOBO for different cost functions.", "startOffset": 69, "endOffset": 72}, {"referenceID": 19, "context": "Recently, it has been extensively used to demonstrate aggressive flights [21], [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "Recently, it has been extensively used to demonstrate aggressive flights [21], [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "The full non-linear dynamics of Crazyflie can be obtained from [23] using the physical parameters computed in [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "The full non-linear dynamics of Crazyflie can be obtained from [23] using the physical parameters computed in [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "In this experiment, our goal is to track a desired position p\u2217 starting from the initial position p0 = [0, 0, 1].", "startOffset": 103, "endOffset": 112}, {"referenceID": 21, "context": "Given the dynamics in [23], the desired optimal control problem can be solved using LQR; however, the resultant controller may not be optimal for the actual system since (a) true underlying system is non-", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "linear (b) the actual system may not follow the dynamics in [23] due to several unmodeled effects, as illustrated in our results.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "Method Advantages Limitations (Q,R) learning [5] Only (nx + nu) parameters are to be learned so learning will be faster.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "F learning [8] Only nxnu parameters are to be learned so learning will be faster.", "startOffset": 11, "endOffset": 14}, {"referenceID": 21, "context": "For comparison, we compute the nominal optimal controller using the full dynamics in [23].", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "Real-world robots are becoming increasingly complex and commonly act in poorly understood environments where it is extremely challenging to model or learn their true dynamics. Therefore, it might be desirable to take a taskspecific approach, wherein the focus is on explicitly learning the dynamics model which achieves the best control performance for the task at hand, rather than learning the true dynamics. In this work, we use Bayesian optimization in an active learning framework where a locally linear dynamics model is learned with the intent of maximizing the control performance, and used in conjunction with optimal control schemes to efficiently design a controller for a given task. This model is updated directly based on the performance observed in experiments on the physical system in an iterative manner until a desired performance is achieved. We demonstrate the efficacy of the proposed approach through simulations and real experiments on a quadrotor testbed.", "creator": "LaTeX with hyperref package"}}}