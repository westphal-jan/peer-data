{"id": "1703.09327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Iterative Noise Injection for Scalable Imitation Learning", "abstract": "In Imitation Learning, a supervisor's policy is observed and the intended behavior is learned. A known problem with this approach is covariate shift, which occurs because the agent visits different states than the supervisor. Rolling out the current agent's policy, an on-policy method, allows for collecting data along a distribution similar to the updated agent's policy. However this approach can become less effective as the demonstrations are collected in very large batch sizes, which reduces the relevance of data collected in previous iterations. In this paper, we propose to alleviate the covariate shift via the injection of artificial noise into the supervisor's policy. We prove an improved bound on the loss due to the covariate shift, and introduce an algorithm that leverages our analysis to estimate the level of $\\epsilon$-greedy noise to inject. In a driving simulator domain where an agent learns an image-to-action deep network policy, our algorithm Dart achieves a better performance than DAgger with 75% fewer demonstrations.", "histories": [["v1", "Mon, 27 Mar 2017 22:26:16 GMT  (923kb,D)", "http://arxiv.org/abs/1703.09327v1", null], ["v2", "Wed, 18 Oct 2017 03:52:18 GMT  (4585kb,D)", "http://arxiv.org/abs/1703.09327v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michael laskey", "jonathan lee", "wesley hsieh", "richard liaw", "jeffrey mahler", "roy fox", "ken goldberg"], "accepted": false, "id": "1703.09327"}, "pdf": {"name": "1703.09327.pdf", "metadata": {"source": "META", "title": "Iterative Noise Injection for Scalable Imitation Learning", "authors": ["Michael Laskey", "Jonathan Lee", "Wesley Hsieh", "Richard Liaw", "Jeffrey Mahler", "Roy Fox", "Ken Goldberg"], "emails": ["<laskeymd@berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Problem Statement", "text": "We assume that we assume access to a robot or a simulator, so that we assume a sequence of action sequences induced by a sequence of actions. Finally, we assume that we can get access to a supervisor who pretends to provide a state with an action that could be noisy.Policies and state densities. We assume that the set consists of observable states for an agent. We assume that S considers a finite number of states. We consider an action of size K =. We assume that dynamics such as Markovian, so that the probability of a state can be set st + 1."}, {"heading": "3. Off and On-Policy Imitation Learning", "text": "One approach to optimizing Equation 1 is to train an agent based on courses queried from a different distribution p (\u03c0\u03b8). If p (\u03c0\u03b8) is close to the distribution p (\u03c0\u03b8) of the learned agent, it can be expected that the performance of the agent is similar. Algorithms designed to select p (\u043d\u043a) can be considered to ask a prior about the distribution p (\u043d\u043a | \u03c0\u043a) of the learned agent. Questions about how best to select the prior, \u043d\u0438, and how narrow the distributions must be have led to two different classes of algorithms (Syed & Schapire, 2010; Daume \u0301 et al., 2009)."}, {"heading": "3.1. Off-Policy IL", "text": "& & & 10; & & 10; & & 10; & & 10; & & 10; & & 10; & & 10; & & 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & & & & # 10; & & & & & # 10; & # 10; & & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & & & & & 10; & & & 10; & & & & 10; & & & 10; & # 10; & & & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & # 10; & & # 10; & & & & # 10; & & & # 10; & & # 10; & & & & # 10; & & # 10; & & & # 10; & & # 10; & & & & # 10; & & & & & & # 10; & & & # 10; & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & & & # 10; & & & # 10; & & # 10; & & & & # 10; & & & & # 10; & & & & # 10; & & & # 10; & & & & & # 10; & & & & # 10; & & & & & & & & & # 10; & & & & & # 10; & & & # 10; & & & & # 10; & & & & & # 10; & & & & # 10; & & & & & & & & # 10; & & & & & # 10; & & & & & # 10; & & & & # 10; & & & & & & 10; & & & & & & & & & & &"}, {"heading": "3.2. On-Policy IL", "text": "Similar to policy gradient algorithms in Reinforcement Learning, on-policy IL methods sample trajectories from the current distribution of the agent and update the model based on the data obtained (Schulman et al., 2015; Daume \u0301 et al., algorithm 1 DAgger Input: \u03b2 Initialize: 0 for n = 0 to N \u2212 1 dofor m = 1 to M do., m \u00b2 p: 0 to M do, m \u00b2 p: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 0 to D: 1: 0 to D: 0 to D: 0 to D: 0: 0 to D: 0 to D: 0 to D: 0."}, {"heading": "4. The Covariate Shift Problem", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Bounding the Mismatch", "text": "In the field of statistical learning, it is unlikely that a learned model generalizes the number of errors made by the agent during execution for the binary classification case. (Quionero-Candela et al., 2009) Theorem 4.1 (Ross and Bagnell 2010) Denote by \u03c0\u03b8 a policy found using Off-Policy IL. The following inequality states: | E\u03b8 J (\u03b8, \u03b8) - Eriminante sales representative J (\u03b8, \u03b8)."}, {"heading": "4.2. The Effect on Off-Policy IL", "text": "Assuming that both the supervisor and the agent have deterministic distributions over controls, we can show the following: Theorem 4.3 denote a policy by \u03c0\u03b8, with a deterministic supervisor. The following inequality applies: | E\u03b8 J (\u03b8, \u03b8 |) \u2212 Eordable J (\u03b8, \u03b8 |). Consider, for example, the situation in which there are M possible pathways with time horizon T = 100. The supervisor's distribution indicates a uniform probability for each pathway that has occurred (i.e., p = zipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipT = T). Consider the situation in which M possible pathways with time horizon T = 100. The supervisor's distribution indicates a uniform probability for each pathway that has occurred (i.e. p = zipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipzipkt = T = T."}, {"heading": "5. Stochastic Off Policy IL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Stochastic Supervisor", "text": "In the previous section, Lemma 4.2 illustrated how the expected perfect agreement between the supervisor and the agent can control the entire range of variations in their distributions (in the case of a long-term horizon, however, any deviation in action between the supervisor and the agent along a path can lead to very different states that are visited in practice. We can overcome this by adding deterministic distribution to the distribution of the supervisor. Even if the supervisor and the agent deviate along a path, any deviation in action between the supervisor and the agent could still follow the policy of the unknown agent. A choice of stochasicality to be taken into account is the -greedy probability distribution, which is defined as: the action of the supervisor and the agent in politics."}, {"heading": "5.2. Optimizing the Mismatch", "text": "A technique for selecting the correct noise level is the search for possible values of noise, however, in tasks that require a real execution of the policy. (J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J J = J = J = J = J = J = J J = J = J J = J = J = J J = J = J = J = J J = J = J J = J = J = J J = J J J = J = J = J J J = J = J = J = J"}, {"heading": "6. Experiments", "text": "In the experiments we look at two areas: Grid-World and a driving simulator. Grid-World domain has a low state variance and a low state space. The driving simulator has a high state variance and a state representation based on a high-dimensional image."}, {"heading": "6.1. Grid-World Domain", "text": "In Grid-World, we have an agent who tries to reach a target state where he receives a + 10 reward. The agent receives \u2212 10 reward when he touches a penalty state. The agent has a state range of (x, y) coordinates and a series of actions consisting of {Left, Right, Forward, Backward, Stay}. The grid size for the environment is 30 x 30. A state is randomly marked as a penalty state with a probability of 8%. For the transition dynamics p (st + 1 | st, at), the agent enters an adjacent state that differs from the desired one, randomly with a probability of 0.16. The time horizon for policy is T = 70. The agent must learn to be resilient to the noise in the dynamics, reach the target state and then stay there. We use Value Iteration to determine optimal supervision. We conduct all studies of 50 randomly generated environments. We report on normalized performance, where the reward represents 1.0 or the expected supervision."}, {"heading": "6.1.1. BATCH LEARNING WITH DART", "text": "In this experiment, we will try to optimize iteratively for the best value and explore how the variation in batch size M affects the two on-policy and off-policy algorithms. We will optimize using batch sizes M = 10 and M = 20. Our policy class is a linear SVM trained in Sci-Kit Learn (Pedregosa et al., 2011).We will compare the deterministic off-policy approach (i.e. supervised learning) and DAgger. We will also compare a stochastic mixture of the supervisor and the current robotic policy, where \u03b2 = 0.5, i.e. with a 50% probability, the action of the agent is performed instead of the supervisor. This comparison is useful to test the need for the more conservative -greedy distribution. We refer to this method as' 50 / 50 '.For Dart, we will report two initializations 0 = 0.4 and0 = 0.5. For DAgger, we will use \u03b2-search function {9,} above 0.1, we will find the best indicator."}, {"heading": "6.1.2. DART\u2019S SENSITIVITY TO INITIALIZATION", "text": "We will now examine how sensitive Dart is to the selection of 0. We will also record the performance of the selected 0 when it is fixed with each iteration. Ideally, we would like Dart to be robust to this parameter and get the agent to converge to a good policy. We chose a batch size of M = 10 and used a linear SVM as the political class. The results shown in Fig. 2 suggest that Dart is robust when the selection of 0 increases with the increasing number of iterations. For example, 0 = 0.8 has a performance of 0.25 if it remains unchanged for all iterations, but with Dart's adaptability it converges to 0.61. We find that there is a fixed 0 = 0.5 converging to a value of 0.1 higher than Dart when the normalized performance remains unchanged. So, if an application allows an exhaustive search for the parameters, it may be more advantageous than an adaptive algorithm."}, {"heading": "6.2. Driving Simulator", "text": "In order to test our algorithms in areas of high state variance, we have developed a driving simulator in which the agent must learn to drive around other vehicles; the other vehicles are randomly placed on the road, so the agent must learn a policy that generalizes random vehicle configurations; each car has a different way of dealing with an internal state of position, acceleration and speed; the position specifies both the ratio and rotation; the agent's car has initial state variance that extends over the other cars; the other cars have uniform initial state variance over the ratio, but they have always driven forward; there are 5 other vehicles per lane; the agent drives twice as fast as the other vehicles, so he must learn to navigate around them; the state space of the simulator is gray-scaled."}, {"heading": "7. Conclusion & Future Work", "text": "Finally, we have provided an algorithm, Dart, which enables the robustness of the initial selection and speeds up learning. Our analysis in Theroem 4.3 shows that an expected cumulative replacement loss is not always a good indicator of agent performance. The proposed idea of perfect match provides new insights into the problem of covariant shift by examining errors at the trajectory level. In future work, we hope to determine the decay rate of this quantity using the Rademacher complexity analysis. While Dart is able to perform well in the areas presented, it can be challenging for large or continuous scope of action. In the scope of action, the problem of predicting the policy of the learned agent will naturally become more difficult. In future work, we will investigate distributions of continuous actions involving the concentration of the agent, or investigate alternative scope of action to estimate the loss of monitoring by Menlettes."}, {"heading": "8. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1. Additional Experiments", "text": "In Grid World we have a robot that tries to reach a target state, where it receives a reward of + 10. The robot receives a reward of \u2212 10 if it touches a penalty state. The robot also receives a penalty of \u2212 0.02 for each empty state. The robot must learn to be resilient to the noise in the dynamics, reach the target state and then stay there. The robot has a state space of (x, y) coordinates and a series of actions consisting of {Left, Right, Forward, Backward, Stay} state. The grid size for the environment is 30 x 30. 8% of the randomly drawn states are marked as punishment, while only one is a target state. For the transition dynamics p (st + 1 | st, at) the robot goes into an adjacent state, which differs from the desired one, with the probability 0.16 being uniform. The time horizon for the policy is T = 70. We use Value Iteration to calculate an optimal supervisor, we expect all of the SVR studies to be randomly generated over 100 or M."}, {"heading": "6.1.3. SWEEPING", "text": "In this experiment, we are interested in which setting is best for different types of agent policies. We consider two policy classes as linear SVM and a decision tree with depth 4. The reason for two different classes is to see how different test errors work best. Each model is trained in Sci-Kit Learn (Pedregosa et al., 2011). The results shown in Fig. 5 show performance over a range of parameters. We observed the best 0.5 and 0.6 for the linear SVM, which had an average E\u03b8J (\u03b8, \u03b8) = 0.2, but for the decision tree, which had an average EsuccessJ (\u03b8, \u03b8) = 0.04 test error, the best noise date was between 0.1 and 0.2."}, {"heading": "6.2. Theoretical Analysis", "text": "Lemma 4.2 For two deterministic agents with distributions on the paths p (=) and p (=), the following inequality applies: \u2212 p (=). \u2212 p (\u2212 p) \u2212 p (=). \u2212 p (=). (p) p (= p). (p) p (= p). (p) p (= p). (p) p (= p). (p). (p) p (= p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (). (p). (). (p). (). (p).). (p)."}], "references": [{"title": "Apprenticeship learning via reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the twentyfirst international conference on Machine learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["Argall", "Brenna D", "Chernova", "Sonia", "Veloso", "Manuela", "Browning", "Brett"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Training with exploration improves a greedy stack-lstm parser", "author": ["Ballesteros", "Miguel", "Goldberg", "Yoav", "Dyer", "Chris", "Smith", "Noah A"], "venue": "arXiv preprint arXiv:1603.03793,", "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Bartlett", "Peter L", "Mendelson", "Shahar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2002}, {"title": "Convexity, classification, and risk bounds", "author": ["Bartlett", "Peter L", "Jordan", "Michael I", "McAuliffe", "Jon D"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Searchbased structured prediction", "author": ["Daum\u00e9", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "Probability models in engineering and science", "author": ["Esmaili", "Ali"], "venue": null, "citeRegEx": "Esmaili and Ali.,? \\Q2006\\E", "shortCiteRegEx": "Esmaili and Ali.", "year": 2006}, {"title": "Deep learning for realtime atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In NIPS,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Hazan", "Elad", "Kalai", "Adam", "Kale", "Satyen", "Agarwal", "Amit"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "Imitation learning by coaching", "author": ["He", "Eisner", "Jason", "Daume", "Hal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Maximum mean discrepancy imitation learning", "author": ["Kim", "Beomjoon", "Pineau", "Joelle"], "venue": "In Robotics Science and Systems,", "citeRegEx": "Kim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Comparing human-centric and robotcentric sampling for robot deep learning from demonstrations", "author": ["Laskey", "Michael", "Chuck", "Caleb", "Lee", "Jonathan", "Mahler", "Jeffrey", "Krishnan", "Sanjay", "Jamieson", "Kevin", "Dragan", "Anca", "Goldberg", "Ken"], "venue": "arXiv preprint arXiv:1610.00850,", "citeRegEx": "Laskey et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Laskey et al\\.", "year": 2016}, {"title": "Smooth imitation learning for online sequence prediction", "author": ["Le", "Hoang M", "Kang", "Andrew", "Yue", "Yisong", "Carr", "Peter"], "venue": "arXiv preprint arXiv:1606.00968,", "citeRegEx": "Le et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Le et al\\.", "year": 2016}, {"title": "Asymptotics in statistics: some basic concepts", "author": ["Le Cam", "Lucien", "Yang", "Grace Lo"], "venue": "Springer Science & Business Media,", "citeRegEx": "Cam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cam et al\\.", "year": 2012}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Pomerleau", "Dean A"], "venue": "Technical report, Carnegie-Mellon University,", "citeRegEx": "Pomerleau and A.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1989}, {"title": "Dataset shift in machine learning", "author": ["Quionero-Candela", "Joaquin", "Sugiyama", "Masashi", "Schwaighofer", "Anton", "Lawrence", "Neil D"], "venue": null, "citeRegEx": "Quionero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quionero.Candela et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "J Andrew"], "venue": "arXiv preprint arXiv:1011.0686,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael", "Moritz", "Philipp"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the conference on empirical methods in natural language processing, pp. 1070\u20131079", "author": ["Settles", "Burr", "Craven", "Mark"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Settles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Settles et al\\.", "year": 2008}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": null, "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2011}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "In Imitation Learning (IL), an agent learns to mimic a supervisor on a task involving sequential decisions (Argall et al., 2009).", "startOffset": 107, "endOffset": 128}, {"referenceID": 2, "context": "The generality of this approach has led to a wide range of applications: parsing sentence structure (Ballesteros et al., 2016), playing video games (Guo et al.", "startOffset": 100, "endOffset": 126}, {"referenceID": 7, "context": ", 2016), playing video games (Guo et al., 2014), robotic manipulation (Laskey et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 17, "context": "On-Policy methods, such as DAgger, attempt to remedy this by sampling trajectories using the current agent\u2019s policy (Ross et al., 2010) and querying the supervisor at each state for the correct action.", "startOffset": 116, "endOffset": 135}, {"referenceID": 7, "context": "On-Policy methods have been shown to empirically converge faster than existing OffPolicy methods (Guo et al., 2014).", "startOffset": 97, "endOffset": 115}, {"referenceID": 19, "context": "Similar results have been shown in On-Policy Reinforcement Learning and active learning settings (Schulman et al., 2015; Settles & Craven, 2008).", "startOffset": 97, "endOffset": 144}, {"referenceID": 17, "context": "We measure the difference between controls using a surrogate loss l : A \u00d7 A \u2192 R (Ross et al., 2010; Ross & Bagnell, 2010).", "startOffset": 80, "endOffset": 121}, {"referenceID": 5, "context": "The questions of how to best select the prior, \u03c0\u03b8\u0302 ,and how close the distributions need to be has led to two different classes of algorithms (Syed & Schapire, 2010; Daum\u00e9 et al., 2009)", "startOffset": 142, "endOffset": 185}, {"referenceID": 4, "context": "It is important to note that during optimization of \u03b8 our current indicator loss function l should be replaced with a smooth classification calibrated loss function such as the Hinge Loss (Bartlett et al., 2006).", "startOffset": 188, "endOffset": 211}, {"referenceID": 17, "context": "This notion of a distribution mismatch, or covariate shift, has led to both theoretical and empirical evidence (Ross et al., 2010; Guo et al., 2014) that suggests off-policy learning is not a robust algorithm.", "startOffset": 111, "endOffset": 148}, {"referenceID": 7, "context": "This notion of a distribution mismatch, or covariate shift, has led to both theoretical and empirical evidence (Ross et al., 2010; Guo et al., 2014) that suggests off-policy learning is not a robust algorithm.", "startOffset": 111, "endOffset": 148}, {"referenceID": 9, "context": "A large number of extensions to DAgger have been proposed, such as modifying the supervisor\u2019s policy to be easier to learn (He et al., 2012; Levine et al., 2015) or querying the supervisor for only informative states (Kim & Pineau, 2013; Laskey et al.", "startOffset": 123, "endOffset": 161}, {"referenceID": 14, "context": "A large number of extensions to DAgger have been proposed, such as modifying the supervisor\u2019s policy to be easier to learn (He et al., 2012; Levine et al., 2015) or querying the supervisor for only informative states (Kim & Pineau, 2013; Laskey et al.", "startOffset": 123, "endOffset": 161}, {"referenceID": 8, "context": "\u2016\u2207\u03b8nE\u03b8nJ(\u03b8, \u03b8\u2217|\u03be)\u2016 \u2264 G) (Hazan et al., 2006) and the number of iterations, N .", "startOffset": 24, "endOffset": 44}, {"referenceID": 16, "context": "Under a significant shift between the two distributions, it is unlikely for a learned model to generalize to the shifted test distribution (Quionero-Candela et al., 2009).", "startOffset": 139, "endOffset": 170}, {"referenceID": 17, "context": "This result has been a motivation for many to perform onpolicy methods (Ross et al., 2010; Le et al., 2016; Levine et al., 2015).", "startOffset": 71, "endOffset": 128}, {"referenceID": 12, "context": "This result has been a motivation for many to perform onpolicy methods (Ross et al., 2010; Le et al., 2016; Levine et al., 2015).", "startOffset": 71, "endOffset": 128}, {"referenceID": 14, "context": "This result has been a motivation for many to perform onpolicy methods (Ross et al., 2010; Le et al., 2016; Levine et al., 2015).", "startOffset": 71, "endOffset": 128}, {"referenceID": 17, "context": "supervised learning) does poorly in this domain, which is consistent with prior literature (Ross et al., 2010).", "startOffset": 91, "endOffset": 110}], "year": 2017, "abstractText": "In Imitation Learning, a supervisor\u2019s policy is observed and the intended behavior is learned. A known problem with this approach is covariate shift, which occurs because the agent visits different states than the supervisor. Rolling out the current agent\u2019s policy, an on-policy method, allows for collecting data along a distribution similar to the updated agent\u2019s policy. However this approach can become less effective as the demonstrations are collected in very large batch sizes, which reduces the relevance of data collected in previous iterations. In this paper, we propose to alleviate the covariate shift via the injection of artificial noise into the supervisor\u2019s policy. We prove an improved bound on the loss due to the covariate shift, and introduce an algorithm that leverages our analysis to estimate the level of -greedy noise to inject. In a driving simulator domain where an agent learns an image-to-action deep network policy, our algorithm Dart achieves a better performance than DAgger with 75% fewer demonstrations.", "creator": "LaTeX with hyperref package"}}}