{"id": "1703.09387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Adversarial Transformation Networks: Learning to Generate Adversarial Examples", "abstract": "Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier's outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.", "histories": [["v1", "Tue, 28 Mar 2017 03:24:33 GMT  (3353kb,D)", "http://arxiv.org/abs/1703.09387v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CV", "authors": ["shumeet baluja", "ian fischer"], "accepted": false, "id": "1703.09387"}, "pdf": {"name": "1703.09387.pdf", "metadata": {"source": "META", "title": "Adversarial Transformation Networks: Learning to Generate Adversarial Examples ", "authors": ["Shumeet Baluja", "Ian Fischer"], "emails": [], "sections": [{"heading": "1. Introduction and Background", "text": "In fact, it is not the case that one would have been able to engage with the \"wrong\" answers, but with the \"wrong\" answers, the \"wrong\" answers to the \"wrong\" questions of the \"wrong\" questions, the \"wrong\" answers to the \"wrong\" questions of the \"wrong\" answers, \"the\" wrong \"answers,\" the \"wrong\" answers to the \"wrong\" questions, \"the\" wrong \"answers to the\" wrong \"questions,\" the \"wrong\" answers to the \"wrong\" answers, \"the\" wrong \"answers to the\" wrong \"questions,\" the \"wrong\" answers to the \"wrong\" questions, \"the\" wrong \"answers to the\" wrong \"questions,\" the \"wrong\" answers to the \"wrong\" questions, \"the\" wrong answers, \"the\" the \"wrong answers,\" the \"the,\" the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the"}, {"heading": "2. Adversarial Transformation Networks", "text": "An ATN is a neural network that transforms an input into an adverse example against a target network or a series of networks. Therefore, ATNs cannot be targeted or targeted and can be trained in a black box manner. In this work, we will focus on the target network that outputs a probability distribution across class labels, and an ATN can be defined as a neural network: gf = argmax f (x): x \"X\" is the parameter vector of g, f is the target network that outputs a probability distribution across class labels, but argmax f = argmax f (x). To find gf, we must solve the following optimization: argmin \"xi\" X \u03b2LX \"(gf, xi), xi\" xi \"xi\" (xi), xi \"xi\" LY. \""}, {"heading": "2.1. Adversarial Example Generation", "text": "There are two approaches to creating opposing examples with an ATN: The ATN can be trained to produce only the disturbance of x, or it can be trained to produce an opposing autocoding of x. \u2022 Disturbance ATN (P-ATN): To simply produce a disturbance, it is sufficient to structure the ATN as a variation on the residual block (He et al., 2015): gf (x) = tanh (x + G (x))), where G (\u00b7) is the core function of gf. With small initial weight vectors, this structure makes it easy for the network to learn to produce small but effective disturbances. \u2022 Conflicting autocoding (AAE): AAE-ATNs are similar to standard encoders as they attempt to accurately reconstruct the original input, subject to regulation, such as weight decay or an additional noise signal."}, {"heading": "2.2. Related Network Architectures", "text": "This training goal is similar to the usual Generative Adversarial Network Training (Goodfellow et al., 2014a) in that the goal is to find weaknesses in the classifier. It is interesting to note the similarity of working outside the adversary's training paradigm - the recent use of feed-forward neural networks for the transmission of artistic style in images (Gatys et al., 2015) (Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \"back-driving networks\" (Linden & Kindermann, 1989) to modify the inputs of a fully trained network to find a set of inputs that maximize a set of outputs and activate hidden units. Unlike conventional network training, in which the gradients are used to modify the weights of the network, here the network weights are frozen al al, and the input itself is modified by the subsequent work in January et to replace a gradient-directed method."}, {"heading": "3. MNIST Experiments", "text": "To begin with the empirical research of a particular class, we will train five networks on the standard MNIST digit classification task (LeCun et al., 1998). The networks are trained and tested on the same data; they differ only in weight initialization and architecture, as shown in Table 1. Each network has a mixture of convolution (Conv) and fully connected (FC) levels. The input into the networks is a 28x28 grayscale image and the output is 10 logit units. Classifierp and Classifiera0 use the same architecture and differ only in the initialization of weights. We will primarily use classifierp for the experiments in this section. The other networks will be used later to analyze the generalization capabilities of the adversaries. Table 1 shows that all networks respond well to the digital recognition task.3We are trying to create an Adversarial Autocoding ATN that can target a particular class."}, {"heading": "4. A Deeper Look into ATNs", "text": "This section discusses three extensions to the basic ATNs: increasing the number of networks that can attack the ATNs, using hidden states in the target network, and using ATNs in serial and parallel form."}, {"heading": "4.1. Adversarial Transfer to Other Networks", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "4.2. \u201cInsider\u201d Information", "text": "In previous experiments, the classifier, C, was treated as a white field, from which two pieces of information were needed to train the ATN. Firstly, the actual results of C were used to generate the new target vector. Secondly, the error derivatives of the new target vector were routed through C and propagated into the ATN. In this section, we will examine the possibility of \"opening\" the classifier and accessing more of its internal state. From C, the actual hidden activations for each example can be used as additional inputs to the ATN. Intuitively, because the goal is to maintain as much similarity as possible with the original image and maintain the same sequence of non-uppermost classifications as the original image, access to these activations can transmit usable signals. Due to the very large number of hidden units associated with folding layers, in practice, we only use the penultimate completely connected layer of C. The results of the training of the ATN with this additional information are presented most interestingly in the table 6, where the two-fold information is between the classification."}, {"heading": "4.3. Serial and Parallel ATNs", "text": "In this section we will examine whether the ATNs can be used in parallel (can the same original image of each of the ATNs be successfully transformed?) and in the series (can the same image of one ATN then be successfully transformed so that the resulting image of another can be successfully transformed?) In the first test, we started with 1000 images of the digits from the test set. Each was guided through all 10 ATNs (ATNc, \u03b2 = 0.005); the resulting images were then classified with Classifierp. For each image, how many ATNs were able to successfully transform the image (success is defined for ATNt as causing the classification as the top class) was measured. Of the 1000 experiments, 283 were successfully transformed by all 10 of the ATNs. Examples of the results and a histogram of the results will be constructed in Figure 6.A second experiment in which the 10 ATNs are applied, one by one scenario."}, {"heading": "5. ImageNet Experiments", "text": "We are investigating the effectiveness of ATNs on the ImageNet dataset (Deng et al., 2009), which consists of 1.2 million natural images categorized into 1 of 1000 classes. The target classifier f used in these experiments is a pre-formed state-of-the-art classifier, Inception ResNet v2 (IR2), which has a 19.9% error rate when verifying 50,000 images, and a 4.9% error rate when verifying images in the top 5. It is fully described in Szegedy et al. (2016)."}, {"heading": "5.1. Experiment Setup", "text": "We trained AAE ATNs and P-ATNs to attack IR2 as described in Section 2. Training an ATN against IR2 follows the process described in Section 3.IR2 as input images scaled to 299 x 299 pixels of 3 channels each. To autocode images of this size for the AAE task, we use three different fully revolutionary architectures (Table 7): \u2022 IR2-Base-Deconv, a small architecture that uses the first few layers of IR2 and loads the pre-trained parameter values at the beginning of the ATN training, followed by deconvolutionary layers; \u2022 IR2-Resize-Conv, a small architecture that avoids checkerboard artifacts common in deconvolutionary layers by using bilinear size layers to minimize samples and upsample between step 1 convolutions; and \u2022 IR2-Conv-Deconv, a medium architecture that is a tower of upheavals followed by upheavals."}, {"heading": "5.2. Results Overview", "text": "However, the results in Figures 9 and 7 show that the use of an architecture such as IR2-Conv-FC can produce a qualitatively different type of opponent from the AAE approach. Examples generated by the perturbation approach preserve more pixels in the original image at the expense of a small region of large disturbances. In contrast to the approaches to the disturbance, the AAE architectures distribute the differences across wider regions of the image. However, IR2-Base-Deconv and IR2-Conv-Deconv tend to exhibit checkerboard patterns that are a common problem in image generation."}, {"heading": "5.3. Detailed Discussion", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "6. Conclusions and Future Work", "text": "We have presented a fundamentally different approach to finding examples by training neural networks to transform input into opposing examples. Our method is efficient in training, quick to execute, and produces remarkably diverse, successful opposing examples. Future work should explore the possibility of using ATNs in opposing training. A successful ATN-based system could pave the way to models with better generalization and robustness. Hendrik Metzen et al. (2017) recently demonstrated that it is possible to detect when input is contrary to current types of opponents. It may be possible to train such detectors at the ATN output. If so, using this signal as an additional loss for the Williams ATN could improve the results (as well as research into the use of a GAN discriminator during training)."}], "references": [{"title": "The virtues of peer pressure: A simple method for discovering high-value mistakes", "author": ["Baluja", "Shumeet", "Covell", "Michele", "Sukthankar", "Rahul"], "venue": "In Int. Conf. on Computer Analysis of Images and Patterns,", "citeRegEx": "Baluja et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baluja et al\\.", "year": 2015}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Carlini", "Nicholas", "Wagner", "David"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "Carlini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carlini et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "A neural algorithm of artistic", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "style. CoRR,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "On Detecting Adversarial Perturbations", "author": ["J. Hendrik Metzen", "T. Genewein", "V. Fischer", "B. Bischoff"], "venue": "ArXiv e-prints,", "citeRegEx": "Metzen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Metzen et al\\.", "year": 2017}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["Johnson", "Justin", "Alahi", "Alexandre", "Fei-Fei", "Li"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Adversarial examples for generative models", "author": ["Kos", "Jernej", "Fischer", "Ian", "Song", "Dawn"], "venue": "arXiv preprint arXiv:1702.06832,", "citeRegEx": "Kos et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kos et al\\.", "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["Kurakin", "Alexey", "Goodfellow", "Ian J", "Bengio", "Samy"], "venue": "CoRR, abs/1607.02533,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Adversarial machine learning at scale", "author": ["Kurakin", "Alexey", "Goodfellow", "Ian J", "Bengio", "Samy"], "venue": "CoRR, abs/1611.01236,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Inversion of multilayer nets", "author": ["Linden", "Alexander", "J. Kindermann"], "venue": "In Neural Networks,", "citeRegEx": "Linden et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Linden et al\\.", "year": 1989}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Liu", "Yanpei", "Chen", "Xinyun", "Chang", "Song", "Dawn"], "venue": "CoRR, abs/1611.02770,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["Miyato", "Takeru", "Maeda", "Shin-ichi", "Koyama", "Masanori", "Nakae", "Ken", "Ishii", "Shin"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Universal adversarial perturbations", "author": ["Moosavi-Dezfooli", "Seyed-Mohsen", "Fawzi", "Alhussein", "Omar", "Frossard", "Pascal"], "venue": "CoRR, abs/1610.08401,", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Moosavi-Dezfooli", "Seyed-Mohsen", "Fawzi", "Alhussein", "Frossard", "Pascal"], "venue": "In Proceedings of the IEEE CVPR,", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable", "author": ["Nguyen", "Anh Mai", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "images. CoRR,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Deconvolution and checkerboard artifacts. Distill, 2016", "author": ["Odena", "Augustus", "Dumoulin", "Vincent", "Olah", "Chris"], "venue": null, "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Jha", "Somesh", "Fredrikson", "Matt", "Celik", "Z Berkay", "Swami", "Ananthram"], "venue": "In Proceedings of the 1st IEEE European Symposium on Security and Privacy,", "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Goodfellow", "Ian"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Goodfellow", "Ian", "Jha", "Somesh", "Celik", "Z Berkay", "Swami", "Ananthram"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent", "Alemi", "Alex"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Stealing machine learning models via prediction apis", "author": ["Tram\u00e8r", "Florian", "Zhang", "Fan", "Juels", "Ari", "Reiter", "Michael K", "Ristenpart", "Thomas"], "venue": "In USENIX Security,", "citeRegEx": "Tram\u00e8r et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tram\u00e8r et al\\.", "year": 2016}, {"title": "Texture networks: Feed-forward synthesis of textures and stylized images", "author": ["Ulyanov", "Dmitry", "Lebedev", "Vadim", "Vedaldi", "Andrea", "Lempitsky", "Victor S"], "venue": "CoRR, abs/1603.03417,", "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 22, "context": "Seminal work by Szegedy et al. (2013) and Goodfellow et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 4, "context": "(2013) and Goodfellow et al. (2014b), as well as much recent work, has shown that adversarial examples are abundant, and that there are many ways to discover them.", "startOffset": 11, "endOffset": 37}, {"referenceID": 20, "context": "Until now, these optimization problems have been solved using three broad approaches: (1) By directly using optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as proposed in Szegedy et al. (2013) and Carlini & Wagner (2016).", "startOffset": 175, "endOffset": 197}, {"referenceID": 20, "context": "Until now, these optimization problems have been solved using three broad approaches: (1) By directly using optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as proposed in Szegedy et al. (2013) and Carlini & Wagner (2016). Such optimizer-based approaches tend to be much slower and more powerful than the other approaches.", "startOffset": 175, "endOffset": 225}, {"referenceID": 20, "context": "Attacks without internal access are possible by transferring successful attacks on one model to another model, as in Szegedy et al. (2013); Papernot et al.", "startOffset": 117, "endOffset": 139}, {"referenceID": 20, "context": "(2013); Papernot et al. (2016a), and others.", "startOffset": 8, "endOffset": 32}, {"referenceID": 20, "context": "(2013); Papernot et al. (2016a), and others. A more challenging class of blackbox attacks involves having no access to any relevant model, and only getting online access to the target model\u2019s output, as explored in Papernot et al. (2016b); Baluja et al.", "startOffset": 8, "endOffset": 239}, {"referenceID": 0, "context": "(2016b); Baluja et al. (2015); Tram\u00e8r et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 0, "context": "(2016b); Baluja et al. (2015); Tram\u00e8r et al. (2016). See Papernot et al.", "startOffset": 9, "endOffset": 52}, {"referenceID": 0, "context": "(2016b); Baluja et al. (2015); Tram\u00e8r et al. (2016). See Papernot et al. (2015) for a detailed discussion of threat models.", "startOffset": 9, "endOffset": 80}, {"referenceID": 8, "context": ", L2 loss or a perceptual similarity loss like Johnson et al. (2016)), LY is a specially-formed loss on the output space of f (described below) to avoid learning the identity function, and \u03b2 is a weight to balance the two loss functions.", "startOffset": 47, "endOffset": 69}, {"referenceID": 6, "context": "\u2022 Perturbation ATN (P-ATN): To just generate a perturbation, it is sufficient to structure the ATN as a variation on the residual block (He et al., 2015): gf (x) = tanh(x+G(x)), where G(\u00b7) represents the core function of gf .", "startOffset": 136, "endOffset": 153}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al.", "startOffset": 178, "endOffset": 198}, {"referenceID": 27, "context": ", 2015)(Ulyanov et al., 2016).", "startOffset": 7, "endOffset": 29}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \u201cback-driving networks\u201d (Linden & Kindermann, 1989) to modify the inputs of a fully-trained network to find a set of inputs that maximize a desired set of outputs and hidden unit activations.", "startOffset": 179, "endOffset": 242}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \u201cback-driving networks\u201d (Linden & Kindermann, 1989) to modify the inputs of a fully-trained network to find a set of inputs that maximize a desired set of outputs and hidden unit activations. Unlike standard network training in which the gradients are used to modify the weights of the network, here, the network weights are frozen and the input itself is changed. In subsequent work, Ulyanov et al. (2016) created a method to approximate the results of the gradient descent procedure through the use of an off-line trained neural network.", "startOffset": 179, "endOffset": 707}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \u201cback-driving networks\u201d (Linden & Kindermann, 1989) to modify the inputs of a fully-trained network to find a set of inputs that maximize a desired set of outputs and hidden unit activations. Unlike standard network training in which the gradients are used to modify the weights of the network, here, the network weights are frozen and the input itself is changed. In subsequent work, Ulyanov et al. (2016) created a method to approximate the results of the gradient descent procedure through the use of an off-line trained neural network. Ulyanov et al. (2016) removed the need for a gradient descent procedure to operate on every source image to which a new artistic style was to be applied, and replaced it with a single forward pass through a separate network.", "startOffset": 179, "endOffset": 862}, {"referenceID": 13, "context": "To begin our empirical exploration, we train five networks on the standard MNIST digit classification task (LeCun et al., 1998).", "startOffset": 107, "endOffset": 127}, {"referenceID": 19, "context": "Unlike many previous studies in attacking classifiers, the addition of salt-and-pepper type noise did not appear (Nguyen et al., 2014; MoosaviDezfooli et al., 2016b).", "startOffset": 113, "endOffset": 165}, {"referenceID": 16, "context": "ples that generalize to other classifiers? Much research has studied adversarial transfer for traditional adversaries, including the recent work of Moosavi-Dezfooli et al. (2016a); Liu et al.", "startOffset": 148, "endOffset": 180}, {"referenceID": 15, "context": "(2016a); Liu et al. (2016).", "startOffset": 9, "endOffset": 27}, {"referenceID": 2, "context": "We explore the effectiveness of ATNs on the ImageNet dataset (Deng et al., 2009), which consists of 1.", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": "We explore the effectiveness of ATNs on the ImageNet dataset (Deng et al., 2009), which consists of 1.2 million natural images categorized into 1 of 1000 classes. The target classifier, f , used in these experiments is a pre-trained state-of-the-art classifier, Inception ResNet v2 (IR2), that has a top-1 single-crop error rate of 19.9% on the 50,000 image validation set, and a top-5 error rate of 4.9%. It is described fully in Szegedy et al. (2016).", "startOffset": 62, "endOffset": 453}, {"referenceID": 20, "context": "However, IR2-Base-Deconv and IR2-Conv-Deconv tend to exhibit checkerboard patterns, which is a common problem in image generation with deconvolutions (Odena et al. (2016)).", "startOffset": 151, "endOffset": 171}, {"referenceID": 7, "context": "Indeed, Hendrik Metzen et al. (2017) recently showed that it may be possible to train a detector for previous adversarial attacks.", "startOffset": 16, "endOffset": 37}, {"referenceID": 11, "context": "In Kurakin et al. (2016b), the authors show the current state-of-the-art in", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": "This procedure conceptually resembles GAN training (Goodfellow et al., 2014a) in many ways, but the goal is different: for GANs, the focus is on using an easy-to-train discriminator to learn a hard-to-train generator; for this adversarial training system, the focus is on using easy-to-train generators to learn a hard-to-train multi-class classifier. Note also that we can run the adversarial example generation in this algorithm on unlabeled data, as described in Section 2. Miyato et al. (2016) also describe a method for using unlabeled data in a manner conceptually similar to adversarial training.", "startOffset": 52, "endOffset": 498}, {"referenceID": 6, "context": "Hendrik Metzen et al. (2017) recently showed that it is possible to detect when an input is adversarial, for current types of adversaries.", "startOffset": 8, "endOffset": 29}, {"referenceID": 6, "context": "Hendrik Metzen et al. (2017) recently showed that it is possible to detect when an input is adversarial, for current types of adversaries. It may be possible to train such detectors on ATN output. If so, using that signal as an additional loss for the ATN may improve the outputs. Similarly, exploring the use of a GAN discriminator during training may improve the realism of the ATN outputs. It would be interesting to explore the impact of ATNs on generative models, rather than just classifiers, similar to work in Kos et al. (2017). Finally, it may also be possible to train ATNs in a black-box manner, similar to recent work in Tram\u00e8r et al.", "startOffset": 8, "endOffset": 536}, {"referenceID": 6, "context": "Hendrik Metzen et al. (2017) recently showed that it is possible to detect when an input is adversarial, for current types of adversaries. It may be possible to train such detectors on ATN output. If so, using that signal as an additional loss for the ATN may improve the outputs. Similarly, exploring the use of a GAN discriminator during training may improve the realism of the ATN outputs. It would be interesting to explore the impact of ATNs on generative models, rather than just classifiers, similar to work in Kos et al. (2017). Finally, it may also be possible to train ATNs in a black-box manner, similar to recent work in Tram\u00e8r et al. (2016); Baluja et al.", "startOffset": 8, "endOffset": 654}, {"referenceID": 0, "context": "(2016); Baluja et al. (2015), or using REINFORCE (Williams, 1992) to compute gradients for the ATN using the target network simply as a reward signal.", "startOffset": 8, "endOffset": 29}], "year": 2017, "abstractText": "Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier\u2019s outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.", "creator": "LaTeX with hyperref package"}}}