{"id": "1703.09452", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "SEGAN: Speech Enhancement Generative Adversarial Network", "abstract": "Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.", "histories": [["v1", "Tue, 28 Mar 2017 08:39:06 GMT  (514kb,D)", "http://arxiv.org/abs/1703.09452v1", null], ["v2", "Fri, 21 Apr 2017 12:37:03 GMT  (514kb,D)", "http://arxiv.org/abs/1703.09452v2", "5 pages, 4 figures"], ["v3", "Fri, 9 Jun 2017 11:34:06 GMT  (514kb,D)", "http://arxiv.org/abs/1703.09452v3", "5 pages, 4 figures, accepted in INTERSPEECH 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.SD", "authors": ["santiago pascual", "antonio bonafonte", "joan serr\\`a"], "accepted": false, "id": "1703.09452"}, "pdf": {"name": "1703.09452.pdf", "metadata": {"source": "CRF", "title": "SEGAN: Speech Enhancement Generative Adversarial Network", "authors": ["Santiago Pascual", "Antonio Bonafonte", "Joan Serr\u00e0"], "emails": ["santi.pascual@upc.edu,", "antonio.bonafonte@upc.edu", "joan.serra@telefonica.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most people who are able to deny themselves, deny themselves and overburden themselves, overburden themselves. In fact, it is so that they are able to deny themselves. In fact, it is so that they are able to deny themselves. In fact, it is so that they are able to deny themselves. In fact, it is so that they are able to deny themselves. In fact, it is so that they are able to deny themselves. In fact, it is so that they are able to deny themselves."}, {"heading": "2. Generative Adversarial Networks", "text": "GANs [19] are generative models that learn to classify samples z from some previous distributions Z to samples x from other distributions X, whereas samples x from other distributions X, which is one of the training examples (e.g. images, audio, etc.) The component within the GAN structure that performs the mapping is called a generator (G), and its main task is to learn an effective mapping that can mimic the real data distribution in order to generate novel samples related to those of the training. Importantly, G does this not by memorizing input-output pairs, but by mapping the data distribution characteristics to the multiplicity defined in our previous Z. The way in which G learns to do the mapping is by means of an adversarial training, where we have another component that is called a discriminator (iD) that comes from either a more fictitious data classification (iD) or its own."}, {"heading": "3. Speech Enhancement GAN", "text": "It is about the question of how people in the world will be able to put themselves in the world, and about the question of whether they will be able to put themselves in the world, and about the question of whether they will be able to put themselves in the world, in the world in which they are able to stay in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live"}, {"heading": "4. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data Set", "text": "To evaluate the effectiveness of the SEGAN, we use the data set from Valentini et al. [27]. We choose it because it is open and available1, and because the amount and type of data fits our purposes for this work: generalization of many types of noise for many different speakers. The data set is a selection of 30 speakers from the Voice Bank corpus [28]: 28 are included in the train set and 2 in the test set. To form the loud training set, a total of 40 different conditions are taken into account [27]: 10 types of noise (2 artificial and 8 from the Demand database [29]), each with 4 signal-noise ratios (SNR) (15, 10, 5 and 0 dB). There are approximately 10 different sets per training loudspeaker. To create the test set, a total of 20 different conditions are considered [27]: 5 types of noise (all from the Demand database), each with 4 SNR (17.5, 12.5, 7.5 and 2.5 dB)."}, {"heading": "4.2. SEGAN Setup", "text": "The model is trained for 86 epochs with RMSprop [30] and a learning rate of 0.0002, with an effective batch size of 400. We structure the training examples into two pairs (fig. 3): the real pair, consisting of a loud signal and a clean signal (x-and x-64), and the fake sample pair, consisting of a loud signal and an improved signal (x-4 and x-4). In order to address the data set files for our Waveform1http: / / dx.doi.org / 10.7488 / ds / 1356Generation purposes, we will reduce the original expressions from 48 kHz to 16 kHz. During the turn, we extract chunks of waveforms with a sliding window of approximately one second of speech (16384 samples) every 500 ms (50% overlap). During the test, we basically push the window without overlapping through the entire duration of our tests and we link the results to the end of the stream we have observed."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Objective Evaluation", "text": "As an initial assessment of the performance of our approach, we calculate the following objective metrics to evaluate the quality of improved speech based on the 824 files in the test set. All metrics are defined as predictors of the mean opinion value (MOS) that would be achieved in subjective tests, ranging from 1 (poor quality / high distortion / intrusive noise) to 5 (excellent quality / no deterioration / imperceptible noise).All metrics compare the improved signal with the clean reference. They were calculated using the implementation included in [1] and are available on the publisher's website. \u2022 PESQ: Perceptible assessment of speech quality using the broadband version recommended in ITU-T P.862.2. \u2022 CSIG: MOS evaluation of signal distortions that only affect the voice signal [33]. \u2022 CBAK: MOS evaluation of the unreliability of background noise [Overall COOS-V33] evaluation."}, {"heading": "5.2. Subjective Evaluation", "text": "To get a better score, a perception test was performed to compare SEGAN with the noise signal and the Viennese baseline. To do this, 20 records from the test set were selected. As the database does not specify the amount and type of noise for each file, the selection was made by listening to some of the provided noise files and trying to balance different types of noise. Most of the files have a low SNR, but a few with high SNR were also included. In total, 19 listeners were presented with the 20 records in a randomized order. For each set, three versions were2https: / / www.crcpress.com / downloads / K14513 / K14513 _ CD _ Files.zippresented, also in random order: Noisy signal, Viennese-amplified signal, and SEGAN-enhanced signal, with the listener scoring the overall quality on a scale of 1 to 5."}, {"heading": "6. Conclusions", "text": "The results show that the method is not only practicable, but can also be an effective alternative to current approaches. Possible future work includes exploring better revolutionary structures and incorporating perceptual weights into opposing training, so that we can reduce potential high-frequency artifacts that could be introduced by the current model. Further experiments will need to be conducted to compare SEGAN with other competing approaches."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the project TEC2015-69266-P (MINECO / FEDER, UE)."}, {"heading": "8. References", "text": "In fact, it is as if most people are able to save themselves if they cannot save themselves. (...) It is not as if they are able to save themselves. (...) It is not as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save themselves. \"(...).\" (...). \""}], "references": [{"title": "Speech Enhancement: Theory and Practice, 2nd ed", "author": ["P.C. Loizou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Spectral subtraction-based speech enhancement for cochlear implant patients in background noise", "author": ["L.-P. Yang", "Q.-J. Fu"], "venue": "The Journal of the Acoustical Society of America, vol. 117, no. 3, pp. 1001\u20131004, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A minimum-mean-square-error noise reduction algorithm on melfrequency cepstra for robust speech recognition", "author": ["D. Yu", "L. Deng", "J. Droppo", "J. Wu", "Y. Gong", "A. Acero"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4041\u20134044.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Recurrent neural networks for noise reduction in robust asr.", "author": ["A.L. Maas", "Q.V. Le", "T.M. O\u2019Neil", "O. Vinyals", "P. Nguyen", "A.Y. Ng"], "venue": "in Interspeech,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Overview of speech enhancement techniques for automatic speaker recognition", "author": ["J. Ortega-Garcia", "J. Gonzalez-Rodriguez"], "venue": "Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, vol. 2, Oct 1996, pp. 929\u2013932 vol.2.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Enhancement of speech corrupted by acoustic noise", "author": ["M. Berouti", "R. Schwartz", "J. Makhoul"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP \u201979., vol. 4, Apr 1979, pp. 208\u2013211.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1979}, {"title": "All-pole modeling of degraded speech", "author": ["J. Lim", "A. Oppenheim"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, no. 3, pp. 197\u2013210, Jun 1978.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1978}, {"title": "Statistical-model-based speech enhancement systems", "author": ["Y. Ephraim"], "venue": "Proceedings of the IEEE, vol. 80, no. 10, pp. 1526\u20131555, Oct 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Speech enhancement from noise: A regenerative approach", "author": ["M. Dendrinos", "S. Bakamidis", "G. Carayannis"], "venue": "Speech Communication, vol. 10, no. 1, pp. 45\u201357, 1991.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "A signal subspace approach for speech enhancement", "author": ["Y. Ephraim", "H.L. Van Trees"], "venue": "IEEE Transactions on speech and audio processing, vol. 3, no. 4, pp. 251\u2013266, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Noise reduction using connectionist models", "author": ["S. Tamura", "A. Waibel"], "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 1988, pp. 553\u2013556.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Speech enhancement with missing data techniques using recurrent neural networks", "author": ["S. Parveen", "P. Green"], "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2004, pp. 733\u2013736.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Speech enhancement based on deep denoising autoencoder.", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "in Interspeech,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR", "author": ["F. Weninger", "H. Erdogan", "S. Watanabe", "E. Vincent", "J. Le Roux", "J.R. Hershey", "B. Schuller"], "venue": "Proc. of the Int. Conf. on Latent Variable Analysis and Signal Separation, 2015, pp. 91\u201399.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE/ACM Trans. on Audio, Speech and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement in multiplenoise conditions using deep neural networks", "author": ["A. Kumar", "D. Florencio"], "venue": "Proc. of the Int. Speech Communication Association Conf. (INTERSPEECH), 2016, pp. 3738\u20133742.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The unimportance of phase in speech enhancement", "author": ["D. Wang", "J. Lim"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 30, no. 4, pp. 679\u2013681, Aug 1982.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1982}, {"title": "The importance of phase in speech enhancement", "author": ["K. Paliwal", "K. W\u00f3jcicki", "B. Shannon"], "venue": "Speech Communication, vol. 53, no. 4, pp. 465 \u2013 494, 2011. [Online]. Available: http://www. sciencedirect.com/science/article/pii/S0167639310002086", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde- Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, 2014, pp. 2672\u20132680.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Image-toimage translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "ArXiv: 1611.07004, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y.K. Lau", "Z. Wang"], "venue": "ArXiv: 1611.04076, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["\u2014\u2014"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2536\u20132544.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigating rnn-based speech enhancement methods for noiserobust text-to-speech", "author": ["C. Valentini-Botinhao", "X. Wang", "S. Takaki", "J. Yamagishi"], "venue": "9th ISCA Speech Synthesis Workshop, pp. 146\u2013152.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 0}, {"title": "The voice bank corpus: Design, collection and data analysis of a large regional accent speech database", "author": ["C. Veaux", "J. Yamagishi", "S. King"], "venue": "Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE), 2013 International Conference. IEEE, 2013, pp. 1\u20134.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "The diverse environments multi-channel acoustic noise database: A database of multichannel environmental noise recordings", "author": ["J. Thiemann", "N. Ito", "E. Vincent"], "venue": "The Journal of the Acoustical Society of America, vol. 133, no. 5, pp. 3591\u20133591, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-RMSprop: divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURS- ERA: Neural Networks for Machine Learning 4, 2, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluation of objective quality measures for speech enhancement", "author": ["Y. Hu", "P.C. Loizou"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 229\u2013238, Jan 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Speech enhancement based on a priori signal to noise estimation", "author": ["P. Scalart", "J.V. Filho"], "venue": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, vol. 2, May 1996, pp. 629\u2013632 vol. 2.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Speech enhancement tries to improve the intelligibility and quality of speech contaminated by additive noise [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "However, we also find important applications related to hearing aids and cochlear implants, where enhancing the signal before amplification can significantly reduce discomfort and increase intelligibility [2].", "startOffset": 205, "endOffset": 208}, {"referenceID": 2, "context": "Speech enhancement has also been successfully applied as a preprocessing stage in speech recognition and speaker identification systems [3, 4, 5].", "startOffset": 136, "endOffset": 145}, {"referenceID": 3, "context": "Speech enhancement has also been successfully applied as a preprocessing stage in speech recognition and speaker identification systems [3, 4, 5].", "startOffset": 136, "endOffset": 145}, {"referenceID": 4, "context": "Speech enhancement has also been successfully applied as a preprocessing stage in speech recognition and speaker identification systems [3, 4, 5].", "startOffset": 136, "endOffset": 145}, {"referenceID": 5, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 148, "endOffset": 155}, {"referenceID": 9, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "Neural networks have been also applied to speech enhancement since the 80s [11, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "Neural networks have been also applied to speech enhancement since the 80s [11, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 12, "context": "Recently, the denoising auto-encoder architecture [13] has been widely adopted.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "Most recent approaches apply long short-term memory RNNs to the denoising task [4, 14].", "startOffset": 79, "endOffset": 86}, {"referenceID": 13, "context": "Most recent approaches apply long short-term memory RNNs to the denoising task [4, 14].", "startOffset": 79, "endOffset": 86}, {"referenceID": 14, "context": "In [15] and [16], noise features are estimated and included in the input features of deep neural networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [15] and [16], noise features are estimated and included in the input features of deep neural networks.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "Most of the current systems are based on the short-time Fourier analysis/synthesis framework [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "They only modify the spectrum magnitude, as it is often claimed that short-time phase is not important for speech enhancement [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "However, further studies [18] show that significant improvements of speech quality are possible especially when a clean phase spectrum is known.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "[11] proposed a deep network that worked directly on the raw audio waveform, but they used feed-forward layers that worked frame-by-frame (60 samples) on a speaker-dependent and isolated-word database.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 296, "endOffset": 308}, {"referenceID": 20, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 296, "endOffset": 308}, {"referenceID": 21, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 296, "endOffset": 308}, {"referenceID": 18, "context": "GANs [19] are generative models that learn to map samples z from some prior distribution Z to samples x from another distribution X , which is the one of the training examples (e.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "We can also work with a conditioned version of GANs, where we have some extra information in G and D to perform mapping and classification (see [20] and references therein).", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "To solve this, we employ the least-squares GAN (LSGAN) approach [21], which substitutes the cost function by the least-squares function with binary coding (1 is real, 0 is fake).", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "In the encoding stage, the input signal is projected and compressed through a number of strided convolutional layers followed by PReLUs [23], getting a convolution result out of every N steps of the filter .", "startOffset": 136, "endOffset": 140}, {"referenceID": 21, "context": "We choose strided convolutions as they were shown to be more stable for GAN training than other pooling approaches [22].", "startOffset": 115, "endOffset": 119}, {"referenceID": 23, "context": "In addition, they offer a better training behavior, as the gradients can flow deeper through the whole structure without suffering much vanishing [24].", "startOffset": 146, "endOffset": 150}, {"referenceID": 24, "context": "In this type of model, we have to be careful with typical regression losses like mean absolute error or mean squared error, as noted in the raw speech generative model WaveNet [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "To measure such distance, we chose the L1 norm, as it has been proven to be effective in the image manipulation domain [20, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 25, "context": "To measure such distance, we chose the L1 norm, as it has been proven to be effective in the image manipulation domain [20, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The data set is a selection of 30 speakers from the Voice Bank corpus [28]: 28 are included in the train set and 2 in the test set.", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "To make the noisy training set, a total of 40 different conditions are considered [27]: 10 types of noise (2 artificial and 8 from the Demand database [29]) with 4 signal-to-noise ratio (SNR) each (15, 10, 5, and 0 dB).", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "To make the noisy training set, a total of 40 different conditions are considered [27]: 10 types of noise (2 artificial and 8 from the Demand database [29]) with 4 signal-to-noise ratio (SNR) each (15, 10, 5, and 0 dB).", "startOffset": 151, "endOffset": 155}, {"referenceID": 26, "context": "To make the test set, a total of 20 different conditions are considered [27]: 5 types of noise (all from the Demand database) with 4 SNR each (17.", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "The model is trained for 86 epochs with RMSprop [30] and a learning rate of 0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "All the project is developed with the TensorFlow deep learning framework [31], and the code is available at https: //github.", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "They have been computed using the implementation included in [1], and available at the publisher website.", "startOffset": 61, "endOffset": 64}, {"referenceID": 31, "context": "\u2022 CSIG: MOS rating of the signal distortion attending only to the speech signal [33].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "\u2022 CBAK: MOS rating of the intrusiveness of background noise [33].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "\u2022 COVL: MOS rating of the overall effect [33].", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "To have a comparative reference, the table also shows the results of these metrics when applied directly to the noisy signals and to signals filtered using the Wiener algorithm based on a priori SNR estimation [34], as provided in [1].", "startOffset": 210, "endOffset": 214}, {"referenceID": 0, "context": "To have a comparative reference, the table also shows the results of these metrics when applied directly to the noisy signals and to signals filtered using the Wiener algorithm based on a priori SNR estimation [34], as provided in [1].", "startOffset": 231, "endOffset": 234}], "year": 2017, "abstractText": "Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.", "creator": "LaTeX with hyperref package"}}}