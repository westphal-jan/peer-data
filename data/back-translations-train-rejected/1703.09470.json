{"id": "1703.09470", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Learned Spectral Super-Resolution", "abstract": "We describe a novel method for blind, single-image spectral super-resolution. While conventional super-resolution aims to increase the spatial resolution of an input image, our goal is to spectrally enhance the input, i.e., generate an image with the same spatial resolution, but a greatly increased number of narrow (hyper-spectral) wave-length bands. Just like the spatial statistics of natural images has rich structure, which one can exploit as prior to predict high-frequency content from a low resolution image, the same is also true in the spectral domain: the materials and lighting conditions of the observed world induce structure in the spectrum of wavelengths observed at a given pixel. Surprisingly, very little work exists that attempts to use this diagnosis and achieve blind spectral super-resolution from single images. We start from the conjecture that, just like in the spatial domain, we can learn the statistics of natural image spectra, and with its help generate finely resolved hyper-spectral images from RGB input. Technically, we follow the current best practice and implement a convolutional neural network (CNN), which is trained to carry out the end-to-end mapping from an entire RGB image to the corresponding hyperspectral image of equal size. We demonstrate spectral super-resolution both for conventional RGB images and for multi-spectral satellite data, outperforming the state-of-the-art.", "histories": [["v1", "Tue, 28 Mar 2017 09:17:38 GMT  (3427kb,D)", "http://arxiv.org/abs/1703.09470v1", "Submitted to ICCV 2017 (10 pages, 8 figures)"]], "COMMENTS": "Submitted to ICCV 2017 (10 pages, 8 figures)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["silvano galliani", "charis lanaras", "dimitrios marmanis", "emmanuel baltsavias", "konrad schindler"], "accepted": false, "id": "1703.09470"}, "pdf": {"name": "1703.09470.pdf", "metadata": {"source": "CRF", "title": "Learned Spectral Super-Resolution", "authors": ["Silvano Galliani", "Charis Lanaras", "Dimitrios Marmanis", "Emmanuel Baltsavias", "Konrad Schindler"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2. Related Work", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "3. Method", "text": "In our work, we follow the current trend in computer vision research and learn the desired super-resolution mapping end-to-end with a (convolutonal) neural network. Below, we present the network architecture and give implementation details. Selecting a network architecture for deep learning is therefore not easy for a novel application where no previous studies indicate suitable designs. However, it is clear that the output should have the same image size as the input (but more channels). We build on recent work in semantic segmentation. Our proposed network is a variant of the semantic segmentation architecture Tiramisu et al."}, {"heading": "3.1. Relation to spectral unmixing", "text": "Hyperspectral images are often interpreted as \"end members\" and \"abundances\": the end members can be presented as the pure spectra of the observed materials and form a natural basis. Observed pixel spectra are additive combinations of end members, with the abundances (proportions of different materials) being considered coefficients. Dong et al. [9] showed how a flat CNN for superresolution can be interpreted in terms of basic learning and reconstruction. Similarly, our CNN can be regarded as an implicit, nonlinear extension of the separation model, in which knowledge of the end members is embedded in the folding weights at both low and high spectral resolution."}, {"heading": "3.2. Implementation details", "text": "The network implemented with Keras [8] is trained from the ground up with the Adam Optimizer [23] with Nesterov moment [34, 11]. We iterate it for 100 epochs with a learning rate of 0.002, then for another 200 epochs with a learning rate of 0.0002, the rest of the parameters follow the parameters given in the thesis. We initialize our model with HeUniform [15] and apply 50% dropouts in the sinuous layers to avoid overadjustment. Furthermore, we found it important to carefully adjust the Euclidean regulation, probably due to the general lack of abundant images on the training set. We fix it to 10 \u2212 6, higher values lead to excessively smooth, less precise solutions."}, {"heading": "4. Results", "text": "We evaluate our result using four different data sets. Where possible, we compare it with the other two methods [3, 28], which are also able to estimate a hyper-spectral image from a single RGB. Note: Both baselines require the spectral response of the RGB camera to be known. Therefore, we provide it to them as additional input, as opposed to our method. Despite the disadvantage, our CNN super resolution is more precise, see below. Error calculation We evaluate three different error metrics over 8-bit images (where available): Root Mean Square Error (RMSE), Relative Root Mean Square Error (RMSERel), and Spectral Angle Mapper (SAM) [46], i.e. the average angular deviation between the estimated pixel spectra measured in degrees. We would like to emphasize how we measure RMSERel and RMSE as predicted quality..... 20.1 A comparison between the two values results in a smaller deviation between the estimated pixel spectra and those measured in degrees."}, {"heading": "4.1. Training data", "text": "We follow the common practice of quantitative evaluation and synthesize the input data because it is difficult to capture separate hyperspectral and RGB images that are aligned and have comparable resolution and sharpness. That is, the RGB image is emulated by integrating it via hyperspectral channels according to a predefined camera function. We always use the response functions provided by the authors to ensure that the images are absolutely equal and the comparisons are fair. If a data set already provides a pull test split, we follow it. Otherwise, we perform a double cross-validation: split the data set in two, train in the first half to predict the second half, and vice versa."}, {"heading": "4.2. ICVL dataset", "text": "The ICVL dataset was published by Arad and BenShahar [3], along with their method. It contains 201 images taken with a line-scan camera (Specim PS Kappa DX4 hyperspectral) mounted on a rotary table for spatial scanning. The dataset contains a variety of scenes taken both indoors and outdoors, including artificially created natural objects. The images were originally captured at a spatial resolution of 1392 \u00d7 1300 over 519 spectral bands (400-1,000 nm), but were sampled down to 31 spectral channels from 400 nm to 700 nm in 10 nm increments. We map the hyper-spectral images with RGB using CIE 1964 color matching functions, as in the original paper. There, a sparse dictionary of K-SVD spectral channels from 400 nm to 700 nm is sampled in increments."}, {"heading": "4.3. NUS dataset", "text": "The NUS dataset [28] contains the spectral irradiation and spectral illumination (400-700 nm with 10 nm steps) for 66 outdoor and indoor images taken under different lighting conditions. In this dataset, the authors already prescribe a traction test split. Their learning method is to estimate both reflection and illumination from an RGB image with a known camera reaction function. To evaluate their method fairly, we use the authors \"original code to estimate the reflection and convert it into radiation with the ground truth illumination. In addition, we apply their basic truth illumination to our result to compare the reflection as well. Also, for this dataset, we obtain the best result with respect to RMSE, see tables 2 and 3. In this case, our error was slightly worse due to SAM."}, {"heading": "4.4. CAVE dataset", "text": "The CAVE dataset [44] is a popular hyperspectral dataset. Unlike all the others, it is not captured with a rotating line scanner, but the hyperspectral bands are recorded sequentially using a tunable filter.The main advantage is the elimination of possible noise when using a brush scanner, while moving objects such as trees cause problems because the bands are not aligned correctly.The dataset contains a variety of difficult predictable objects. The heterogeneity of the captured scenes makes it difficult to obtain global run-time for all scenes and challenges learning-based methods such as ours. Nevertheless, our method is competitive with the number provided by [3], see Table 1."}, {"heading": "4.5. Satellite Data", "text": "We also tested our method on data from the Hyperion satellite [31], a sensor on board the EO-1 satellite. The satellite carries a hyperspectral line scanner that records 242 channels (from 0.4 to 2.5 m) with 30 m ground resolution, of which 198 are calibrated and can be used. Our scenes are already cloudless, have a size \u2248 256 x 7000 pixels and show the Rhine in Western Europe. Note that, like most satellite data, the images are stored with an intensity range of 16 bit beer channel and have an effective radiometric depth of minimum 5000 different gray values. The input image is emulated by integrating the hyperspectral bands into the channels of ALI, the 9-channel multi-spectral sensor on board the same satellite. As a test bed, we use different acquisition dates over (approximately) the same area. This is of course a favorable scenario for our method: since training and test data show the same region, the network may fit both the specific structure of the given region > the results in a certain way."}, {"heading": "4.6. Denoising", "text": "An interesting feature of our learned upsampling method is that it can be used as a denocialization method: Downsampling the original images (as we do in our experiments) removes the noise, but upsampling does not reinsert it. In fact, deep neural networks are known to be state-of-the-art, leading to image denocialization [42]. See the prediction in Fig. 7, note how the marked line artifacts are removed in the basic truth by our method. On the satellite data, which are generally much louder, this effect is very evident. In most cases, the predicted images for Hyperion are cleaner and more useful than the original \"Ground Truth\" images. In Fig. 6, the differential images dominate the noise, while the \"true\" prediction error appears minimal. This assertion is also supported by the fact that we were able to extract plausible endpoint spectrals of the original hyperspectrals we deemed impossible to extract."}, {"heading": "4.7. Hyperspectral Unmixing", "text": "We also verify our reconstruction using satellite data by performing a hyper-spectral separation [5], a process that separates material information (also called endpoints) from its position in the image (also called abundances). We take one of the abundance maps to identify dominant spectral signatures on the images. We then perform a complete adaptation of the abundance maps (FCLS) to extract the abundance maps according to the Linear Mixing Model (LMM) [19]. The abundance maps show the presence of each endpoint in each pixel and are limited to not being negative and total. We select a subset of an image and extract 15 endpoints and their corresponding abundances, because three different cases of hyper-spectral images are not sufficient to see the signal."}, {"heading": "5. Conclusions", "text": "We show that it is possible to achieve super-resolutions for images not only in the spatial range, but also in the spectral range. Unlike other work on spectral super-resolution, we train and predict directly the final relationship between an RGB image and its corresponding hyper-spectral image, without additional inputs such as the spectral response function. We show the performance of our work on multiple indoor, outdoor and satellite datasets, where we can compare favorably with other, less generic methods. We believe that our work can be useful for a number of applications that would benefit from higher spectral resolution, but the shooting conditions or costs do not allow the routine use of hyper-spectral cameras."}], "references": [{"title": "Sparse spatio-spectral representation for hyperspectral image super-resolution", "author": ["N. Akhtar", "F. Shafait", "A. Mian"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical beta process with gaussian process prior for hyperspectral image super resolution", "author": ["N. Akhtar", "F. Shafait", "A. Mian"], "venue": "European Conference on Computer Vision, pages 103\u2013120. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse recovery of hyperspectral signal from natural rgb images", "author": ["B. Arad", "O. Ben-Shahar"], "venue": "European Conference on Computer Vision, pages 19\u201334. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Hyperspectral remote sensing data analysis and future challenges", "author": ["J.M. Bioucas-Dias", "A. Plaza", "G. Camps-Valls", "P. Scheunders", "N.M. Nasrabadi", "J. Chanussot"], "venue": "IEEE, Geoscience and Remote Sensing Magazine, 1(2):6\u201336", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Hyperspectral unmixing overview: Geometrical", "author": ["J.M. Bioucas-Dias", "A. Plaza", "N. Dobigeon", "M. Parente", "Q. Du", "P. Gader", "J. Chanussot"], "venue": "statistical, and sparse regressionbased approaches. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 5(2):354\u2013 379", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Advances in hyperspectral image classification: Earth  monitoring with statistical learning methods", "author": ["G. Camps-Valls", "D. Tuia", "L. Bruzzone", "J.A. Benediktsson"], "venue": "IEEE Signal Processing Magazine, 31(1):45\u201354", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-spectral imaging by optimized wide band illumination", "author": ["C. Chi", "H. Yoo", "M. Ben-Ezra"], "venue": "International Journal of Computer Vision, 86(2-3):140", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/ keras", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "European Conference on Computer Vision, pages 184\u2013199. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization", "author": ["W. Dong", "L. Zhang", "G. Shi", "X. Wu"], "venue": "IEEE Transactions on Image Processing, 20(7):1838\u20131857", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Incorporating nesterov momentum into adam", "author": ["T. Dozat"], "venue": "Technical report, Stanford University, Tech. Rep., 2015.[Online]. Available: http://cs229. stanford. edu/proj2015/054 report. pdf", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multispectral camera and radiative transfer equation used to depict leonardo\u2019s sfumato in mona lisa", "author": ["M. Elias", "P. Cotte"], "venue": "Applied optics, 47(12):2146\u20132154", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Imaging spectroscopy using tunable filters: a review", "author": ["N. Gat"], "venue": "AeroSense 2000, pages 50\u201364. International Society for Optics and Photonics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "and L", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger"], "venue": "van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation", "author": ["S. J\u00e9gou", "M. Drozdzal", "D. Vazquez", "A. Romero", "Y. Bengio"], "venue": "arXiv preprint arXiv:1611.09326", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["R. Kawakami", "Y. Matsushita", "J. Wright"], "venue": "Ben-Ezra, Y.- W. Tai, and K. Ikeuchi. High-resolution hyperspectral imaging via matrix factorization. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 2329\u20132336. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral unmixing", "author": ["N. Keshava", "J.F. Mustard"], "venue": "IEEE signal processing magazine, 19(1):44\u201357", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Accurate image superresolution using very deep convolutional networks", "author": ["J. Kim", "J. Kwon Lee", "K. Mu Lee"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Deeply-recursive convolutional network for image super-resolution", "author": ["J. Kim", "J. Kwon Lee", "K. Mu Lee"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Visual enhancement of old documents with hyperspectral imaging", "author": ["S.J. Kim", "F. Deng", "M.S. Brown"], "venue": "Pattern Recognition, 44(7):1461\u20131469", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Hyperspectral super-resolution by coupled spectral unmixing", "author": ["C. Lanaras", "E. Baltsavias", "K. Schindler"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 3586\u20133594", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "European Conference on Computer Vision, pages 577\u2013593. Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Cunningham", "A. Acosta", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang"], "venue": "Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum volume simplex analysis: A fast algorithm to unmix hyperspectral data", "author": ["J. Li", "J.M. Bioucas-Dias"], "venue": "Geoscience and Remote Sensing Symposium, 2008. IGARSS 2008. IEEE International, volume 3, pages III\u2013250. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Trainingbased spectral reconstruction from a single rgb image", "author": ["R.M. Nguyen", "D.K. Prasad", "M.S. Brown"], "venue": "European Conference on Computer Vision, pages 186\u2013201. Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantitative hyperspectral imaging of historical documents: technique and applications", "author": ["R. Padoan", "T.A. Steemers", "M. Klein", "B. Aalderink", "G. De Bruin"], "venue": "Art Proceedings, pages 25\u201330", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Face recognition in hyperspectral images", "author": ["Z. Pan", "G. Healey", "M. Prasad", "B. Tromberg"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(12):1552\u20131560", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Hyperion", "author": ["J.S. Pearlman", "P.S. Barry", "C.C. Segal", "J. Shepanski", "D. Beiso", "S.L. Carman"], "venue": "a space-based imaging spectrometer. IEEE Transactions on Geoscience and Remote Sensing, 41(6):1160\u20131173", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network", "author": ["W. Shi", "J. Caballero", "F. Huszar", "J. Totz", "A.P. Aitken", "R. Bishop", "D. Rueckert", "Z. Wang"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "A convex formulation for hyperspectral image superresolution via subspace-based regularization", "author": ["M. Sim\u00f5es", "J. Bioucas-Dias", "L.B. Almeida", "J. Chanussot"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, 53(6):3373\u20133388", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "ICML (3), 28:1139\u20131147", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmentation and classification of hyperspectral images using watershed transformation", "author": ["Y. Tarabalka", "J. Chanussot", "J.A. Benediktsson"], "venue": "Pattern Recognition, 43(7):2367\u20132379", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Anchored neighborhood regression for fast example-based super-resolution", "author": ["R. Timofte", "V. De Smet", "L. Van Gool"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Tracking via object reflectance using a hyperspectral video camera", "author": ["H. Van Nguyen", "A. Banerjee", "R. Chellappa"], "venue": "IEEE Computer Vision and Pattern Recognition Workshops (CVPRW), pages 44\u201351. IEEE", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast fusion of multi-band images based on solving a sylvester equation", "author": ["Q. Wei", "N. Dobigeon", "J.-Y. Tourneret"], "venue": "IEEE Transactions on Image Processing, 24(11):4109\u20134121", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Advanced applications of hyperspectral imaging technology for food quality and safety analysis and assessment: A reviewpart i: Fundamentals", "author": ["D. Wu", "D.-W. Sun"], "venue": "Innovative Food Science & Emerging Technologies, 19:1\u201314", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Do it yourself hyperspectral imaging with everyday digital cameras", "author": ["S. Wug Oh", "M.S. Brown", "M. Pollefeys", "S. Joo Kim"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2461\u20132469", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "A non-negative sparse promoting algorithm for high resolution hyperspectral imaging", "author": ["E. Wycoff", "T.-H. Chan", "K. Jia", "W.-K. Ma", "Y. Ma"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 1409\u20131413. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Advances in Neural Information Processing Systems, pages 341\u2013349", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Image superresolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE transactions on image processing, 19(11):2861\u20132873", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized Assorted Pixel Camera: Post-Capture Control of Resolution, Dynamic Range and Spectrum", "author": ["F. Yasuma", "T. Mitsunaga", "D. Iso", "S. Nayar"], "venue": "Technical report,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Coupled nonnegative matrix factorization unmixing for hyperspectral and multispectral data fusion", "author": ["N. Yokoya", "T. Yairi", "A. Iwasaki"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, 50(2):528\u2013537", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Discrimination among semi-arid landscape endmembers using the spectral angle mapper (sam", "author": ["R.H. Yuhas", "A.F. Goetz", "J.W. Boardman"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1992}, {"title": "On single image scale-up using sparse-representations", "author": ["R. Zeyde", "M. Elad", "M. Protter"], "venue": "International conference on curves and surfaces, pages 711\u2013730. Springer", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "European Conference on Computer Vision, pages 649\u2013666. Springer", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Still, several successful schemes have been designed [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 115, "endOffset": 119}, {"referenceID": 34, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 181, "endOffset": 189}, {"referenceID": 28, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 181, "endOffset": 189}, {"referenceID": 11, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 213, "endOffset": 217}, {"referenceID": 38, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 235, "endOffset": 239}, {"referenceID": 5, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 265, "endOffset": 268}, {"referenceID": 1, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 17, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 23, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 32, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 3, "context": "For practical processing, hyper-spectral signatures are sometimes projected to a lower-dimensional subspace [4], indicating that there is a significant amount of correlation between their bands.", "startOffset": 108, "endOffset": 111}, {"referenceID": 35, "context": "Early method attempted to devise clever upsampling functions, sometimes by manually analyzing the image statistics, while recently the trend has been to learn dictionaries of image patches, often in combination with a sparsity prior [36, 43, 47].", "startOffset": 233, "endOffset": 245}, {"referenceID": 42, "context": "Early method attempted to devise clever upsampling functions, sometimes by manually analyzing the image statistics, while recently the trend has been to learn dictionaries of image patches, often in combination with a sparsity prior [36, 43, 47].", "startOffset": 233, "endOffset": 245}, {"referenceID": 46, "context": "Early method attempted to devise clever upsampling functions, sometimes by manually analyzing the image statistics, while recently the trend has been to learn dictionaries of image patches, often in combination with a sparsity prior [36, 43, 47].", "startOffset": 233, "endOffset": 245}, {"referenceID": 8, "context": "Lately, CNNs have boosted the performance of super-resolution, showing significant improvements [9, 20, 21].", "startOffset": 96, "endOffset": 107}, {"referenceID": 19, "context": "Lately, CNNs have boosted the performance of super-resolution, showing significant improvements [9, 20, 21].", "startOffset": 96, "endOffset": 107}, {"referenceID": 20, "context": "Lately, CNNs have boosted the performance of super-resolution, showing significant improvements [9, 20, 21].", "startOffset": 96, "endOffset": 107}, {"referenceID": 31, "context": "They are also able to perform the task in real time [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "Other loss functions aiming for \u201cphotorealism\u201d better match human perception, although the actual intensity differences are higher [26].", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "Some methods require only known spectral response of the RGB camera, but can correct for spatial mis-alignment known [1, 2, 18].", "startOffset": 117, "endOffset": 127}, {"referenceID": 1, "context": "Some methods require only known spectral response of the RGB camera, but can correct for spatial mis-alignment known [1, 2, 18].", "startOffset": 117, "endOffset": 127}, {"referenceID": 17, "context": "Some methods require only known spectral response of the RGB camera, but can correct for spatial mis-alignment known [1, 2, 18].", "startOffset": 117, "endOffset": 127}, {"referenceID": 23, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 32, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 37, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 40, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 44, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 24, "context": "There CNNs have also shown promising results [25, 48] by converting the input to a Lab colorspace and predicting the ab channels.", "startOffset": 45, "endOffset": 53}, {"referenceID": 47, "context": "There CNNs have also shown promising results [25, 48] by converting the input to a Lab colorspace and predicting the ab channels.", "startOffset": 45, "endOffset": 53}, {"referenceID": 6, "context": "Acquiring a hyperspectral image by using only an RGB camera has been attempted with the help of active lighting [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 12, "context": "A similar idea is to use tunable narrow-band filters and take multiple images, such that narrow spectral bands are recorded sequentially [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 39, "context": "[40] proposed the use of multiple RGB", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Note that, except for the first convolutions, the other blocks are made of a dense block as in [17]", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "[28] use a radial basis function network to model the mapping from RGB values to scene reflectance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed to learn sparse dictionary with K-SVD as hyper-spectral upsampling prior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 46, "context": "Closely related methods exist for spatial super-resolution [47] as well as hyper-spectral superresolution [1].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Closely related methods exist for spatial super-resolution [47] as well as hyper-spectral superresolution [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 46, "context": "[47] employ low resolution hyperspectral image as prior while Akhtar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] compute their prior on a similar image contained inside their dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17], which in turn is based on the Densenet [16] architecture for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17], which in turn is based on the Densenet [16] architecture for classification.", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Skip connections, within and across Densenet blocks (see 2, 3) perform concatenation instead of summation of layers, as opposed to ResNet [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "For a more details about the Densenet/Tiramisu architecture, please refer to the original papers [16, 17].", "startOffset": 97, "endOffset": 105}, {"referenceID": 16, "context": "For a more details about the Densenet/Tiramisu architecture, please refer to the original papers [16, 17].", "startOffset": 97, "endOffset": 105}, {"referenceID": 8, "context": "[9] showed how a shallow CNN for superresolution can be interpreted in terms of basis learning and reconstruction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 36, "endOffset": 39}, {"referenceID": 22, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 117, "endOffset": 125}, {"referenceID": 14, "context": "We initialize our model with HeUniform [15], and apply 50% dropout in the convolutional layers to avoid overfitting.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "Where possible, we compare it with the other two methods [3, 28] that are also able to estimate an hyperspectral image from an single RGB.", "startOffset": 57, "endOffset": 64}, {"referenceID": 27, "context": "Where possible, we compare it with the other two methods [3, 28] that are also able to estimate an hyperspectral image from an single RGB.", "startOffset": 57, "endOffset": 64}, {"referenceID": 45, "context": "three different error metric over 8bit images (as far as they are available): root mean square error (RMSE), relative root mean square error (RMSERel), and the spectral angle mapper (SAM) [46], i.", "startOffset": 188, "endOffset": 192}, {"referenceID": 27, "context": "[28] over three non consecutive spectral bands.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The ICVL dataset has been released by Arad and BenShahar [3], together with their method.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "[3] on ICVL and CAVE dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Ours Arad et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] RMSE 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28] on NUS dataset RMSE RMSERel SAM Nguyen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Error evaluation on the reflectance using the same procedure as in [28] on NUS dataset RMSE RMSERel SAM Nguyen et al.", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "[28] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The NUS dataset [28] contains the spectral irradiance and spectral illumination (400-700 nm with step of 10 nm) for 66 outdoor and indoor scenes, captured under several different illuminations.", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "The CAVE dataset [44] is a popular hyper-spectral dataset.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "the number provided by [3], see Table 1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 30, "context": "We tested our method also on data captured from Hyperion satellite [31] a sensor on board the satellite EO-1.", "startOffset": 67, "endOffset": 71}, {"referenceID": 41, "context": "Indeed it is known that deep neural networks achieve state-of-the-art results in image denoising [42].", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "We also check our reconstruction on satellite data, by performing hyperspectral unmixing [5] a process that separates material information (also called endmembers) and their location in the image (also called abundances).", "startOffset": 89, "endOffset": 92}, {"referenceID": 26, "context": "We take an of the shelf endmember extraction algorithm (VCA, [27]) to identify dominant spectral signatures in the images.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Then, we perform a Fully Constrained Least Squares (FCLS) adjustment to extract the abundance maps, according to the Linear Mixing Model (LMM) [19].", "startOffset": 143, "endOffset": 147}], "year": 2017, "abstractText": "We describe a novel method for blind, single-image spectral super-resolution. While conventional superresolution aims to increase the spatial resolution of an input image, our goal is to spectrally enhance the input, i.e., generate an image with the same spatial resolution, but a greatly increased number of narrow (hyper-spectral) wavelength bands. Just like the spatial statistics of natural images has rich structure, which one can exploit as prior to predict high-frequency content from a low resolution image, the same is also true in the spectral domain: the materials and lighting conditions of the observed world induce structure in the spectrum of wavelengths observed at a given pixel. Surprisingly, very little work exists that attempts to use this diagnosis and achieve blind spectral super-resolution from single images. We start from the conjecture that, just like in the spatial domain, we can learn the statistics of natural image spectra, and with its help generate finely resolved hyper-spectral images from RGB input. Technically, we follow the current best practice and implement a convolutional neural network (CNN), which is trained to carry out the end-to-end mapping from an entire RGB image to the corresponding hyperspectral image of equal size. We demonstrate spectral super-resolution both for conventional RGB images and for multi-spectral satellite data, outperforming the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}