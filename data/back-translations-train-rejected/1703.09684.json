{"id": "1703.09684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "An Analysis of Visual Question Answering Algorithms", "abstract": "In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.", "histories": [["v1", "Tue, 28 Mar 2017 17:48:07 GMT  (1488kb,D)", "http://arxiv.org/abs/1703.09684v1", null], ["v2", "Wed, 13 Sep 2017 18:56:45 GMT  (1488kb,D)", "http://arxiv.org/abs/1703.09684v2", "To appear in ICCV 2017. Visitthis http URLto download the dataset"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["kushal kafle", "christopher kanan"], "accepted": false, "id": "1703.09684"}, "pdf": {"name": "1703.09684.pdf", "metadata": {"source": "CRF", "title": "An Analysis of Visual Question Answering Algorithms", "authors": ["Kushal Kafle", "Christopher Kanan", "Chester F. Carlson"], "emails": ["kanan}@rit.edu"], "sections": [{"heading": "1. Introduction", "text": "In the open question of visualization (VQA), an algorithm must provide answers to any text-based questions about images [19, 3]. VQA is an exciting computer education problem that requires a system to solve many tasks. VQA research began in late 2014 when the DAQUAR data was published [19]. Including DAQUAR, six large VQA datasets that were released, and algorithms that rapidly improved. VQA research began in 2014 when the DAQUAR datasets were published [19]."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Prior Natural Image VQA Datasets", "text": "In fact, most people who are able to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo themselves, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to prepare them, to prepare them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to outdo them, to them, to outdo them, to outdo them, to them, to outdo them, to outdo them, to them, to outdo them, to them, to outdo them, to them, to outdo them, to outdo them, to them, to outdo them, to them, to outdo them, to them, to outdo them, to them, to them, to outdo them, to them, to outdo them, to outdo them, to outdo them, to them, and to them, to outline them, to them, to outline, to them, to outline, to them, to them, to them, to outline, to them, to outline, to them, to outline, to them, to them, to them, to outline, to them, to them, to them, to outline, to them, to outline,"}, {"heading": "2.2. Synthetic Datasets that Fight Bias", "text": "They found that answering questions from a balanced dataset was more difficult. This work is significant, but it was limited to yes / no questions, and its approach using cartoon images cannot be directly extended to real images. One of the objectives of this paper is to determine what types of questions an algorithm can easily answer. In [1], the SHAPES datasets were proposed that have similar objectives. SHAPES is a small dataset consisting of 64 images composed by the arrangement of colored geometric shapes in different spatial orientations. Each image has the same 244 yes / no questions that arise from the four different categories."}, {"heading": "3. TDIUC for Nuanced VQA Analysis", "text": "Over the past two years, several publicly released datasets have driven VQA research. However, we propose a new benchmark dataset that explicitly divides questions into 12 distinct categories, allowing you to measure performance within each category and understand what types of questions are easy or difficult for today's best systems. In addition, we use evaluation metrics that further offset the distortions. We call the dataset the Task Driven Image Understanding Challenge (TDIUC). The aggregate statistics and sample images of this dataset are shown in Table 1 and Figure 2."}, {"heading": "3.1. Importing Questions from Existing Datasets", "text": "We imported questions from COCO-VQA and the Visual Genome, which belonged to all question types except for \"object services and affordability,\" using a large number of templates and regular expressions; for the Visual Genome, we imported one-word answers; for COCO-VQA, we imported questions with one or two-word answers in which five or more commentators agreed; for color questions, a question was imported if it contained the word \"color\" and the answer was a frequently used color; questions were classified as activity or sport recognition questions if the answer was one of nine common sports or one of fifteen common activities and the question contained general verbs describing actions or sports, such as playing, throwing, etc. To count, the question had to begin with \"How many\" and the answer had to be a small countable integer (1-16); the other categories were determined by regular expressions; a \"Form of the Season\" and was classified as an object?"}, {"heading": "3.2. Generating Questions using Image Annotations", "text": "Images in the COCO dataset and the Visual Genome each contain individual regions of semantic knowledge. We use this information to generate new questions based on question templates. To create variety, we define multiple templates for each question type and use the annotations to fill them in. For example, since COCO and Visual Genome use different annotation formats, we use 8 templates: \"How many < objects > are there?,\" \"How many < objects > are in the photo?,\" and so on. Since COCO and Visual Genome use different annotation formats, we discuss them separately."}, {"heading": "3.2.1 Questions Using COCO annotations", "text": "In fact, most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "3.2.2 Questions Using Visual Genome annotations", "text": "The annotations of Visual Genomes contain regional descriptions, relationship diagrams and object boundaries. However, the annotations may not be exhaustive or duplicated, making them difficult to use to automatically aggravate QA pairs. We only use Visual Genomes to ask color and position questions. The methods we use are similar to those of COCO, but additional precautions were required due to quirks in their annotations. See the appendix for more details."}, {"heading": "3.3. Manual Annotation", "text": "Sentiment understanding and object-use / affordability questions cannot easily be created using templates, so we used manual annotations to create them. Twelve volunteer annotators were trained to generate these questions, and they used a web-based annotation tool we developed. They were shown random images of COCO and Visual Genome, and they were also able to upload images."}, {"heading": "3.4. Post Processing", "text": "All numbers were converted to text, e.g. 2 became two, all answers were converted to lowercase letters, and the subsequent punctuation was removed, duplicate questions for the same picture were removed, all questions had to have answers that occurred at least twice, and the record was split into train and test splits with 70% for the turn and 30% for the test."}, {"heading": "4. Proposed Evaluation Metric", "text": "One of the main objectives of VQA research is to build computer vision systems that are capable of solving many tasks, rather than just having expertise on a specific task (e.g. object detection). Therefore, some argue that VQA is a kind of Visual Turing Test [19]. However, when simple accuracy is used to evaluate performance, it is difficult to know whether a system achieves this goal because some question types have much more questions than others. In VQA, distorted distributions of question types are to be expected. If each test question is treated the same, then it is difficult to evaluate performance on rarer question types and balance distortions. We propose several measures to compensate for distortions and distorted distributions. To balance the distorted question type distribution, we calculate accuracy equal for each of the 12 question types, then it is difficult to assess performance on rare question types and calculate MPT types differently."}, {"heading": "5. Algorithms for VQA", "text": "While there are alternative formulations (e.g. [6, 9]), most VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories. [3, 23, 5, 25, 8, 18, 22, 24, 27, 29, 30, 32, 9, 20] Almost all systems use CNN features to represent the image and either a recurrent neural network (RNN) or a bag of words for the question. We briefly look at some of these systems and focus on the models we compare in experiments."}, {"heading": "6. Experiments", "text": "The methods we use are: \u2022 YES: Predicts \"yes\" for all questions. \u2022 REP: Predicts the most repeated score in a question-type category using an oracle. \u2022 QUES: A linear softmax classifier given only questionfeatures (image blind). \u2022 IMG: A linear softmax classifier given only questionfeatures (image blind). \u2022 IMG: A linear softmax classifier given only image fea-tures (question blind). \u2022 Q + I: A linear classifier given both question and imagefeatures. \u2022 MLP: A 4-layer MLP fed question and image features. \u2022 MCB: MCB [5] without spatial attention. \u2022 MCB-A: MCB [5] with spatial attention. \u2022 NMN: NMN: NMN from [1] with minor modifications. \u2022 RAU: RAU [21] with image features, ResNet-157] with spatial attention."}, {"heading": "7. Detailed Analysis of VQA Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Easy Question-Types for Today\u2019s Methods", "text": "From Table 3, we can see that some question types under MPT are comparatively simple (> 90%): scene detection, sport detection, and object presence. High accuracy is achieved even with absurdities, which we discuss in more detail in paragraph 7.4. Subordinate object detection is moderately high (> 80%), although there are a large number of unique answers. Accuracy in counting is low in all methods, despite a large number of training data. For the remaining question types, more analysis is needed to determine whether the weaker performance is due to lower amounts of training data, bias, or limitations of the models. Next, we examine how much of good performance is due to bias in the response distribution that N-MPT compensates for."}, {"heading": "7.2. Effects of the Proposed Accuracy Metrics", "text": "One of our main objectives was to compensate for the fact that algorithms can achieve high values by simply learning to answer more populous and simpler question types. Previous work has shown for existing datasets that simple baseline methods routinely exceed more complex methods with simple accuracy [12, 32, 9]. On TDIUC, MLP outperforms MCB and NMN in terms of simple accuracy, but closer scrutiny shows that MLP's result is highly determined by performance in large categories, such as \"absurd\" and \"object presence.\" With the help of MPT, we find that both NMN and MCB perform better than MLP in terms of simple accuracy. Inspected normalized values for each question type (Table 5) show even more pronounced differences, which are also reflected in arithmetic N-MPT scores presented in Table 3. This indicates that MLP is susceptible to being traversed by RAB compared to RAB, where RAB overfits can be traversed."}, {"heading": "7.3. Can Algorithms Predict Rare Answers?", "text": "Are the less frequently repeated questions actually more difficult to answer, or do the algorithms simply tend to answer more frequently? To investigate this, we created a subset of TDIUC, consisting only of questions whose answers are repeated less than 1,000 times. We call this data set TDIUC-Tail, which includes 46,590 train and 22,065 test questions. We then trained MCB on: 1) the full TDIUC data set and 2) TDIUC-Tail. Both versions were evaluated based on the validation splitting of TDIUC-Tail. We found that MCB is trained only on TDIUC-Tail, which has been trained on all TDIUC data types (Table 6). This shows that MCB is able to accurately predict rarer answers, but only tends to maximize more frequent answers to optimize QUC solutions for all types of questions."}, {"heading": "7.4. Effects of Including Absurd Questions", "text": "Absurd questions force a VQA system to look at the image to answer the question. In TDIUC, these questions are sampled from the rest of the dataset, and they have a high probability of being answered \"not true.\" This is confirmed by the QUES model, which achieves high accuracy with absurdity; however, for the same questions, if they are real for an image, it only achieves an accuracy of 6.77% for these questions. Good absurd performance is achieved by sacrificing performance in other categories. A robust VQA system should be able to detect absurd deviations without then failing in others. By examining the accuracy of real questions that are identical with absurd questions, we can quantify the ability of an algorithm to distinguish the absurd questions from the real questions. We found that simpler models had much lower accuracy with these questions, without then failing with other questions (U44% QES + 6Q: both models are coached with absurd questions)."}, {"heading": "7.5. Effects of Balancing Object Presence", "text": "In paragraph 7.3, we have seen that a distorted response distribution can affect generalization, and this effect is strong even for simple questions and affects even the most complex algorithms. Consider MCB-A when it is trained on both COCO-VQA and the Visual Genome, i.e., the winner of the CVPR-2016 VQA Workshop Challenge. When it is evaluated on TDIUC object presence questions containing 50% \"yes\" and 50% \"no\" questions, it correctly predicts \"yes\" answers with 86.3% accuracy, but only 11.2% for questions with \"no\" as an answer. However, after training on TDIUC, MCB-A is able to achieve 95.02% for \"yes\" and 92.26% for \"no.\" MCB-A performs poorly by learning the distortions in the COCO-VQA dataset, but is able to achieve good results when the categories of data are unbiased, which means that observations are not biased."}, {"heading": "7.6. Advantages of Attentive Models", "text": "By dividing the questions into types, we can assess which types benefit most from attention. We do this by comparing the MCB model with and without attention, i.e. with MCB and MCB-A. As shown in Table 3, attention helped to verify the results in several question categories, with the most pronounced increases in color recognition, attribute recognition, absurdity, and counting. All of these question types require the algorithm for detecting specific objects (or their missing) to be correctly answered. MCB-A calculates attention based on local characteristics from different spatial locations instead of global image characteristics. This helps locate individual objects. The attention mechanism learns the relative meaning of these characteristics. RAU also uses spatial attention and shows similar increases."}, {"heading": "7.7. Compositional and Modular Approaches", "text": "NMN and to a lesser extent RAU propose compositional approaches to VQA. For COCO-VQA, NMN has performed worse with simple accuracy than some MLP models [12]. We hoped that it would perform better than other models for questions that require logical analysis of an image in a step-by-step manner, such as position logic. However, while NMN performs better than MLP with MPT and N-MPT metrics, we did not see significant benefit in certain question types. This may be because NMN is limited by the quality of the \"S expression\" parser. Since NMN arranges the modules on the basis of these \"S expressions,\" any error in parsing these expressions will spread throughout the rest of the training. The S expression parser used in [1] may cause false or misleading parses."}, {"heading": "8. Conclusion", "text": "In this paper, we presented a new VQA dataset consisting of 12 explicitly defined question types, including the absurd question type, and conducted a rigorous comparison and analysis of the VQA algorithms on that dataset. Additionally, we proposed new evaluation metrics to compensate for the distortions in the dataset. Results show that the absurd questions and the new evaluation metrics allow us to better understand the performance of the VQA algorithms."}, {"heading": "A. Additional Details About TDIUC", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "B. Additional Experimental Results", "text": "In this section, we present additional experimental results omitted in the main paper due to insufficient space. First, the detailed normalized values for each question type are presented in Table 3. To calculate these values, the accuracy for each unique answer is calculated and averaged separately within a question type. Second, we present the results of the experiment in Section 7.3 in Table 6 (unnormalized) and Table 7 (normalized). Results are evaluated on TDIUC tail, a subset of TDIUC that consists only of questions whose answers have been repeated less than 1,000 times (unusual answers). Note that the TDIUC tail excludes absurd question types and object presence because they do not include questions with unusual answers. The algorithms are identical in both Table 6 and Table 7 and are named as follows: 1. MCB TDIUC tail: TDIUC tail and TUC-Tail: DIUC-Tail."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1601.01705,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 28:594\u2013611,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "EMNLP,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Are you talking to a machine? Dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "arXiv preprint arXiv:1604.01485,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Compositional memory for visual question answering", "author": ["A. Jiang", "F. Wang", "F. Porikli", "Y. Li"], "venue": "arXiv preprint arXiv:1511.05676,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick"], "venue": "arXiv preprint arXiv:1612.06890,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Answer-type prediction for visual question answering", "author": ["K. Kafle", "C. Kanan"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual question answering: Datasets, algorithms, and future challenges", "author": ["K. Kafle", "C. Kanan"], "venue": "arXiv preprint arXiv:1610.01465,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D.-H. Kwak", "M.-O. Heo", "J. Kim", "J.- W. Ha", "B.-T. Zhang"], "venue": "arXiv preprint arXiv:1606.01455,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Training recurrent answering units with joint loss minimization for VQA", "author": ["H. Noh", "B. Han"], "venue": "arXiv preprint arXiv:1606.03647,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["K. Saito", "A. Shin", "Y. Ushiku", "T. Harada"], "venue": "arXiv preprint arXiv:1606.06108,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual question answering: A survey of methods and datasets", "author": ["Q. Wu", "D. Teney", "P. Wang", "C. Shen", "A. Dick", "A. v. d. Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. van den Hengel", "A.R. Dick"], "venue": "In CVPR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "arXiv preprint arXiv:1603.01417,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "CVPR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "CoRR, abs/1512.02167,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images [19, 3].", "startOffset": 127, "endOffset": 134}, {"referenceID": 2, "context": "In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images [19, 3].", "startOffset": 127, "endOffset": 134}, {"referenceID": 18, "context": "VQA research began in earnest in late 2014 when the DAQUAR dataset was released [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "VQA Dataset\u2019 [3], the best algorithms are now approaching 70% accuracy [5] (human performance is 83%).", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "VQA Dataset\u2019 [3], the best algorithms are now approaching 70% accuracy [5] (human performance is 83%).", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 133, "endOffset": 136}, {"referenceID": 32, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 11, "context": "Following others [12, 32, 27], we refer to the portion of The VQA Dataset containing natural images as COCO-VQA.", "startOffset": 17, "endOffset": 29}, {"referenceID": 31, "context": "Following others [12, 32, 27], we refer to the portion of The VQA Dataset containing natural images as COCO-VQA.", "startOffset": 17, "endOffset": 29}, {"referenceID": 26, "context": "Following others [12, 32, 27], we refer to the portion of The VQA Dataset containing natural images as COCO-VQA.", "startOffset": 17, "endOffset": 29}, {"referenceID": 12, "context": "Detailed dataset reviews can be found in [13] and [26].", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "Detailed dataset reviews can be found in [13] and [26].", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "For COCO-VQA, a system trained using only question features achieves 50% accuracy [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "While some do compute additional statistics for basic question-types, overall performance is not computed from these sub-scores [3, 23].", "startOffset": 128, "endOffset": 135}, {"referenceID": 22, "context": "While some do compute additional statistics for basic question-types, overall performance is not computed from these sub-scores [3, 23].", "startOffset": 128, "endOffset": 135}, {"referenceID": 12, "context": "1% [13].", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "For example, on Caltech101 [4], even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.", "startOffset": 27, "endOffset": 30}, {"referenceID": 30, "context": "In [31], the Yin and Yang dataset was created to study the effect of having an equal number of binary (yes/no) questions about cartoon images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In [1], the SHAPES dataset was proposed, which has similar objectives.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "An unreleased dataset that improves on SHAPES is Compositional Language and Elementary Visual Reasoning (CLEVR) [11], which was described in a recent arXiv", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Both SHAPES and CLEVR were specifically tailored for compositional language approaches [1] and downplay the importance of visual reasoning.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Second, we created algorithms that generated questions from COCO\u2019s semantic segmentation annotations [17], and Visual Genome\u2019s objects and attributes annotations [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "Second, we created algorithms that generated questions from COCO\u2019s semantic segmentation annotations [17], and Visual Genome\u2019s objects and attributes annotations [16].", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": "For this reason, some have argued that VQA is a kind of Visual Turing Test [19].", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": ", [6, 9]), the majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories.", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": ", [6, 9]), the majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories.", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 22, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 4, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 24, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 7, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 13, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 9, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 17, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 21, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 23, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 26, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 28, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 29, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 31, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 8, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 19, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 12, "context": "For a more comprehensive review, see [13] and [26].", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "For a more comprehensive review, see [13] and [26].", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other [3, 12, 32], where the image features come from the last hidden layer of a CNN.", "startOffset": 160, "endOffset": 171}, {"referenceID": 11, "context": "Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other [3, 12, 32], where the image features come from the last hidden layer of a CNN.", "startOffset": 160, "endOffset": 171}, {"referenceID": 31, "context": "Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other [3, 12, 32], where the image features come from the last hidden layer of a CNN.", "startOffset": 160, "endOffset": 171}, {"referenceID": 11, "context": "These simple approaches often work well and can be competitive with complex attentive models [12, 32].", "startOffset": 93, "endOffset": 101}, {"referenceID": 31, "context": "These simple approaches often work well and can be competitive with complex attentive models [12, 32].", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 24, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 29, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 27, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 28, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 17, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 7, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 4, "context": "The MCB system [5] won the CVPR-2016 VQA Workshop Challenge.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "The neural module network (NMN) is an especially interesting compositional approach to VQA [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "The neural module network (NMN) is an especially interesting compositional approach to VQA [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 20, "context": "The multi-step recurrent answering units (RAU) model for VQA is another state-of-the-art method [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "\u2022 MCB: MCB [5] without spatial attention.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "\u2022 MCB-A: MCB [5] with spatial attention.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "\u2022 NMN: NMN from [1] with minor modifications.", "startOffset": 16, "endOffset": 19}, {"referenceID": 20, "context": "\u2022 RAU: RAU [21] with minor modifications.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "For image features, ResNet-152 [7] was used for all models.", "startOffset": 31, "endOffset": 34}, {"referenceID": 14, "context": "QUES, Q+I, and MLP all use 4800-dimensional skip-thought vectors [15] to embed the question, as was done in [12].", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "QUES, Q+I, and MLP all use 4800-dimensional skip-thought vectors [15] to embed the question, as was done in [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy [12, 32, 9].", "startOffset": 135, "endOffset": 146}, {"referenceID": 31, "context": "For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy [12, 32, 9].", "startOffset": 135, "endOffset": 146}, {"referenceID": 8, "context": "For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy [12, 32, 9].", "startOffset": 135, "endOffset": 146}, {"referenceID": 30, "context": "Similar observations about balancing yes/no questions were made in [31].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "For COCO-VQA, NMN has performed worse than some MLP models [12] using simple accuracy.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "The S-expression parser used in [1] can produce wrong or misleading parses in many cases.", "startOffset": 32, "endOffset": 35}], "year": 2017, "abstractText": "In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.", "creator": "LaTeX with hyperref package"}}}