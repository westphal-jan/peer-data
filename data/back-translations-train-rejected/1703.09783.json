{"id": "1703.09783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Two-Stream RNN/CNN for Action Recognition in 3D Videos", "abstract": "The recognition of actions from video sequences has many applications in health monitoring, assisted living, surveillance, and smart homes. Despite advances in sensing, in particular related to 3D video, the methodologies to process the data are still subject to research. We demonstrate superior results by a system which combines recurrent neural networks with convolutional neural networks in a voting approach. The gated-recurrent-unit-based neural networks are particularly well-suited to distinguish actions based on long-term information from optical tracking data; the 3D-CNNs focus more on detailed, recent information from video data. The resulting features are merged in an SVM which then classifies the movement. In this architecture, our method improves recognition rates of state-of-the-art methods by 14% on standard data sets.", "histories": [["v1", "Wed, 22 Mar 2017 22:29:56 GMT  (4926kb)", "http://arxiv.org/abs/1703.09783v1", "8 pages, 8 figures"]], "COMMENTS": "8 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rui zhao", "haider ali", "patrick van der smagt"], "accepted": false, "id": "1703.09783"}, "pdf": {"name": "1703.09783.pdf", "metadata": {"source": "CRF", "title": "Two-Stream RNN/CNN for Action Recognition in 3D Videos", "authors": ["Rui Zhao", "Haider Ali", "Patrick van der Smagt"], "emails": ["Haider.Ali}@dlr.de"], "sections": [{"heading": null, "text": "In fact, it is true that most people are able to survive by themselves, in the form of people who survive by themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "II. METHODOLOGY", "text": "In this section, we first present the concept of recurrent neural networks, and then the applied 3D CNN structure is processed with different temporal sequences. (i) This transforms the input X into an internal hidden state. (i. the network guides the state along the next input xt + 1 to the neural networks. The neural networks can handle sequence information with different lengths of time. (i.) The neurons learn when they remember and forget information with nonlinear activation functions: ht = (W (xtht))) (1) yt = 1)."}, {"heading": "III. EXPERIMENTS", "text": "The models presented in the previous section are evaluated in the experiments. First, the data set is presented in this section, then the setups and parameter settings for the experiments are presented. We compare the results of the proposed models with the current best methods. Finally, we analyze and discuss the problems associated with the deep learning methods for 3D Action Recognition.A. NTU RGB + D Dataset [2] The proposed approaches are evaluated on the NTU RGB + D Datasets, which we know as the largest publicly available 3D Action Recognition Datasets at the moment. The Dataset consists of more than 56k action videos and 4 million frames collected by 3 Kinect V2 cameras from 40 different subjects, and divided into 60 different action classes, including 40 daily (drinking, eating, reading, etc.), 9 extensive (sneezing, falling down, etc.) and 11 reciprocal actions (punching, hugging, etc.)."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "In this article, we propose a novel RNN structure for 3D skeletons that achieves state-of-the-art performance on the largest 3D action detection dataset. The proposed RNN model can also be trained 13 times faster and saves 20% computing power in each training step. In addition, the RGB videos from the same dataset are used to refine a 3DCNN model. In the end, an efficient fusion structure is introduced, two streams RNN / CNN, to merge the capabilities of both RNN and CNN models. Results of this method are 13% higher than the use of the proposed RNN alone and 14% higher than the best result published in literature. In the future, we will consider the use of other sensor modalities such as depth maps and IR sequences and see which is the best architecture to merge all these modalities."}], "references": [{"title": "Action recognition based on a bag of 3D points", "author": ["W. Li", "Z. Zhang", "Z. Liu"], "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops. IEEE, 2010, pp. 9\u201314.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "NTU RGB+D: A large scale dataset for 3D human activity analysis", "author": ["A. Shahroudy", "J. Liu", "T.-T. Ng", "G. Wang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical recurrent neural network for skeleton based action recognition", "author": ["Y. Du", "W. Wang", "L. Wang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1110\u20131118. Fig. 8: Some correctly classified action samples (first four frames with green predictions) and some mis-classified action samples (last two frames with red predictions). These samples are randomly picked from the feature fusion model cross view testing results.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatio-temporal LSTM with trust gates for 3d human action recognition", "author": ["J. Liu", "A. Shahroudy", "D. Xu", "G. Wang"], "venue": "arXiv preprint arXiv:1607.07043, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Cooccurrence feature learning for skeleton based action recognition using regularized deep lstm networks", "author": ["W. Zhu", "C. Lan", "J. Xing", "W. Zeng", "Y. Li", "L. Shen", "X. Xie"], "venue": "arXiv preprint arXiv:1603.07772, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "3d human activity recognition with reconfigurable convolutional neural networks", "author": ["K. Wang", "X. Wang", "L. Lin", "M. Wang", "W. Zuo"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 97\u2013106.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Action recognition from depth maps using deep convolutional neural networks", "author": ["P. Wang", "W. Li", "Z. Gao", "J. Zhang", "C. Tang", "P.O. Ogunbona"], "venue": "2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning spatiotemporal features with 3D convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "2015 IEEE International Conference on Computer Vision (ICCV). IEEE, 2015, pp. 4489\u20134497.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Skeletal quads: Human action recognition using joint quadruples", "author": ["G. Evangelidis", "G. Singh", "R. Horaud"], "venue": "International Conference on Pattern Recognition, 2014, pp. 4513\u20134518.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Human action recognition by representing 3D skeletons as points in a lie group", "author": ["R. Vemulapalli", "F. Arrate", "R. Chellappa"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 588\u2013595.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Jointly learning heterogeneous features for RGB-D activity recognition", "author": ["J.-F. Hu", "W.-S. Zheng", "J. Lai", "J. Zhang"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5344\u20135352.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Histogram of oriented principal components for cross-view action recognition", "author": ["H. Rahmani", "A. Mahmood", "D. Huynh", "A. Mian"], "venue": "2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Human activity recognition process using 3-d posture data", "author": ["S. Gaglio", "G.L. Re", "M. Morana"], "venue": "IEEE Transactions on Human- Machine Systems, vol. 45, no. 5, pp. 586\u2013597, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Real-time human action recognition based on depth motion maps", "author": ["C. Chen", "K. Liu", "N. Kehtarnavaz"], "venue": "Journal of real-time image processing, pp. 1\u20139, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Rgbd-hudaact: A color-depth video database for human daily activity recognition", "author": ["B. Ni", "G. Wang", "P. Moulin"], "venue": "Consumer Depth Cameras for Computer Vision. Springer, 2013, pp. 193\u2013208.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Unstructured human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "Robotics and Automation  (ICRA), 2012 IEEE International Conference on. IEEE, 2012, pp. 842\u2013849.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining actionlet ensemble for action recognition with depth cameras", "author": ["J. Wang", "Z. Liu", "Y. Wu", "J. Yuan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 1290\u20131297.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "View invariant human action recognition using histograms of 3d joints", "author": ["L. Xia", "C.-C. Chen", "J. Aggarwal"], "venue": "2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE, 2012, pp. 20\u201327.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic feature selection for online action recognition", "author": ["V. Bloom", "V. Argyriou", "D. Makris"], "venue": "International Workshop on Human Behavior Understanding. Springer, 2013, pp. 64\u201376.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Human action recognition and retrieval using sole depth information", "author": ["Y.-C. Lin", "M.-C. Hu", "W.-H. Cheng", "Y.-H. Hsieh", "H.-M. Chen"], "venue": "Proceedings of the 20th ACM international conference on Multimedia. ACM, 2012, pp. 1053\u20131056.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Privacy preserving automatic fall detection for elderly using rgbd cameras", "author": ["C. Zhang", "Y. Tian", "E. Capezuti"], "venue": "International Conference on Computers for Handicapped Persons. Springer, 2012, pp. 625\u2013 633.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences", "author": ["O. Oreifej", "Z. Liu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 716\u2013723.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning human activities and object affordances from rgb-d videos", "author": ["H.S. Koppula", "R. Gupta", "A. Saxena"], "venue": "The International Journal of Robotics Research, vol. 32, no. 8, pp. 951\u2013970, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A decision forest based feature selection framework for action recognition from rgb-depth cameras", "author": ["F. Negin", "F. \u00d6zdemir", "C.B. Akg\u00fcl", "K.A. Y\u00fcksel", "A. Er\u00e7il"], "venue": "International Conference Image Analysis and Recognition. Springer, 2013, pp. 648\u2013657.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Concurrent action detection with structural prediction", "author": ["P. Wei", "N. Zheng", "Y. Zhao", "S.-C. Zhu"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 3136\u20133143.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "An evaluation of 3d motion flow and 3d pose estimation for human action recognition", "author": ["M. Munaro", "S. Michieletto", "E. Menegatti"], "venue": "RSS Workshops: RGB-D: Advanced Reasoning with Depth Cameras, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring the trade-off between accuracy and observational latency in action recognition", "author": ["C. Ellis", "S.Z. Masood", "M.F. Tappen", "J.J. Laviola Jr", "R. Sukthankar"], "venue": "International Journal of Computer Vision, vol. 101, no. 3, pp. 420\u2013436, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Inverse dynamics for action recognition", "author": ["A. Mansur", "Y. Makihara", "Y. Yagi"], "venue": "IEEE transactions on cybernetics, vol. 43, no. 4, pp. 1226\u20131236, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Rgb-depth feature for 3d human activity recognition", "author": ["Z. Yang", "L. Zicheng", "C. Hong"], "venue": "China Communications, vol. 10, no. 7, pp. 93\u2013 103, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Recognition of human actions from rgb-d videos using a reject option", "author": ["V. Carletti", "P. Foggia", "G. Percannella", "A. Saggese", "M. Vento"], "venue": "International Conference on Image Analysis and Processing. Springer, 2013, pp. 436\u2013445.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Gait-based gender recognition using pose information for real time applications", "author": ["D. Kastaniotis", "I. Theodorakopoulos", "G. Economou", "S. Fotopoulos"], "venue": "Digital Signal Processing (DSP), 2013 18th International Conference on. IEEE, 2013, pp. 1\u20136.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Coupled hidden conditional random fields for rgb-d human action recognition", "author": ["A.-A. Liu", "W.-Z. Nie", "Y.-T. Su", "L. Ma", "T. Hao", "Z.-X. Yang"], "venue": "Signal Processing, vol. 112, pp. 74\u201382, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequential maxmargin event detectors", "author": ["D. Huang", "S. Yao", "Y. Wang", "F. De La Torre"], "venue": "European conference on computer vision. Springer, 2014, pp. 410\u2013424.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative hierarchical modeling of spatio-temporally composable human activities", "author": ["I. Lillo", "A. Soto", "J. Carlos Niebles"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 812\u2013819.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative orderlet mining for realtime recognition of human-object interaction", "author": ["G. Yu", "Z. Liu", "J. Yuan"], "venue": "Asian Conference on Computer Vision. Springer, 2014, pp. 50\u201365.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Watch-n-patch: Unsupervised understanding of actions and relations", "author": ["C. Wu", "J. Zhang", "S. Savarese", "A. Saxena"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4362\u20134370.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Exemplarbased recognition of human\u2013object interactions", "author": ["J.-F. Hu", "W.-S. Zheng", "J. Lai", "S. Gong", "T. Xiang"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 26, no. 4, pp. 647\u2013660, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor", "author": ["C. Chen", "R. Jafari", "N. Kehtarnavaz"], "venue": "Image Processing (ICIP), 2015 IEEE International Conference on. IEEE, 2015, pp. 168\u2013172.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Human daily action analysis with multi-view and color-depth data", "author": ["Z. Cheng", "L. Qin", "Y. Ye", "Q. Huang", "Q. Tian"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 52\u201361.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A viewpointindependent statistical method for fall detection", "author": ["Z. Zhang", "W. Liu", "V. Metsis", "V. Athitsos"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3626\u20133630.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Berkeley mhad: A comprehensive multimodal human action database", "author": ["F. Ofli", "R. Chaudhry", "G. Kurillo", "R. Vidal", "R. Bajcsy"], "venue": "Applications of Computer Vision (WACV), 2013 IEEE Workshop on. IEEE, 2013, pp. 53\u201360.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonintrusive human activity monitoring in a smart home environment", "author": ["S.M. Amiri", "M.T. Pourazad", "P. Nasiopoulos", "V.C. Leung"], "venue": "e-Health Networking, Applications & Services (Healthcom), 2013 IEEE 15th International Conference on. IEEE, 2013, pp. 606\u2013610.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling 4d humanobject interactions for event and object recognition", "author": ["P. Wei", "Y. Zhao", "N. Zheng", "S.-C. Zhu"], "venue": "2013 IEEE International Conference on Computer Vision. IEEE, 2013, pp. 3272\u2013 3279.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-view action modeling, learning and recognition", "author": ["J. Wang", "X. Nie", "Y. Xia", "Y. Wu", "S.-C. Zhu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 2649\u20132656.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Hopc: Histogram of oriented principal components of 3d pointclouds for action recognition", "author": ["H. Rahmani", "A. Mahmood", "D.Q. Huynh", "A. Mian"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 742\u2013757.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Multipe/single-view human action recognition via part-induced multitask structural learning", "author": ["A.-A. Liu", "Y.-T. Su", "P.-P. Jia", "Z. Gao", "T. Hao", "Z.-X. Yang"], "venue": "IEEE transactions on cybernetics, vol. 45, no. 6, pp. 1194\u20131208, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Body surface context: A new robust feature for action recognition from depth videos", "author": ["Y. Song", "J. Tang", "F. Liu", "S. Yan"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 24, no. 6, pp. 952\u2013964, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-person interaction detection using body-pose features and multiple instance learning", "author": ["K. Yun", "J. Honorio", "D. Chattopadhyay", "T.L. Berg", "D. Samaras"], "venue": "2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE, 2012, pp. 28\u201335.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient interaction recognition through positive action representation", "author": ["T. Hu", "X. Zhu", "W. Guo", "K. Su"], "venue": "Mathematical Problems in Engineering, vol. 2013, 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation of video activity localizations integrating quality and quantity measurements", "author": ["C. Wolf", "E. Lombardi", "J. Mille", "O. Celiktutan", "M. Jiu", "E. Dogan", "G. Eren", "M. Baccouche", "E. Dellandr\u00e9a", "C.-E. Bichot"], "venue": "Computer Vision and Image Understanding, vol. 127, pp. 14\u201330, 2014.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "G3di: A gaming interaction dataset with a real time detection and evaluation framework", "author": ["V. Bloom", "V. Argyriou", "D. Makris"], "venue": "Workshop at the European Conference on Computer Vision. Springer, 2014, pp. 698\u2013712.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Dyadic interaction detection from pose and flow", "author": ["C. Van Gemeren", "R.T. Tan", "R. Poppe", "R.C. Veltkamp"], "venue": "International Workshop on Human Behavior Understanding. Springer, 2014, pp. 101\u2013115.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep structured model with radius\u2013margin bound for 3d human activity recognition", "author": ["L. Lin", "K. Wang", "W. Zuo", "M. Wang", "J. Luo", "L. Zhang"], "venue": "International Journal of Computer Vision, pp. 1\u201318, 2015.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Convnets-based action recognition from depth maps through virtual cameras and pseudocoloring", "author": ["P. Wang", "W. Li", "Z. Gao", "C. Tang", "J. Zhang", "P. Ogunbona"], "venue": "Proceedings of the 23rd ACM international conference on Multimedia. ACM, 2015, pp. 1119\u20131122.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch normalization: Accelerating deep  network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2625\u20132634.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 568\u2013576.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 1, pp. 221\u2013231, 2013.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1997}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensorflow: Largescale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Tflearn", "author": ["A. Damien"], "venue": "https://github.com/tflearn/tflearn, 2016.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 675\u2013678.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2014}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems, vol. 2, no. 4, pp. 303\u2013314, 1989.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 1989}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": "Communications of the ACM, vol. 56, no. 1, pp. 116\u2013124, 2013.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2013}, {"title": "Characterizations of noise in kinect depth images: a review", "author": ["T. Mallick", "P.P. Das", "A.K. Majumdar"], "venue": "IEEE Sensors Journal, vol. 14, no. 6, pp. 1731\u20131740, 2014.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recognition of human activity in 3D videos has received increasing attention since 2010 [1]\u2013[7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Recognition of human activity in 3D videos has received increasing attention since 2010 [1]\u2013[7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Our proposed deep-learning methods consist mainly of three parts: a novel skeleton-based recurrent neural network structure, using a 3D-convolutional [8] neural network for RGB videos, and sketching a new two-stream fusion method to combine RNN and CNN.", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "All methods are evaluated on the NTU RGB+D Dataset [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 81, "endOffset": 84}, {"referenceID": 51, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 158, "endOffset": 161}, {"referenceID": 52, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 163, "endOffset": 167}, {"referenceID": 53, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "Focussing on the latter, for skeleton-based activity analysis, [2]\u2013[5] used different kinds of recurrent neural networks to acquire state-of-the-art performances on various of 3D action datasets.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Focussing on the latter, for skeleton-based activity analysis, [2]\u2013[5] used different kinds of recurrent neural networks to acquire state-of-the-art performances on various of 3D action datasets.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "[3] propose an hierarchical RNN, which is", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] present a novel long short-term memory (LSTM) [55] cell, called part-aware LSTM, which is also fed with separated five parts of skeleton.", "startOffset": 0, "endOffset": 3}, {"referenceID": 54, "context": "[2] present a novel long short-term memory (LSTM) [55] cell, called part-aware LSTM, which is also fed with separated five parts of skeleton.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "[5] provide a novel deep RNN structure, which can automatically learn the co-occurrence, similar to grouping data into five human body parts, from skeleton data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] propose a skeleton tree traversal algorithm and a new gating mechanism to improve robustness against noise and occlusion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 55, "context": "Our method is inspired by recent normalization technologies [56] and a novel recurrent neuron mechanism [57].", "startOffset": 60, "endOffset": 64}, {"referenceID": 56, "context": "Our method is inspired by recent normalization technologies [56] and a novel recurrent neuron mechanism [57].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "To process RGB videos, our method is inspired by different kinds of convolutional neural networks [8], [58]\u2013[60].", "startOffset": 98, "endOffset": 101}, {"referenceID": 57, "context": "To process RGB videos, our method is inspired by different kinds of convolutional neural networks [8], [58]\u2013[60].", "startOffset": 103, "endOffset": 107}, {"referenceID": 59, "context": "To process RGB videos, our method is inspired by different kinds of convolutional neural networks [8], [58]\u2013[60].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "We use a 3D-CNN [8] model on the RGB videos of the NTU RGB+D dataset.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Our two-stream RNN/CNN structure outperforms the current state-of-the-art method [4] more than 14% on both cross subject and cross view settings.", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "decision fusion and feature fusion, are proposed to combine the proposed RNN structure and a 3D-CNN structure [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 54, "context": "2) Long Short-Term Memory: This problem can be solved by LSTM [55] which stores information in gated cells at the neurons.", "startOffset": 62, "endOffset": 66}, {"referenceID": 56, "context": "3) Gated Recurrent Unit: An improvement to LSTM called gated recurrent unit (GRU) was proposed in [57].", "startOffset": 98, "endOffset": 102}, {"referenceID": 60, "context": "4) Bidirectional Recurrent Neural Network: A bidirectional RNN [61] performs a forward pass and a backward pass, which runs input data from t = T to t = 1 and from t = 1 to t = T , respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 55, "context": "5) Batch Normalization: To train a deep neural network, the internal covariate shift [56] slows down the training process.", "startOffset": 85, "endOffset": 89}, {"referenceID": 61, "context": "Then the normalized activations flow to the next fully-connected layer with 600 rectified linear unit (ReLU) [62] activation functions.", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "To process RGB videos, we choose to use the 3D-CNN model from [8], as it shows promising performances on 2D video action recognition tasks.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "To be specific, the 3D-CNN model [8], which we choose, Conv1 64 Pool 1 Conv2 128 Pool 2 Conv3 2x256 Pool 3 Conv4 2x512 Pool 4 Conv5 2x512 Pool 5 FC6 4096 FC7 4096 Output 60", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "We finetune this model with pretrained parameters [8] on Sports-1M Dataset, which has approximately one million YouTube videos.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "NTU RGB+D Dataset [2]", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "The proposed approaches are evaluated on the NTU RGB+D dataset [2], which we know as the current largest publicly available 3D action recognition dataset.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "This dataset has two standard evaluation criteria [2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "The validation set is composed of 10% of the subjects in the training set in [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "The remaining subjects in the training set [2] make up the training set.", "startOffset": 43, "endOffset": 46}, {"referenceID": 62, "context": "We use TensorFlow [63] with TFlearn [64] and run the experiments on either one NVIDIA GTX 1080 GPU or one NVIDIA GTX TITAN X GPU.", "startOffset": 18, "endOffset": 22}, {"referenceID": 63, "context": "We use TensorFlow [63] with TFlearn [64] and run the experiments on either one NVIDIA GTX 1080 GPU or one NVIDIA GTX TITAN X GPU.", "startOffset": 36, "endOffset": 40}, {"referenceID": 64, "context": "We train the network using RMSprop [65] optimizer and set learning rate as 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "2) CNN Implementation: We use the 3D-CNN model [8] in Caffe [66] and train it on RGB frames from the NTU RGB+D dataset, with pretrained parameters [8] from the Sport1M dataset.", "startOffset": 47, "endOffset": 50}, {"referenceID": 65, "context": "2) CNN Implementation: We use the 3D-CNN model [8] in Caffe [66] and train it on RGB frames from the NTU RGB+D dataset, with pretrained parameters [8] from the Sport1M dataset.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "2) CNN Implementation: We use the 3D-CNN model [8] in Caffe [66] and train it on RGB frames from the NTU RGB+D dataset, with pretrained parameters [8] from the Sport1M dataset.", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "From RGB videos, we extract the frames, crop and resize them from 1920 \u00d7 1080 pixels to 320 \u00d7 240 pixels [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 64, "context": "The learning rate is then reduced by half, when no training progress was observed [65].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "For feature fusion, we extract the RNN features (600 dimensions) from the fully-connected layer, and extract CNN features (4096dimensions) from the fc-6 layer [8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 66, "context": "We use training and validation splits to find the optimal value of C for linear SVM [67] model.", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "I shows that our RNN structure, the 1 Layer LSTMBN, already outperforms the baseline method part-aware LSTM reported in [2] because batch normalization improves the LSTM model.", "startOffset": 120, "endOffset": 123}, {"referenceID": 67, "context": "From rows 12 and 13 we can see that the performances of LSTM and GRU cells are similar [68].", "startOffset": 87, "endOffset": 91}, {"referenceID": 68, "context": "This increases the complexity of the neural network, which helps the model capture more inherent features from the 3D skeleton data [69].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "Method cross subject cross view 01 Skeleton Quads [2], [9] 38.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Method cross subject cross view 01 Skeleton Quads [2], [9] 38.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "36% 02 Lie Group [2], [10] 50.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "36% 02 Lie Group [2], [10] 50.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "76% 03 FTP Dynamic Skeletons [2], [11] 60.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "76% 03 FTP Dynamic Skeletons [2], [11] 60.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "22% 04 HBRNN-L [2], [3] 59.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "22% 04 HBRNN-L [2], [3] 59.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "97% 05 Deep RNN [2] 56.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "09% 06 Deep LSTM [2] 60.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "29% 07 Part-aware LSTM [2] 62.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "27% 08 ST-LSTM (Tree) + Trust Gate [4] 69.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "23% 17 3D-CNN [8] 79.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "skeleton-based models including ST-LSTM (Tree traversal) + Trust Gate [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "In the next step, we utilize a linear SVM [8] to fuse the fc-6 features from the CNN and the fc features from the RNN.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "This further improves results by over 13% in comparison to our best RNN, and by more than 14% compared to literature models [2], [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "This further improves results by over 13% in comparison to our best RNN, and by more than 14% compared to literature models [2], [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 69, "context": "Kinect depth information, from which the NTU skeleton data is created, is quite noisy [70], [71].", "startOffset": 86, "endOffset": 90}, {"referenceID": 70, "context": "Kinect depth information, from which the NTU skeleton data is created, is quite noisy [70], [71].", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "Correspondingly, the 3D skeleton data used in our RNNs are also quite noisy [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Thirdly, the 3D-CNN model [8] is trained with small video clips, which are 16 time steps long.", "startOffset": 26, "endOffset": 29}], "year": 2017, "abstractText": "The recognition of actions from video sequences has many applications in health monitoring, assisted living, surveillance, and smart homes. Despite advances in sensing, in particular related to 3D video, the methodologies to process the data are still subject to research. We demonstrate superior results by a system which combines recurrent neural networks with convolutional neural networks in a voting approach. The gated-recurrent-unit-based neural networks are particularly well-suited to distinguish actions based on long-term information from optical tracking data; the 3D-CNNs focus more on detailed, recent information from video data. The resulting features are merged in an SVMwhich then classifies the movement. In this architecture, our method improves recognition rates of state-of-the-art methods by 14% on standard data sets.", "creator": "LaTeX with hyperref package"}}}