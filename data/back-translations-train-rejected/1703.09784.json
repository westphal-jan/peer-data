{"id": "1703.09784", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Perception Driven Texture Generation", "abstract": "This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. Meanwhile, perceptual attributes, such as directionality, regularity and roughness are important factors for human observers to describe a texture. In this paper, we propose a joint deep network model that combines adversarial training and perceptual feature regression for texture generation, while only random noise and user-defined perceptual attributes are required as input. In this model, a preliminary trained convolutional neural network is essentially integrated with the adversarial framework, which can drive the generated textures to possess given perceptual attributes. An important aspect of the proposed model is that, if we change one of the input perceptual features, the corresponding appearance of the generated textures will also be changed. We design several experiments to validate the effectiveness of the proposed method. The results show that the proposed method can produce high quality texture images with desired perceptual properties.", "histories": [["v1", "Fri, 24 Mar 2017 01:25:30 GMT  (2611kb,D)", "http://arxiv.org/abs/1703.09784v1", "7 pages, 4 figures, icme2017"]], "COMMENTS": "7 pages, 4 figures, icme2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["yanhai gan", "huifang chi", "ying gao", "jun liu", "guoqiang zhong", "junyu dong"], "accepted": false, "id": "1703.09784"}, "pdf": {"name": "1703.09784.pdf", "metadata": {"source": "CRF", "title": "PERCEPTION DRIVEN TEXTURE GENERATION", "authors": ["Yanhai Gan", "Huifang Chi", "Ying Gao", "Jun Liu", "Guoqiang Zhong", "Junyu Dong"], "emails": ["gaoying}@stu.ouc.edu.cn,", "liujunqd@163.com,", "dongjunyu}@ouc.edu.cn"], "sections": [{"heading": null, "text": "In fact, it is as if most people are able to survive themselves by blaming themselves and others. (...) In fact, it is as if most people are able to survive themselves. (...) \"It is as if they would survive themselves.\" (...) \"It is as if they would survive themselves.\" (...) \"It is as if they would survive themselves.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"(...)\" It is as if. \"(...)\" It is as if. \"(...)\" (It is. \"(...)\" It is as if. \"(...). (...\" It is. \"(...)\" It is as if. \"(...). (...\" It is as if. \"It is.\" (...) It is. (... \"It is.\" It is. () It is. (... \"It is as if. (...).\" It is. \"It is. (...\" It is. \"It is.\" It is. \"It is. (). (...\" It is. \"It is.\" It is. (). \"It is. (). (It is. (). (It is. (...). (It is. (It is.\" It is. (). (It is. (). (It is. (). (It is. (It is. (It is.). (It is."}, {"heading": "3.1. Overall Architecture of the Joint Model", "text": "According to [20], there are 12 prominent perceptual traits for humans to perceive a texture. In practice, humans can perceive these traits not only from a texture, but also imagine a texture from these perceptual descriptions. Therefore, we have designed a common deep model to achieve such a goal. As shown in Figure 1, the general architecture comprises three parts: a perceptual trait regression model, a conditional generative model, and a discriminatory model. The generative model is responsible for conditional texture generation, whereas the discriminatory model is used to distinguish whether the generated texture originates from training sample distribution, and the perceptual model to produce textures that possess certain characteristics."}, {"heading": "3.2. Network Design Details", "text": "In this section, we first present the initialization scheme for our deep networks, and then we introduce strategies for designing certain parts of the network. Inspired by [22], we can expand the weights of a layer of the proposed network simply by random shapes. However, in most cases, we only look at the reverse propagation situation, so we can reduce the number of units that can be achieved by an input neuron, and w represent the weight in a continuous or fully connected layer. ReLU is used as an activation function in the network, as it can reduce the gradient flight effect and make the model fast. However, we want to have the output of the generator in a specific range, because an image always has limited pixel values. The discriminator should provide a probability result indicating whether an image disappears from the real training samples and we use the activation function as a starting point."}, {"heading": "4.1. The Data Set", "text": "In our experiments, we use the Perceptual Texture Database (PTD), in which there are 450 textures with corresponding 12-D perceptual characteristics. [20] Textures in PTD have a resolution of 512 \u00d7 512, and the 12-D perceptual characteristics include contrast, repeatability, granularity, randomness, roughness, density, directionality, structural complexity, coarseness, regularity, orientation, and uniformity. However, since 450 textures are still too few to train a deep neural network, we expand the examples in the following ways: First, we cut each texture into 81 textures of size 448 \u00d7 448; the step used for cropping is 8. Second, we reduce the resulting textures to 299 \u00d7 299. In terms of perceptual characteristics, we let the resulting textures have the same values as their original ones. Finally, we get 3699 \u00d7 299 examples of size, 299 \u00d7 248, and recognize that they are 448 among them."}, {"heading": "4.2. Perceptual Feature Regression", "text": "Since our perception model was modified by Inceptionv3, we did not need to train it from scratch.The pre-trained Inception v3 on ImageNet can be found in [23]. Because our perception model differed from Inception v3 only in the output layer and the loss definition, we initialized the output layer with abbreviated Gaussian noise, and the other layers were reloaded from the pre-trained Inception v3 model. Subsequently, we refined the perception model at the initial learning rate of 0.001. The RMSProp method was used for gradient descent [24]. We performed the optimization algorithm for 5000 iterations. The process is illustrated in Fig. 2 (a). Finally, the euclidean loss converged to 0.01161, and the final evaluation error was 0.0039. Since the perceptual attributes have 12 attributes, the standard model can be calculated with an error of 0.0001, which means that for each of these attributes we can accurately predict."}, {"heading": "4.3. Generating Textures from Perceptual Features", "text": "In fact, it is not as if one sees oneself as being able to surpass oneself, as if one could surpass oneself. (...) It is not as if one were to surpass oneself. (...) It is not as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself. (...) It is as if one were to surpass oneself."}], "references": [{"title": "An in-place texture synthesis technique for memory constrained multimedia applications", "author": ["Alexey Badalov", "Irene Cheng", "Claudio Silva", "Anup Basu"], "venue": "IEEE International Conference on Multimedia and Expo, 2011, pp. 1\u20134.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "Yann Lecun"], "venue": "2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp. 2146\u20132153.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems, vol. 25, no. 2, pp. 2012, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Draw: A  recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Thomas Brox"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1538\u20131546.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u20132680.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "arXiv preprint arXiv:1603.05631, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei F Li"], "venue": "Advances in neural information processing systems, 2014, pp. 1889\u20131897.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "arXiv preprint arXiv:1512.00570, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "The texture lexicon: Understanding the categorization of visual texture terms and their relationship to texture images", "author": ["Nalini Bhushan", "A Ravishankar Rao", "Gerald L Lohse"], "venue": "Cognitive Science, vol. 21, no. 2, pp. 219\u2013246, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "On pixel-based texture synthesis by nonparametric sampling", "author": ["Seunghyup Shin", "Tomoyuki Nishita", "Sung Yong Shin"], "venue": "Computers & Graphics, vol. 30, no. 5, pp. 767\u2013778, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast texture synthesis using tree-structured vector quantization", "author": ["Li-Yi Wei", "Marc Levoy"], "venue": "Proceedings of the 27th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., 2000, pp. 479\u2013488.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Leon Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 262\u2013270.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Texture networks: Feed-forward synthesis of textures and stylized images", "author": ["Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1603.03417, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional generative adversarial nets for convolutional face generation", "author": ["Jon Gauthier"], "venue": "Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, vol. 2014, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards a texture naming system: Identifying relevant dimensions of texture", "author": ["A. Ravishankar Rao", "Gerald L. Lohse"], "venue": "Vision Research, vol. 36, no. 11, pp. 1649\u2013 1669, 1996.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "Visual perception of procedural textures: Identifying perceptual dimensions and predicting generation models", "author": ["Jun Liu", "Junyu Dong", "Xiaoxu Cai", "Lin Qi", "Mike Chantler"], "venue": "PloS one, vol. 10, no. 6, pp. e0130335, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026\u20131034.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Overview of mini-batch gradient descent", "author": ["Kevin Swersky Geoffrey Hinton", "Nitish Srivastava"], "venue": "http:// www.cs.toronto.edu/%7Etijmen/csc321/ slides/lecture_slides_lec6.pdf.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 0}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Texture synthesis and generation have also been extensively investigated in the past years [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Convolutional Neural Network (CNN), which was inspired by the mechanism of visual cortex, has shown great superiority in latest studies [2] [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Convolutional Neural Network (CNN), which was inspired by the mechanism of visual cortex, has shown great superiority in latest studies [2] [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "For example, in the ImageNet Large Scale Visual Recognition Challenge, the performance of computer algorithms even surpassed human\u2019s [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Consequently, researchers have been investigating different approaches based on CNN for image generation [5] [6] [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Consequently, researchers have been investigating different approaches based on CNN for image generation [5] [6] [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Consequently, researchers have been investigating different approaches based on CNN for image generation [5] [6] [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "proposed a generative adversarial framework (GAN) [8] and produced excellent results in many image generation tasks.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "In order to generate more realistic images, Wang and Gupta factorized the image generation process and proposed a joint model consisting of Style and Structure Generative Adversarial Networks [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 8, "context": "Experimental results in [9] suggested that a great gain could be obtained through this factoring trick for generating realistic indoor scenes.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "proposed a fragment embedding method in 2014 [10], which was essentially a bidirectional retrieval scheme, as the desired image must exist in the image database.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "modeled images as composite of foreground and background and developed a layered generative model [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Unlike existing Conditional Generative Adversarial Networks (CGAN) [12], in which the discriminative model need to estimate the joint distribution of condition vectors with samples and can not always provide enough information for the generator to adjust parameters, in our new model, perceptual feature regression can supervise the generator to produce textures in consistence with human visual system.", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "identified the perceptual features people used to classify the textures and also established the correlation between semantic attributes and textures [13], which showed the importance of perceptual features for understanding texture images.", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "proposed a pixel-based method for texture synthesis with non-parametric sampling [14], and Wei proposed an efficient algorithm using tree-structured vector quantization for realistic texture synthesis, which required only a sample texture as input [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "proposed a pixel-based method for texture synthesis with non-parametric sampling [14], and Wei proposed an efficient algorithm using tree-structured vector quantization for realistic texture synthesis, which required only a sample texture as input [15].", "startOffset": 248, "endOffset": 252}, {"referenceID": 15, "context": "Texture synthesis based on CNN is a new research topic [16], which has produced promising results.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "In [16], Gatys combined the conceptual framework of spatial summary statistics on feature responses with the feature space of a convolutional neural network, and the goal is to generate textures from a given source image.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Ulyanov also trained feed-forward generation networks to generate multiple samples of the same texture with arbitrary sizes [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "Goodfellow [8] proposed a generative adversarial framework that could estimate generative models via an adversarial process, in which a generative model G and a discriminative modelD were simultaneously trained.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "It has been proven that GAN can be used to generate realistic images from uniformly distributed random noise [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 11, "context": "Furthermore, GAN was extended as CGAN for conditional image generation by Mirza and Osindero [12], where both models G and D received an additional vector of information as condition.", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "CGAN has been successfully applied in digit and face image generation [18], whereas we are interested in generating textures with given perceptual attributes.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "regularity and repetitiveness [19].", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "According to [20], there are 12 prominent perceptual features for human to perceive a texture.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "Inspired by the success of the Inception-v3 model [4], which reached 3.", "startOffset": 50, "endOffset": 53}, {"referenceID": 20, "context": "Furthermore, tanh is much easier to be trained than sigmoid [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "Inspired by [22], we initialize weights of one layer of the proposed network by formulation V ar[w] = 2/n.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "In our experiments, we use the Perceptual Texture Database (PTD), in which there are 450 textures with corresponding 12D perceptual features [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "The RMSProp method was used for gradient descent [24].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Thus we used the ADAM [25] method for optimization.", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. Meanwhile, perceptual attributes, such as directionality, regularity and roughness are important factors for human observers to describe a texture. In this paper, we propose a joint deep network model that combines adversarial training and perceptual feature regression for texture generation, while only random noise and user-defined perceptual attributes are required as input. In this model, a preliminary trained convolutional neural network is essentially integrated with the adversarial framework, which can drive the generated textures to possess given perceptual attributes. An important aspect of the proposed model is that, if we change one of the input perceptual features, the corresponding appearance of the generated textures will also be changed. We design several experiments to validate the effectiveness of the proposed method. The results show that the proposed method can produce high quality texture images with desired perceptual properties.", "creator": "LaTeX with hyperref package"}}}