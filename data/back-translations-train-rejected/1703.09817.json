{"id": "1703.09817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Learning Similarity Functions for Pronunciation Variations", "abstract": "A significant source of errors in Automatic Speech Recognition (ASR) systems is due to pronunciation variations which occur in spontaneous and conversational speech. Usually ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this paper, we focus on learning a similarity function between two pronunciations. The pronunciation can be the canonical and the surface pronunciations of the same word or it can be two surface pronunciations of different words. This task generalizes problems such as lexical access (the problem of learning the mapping between words and their possible pronunciations), and defining word neighborhoods. It can also be used to dynamically increase the size of the pronunciation lexicon, or in predicting ASR errors. We propose two methods, which are based on recurrent neural networks, to learn the similarity function. The first is based on binary classification, and the second is based on learning the ranking of the pronunciations. We demonstrate the efficiency of our approach on the task of lexical access using a subset from the Switchboard conversational speech corpus. Results suggest that our method is superior to previous methods which are based on graphical Bayesian methods.", "histories": [["v1", "Tue, 28 Mar 2017 21:47:16 GMT  (208kb,D)", "https://arxiv.org/abs/1703.09817v1", null], ["v2", "Sun, 28 May 2017 19:05:38 GMT  (209kb,D)", "http://arxiv.org/abs/1703.09817v2", null], ["v3", "Sun, 18 Jun 2017 11:52:54 GMT  (209kb,D)", "http://arxiv.org/abs/1703.09817v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["einat naaman", "yossi adi", "joseph keshet"], "accepted": false, "id": "1703.09817"}, "pdf": {"name": "1703.09817.pdf", "metadata": {"source": "CRF", "title": "Learning Similarity Functions for Pronunciation Variations", "authors": ["Einat Naaman", "Yossi Adi", "Joseph Keshet"], "emails": ["jkeshet@cs.biu.ac.il"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to go in search of a solution that has its origins in the past."}, {"heading": "2. Problem settings", "text": "We denote a word pronunciation by a sequence of telephones, p = (p1,.., pN), where pn-P for all 1 \u2264 n \u2264 N and P is the set of all subunits (all telephones). Of course, N is not specified, because the number of phonemes in different words varies. We denote the set of all finite telephone streams as P. Given two pronunciation sequences, our goal is to find a similarity function between these two sequences. Formally, our goal is to learn a function f: P-P-P-P-P-P-P-R, which receives two pronunciation sequences as input and returns the similarity between the two. That is, if two pronunciations are similar p1, p2-P-P, then the function f (p1, p2) is high. Otherwise, it will be low. This type of function can be used in different tasks."}, {"heading": "3. Network architecture", "text": "In this section, we propose two network architectures to learn the similarity function. Both architectures are based on recurrent neural networks (RNNs) [10]; the first is based on Siamese RNNs designed as binary classifiers and trained to minimize the function of negative protocol losses; the second architecture is based on three identical RNNNs that are combined into a ranking architecture and optimized with a ranking loss function; and we have also tried several sequence-to-sequence architectures [11, 12], but as they all produced poor results in terms of Word Error Rate (WHO), we will not discuss them in the paper."}, {"heading": "3.1. Binary loss function", "text": "The first architecture is a network designed to learn a mapping between two pronunciations using a binary classifier trained to predict whether two pronunciations are of the same word. Each example in the educational set ofm examples, S = {(psi, pci, yi)} mi = 1, consists of a surface pronunciation ps \u00b2 P \u00b2, a canonical pronunciation PC \u00b2 P *, and a binary designation y \u00b2 {\u2212 1, + 1} indicating whether both pronunciations have the same word or the same note. We want to train a neural network to learn this binary mapping. To do this, we code both pronunciations using two RNNs with a common set of parameters. The input to each of the RNNs is a sequence of phones representing either surface or canon pronunciations, and the output is a real vector."}, {"heading": "3.2. Ranking loss function", "text": "Another approach to learning the similarity function is to evaluate the similarity between a given pronunciation and other pronunciations according to their order of precedence. To do this, we use a slightly different pronunciation than the one used to train the binary network. u Each example in the new pronunciation consists of a triplet: a surface pronunciation associated with the surface pronunciation, and a positive pronunciation associated with the surface pronunciation, and the negative pronunciation associated with the surface pronunciation, and the negative canonical pronunciation, which is a canonical pronunciation of another word. Overall, the pronunciation of m examples is referred to as S (psi, p \u2212 i), p \u2212 i = 1. As in the previous model, we represent each of the three pronunciations using an RNN."}, {"heading": "4. Experiments", "text": "We evaluated the proposed architectures on the lexical access task, where we would like to predict the word in the dictionary associated with a particular interface pronunciation. All experiments are performed on a subset of the conversation corpus switchboard labeled on a fine phonetic level [1]; these phonetic transcriptions are the input to our similarity models. The data subsets, phone set P, and dictionary V are the same as those previously used in [9, 4, 5]. The dictionary contains 5,117 words consisting of the most common words in the switchboard. The base form uses a similar, slightly smaller phone group (e.g. nasalization). We used the same partition of the corpus as in [9, 4, 5] in 2,942 words in education, 165 words in development group, and 236 words in testset.Results are presented as the top word rate when presented as the three-dimensional."}, {"heading": "5. Analysis", "text": "In this section, we analyze the performance of the ranking model. We examine the effect of the number of negative examples, the effect of the embed size and present some of the results of the model and the way it is flawed."}, {"heading": "5.1. The effect of the number of negative samples", "text": "Remember that the ranking loss model was trained to triplet, which consists of a canonical pronunciation of a word, a surface pronunciation of the word (positive sample), and a surface pronunciation that is not associated with the word (negative sample). In our experiments, negative surface pronunciation was a surface pronunciation of a random word. Deep neural networks require a lot of training data to converge to a good local minimum. We expanded the training set by using many different negative samples for each positive sample. To investigate whether this approach leads to better performance, we trained the network several times with a different number of negative samples per positive sample and evaluated the performance on the test set. Figure 3 shows WHO @ 1 and WHO @ 2 of the lexical access task as a function of the number of negative samples per positive sample. Note that the error rate remains roughly the same with additional examples added to the limit of 50 samples."}, {"heading": "5.2. The effect of the embedding size", "text": "Next, we investigated the effect of embedding size n into the pronunciation, i.e. the size of the output of the last layer after the RNN. We tried different embedding sizes and evaluated their performance on the validation and test sets. Table 2 summarizes the results. It is clear from the table that embedding size 40 is too small, but the overall performance of embedding size 80 and higher is good. We found that embedding size 120 yields the best performance."}, {"heading": "5.3. Visualization", "text": "Figure 4 illustrates a subset of the embedding space of the canonical pronunciations in the dictionary, using t-SNE [17] for dimensional reduction. Words that have a similar pronunciation appear to be close to each other in the embedding space. In Table 3, we present the four most similar words according to the learned similarity function for the words sense, writing, dice, male, and the like. Again, we can see that the most similar words in the embedding space are the words that contain a similar telephone sequence. In Table 4, we illustrate the predictions of the model for the lexical access task for hard cases of surface pronunciations. The table shows the first three predictions of the model, ordered (from left to right) according to the most similar to least similar words. The table is divided into two fields: The upper field shows the correct predictions made by the model, and the lower field shows the wrong predictions of the model, ordered (from left to right) according to the most similar to the least similar words. The table is divided into two fields: The upper field shows the correct predictions made by the model for hard cases of surface pronunciations, and the lower field shows the model, ordered (from left to right) according to the most similar words, the most similar to the least similar words."}, {"heading": "6. Conclusion", "text": "We presented two very different architectures to learn similarities between two telephone streams: the first architecture learns the alignment between telephone streams representing narrow pronunciations; the second architecture is designed to learn a mapping of a telephone stream onto a vector space so that narrow pronunciations have a narrow representation in the output vector space; and future work will examine the similarity between other subordinate units. In particular, we would like to propose a similarity function between articulation features and analyze them in the light of articulatory phonology [18]."}, {"heading": "7. Acknowledgment", "text": "We would like to thank Karen Livescu, Preethi Jyothi and the anonymous reviewers for their helpful comments."}, {"heading": "8. References", "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves."}], "references": [{"title": "Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus", "author": ["S. Greenberg", "J. Hollenback", "D. Ellis"], "venue": "Proceeding of the 4th International Conference on Spoken Language Processing (ICSLP), 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Feature-based pronunciation modeling for automatic speech recognition", "author": ["K. Livescu"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Lexical access to large vocabularies for speech recognition", "author": ["L. Fissore", "P. Laface", "G. Micca", "R. Pieraccini"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37, no. 8, pp. 1197\u20131213, 1989.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Lexical access experiments with context-dependent articulatory feature-based models", "author": ["P. Jyothi", "K. Livescu", "E. Fosler-Lussier"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminative pronunciation modeling: A large-margin, feature-rich approach", "author": ["H. Tang", "J. Keshet", "K. Livescu"], "venue": "The 50th Annual Meeting of the Association of Computational Linguistics (ACL), 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Revisiting word neighborhoods for speech recognition", "author": ["P. Jyothi", "K. Livescu"], "venue": "ACL MORPHFSM Workshop, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "What kind of pronunciation variation is hard for triphones to model?", "author": ["D. Jurafsky", "W. Ward", "Z. Jianping", "K. Herold", "Y. Xiuyang", "Z. Sen"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Pronunciation change in conversational speech and its implications for automatic speech recognition", "author": ["M. Sara\u00e7lar", "S. Khudanpur"], "venue": "Computer Speech and Language, vol. 18, no. 4, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Feature-based pronunciation modeling with trainable asynchrony probabilities.", "author": ["K. Livescu", "J.R. Glass"], "venue": "Proceeding of the 8th International Conference on Spoken Language Processing (ICSLP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1990}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["K. Yao", "G. Zweig"], "venue": "arXiv preprint arXiv:1506.00196, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative keyword spotting", "author": ["J. Keshet", "D. Grangier", "S. Bengio"], "venue": "Speech Communication, vol. 51, no. 4, pp. 317\u2013329, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, no. Jul, pp. 2121\u20132159, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML), 2010, pp. 807\u2013814.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u2013 2605, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Articulatory phonology: An overview", "author": ["C.P. Browman", "L. Goldstein"], "venue": "Phonetica, vol. 49, no. 3-4, pp. 155\u2013180, 1992.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Fewer than half of the word productions are pronounced canonically in the phonetically transcribed portion of Switchboard [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "In the task of lexical access, the goal is to predict which word in the dictionary was uttered given its pronunciation in terms of sub-word units [3, 4, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 3, "context": "In the task of lexical access, the goal is to predict which word in the dictionary was uttered given its pronunciation in terms of sub-word units [3, 4, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 4, "context": "In the task of lexical access, the goal is to predict which word in the dictionary was uttered given its pronunciation in terms of sub-word units [3, 4, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 5, "context": "In the word neighborhood task the goal is to find the acoustic neighborhood density of a given word [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "Here we propose a different definition which is closer to the definition suggested in [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "The variance of pronunciations in spontaneous and conversational speech leads to a high rate of errors in ASR systems [7, 8].", "startOffset": 118, "endOffset": 124}, {"referenceID": 7, "context": "The variance of pronunciations in spontaneous and conversational speech leads to a high rate of errors in ASR systems [7, 8].", "startOffset": 118, "endOffset": 124}, {"referenceID": 8, "context": "Mapping between words and their possible pronunciations in terms of sub-word units was explored in light of the lexical access task [9, 4, 5].", "startOffset": 132, "endOffset": 141}, {"referenceID": 3, "context": "Mapping between words and their possible pronunciations in terms of sub-word units was explored in light of the lexical access task [9, 4, 5].", "startOffset": 132, "endOffset": 141}, {"referenceID": 4, "context": "Mapping between words and their possible pronunciations in terms of sub-word units was explored in light of the lexical access task [9, 4, 5].", "startOffset": 132, "endOffset": 141}, {"referenceID": 5, "context": "Learning word neighborhood automatically was proposed by [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "For example, in the lexical access task [5] the goal is to predict which word w from a finite dictionary V is associated with a given surface pronunciation p.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "In the word neighborhood task the goal is to find the acoustic neighborhood density of a given word [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Both architectures are based on Recurrent Neural Networks (RNNs) [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "We also tried several sequence-to-sequence architectures [11, 12], but since all of them led to poor results in terms of Word Error Rate (WER), we will not discuss them in the paper.", "startOffset": 57, "endOffset": 65}, {"referenceID": 11, "context": "We also tried several sequence-to-sequence architectures [11, 12], but since all of them led to poor results in terms of Word Error Rate (WER), we will not discuss them in the paper.", "startOffset": 57, "endOffset": 65}, {"referenceID": 12, "context": "score [13]:", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Formally, the loss function is defined as [14, 13]:", "startOffset": 42, "endOffset": 50}, {"referenceID": 12, "context": "Formally, the loss function is defined as [14, 13]:", "startOffset": 42, "endOffset": 50}, {"referenceID": 0, "context": "All experiments are conducted on a subset of Switchboard conversational speech corpus that has been labeled at a fine phonetic level [1]; these phonetic transcriptions are the input to our similarity models.", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "The data subsets, phone set P , and dictionary V are the same as those previously used in [9, 4, 5].", "startOffset": 90, "endOffset": 99}, {"referenceID": 3, "context": "The data subsets, phone set P , and dictionary V are the same as those previously used in [9, 4, 5].", "startOffset": 90, "endOffset": 99}, {"referenceID": 4, "context": "The data subsets, phone set P , and dictionary V are the same as those previously used in [9, 4, 5].", "startOffset": 90, "endOffset": 99}, {"referenceID": 8, "context": "We used the same partition of the corpus as in [9, 4, 5] into 2,942 words in the training set, 165 words in the development set, and 236 words in the test set.", "startOffset": 47, "endOffset": 56}, {"referenceID": 3, "context": "We used the same partition of the corpus as in [9, 4, 5] into 2,942 words in the training set, 165 words in the development set, and 236 words in the test set.", "startOffset": 47, "endOffset": 56}, {"referenceID": 4, "context": "We used the same partition of the corpus as in [9, 4, 5] into 2,942 words in the training set, 165 words in the development set, and 236 words in the test set.", "startOffset": 47, "endOffset": 56}, {"referenceID": 14, "context": "We optimize all our models using Adagrad [15] with learning rate value of 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "We use ReLU [16] as an activation function after each fully connected layer.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "dictionary lookup [2] 59.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": ", 2011 [4] 29.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": ", 2012 [5] 15.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "For comparison we added to Table 1 the word error rate of other algorithms for lexical access: a dictionary lookup with and without Levenshtein distance [9], a dynamic Bayesian network (Jyothi et al.", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": ", 2011 [4]), and discriminative structured prediction model (Hao et al.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": ", 2012 [5]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "It can be seen from the table that both of our models outperform the dictionary lookup approaches and the model which is based on dynamic Bayesian networks [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "However the discriminative structured prediction model [5] performs much better than any of our models.", "startOffset": 55, "endOffset": 58}, {"referenceID": 16, "context": "In Figure 4 we visualized a subset from the embedding space of the canonical pronunciations in the dictionary, using t-SNE [17] for dimensionality reduction.", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "Specifically we would like to propose a similarity function between articulatory features, and analyze it in the light of articulatory phonology [18].", "startOffset": 145, "endOffset": 149}], "year": 2017, "abstractText": "A significant source of errors in Automatic Speech Recognition (ASR) systems is due to pronunciation variations which occur in spontaneous and conversational speech. Usually ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this paper, we focus on learning a similarity function between two pronunciations. The pronunciations can be the canonical and the surface pronunciations of the same word or they can be two surface pronunciations of different words. This task generalizes problems such as lexical access (the problem of learning the mapping between words and their possible pronunciations), and defining word neighborhoods. It can also be used to dynamically increase the size of the pronunciation lexicon, or in predicting ASR errors. We propose two methods, which are based on recurrent neural networks, to learn the similarity function. The first is based on binary classification, and the second is based on learning the ranking of the pronunciations. We demonstrate the efficiency of our approach on the task of lexical access using a subset of the Switchboard conversational speech corpus. Results suggest that on this task our methods are superior to previous methods which are based on graphical Bayesian methods.", "creator": "LaTeX with hyperref package"}}}