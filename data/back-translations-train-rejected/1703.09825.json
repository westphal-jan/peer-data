{"id": "1703.09825", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Semi-Supervised Affective Meaning Lexicon Expansion Using Semantic and Distributed Word Representations", "abstract": "In this paper, we propose an extension to graph-based sentiment lexicon induction methods by incorporating distributed and semantic word representations in building the similarity graph to expand a three-dimensional sentiment lexicon. We also implemented and evaluated the label propagation using four different word representations and similarity metrics. Our comprehensive evaluation of the four approaches was performed on a single data set, demonstrating that all four methods can generate a significant number of new sentiment assignments with high accuracy. The highest correlations (tau=0.51) and the lowest error (mean absolute error &lt; 1.1%), obtained by combining both the semantic and the distributional features, outperformed the distributional-based and semantic-based label-propagation models and approached a supervised algorithm.", "histories": [["v1", "Tue, 28 Mar 2017 22:05:20 GMT  (33kb)", "http://arxiv.org/abs/1703.09825v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["areej alhothali", "jesse hoey"], "accepted": false, "id": "1703.09825"}, "pdf": {"name": "1703.09825.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["aalhotha@cs.uwaterloo.ca", "jhoey@cs.uwaterloo.ca"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.09 825v 1 [cs.C L] 28 Mar 201 7graph-based sentiment induction methods by including distributed and semantic word representations in the construction of the similarity graph to expand a three-dimensional sentiment lexicon. We also implemented and evaluated label propagation using four different word representations and similarity metrics. Our comprehensive evaluation of the four approaches was based on a single data set and showed that all four methods can generate a significant number of new sentiment mappings with high accuracy, with the highest correlations (\u03c4 = 0.51) and lowest errors (mean absolute error < 1.1%) achieved by combining both semantic and distributional characteristics outperforming the distribution-based and semantically based label propagation models and approaching a monitored algorithm."}, {"heading": "1 Introduction", "text": "In fact, it is as if most people are able to understand themselves and understand how they have behaved. (...) In fact, it is as if most people are able to know and understand themselves. (...) It is as if they were able to know themselves. (...) It is as if they were able to identify themselves. (...) It is as if they were able to identify themselves. (...) It is as if they were able to identify themselves. (...) It is as if they were able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves."}, {"heading": "2 Related Work", "text": "The lexicon augmentation methods in this study were performed using variations in word representation and similarity metrics. This section provides a brief background on the different vector space models used."}, {"heading": "2.1 Statistical language modeling", "text": "The statistical language model (or vector space model (VSM)) is a distributional estimate of various language phenomena estimated on real-world data through the use of statistical techniques. In order to capture the semantic or syntactic properties and present words as proximity in n-dimensional space, several VSMs have been proposed, ranging from simple onehotype representation, which considers words as atomic symbols of simultaneous occurrence with other words in a vocabulary, to neural word embedding, which represents words in a dense and more compact space. The most commonly used word representation is the distributional word embedding, which represents words based on co-occurrence neuroscience statistics with other words in a document or corpora (Harris, 1981; Firth, 1957)."}, {"heading": "2.2 Acquisition of Sentiment Lexicon", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc lrf\u00fc lrf\u00fc lrf\u00fc nlf\u00fc eeirf\u00fc lf\u00fc nlf\u00fc eeirf\u00fc rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "3 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Graph-based Label-Propagation", "text": "To evaluate the effectiveness of graph-based approaches to expanding multidimensional sentiment lexicographies, we use the label propagation algorithm in this paper (Zhu and Ghahramani, 2002; Zhou et al., 2004), combined with four methods for calculating word vectors and word similarities. The label propagation algorithms are based on the idea of building a similarity diagram with labeled words / paradigm words (words) and unlabeled word nodes (words), and then propagating the labels or scores of the known nodes (words) through the graph to the unlabeled nodes."}, {"heading": "3.1.1 Semantic Lexicon-based Label Propagation (SLLP)", "text": "The SLLP algorithm follows the general principle of the graph-based label propagation approach as described in the previous section, but the affinity matrixW is calculated based on the semantic characteristics derived from semantic lexicon. In this algorithm, two semantic lexicon were used: WordNet dictionary (WN) (Miller, 1995) and the paraphrase database (PPDB) (Ganitkevitch et al., 2013). The SLLP algorithm constructs the vocabulary V from the words of the dictionaries and calculates and normalizes the weight matrix W using the synonym relationship between words. The semantic similarity wi, j of each word pair xi and xj in the vocabulary V is as follows: wi, j = {1.0, if xj is a synonym of xi 0.0 otherwise (1)."}, {"heading": "3.1.2 Corpus-based Label Propagation (CLP)", "text": "Corpus-based label propagation (CLP) is one of the most commonly used methods for sentiment lexicon generation, which uses the co-occurence statistics aggregated from various corpora (news articles, Twitter, etc.) to create the similarity graph in the label propagation algorithms. We used an n-gram features from the signal media (SM) one million news articles dataset which contains \u0445 265K blog articles and \u0445 734K news articles (Corney et al., 2016) and the North American News (NAN) text corpus (Graff, 1995) which has \u0445 931K articles from a variety of news sources.The co-occurrence matrix R was compuit on a window size of four words. Bigrams with stop words, words less than three letters, proper nouns, non-alpha words, and the bigrams that do volox."}, {"heading": "3.1.3 Neural Word Embeddings", "text": "This method uses word embedding (word vectors) that capture syntactic and semantic properties. We use two pre-trained word embedding models that are trained on coexistence statistics. We used word vectors with skip-over graphs (SG) (Mikolov et al., 2013) that were trained on a Skip-gram model for coexistence statistics aggregated from the Google News dataset and the Global Vector for Word Representation (GloVe) (Pennington et al., 2014) that were aggregated on coexistence statistics. Vocabulary V in this algorithm is all words in the word embedding set (we filtered non-alpha words and words containing digits), and the affinity matrix W is calculated using cosmic similarity (equation 5) between word vectors (each vi V is a 300-dimensional vector)."}, {"heading": "3.1.4 Semantic and Neural Word Embeddings Label-propagation (SNWELP)", "text": "To improve the results of the NWELP algorithm, we propose the SNWELP algorithm, a model that combines both semantic and distributional information from neural word embedding models and a semantic lexicon (a dictionary).The SNWELP algorithm constructs the affinity matrix W using the characteristics of neural word embedding (SG or GloVe) and semantic characteristics derived from a semantic lexicon (WN or PDB).In this case, V is the intersection between the words in the lexicon and the word in the filtered embedding set, W the mean cosine similarity value (Eq.5) of the neural and semantic word representations (Eq.1)."}, {"heading": "3.2 Sampling Methods", "text": "The selection of selected words (also called paradigm or seed terms) in the graph-based propagation methods is one of the decisive factors. We used two methods: 1) fixed seed sentences (fixed paradigms) and 2) words from the vocabulary V used in the label propagation algorithm (vocabulary paradigms).The set of fixed paradigms was selected from research by Osgood et al (Osgood, 1957) as shown in Table 1, while the set of vocabulary paradigms was randomly selected from the vocabulary of the corpus for words with the highest and lowest EPA values (words with E, P or A \u2264 \u2212 2.5 or \u2265 2.5).The aim is to use words in extreme cases of each dimension E, P and A as paradigm words in order to spread these highly influencing EPA words throughout the diagram. The seed terms carry no more than 1% of all words in each algorithm."}, {"heading": "3.3 Evaluation Metrics", "text": "To evaluate the effectiveness of the algorithm in generating a multi-dimensional sentiment lexicon, we used the most recent manually commented affective dictionary (Warriner et al., 2013) as a starting point. We used the (Warriner et al., 2013) affective dictionary in the lexicon induction process by selecting the paradigm words from it at random and comparing the generated lexicon against it. We randomly divided the (Warriner et al., 2013) affective dictionary (original EPO) into EPO training (third of the set equals 5566 words) and EPO tests (two thirds of the set equals 8349 words). Seed words for all algorithms are only sampled from the EPO training set, and all results are tested in the EPO testset.The EPA values of (Warriner et al., 2013) are negative."}, {"heading": "3.4 Baseline and State-of-the-art Comparison", "text": "We compared our induced results with some of the standard state-of-the-art embedding algorithms (evaluation results).We implemented the PMI-IR algorithm proposed by (Turney et al., 2003) and evaluated the sensation orientation (either positive or negative) of a word by measuring the difference between the strength of word associations with positive paradigm words and negative paradigm words using statistics aggregated from search engine results on occurrence. We also compared our results with the reported results of orthogonal transformation of word vectors (Rothe et al., 2016) and a label propagation algorithm trained on (a domain-specific) SVDword vector model (Hamilton et al., 2016).We also experimented with the retrofitted word vector model that improves neural word vector training (neural word training gained from neural sectoral use, the incorporation of the word sectors)."}, {"heading": "4 Results", "text": "In this section, we present the results of comparing the induced EPA results using the label dispersion algorithms either from the DB or from the generated properties 4F1 results in the EPA test method. As shown in Table 2, the use of SVD word embedding in the CLP algorithm shows the lowest ranking correlation between EPA values and the highest error rate (MAE) compared to the other label dispersion methods. The results of comparing the induced EPA results against their true values in the test group show that the MAE values ranged between 0.99 and 1.3 and the ranking of the correlation 2 less than 0.2 using cosmic similarity and hard brackets (\u03b1 = 1.0) assumptions. We also experimented with the unsmoothed points of the mutual information (PPMI), but there was no significant difference between the smooth and the unsmoothed PMI. We also tried other dimensions of the SVD word vector = 300, but there was no significant difference between them."}, {"heading": "5 Discussion", "text": "Sentiment analysis is a feature engineering problem in which sentiment lexicons play an important role in improving model accuracy. One of the challenges of sentiment analysis is the increasing number of new words and terms on social media or in news resources (e.g., homosexuality, abortion) that are not associated with a sentiment, and the number of recognized terms in individual countries is too high (e.g. to explore other dimensions of human feelings)."}, {"heading": "6 Conclusion", "text": "In this study, we propose an extension of graph-based lexicon induction algorithms to expand sentiment lexicon and explore other dimensions of emotion. To the best of our knowledge, this study is the first to expand a multi-dimensional sentiment lexicon and the first to include both semantic and neural word representation in the label propagation algorithm. We also conducted a comprehensive evaluation of label propagation algorithms using a variety of word representations that provide greater accuracy in many NLP tasks compared to other standard methods. Results show that semantic label embedding produces the highest correlations compared to corpus-based, semantic lexicon-based and neural embedding algorithms."}], "references": [{"title": "Good news or bad news: Using affect control theory to analyze readers reaction towards news articles", "author": ["Areej Alhothali", "Jesse Hoey."], "venue": "Proc. Conference of the North American Chapter of the Association for Computational Linguistics - Human", "citeRegEx": "Alhothali and Hoey.,? 2015", "shortCiteRegEx": "Alhothali and Hoey.", "year": 2015}, {"title": "Semantic tag extraction from wordnet glosses", "author": ["Alina Andreevskaia", "Sabine Bergler."], "venue": "Proceedings of 5th International Conference on Language Resources and Evaluation (LREC06). Citeseer.", "citeRegEx": "Andreevskaia and Bergler.,? 2006", "shortCiteRegEx": "Andreevskaia and Bergler.", "year": 2006}, {"title": "Inesc-id: A regression model for large scale twitter sentiment lexicon induction", "author": ["Ramon F Astudillo", "Silvio Amir", "Wang Ling", "Bruno Martins", "M\u00e1rio Silva", "Isabel Trancoso", "Rua Alves Redol."], "venue": "SemEval-2015 page 613.", "citeRegEx": "Astudillo et al\\.,? 2015", "shortCiteRegEx": "Astudillo et al\\.", "year": 2015}, {"title": "Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani"], "venue": "In LREC", "citeRegEx": "Baccianella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baccianella et al\\.", "year": 2010}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, Springer, pages 137\u2013186.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "New Directions in Contemporary Sociological Theories", "author": ["Joseph Berger", "Morris Zelditch."], "venue": "Rowman & Littlefield.", "citeRegEx": "Berger and Zelditch.,? 2002", "shortCiteRegEx": "Berger and Zelditch.", "year": 2002}, {"title": "Retrieval time from semantic memory", "author": ["Allan M Collins", "M Ross Quillian."], "venue": "Journal of verbal learning and verbal behavior 8(2):240\u2013247.", "citeRegEx": "Collins and Quillian.,? 1969", "shortCiteRegEx": "Collins and Quillian.", "year": 1969}, {"title": "What do a million news articles look like", "author": ["David Corney", "Dyaa Albakour", "Miguel Martinez", "Samir Moussa"], "venue": "In Proceedings of the First International Workshop on Recent Trends in News Information Retrieval", "citeRegEx": "Corney et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Corney et al\\.", "year": 2016}, {"title": "The approximation of one matrix by another of lower rank", "author": ["Carl Eckart", "Gale Young."], "venue": "Psychometrika 1(3):211\u2013218.", "citeRegEx": "Eckart and Young.,? 1936", "shortCiteRegEx": "Eckart and Young.", "year": 1936}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "Proceedings of LREC. volume 6, pages 417\u2013422.", "citeRegEx": "Esuli and Sebastiani.,? 2006", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2006}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "arXiv preprint arXiv:1411.4166 .", "citeRegEx": "Faruqui et al\\.,? 2014", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "A synopsis of linguistic theory, 1930-1955", "author": ["John Rupert Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "The world of emotions is not two-dimensional", "author": ["Johnny RJ Fontaine", "Klaus R Scherer", "Etienne B Roesch", "Phoebe C Ellsworth."], "venue": "Psychological science 18(12):1050\u20131057.", "citeRegEx": "Fontaine et al\\.,? 2007", "shortCiteRegEx": "Fontaine et al\\.", "year": 2007}, {"title": "Ppdb: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "HLT-NAACL. pages 758\u2013764.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Large-scale sentiment analysis for news and blogs", "author": ["Namrata Godbole", "Manja Srinivasaiah", "Steven Skiena."], "venue": "ICWSM 7.", "citeRegEx": "Godbole et al\\.,? 2007", "shortCiteRegEx": "Godbole et al\\.", "year": 2007}, {"title": "Inducing domain-specific sentiment lexicons from unlabeled corpora", "author": ["William L Hamilton", "Kevin Clark", "Jure Leskovec", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1606.02820 .", "citeRegEx": "Hamilton et al\\.,? 2016", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Springer.", "citeRegEx": "Harris.,? 1981", "shortCiteRegEx": "Harris.", "year": 1981}, {"title": "Expressive order: Confirming sentiments in social actions", "author": ["David R Heise."], "venue": "Springer.", "citeRegEx": "Heise.,? 2007", "shortCiteRegEx": "Heise.", "year": 2007}, {"title": "Surveying Cultures: Discovering Shared Conceptions and Sentiments", "author": ["David R. Heise."], "venue": "Wiley.", "citeRegEx": "Heise.,? 2010", "shortCiteRegEx": "Heise.", "year": 2010}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 168\u2013 177.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Principal component analysis", "author": ["Ian Jolliffe."], "venue": "Wiley Online Library.", "citeRegEx": "Jolliffe.,? 2002", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "Using wordnet to measure semantic orientation of adjectives", "author": ["Jaap KAMPS."], "venue": "Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004). pages 1115\u2013 1118.", "citeRegEx": "KAMPS.,? 2004", "shortCiteRegEx": "KAMPS.", "year": 2004}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais."], "venue": "Psychological review 104(2):211.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Mean affective ratings of 2, 294 concepts by guelph university undergraduates, ontario, canada", "author": ["Neil J. MacKinnon."], "venue": "2001-3 [Computer file].", "citeRegEx": "MacKinnon.,? 2006", "shortCiteRegEx": "MacKinnon.", "year": 2006}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH. pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "The measurement", "author": ["Charles Egerton Osgood"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1957}, {"title": "The general inquirer: A", "author": ["DM Ogilvie"], "venue": null, "citeRegEx": "Ogilvie.,? \\Q1968\\E", "shortCiteRegEx": "Ogilvie.", "year": 1968}, {"title": "Verbs semantics and lexical selection", "author": ["ZhibiaoWu andMartha Palmer."], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 133\u2013138.", "citeRegEx": "Palmer.,? 1994", "shortCiteRegEx": "Palmer.", "year": 1994}, {"title": "Learning with local and global consistency", "author": ["Dengyong Zhou", "Olivier Bousquet", "Thomas Navin Lal", "Jason Weston", "Bernhard Sch\u00f6lkopf."], "venue": "Advances in neural information processing systems 16(16):321\u2013328.", "citeRegEx": "Zhou et al\\.,? 2004", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Xiaojin Zhu", "Zoubin Ghahramani."], "venue": "Technical report, Technical Report CMUCALD-02-107, Carnegie Mellon University.", "citeRegEx": "Zhu and Ghahramani.,? 2002", "shortCiteRegEx": "Zhu and Ghahramani.", "year": 2002}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty"], "venue": "In ICML", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": ", 1968) or real-valued onedimensional scores (Baccianella et al., 2010) to the words.", "startOffset": 45, "endOffset": 71}, {"referenceID": 12, "context": "It is well known; however, that one dimension is insufficient to adequately characterise the complexity of emotion (Fontaine et al., 2007).", "startOffset": 115, "endOffset": 138}, {"referenceID": 18, "context": "sentiment lexicons have been manually labeled using surveys in different countries (Heise, 2010).", "startOffset": 83, "endOffset": 96}, {"referenceID": 5, "context": "3 (infinitely good, powerful, or lively) (Berger and Zelditch, 2002; Heise, 2007).", "startOffset": 41, "endOffset": 81}, {"referenceID": 17, "context": "3 (infinitely good, powerful, or lively) (Berger and Zelditch, 2002; Heise, 2007).", "startOffset": 41, "endOffset": 81}, {"referenceID": 18, "context": "combinations of identities, behaviours, adjectives and institutions) (Heise, 2010) on 5item scales ranging from \u201dInfinitely negative (e.", "startOffset": 69, "endOffset": 82}, {"referenceID": 16, "context": "The most commonly used word representation is the distributional word embeddings representing word based on the co-occurrence statistics with other words in a document or corpora (Harris, 1981; Firth, 1957).", "startOffset": 179, "endOffset": 206}, {"referenceID": 11, "context": "The most commonly used word representation is the distributional word embeddings representing word based on the co-occurrence statistics with other words in a document or corpora (Harris, 1981; Firth, 1957).", "startOffset": 179, "endOffset": 206}, {"referenceID": 8, "context": "The dimensionality of this sparse representation can be reduced using Singular value decomposition (Eckart and Young, 1936), Latent Semantic Analysis (Landauer and Dumais, 1997) or Principal Component Analysis (Jolliffe, 2002).", "startOffset": 99, "endOffset": 123}, {"referenceID": 22, "context": "The dimensionality of this sparse representation can be reduced using Singular value decomposition (Eckart and Young, 1936), Latent Semantic Analysis (Landauer and Dumais, 1997) or Principal Component Analysis (Jolliffe, 2002).", "startOffset": 150, "endOffset": 177}, {"referenceID": 20, "context": "The dimensionality of this sparse representation can be reduced using Singular value decomposition (Eckart and Young, 1936), Latent Semantic Analysis (Landauer and Dumais, 1997) or Principal Component Analysis (Jolliffe, 2002).", "startOffset": 210, "endOffset": 226}, {"referenceID": 4, "context": "Similar to distributional word embeddings, neural word embeddings are usually based upon co-occurrence statistics, but they are more compact, less sensitive to data sparsity, and able to represent an exponential number of word clusters (Bengio et al., 2006) (Mikolov et al.", "startOffset": 236, "endOffset": 257}, {"referenceID": 6, "context": "Thesaurus-based methods use the lexical relationship such as the depth of a concept in taxonomy tree (Wu and Palmer, 1994) or edge counting (Collins and Quillian, 1969) to build sentiment lexicons.", "startOffset": 140, "endOffset": 168}, {"referenceID": 21, "context": "Similar to Turney\u2019s PMI approach (KAMPS, 2004) they use WordNet based relatedness metric between words and given seed words.", "startOffset": 33, "endOffset": 46}, {"referenceID": 9, "context": ", word-net synonym, antonym) (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006).", "startOffset": 29, "endOffset": 90}, {"referenceID": 14, "context": "Some researchers have developed a weighted label propagation algorithm that propagates a continuous sentiment score from seed words to lexically related words (Godbole et al., 2007).", "startOffset": 159, "endOffset": 181}, {"referenceID": 2, "context": "gram 600 word embedding to create a Twitterbased sentiment lexicon (Astudillo et al., 2015).", "startOffset": 67, "endOffset": 91}, {"referenceID": 15, "context": "A recent study has also proposed a label propagation based model that uses word embedding, built using singular value decomposition (SVD) and PMI, to induce a domain-specific sentiment lexicon (Hamilton et al., 2016).", "startOffset": 193, "endOffset": 216}, {"referenceID": 21, "context": "(KAMPS, 2004) use a WordNet-based metric to elicit semantic orientation of adjectives.", "startOffset": 0, "endOffset": 13}, {"referenceID": 0, "context": "A three-dimensional sentiment lexicon was extended using a thesaurus-based label propagation algorithm based upon WordNet similarity (Alhothali and Hoey, 2015), and their results were compared against the Ontario dataset (MacKinnon, 2006).", "startOffset": 133, "endOffset": 159}, {"referenceID": 24, "context": "A three-dimensional sentiment lexicon was extended using a thesaurus-based label propagation algorithm based upon WordNet similarity (Alhothali and Hoey, 2015), and their results were compared against the Ontario dataset (MacKinnon, 2006).", "startOffset": 221, "endOffset": 238}, {"referenceID": 19, "context": "Expanding sentiment lexicons using graph-based propagation algorithms was pursued previously and found to give higher accuracy in comparison with other standard methods (Hu and Liu, 2004; Andreevskaia and Bergler, 2006; Rao and Ravichandran, 2009).", "startOffset": 169, "endOffset": 247}, {"referenceID": 1, "context": "Expanding sentiment lexicons using graph-based propagation algorithms was pursued previously and found to give higher accuracy in comparison with other standard methods (Hu and Liu, 2004; Andreevskaia and Bergler, 2006; Rao and Ravichandran, 2009).", "startOffset": 169, "endOffset": 247}, {"referenceID": 31, "context": "To evaluate the effectiveness of graph-based approaches in expanding multidimensional sentiment lexicons, in this paper, we use the label propagation algorithm (Zhu and Ghahramani, 2002; Zhou et al., 2004), combined with four methods for computing words vectors and word similarities.", "startOffset": 160, "endOffset": 205}, {"referenceID": 30, "context": "To evaluate the effectiveness of graph-based approaches in expanding multidimensional sentiment lexicons, in this paper, we use the label propagation algorithm (Zhu and Ghahramani, 2002; Zhou et al., 2004), combined with four methods for computing words vectors and word similarities.", "startOffset": 160, "endOffset": 205}, {"referenceID": 13, "context": "Two semantic lexicons were used in this algorithm: WordNet dictionary (WN) (Miller, 1995) and the paraphrase database (PPDB) (Ganitkevitch et al., 2013).", "startOffset": 125, "endOffset": 152}, {"referenceID": 7, "context": "one million news articles dataset which contains \u223c 265K blog articles and \u223c 734K news articles (Corney et al., 2016) and the North American News (NAN) text corpus (Graff, 1995) which has \u223c931K articles from a variety of news sources.", "startOffset": 95, "endOffset": 116}, {"referenceID": 23, "context": "We constructed the word vectors by computing the smoothed positive point-wise mutual information(SPPMI) (Levy et al., 2015) of the co-occurrence matrix R.", "startOffset": 104, "endOffset": 123}, {"referenceID": 23, "context": "This smoothing technique reduces the PMI\u2019s bias towards rare words and found to improve the performance of NLP tasks (Levy et al., 2015).", "startOffset": 117, "endOffset": 136}, {"referenceID": 23, "context": "75 as it is found to give better results (Levy et al., 2015) (Mikolov et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 8, "context": "The SPPMI matrix is then factorized with truncated Singular Value Decomposition (SVD) (Eckart and Young, 1936) as follows:", "startOffset": 86, "endOffset": 110}, {"referenceID": 18, "context": "3] to follow the same EPA scale used in the other lexicons we have considered (Heise, 2010).", "startOffset": 78, "endOffset": 91}, {"referenceID": 15, "context": "We used F1binary to evaluate the binary classification performance of the model (positive \u2265 0 and negative < 0 ) and similar to most recently proposed studies in the field (Hamilton et al., 2016), we computed F1-ternary to measure the ternary classification accuracy: positive \u2208 (1, 4.", "startOffset": 172, "endOffset": 195}, {"referenceID": 32, "context": "To calculate the F1-ternary, we used the class-mass normalization (CMN) methods (Zhu et al., 2003) that rescale the predicted label (\u0177i,l) for a point xi by", "startOffset": 80, "endOffset": 98}, {"referenceID": 15, "context": ", 2016)\u2019s orthogonal transformation of word vectors, and a label spreading algorithm trained on ( a domainspecific) SVDword vector model (Hamilton et al., 2016).", "startOffset": 137, "endOffset": 160}, {"referenceID": 10, "context": "We also experimented with the retrofitted word vector model that improves the neural word embedding vectors using semantic features obtained from the lexical resources (WN, PPDB) (Faruqui et al., 2014).", "startOffset": 179, "endOffset": 201}, {"referenceID": 10, "context": "To make a fair comparison, we implemented our label propagation algorithm and the retrofitted word vector approach (Faruqui et al., 2014) to recreate the General Inquirer lexicon (Stone et al.", "startOffset": 115, "endOffset": 137}, {"referenceID": 15, "context": ", 2013) lexicon to compare our results to (Hamilton et al., 2016) and (Rothe et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 15, "context": "by (Hamilton et al., 2016) and other researchers in the field.", "startOffset": 3, "endOffset": 26}, {"referenceID": 15, "context": "94 (Hamilton et al., 2016) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 10, "context": "91 (Faruqui et al., 2014) 0.", "startOffset": 3, "endOffset": 25}, {"referenceID": 15, "context": ", 2016) and (Hamilton et al., 2016) approaches.", "startOffset": 12, "endOffset": 35}, {"referenceID": 10, "context": "results are of the improved skip-gram model (SG) using semantic features obtained from wordnet (WN)) (Faruqui et al., 2014).", "startOffset": 101, "endOffset": 123}, {"referenceID": 15, "context": "Only one study have experimented with neural word embedding label propagation to expand the one-dimensional sentiment lexicon (Hamilton et al., 2016) with only reporting the result of using SVD word embedding model.", "startOffset": 126, "endOffset": 149}], "year": 2017, "abstractText": "In this paper, we propose an extension to graph-based sentiment lexicon induction methods by incorporating distributed and semantic word representations in building the similarity graph to expand a threedimensional sentiment lexicon. We also implemented and evaluated the label propagation using four different word representations and similarity metrics. Our comprehensive evaluation of the four approaches was performed on a single data set, demonstrating that all four methods can generate a significant number of new sentiment assignments with high accuracy. The highest correlations (\u03c4 = 0.51) and the lowest error (mean absolute error < 1.1%), obtained by combining both the semantic and the distributional features, outperformed the distributional-based and semantic-based label-propagation models and approached a supervised algorithm.", "creator": "LaTeX with hyperref package"}}}