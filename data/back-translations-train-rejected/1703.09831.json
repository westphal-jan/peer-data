{"id": "1703.09831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment", "abstract": "We tackle a task where an agent learns to navigate in a 2D maze-like environment called XWORLD. In each session, the agent perceives a sequence of raw-pixel frames, a natural language command issued by a teacher, and a set of rewards. The agent learns the teacher's language from scratch in a grounded and compositional manner, such that after training it is able to correctly execute zero-shot commands: 1) the combination of words in the command never appeared before, and/or 2) the command contains new object concepts that are learned from another task but never learned from navigation. Our deep framework for the agent is trained end to end: it learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and the action module that outputs actions. The zero-shot learning capability of our framework results from its compositionality and modularity with parameter tying. We visualize the intermediate outputs of the framework, demonstrating that the agent truly understands how to solve the problem. We believe that our results provide some preliminary insights on how to train an agent with similar abilities in a 3D environment.", "histories": [["v1", "Tue, 28 Mar 2017 22:29:53 GMT  (1597kb,D)", "http://arxiv.org/abs/1703.09831v1", null], ["v2", "Thu, 13 Apr 2017 20:28:59 GMT  (1592kb,D)", "http://arxiv.org/abs/1703.09831v2", null], ["v3", "Fri, 19 May 2017 23:33:28 GMT  (1888kb,D)", "http://arxiv.org/abs/1703.09831v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["haonan yu", "haichao zhang", "wei xu"], "accepted": false, "id": "1703.09831"}, "pdf": {"name": "1703.09831.pdf", "metadata": {"source": "CRF", "title": "A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment", "authors": ["Haonan Yu", "Haichao Zhang"], "emails": ["haonanyu@baidu.com", "zhanghaichao@baidu.com", "xuwei06@baidu.com"], "sections": [{"heading": null, "text": "The development of a complex language system is a very crucial part of achieving human intelligence for a machine. The semantics of language, if it is based on perceptual experience, can encode knowledge about the perception of the world. This knowledge is transferred from task to task, which empowers the machine with its generalization ability. It is argued that a machine must go through physical experience in order to learn the semantics of the human level [Kiela et al., 2016], i.e., a process of human language acquisition does not have a reasonably fast learning rate to make this happen. Thus, we opt for a model of this problem in a virtual environment, as a first step towards the formation of a physical intelligent machine.Human generalize well if we learn new concepts and skills through natural language instructions. We are able to apply existing skills to newly acquired concepts without Xiv: 170 3.09 831v 1 [cs.C] 28 Mar little difficulty."}, {"heading": "2 Related Work", "text": "In fact, it is the case that we are able to go in search of a solution that meets the needs of the individual. (...) In fact, it is the case that we are able to find a solution that meets the needs of society. (...) In fact, it is the case that we are able to find a solution that meets the needs of the people. (...) In fact, it is the case that we are able to bring about a solution. (...) It is as if we are able to find a solution that meets the needs of the individual. (...)"}, {"heading": "3 XWORLD Environment", "text": "We first briefly describe the XWORLD environment. Further details are given in Appendix 8.3. XWORLD is a 2D grid world (Figure 1). An agent interacts with the environment through a number of time steps T, with four actions: up, down, left, and right. He does this for many sessions. At the beginning of each session, a teacher starts a timer and issues a command in natural language that asks the agent to reach a place that objects in the environment refer to. There may be objects other than distractions. So, the agent must differentiate and navigate to the right place. He perceives the entire environment through RGB pixels with an egocentric glance (Figure 2c). If the agent executes the command correctly before running out of time, he gets a positive reward R +. Whenever he hits a wall or steps on an object that is not the target, he gets a negative reward R or R \u2212 w \u2212 step \u2212 with each small reward."}, {"heading": "4 Compositional Framework for Zero-shot Navigation", "text": "In fact, it is the case that it is a matter of a way in which people are able to put themselves into the world, in which they are able to live in the world. (...) It is the time in which they are able to understand the world. (...) It is the time in which people in the world retreat into the world. (...) It is the time in which they live in the world. (...) It is the time in which they live in the world. (...) It is the time in which they live in the world. (...) It is the time in which they live in the world. (...) It is the time in which they live in the world. (...) It is the time in which they live. (...) It is the time in which they live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (... It is the time in which we live.) It is the time in which we live. (... It is the time in which we live.) It is the time in which we live. (... It is the time in which we live."}, {"heading": "5 Training", "text": "Our training goal contains two partial goals, one for navigation and the other for recognition LSL (\u03b8) = LRL (\u03b8) + LSL (\u03b8), where \u03b8 are the common parameters of the framework. Most parameters are divided between the two tasks 3. We calculate the loss of recognition LSL as multi-class entropy with the gradients LSL (\u03b8) = EQ [\u2212 \u03b8], where EQ is the expectation of all questions that the teacher asks in all training sessions, m is the correct answer to every question, and f \u0445 \u03b8 is the corresponding characteristic. We calculate the navigation loss LRL (\u03b8) as the negative expected reward \u2212 EE, which the Agent receives by following its policy directives. With the Actor-Critic (AC) algorithm [Sutton and Barto, 1998] we have the approximate gradations LRL (\u03b8) = EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EE, EQE, EE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EQE, EL, EQE, EQE, EQE, EQE."}, {"heading": "6 Experiments", "text": "We use Adagrad [Duchi et al., 2011] with a learning rate of 10 \u2212 5 for Stochastic Gradient Descent (SGD). In all experiments, we set the batch size to 16 and 200k stacks. Target parameters are updated to the number of parameters of each layer. All parameters have a default value of a total of 10 \u2212 4 x batch size. For each layer that reduces its parameters by default to 0, we have zero mean and a default deviation of 1 / 2, where N is the number of parameters of this layer. The agent has 500k exploration steps in total, and the exploration rate decreases. We fix the number of programming steps S as 3, each model with 4 random initializations. The entire framework is implemented and trained to the end with PaddlePaddlePaddle 4."}, {"heading": "7 Conclusion", "text": "Such a generalization is made possible by the reuse of knowledge learned in other tasks and encoded by language. By putting together words in different ways, the agent is able to tackle new tasks while using existing knowledge at the same time. This ability is crucial for quick learning and good generalization. We reflect these important ideas in the design of our framework and apply them to a concrete example: zero-shot navigation in XWORLD. Our framework is only one possible implementation. Some components of the framework can be improved. Our contention is not that an intelligent agent must have a mental model like the one shown, but it must have several crucial characteristics that are present in Section 1 and Section 4. Currently, the agent is researching in a 2D environment. In the future, we plan to migrate the agent to a 3D world like Malm\u00f6 [Johnson et al., 2016]. There will be several new challenges, such as visual perception and a preliminary 3D learning environment that is more similar to a geometric model."}, {"heading": "Acknowledgments", "text": "We thank Yuanpeng Li, Liang Zhao and Yi Yang for their helpful conversations."}, {"heading": "8 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Implementation Details", "text": "The agent receives a 156 x 156 RGB image at each step. This image is egocentric and includes both the environment and the black cushion region. The agent processes the input image with a CNN, which has four revolutionary layers: (3, 3, 64), (2, 2, 64), (2, 2, 512), with (x, y, z) z x x x x filters of the same size. All four layers have the ReLU activation function. The output is the visual feature card with 512 channels. We stack it along the channel dimension with another parametric feature card of the same size. This spatial feature card is initialized with zero mean and standard deviation (Figure 2).The agent also receives a navigation command at the beginning of a session. The same command repeats until the end of the session."}, {"heading": "8.2 Baseline Models", "text": "The language module of SimpleAttention sets the word embed size to 1024. The RNN is the same size as the word embedded.The FC layer that produces the 3 \u00d7 3 filter has an output size of 4608, which is 9 times the channel number of the visual characteristic card. The remaining layer configuration corresponds to our framework. VIS-LSTM has a CNN with four revolutionary layers (3, 2, 64), (3, 2, 128) and (3, 1, 128) followed by three FC layers with size 1024. The word embedding and the RNN are both 1024 in size. The RNN output passes through three hidden layers of size 512 either for detection or navigation. The layer size configuration of Multimodal is the same with VIS-LSTM. The output of all layers here is reLU enabled, except the last word layer of CNN is not embedded in the STAR."}, {"heading": "8.3 XWORLD Setup", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "8.4 Experience Replay and Curriculum Learning", "text": "We employ Experience Replay [Mnih et al., 2015] to train both navigation and recognition tasks. Environmental inputs, rewards and actions taken by the agent in the last 10k time steps are stored in a replay buffer. During the training, two minibatches are scanned each time with the same number of experiences from the buffer, one for calculating LSL (\u03b8) and the other for calculating LRL experiences. For the former, only individual experiences are scanned. We consistently stomp experiences from a subset of the buffer containing the teacher's questions. For the latter, we have to scan transitions (i.e. pairs of experiences) so that TD errors can be calculated. We stomp from the entire buffer using the rank-based sampler [Schaul et al., 2016], which increases learning efficiency by prioritizing rare experiences in the buffer."}], "references": [{"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "In ACL,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Robot language learning, generation, and comprehension", "author": ["D.P. Barrett", "S.A. Bronikowski", "H. Yu", "J.M. Siskind"], "venue": "arXiv preprint arXiv:1508.06161,", "citeRegEx": "Barrett et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In AAAI,", "citeRegEx": "Chen and Mooney.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "In NIPS,", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Physical causality of action verbs in grounded language understanding", "author": ["Q. Gao", "M. Doering", "S. Yang", "J.Y. Chai"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Gao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "The malmo platform for artificial intelligence experimentation", "author": ["M. Johnson", "K. Hofmann", "T. Hutton", "D. Bignell"], "venue": "In Proc. 25th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Ja\u015bkowski"], "venue": "In IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "Kempka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kempka et al\\.", "year": 2016}, {"title": "Virtual embodiment: A scalable long-term strategy for artificial intelligence research", "author": ["D. Kiela", "L. Bulat", "A.L. Vero", "S. Clark"], "venue": "In NIPS Workshop,", "citeRegEx": "Kiela et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "Behavioral and Brain Sciences, pages 1\u2013101,", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["J. Mao", "X. Wei", "Y. Yang", "J. Wang", "Z. Huang", "A.L. Yuille"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "A roadmap towards machine intelligence", "author": ["T. Mikolov", "A. Joulin", "M. Baroni"], "venue": "arXiv preprint arXiv:1511.08130,", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["A. Rohrbach", "M. Rohrbach", "R. Hu", "T. Darrell", "B. Schiele"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Rohrbach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2016}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Mazebase: A sandbox for learning from games", "author": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus"], "venue": "arXiv preprint arXiv:1511.07401,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Value iteration networks", "author": ["A. Tamar", "S. Levine", "P. Abbeel", "Y. WU", "G. Thomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Zero-shot visual question answering", "author": ["D. Teney", "A. v. d. Hengel"], "venue": "arXiv preprint arXiv:1611.05546,", "citeRegEx": "Teney and Hengel.,? \\Q2016\\E", "shortCiteRegEx": "Teney and Hengel.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "It is argued that a machine has to go through physical experience in order to learn human-level semantics [Kiela et al., 2016], i.", "startOffset": 106, "endOffset": 126}, {"referenceID": 25, "context": "Moreover, the language is not pre-parsed [Sukhbaatar et al., 2016] or -linked [Mikolov et al.", "startOffset": 41, "endOffset": 66}, {"referenceID": 25, "context": "Our XWORLD is similar to the MazeBase environment [Sukhbaatar et al., 2016] in that both are 2D rectangular grid world.", "startOffset": 50, "endOffset": 75}, {"referenceID": 12, "context": "There are several challenging 3D environments for RL such as Kempka et al. [2016] and Jaderberg et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 11, "context": "[2016] and Jaderberg et al. [2017]. The visual perception problems posed by them are much more difficult than ours.", "startOffset": 11, "endOffset": 35}, {"referenceID": 18, "context": "Our setting of language learning shares some similar ideas of the AI roadmap proposed by Mikolov et al. [2015]. Like theirs, we also have a teacher in the environment that assigns tasks and rewards to the agent.", "startOffset": 89, "endOffset": 111}, {"referenceID": 20, "context": "The compositionality of our framework is inspired by the ideas in Neural Programmer [Neelakantan et al., 2016] and Neural Module Networks [Andreas et al.", "startOffset": 84, "endOffset": 110}, {"referenceID": 13, "context": "The importance of compositionality and modularity of a learning framework has been discussed at length in cognitive science by Lake et al. [2016]. The compositionality of our framework is inspired by the ideas in Neural Programmer [Neelakantan et al.", "startOffset": 127, "endOffset": 146}, {"referenceID": 15, "context": "\u25e6 Inductive bias [Lake et al., 2016] must be learned from existing sentences.", "startOffset": 17, "endOffset": 36}, {"referenceID": 2, "context": "We show that word attention [Bahdanau et al., 2015], visual attention, and external memory [Graves et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 10, "context": ", 2015], visual attention, and external memory [Graves et al., 2014] are the keys.", "startOffset": 47, "endOffset": 68}, {"referenceID": 17, "context": "Inspired by the transposed weight sharing scheme [Mao et al., 2015], we set S = ET where E is the word embedding table.", "startOffset": 49, "endOffset": 67}, {"referenceID": 6, "context": "We use a simple gated RNN [Cho et al., 2014] to encode and summarize a question to generate an embedding mask x \u2208 [0, 1].", "startOffset": 26, "endOffset": 44}, {"referenceID": 24, "context": "The syntax embeddings are fed into a Bidirectional RNN [Schuster and Paliwal, 1997] to obtain sentence context vectors el .", "startOffset": 55, "endOffset": 83}, {"referenceID": 6, "context": "The programmer controller, designed as a gated RNN [Cho et al., 2014], is initialized with this booting vector.", "startOffset": 51, "endOffset": 69}, {"referenceID": 26, "context": "With the Actor-Critic (AC) algorithm [Sutton and Barto, 1998], we have the approximate gradients", "startOffset": 37, "endOffset": 61}, {"referenceID": 4, "context": "For effective training, we employ Curriculum Learning [Bengio et al., 2009] and Experience Replay [Mnih et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 19, "context": ", 2009] and Experience Replay [Mnih et al., 2015] with Prioritized Sampling [Schaul et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 23, "context": ", 2015] with Prioritized Sampling [Schaul et al., 2016] (Appendix 8.", "startOffset": 34, "endOffset": 55}, {"referenceID": 7, "context": "We use Adagrad [Duchi et al., 2011] with a learning rate of 10\u22125 for Stochastic Gradient Descent (SGD).", "startOffset": 15, "endOffset": 35}, {"referenceID": 17, "context": "\u25e6 Multimodal We implement a multimodal framework [Mao et al., 2015].", "startOffset": 49, "endOffset": 67}, {"referenceID": 20, "context": "\u25e6 VIS-LSTM Following Ren et al. [2015], we use CNN to get an image embedding which is then projected to the word embedding space and used as the first word of the sentence.", "startOffset": 21, "endOffset": 39}, {"referenceID": 27, "context": "There is no support for either history remembering or route planning [Tamar et al., 2016].", "startOffset": 69, "endOffset": 89}, {"referenceID": 12, "context": "In the future, we plan to migrate the agent to a 3D world like Malmo [Johnson et al., 2016].", "startOffset": 69, "endOffset": 91}], "year": 2017, "abstractText": "We tackle a task where an agent learns to navigate in a 2D maze-like environment called XWORLD. In each session, the agent perceives a sequence of raw-pixel frames, a natural language command issued by a teacher, and a set of rewards. The agent learns the teacher\u2019s language from scratch in a grounded and compositional manner, such that after training it is able to correctly execute zero-shot commands: 1) the combination of words in the command never appeared before, and/or 2) the command contains new object concepts that are learned from another task but never learned from navigation. Our deep framework for the agent is trained end to end: it learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and the action module that outputs actions. The zero-shot learning capability of our framework results from its compositionality and modularity with parameter tying. We visualize the intermediate outputs of the framework, demonstrating that the agent truly understands how to solve the problem. We believe that our results provide some preliminary insights on how to train an agent with similar abilities in a 3D environment.", "creator": "LaTeX with hyperref package"}}}