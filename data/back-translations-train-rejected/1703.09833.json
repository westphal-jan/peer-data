{"id": "1703.09833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Theory II: Landscape of the Empirical Risk in Deep Learning", "abstract": "Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima. However, the practical observation is that, at least for the most successful Deep Convolutional Neural Networks (DCNNs) for visual processing, practitioners can always increase the network size to fit the training data (an extreme example would be [1]). The most successful DCNNs such as VGG and ResNets are best used with a small degree of \"overparametrization\". In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs. We first prove the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations). The zero-minimizers -- in the case of classification -- have a non-zero margin. The same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the zero-minimizer with larger margin, as discussed in Theory III (to be released). We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima. Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN's empirical loss surface, which might not be as complicated as people commonly believe.", "histories": [["v1", "Tue, 28 Mar 2017 22:47:04 GMT  (4386kb,D)", "http://arxiv.org/abs/1703.09833v1", null], ["v2", "Thu, 22 Jun 2017 09:33:35 GMT  (5314kb,D)", "http://arxiv.org/abs/1703.09833v2", "Merged figures to make the main text more compact. Moved some similar figures to the appendix"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["qianli liao", "tomaso poggio"], "accepted": false, "id": "1703.09833"}, "pdf": {"name": "1703.09833.pdf", "metadata": {"source": "CRF", "title": "Theory II: Landscape of the Empirical Risk in Deep Learning", "authors": ["Tomaso Poggio", "Qianli Liao"], "emails": [], "sections": [{"heading": null, "text": "Previous theoretical work on deep learning and neural network optimization tended to focus on avoiding saddle points and local minima, but the practical observation is that, at least in the most successful Deep Convolutional Neural Networks (DCNNs) for visual processing, practitioners can always adapt the network size to the training data (an extreme example would be [1]).The most successful DCNNs, such as VGG and ResNets, are best used with a low degree of \"over-parameterization.\" In this work, we characterize the landscape of empirical risk of over-parameterized DCNNs using a mixture of theory and experiments.First, we prove the existence of a large number of degenerated global minimizers with zero empirical errors (modulo inconsistent equations).The zero minimizers - in the case of classification - exhibit a non-zero-zero margin margin margin."}, {"heading": "1 Introduction 3", "text": "1.1 Organisation of the paper and main results.................................................................................................................................................................................................................................................."}, {"heading": "2 Framework 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Landscape of the Empirical Risk: Theoretical Analyses 5", "text": "3.1 Optimization of compositional functions: Bezout theorem.................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 The Landscape of the Empirical Risk: Visualizing and Analysing the Loss Surface During the Entire Training Process (on", "text": "CIFAR-10) 74.1. Experimental settings.......... 74.2. Global visualization of SGD training pathways......................................................... 74.3... 74.3 Global visualization of training losses with battle tendency................................................................................................................................................................................................................................................................................................................................................................................................................................... More detailed analysis of various local landscapes. 74.3 Global visualization."}, {"heading": "5 The Landscape of the Empirical Risk: Towards an Intuitive Baseline Model 29", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Discussion 32", "text": "6.1 Are the results of this work dependent on data?........................................................................................................................"}, {"heading": "7 Conclusions 32", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Acknowledgment 32", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Appendix: Miscellaneous 33", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Appendix: Study the flat global minima by perturbations on CIFAR-10 (with smaller perturbations) 33", "text": ""}, {"heading": "1 Introduction", "text": "In a theory of deep neural networks, there are at least three main parts: the first part is about approximation - how and when can deep neural networks avoid the curse of dimensionality? The second part is about the landscape of minima of empirical risk: What can we say generally about global and local minima? The third part is about generalization: Why can SGD (Stochastic Gradient Descent) be generalized so well despite the standard over-parameterization of deep neural networks? In this paper, we focus on the second part: the landscape of empirical risk."}, {"heading": "1.1 Organization of the paper and main results", "text": "We characterize the landscape of empirical risk from three perspectives: \u2022 Section 3. Theoretical Analyses: We examine the non-linear system of equations corresponding to the critical points of the gradient of loss (for the L2 loss function) and the zero minimizers corresponding to interpolating solutions. In the equations, the functions representing the results of the network contain the RELU nonlinearity. We consider an approach to them in the sup norm using a polynomial approximation or the corresponding legendary expansion. We can then invoke the bezout theorem to conclude that there are a very large number of zero error minima, and that the zero error minima minima are highly degenerated, while the local non-zero minimum minimum minimum minima minima properties, if they exist, are not degenerated."}, {"heading": "1.2 Previous theoretical work", "text": "The neocognitron [3] was a revolutionary neural network trained to recognize signs. [4] The property of composition was a major motivation for hierarchical models of the visual cortex such as HMAX, which can be considered a pyramid of AND and OR layers [4], which is a sequence of conjunctions and disjunctions. In \"Theory of Deep Learning I,\" we have created formal conditions under which deep networks can avoid the curse of dimensionality. More specifically, several essays have appeared on the landscape of the training error for deep networks [4], which is a sequence of conjunctions and disjunctions. Techniques borrowed from the physics of spin in spin glasses (which in turn are based on old works by Marc Kac on the zeros of tradition)."}, {"heading": "2 Framework", "text": "We are assuming a deep network of the revolutionary type. We are also assuming an over-parameterization, i.e. more weight than data points, since such successful deep networks have been used. Under these conditions, we show that the introduction of a zero error system provides an equation system (at zeros) with a large number of degenerated solutions in the weights. Equations are polynmic in the weights, with coefficients reflecting components of the data vectors (one vector per data point). The equation system is underdetermined (less known than equations, e.g. data points) due to the assumed over-parameterization. Since the global minima are degenerated, i.e. flat in many dimensions, they are more likely to be found by the SGD than local minima, which are less degenerated.In Theory III, we then show that for the weighting regions, which correspond to the global minima of the empirical error, SGD is well generalized."}, {"heading": "3 Landscape of the Empirical Risk: Theoretical Analyses", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Optimization of compositional functions: Bezout theorem", "text": "The following analysis of the landscape of empirical risk is based on two assumptions that apply in most applications of deep revolutionary networks: 1. An over-parameterization of the network, which typically uses many times more parameters (the weights of the network) as data points. In practice, even with data augmentation, the model can be made larger and larger in order to achieve over-parameterization without sacrificing either the training or generalization power. 2. Any of the equations corresponding to the zeros of empirical risk (training in a regression framework that attempts to minimize a loss such as square losses or cross entropies) can be approached by a polynomial equation in the weights, by a polynomial approximation within (in the sup-norm) the RELU nonlinearity."}, {"heading": "3.2 Global minima with zero error", "text": "We will consider a simple example in which the zeros of the empirical error (i.e. exact solutions of the equation set obtained by setting f (xi) \u2212 yi = 0, where i = 1, \u00b7 \u00b7 \u00b7, N are the data points and f (x) the network is parameterized by weights w. Specifically, we will consider the zeros on the training set of a network with ReLUs activating a function of four variables with the following structure: f (x1, x2, x4) = g (h (x1, x2), h \u2032 (x3, x4)). (1) We will assume that the deep approximation network uses ReLUs as the following structure (h, h) = A (W1h + W2h + W3) + B (W \u2032 1h + W \u2032 2h + W \u2032 3) and h (x2)."}, {"heading": "3.3 Minima", "text": "As already mentioned, we expect absolute zeros with zero empirical errors to exist in several cases. If the equations are inconsistent, it seems likely that global minima with similar characteristics exist. Episode 1. In general, there are non-zero minima with a higher dimensionality than the zero error minima: Their dimensionality is the number of weights K against the number of data points N. This is true for both the linear case and the presence of ReLUS. Let us consider the same example as before, let us consider the critical points of the gradient. In a square loss function, the critical points of the gradient are: area losses I = 1 (f (xi) \u2212 yi) 2) = 0 (7), resulting in K equations I = 1 (f (xi) \u2212 yi) 2)."}, {"heading": "4 The Landscape of the Empirical Risk: Visualizing and Analysing the Loss Surface During the Entire Training Process (on CIFAR-10)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Settings", "text": "Unless otherwise stated, we trained a 6-layer Deep Convolutional Neural Network (DCNN) on CIFAR-10. All layers are 3x3 Convolutionary layers with step 2. No pooling is performed. Batch normalizations (BNs) are used between hidden layers. Shifting and scaling parameters in BNs are not used. No data augmentation is performed so that the training set is fixed (size = 50,000). There are 188,810 parameters in DCNN.Multidimensional Scaling The heart of our visualization approach is Multidimensional Scaling (MDS) [9]. We capture a large number of intermediate models during the process of multiple training programs. Each model is a high-dimensional point, where the number of dimensions is the number of parameters. The stress-based MDS algorithm is applied to such points, and a corresponding set of 2D points are found so that the dissimilation between the two dimensions is similar."}, {"heading": "4.2 Global Visualization of SGD Training Trajectories", "text": "We show the optimization paths of the stochastic gradient descent (SGD), as this is what people use in practice.The SGD trajectories follow the mini-batch approximations of the training loss area. Although the trajectories are according to the SGD, the collected points along the trajectories provide a visualization of the landscape empirical risk.We train a 6-layer (where the 1st layer of the input is) folding network on CIFAR-10 with stochastic gradient descent (batch size = 100) We train the training process in 12 stages. In each stage we perform 8 parallel SGDs with training rate 0.01 for 10 stages, resulting in 8 parallel trajectories Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure Figure"}, {"heading": "4.3 Global Visualization of Training Loss Surface with Batch Gradient Descent", "text": "Next, we visualize the exact training loss area by training the models with Batch Gradient Descent (BGD). We apply the following procedures: We train a model from the ground up with BGD. At epochs 0, 10, 50 and 200, we create a branch by disrupting the model by adding a Gaussian noise to all layers. The standard deviation of Gaussian noise is a meta parameter, and we have tried 0.25 * S, 0.5 * S and 1 * S, with S indicating the standard deviation of weights in each layer, respectively the standard deviation. We also interpolate the models (by averaging) between the branches and the main track, epoch by epoch. The interpolated models are evaluated over the entire training set to obtain a performance (error percentage).The main track, branches and interpolated models together provide a good visualization of the landscape of empirical risk."}, {"heading": "4.4 More Detailed Analyses of Several Local Landscapes (especially the flat global minima)", "text": "After the global visualization of the loss area, we perform some more detailed analyses at several points of the landscape. In particular, we want to check if the global minima are flat. We train a 6-layer (the first layer is the input) DCNN on CIFAR-10 with 60 epochs of SGD (batch size = 100) and 400 epochs of Batch Gradient Descent (BGD). BGD is performed to get as close to the global minima as possible. Next, we select three models from this learning path. \u2022 M5: the model in SGD epoch 5. \u2022 M30: the model in SGD epoch 30. \u2022 Mfinal: the final model after 60 epochs of SGD and 400 epochs of BGD. We disturb the weights of these models and apply them with BGD epoch."}, {"heading": "4.4.1 Perturbing the model at SGD Epoch 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.2 Perturbing the model at SGD Epoch 30", "text": "4.4.3 Disruption of the final model (SGD 60 + GD 400 epochs)"}, {"heading": "5 The Landscape of the Empirical Risk: Towards an Intuitive Baseline Model", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "6 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Are the results shown in this work data dependent?", "text": "We visualized the SGD trajectories when applying random labels. There is no qualitative difference between the results of normal labels, so it is safe to say that the results are at least not label dependent. We will continue to examine whether matching random input data to random labels leads to similar results."}, {"heading": "6.2 What about Generalization?", "text": "It is experimentally observed that at least in all our experiments an over-parameterization (e.g. 60x more parameters than data) does not harm the generalization at all. We will discuss the generalization in more detail in Theory III (coming soon)."}, {"heading": "7 Conclusions", "text": "Overall, we characterize the landscape of empirical risk of hyper-parameterized DCNNs using a mix of theoretical analysis and experimental exploration. We provide a simple baseline model of the landscape that can take into account all of our theoretical and experimental results. However, since the final model is so simple, it is hard to believe that it would fully characterize the true loss area of DCNN."}, {"heading": "8 Acknowledgment", "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by the NSF STC Award CCF - 1231216."}, {"heading": "A Appendix: Miscellaneous", "text": "The rank of the weight matrix that connects each node to the lower nodes in the tree should be in the same order as the number of inputs to the lower level and, ultimately, to the first node in the tree (in the case of ResNet, this predicts a value of about 9), as shown in episode 2. Consider the three nodes in the figure. Suppose there are Q inputs for each of the first layer nodes, then the effective inputs to the second layer unit should also be in sequence Q to maintain the same accuracy throughout the network. The above statement follows from episode 4 of \"Theory I.\" It implies that the number of monomials zi is generally not greater than the number of effective, unknown weights. This, in turn, implies that the weights are underdetermined than the monomials themselves."}, {"heading": "B Appendix: Study the flat global minima by perturbations on CIFAR-10 (with smaller perturbations)", "text": "Flawless Model Mfinal: First, on CIFAR-10, we train a 6-layer (1st layer is the input layer) wavy network. After 60 epochs of stochastic gradient descent (lot size = 100) and 372 epochs of gradient descent (lot size = training set size), Model 0 achieves training classification errors. We call this model Mfinal. Next, we disrupt the weights of this zero error model and continue with it. This procedure has been performed several times to see if the weights converge to the same point. Note that no data augmentation is performed so that the training set is fixed (size = 50,000).The procedures are essentially the same as described in main text Section 4.4. The main difference is that the disturbances are smaller. The classification errors are even 0 after the disturbance and throughout the subsequent training process."}], "references": [{"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "arXiv preprint arXiv:1611.03530, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, pp. 436\u2013444, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Neocognitron: A self-organizing neural network for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics, vol. 36, no. 4, pp. 193\u2013202, 1980.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1980}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nature Neuroscience, vol. 2, pp. 1019\u20131025, Nov. 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "author": ["A. Choromanska", "Y. LeCun", "B. Arous"], "venue": "JMLR: Workshop and Conference Proceedings 28th Annual Conference on Learning Theory, p. 1\u20135, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "author": ["D. Soudry", "Y. Carmon"], "venue": "arXiv preprint arXiv:15605.08361, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning without poor local minima", "author": ["K. Kawaguchi"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Complexity of bezout theorem v: Polynomial time", "author": ["M. Shub", "S. Smale"], "venue": "Theoretical Computer Science, no. 133, pp. 141\u2013164, 1994.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}], "referenceMentions": [{"referenceID": 4, "context": "Techniques borrowed from the physics of spin glasses (which in turn were based on old work by Marc Kac on the zeros of algebraic equations) were used [5] to suggest the existence of a band of local minima of high quality as measured by the test error.", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "The argument however depends on a number of assumptions which are rather implausible (see [6] and [7] for comments and further work on the problem).", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "The argument however depends on a number of assumptions which are rather implausible (see [6] and [7] for comments and further work on the problem).", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "Soudry and Carmon [6] show that with mild over-parameterization and dropout-like noise, training error for a neural network with one hidden layer and piece-wise linear activation is zero at every local minimum.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "Even if the number of real zero \u2013 corresponding to zero empirical error \u2013 is much smaller (Smale and Shub estimate [8] l Nd 2 ), the number is still enormous: for a CiFAR situation this may be as high as 2 5 .", "startOffset": 115, "endOffset": 118}], "year": 2017, "abstractText": "Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima. However, the practical observation is that, at least for the most successful Deep Convolutional Neural Networks (DCNNs) for visual processing, practitioners can always increase the network size to fit the training data (an extreme example would be ). The most successful DCNNs such as VGG and ResNets are best used with a small degree of \u201coverparametrization\u201d. In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs. We first prove the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations). The zero-minimizers \u2013 in the case of classification \u2013 have a non-zero margin. The same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the zero-minimizer with larger margin, as discussed in Theory III (to be released). We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima. Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN\u2019s empirical loss surface, which might not be as complicated as people commonly believe. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. 1 ar X iv :1 70 3. 09 83 3v 1 [ cs .L G ] 2 8 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}