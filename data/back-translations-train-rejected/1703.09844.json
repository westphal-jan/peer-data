{"id": "1703.09844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Multi-Scale Dense Convolutional Networks for Efficient Prediction", "abstract": "This paper studies convolutional networks that require limited computational resources at test time. We develop a new network architecture that performs on par with state-of-the-art convolutional networks, whilst facilitating prediction in two settings: (1) an anytime-prediction setting in which the network's prediction for one example is progressively updated, facilitating the output of a prediction at any time; and (2) a batch computational budget setting in which a fixed amount of computation is available to classify a set of examples that can be spent unevenly across 'easier' and 'harder' examples. Our network architecture uses multi-scale convolutions and progressively growing feature representations, which allows for the training of multiple classifiers at intermediate layers of the network. Experiments on three image-classification datasets demonstrate the efficacy of our architecture, in particular, when measured in terms of classification accuracy as a function of the amount of compute available.", "histories": [["v1", "Wed, 29 Mar 2017 00:19:20 GMT  (4488kb,D)", "http://arxiv.org/abs/1703.09844v1", null], ["v2", "Tue, 6 Jun 2017 14:17:22 GMT  (3555kb,D)", "http://arxiv.org/abs/1703.09844v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gao huang", "danlu chen", "tianhong li", "felix wu", "laurens van der maaten", "kilian q weinberger"], "accepted": false, "id": "1703.09844"}, "pdf": {"name": "1703.09844.pdf", "metadata": {"source": "META", "title": "Multi-Scale Dense Convolutional Networks for Efficient Prediction", "authors": ["Gao Huang", "Danlu Chen", "Tianhong Li", "Felix Wu", "Laurens van der Maaten", "Kilian Q. Weinberger"], "emails": ["<gh349@cornell.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is a matter of the way in which it has taken place in the United States in recent years, a time in which the people of the United States, in which it has taken place in the United States, in the United States, in the United States, in Europe, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in Europe and in the United States, in the United States, in the United States, in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States and in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States and in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States, in the United States, in the United States and in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States, in the United States, in the United States and in the United States, in the United States, in the United States and in the United States, in the United States, in the United States and in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States, in the United States, in the United States and in the United States, in the United States and in the United States and in the United States, in the United States, in the United States and in the United States, in the United States and in the United States, in the United States and in the United"}, {"heading": "2. Related Work", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "3. Problem Setup", "text": "We look at two settings, the computational constraints on prediction time.Any-time prediction (Grubb & Bagnell, 2012).The budget is non-deterministic and varies by test instance; it is determined by the occurrence of an event that requires the model to make a prediction instantaneously.We assume that the budget comes from a common distribution P (x, B).In some applications, P (B) can be independent of P (x) and can be estimated. For example, if the event is regulated by a Poisson process, P (B) is an exponential distribution. We refer to the loss of a model f (x) that must generate a prediction, for example x within budget B by L (f (x), B)."}, {"heading": "4. Network Architecture", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "5. Experiments", "text": "Code to reproduce all results is available at https: / / github.com / gaohuang / MSDNet.Datasets. We conduct image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009) and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50,000 training sessions and 10,000 test images of 32 x 32 pixels; we record 5,000 training images as validation. The datasets comprise 10 and 100 classes, respectively. We follow it et al. (2016) and apply standard data augmentation techniques to the training images."}, {"heading": "5.1. Anytime Prediction", "text": "In fact, most of them will be able to move to another world in which they are able to live, in which they want to live."}, {"heading": "5.2. Batch computational budget setting", "text": "In this context, the best classification accuracy can be achieved by performing a kind of dynamic evaluation, which we use to evaluate a deeper classification using \"hard\" examples. Specifically, we associate each classifier with a confidence threshold, and allow a test example of the exit from the first classification threshold where it achieves sufficiently reliable prediction, i.e., its maximum soft-max value is not lower than the threshold at that classifier. The trust threshold at the kth classifier is determined using the validation set."}, {"heading": "6. Conclusion", "text": "We have presented a study on the training of convolutional networks optimized to work in settings with computation budgets at test time. In particular, we are focusing on two different convolutional budgets, namely tests under batch computation budgets and predictions available at all times. Both settings require individual test samples to achieve competitive results. We are introducing a new convolutional architecture that includes two changes: (1) the maintenance of multi-scale function cards in early layers of the convolutional network and (2) the use of an approach for linking function cards that facilitates the reuse of functions in subsequent layers. These two changes allow us to insert intermediate categories in the network architecture and to avoid wasted computing by reusing function cards across classifiers. The results of our experiments show the effectiveness of these changes in settings with computational constraints. In future work, we plan to examine the effects of these model changes on architectural tasks other than the classification of images in 2015, for example."}, {"heading": "Acknowledgements", "text": "The authors are supported in part by grants III-1618134, III1526012, IIS-1149882 from the National Science Foundation and the Bill and Melinda Gates Foundation. We also thank Geoff Pleiss, Yu Sun and Wenlin Wang for helpful and interesting discussions."}], "references": [{"title": "End to end learning for self-driving cars", "author": ["Jackel", "Lawrence D", "Monfort", "Mathew", "Muller", "Urs", "Zhang", "Jiakai"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "Jackel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jackel et al\\.", "year": 2016}, {"title": "Adaptive neural networks for fast test-time prediction", "author": ["Bolukbasi", "Tolga", "Wang", "Joseph", "Dekel", "Ofer", "Saligrama", "Venkatesh"], "venue": "arXiv preprint arXiv:1702.07811,", "citeRegEx": "Bolukbasi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bolukbasi et al\\.", "year": 2017}, {"title": "Compressing neural networks with the hashing trick", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Compressing convolutional neural networks in the frequency domain", "author": ["Chen", "Wenlin", "Wilson", "James", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "In ACM SIGKDD,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Spatially adaptive computation time for residual networks", "author": ["Figurnov", "Michael", "Collins", "Maxwell D", "Zhu", "Yukun", "Zhang", "Li", "Huang", "Jonathan", "Vetrov", "Dmitry", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1612.02297,", "citeRegEx": "Figurnov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Figurnov et al\\.", "year": 2016}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "Graves and Alex.,? \\Q2016\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2016}, {"title": "Training and investigating residual nets. 2016", "author": ["Gross", "Sam", "Wilber", "Michael"], "venue": "URL http://torch.ch/", "citeRegEx": "Gross et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gross et al\\.", "year": 2016}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["Grubb", "Alexander", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Grubb et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grubb et al\\.", "year": 2012}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "coding. CoRR,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "In ICLR,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Optimal brain surgeon and general network pruning", "author": ["Hassibi", "Babak", "Stork", "David G", "Wolff", "Gregory J"], "venue": "In IJCNN,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Deep networks with stochastic depth", "author": ["Huang", "Gao", "Sun", "Yu", "Liu", "Zhuang", "Sedra", "Daniel", "Weinberger", "Kilian Q"], "venue": "In ECCV,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML, pp", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Anytime recognition of objects and scenes", "author": ["Karayev", "Sergey", "Fritz", "Mario", "Darrell", "Trevor"], "venue": "In CVPR, pp", "citeRegEx": "Karayev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Tech Report,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Larsson", "Gustav", "Maire", "Michael", "Shakhnarovich", "Gregory"], "venue": "In ICLR,", "citeRegEx": "Larsson et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2017}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Pruning filters for efficient convnets", "author": ["Li", "Hao", "Kadav", "Asim", "Durdanovic", "Igor", "Samet", "Hanan", "Graf", "Hans Peter"], "venue": "In ICLR,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In CVPR, pp", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Feature-budgeted random forest", "author": ["Nan", "Feng", "Wang", "Joseph", "Saligrama", "Venkatesh"], "venue": "In ICML,", "citeRegEx": "Nan et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Nan et al\\.", "year": 1991}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "In ECCV,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Convolutional neural fabrics", "author": ["Saxena", "Shreyas", "Verbeek", "Jakob"], "venue": "In NIPS,", "citeRegEx": "Saxena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR, pp", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Supervised sequential classification under budget constraints", "author": ["Trapeznikov", "Kirill", "Saligrama", "Venkatesh"], "venue": "In AISTATS, pp", "citeRegEx": "Trapeznikov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Trapeznikov et al\\.", "year": 2013}, {"title": "Robust real-time object detection", "author": ["Viola", "Paul", "Jones", "Michael"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2001}, {"title": "Deep learning for content-based image retrieval: A comprehensive study", "author": ["Wan", "Ji", "Wang", "Dayong", "Hoi", "Steven Chu Hong", "Wu", "Pengcheng", "Zhu", "Jianke", "Zhang", "Yongdong", "Li", "Jintao"], "venue": "In ACM Multimedia,", "citeRegEx": "Wan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2014}, {"title": "Efficient learning by directed acyclic graph for resource constrained prediction", "author": ["Wang", "Joseph", "Trapeznikov", "Kirill", "Saligrama", "Venkatesh"], "venue": "In NIPS,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Xu", "Zhixiang", "Chapelle", "Olivier", "Weinberger", "Kilian Q"], "venue": "In ICML, pp", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Cost-sensitive tree of classifiers", "author": ["Xu", "Zhixiang", "Kusner", "Matt", "Chen", "Minmin", "Weinberger", "Kilian Q"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 30, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 17, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 26, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 4, "context": "This development was in part driven by several public competition datasets, such as ILSVRC (Deng et al., 2009) and COCO (Lin et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 25, "context": ", 2009) and COCO (Lin et al., 2014), where state-of-the-art models may have even managed to surpass human-level performance (He et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 13, "context": ", 2014), where state-of-the-art models may have even managed to surpass human-level performance (He et al., 2015; 2016).", "startOffset": 96, "endOffset": 119}, {"referenceID": 33, "context": "(Wan et al., 2014).", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "But what if the budget varies or is amortized across test cases? Here, convolutional networks pose an inherent dilemma: Deeper (He et al., 2016) and wider (Zagoruyko & Komodakis, 2016) networks with more parameters are better at prediction (Huang et al.", "startOffset": 127, "endOffset": 144}, {"referenceID": 17, "context": ", 2016) and wider (Zagoruyko & Komodakis, 2016) networks with more parameters are better at prediction (Huang et al., 2017), but only the last layers produce the high-level features required to obtain their high level of accuracy.", "startOffset": 103, "endOffset": 123}, {"referenceID": 17, "context": "To solve this dilemma, we propose a novel convolutional network architecture which combines multi-scale feature maps (Saxena & Verbeek, 2016) with dense connectivity (Huang et al., 2017).", "startOffset": 166, "endOffset": 186}, {"referenceID": 23, "context": "In particular, many studies prune weights (LeCun et al., 1989; Hassibi et al., 1993) during or after training and finetune the resulting smaller models (Han et al.", "startOffset": 42, "endOffset": 84}, {"referenceID": 12, "context": "In particular, many studies prune weights (LeCun et al., 1989; Hassibi et al., 1993) during or after training and finetune the resulting smaller models (Han et al.", "startOffset": 42, "endOffset": 84}, {"referenceID": 11, "context": ", 1993) during or after training and finetune the resulting smaller models (Han et al., 2016; Li et al., 2017).", "startOffset": 75, "endOffset": 110}, {"referenceID": 24, "context": ", 1993) during or after training and finetune the resulting smaller models (Han et al., 2016; Li et al., 2017).", "startOffset": 75, "endOffset": 110}, {"referenceID": 15, "context": "For example, the popular knowledge-distillation method (Bucilua et al., 2006; Hinton et al., 2014) trains small student networks to reproduce the output of a much larger teacher network or ensemble.", "startOffset": 55, "endOffset": 98}, {"referenceID": 6, "context": "Most of these studies perform some type of quantization of the network\u2019s weights (Gong et al., 2014) or use hashing (Chen et al.", "startOffset": 81, "endOffset": 100}, {"referenceID": 2, "context": ", 2014) or use hashing (Chen et al., 2015; 2016) with some even going as far as to train networks with binary weights (Hubara et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 28, "context": ", 2015; 2016) with some even going as far as to train networks with binary weights (Hubara et al., 2016; Rastegari et al., 2016).", "startOffset": 83, "endOffset": 128}, {"referenceID": 21, "context": "(2016) recently showed that a binary-weight version of AlexNet (Krizhevsky et al., 2012) can match the performance of its 32-bits floating point counterpart on the ImageNet dataset.", "startOffset": 63, "endOffset": 88}, {"referenceID": 2, "context": ", 2014) or use hashing (Chen et al., 2015; 2016) with some even going as far as to train networks with binary weights (Hubara et al., 2016; Rastegari et al., 2016). Rastegari et al. (2016) recently showed that a binary-weight version of AlexNet (Krizhevsky et al.", "startOffset": 24, "endOffset": 189}, {"referenceID": 19, "context": "Various prior studies explore computationally efficient variants of traditional machine-learning models (Viola & Jones, 2001; Grubb & Bagnell, 2012; Karayev et al., 2014; Trapeznikov & Saligrama, 2013; Xu et al., 2012; 2013; Nan et al., 2015; Wang et al., 2015).", "startOffset": 104, "endOffset": 261}, {"referenceID": 35, "context": "Various prior studies explore computationally efficient variants of traditional machine-learning models (Viola & Jones, 2001; Grubb & Bagnell, 2012; Karayev et al., 2014; Trapeznikov & Saligrama, 2013; Xu et al., 2012; 2013; Nan et al., 2015; Wang et al., 2015).", "startOffset": 104, "endOffset": 261}, {"referenceID": 34, "context": "Various prior studies explore computationally efficient variants of traditional machine-learning models (Viola & Jones, 2001; Grubb & Bagnell, 2012; Karayev et al., 2014; Trapeznikov & Saligrama, 2013; Xu et al., 2012; 2013; Nan et al., 2015; Wang et al., 2015).", "startOffset": 104, "endOffset": 261}, {"referenceID": 22, "context": "Our work is most closely related to recent work on FractalNets (Larsson et al., 2017), which can perform anytime prediction by progressively evaluating subnetworks of the full network.", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "Our dynamic evaluation strategy for reducing batch computational cost is closely related to the the adaptive computation time approach (Graves, 2016; Figurnov et al., 2016), and the recently proposed method of adaptively evaluating neural networks (Bolukbasi et al.", "startOffset": 135, "endOffset": 172}, {"referenceID": 1, "context": ", 2016), and the recently proposed method of adaptively evaluating neural networks (Bolukbasi et al., 2017).", "startOffset": 83, "endOffset": 107}, {"referenceID": 5, "context": "The adaptive computation time method (Graves, 2016) and its extension (Figurnov et al., 2016) also perform adaptive evaluation on test examples to save batch computational cost.", "startOffset": 70, "endOffset": 93}, {"referenceID": 17, "context": "We use the same featureconcatenation approach as DenseNets (Huang et al., 2017), which allows us to construct highly compact models.", "startOffset": 59, "endOffset": 79}, {"referenceID": 30, "context": ", 2015) and Inception models (Szegedy et al., 2015), our convolutional network has classifiers operating on feature maps at intermediate layers in the network.", "startOffset": 29, "endOffset": 51}, {"referenceID": 17, "context": "The second change we make is that we use dense connections (Huang et al., 2017) that allow each layer to receive inputs directly from all its previous layers.", "startOffset": 59, "endOffset": 79}, {"referenceID": 14, "context": "(2017), such dense connections (1) significantly reduce redundant computations by encouraging feature reuse and (2) make learning easy by eliminating the gradient vanishing problem in a way similar to residual networks (He et al., 2016).", "startOffset": 219, "endOffset": 236}, {"referenceID": 14, "context": "The second change we make is that we use dense connections (Huang et al., 2017) that allow each layer to receive inputs directly from all its previous layers. As shown by Huang et al. (2017), such dense connections (1) significantly reduce redundant computations by encouraging feature reuse and (2) make learning easy by eliminating the gradient vanishing problem in a way similar to residual networks (He et al.", "startOffset": 60, "endOffset": 191}, {"referenceID": 16, "context": "Following the dense connectivity pattern proposed by Huang et al. (2017), the output feature maps x ` produced at subsequent layers, `>1, and scales, s, are a concatenation of transformed feature maps from all previous feature maps of scale s and s\u2212 1 (if s > 1).", "startOffset": 53, "endOffset": 73}, {"referenceID": 17, "context": "Second, we can add a transition layer between two blocks to merge the concatenated features with 1 \u00d7 1 convolution and reduce the number of channels by half, similar to the DenseNet-BC architecture (Huang et al., 2017).", "startOffset": 198, "endOffset": 218}, {"referenceID": 17, "context": "For subsequent feature layers, the transformations h` and h\u0303` are defined following the design in DenseNets (Huang et al., 2017): Conv(1 \u00d7 1)-BN-ReLUConv(3\u00d73)-BN-ReLU.", "startOffset": 108, "endOffset": 128}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets.", "startOffset": 155, "endOffset": 174}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50, 000 training and 10, 000 test images of 32\u00d732 pixels; we hold out 5, 000 training images as a validation set. The datasets comprise 10 and 100 classes, respectively. We follow He et al. (2016) and apply standard dataaugmentation techniques to the training images: images are zero-padded with 4 pixels on each side, and then randomly cropped to produce 32\u00d732 images.", "startOffset": 156, "endOffset": 413}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50, 000 training and 10, 000 test images of 32\u00d732 pixels; we hold out 5, 000 training images as a validation set. The datasets comprise 10 and 100 classes, respectively. We follow He et al. (2016) and apply standard dataaugmentation techniques to the training images: images are zero-padded with 4 pixels on each side, and then randomly cropped to produce 32\u00d732 images. Images are flipped horizontally with probability 0.5, and normalized by subtracting channel means and dividing by channel standard deviations. The ImageNet dataset comprises 1, 000 classes, with a total of 1.2 million training images and 50,000 validation images. We hold out 50,000 images from the training set in order to estimate the confidence threshold for classifiers in MSDNet. We adopt the data augmentation scheme of He et al. (2016); Huang et al.", "startOffset": 156, "endOffset": 1029}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50, 000 training and 10, 000 test images of 32\u00d732 pixels; we hold out 5, 000 training images as a validation set. The datasets comprise 10 and 100 classes, respectively. We follow He et al. (2016) and apply standard dataaugmentation techniques to the training images: images are zero-padded with 4 pixels on each side, and then randomly cropped to produce 32\u00d732 images. Images are flipped horizontally with probability 0.5, and normalized by subtracting channel means and dividing by channel standard deviations. The ImageNet dataset comprises 1, 000 classes, with a total of 1.2 million training images and 50,000 validation images. We hold out 50,000 images from the training set in order to estimate the confidence threshold for classifiers in MSDNet. We adopt the data augmentation scheme of He et al. (2016); Huang et al. (2017) at training time; at test time, we classify a 224\u00d7224 center crop of images that were resized to 256\u00d7256 pixels.", "startOffset": 156, "endOffset": 1050}, {"referenceID": 22, "context": "There exist two convolutional network architectures that are suitable for anytime prediction: namely, FractalNets (Larsson et al., 2017) and deeply supervised networks Lee et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 22, "context": "There exist two convolutional network architectures that are suitable for anytime prediction: namely, FractalNets (Larsson et al., 2017) and deeply supervised networks Lee et al. (2015).", "startOffset": 115, "endOffset": 186}, {"referenceID": 14, "context": "In all experiments, we use the state-of-the-art DenseNet with bottleneck and channel reduction layers, which is referred to as DenseNet-BC by Huang et al. (2017). competitive ensemble of ResNets with varying depths.", "startOffset": 142, "endOffset": 162}, {"referenceID": 13, "context": "All the models are the same as those described by He et al. (2016), except that the ResNet-10 and ResNet26 are variants of ResNet-18 that were modified by removing/adding one residual block from/to each of the last four spatial resolutions in the network.", "startOffset": 50, "endOffset": 67}, {"referenceID": 14, "context": "We include ResNets (He et al., 2016), DenseNets (Huang et al.", "startOffset": 19, "endOffset": 36}, {"referenceID": 17, "context": ", 2016), DenseNets (Huang et al., 2017) and DenseNets* (described in Section 5.", "startOffset": 19, "endOffset": 39}, {"referenceID": 16, "context": "1) of varying sizes, Stochastic Depth Networks (Huang et al., 2016), Wide ResNets (Zagoruyko & Komodakis, 2016) and FractalNets (Larsson et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 22, "context": ", 2016), Wide ResNets (Zagoruyko & Komodakis, 2016) and FractalNets (Larsson et al., 2017), with test time costs within our region of interest.", "startOffset": 68, "endOffset": 90}, {"referenceID": 13, "context": "ResNets (He et al., 2015)", "startOffset": 8, "endOffset": 25}, {"referenceID": 16, "context": "DenseNets (Huang et al., 2016)", "startOffset": 10, "endOffset": 30}, {"referenceID": 30, "context": "GoogLeNet (Szegedy et al., 2015)", "startOffset": 10, "endOffset": 32}, {"referenceID": 21, "context": "AlexNet (Krizhevsky et al., 2012)", "startOffset": 8, "endOffset": 33}, {"referenceID": 17, "context": "1, the 121layer DenseNet (Huang et al., 2017), AlexNet (Krizhevsky et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 21, "context": ", 2017), AlexNet (Krizhevsky et al., 2012) and GoogleLeNet (Szegedy et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 30, "context": ", 2012) and GoogleLeNet (Szegedy et al., 2015).", "startOffset": 24, "endOffset": 46}, {"referenceID": 26, "context": ", image segmentation (Long et al., 2015).", "startOffset": 21, "endOffset": 40}, {"referenceID": 2, "context": "We also intend to explore approaches that combine MSDNets, for instance, with model compression (Chen et al., 2015; Han et al., 2015) to further improve computational efficiency.", "startOffset": 96, "endOffset": 133}, {"referenceID": 10, "context": "We also intend to explore approaches that combine MSDNets, for instance, with model compression (Chen et al., 2015; Han et al., 2015) to further improve computational efficiency.", "startOffset": 96, "endOffset": 133}], "year": 2017, "abstractText": "This paper studies convolutional networks that require limited computational resources at test time. We develop a new network architecture that performs on par with state-of-the-art convolutional networks, whilst facilitating prediction in two settings: (1) an anytime-prediction setting in which the network\u2019s prediction for one example is progressively updated, facilitating the output of a prediction at any time; and (2) a batch computational budget setting in which a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \u201ceasier\u201d and \u201charder\u201d examples. Our network architecture uses multi-scale convolutions and progressively growing feature representations, which allows for the training of multiple classifiers at intermediate layers of the network. Experiments on three image-classification datasets demonstrate the efficacy of our architecture, in particular, when measured in terms of classification accuracy as a function of the amount of compute available.", "creator": "LaTeX with hyperref package"}}}