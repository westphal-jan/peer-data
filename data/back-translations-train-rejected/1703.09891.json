{"id": "1703.09891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "LabelBank: Revisiting Global Perspectives for Semantic Segmentation", "abstract": "Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixel-level segmentation.", "histories": [["v1", "Wed, 29 Mar 2017 05:58:21 GMT  (5228kb,D)", "http://arxiv.org/abs/1703.09891v1", "Pre-prints"]], "COMMENTS": "Pre-prints", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["hexiang hu", "zhiwei deng", "guang-tong zhou", "fei sha", "greg mori"], "accepted": false, "id": "1703.09891"}, "pdf": {"name": "1703.09891.pdf", "metadata": {"source": "CRF", "title": "LabelBank: Revisiting Global Perspectives for Semantic Segmentation", "authors": ["Hexiang Hu", "Zhiwei Deng", "Simon Fraser", "Guang-Tong Zhou", "Fei Sha", "Greg Mori"], "emails": ["hexiang.frank.hu@gmail.com", "zhiweid@sfu.ca", "zhouguangtong@gmail.com", "feisha@usc.edu", "mori@cs.sfu.ca"], "sections": [{"heading": null, "text": "We demonstrate our framework's ability to improve semantic segmentation performance in a variety of environments, learn models for extracting a holistic LabelBank from visual clues, attributes and / or text descriptions, and demonstrate improvements in the semantic segmentation accuracy of standard data sets across a range of state-of-the-art segmentation architectures and holistic inference approaches."}, {"heading": "1. Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2. Related Work", "text": "The success of CNNs in object recognition has led to renewed attention in semantic segmentation. A representative work is the Fully Convolutional Network (FCN) [34], which uses the features of CNNs for detailed pixel labeling. FCN combines multi-level characteristic descriptions to utilize coarse to fine local pixel texture. In a recent advance, the Atrous Convolution by Chen et al. [4] is introduced as a technique that maintains a wide field of view while exhibiting less portable weights in semantic segmentation networks."}, {"heading": "3. Framework", "text": "Remember that LabelBank is defined as a continuous vector of trust for the presence of semantic object categories in an image. Figure 2 provides an overview of our framework. In detail, it consists of three components. First, we have a holistic inference process that uses different information sources to represent an image by LabelBank. Second, we have a detailed semantic segmentation process to perform preliminary semantic segmentation on the image to generate a segmentation map. Finally, we have a holistic filtering process that uses the derived LabelBank to filter out false-positive pixel forecasts in preliminary semantic segmentation. Our framework is generic and can flexibly integrate different data sources / architectures into LabelBank segmentation networks by using different semantic segmentation networks."}, {"heading": "3.1. Holistic Filtering", "text": "It uses the LabelBank representation = = high confidence derived from the holistic inference method to actively filter false-positive pixel predictions in the segmentation map generated by the semantic segmentation process. Note that a segmentation map is typically organized as a trust matrix for mapping each semantic category to each image pixel. Our idea is to use the segmentation map on each pixel to recommend labels for pixel predictions - if LabelBank suggests a semantic label, it is unlikely that pixels will be predicted as such label. To implement the idea, we will recommend the segmentation map on each pixel through the LabelBank trust elements. Weighting is done by multiplying both LabelBank and segmentation trust systems."}, {"heading": "3.2. LabelBank Inference", "text": "The LabelBank can be derived from a variety of data sources. An easy option is to use the image itself, i.e. the visual appearance. We present two exemplary visual inference architectures in sections 3.2.1 and 3.2.2, but other architectures are possible and can easily be incorporated into our framework. In addition, we can also derive the LabelBank from commonly available image metadata, such as labels based on image attributes or sentences describing the image content. To this end, we implement sample architectures in Section 3.2.3. In addition, we are also experimenting with combined visual appearance and metadata for a more accurate representation of the LabelBank. To design the architectures, we would like to emphasize the following two principles: First, we prefer a process where an image and / or its metadata is used as input and the LabelBank representation is produced as output, where the color gradient for a font-to-end-to-training object should not be propagated back over the second architectures."}, {"heading": "3.2.1 SPP for Visual Appearance", "text": "Following the design principles, we first implement a Spatial Pyramid Pooling Architecture (SPP) for visual conclusions. This is illustrated in the upper part of Figure 3.The SPP architecture is motivated by the success of spatial pyramid pooling for image recognition [13]. First, it uses a feature network (i.e. low layers of folding and pooling) to extract a feature map onto an image. Then, it applies spatial pyramid pooling on the feature map, followed by a multi-layer perception to predict the LabelBank. For training purposes, we get LabelBank from the ground-truth pixel labels - an image is labeled with a semantic category if at least one pixel is labeled with this category. We adopt a sigmoid activation layer on the output of multi-layer perception and evaluate a categorical cross-entropy loss."}, {"heading": "3.2.2 DC for Visual Appearance", "text": "The DC architecture is motivated by the fact that a semantic object rarely occupies the entire image, but only a portion of an image. Therefore, we perform location-based predictions that inspect tightly cropped image windows. To implement, we first apply the same feature network as the SPP architecture. We then cut the feature map tightly to obtain features on image windows for location-based predictions. As a prediction model, we implement a two-layer CNN structure (details in the appendix). Each location-based prediction results in a k-dimensional vector that describes the reliability for each semantic category that appears in the corresponding image window. Finally, we compose the LabelBank representation by a max pooling over the location-aware prediction. We impose a loss on the location-aware prediction. Specifically, the truth labels on each image category are first identified as the xbox inscribed category."}, {"heading": "3.2.3 OHE and W2V Embedding for Meta Data", "text": "This architecture first embeds metadata into a feature ace. Depending on the type of metadata, we can then apply a one-time encoding for attributes (OHE architecture) or a word2vec-like representation [30, 32] for sentence descriptions (W2V architecture). With the embedded features, we then apply a multi-layer perception to predict the LabelBank of an image. We train by imposing a loss on the final LabelBank predictions. The basic LabelBank and the loss are calculated in the same way as in our SPP architecture."}, {"heading": "3.2.4 Training", "text": "Our framework is traceable throughout, as gradient back propagation is enabled in the LabelBank inference processes mentioned above, preliminary semantic segmentation (FCN and DilatedNet), and holistic filtering (Section 3.1). Together, we optimize both LabelBank inference loss and semantic segmentation loss in education. In the subsections described above, we have described LabelBank inference losses. Segmentation loss is enforced on the complete segmentation map derived from holistic filtering. In accordance with the default setting of semantic segmentation [34, 4] we first apply a Softmax activation layer to the complete segmentation map, and then calculate a categorical cross entropy loss for the segmentation task. The balance between the two losses is determined by a constant multiplicator to further clarify our order of size so that both our losses are similar."}, {"heading": "4. Experiments", "text": "In this section, we will conduct experiments to test the effectiveness and generalizability of our framework. We will examine a variety of settings for LabelBank inference and semantic segmentation. We will present the experiments from two perspectives. First, we will examine the variants of LabelBank inference in Section 4.2 using different data sources and architectures. Second, we will examine the variants of our semantic segmentation process in Section 4.3 using FCN and DilatedNet. Before delving into the details, we will first describe the common experimental settings in Section 4.1."}, {"heading": "4.1. Settings", "text": "We evaluate our framework on the basis of five benchmark data sets: PASCAL context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35] and SIFT-Flow [27]. All of these data sets contain abundant contextual labels on pixels and are therefore a challenge for semantic segmentation. Table 1 summarizes the five data sets. In the appendix, the details of these data sets are elaborated.Baselines. a direct starting line is to use only the semantic segmentation process and to ignore the conclusions of the LabelBank and holistic filtering. As mentioned above, we have modified FCN [34] and DilatedNet [40, 4] as our segmentation networks and refer to FCN + and DilatedNet +: the semantic segmentation networks and the segmentation networks i: the segmentation networks i, the segmentation categories i - the segmentation networks i and the segmentation networks i - the segmentation networks i i and the segmenti i i i i i i i i, the segmenti i i i i i i i and the segmenti i i i i i i, the segmenti i i i and the segmenti i i i."}, {"heading": "4.2. Study on the LabelBank Inference Process", "text": "In this section, we evaluate variations of LabelBank inference with different data sources and architectures. Specifically, we have SPP and DC for visual appearance, OHE for attributes, and W2V for text descriptions. At this point, we correct our semantic segmentation network as FCN + and show that our framework is effective and general with respect to different LabelBank inference processes."}, {"heading": "4.2.1 Inference from Visual Appearance", "text": "This setting does not require additional metadata and is directly comparable to existing semantic segmentation approaches. We have compared our methods (SPP + FCN + and DC +) with the baseline FCN +, as well as with existing methods in the literature. Results are presented in Table 2. It turns out that our methods perform better than all the methods compared, confirming the effectiveness of our framework. Table 2 also confirms the usefulness of LabelBank-based holistic filters - it substantially increases the FCN substance."}, {"heading": "4.2.2 Inference from Meta Data", "text": "Next, we conduct experiments to derive LabelBank from metadata, using image attributes and text descriptions to discover holistic concepts in images. Specifically, we extract image attributes on ADE20K and COCO-Stuff. We rely on the provided taxonomies of the semantic object categories in the two datasets. For each object category of an image, we assign attributes that should be its ancestral hypernyms. The image attributes are collected as a union of attributes for all semantic categories present in the image. Subsequently, we apply the OHE architecture to derive the representation of the LabelBank. Text descriptions are available on COCO-Stuff, and we apply our W2V architecture to LabelBank sequences. In detail, we first perform GloVe embedding [32] on individual words, and then we obtain a feature vector of the text inscription by means of all words."}, {"heading": "4.2.3 Combining Meta Data and Visual Appearance", "text": "In our implementation, we simply attach the embedded OHE / W2V features to any location on the characteristic card of our DC architecture, resulting in the OHE + DC + FCN + and W2V + DC + FCN + methods, and we report on their performance in Tables 3 and 4. It clearly shows that OHE + DC + FCN + and W2V + DC + FCN + achieve the best performance on ADE20K and COCO Stuff. Also, note that the combination of metadata and visual appearance together produces better LabelBank than the use of single data sources - OHE + DC + FCN + surpasses OHE + FCN + and DC + FCN + on the two datasets."}, {"heading": "4.3. Study on the Semantic Segmentation Process", "text": "We use DC for LabelBank conclusions and instantiate the segmentation process with FCN + and DilatedNet +. Comparison results are presented in Table 5. For a full comparison with ex-isting approaches in the literature, refer to Tables 2, 3 and 4. As shown, DC + FCN + and DC + DilatedNet + are improving over their baseline counterparts FCN + and DilatedNet + (with the exception of SIFT-Flow, DC + FCN +), confirming that our framework is general and can improve the state of the art CNN-based semantic segmentation networks."}, {"heading": "5. Analysis", "text": "In this section, we conduct empirical studies to better understand our framework. We asked the following two questions: (i) What is the maximum performance gain we could achieve if we had perfect LabelBank conclusions? and (ii) How can we refine the current LabelBank conclusion to further improve performance? Below, we provide answers and insights to these two questions."}, {"heading": "5.1. Oracle LabelBank", "text": "Our experiments in Section 4 show that semantic segmentation benefits from the leadership of LabelBank. However, we would by no means claim that our LabelBank inference is perfect, so an interesting question we need to raise is: How well could our framework perform if we had a perfect LabelBank inference? We answer this question by replacing our LabelBank inference process with a static process that returns the \"oracle\" LabelBank. The Oracle LabelBank can be easily derived from the pixel labels by taking the value of infinity when a semantic class is present on the image, and otherwise negative infinity. We keep the rest of the framework intact and train it end-to-end to optimize the segmentation loss (see Section 3.2.4 for details). We compare FCN +, DC + FCN + and Oracle + FCN + in Figure 4. It clearly shows that Oracle + FCN + Segmentation performance increases for significant label bank (see Section 3.2.4 for details)."}, {"heading": "5.2. Noisy LabelBank", "text": "For example, if we apply a 0 threshold to the LabelBank trust values derived from DC + FCN +, we observe 90.93% recall and 46.75% accuracy on PASCALContext images and 69.07% recall and 47.59% accuracy on ADE20K images. We believe that precision and recall are two key parameters for the quality of LabelBank representation, so we start with the Oracle LabelBank precision and recall on semantic segmentation performance, and provide insights into how the current recall process can be refined for further performance improvements. We have evaluated our framework with various precision and recall settings on the LabelBank. We start with the Oracle LabelBank and contaminate it with some precision and recall level. Specifically, we degrade precision by adding more and noisier image labels and removing the recall by removing more and more labels."}, {"heading": "6. Conclusion", "text": "We presented a generic framework consisting of three components: LabelBank inference, semantic segmentation, and holistic filtering; the LabelBank inference process derives a holistic LabelBank representation of an image from different data sources and inference architectures; semantic segmentation uses state-of-the-art CNN-based networks to create a preliminary segmentation map; and finally, the holistic filtering process refines the segmentation results by using LabelBank information to filter out false-positive pixel predictions; experiments with semantic benchmark segmentation data sets demonstrate the effectiveness of the proposed framework; we believe that our solution is general and could be applied to many other applications, such as integral LabelBank object recognition, and so on."}, {"heading": "Appendix A. Implementation Details", "text": "In this section, we will explain the detailed implementation of FCN + and DilatedNet + in Section A.1, the two-layer CNN structure used in the DC architecture in Section A.2, Multi-layer Perception (MLP) in Section A.4.A.1. FCN + and DilatedNet + The semantic segmentation process is responsible for generating a preliminary segmentation map for the holistic filtering process being implemented under the direction of LabelBank. As mentioned in Section 3, we have FCN + and DilatedNet + The semantic segmentation method that serves as the basis for our semantic segmentation process, i.e. FCN + and DilatedNet +. We will show the detailed network architectures in Figure 6, and describe the details below. FCN + has a structure similar to FCN [34]: It first applies a feature network to get a feature map on an image, and then generate three functions."}, {"heading": "Appendix B. Training Strategy", "text": "Optimization. We follow the practice of FCN [34] to train our framework - the optimization of the target by stochastic gradient descent with small batch size (e.g. 1) and large pulse (e.g. 0.99). We train about 60 epochs and select the best models by validation. Data augmentation. Data augmentation has been shown to be a handy technique to increase semantic segmentation performance. In our experiments, we used both horizontal flipping and scale augmentation in training our networks. To scale augmentation, we randomly select a scale factor in the set of {0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3} to scale each image."}, {"heading": "Appendix C. Experimental Datasets", "text": "PASCAL-Context is an extension of the PASCAL VOC 2010 dataset with detailed pixel-by-pixel annotations [31]. The semantic annotations contain both objects and stuff present in the image. Following [31, 34], we evaluate our network using the most common 59 classes in addition to a background class. The training and test sets contain 4,998 and 5,105 images of the respective age. ADE20K is another dataset with densely annotated objects and stuff. We learn our model using the 20,210 training images and report on the performance on the 2,000 validation images. We did not evaluate the 5,000 test images because the ground annotations are not publicly available. Following [42], we select the 150 semantic classes according to their total pixel ratios, including 35 object classes and 115 material classes. The pixels from the 150 classes occupy max. 92.75% of all pixel images. COCO-Stuff is a densely annotated 115 object classes including their total data classes."}, {"heading": "Appendix D. Additional Results", "text": "Due to the spatial limitation in our main article, we postpone our experimental results on SIFT-Flow and NYUDv2 at this point. These two datasets have no additional metadata beyond the visual appearance of the image. Therefore, we are comparing our SPP + FCN + and DC + FCN + with the FCN + baseline and state-of-the-art approaches here. Details are described as follows: D.1. SIFT-Flow The results are reported in Table 7, which shows that our methods work best across all comparative methods. Note that the current state of the art FCN-8s [34] uses the available pixel-by-pixel geometric labels (i.e. horizontal, vertical and sky-shaped) as additional monitoring, while our methods do not use them. These observations show the utility of our framework in semantic segmentation."}, {"heading": "Appendix E. Ablation Studies on Feature Network", "text": "In this section, we conduct ablation studies of our functional network. First, in Section E.1, we compare our framework with multi-task learning, which also uses a common functional network. Second, in Section E.2, we evaluate the flexibility of our framework against different functional networks. The resulting framework has a structure similar to multi-task learningIn our framework implementation, we divide the functional network through the visual appearance of LabelBank inference and the semantic segmentation process. However, we emphasize that our performance gains come mainly from holistic filtering, not from the generic features we have learned from the shared functional network. We conduct experiments to support it subsequently. We have evaluated a standard multi-task learning method where we perform classification and segmentation."}, {"heading": "Appendix F. Settings for Noisy LabelBank Experiments", "text": "In Section 5.2, we experimented with various precision and recall settings on LabelBank. The detailed setup is as follows: To reduce precision, we added noisy labels per image in np: 1, 2, \u00b7 \u00b7 \u00b7, 10}. Similarly, we removed nr: 1, 2, \u00b7 \u00b7, 10} ground truth labels per image (if there are fewer than nr: ground truth labels in an image, we simply remove them all). Note that np and nr can be set as fractions to increase the numerical accuracy of precision and recall: For example, setting nr: 2.3 means that we first remove 2 (the whole part of nr: ground truth: ground labels on each image and then randomly take 30% (the decimal part of nr:) of the images to remove another ground truth label each time."}, {"heading": "Appendix G. Visualizations", "text": "We select sample images from the PASCAL context and ADE20K and visualize the semantic segmentation results in Figures 7 and 8. Here, we use the DC architecture to derive LabelBank from the visual appearance of the image. As a comparison to existing methods, we have also provided the FCN [34] results for PASCAL context and the DilatedNet [42] results for ADE20K. Qualitative comparison confirms the usefulness of LabelBank for semantic segmentation - it helps to filter out false-positive pixel predictions through the holistic filtering process."}], "references": [{"title": "Cost-sensitive top-down/bottom-up inference for multiscale activity recognition", "author": ["M.R. Amer", "D. Xie", "M. Zhao", "S. Todorovic", "S.-C. Zhu"], "venue": "ECCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Higher order conditional random fields in deep neural networks", "author": ["A. Arnab", "S. Jayasumana", "S. Zheng", "P.H.S. Torr"], "venue": "ECCV,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "COCO-Stuff: Thing and stuff classes in context", "author": ["H. Caesar", "J. Uijlings", "V. Ferrari"], "venue": "arXiv preprint arXiv:1612.03716,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A. Yuille"], "venue": "arXiv Preprint, abs/1606.00915,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "ICCV,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional feature masking for joint object and stuff segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale object classification using label relation graphs", "author": ["J. Deng", "N. Ding", "Y. Jia", "A. Frome", "K. Murphy", "S. Bengio", "Y. Li", "H. Neven", "H. Adam"], "venue": "ECCV,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE T-PAMI, 35(8):1915\u20131929,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Contextual action recognition with R*CNN", "author": ["G. Gkioxari", "R. Girshick", "J. Malik"], "venue": "ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual organization and recognition of indoor scenes from RGB-D images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": "CVPR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning rich features from RGB-D images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbel\u00e1ez", "J. Malik"], "venue": "ECCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiscale conditional random fields for image labeling", "author": ["X. He", "R.S. Zemel", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning structured inference neural networks with label relations", "author": ["H. Hu", "G.-T. Zhou", "Z. Deng", "Z. Liao", "G. Mori"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Structural- RNN: Deep learning on spatio-temporal graphs", "author": ["A. Jain", "A.R. Zamir", "S. Savarese", "A. Saxena"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient inference in fully connected CRFs with Gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "OBJ CUT", "author": ["M.P. Kumar", "P.H.S. Torr", "A. Zisserman"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Object Bank: A high-level image representation for scene classification and semantic feature sparsification", "author": ["L.-J. Li", "H. Su", "E.P. Xing", "L. Fei-Fei"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I. Reid", "A. van den Hengel"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "L.C. Zitnick"], "venue": "ECCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric scene parsing via label transfer", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "IEEE T-PAMI, 33(12):2368\u20132382,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "SIFT flow: Dense correspondence across scenes and its applications", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "IEEE T-PAMI, 33(5):978\u2013994,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "ParseNet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "ICLR Workshop,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P.H.O. Pinheiro", "R. Collobert"], "venue": "ICML,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["E. Shelhamer", "J. Long", "T. Darrell"], "venue": "IEEE T-PAMI,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Action bank: A high-level representation of activity in video", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S.E. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding things: Image parsing with regions and per-exemplar detectors", "author": ["J. Tighe", "S. Lazebnik"], "venue": "CVPR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous object detection and segmentation by boosting local shape feature based classifier", "author": ["B. Wu", "R. Nevatia"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H.S. Torr"], "venue": "ICCV,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic understanding of scenes through the ADE20K dataset", "author": ["B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba"], "venue": "arXiv Preprint, abs/1608.05442,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "the seminal ObjectBank work [24]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "State-of-the-art methods for semantic segmentation leverage the successes of Convolutional Neural Networks (CNNs) [23].", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "CNNs have transformed the field of image classification, especially since the development of AlexNet [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 35, "context": "There have been many follow-up CNN architectures to further boost image classification, including VGGNet [36], Google Inception [37], ResNet [14], etc.", "startOffset": 105, "endOffset": 109}, {"referenceID": 36, "context": "There have been many follow-up CNN architectures to further boost image classification, including VGGNet [36], Google Inception [37], ResNet [14], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "There have been many follow-up CNN architectures to further boost image classification, including VGGNet [36], Google Inception [37], ResNet [14], etc.", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "A representative work is the Fully Convolutional Network (FCN) [34] that leverages skip features of CNNs to produce a detailed pixel labeling.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "Another example is the DeepLab [4] framework, which augments FCN with dilated convolution [40], atrous spatial pyramid pooling and Conditional Random Fields (CRFs), and obtains state-of-art semantic segmentation performance.", "startOffset": 31, "endOffset": 34}, {"referenceID": 39, "context": "Another example is the DeepLab [4] framework, which augments FCN with dilated convolution [40], atrous spatial pyramid pooling and Conditional Random Fields (CRFs), and obtains state-of-art semantic segmentation performance.", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "The semantic segmentation process can be implemented using state-of-the-art approaches, such as FCN [34] and DilatedNet [40, 4].", "startOffset": 100, "endOffset": 104}, {"referenceID": 39, "context": "The semantic segmentation process can be implemented using state-of-the-art approaches, such as FCN [34] and DilatedNet [40, 4].", "startOffset": 120, "endOffset": 127}, {"referenceID": 3, "context": "The semantic segmentation process can be implemented using state-of-the-art approaches, such as FCN [34] and DilatedNet [40, 4].", "startOffset": 120, "endOffset": 127}, {"referenceID": 30, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 41, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 135, "endOffset": 138}, {"referenceID": 34, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 147, "endOffset": 151}, {"referenceID": 26, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 166, "endOffset": 170}, {"referenceID": 33, "context": "A representative work is the Fully Convolutional Network (FCN) [34] that uses skip features of CNNs for detailed pixel labeling.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "[4] as a technique to retain a large field of view while keeping fewer trainable weights in semantic segmentation networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 39, "context": "The same method termed as dilated convolution was also pursued by Yu and Koltun [40] by a cascading series of dilated convolution layers.", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "A fully connected CRF is proposed by Kr\u00e4henb\u00fchl and Koltun [19] as an efficient dense pixel modeling method.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "[4] and Zheng et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[41] on top of FCNs as a further refinement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] modeled hierarchical and exclusive relations among semantic categories.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] developed recurrent neural network structures for spatio-temporal inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] proposed a neural graph inference model to propagate information among multiple levels of visual classes, in-", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "replicate (k) [0, 1] LabelBank: c\u03c3", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "[0, 1] expanded LabelBank: C\u03c3", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "(k, h, w) [0, 1] LabelBank Inference", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "(k, h, w) [0, 1] element-wise mul&plica&on", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "[1] adopted the and-or graph structure to reason about human activities at multiple levels of granularity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] exploited contextual cues to improve action recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed to use multi-scale CRFs to capture features at various image resolutions for semantic segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Approaches for modeling object instances and their segmentations have also been developed [21, 39].", "startOffset": 90, "endOffset": 98}, {"referenceID": 38, "context": "Approaches for modeling object instances and their segmentations have also been developed [21, 39].", "startOffset": 90, "endOffset": 98}, {"referenceID": 5, "context": "Another example of this line improves instance-aware semantic segmentation with object detection and classification [6, 7].", "startOffset": 116, "endOffset": 122}, {"referenceID": 6, "context": "Another example of this line improves instance-aware semantic segmentation with object detection and classification [6, 7].", "startOffset": 116, "endOffset": 122}, {"referenceID": 33, "context": "Furthermore, our semantic segmentation process is also generic, and can be implemented with state-of-the-art CNN-based network architectures like FCN [34] or DilatedNet [40, 4].", "startOffset": 150, "endOffset": 154}, {"referenceID": 39, "context": "Furthermore, our semantic segmentation process is also generic, and can be implemented with state-of-the-art CNN-based network architectures like FCN [34] or DilatedNet [40, 4].", "startOffset": 169, "endOffset": 176}, {"referenceID": 3, "context": "Furthermore, our semantic segmentation process is also generic, and can be implemented with state-of-the-art CNN-based network architectures like FCN [34] or DilatedNet [40, 4].", "startOffset": 169, "endOffset": 176}, {"referenceID": 33, "context": ", FCN [34] and DilatedNet [40, 4]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 39, "context": ", FCN [34] and DilatedNet [40, 4]).", "startOffset": 26, "endOffset": 33}, {"referenceID": 3, "context": ", FCN [34] and DilatedNet [40, 4]).", "startOffset": 26, "endOffset": 33}, {"referenceID": 3, "context": "The up-sampling is simply done by bi-linear interpolation (following [4]) to increase the resolution to the original image size.", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "The SPP architecture is motivated by the success of spatial pyramid pooling for image recognition [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "Specifically, depending on the type of the meta data, we could apply one-hot encoding for attributes (OHE architecture), or word2vec-style representation [30, 32] for sentence descriptions (W2V architecture).", "startOffset": 154, "endOffset": 162}, {"referenceID": 31, "context": "Specifically, depending on the type of the meta data, we could apply one-hot encoding for attributes (OHE architecture), or word2vec-style representation [30, 32] for sentence descriptions (W2V architecture).", "startOffset": 154, "endOffset": 162}, {"referenceID": 33, "context": "Following the standard setting of semantic segmentation [34, 4], we first apply a softmax activation layer to the full segmentation map, and then compute a categorical cross-entropy loss for the segmentation task.", "startOffset": 56, "endOffset": 63}, {"referenceID": 3, "context": "Following the standard setting of semantic segmentation [34, 4], we first apply a softmax activation layer to the full segmentation map, and then compute a categorical cross-entropy loss for the segmentation task.", "startOffset": 56, "endOffset": 63}, {"referenceID": 30, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 69, "endOffset": 73}, {"referenceID": 41, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 98, "endOffset": 101}, {"referenceID": 34, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 15, "endOffset": 19}, {"referenceID": 41, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 91, "endOffset": 94}, {"referenceID": 34, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 123, "endOffset": 127}, {"referenceID": 26, "context": "SIFT-Flow [27] 33 2488/200/Table 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "As mentioned above, we modified FCN [34] and DilatedNet [40, 4] to be our segmentation networks, and refer them to FCN and DilatedNet.", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "As mentioned above, we modified FCN [34] and DilatedNet [40, 4] to be our segmentation networks, and refer them to FCN and DilatedNet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 3, "context": "As mentioned above, we modified FCN [34] and DilatedNet [40, 4] to be our segmentation networks, and refer them to FCN and DilatedNet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 33, "context": "Following [34], we evaluate four common performance metrics for semantic segmentation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "CFM [6] - - 34.", "startOffset": 4, "endOffset": 7}, {"referenceID": 33, "context": "4 FCN-8s [34] 67.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "03 CRF-RNN [41] - - 39.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "3 DeepLab [4] - - 39.", "startOffset": 10, "endOffset": 13}, {"referenceID": 28, "context": "6 ParseNet [29] - - 40.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "4 BoxSup [5] - - 40.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "5 HO-CRF [2] - - 41.", "startOffset": 9, "endOffset": 12}, {"referenceID": 24, "context": "3 Context [25] 71.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "3 DeepLab + COCO [4] - - 44.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "7 DeepLab + COCO + CRF [4] - - 45.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "It is worth noting that DC+ FCN outperforms the current state-of-the-art method, DeepLab + COCO + CRF [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 41, "context": "We follow the settings of [42] in our experiments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "SegNet [42] 71.", "startOffset": 7, "endOffset": 11}, {"referenceID": 41, "context": "84 SegNet Cascade [42] 71.", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "05 FCN-8s [42] 71.", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "33 DilatedNet [42] 73.", "startOffset": 14, "endOffset": 18}, {"referenceID": 41, "context": "14 DilatedNet Cascade [42] 74.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "FCN [3] 52.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "7 DeepLab [3] 57.", "startOffset": 10, "endOffset": 13}, {"referenceID": 41, "context": "This experiment enables us to directly compare with those reported in [42].", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "We follow the standard setting of [3] in our experiments.", "startOffset": 34, "endOffset": 37}, {"referenceID": 31, "context": "In detail, we first perform GloVe embedding [32] on individual words, and then obtain a feature vector of the textual description by averaging the embeddings of all words.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": ", skip-thought vector [18] or paragraph vector [22]) are applicable, and we leave these for future exploration.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": ", skip-thought vector [18] or paragraph vector [22]) are applicable, and we leave these for future exploration.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixellevel segmentation. We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches.", "creator": "LaTeX with hyperref package"}}}