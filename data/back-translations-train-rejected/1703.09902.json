{"id": "1703.09902", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation", "abstract": "This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.", "histories": [["v1", "Wed, 29 Mar 2017 06:51:00 GMT  (3455kb,D)", "http://arxiv.org/abs/1703.09902v1", "111 pages, 8 figures, 2 tables"]], "COMMENTS": "111 pages, 8 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["albert gatt", "emiel krahmer"], "accepted": false, "id": "1703.09902"}, "pdf": {"name": "1703.09902.pdf", "metadata": {"source": "CRF", "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation", "authors": ["Albert Gatt"], "emails": ["albert.gatt@um.edu.mt", "e.j.krahmer@tilburguniversity.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.09 902v 1Contents"}, {"heading": "1 Introduction 4", "text": "1.1......................................................."}, {"heading": "2 NLG Tasks 9", "text": "2.1. Determination of content..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 NLG Architectures and Approaches 21", "text": "3.1. Rule-based, modular approaches..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 The vision-language interface: Image captioning and beyond 40", "text": "4.1 Data................................................ 41 4.2 The core tasks.................. 424.2.1 Image analysis................................. 424.2.1 Image analysis............."}, {"heading": "5 Variation: Generating text with style, personality and affect 47", "text": "5.1 Generating with style: textual variation and personality..... 48 5.2 Generating with feeling: affection and courtesy.......... 50 5.3 Style and affect: concluding remarks........... 52"}, {"heading": "6 Generating creative and entertaining text 53", "text": "6.1 Making puns and jokes..................................................................................................................................................."}, {"heading": "7 Evaluation 61", "text": "7.1. Intrinsic Methods............................ 637.1.1 Subjective (human) judgments............ 63 7.1.2 Objective Humanitarian Measures by Corporate...... 65 7.1.3 Assessment of Genre Compatibility................. 677.2 Extrinsic Assessment Methods.............. Metrics vs. Human Judgements..... 68 7.3 Black Box vs. Glass Box Assessment....... 70 7.4.2 Use of Controlled Experiments...... 69 7.4 On the Relationship between Assessment Methods....."}, {"heading": "8 Discussion and future directions 74", "text": "8.1 Why (and how) should the NLG be used?............. 75 8.2 NLG is not text-to-text.... or is it?............ 75 8.3 Theories and models in search of applications?........... 76 8.4 Where does it go from here?................"}, {"heading": "9 Conclusion 79", "text": "References 79"}, {"heading": "1 Introduction", "text": "In his fascinating history The Library of Babel (La biblioteca de Babel, 1941), Jorge Luis Borges has described a library where every conceivable book can be found automatically. It's probably the wrong question to ask, but readers can't help asking: Who wrote all these books? Surely this could not be the work of human authors? The emergence of automatic text generation techniques in recent years offers an interesting twist to this question. Consider Philip M. Parker, who offered more than 100,000 books for sale on Amazon.com, including, for example, The 2007-2012 Outlook for Tufted Washable Scatter Rugs, Bathmats, and Sets That Measure 6-Feet by 9-Feet or Smaller in India. Apparently, Parker did not write these 100,000 books by hand. Rather, he used a computer program that gathers publicly available information, possibly packaged in human-written texts, and assembles them into a book. Just like the library of Babel, many books are unlikely to appeal to a wider audience, many are books that are sold to an audience."}, {"heading": "1.1 What is Natural Language Generation?", "text": "In the most frequently cited study of nlg methods to date (Reiter & Dale, 1997, 2000), nlg is similarly characterized as \"the subfield of artificial intelligence and computer-aided linguistics that deals with the construction of computer systems, than can produce intelligible texts in English or other human languages from some underlying non-linguistic representations of information\" (Reiter & Dale, 1997, p. 1). This definition is clearly more suited to text generation than text-to-text generation, and in fact Reiter and Dale (2000) focus exclusively on the former, helpful and clearly descriptive rule-based approaches that dominated the field at the time. It has been pointed out that the precise definition of nlg is quite difficult (e.g. Evans et al., 2002): everyone seems to agree on what the output of an nlg system should be (text), but what the exact input is is complicated."}, {"heading": "1.2 Why a survey on Natural Language Generation?", "text": "While Reiter and Dale (2000) are still the most complete overview of nlg and the most frequently cited texts available, the nlg area has changed drastically in the last 15 years as successful applications have generated tailor-made reports for specific audiences, and with the advent of text-to-text and vision-to-text applications, which also tend to rely more on statistical methods than traditional data-to-text. None of them is covered in Reiter and Dale (2000). Also striking is the lack of discussion about applications that go beyond standard, \"factual\" text generation, such as those that take personality and affects into account, or creative text such as metaphors and narratives. Finally, a conspicuous omission between Reiter and Dale (2000) is the lack of discussion about evaluation methodology. In fact, the evaluation of these nlg editions has only recently begun to receive systematic attention due to a number of common tasks carried out within the ng community."}, {"heading": "1.3 Goals of this survey", "text": "The aim of the current study is to obtain a comprehensive overview of developments since 2000, both in terms of synthetics and in terms of the way in which the results are presented."}, {"heading": "2 NLG Tasks", "text": "Traditionally, the nlg problem of converting input data into output text has been addressed by dividing it into a series of sub-problems: the following six are commonly found in many nlg systems (Tab & Dale, 1997, 2000); their role is determined in Figure 1: 1. Content determination: Deciding what information should be included in the text under construction, 2. text structuring: Determining in which order information should be presented in the text, 3. sentence aggregation: Determining what information should be presented in individual sentences, 4. lexicalization: Finding the right words and phrases to express information, 5. expression generation: selecting the words and phrases to identify domain objects, 6. language implementation: Combining all words and phrases into well-formed sentences, 4. lexicalization: Finding the right words and phrases to express information, 5. expression generation: selecting the words and phrases to identify domain objects, how to interpret the phrases in a sentence, how to interpret the phrases in a sentence)."}, {"heading": "2.1 Content determination", "text": "As a first step in the generational process, the nlg system must decide which information should be included in the text under construction and which should not. Typically, the data contains more information than we want to convey through text, or the data is more detailed than we want to express in the text. This is clear in Figure 1a, where the input signal - a patient's heart rate - contains only a few interest patterns. Selection may also depend on the target audience (e.g. it may consist of experts or beginners, for example) and on the general communicative intention (e.g. the text should inform the reader or persuade him to do something).Content determination includes choice. In a football report, we may not want to verbalize every pass and foul, although the data may contain this information. In the case of neonatal care, data could be continuously collected from sensors that measure heart rate, blood pressure and other physiological parameters. Data therefore need to be filtered and abstracted into a number of preverbal messages, often expressed in logical representations."}, {"heading": "2.2 Text structuring", "text": "Once they have determined which messages to convey, the nlg system must decide on their order of presentation to the reader. Thus, Figure 1b shows three events of the same type (all bradycardia events, i.e. short heart rate crashes), which are selected (by abstraction) from the input signal and sorted as a temporal sequence. This phase is often referred to as textual (or discourse or document) structuring. In the case of the football domain, for example, it seems reasonable to start with general information (where and when the game was played, how many people participated, etc.) before describing the goals, typically in chronological order. In the field of neonatal care, a temporal order can be imposed between specific events, as in Figure 1b, but larger text spans can reflect orders based on importance, and grouping of information based on connections (e.g., all events related to a patient's respiration)."}, {"heading": "2.3 Sentence aggregation", "text": "It is not as if one has to express oneself in another sentence; by combining several messages in a single sentence, the generated text may become more fluid and readable (e.g., Dalianis, 1999; Cheng & Mellish, 2000), although there are also situations in which it is argued that the summary of individual sentences should be avoided (discussed in Section 5.2). Thus, the three events selected in Figure 1b are presented as \"merged\" into a single pre-linguistic representation, which is summarized in a single sentence. The process of summarizing related messages into sentences is known as a sentence aggregation.To take another example, from the football domain one would describe (unaggregated), the fastest hat trick in the English Premier League: (1) Sadio Mane scored for Southampton after 12 minutes and 22 seconds. (2) Sadio Mane scored for Southampton after 13 minutes and 46 seconds."}, {"heading": "2.4 Lexicalisation", "text": "As a matter of fact, most of them are able to survive themselves, and that they are able to survive themselves. Most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, and most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "2.5 Referring expression generation", "text": "(...). (...). (...). (...). (...). (...). (...). (...). It is as if they were able to determine for themselves what they want to do. (...). (...). It is as if they do not do it. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). ().). (...). (...). ().). (...). (...). (...).). (...). (...). (...).). (...). (...).). (...). (...). (...). (). ().). (...). (...). ().). (...). (). (). (). (). ().). (). (). ().). (). (). (). (). (). ().).). (). (). (). ().). (). ().). (). (). ().).). (). ().).). (). (). (). ().).). (). (). ().). (). ().).).). (). ()"}, {"heading": "2.6 Linguistic realisation", "text": "The simple example in Figure 1d shows the structure underlying the sentence: there were three consecutive bradycardia up to 69, with the linguistic message corresponding to the part selected from the original signal in Figure 1a. Commonly referred to as linguistic implementation, this task involves arranging components of a sentence and generating the correct morphological forms (including verb conjugations and matches, in the languages in which this is relevant); often realists also need to insert functional words (such as auxiliary verbs and prepositions) and punctuation marks. An important complication at this stage is that the output must include various linguistic components that may not be present in the input (an example of the \"generation gap\" discussed in Section 3.1); therefore, this generation problem can be thought from the point of view of projection between non-isomphic 3 structures. (In 2015, different approaches to human grammar were designed; we have developed different steretic approaches)."}, {"heading": "2.6.1 Templates", "text": "If the application areas are small and the variation should be minimal, the implementation is a relatively simple task, and the results can be specified on the basis of templates (e.g. Reiter et al., 1995; McRoy et al., 2003), such as the following. (7) $players scored a goal for a team in the minute of $in the minute of $. This template has three variables that can be filled with the names of a player, a team and the minute in which that player scored a goal. Therefore, it can be used to generate phrases such as: (8) Ivan Rakitic scored for Barcelona in the 4th minute. One advantage of templates is that they allow full control over the quality of the output and avoid generating ungrammatic structures. Modern variants of the template-based method include syntactical information in the templates as well as ingenious rules for filling the gaps (Theune et al., 2001), making it difficult to distinguish templates from more complex methods (van Deemter et al., 2005)."}, {"heading": "2.6.2 Hand-coded grammar-based systems", "text": "Most of these systems are grammar-based, meaning that they make some or all of their decisions based on the grammar of the language in question, which can be written manually, as in many standard programs such as fuf / surge (Elhadad & Robin, 1996), murmur (Meteer et al., 1987), kpml (Bateman, 1997), nigel (Mann & Matthiessen, 1983), and RealPro (Lavoie & Rambow, 1997). Hand-coded grammar creators tend to require very detailed input, for example, kpml (Bateman, 1997) is based on systemic-functional grammar (sfg; Halliday & Matthiessen, 2004), and implementation is modeled as a cross-section of a network where decisions are made on both a grammatical and a mantic-pragmatic basis."}, {"heading": "2.6.3 Statistical approaches", "text": "The second approach is not based on a computer-generated statistical approach, which directly relates to the amount of manual work required, but on increasing the range. In essence, two approaches have been taken to incorporate statistical information into the realization process, an approach based on the basic work of Langkilde and Knight (Langkilde-Geary, 2000) on the basis of the halogen / nitrogen systems, which relies on a two-step approach in which a small, handmade grammar is used to generate alternative realizations from which a stochastic re-evaluation of the individual candidates is carried out. Langkilde and Knight rely on portrayed statistical knowledge in the form in which others have experimented with more complex statistical models to perform ringing (e.g. Bangalore & Rambow, 2000; Ratnaparkhi, 2000; Cahill et al)."}, {"heading": "2.7 Discussion", "text": "This section gives an overview of some of the classic tasks found in most nlg systems. One of the most common trends that can be identified in any case is the steady transition from early, handmade approaches based on rules to newer stochastic approaches based on corpus data, with a related trend towards more domain-independent approaches. Historically, this has already been the case for tasks such as generating or implementing reference expressions, which have themselves become subjects of intense research. However, as more and more approaches to all nlg tasks begin to take a statistical turn, there is an increasing emphasis on learning techniques; the domain-specific aspect is also, as it were, a property of the training data itself. As we will see in the next section, this trend has also influenced the way various nlg tasks are organized, i.e. the architecture of systems for generating text from data."}, {"heading": "3 NLG Architectures and Approaches", "text": "After giving an overview of the most common subtasks contained in nlg systems, we now turn to the way such tasks can be organized. Broadly speaking, we can distinguish between three dominant approaches for nlg architectures: 1. Modular architectures: These are often typical of systems rooted in the classical paradigm of symbol processing that dominated early ai research. In terms of design, such architectures contain fairly clear divisions between subtasks, though with significant differences; 2. Planning Prospects: Once again deeply rooted in the ai tradition, consideration of text generation as planning allows for a more integrated, less modular design; 3. Data-driven, integrated approaches: Today, the prevailing trend in nlg (as is more general in nlp) is that such approaches rely heavily on statistical learning of correspondence between (non-linguistic) inputs and outputs. Such intersecting of correspondence often leads us to the oldest subsets, which, in turn, is the first three subsets of the problematics."}, {"heading": "3.1 Rule-based, modular approaches", "text": "In fact, most of them will be able to play by the rules."}, {"heading": "3.2 Planning-based approaches", "text": "In ai, the planning problem can be described as the process of identifying a sequence of one or more actions in order to achieve a particular goal. An initial goal can be broken down into partial goals that are satisfied by actions, each of which has its preconditions and effects. In the classical planning paradigm (Strips; Fikes & Nilsson, 1971), actions are presented as tuples of such preconditions and effects. In this text generation, the link between planning and nlg is considered to be the execution of planned behavior in order to achieve a communicative goal in which each action leads to a new state, that is, a change in a context that encompasses both linguistic interaction and the history of discourse to date, but also the physical or situated context and the actions of the user (see Lemon, 2008; Rieser & Lemon, 2009; Dethlefs, 2014; Garoufi & Koller, 2013; Garoufi, 2014, for some more recent perspectives on this topic)."}, {"heading": "3.2.1 Planning through the grammar", "text": "In fact, most of them will be able to move to another world, in which they will be able to move, and in which they will be able to move to another world, in which they will be able to move, in which they will be able to move."}, {"heading": "3.2.2 Stochastic planning under uncertainty using Reinforcement Learning", "text": "The approaches to planning that we have discussed so far are largely rules-based and tend to fix the relationship between a planned action and its consequences (i.e., its impact on the context) as fixed (although there are exceptions, such as in contingency planning, which generates multiple plans to address different possible outcomes; Riesaya & Petrick, 2007).Like Rieser and Lemon, this view is unrealistic, we consider a system that generates a restaurant recommendation.The consequences of its output (i.e. the new state it generates) are subject to noise arising from multiple sources of uncertainty, in part due to trade-offs, such as between the need to include the right amount of information while avoiding excessive prolixity. Another source of uncertainty may be the actions predicted by the system, not those predicted by the system. An instance of meteors (1991) generation gap can trace its head back when a stochastic content of a message renders the content of a message, renders the content of a message over-long or ambiguous message."}, {"heading": "3.3 Data-driven approaches", "text": "Although the shift to data-driven methods in nlg began somewhat later than in other areas of nlp, there is little doubt that this is the dominant trend today. Later in this section, we begin with an overview of methods for collecting training data for nlg - in particular pairings of inputs (data) and outputs (text) - before turning to an overview of techniques and frameworks. One of the topics that will emerge from this overview is that statistical methods, as in the case of planning, often take a uniform or \"global\" view of the nlg process, rather than a modularized one."}, {"heading": "3.3.1 Acquiring data", "text": "In fact, it is in such a way that it acts in a manner and manner, in which people move themselves in a world, in which they see themselves in a position to change the world, and in which they seek to place themselves in a world, in which they see themselves in a position, in which they place themselves in a world, in which they place themselves in a world, in which they place themselves in a world, in a world, in a world, in a world, in which they themselves, in a world, in which they see themselves, in a world, in which they place themselves in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in which they place themselves, in a world, in which they place themselves, in a world, in which they place themselves, in a world, in a world, in which they place themselves, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in which they find themselves, in a world, in a world, in a world, in which they find themselves, in a world, in a world, in a world, in which they find themselves, in a world, in a world, in which they find themselves, in a world, in a world, in which they find themselves, in a world, in which they find themselves, in a world, in a world, in which they find themselves, in a world, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves, in themselves."}, {"heading": "3.3.2 NLG based on language models", "text": "In fact, it is not surprising that most people who work for the rights of people, work for the rights of people who work for the rights of people who work for the rights of people who work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for the rights of people, work for"}, {"heading": "3.3.3 NLG as classification and optimisation", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "3.3.4 NLG as \u2018parsing\u2019", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to integrate themselves, in which they are able to live, in which they live, in which they live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3.3.5 Deep learning methods", "text": "In fact, it is so that most people are able to recognize themselves and understand what they are doing. (...) Most of them are able to recognize themselves and understand themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to recognize themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves."}, {"heading": "3.4 Discussion", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "4 The vision-language interface: Image caption-", "text": "In recent years, there has been an explosion of interest in the task of automatically generating captions, as part of a broader effort to explore the interface between vision and language (Barnard, 2016). Captions have arguably become a paradigm case for generating data to text, where the input comes in the form of an image, and the task has long become a focus of research not only in the nlg community, but also in the computer vision community, increasing the possibility of more effective synergies between the two research groups. Aside from their practical applications, the creation of language in perception data is a matter of scientific interest (cf. Winograd, 1972; Harnad, 1990; Roy & Reiter, 2005, for a variety of theoretical views on the computational challenges of the perception language interface). Figure 6 shows some examples of captioning generation spanning about six years. Current captioning research focuses on 2016, mainly describing 2015 as the year Hodosh is presented as an object."}, {"heading": "4.1 Data", "text": "Bernardi et al. (2016) provides a detailed overview of datasets, while Ferraro et al. (2015) provides a systematic comparison of datasets both for generating captions and answering visual questions with an accompanying online resource. Datasets typically consist of images paired with one or more human-made captions (mostly in English) and vary from artificially created scenes (Zitnick et al., 2013) to real-world photos. Among the most common datasets are Flickr8k (Hodosh et al., 2013), Flickr30k (Young et al., 2014) and ms-coco (Lin6http: / / visionandlanguage.netet al al al al al al., 2014). Datasets such as the sbu1m Captioned Photo Dataset (Ordonez et al., 2011) include naturally occurring captions of users sharing photos on websites such as Flickr; therefore, the captions are not limited to the actual image concepts contained therein."}, {"heading": "4.2 The core tasks", "text": "There are two logically distinguishable sub-tasks in a captioning system, i.e. image analysis and text generation. This does not mean that they need to be organized separately or sequentially. However, before we deal with architectures as such, it is worth briefly giving an overview of the methods by which these two tasks are accomplished."}, {"heading": "4.2.1 Image analysis", "text": "There are three main groups of approaches to treating visual information for labeling targets. Some systems rely on computer vision methods to recognize and label objects, attributes, \"stuff\" (typically mass nouns such as grass), spatial relationships, and possibly action and pose information. This is usually followed by a step that applies these outputs to linguistic structures (\"sentence plans\" of the kind discussed in sections 2 and 3), such as trees or templates (e.g. Kulkarni et al., 2011; Mitchell et al., 2012; Elliott & De Vries, 2015; Yatskar et al., 2014; Kuznetsova et al., 2014; performance depends on the range and accuracy of detectors (Kuznetsova et al., 2014; Bernardi et al., 2016), some work has also explored the generation of gold standard image annotations."}, {"heading": "4.2.2 Text generation or retrieval", "text": "Depending on the type of image analysis we have previously covered in conjunction with realisation (Section 2.6), captions can be generated using a variety of different methods, the following of which are well established: in 2015, using templates or tree-based systems based on detectors, the results of linguistic structures can be mapped in a sentence planning phase. Other approaches rely on sequence classification algorithms, such as Hidden Markov Models (Yang et al., 2011) and conditional random fields (Kulkarni et al., 2011, 2013). Kulkarni et al. (See the example in Figure 6b) Experiment with both templates and web-derived n \u2212 gram language models, which find that alumni are more fluid but suffer from lack of variation."}, {"heading": "4.3 How is language grounded in visual data?", "text": "As the discussion above suggests, views on the relationship between visual and linguistic data depend on the way each of the two sub-tasks is handled. Systems based on recognition characteristics tend to make a fairly clear distinction between input processing and content architecture on the one hand, and sentence planning and realization on the other (e.g. Kulkarni et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013). The connection between linguistic expressions and visual characteristics is mediated by the results of the detectors. Thus, Midge (Mitchell et al., 2012) uses object detections to determine which nouns to mention before filling in the label with attributes. Elliott and Keller (2013) use vdrs to determine spatial expressions. Retrieval-based systems based on unimodal or multimodal similarity represent the link between linguistic expressions and visual characteristics."}, {"heading": "4.4 Vision and language: Current and future directions for NLG", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "5 Variation: Generating text with style, person-", "text": "Based on the preceding sections, one could excuse the reader if he thinks that nlg is mainly about providing factual information, whether in the form of a summary of weather data or a description of an image. This distortion was also highlighted in the introduction, where we gave a brief overview of some areas of application and noted that information was often, though not always, the target in nlg. However, over the last ten years, a growing trend has emerged in the nlg literature to also focus on aspects of providing textual information that may not be propositional, that is, features of texts that are not strictly based on input data, but are related to the way in which they are provided. In this section, we will focus on these trends, starting with the broad concept of \"stylistic variation,\" before turning to generating affective text and courtesy."}, {"heading": "5.1 Generating with style: textual variation and personality", "text": "What does the term \"linguistic style\" mean in linguistic terms? Most of the work we refer to as \"stylistic nlg\" is far from a rigorous definition, preferring to operationalize the term in the terms that are most relevant to the problem in question. \"Style\" is usually understood to refer to the characteristics of lexis, grammar, and semantics that collectively contribute to the identification of a particular author, or to a specific situation (hence, one distinguishes between levels of stylistic formality or speaks of the characteristics of William Faulkner's style), implying that any study of style must concern itself, with variations between the characteristics that characterize such authoritative or situative variables. This section examines developments in the nlg, where variation is the central concern, rather than the strategic level, the idea that a particular piece of information can be a particular piece of information."}, {"heading": "5.2 Generating with feeling: affect and politeness", "text": "In fact, it is so that most of them are able to determine themselves what they want, and that they are able to determine themselves. (...) Most of them are not able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are not able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...)"}, {"heading": "5.3 Style and affect: concluding remarks", "text": "In summary, work on variation in nlg, which mainly focuses on tactical decisions or linguistic characteristics at the surface level, has attempted to model differences in registers or style, as well as individual differences based on personality and affective state. This area of nlg research is still at a relatively young stage, with several open questions of both theoretical and computational significance. These include how best to model complex, multidimensional constructs such as personality or emotion; this question concerns both the cognitive plausibility of the models that influence linguistic decisions, and the practical feasibility of various machine learning strategies that can be used for the task (e.g. linear, additive models versus \"global\" personality or style models)."}, {"heading": "6 Generating creative and entertaining text", "text": "\"Good\" writers not only present their ideas in coherent and well-structured prose, but they also manage to capture the reader's attention through narrative techniques and occasionally surprise the reader, for example through creative language usage such as small jokes or well-placed metaphors (see e.g. Flower & Hayes, 1981; Nauman et al., 2011; Veale & Li, 2015).The nlg techniques and applications that we have discussed so far in this study probably do not simulate good writers in this sense, and as a result, automatically generated texts can be perceived as somewhat boring and repetitive.This lack of attention to creative aspects of language production within nlg is not due to a general lack of scientific interest in these phenomena. Indeed, computational research into creativity has a long tradition, with roots that date back to the early days of the ai (as Gerva \u2012 s, 2013, noted, the first narrative algorithm on record, Novel Writer, was developed by researchers in 1973 between Sheldon and others, although it may have been improved by others)."}, {"heading": "6.1 Generating puns and jokes", "text": "Consider: (29) What is the difference between money and a reason? One spares oneself and bank, the other one bare and spank. (30) What do you call a strange market? A bizarre joke that automatically generates a strange market? These two (pretty good!) puzzles were automatically generated by the jape system developed by Binsted and Ritchie, which is often based on spelling or literal ambiguities. Many good, man-made examples have been collected in joke books and websites and can therefore act as a source of inspiration. Simplifying somewhat, Jape (Joke Analysis and Production Engine) relies on a template-based nlg-based text (What's the difference between X and Y? or What do you call X?) with slots that are the source of the puzzle."}, {"heading": "6.2 Generating metaphors and similes", "text": "Whether something is funny or not may be subjective, but in any case insights from the joke generation can be useful as a springboard for a better understanding of creative language use, including metaphors, similarities and analogies. In all of these cases, an illustration is made between two conceptual areas, in such a way that terminology from the source domain is used to say something about the target domain, which is typically used in a non-literal way that can be helpful in computer-generated texts to illustrate complex information. (2006) Study analogies in narrative contexts, such as Luke Skywalker was the King of the Jedi Knights, which is an important aspect of Luke Skywalker for those who are not in the know. In a similarity, the two domains are compared (A \"like\" B); in a metaphor metaphor they are equated."}, {"heading": "No Monopoly Is More Ruthless", "text": "No crime family is worse organized or controls me more ruthlessly with your centralized organization Let me support your privileged security O Microsoft, you oppress me with your corrupt rulersIn fact, the automatic generation of poems is an emerging area at the crossroads between computational creativity and the production of natural language (see, for example, Lutz, 1959; Gerva's, 2001; Wong et al., 2008; Netzer et al., 2009; Greene et al., 2010; Colton et al., 2012; Manurung et al., 2012; Zhang & Lapata, 2014b, for variations on this subject)."}, {"heading": "6.3 Generating narratives", "text": "The starting point for many approaches to narrative creation is a view of narration based on classical narrativology, a branch of literary studies rooted in the formal and structural tradition (e.g. Propp, 1968; Genette, 1980; Bal, 2009), which has dealt with the analysis of both the characteristics of narrative as well as the subtle features, such as the handling of time and time shifts, focus (i.e., the ability to convey to the reader that a story is told from a certain point of view) and the interaction of several narrative theses, in the form of sub-plots, etc. An important recent development is the interest of narrativity in having the narrator's interest in the narrative."}, {"heading": "6.4 Generating creative language: Concluding remarks", "text": "In this section, we have highlighted recent developments in the broad field of creative language generation, a topic that nlg tends to underestimate. Nevertheless, we would argue that nlg researchers can improve the quality of their results by taking insights from computational creativity on board. Labor, which uses corpora and other lexical resources for the automatic generation of jokes, puns, metaphors, and similarities, has identified various ways in which words relate to each other and can be juxtaposed to form unexpected and possibly even \"poetic\" combinations. Given that metaphors are ubiquitous in everyday language (such as from Lakoff & Johnson), it is not just a creative use, nlg researchers are interested in improving readability - and, above all, the ability to question the text-generating capabilities of their models."}, {"heading": "7 Evaluation", "text": "Although we have touched on the issue of evaluation at various points, it deserves a full discussion as a topic that has become a central methodological concern in nlg. One factor that has contributed to this development has been the establishment of a number of nlg shared tasks that were launched following an nsffunded workshop in Virginia in 2007, focusing on the reference to expressions (Belz et al., 2010; Gatt & Belz al.); surface realization (Belz et al., 2011); generation of instructions in virtual environments (Striegnitz et al., 2011; Janarthanam et al., 2011; Lemon.); content determination (Bouayad-Agha et al., 2013; and Question Generation (Rus et al., 2011)."}, {"heading": "7.1 Intrinsic methods", "text": "Intrinsic evaluation in nlg is dominated by two methods, one based on human judgment (and thus subjective), the other on corpora."}, {"heading": "7.1.1 Subjective (human) judgements", "text": "It's not as if people are able to do the things they do, as if they don't do what they want to do. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it. It's as if they do it, as if they do it. It's as if they do it, as if they do it."}, {"heading": "7.1.2 Objective humanlikeness measures using corpora", "text": "In recent years, it has become clear that most of them are not self-portrayals, but self-portrayals, which are about showing people what they can do. (...) In recent years, it has become clear that this is self-portrayals. (...) In recent years, people's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed. (...) People's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed. (...) In the last ten years, people's self-portrayals have changed."}, {"heading": "7.1.3 Evaluating genre compatibility", "text": "A slightly different question, which has occasionally been asked in evaluation studies, asks whether the linguistic artifact produced by a system is actually a recognizable example of a particular genre or style. Examples of this we have in our discussion of creative language generation in Section 6. One of the questions asked, for example, by Binsted et al. (1997) was whether the output of jape was recognizably a joke. Hardcastle and Scott (2008) describe an evaluation of a generational system for cryptic, cross-border references, since the ability to recognize an artifact as an instance of a genre or to exhibit a certain style or personality is recognizably different from the cues written by humans. While such questions clearly have an intrinsic orientation, they also have an impact on extrinsic factors, since the ability to recognize an artifact as an instance of a genre or to exhibit a certain style or personality is one of the sources of its effects, especially in the case of creative language use."}, {"heading": "7.2 Extrinsic evaluation methods", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "7.3 Black box vs glass box evaluation", "text": "Except for assessments of specific modules or algorithms, as in the case of reg or surface creators, most of the evaluation studies discussed so far would be classified as \"black box\" assessments of end-to-end systems. In a \"glass box\" assessment, however, it is the contribution of individual components that is scrutinized, ideally in a constellation in which versions of a system with and without components are evaluated in the same way. Note that the distinction between black box and glass box evaluation is orthogonal to answer the question of which methods are used. An excellent example of a glass box evaluation is Callaway and Lester (2002), who used an ablative design to make judgments about the quality of the output of their narrative-generating system based on different configurations that omit or include key components."}, {"heading": "7.4 On the relationship between evaluation methods", "text": "To what extent do the multiplicity of methods studied - from extrinsic, task-oriented to intrinsic, based on automatic metrics or human judgments - actually correlate? It turns out that multiple evaluation methods rarely make convergent judgments about a system or the relative ranking of a set of comparative systems."}, {"heading": "7.4.1 Metrics versus human judgements", "text": "Although corpus-based metrics used in mt and summaries are typically validated by showing their correlation with human assessments, metrics in these areas have shown that correspondence is somewhat weak (e.g., Dorr et al., 2004; Callison-Burch et al., 2006; Caporaso et al., 2008). Similarly, a similar problem with the way the system is expressed has been pointed out that corpus-based and experimental methods often do not correlate with humans (2010). In their recent review Bernardi et al al. (2016) note a similar problem with the evaluation of the system. Thus, Kulkarni et al. (2013) found that their image-description system does not perform two previous methods (Farhadi et al al al al al al al al., 2011) on Bleu scores; human judgments pointed to the opposite trend with readers preferring their system."}, {"heading": "7.4.2 Using controlled experiments", "text": "Some studies have validated the yardsticks against experimental data. Siddharthan and Katsos (2012), for example, compared the results of their size study (see Section 7.1 above) with the results of a sentence recall, and found that the results of the latter largely agree with the judgments and conclude that they can replace task-based assessments to illuminate breakdown of understanding at the sentence level. A handful of studies have also used behavioral experiments and compared \"online\" processing yardsticks, such as the reading time of reference expressions, with corpus-based metrics (e.g. Belz et al., 2010). Correlations with automatic metrics are usually bad. Lapata (2006) made a slightly different use of reading times, using them as an objective metric to validate Kendall \"s\" as a metric for assessing the order of information in text (an aspect of text stucturing)."}, {"heading": "7.5 Evaluation: Concluding remarks", "text": "Against the background of this section, three main conclusions can be drawn: 1. There is a widespread acceptance of the need to apply several evaluation methods in nlg. While these are not always consistent with each other, they are useful in illuminating various aspects of quality, from the fluidity and clarity of results to the adequacy of semantic content and effectiveness in achieving communicative intentions. The choice of method has direct implications for the way in which results can be interpreted. 2. Meta-evaluation studies have produced conflicting results regarding the relationship between human judgments, behavioral measures and automatically calculated metrics; the correlation between them differs depending on the task and scope of application. This is the subject of ongoing research, with many studies focusing on the reliability of metrics and their relationship to other metrics, in particular human judgments. 3. One question that remains under-explored concerns the dimensions of quality that are the subject of the investigation itself to which some lines are sought (that it is related to)."}, {"heading": "8 Discussion and future directions", "text": "Over the last two decades, the field of nlg has evolved considerably, and many of these recent advances have not yet been covered in a comprehensive survey, which has sought to bridge this gap with the following objectives: 1. to update core tasks and architectures in this area, with a focus on recent data-driven techniques; 2. to briefly highlight recent developments in relatively new areas, including the generation of vision-to-text and the generation of stylistically diverse, responsive, or creative texts; and 3. to discuss extensively the problems and prospects of evaluating nlg applications.During this survey, various general, related issues have emerged. Probably, the central issue has been the gradual shift from traditional, rule-based approaches to statistical, data-driven texts, which have, of course, occurred in general. In nlg, this has had a significant impact on how individual tasks are addressed (e.g. the shift away from domain-based approaches to domain-independent approaches that are based on available data)."}, {"heading": "8.1 Why (and how) should NLG be used?", "text": "More than a decade ago, Reiter and Dale (2000) recommended to the developer to ask this question before starting to design and implement a system.Can nlg really help in the target area? Is there a cheaper, more standardized solution and would it work just as well? From the perspective of an engineer or a company, these are obviously relevant questions. As current industry-specific applications of nlg show, this technology is usually valuable when information that needs to be presented to users is relatively extensive and in a form that is not easy to consume and not a simple mapping to a more user-friendly modality without significant transformation.At this point nlg comes into its own, offering a battery of techniques for selecting, structuring and presenting the information.However, the question arises whether nlg is worth using in a particular environment and not a simple mapping to a more user-friendly modality without significant transformation.Our survey focuses on text for generating text, but is always presented between large text structures and large text structures."}, {"heading": "8.2 NLG isn\u2019t about text-to-text. . . or is it?", "text": "In our introductory section, we distinguished text-to-text generation from datato-text generation; this study focused mainly on the latter. The two areas have distinctive features, not least the fact that nlg inputs tend to vary widely, as do the goals of nlg systems as a function of the domain under consideration. In contrast, input into text-to-text generation, especially auto-summary, is comparatively homogeneous, and although their goals can vary widely, the field has also been successful in defining tasks and datasets (for example, through duc shared tasks) that have set the standard for subsequent research. However, a closer look at the two types of generation will reveal more room for convergence than the above characterization suggests."}, {"heading": "8.3 Theories and models in search of applications?", "text": "In their entirety, they have concentrated in recent years in the areas of social policy, social policy, social policy, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and social and health systems, social and social and health systems, social and health systems, social and health systems, social and social and social and health systems, social and social and health systems, social and social and health systems, social and social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, and health systems, social and health systems, social and health systems, social and health systems, social and health systems, and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems, social and health systems,"}, {"heading": "9 Conclusion", "text": "Interest in automated text generation has grown significantly in recent years. Companies today offer nlg technology for a range of applications in areas such as journalism, weather and finance. The huge increase in available data and computing power, as well as the rapid development of machine learning, have created many new opportunities and motivated nlg researchers to explore a range of new applications related, for example, to image-to-text generation, while applications related to social media are imminent, such as the emergence of nlg-related techniques for automated content creation, and nlg for Twitter and chatbots (e.g. Dale, 2016). As developments continue at a steady pace and the technology finds their way into industrial applications, the future of the field seems promising. We believe that nlg research should be further strengthened through more collaboration with related disciplines."}, {"heading": "Acknowledgements", "text": "This work benefited greatly from discussions and comments with Grzegorz Chrupala, Robert Dale, Raquel Herva \u0301 s, Thiago Castro Ferreira, Ehud Reiter, Marc Tanti, Marie \ufffd t Theune, Kees van Deemter, Michael White and Sander Wubben. EK received support from RAAK-PRO SIA (2014-01-51PRO) and the Dutch Organization for Scientific Research (NWO 360-89-050), which is gratefully acknowledged."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper surveys the current state of the art in Natural Language Generation (nlg), defined as the task of generating text or speech from non-linguistic input. A survey of nlg is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of nlg technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in nlg and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between nlg and other areas of artificial intelligence; (c) draw attention to the challenges in nlg evaluation, relating them to similar challenges faced in other areas of nlp, with an emphasis on different evaluation methods and the relationships between them. 1 ar X iv :1 70 3. 09 90 2v 1 [ cs .C L ] 2 9 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}