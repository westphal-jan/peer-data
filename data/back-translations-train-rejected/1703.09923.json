{"id": "1703.09923", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "On Convergence Property of Implicit Self-paced Objective", "abstract": "Self-paced learning (SPL) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task, and then gradually take more complex examples into training. This new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proved that the SPL regime has a close relationship to a implicit self-paced objective function. While this implicit objective could provide helpful interpretations to the effectiveness, especially the robustness, insights under the SPL paradigms, there are still no theoretical results strictly proved to verify such relationship. To this issue, in this paper, we provide some convergence results on this implicit objective of SPL. Specifically, we prove that the learning process of SPL always converges to critical points of this implicit objective under some mild conditions. This result verifies the intrinsic relationship between SPL and this implicit objective, and makes the previous robustness analysis on SPL complete and theoretically rational.", "histories": [["v1", "Wed, 29 Mar 2017 07:53:43 GMT  (17kb)", "http://arxiv.org/abs/1703.09923v1", "9 pages, 0 figures"]], "COMMENTS": "9 pages, 0 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["zilu ma", "shiqi liu", "deyu meng"], "accepted": false, "id": "1703.09923"}, "pdf": {"name": "1703.09923.pdf", "metadata": {"source": "CRF", "title": "On Convergence Property of Implicit Self-paced Objective", "authors": ["Zilu Ma", "Shiqi Liu", "Deyu Meng"], "emails": ["dymeng@mail.xjtu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.09 923v 1 [cs.A I] Humans / animals learn lighter aspects of a learning task and then gradually incorporate more complex examples into the training. This new learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proven that the SPL regime has a close relationship to an implicit, self-determined objective function. Although this implicit goal could provide helpful interpretations for effectiveness, in particular the robustness, persistence among the SPL paradigms, there are still no theoretical results that strictly verify such a relationship. On this topic, we provide some convergence results to this implicit goal of the SPL in this paper. Specifically, we demonstrate that the SPL learning process always converges with critical points of this implicit goal, under some mild conditions. This result confirms the intrinsic relationship between SPL and this implicit goal, and makes the previous SPL rationality and robustness complete."}, {"heading": "1 Introduction", "text": "A variety of SPL implementation programs have been designed and empirically substantiated to be effective in various computer vision and pattern recognition tasks, such as object detector customization [10], specific class segmentation learning [2], visual category recognition [4], concept learning [5], long-term tracking [9], graph clipping [14], co-saliency detection [16], matrix factorization [17], face recognition [6] and multimedia event detection [13]. To explain the underlying effectiveness mechanism within SPL, a new theoretical understanding under the SPL scheme was first provided. Specifically, this work demonstrated that the Alternative Optimization Strategy (AOS) on SPL is implied with a majority minimization (MM) function."}, {"heading": "2 Related work", "text": "In this section, we first briefly present the definition of SPL and then outline its relationship to the implicit goal of NCRP."}, {"heading": "2.1 The SPL objective", "text": "In view of the training data collected {(xi, yi)} N i = 1, many machine learning problems must minimize the following form of objective function: J (w) = \u03c6\u03bb (w) + N \u2211 i = 1L (yi, g (xi, w)), where w-RD are variables to be solved, \u03c6\u03bb is a regulator parameter, L is the loss function and g (\u00b7, w) is the parameterized learning machine, such as a discriminatory or regression function. In order to improve robustness, in particular the negative influence of large noise outliers is avoided, whereby SPL imposes additional weights v = (v1, \u00b7, vn) on loss functions of all samples that can be adjusted by a self-controlled regulator (SP regulator). Here, each vi [0, 1] represents the extent to which the sample (xi, yi) is trained in the learning process."}, {"heading": "2.2 The implicit NCRP objective", "text": "Since the number of discontinuous points does not increase, the number of points is countable and consists only of the discontinuity of the jump. Therefore, v-v-v-v-v is integrable as an implicit target, and F\u03bb is absolutely continuous and concave. An interesting observation is that this implicit SPL target has a close relationship to NCRP, which is widely studied in machine learning and statistics, which provides a helpful explanation for the robust insight under SPL [7]. The AOS algorithm originally used to solve the SPL problem is enhanced by performing a coordinated calculation on E (w, v-v), i.e."}, {"heading": "3 The main convergence result", "text": "In fact, the proof of the convergence of the MM algorithm is basically the same as that of the EM algorithm (see [12]) only with some obvious changes as discussed in [11]. And the convergence of EM and MM is indeed a logical consequence of a global convergence theory of Zangwill (see [15]). We can generalize the proof in the case of the variable analysis. First, we need to clarify some terms that can be referred to in [8]. A function f: Rd \u2192 R is called low semi-continuous or simply lsc iflevf6\u03b1: = {x) 6 \u03b1} is closed to all other terms."}, {"heading": "4 Conclusion", "text": "In this paper, we have demonstrated that the learning process of the traditional SPL regime is guaranteed to converge to rational critical points of the corresponding implicit CRP target. This theory contributes to confirming the intrinsic relationship between SPL and this implicit target, and thus confirms previous SPL robustness analyses based on understanding of such a relationship. In addition, we have used some new theoretical capabilities for demonstrating convergence that tend to be useful to some extent to previous MM and EM convergence theories. Conflict of Interest The authors declare that they have no conflict of interest."}, {"heading": "Appendix A Proof of Theorem 1", "text": "Theorem 1 is actually a consequence of a stronger version of Zangwill's global convergence theorem (15, page 91). We must first consider the following lemmas.Lemma 1. If f (xn) = lim k, and {f (xn)} is not increasing, then f (xn) = lim n (xn). Proof.f (x) = lim inf (xn) = lim k, and (xn) is not increasing, then f (xn) = lim n, n (xn). Proof.f (xn) 2. Suppose that X is an end-dimensional Euclidean space, M is a set value mapping from X to X, and that {xk} is produced by M, the meansxk + 1 (xk), is a subset of X that we call \"set solution.\""}, {"heading": "Appendix B Proof of Theorem 3", "text": "Similar to [8, 5.41], we give the following definition: Definition k = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K: K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K: K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K"}], "references": [{"title": "Easy samples first: self-paced reranking for zeroexample multimedia search", "author": ["L. Jiang", "D. Meng", "T. Mitamura", "A. Hauptmann"], "venue": "In ACM MM,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Learning the easy things first: Self-paced visual category discovery", "author": ["Y. Lee", "K. Grauman"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Lee and Grauman.,? \\Q2010\\E", "shortCiteRegEx": "Lee and Grauman.", "year": 2010}, {"title": "What Objective Does Self-paced Learning Indeed Optimize", "author": ["D. Meng", "Q. Zhao", "L. Jiang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Meng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2017}, {"title": "Self-paced learning for long-term tracking", "author": ["J. Supan\u010di\u010d III", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "III and Ramanan.,? \\Q2013\\E", "shortCiteRegEx": "III and Ramanan.", "year": 2013}, {"title": "Shifting weights: Adapting object detectors from image to video", "author": ["K. Tang", "V. Ramanathan", "F. Li", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Tang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2012}, {"title": "Parameter convergence for EM and MM algorithms", "author": ["Florin Vaida"], "venue": "Statistica Sinica, pages 831\u2013840,", "citeRegEx": "Vaida.,? \\Q2005\\E", "shortCiteRegEx": "Vaida.", "year": 2005}, {"title": "On the convergence properties of the EM algorithm", "author": ["CF Jeff Wu"], "venue": "The Annals of statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "CMU-Informedia@TRECVID 2014 multimedia eventdetection (MED)", "author": ["S. Yu", "L. Jiang", "Z. Mao"], "venue": "In TRECVID Video Retrieval Evaluation Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Semi-supervised learning through adaptive laplacian graph trimming", "author": ["Zongsheng Yue", "Deyu Meng", "Juan He", "Gemeng Zhang"], "venue": "Image and Vision Computing,", "citeRegEx": "Yue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2016}, {"title": "Nonlinear programming: a unified approach. Prentice-Hall international series in management", "author": ["W.I. Zangwill"], "venue": null, "citeRegEx": "Zangwill.,? \\Q1969\\E", "shortCiteRegEx": "Zangwill.", "year": 1969}, {"title": "A self-paced multiple-instance learning framework for co-saliency detection", "author": ["Dingwen Zhang", "Deyu Meng", "Chao Li", "Lu Jiang", "Qian Zhao", "Junwei Han"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Self-paced learning for matrix factorization", "author": ["Qian Zhao", "Deyu Meng", "Lu Jiang", "Qi Xie", "Zongben Xu", "Alexander G Hauptmann"], "venue": "In AAAI,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Self-paced learning (SPL) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task, and then gradually take more complex examples into training. This new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proved that the SPL regime has a close relationship to a implicit self-paced objective function. While this implicit objective could provide helpful interpretations to the effectiveness, especially the robustness, insights under the SPL paradigms, there are still no theoretical results strictly proved to verify such relationship. To this issue, in this paper, we provide some convergence results on this implicit objective of SPL. Specifically, we prove that the learning process of SPL always converges to critical points of this implicit objective under some mild conditions. This result verifies the intrinsic relationship between SPL and this implicit objective, and makes the previous robustness analysis on SPL complete and theoretically rational.", "creator": "LaTeX with hyperref package"}}}