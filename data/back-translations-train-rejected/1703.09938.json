{"id": "1703.09938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Grouped Convolutional Neural Networks for Multivariate Time Series", "abstract": "Analyzing multivariate time series data is important for many applications such as automated control, fault diagnosis and anomaly detection. One of the key challenges is to learn latent features automatically from dynamically changing multivariate input. In visual recognition tasks, convolutional neural networks (CNNs) have been successful to learn generalized feature extractors with shared parameters over the spatial domain. However, when high-dimensional multivariate time series is given, designing an appropriate CNN model structure becomes challenging because the kernels may need to be extended through the full dimension of the input volume. To address this issue, we present two structure learning algorithms for deep CNN models. Our algorithms exploit the covariance structure over multiple time series to partition input volume into groups. The first algorithm learns the group CNN structures explicitly by clustering individual input sequences. The second algorithm learns the group CNN structures implicitly from the error backpropagation. In experiments with two real-world datasets, we demonstrate that our group CNNs outperform existing CNN based regression methods.", "histories": [["v1", "Wed, 29 Mar 2017 09:05:40 GMT  (8196kb,D)", "https://arxiv.org/abs/1703.09938v1", null], ["v2", "Fri, 31 Mar 2017 05:18:33 GMT  (8196kb,D)", "http://arxiv.org/abs/1703.09938v2", null], ["v3", "Tue, 4 Apr 2017 06:05:50 GMT  (8196kb,D)", "http://arxiv.org/abs/1703.09938v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["subin yi", "janghoon ju", "man-ki yoon", "jaesik choi"], "accepted": false, "id": "1703.09938"}, "pdf": {"name": "1703.09938.pdf", "metadata": {"source": "CRF", "title": "Grouped Convolutional Neural Networks for Multivariate Time Series", "authors": ["Subin Yi", "Janghoon Ju", "Man-Ki Yoon", "Jaesik Choi"], "emails": ["<jaesik@unist.ac.kr>."], "sections": [{"heading": "1. Introduction", "text": "The analysis of multiple variables that make up such systems is therefore increasingly important for many applications such as automated control, fault diagnosis and anomaly detection. In complex systems, one of the most important requirements is to maintain the integrity of sensor data so that it is trustworthy and analyzed by moni-1Ulsan National Institute of Science and Technology, Ulsan, 44919, Republic of Korea 2University of Illinois at UrbanaChampaign, IL 61801. Correspondence to: Jaesik Choi < jaesik @ unist.ac.kr >.tored. Previously, sensor integrity was analyzed through feedback controls (Mo & Sinopoli, 2015; Pajic et al.) and non-parametric Bayesian methods (Krause et al, 2008)."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Convolutional Neural Network", "text": "A Convolutionary Neural Network (CNN) is a multi-layer artificial neural network that has successfully detected visual patterns. Conventionally, the most common architecture of CNNs consists of a stack of three types of multiple layers: Convolutionary Layer, Sub-Sampling Layer, and Fully Connected Layer. Conventionally, a CNN consists of alternative layers of Constitutional Layers and Sub-Sampling Layers on the lower and several fully connected layers after them. First, a unit of a Constitutional Layer receives inputs from a series of adjacent nodes of the previous layer in a similar manner with animal visual cortex cells. Local weights of Constitutional Layers are shared with the nodes in the same layer. Such local calculations in the layer reduce memory load and improve classification performance. No linear layer sappling, which is the second layer of the CNN layer, is the Convolutionary Layer after Lex."}, {"heading": "2.2. Recurrent Convolutional Neural Network (RCNN)", "text": "Recurrent Convolutional Neural Network (RCNN) is a type of CNN in which the revolutionary layers are replaced by recurrent revolutionary layers. It improves the expressiveness of the revolutionary layer by using multiple revolutionary layers that share the parameters. RCNN has been applied not only to the image processing problem (Liang & Hu, 2015; Pinheiro & Collobert, 2014), but also to other tasks that require time analysis (Lai et al., 2015). RCNN can effectively extract invariant features in the temporal domain in relation to the time series data as two-dimensional data in one of the dimensions 1. With one of the dimensions, RCNN is able to extract invariant features in the temporal domain effectively."}, {"heading": "2.2.1. RECURRENT CONVOLUTIONAL LAYER", "text": "The recurring revolutionary layer (RCL), which is the most representative building block of an RCNN, is the composition of the l intermediate revolutionary layers that share the same parameters; the first revolutionary layer of an RCL carries the convolution on the input x, which results in output \u03c3 (W * x), whereas W is the revolutionary filter, * is a convolution operator, and \u03c3 (\u00b7) is an activation function; then the next revolutionary layer recursively processes the summation of the original input and output of the previous layer, x + \u03c3 (W \u0445 x), as input; after some iterations of this process, an RCL gives the result of the last intermedial revolutionary layer as its output. During the error backpropagation, the parameters l are updated several times. In each update, the parameters are changed to correct the error that was made by itself as a consistent layer."}, {"heading": "2.3. Spectral Clustering", "text": "The aim of aggregating data x1,... xN is to divide the data into some groups, so that the points in the same group are very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the same group is very different. (...) The aggregation of data in the group is very different. (...) The aggregation of data in the group is very different. (...) The aggregation of data in the group is very different."}, {"heading": "3. Grouped Time Series", "text": "In this section, we present two algorithms for building the CNN group structure: The first method explicitly builds the group structure from spectral clustering, and the second method builds the group structure by backpropagating errors."}, {"heading": "3.1. Learning the Structure by Spectral Clustering", "text": "Our CNN Structure group receives both the input variables X = [x1,..., xN] and its cluster information C = [c1,..., cN], where ci represents the affiliation to the xi variable. In contrast to ordinary revolutionary layers, the group-revolutionary layers divide the input volume based on the ci cluster affiliation and perform the convolution operations via the input variables belonging to the same cluster as in Figure 2. Formally, the k-th group of layer H is defined as: Hk = \u03c3 (\u2211 iW k \u00b7 xi + bk) (3.1.1), where (\u00b7) the convolution requires operations that i-j-vanter, {1,..., N}, cj = k}, which is the weight matrix, and bk is the bias vector of the k-th model. (As in the CNN models, (\u00b7) the input variable kavableX is processed via multiple revolutionary layers."}, {"heading": "3.2. Neural Networks with Clustering Coefficient", "text": "Assuming that the input time series are correlated with each other, we explicitly group these variables to take advantage of such correlations as CNN uses the local connectivity of an image. It can be considered to find the local connectivity and correlations within the channels of CNN. Given an input file consisting of N variables in which each variable is D-dimensional real weighted vectors, i.e. X = [x1,..., xN], we would like to group these variables into K clusters, which have a matrix U = [ui, j; i {1,..., N}, j {1, K}], where ui, j [0, 1], jp [K], j = 1, whose element Erui, j is the cluster coefficient representing the part of the j-th cluster, assumes the variable xi as multidimensional distribution."}, {"heading": "4. Related Work", "text": "Consequently, there is research on the application of these methods to the analysis of complex multivariate systems. Neural networks, which consist only of fully connected layers, are not suitable for handling sequential data because they have to process the entire input sequence. Specifically, such networks are too inefficient in terms of both memory usage and learning efficiency.One of the common choices for processing time series is a recursive neural network (RNN).An RNN (Williams & Zipser, 1989; Funahashi & Nakamura, 1993) processes sequential data with recursive connections to represent transitional models over time.In order for it to store temporal information within the network, RNN models are successfully used for processing sequential data (Mozer, 1993; Pascanu et al, 1993; Koutnik et al, 2014)."}, {"heading": "5. Experimental Results", "text": "In experiments, we compare the regression performance of several CNN-based models on two high-dimensional real-world multivariate datasets, groundwater level data and drone flight data. Groundwater data and drone flight data have 88 and 148 variables, respectively."}, {"heading": "5.1. Settings", "text": "To evaluate regression performance, we randomly selected one of the variables, say xp, from the dataset and constructed the values of the variables as a target at the time t, y = xp (t) by seeing their correlated variable values from time t \u2212 T to twithout including the variables, i.e. X = i, i6 = p [xi (t \u2212 T),..., xi (t)]. We trained our models with 90% of the entire dataset and tested the other 10%. Subsequently, regression performance was compared with other regression models: linear regression, ridge regression, CNN and RCNN. Regression performance was measured on the scale of the standardized root mean square (SRMSE), which is defined as equation 5.1.1 when t is the mean of the target vector t.SRMSE = 5.1 (tt \u2212 yt) 2 / NSE (5.1.1) SE = 1.0 (1.2)."}, {"heading": "5.2. Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1. GROUNDWATER DATA", "text": "We used daily collected groundwater data from the United States Geological Survey (USGS) 1. The dataset is composed of various parameters from the U.S. territories, and we used depth and water levels from regions other than Hawaii and Alaska. Regions were selected where the data was collected over a 28-year period (1987-2015), and those where unrecorded periods lasted longer than two months were excluded. Empty datasets shorter than two months were filled by interpolation."}, {"heading": "5.2.2. DRONE DATA", "text": "Quadcopters are aerodynamically unstable and their actuators, i.e. motors, must be controlled directly from an on-board computer for a stable flight. We used the Pixhawk2 as autopilot hardware for our quadcopter. It features on-board sensors such as Inertialmea-1https: / / waterdata.usgs.gov 2https: / / pixhawk.org / surement unit (IMU), compass and barometer. We run the open source autopilot software PX4 suite3 on the ARM Cortex M4F processor on the Pixhawk. It combines sensor data and flight commands to calculate correct outputs to the motors, which then control the orientation and position of the vehicle. We collected flight data from the quadcopter using the PX4 logging system. Each flight data and flight commands to calculate the correct outputs to the motors, determine the flight mode, launch points and other flight positions."}, {"heading": "5.3. Results", "text": "We have built the CNN and RCNN groups using both the spectral cluster method (explicitly) and the cluster coefficient method (coeff) and compared the performance with corresponding vanilla CNNs and vanilla RCNNs. CNN's deep model architecture is illustrated in Table 1. All learning parameters such as learning rate and weight3http: / px4.io / initialization parameters are compared. Each model has been trained for 200 epochs and the best results have been chosen.The results of our experiments are shown in the following tables, Table 2 and Table 3. Generally, our group CNN models perform better in the groundwater dataset. RCNN with the cluster coefficient model performs best with 0.754 SRMSE compared to 0.985 vanilla RCNN models. Our group CNN models also tend to perform better than the vanilla CNN models in the drone flight dataset. RCNN with the spectral model performs best with MS40.38 compared to MSE with R46SE."}, {"heading": "6. Conclusion", "text": "In this paper, we presented two structure learning algorithms for deep CNN models. Our algorithms used the covariance structure over several time series to divide the input volume into groups; the first algorithm learned the group CNN structures explicitly by clustering individual input sequences; the second algorithm learned the group CNN structures implicitly from error backpropagation; and in experiments with two real data sets, we showed that our group CNN models outperformed existing CNN-based regression methods."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Bo Liu from the Intelligent Robotics Laboratory, University of Illinois, for his help in collecting drone sensor data and anonymous reviewers for their helpful and constructive comments. This work was funded by the National Research Foundation of Korea (NRF), supported by the Korean Government (Ministry of Science, ICT & Future Planning, MSIP) (no. 2014M2A8A2074096) and the POSCO grant (no. 2016X043)."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "In Proceedings of IEEE international conference on Acoustics, speech and signal processing,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Learning spectral clustering", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "In Proceedings of Advances in neural information processing systems,", "citeRegEx": "Bach and Jordan,? \\Q2004\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2004}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "In Proceedings of IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological cybernetics,", "citeRegEx": "Bourlard and Kamp,? \\Q1988\\E", "shortCiteRegEx": "Bourlard and Kamp", "year": 1988}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceeding of the International Conference on Machine learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Nonstationary hydrological time series forecasting using nonlinear dynamic methods", "author": ["P. Coulibaly", "C.K. Baldwin"], "venue": "Hydrology, 307:164\u2013174,", "citeRegEx": "Coulibaly and Baldwin,? \\Q2005\\E", "shortCiteRegEx": "Coulibaly and Baldwin", "year": 2005}, {"title": "Approximation of dynamical systems by continuous time recurrent neural networks", "author": ["K.I. Funahashi", "Y. Nakamura"], "venue": "Neural networks,", "citeRegEx": "Funahashi and Nakamura,? \\Q1993\\E", "shortCiteRegEx": "Funahashi and Nakamura", "year": 1993}, {"title": "Understanding the difficulty of training deep feedforward neural networks. Aistats", "author": ["X. Glorot", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Technical report,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Interpreting neural-network results: a simulation study", "author": ["O. Intrator", "N. Intrator"], "venue": "Computational statistics & data analysis,", "citeRegEx": "Intrator and Intrator,? \\Q2001\\E", "shortCiteRegEx": "Intrator and Intrator", "year": 2001}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Machine Learning Research,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liang and Hu,? \\Q2015\\E", "shortCiteRegEx": "Liang and Hu", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "A tutorial on spectral clustering", "author": ["U.V. Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "Luxburg,? \\Q2007\\E", "shortCiteRegEx": "Luxburg", "year": 2007}, {"title": "Long short term memory networks for anomaly detection in time series", "author": ["P. Malhotra", "L. Vig", "G. Shroff", "P. Agarwal"], "venue": "In Proceedings of European Symposium on Artificial Neural Networks, pp", "citeRegEx": "Malhotra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malhotra et al\\.", "year": 2015}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cirean", "J. Schmidhuber"], "venue": "In Proceedings of International Conference on Artificial Neural Networks,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Secure estimation in the presence of integrity attacks", "author": ["Mo", "Yilin", "Sinopoli", "Bruno"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Mo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mo et al\\.", "year": 2015}, {"title": "Induction of multiscale temporal structure", "author": ["M.C. Mozer"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Mozer,? \\Q1993\\E", "shortCiteRegEx": "Mozer", "year": 1993}, {"title": "Deep convolutional and lstm recurrent neural networks for multimodal wearable activity", "author": ["F.J. Ordez", "D. Roggen"], "venue": "recognition. Sensors,", "citeRegEx": "Ordez and Roggen,? \\Q2016\\E", "shortCiteRegEx": "Ordez and Roggen", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "In Proceedings of the 30 th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P. Pinheiro", "R. Collobert"], "venue": "In Proceedings of the 31 st International Conference on Machine Learning,", "citeRegEx": "Pinheiro and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Pinheiro and Collobert", "year": 2014}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "In Proceedings of Advances in neural information processing systems,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "In Proceedings of IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Between min cut and graph bisection", "author": ["D. Wagner", "F. Wagner"], "venue": null, "citeRegEx": "Wagner and Wagner,? \\Q1993\\E", "shortCiteRegEx": "Wagner and Wagner", "year": 1993}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. Williams", "D. Zipser"], "venue": null, "citeRegEx": "Williams and Zipser,? \\Q1989\\E", "shortCiteRegEx": "Williams and Zipser", "year": 1989}, {"title": "Deep convolutional neural networks on multichannel time series for human activity recognition", "author": ["J.B. Yang", "M.N. Nguyen", "P.P. San", "X.L. Li", "Krishnaswamy", "Sh"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["R.S. Zemel"], "venue": "Proceedings of the Neural Information Processing Systems Conference,", "citeRegEx": "Zemel,? \\Q1994\\E", "shortCiteRegEx": "Zemel", "year": 1994}], "referenceMentions": [{"referenceID": 11, "context": ") and nonparametric Bayesian methods (Krause et al., 2008).", "startOffset": 37, "endOffset": 58}, {"referenceID": 31, "context": "Autoencoders (Bourlard & Kamp, 1988; Zemel, 1994) train model parameters in an unsupervised manner by specifying the same input and output values.", "startOffset": 13, "endOffset": 49}, {"referenceID": 18, "context": "The first structure learning algorithm learns the CNN structure explicitly by clustering input sequences with spectral clustering (Luxburg, 2007).", "startOffset": 130, "endOffset": 145}, {"referenceID": 15, "context": "By reducing the dimensionality, it reduces the local sensitivity of the network and computational complexity (LeCun & Bengio, 1995; LeCun et al., 1998).", "startOffset": 109, "endOffset": 151}, {"referenceID": 15, "context": "CNN has been very effective for solving many computer vision problems such as classification (LeCun et al., 1998; Krizhevsky et al., 2012), object detection and semantic segmentation (Ren et al.", "startOffset": 93, "endOffset": 138}, {"referenceID": 12, "context": "CNN has been very effective for solving many computer vision problems such as classification (LeCun et al., 1998; Krizhevsky et al., 2012), object detection and semantic segmentation (Ren et al.", "startOffset": 93, "endOffset": 138}, {"referenceID": 26, "context": ", 2012), object detection and semantic segmentation (Ren et al., 2015; Long et al., 2015).", "startOffset": 52, "endOffset": 89}, {"referenceID": 17, "context": ", 2012), object detection and semantic segmentation (Ren et al., 2015; Long et al., 2015).", "startOffset": 52, "endOffset": 89}, {"referenceID": 10, "context": "It has been also applied to other problems such as natural language processing (Kalchbrenner et al., 2014; Collobert & Weston, 2008).", "startOffset": 79, "endOffset": 132}, {"referenceID": 30, "context": "Recently, variants of CNN are applied to analyzing various kinds of time-series such as sensor values and EEG (electroencephalogram) signals (Yang et al., 2015; Ordez & Roggen, 2016).", "startOffset": 141, "endOffset": 182}, {"referenceID": 13, "context": "RCNN has been applied to not only the image processing problem (Liang & Hu, 2015; Pinheiro & Collobert, 2014) but also other tasks that require temporal analysis (Lai et al., 2015).", "startOffset": 162, "endOffset": 180}, {"referenceID": 8, "context": "RCL can also be regarded as a skip-layer connection (He et al., 2015; Intrator & Intrator, 2001).", "startOffset": 52, "endOffset": 96}, {"referenceID": 2, "context": "The main motivation is that the deeper networks show a better performance in many cases but they are also harder to train in actual applications due to vanishing gradients and degradation problem (Bengio et al., 1994; Glorot & Bengio, 2010).", "startOffset": 196, "endOffset": 240}, {"referenceID": 8, "context": "(He et al., 2015) designed such layers with skip-layer connection, named as residual learning.", "startOffset": 0, "endOffset": 17}, {"referenceID": 18, "context": "Unfortunately, introducing the additional term to the mincut problem has proven to make the problem NP-hard (Wagner & Wagner, 1993) so (Luxburg, 2007; Bach & Jordan, 2004) solves the relaxed problem, which gives the solution H that consists of the eigenvectors corresponding to the K smallest eigenvalues of the matrix Lrw := D\u22121L or the K smallest generalized eigenvectors of Lu = \u03bbDu.", "startOffset": 135, "endOffset": 171}, {"referenceID": 22, "context": "RNN models have been successfully used for processing sequential data (Mozer, 1993; Pascanu et al., 2013; Koutnik et al., 2014).", "startOffset": 70, "endOffset": 127}, {"referenceID": 24, "context": "RNN models have been successfully used for processing sequential data (Mozer, 1993; Pascanu et al., 2013; Koutnik et al., 2014).", "startOffset": 70, "endOffset": 127}, {"referenceID": 19, "context": "(Coulibaly & Baldwin, 2005) used dynamic RNN to forecast nonstationary hydrological time-series and (Malhotra et al., 2015) used stacked LSTM network as a predictor over a number of time steps and detected anomalies that has high prediction error in time series.", "startOffset": 100, "endOffset": 123}, {"referenceID": 0, "context": "(Abdel-Hamid et al., 2012) used CNN for speech recognition problem and (Zheng et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "Recurrent Convolutional Neural Network (RCNN), which can be considered as a variant of a CNN, is recently proposed and shows state-of-the-art performance on classifying multiple time series (Pinheiro & Collobert, 2014; Liang & Hu, 2015; Lai et al., 2015).", "startOffset": 190, "endOffset": 254}], "year": 2017, "abstractText": "Analyzing multivariate time series data is important for many applications such as automated control, fault diagnosis and anomaly detection. One of the key challenges is to learn latent features automatically from dynamically changing multivariate input. In visual recognition tasks, convolutional neural networks (CNNs) have been successful to learn generalized feature extractors with shared parameters over the spatial domain. However, when high-dimensional multivariate time series is given, designing an appropriate CNN model structure becomes challenging because the kernels may need to be extended through the full dimension of the input volume. To address this issue, we present two structure learning algorithms for deep CNN models. Our algorithms exploit the covariance structure over multiple time series to partition input volume into groups. The first algorithm learns the group CNN structures explicitly by clustering individual input sequences. The second algorithm learns the group CNN structures implicitly from the error backpropagation. In experiments with two realworld datasets, we demonstrate that our group CNNs outperform existing CNN based regression methods.", "creator": "LaTeX with hyperref package"}}}