{"id": "1703.10039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2017", "title": "Cohesion-based Online Actor-Critic Reinforcement Learning for mHealth Intervention", "abstract": "In the wake of the vast population of smart device users worldwide, mobile health (mHealth) technologies are hopeful to generate positive and wide influence on people's health. They are able to provide flexible, affordable and portable health guides to device users. Current online decision-making methods for mHealth assume that the users are completely heterogeneous. They share no information among users and learn a separate policy for each user. However, data for each user is very limited in size to support the separate online learning, leading to unstable policies that contain lots of variances. Besides, we find the truth that a user may be similar with some, but not all, users, and connected users tend to have similar behaviors. In this paper, we propose a network cohesion constrained (actor-critic) Reinforcement Learning (RL) method for mHealth. The goal is to explore how to share information among similar users to better convert the limited user information into sharper learned policies. To the best of our knowledge, this is the first online actor-critic RL for mHealth and first network cohesion constrained (actor-critic) RL method in all applications. The network cohesion is important to derive effective policies. We come up with a novel method to learn the network by using the warm start trajectory, which directly reflects the users' property. The optimization of our model is difficult and very different from the general supervised learning due to the indirect observation of values. As a contribution, we propose two algorithms for the proposed online RLs. Apart from mHealth, the proposed methods can be easily applied or adapted to other health-related tasks. Extensive experiment results on the HeartSteps dataset demonstrates that in a variety of parameter settings, the proposed two methods obtain obvious improvements over the state-of-the-art methods.", "histories": [["v1", "Sat, 25 Mar 2017 23:01:20 GMT  (143kb,D)", "https://arxiv.org/abs/1703.10039v1", null], ["v2", "Wed, 23 Aug 2017 22:40:49 GMT  (148kb,D)", "http://arxiv.org/abs/1703.10039v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["feiyun zhu", "peng liao", "xinliang zhu", "yaowen yao", "junzhou huang"], "accepted": false, "id": "1703.10039"}, "pdf": {"name": "1703.10039.pdf", "metadata": {"source": "META", "title": "Cohesion-based Online Actor-Critic Reinforcement Learning for mHealth Intervention", "authors": ["Feiyun Zhu", "Peng Liao", "Xinliang Zhu", "Yaowen Yao", "Junzhou Huang"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Stakeholder Criticism, Strengthening Learning, Mobile Health (mHealth) Intervention, Cohesion"}, {"heading": "1 INTRODUCTION", "text": "With billions of smartphones and wearable devices such as Fitbit Fuelband and Jawbone, etc. [9] it is becoming increasingly popular among scientists to leverage modern artificial intelligence and mobile health technologies to leverage supercomputers and big data to facilitate prediction of health tasks [1-7]. However, in this paper, the goal of mobile health (mHealth) is to use various smart platforms to collect and analyze raw data (weather, location, social activity, stress, etc.) to provide effective interventions that help users adapt to healthy behaviors, such as reducing alcohol abuse, Xinliang Zhou's, the prevalence of drugs, Yaowen Yao and Junzhou Huang are associated with the Department of CSE at the University of Texas at Arlington. Feiyun Zhu and Peng Liao are with the Department of Statistics at the University of Michigan.1 i.e."}, {"heading": "2 PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Markov Decision Process (MDP)", "text": "We assume that the mHealth intervention is a Markov Decision Process (MDP = 23-26] consisting of a 5-fold (S, A, P, R), where S is the state space and A is the space for action. P: S \u00b7 A \u00b7 S 7 \u2192 [0, 1] is the state transition model, in which P (s, s \u00b2) indicates the probability of the transition from one state to another s \"s\" after action; R (s, s \u00b2) is the corresponding immediate reward for such a transition, in which S \u00b7 A \u00b7 S 7 \u2192 R For the simplification, the expected immediate reward dR (s, a) = It is a) = It is a Vantage-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-old-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-age-"}, {"heading": "2.2 Bellman Equation and Q-value Estimation", "text": "It is well known that the Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "2.4 The actor updating for policy improvement", "text": "In mHealth, the reference distribution of the states dref (s) is unknown and difficult to estimate due to the lack of samples. We use dref (s) as an empirical distribution of the states. Accordingly, the observations in the trajectory, i.e. Dn, are used to form the target for the actualization of the actor."}, {"heading": "3 NETWORK COHESION BASED ONLINE ACTORCRITIC RL", "text": "It is a well-known phenomenon observed in many social behavior studies [32, 33] that people are largely connected in a network and networked users tend to have similar behaviors. Advances in social media greatly help to capture relational information among users, ensuring the availability of network information for health-related studies. Moreover, due to similar characteristics such as age, gender, race, religion, education level, work, income, other socioeconomic status, medical records, and genetic traits, etc. [22] However, current online methods for mHealth simply assume that users are completely different; they do not share information among users and learn a separate RL for each user by using only his or her data. Such an assumption works well in the ideal state in which the sample drawn by each user is large to support separate online learning. However, while the data is abundant for all users, the data for users is not sufficient to ensure optimal learning situations, but to enhance an optimal learning situation."}, {"heading": "3.1 Construct the network cohesion by using the warm start trajectory (WST)", "text": "We assume that there is an undirected network conversion, i.e. that it is a reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, ioactionary, reactionary, reactionary, ioactionary, reactionary, ioactionary, reactionary, ioactionary, reactionary, ioactionary, reactionary, reactionary, reactionary, reactionary, ioactionary, ioactionary, reactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, reactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, reactionary, ioactionary, ioactionary, ioactionary, reactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, reactionary, ioactionary, ioactionary, ioactionary, reactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, reactionary, reactionary, reactionary, reactionary, reactionary, ioactionary, ioactionary, reactionary, ioactionary, ioactionary, reactionary, reactionary, ioactionary, reactionary, ioactionary, ioactionary, ioactionary, ioactionary, ioactionary, reactionary, reactionary, ioactionary, ioactionary, ioactionary, reactionary, ioactionary, reactionary,"}, {"heading": "3.2 Model of cohesion based Actor-Critic RL", "text": "The underlying assumption in this paper is that when two users are interconnected, their values and strategies are similar to how we do it, for example: \"We have the objective function for the critics.\" \"We have the objective function for the critics.\" \"The objective function for the critics.\" \"The objective function for the critics.\" \"The objective function for the critics.\" \"The objective function for the critics.\" \"\" The objective function for the critics. \"\" \"The objective function.\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"..\" \"\" \"\".. \"\" \"\".. \"\" \"..\" \"\".. \"\" \"\".. \"\" \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"\".. \"\""}, {"heading": "4 ALGORITHM#1 FOR THE CRITIC UPDATE", "text": "4.1 Current rules for the Projection Step (13) We discuss first how we can minimize the goal for the Projection Step (12). The goal is J = N \u00b2 n = 1 - 2 - 2 - 3 - 4 - 4 - 4 - 4 - 4 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5"}, {"heading": "5 ALGORITHM#2 FOR THE CRITIC UPDATE", "text": "In this section, we provide another rule for updating the critique (i.e. improving policy). Note that to avoid overadjustment for very small sample sizes, the conventional LSTDQ usually applies the \"2 constraint for variable H. It does not use the\" 2 constraint for the fixed point variable W [30, 47] in the projection step. Following this idea, we have a simpler objective function for updating the critique aswn = h-n = arg min hn. Ui-Dn-xi hn \u2212 (ri + \u03b3y iwn) 2-2, (25) for n-2 {1, \u00b7 \u00b7, N} and s.t. N-i, j = 1cijd (hi, hj). According to the derivation in section 4.1, in which the Frobenius standard is considered a smooth constraint, the updating rule for the project step c (point) (ix.1) is 0.1 (0.1).W (in Fix.1)."}, {"heading": "6 EXPERIMENT RESULTS", "text": "We verify the proposed methods on the HeartSteps dataset. It has two possibilities for an action, namely {0, 1}, where a = 1 means sending the positive intervention, while a = 0 means no intervention [3]. Specifically, it is assumed that the stochastic policy takes the form \u03c0\u03b8 (a | s) = exp (s, a) = a \"exp [s, a\"] [s, a \"] where \u03b8\" Rm \"is the unknown parameter and \u03c6 (\u00b7, \u00b7) is a characteristic process that combines the information in actions and states, i.e. \u03c6 (s, a) = [as, a] Rm."}, {"heading": "6.1 The HeartSteps Dataset", "text": "In order to verify the performance of our method, we use a data set from a mobile health study called HeartSteps [44] to approximate the generative model. This is a 42-day mHealth intervention aimed at increasing the steps users take each day by offering positive treatments (i.e. interventions) adapted to the current status of users, such as recommending to take a walk after prolonged sitting [44], or doing some exercises after work. The initial state is derived from the Gaussian distribution S0 Vacuum Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facili"}, {"heading": "6.2 Compared Methods and Parameter Settings", "text": "There are three actor-critic methods for comparison: (a) Separate RL = = 3900, which is an extension of the contextual bandit of the online actor-critic in [1] to amplify the online actor-critic. (b) CohesionRL # 1 is the first version of our method. (c) Cohesion RL # 2 is the second version of our method (cf. Algorithm 1 for detail). Spably, Cohesion RL # 1 and Cohesion RL # 2 share the same actor. (T) 1,34ate RL # 2 is the second version of our method (cf. Algorithm 1 for detail)."}, {"heading": "6.3 Evaluation Metrics", "text": "We use the expectation of a long-term average reward (ElrAR) to quantify the quality of the estimated insurance policy (n = 45 persons). At this point, we summarize the insurance policies for all 45 users in which the insurance policy (i.e. MDPs) is the n-th user. Intuitively, ElrAR measures how much average reward we could get in the long term by using the learned insurance policy (i.e. MDPs) from the test users, for example, how much alcohol users have in a specified period in the alcohol consumption study [8, 9]. In HeartSteps in particular, ElrAR measures the average steps that users take per day over a long period; a larger ElrAR corresponds to better performance. The average reward for the n-th user, i.e. the n-th user, i.e., is calculated by ensuring the rewards over the last 2,000 elements of a T = 1 000 in 1 of 1 000 policies."}, {"heading": "6.4 Comparisons in three experiment settings", "text": "This year, the number of elderly and elderly people has multiplied in recent years, by more than 20%."}, {"heading": "7 CONCLUSIONS AND DISCUSS", "text": "This paper is a first attempt to apply online actor-critic reinforcement learning to mHealth. According to current methods, which learn a separate policy for each user, the Separate RL cannot achieve satisfactory results. This is because the data for each user is very limited to support separate learning, leading to unstable strategies that contain many variations. Taking into account the universal phenomenon that users are generally connected to a network and networked users tend to have similar behaviors, we propose network cohesion-based actorcritical reinforcement learning for mHealth. It is able to share the information among similar users to transform the limited user information into sharper learned strategies. Extensive experiments show that our methods significantly exceed the Separate RL. We find it easy to apply the proposed methods to other health-related tasks."}, {"heading": "Appendix: the proof of Theorem 1", "text": "Proof: Taking L '(2, 2) = (2, 3, 4, 5) into account, the equation B = PP + 2LIu + 2IuN is obtained; the first term B1 = PP is obviously semi-definitive as B'x, we have xPx = Px + 22 \u2265 0; the graph laplacian L is positively semi-definite, indicating that its eigenvalues are non-negative, i.e. that it is a positive semi-definite matrix; the last term in B is an identical matrix, which is certainly positive; the sum of two positive semi-definite matrices and a positive matrix results in a positive semi-definite matrix; the last term in B is an identical matrix, which is certainly positive."}], "references": [{"title": "An actor-critic contextual bandit algorithm for personalized interventions using mobile devices", "author": ["H. Lei", "A. Tewari", "S. Murphy"], "venue": "NIPS 2014 Workshop: Personalization: Methods and Applications, pp. 1 \u2013 9, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention", "author": ["H. Lei"], "venue": "PhD thesis, University of Michigan,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A batch, off-policy, actorcritic algorithm for optimizing the average reward", "author": ["S.A. Murphy", "Y. Deng", "E.B. Laber", "H.R. Maei", "R.S. Sutton", "K. Witkiewitz"], "venue": "CoRR, vol. abs/1607.05047, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Constructing justin-time adaptive interventions", "author": ["P. Liao", "A. Tewari", "S. Murphy"], "venue": "Phd Section Proposal, pp. 1\u201349, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep correlational learning for survival prediction from multimodality datay", "author": ["J. Yao", "X. Zhu", "F. Zhu", "J. Huang"], "venue": "International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Wsisa: Making survival prediction from whole slide histopathological images", "author": ["X. Zhu", "J. Yao", "F. Zhu", "J. Huang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pp. 7234 \u2013 7242, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery", "author": ["Z. Xu", "S. Wang", "F. Zhu", "J. Huang"], "venue": "ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM- BCB), 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "A smartphone application to support recovery from alcoholism: a randomized clinical trial", "author": ["D. Gustafson", "F. McTavish", "M. Chih", "A. Atwood", "R. Johnson", "M.B. ...", "D. Shah"], "venue": "JAMA Psychiatry, vol. 71, no. 5, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Development and evaluation of a mobile intervention for heavy drinking and smoking among college studen", "author": ["K. Witkiewitz", "S. Desai", "S. Bowen", "B. Leigh", "M. Kirouac", "M. Larimer"], "venue": "Psychology of Addictive Behaviors, vol. 28, no. 3, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Harnessing different motivational frames via mobile phones to promote daily physical activity and reduce sedentary behavior in aging adults", "author": ["K. Abby", "H. Eric", "G. Lauren", "W. Sandra", "S. Jylana", "B. Matthew", ".\u0307.", "C. Jesse"], "venue": "Plos ONE, vol. 8, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Technology-enhanced maintenance of treatment gains in eating disorders: Efficacy of an intervention delivered via text messaging", "author": ["S. Bauer", "E. Okon", "R. Meermann", "H. Kordy"], "venue": "Journal of Consulting and Clinical Psychology, vol. 80, no. 4, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobile interventions for severe mental illness: design and preliminary data from three approaches", "author": ["C. Depp", "B. Mausbach", "E. Granholm", "V. Cardenas", "D. Ben-Zeev", "...", "D. Jeste"], "venue": "The Journal of Nervous and Mental Disease, vol. 198, no. 10, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Mobile technologies among people with serious mental illness: opportunities for future services", "author": ["D. Ben-Zeev", "K.E. Davis", "S. Kaiser", "I. Krzsos", "R.E. Drake"], "venue": "Administration and Policy in Mental Health and Mental Health Services Research, vol. 40, no. 4, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A text message-based intervention for weight loss: randomized controlled trial", "author": ["K. Patrick", "F. Raab", "M. Adams", "L. Dillon", "M. Zabin- 10 \u03bc= 0.001 \u03bc= 0.01 \u03bc= 0.1 \u03bc= 1.0 \u03bc= 10  1", "2  00  1", "3  00  1", "4  00  1", "5  00 (a) Performance vs. \u03bc when \u03b3= 0  A  ve  ra  ge  R  ew  ar  d  Separate-RL Cohesion-RL#1 Cohesion-RL#2 \u03bc= 0.001 \u03bc= 0.01 \u03bc= 0.1 \u03bc= 1.0 \u03bc= 10  1", "3  00  1", "3  50  1", "4  00  1", "4  50  1", "5  00  1", "5  50  1", "6  00 (b) Performance vs. \u03bc when \u03b3= 0.6  A  ve  ra  ge  R  ew  ar  d Separate-RL Cohesion-RL#1 Cohesion-RL#2 \u03bc= 0.001 \u03bc= 0.01 \u03bc= 0.1 \u03bc= 1.0 \u03bc= 10  1", "2  30  1", "2  80  1", "3  30  1", "3  80  1", "4  30  1", "4  80  1", "5  30  1", "5  80  1", "6  30  1", "6  80 (c) Performance vs. \u03bc when \u03b3= 0.95  A  ve  ra  ge  R  ew  ar  d  Separate-RL Cohesion-RL#1 Cohesion-RL#2 Fig. 3. Performance of three RL methods for experiment setting (S3). Each sub-figure shows results under one \u03b3 setting. ski", "C. Rock", "W. Griswold", "G. Norman"], "venue": "Journal of Medical Internet Research, vol. 11, no. 1, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "International Conference on World Wide Web (WWW), pp. 661\u2013670, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "A gang of bandits", "author": ["N. Cesa-Bianchi", "C. Gentile", "G. Zappella"], "venue": "NIPS, pp. 737\u2013745, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Online clustering of bandits", "author": ["C. Gentile", "S. Li", "G. Zappella"], "venue": "ICML, pp. 757\u2013765, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Revealing graph bandits for maximizing local influence", "author": ["A. Carpentier", "M. Valko"], "venue": "AISTATS, pp. 10\u201318, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Structured sparse method for hyperspectral unmixing", "author": ["F. Zhu", "Y. Wang", "S. Xiang", "B. Fan", "C. Pan"], "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 88, no. 0, pp. 101\u2013118, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A label propagation method using spatial-spectral consistency for hyperspectral image classification", "author": ["H. Li", "Y. Wang", "S. Xiang", "J. Duan", "F. Zhu", "C. Pan"], "venue": "International Journal of Remote Sensing, vol. 37, no. 1, pp. 191\u2013 211, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Prediction models for network-linked data", "author": ["T. Li", "E. Levina", "J. Zhu"], "venue": "CoRR, vol. abs/1602.01192, February 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Algorithmic survey of parametric value function approximation", "author": ["M. Geist", "O. Pietquin"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, no. 6, pp. 845\u2013867, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of actor-critic reinforcement learning: Standard and natural policy gradients", "author": ["I. Grondman", "L. Busoniu", "G.A.D. Lopes", "R. Babuska"], "venue": "IEEE Trans. Systems, Man, and Cybernetics, vol. 42, no. 6, pp. 1291\u20131307, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequential cost-sensitive decision making with reinforcement learning", "author": ["E.P.D. Pednault", "N. Abe", "B. Zadrozny"], "venue": "ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min., pp. 259\u2013268, 2002.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "J. of Machine Learning Research (JLMR), vol. 4, pp. 1107\u20131149, 2003.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Variety influences habituation of motivated behavior for food and energy intake in children", "author": ["L. Epstein", "J. Robinson", "J. Temple", "J. Roemmich", "A. Marusewski", "R. Nadbrzuch"], "venue": "The American Journal of Clinical Nutrition, vol. 89, pp. 746 \u2013 754, Mar 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Cross channel optimized marketing by reinforcement learning", "author": ["N. Abe", "N.K. Verma", "C. Apt\u00e9", "R. Schroko"], "venue": "ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min., pp. 767\u2013772, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimizing debt collections using constrained reinforcement learning", "author": ["N. Abe", "P. Melville", "C. Pendus", "C.K. Reddy", "D.L. Jensen", "V.P. Thomas", "J.J. Bennett", "G.F. Anderson", "B.R. Cooley", "M. Kowalczyk", "M. Domick", "T. Gardinier"], "venue": "ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min., pp. 75\u201384, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML), pp. 521\u2013528, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Privacypreserving reinforcement learning", "author": ["J. Sakuma", "S. Kobayashi", "R.N. Wright"], "venue": "International Conference on Machine Learning, pp. 864\u2013871, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Delinquent peers revisited: Does network structure matter", "author": ["D.L. Haynie"], "venue": "American journal of sociology, vol. 106, no. 4, pp. 1013 \u2013 1057, 2001.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Social network influences on adolescent substance use: disentangling structural equivalence from cohesion", "author": ["K. Fujimoto", "T.W. Valente"], "venue": "Social Science & Medicine, vol. 74, no. 12, pp. 1952 \u2013 1960, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1952}, {"title": "Semisupervised hyperspectral image classification via discriminant analysis and robust regression", "author": ["G. Cheng", "F. Zhu", "S. Xiang", "Y. Wang", "C. Pan"], "venue": "IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 9, no. 2, pp. 595\u2013608, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Road centerline extraction via semisupervised segmentation and multidirection nonmaximum suppression", "author": ["G. Cheng", "F. Zhu", "S. Xiang", "C. Pan"], "venue": "IEEE Geoscience and Remote Sensing Letters, vol. 13, no. 4, pp. 545\u2013 549, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral unmixing via data-guided sparsity", "author": ["F. Zhu", "Y. Wang", "B. Fan", "S. Xiang", "G. Meng", "C. Pan"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 23, pp. 5412\u2013 5427, Dec 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Urban road extraction via graph cuts based probability propagation", "author": ["G. Cheng", "Y. Wang", "Y. Gong", "F. Zhu", "C. Pan"], "venue": "Image Processing (ICIP), 2014 IEEE International Conference on, pp. 5072\u20135076, IEEE, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "10,000+ times accelerated robust subset selection (ARSS)", "author": ["F. Zhu", "B. Fan", "X. Zhu", "Y. Wang", "S. Xiang", "11 C. Pan"], "venue": "Proc. Assoc. Adv. Artif. Intell. (AAAI), pp. 3217\u20133224, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Road extraction via adaptive graph cuts with multiple features", "author": ["G. Cheng", "Y. Wang", "F. Zhu", "C. Pan"], "venue": "Image Processing (ICIP), IEEE International Conference on, pp. 3962\u20133966, IEEE, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust hyperspectral unmixing with correntropy-based metric", "author": ["Y. Wang", "C. Pan", "S. Xiang", "F. Zhu"], "venue": "IEEE Transactions on Image Processing, vol. 24, no. 11, pp. 4027\u20134040, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning-based fully 3d face reconstruction from a single image", "author": ["X. Hu", "Y. Wang", "F. Zhu", "C. Pan"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pp. 1651\u20131655, IEEE, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Accurate urban road centerline extraction from vhr imagery via multiscale segmentation and tensor voting", "author": ["G. Cheng", "F. Zhu", "S. Xiang", "Y. Wang", "C. Pan"], "venue": "Neurocomputing, vol. 205, pp. 407\u2013420, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Effective spectral unmixing via robust representation and learning-based sparsity", "author": ["F. Zhu", "Y. Wang", "B. Fan", "G. Meng", "C. Pan"], "venue": "CoRR, vol. abs/1409.0685, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Randomised trials for the fitbit generation", "author": ["W. Dempsey", "P. Liao", "P. Klasnja", "I. Nahum-Shani", "S.A. Murphy"], "venue": "Significance, vol. 12, pp. 20 \u2013 23, Dec 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395 \u2013 416, 2007.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised Hyperspectral Unmixing Methods", "author": ["F. Zhu"], "venue": "PhD thesis,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Regularized least squares temporal difference learning with nested `2 and `1 penalization", "author": ["M.W. Hoffman", "A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "Recent Advances in Reinforcement Learning, pp. 102\u2013114, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "The matrix cookbook", "author": ["K.B. Petersen", "M.S. Pedersen"], "venue": "2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "The kronecker product and stochastic automata networks", "author": ["A.N. Langville", "W.J. Stewart"], "venue": "Journal of Computational and Applied Mathematics, vol. 167, pp. 429\u2013 447, June 2004.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION With billions of smart device1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks [1\u20137].", "startOffset": 297, "endOffset": 302}, {"referenceID": 1, "context": "1 INTRODUCTION With billions of smart device1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks [1\u20137].", "startOffset": 297, "endOffset": 302}, {"referenceID": 2, "context": "1 INTRODUCTION With billions of smart device1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks [1\u20137].", "startOffset": 297, "endOffset": 302}, {"referenceID": 3, "context": "1 INTRODUCTION With billions of smart device1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks [1\u20137].", "startOffset": 297, "endOffset": 302}, {"referenceID": 4, "context": "1 INTRODUCTION With billions of smart device1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks [1\u20137].", "startOffset": 297, "endOffset": 302}, {"referenceID": 5, "context": "1 INTRODUCTION With billions of smart device1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks [1\u20137].", "startOffset": 297, "endOffset": 302}, {"referenceID": 6, "context": "1 INTRODUCTION With billions of smart device1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks [1\u20137].", "startOffset": 297, "endOffset": 302}, {"referenceID": 7, "context": "[8, 9] and promoting physical activities [10].", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[8, 9] and promoting physical activities [10].", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[8, 9] and promoting physical activities [10].", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity [10], eating disorders [11], alcohol use [8, 9], mental illness[12, 13], obesity/weight management [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity [10], eating disorders [11], alcohol use [8, 9], mental illness[12, 13], obesity/weight management [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity [10], eating disorders [11], alcohol use [8, 9], mental illness[12, 13], obesity/weight management [14].", "startOffset": 142, "endOffset": 148}, {"referenceID": 8, "context": "Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity [10], eating disorders [11], alcohol use [8, 9], mental illness[12, 13], obesity/weight management [14].", "startOffset": 142, "endOffset": 148}, {"referenceID": 11, "context": "Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity [10], eating disorders [11], alcohol use [8, 9], mental illness[12, 13], obesity/weight management [14].", "startOffset": 164, "endOffset": 172}, {"referenceID": 12, "context": "Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity [10], eating disorders [11], alcohol use [8, 9], mental illness[12, 13], obesity/weight management [14].", "startOffset": 164, "endOffset": 172}, {"referenceID": 13, "context": "Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity [10], eating disorders [11], alcohol use [8, 9], mental illness[12, 13], obesity/weight management [14].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "It aims to learn the optimal policy to determine when, where and how to deliver the intervention [1, 3, 4] to best serve users.", "startOffset": 97, "endOffset": 106}, {"referenceID": 2, "context": "It aims to learn the optimal policy to determine when, where and how to deliver the intervention [1, 3, 4] to best serve users.", "startOffset": 97, "endOffset": 106}, {"referenceID": 3, "context": "It aims to learn the optimal policy to determine when, where and how to deliver the intervention [1, 3, 4] to best serve users.", "startOffset": 97, "endOffset": 106}, {"referenceID": 0, "context": "In 2014, Lei [1] made a first attempt to formulate the mHealth intervention as an online actor-critic contextual bandit problem.", "startOffset": 13, "endOffset": 16}, {"referenceID": 14, "context": "However, this method did not consider the important delayed effect in the SDM\u2014 the current action may influence not only the immediate reward but also the next states and, through that, all the subsequent rewards [15, 16].", "startOffset": 213, "endOffset": 221}, {"referenceID": 15, "context": "However, this method did not consider the important delayed effect in the SDM\u2014 the current action may influence not only the immediate reward but also the next states and, through that, all the subsequent rewards [15, 16].", "startOffset": 213, "endOffset": 221}, {"referenceID": 2, "context": "Murphy [3] proposed an average reward based RL to consider the delayed effect in the mHealth.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Besides, [3] is in the batch learning setting, which is different from this paper\u2019s focuses.", "startOffset": 9, "endOffset": 12}, {"referenceID": 16, "context": "Cesa-Bianchi [17] proposed a contextual bandit algorithm that considers the network information.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "Besides, there are three drawbacks making the method in [17] impractical for the mHealth: (1) Cesa-Bianchi\u2019s method focues on the bandit algorithm.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "There is lots of misleading network information for the mHealth study [17\u2013 19].", "startOffset": 70, "endOffset": 78}, {"referenceID": 19, "context": "not flexible for the mHealth study [20, 21].", "startOffset": 35, "endOffset": 43}, {"referenceID": 20, "context": "not flexible for the mHealth study [20, 21].", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "(2) Current evidence verifies the wide existence of networks among users [20\u201322].", "startOffset": 73, "endOffset": 80}, {"referenceID": 20, "context": "(2) Current evidence verifies the wide existence of networks among users [20\u201322].", "startOffset": 73, "endOffset": 80}, {"referenceID": 21, "context": "(2) Current evidence verifies the wide existence of networks among users [20\u201322].", "startOffset": 73, "endOffset": 80}, {"referenceID": 16, "context": "(4) Compared with [17], the proposed method has a tuning parameter, which allows us to control how much information we should share with similar users.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "We assume the mHealth intervention is a Markov Decision Process (MDP) [23\u201326] that consists of a 5-tuple {S,A,P,R, \u03b3}, where S is the state space and A is the action space.", "startOffset": 70, "endOffset": 77}, {"referenceID": 23, "context": "We assume the mHealth intervention is a Markov Decision Process (MDP) [23\u201326] that consists of a 5-tuple {S,A,P,R, \u03b3}, where S is the state space and A is the action space.", "startOffset": 70, "endOffset": 77}, {"referenceID": 24, "context": "We assume the mHealth intervention is a Markov Decision Process (MDP) [23\u201326] that consists of a 5-tuple {S,A,P,R, \u03b3}, where S is the state space and A is the action space.", "startOffset": 70, "endOffset": 77}, {"referenceID": 25, "context": "We assume the mHealth intervention is a Markov Decision Process (MDP) [23\u201326] that consists of a 5-tuple {S,A,P,R, \u03b3}, where S is the state space and A is the action space.", "startOffset": 70, "endOffset": 77}, {"referenceID": 0, "context": "P : S \u00d7A\u00d7S 7\u2192 [0, 1] is the state transition model in which P (s, a, s\u2032) indicates the probability of transiting from one state s to another s\u2032 after taking action a; R (s, a, s\u2032) is the corresponding immediate reward for such transition where R : S \u00d7 A \u00d7 S 7\u2192 R.", "startOffset": 14, "endOffset": 20}, {"referenceID": 14, "context": "The policy of an MDP is to choose actions for any state s \u2208 S in the system [15, 24].", "startOffset": 76, "endOffset": 84}, {"referenceID": 23, "context": "The policy of an MDP is to choose actions for any state s \u2208 S in the system [15, 24].", "startOffset": 76, "endOffset": 84}, {"referenceID": 22, "context": "There are two types of policies: (1) the deterministic policy \u03c0 : S 7\u2192 A selects an action directly for the state, and (2) the stochastic policy \u03c0 : s \u2208 S 7\u2192 \u03c0 (\u00b7 | s) \u2208 P (A) chooses the action for any state s by providing s with a probability distribution over all the possible actions [23].", "startOffset": 288, "endOffset": 292}, {"referenceID": 0, "context": "In mHealth, the stochastic policy is preferred due to two reasons: (a) current evidence shows that some randomness in the action is likely to draw users\u2019 interest, thus helpful to reduce the intervention burden/habituation [1, 3, 27]; (b) though some deterministic policy is theoretically optimal for the MDP, however, we do not know where it is for the large state space on the one hand and the MDP is a simplification for the complex behavioral process on the other; some variation may be helpful to explore the system and search for a desirable policy [3].", "startOffset": 223, "endOffset": 233}, {"referenceID": 2, "context": "In mHealth, the stochastic policy is preferred due to two reasons: (a) current evidence shows that some randomness in the action is likely to draw users\u2019 interest, thus helpful to reduce the intervention burden/habituation [1, 3, 27]; (b) though some deterministic policy is theoretically optimal for the MDP, however, we do not know where it is for the large state space on the one hand and the MDP is a simplification for the complex behavioral process on the other; some variation may be helpful to explore the system and search for a desirable policy [3].", "startOffset": 223, "endOffset": 233}, {"referenceID": 26, "context": "In mHealth, the stochastic policy is preferred due to two reasons: (a) current evidence shows that some randomness in the action is likely to draw users\u2019 interest, thus helpful to reduce the intervention burden/habituation [1, 3, 27]; (b) though some deterministic policy is theoretically optimal for the MDP, however, we do not know where it is for the large state space on the one hand and the MDP is a simplification for the complex behavioral process on the other; some variation may be helpful to explore the system and search for a desirable policy [3].", "startOffset": 223, "endOffset": 233}, {"referenceID": 2, "context": "In mHealth, the stochastic policy is preferred due to two reasons: (a) current evidence shows that some randomness in the action is likely to draw users\u2019 interest, thus helpful to reduce the intervention burden/habituation [1, 3, 27]; (b) though some deterministic policy is theoretically optimal for the MDP, however, we do not know where it is for the large state space on the one hand and the MDP is a simplification for the complex behavioral process on the other; some variation may be helpful to explore the system and search for a desirable policy [3].", "startOffset": 555, "endOffset": 558}, {"referenceID": 0, "context": "the estimated \u03b8\u0302, which is important to behavior scientists for the state (feature) design [1, 3].", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "the estimated \u03b8\u0302, which is important to behavior scientists for the state (feature) design [1, 3].", "startOffset": 91, "endOffset": 97}, {"referenceID": 14, "context": "In RL, value is a core concept that quantifies the quality of a policy \u03c0 [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "There are two definitions of values: the state value and the state-action (Q-) value [28].", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "state transition and immediate reward) is assumed to be unknown, andQ-value allows for action selection without knowing the model while the state value requires the model for the action selection [26].", "startOffset": 196, "endOffset": 200}, {"referenceID": 22, "context": "The goal of RL is to learn an optimal policy \u03c0\u2217 that maximizes the Q-value for all the state-action pairs via interactions with the dynamic system [23].", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "Moreover, the actor-critic algorithm has great properties of quick convergence with low variance and learning continuous policies [24].", "startOffset": 130, "endOffset": 134}, {"referenceID": 28, "context": "It is well known that due to the Markovian property, the Q-value satisfies the linear Bellman equation [29] for any policy \u03c0:", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "the stochastic policy matrix, where \u03a0\u03c0 (s, (s, a)) = \u03c0 (a | s) [26].", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "Once both the reward and the state transition models are given [30], it is easy to obtain the analytical solution as q = (I\u2212 \u03b3P\u03a0\u03c0) r.", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "We then learn the value Qw from observations via a supervised learning paramdigm, which, however, is much more challenging than the general supervised learning since the Q-value is not directly observed [23].", "startOffset": 203, "endOffset": 207}, {"referenceID": 14, "context": "Although MC can provide an unbiased estimation of Qw, it is not suitable for mHealth since MC can\u2019t learn from the incomplete trajectory [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "As a central idea of RL [15], the temporal-difference (TD) learning is able to make use of the Bellman equation (3) and to learn the value from the incomplete trajectories.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "We employ the least-square TD for the Q-value (LSTDQ) estimation, due to its advantage of efficient use of samples over the pure temporal-difference algorithms [26, 31].", "startOffset": 160, "endOffset": 168}, {"referenceID": 30, "context": "We employ the least-square TD for the Q-value (LSTDQ) estimation, due to its advantage of efficient use of samples over the pure temporal-difference algorithms [26, 31].", "startOffset": 160, "endOffset": 168}, {"referenceID": 29, "context": "The goal of LSTDQ is to learn a Qw to approximately satisfy the Bellman equation (3), by minimizing the TD error [30] as", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "They share no information and run a separate algorithm for each user [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "Following this idea, we extend [1] to the separate RL setting.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "Here \u2016\u03b8n\u201622 is the constraint to make (9) a well-posed problem and \u03b6a is the tuning parameter that controls the strength of the smooth penalization [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 31, "context": "It is a famous phenomenon observed in lots of social behavior studies [32, 33] that people are widely connected in a network and linked users tend to have similar behaviors.", "startOffset": 70, "endOffset": 78}, {"referenceID": 32, "context": "It is a famous phenomenon observed in lots of social behavior studies [32, 33] that people are widely connected in a network and linked users tend to have similar behaviors.", "startOffset": 70, "endOffset": 78}, {"referenceID": 21, "context": "Besides, individuals are widely connected due to the similar features, such as age, gender, race, religion, education level, work, income, other socioeconomic status, medical records and genetics features etc [22].", "startOffset": 209, "endOffset": 213}, {"referenceID": 16, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 17, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 18, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 33, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 34, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 35, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 36, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 37, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 38, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 39, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 40, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 41, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 42, "context": "There is noisy and misleading relational information in the network for mHealth WS[17\u201319, 34\u201343].", "startOffset": 82, "endOffset": 96}, {"referenceID": 43, "context": "The MDPs of one user on two diverse mHealth studies should be very different; for example, the MDP in the HeartSteps study [44] for one user should be different from that in the alcohol control [8, 9] study.", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "The MDPs of one user on two diverse mHealth studies should be very different; for example, the MDP in the HeartSteps study [44] for one user should be different from that in the alcohol control [8, 9] study.", "startOffset": 194, "endOffset": 200}, {"referenceID": 8, "context": "The MDPs of one user on two diverse mHealth studies should be very different; for example, the MDP in the HeartSteps study [44] for one user should be different from that in the alcohol control [8, 9] study.", "startOffset": 194, "endOffset": 200}, {"referenceID": 44, "context": "where vi \u2208 N (vj) indicates that i-th node is the KNN of the j-th node [45]; (11) is an undirected Graph.", "startOffset": 71, "endOffset": 75}, {"referenceID": 42, "context": "\u2016wi \u2212wj\u2016 and \u2016\u03b8i \u2212 \u03b8j\u2016 are small if i \u2194 j [43, 46].", "startOffset": 42, "endOffset": 50}, {"referenceID": 45, "context": "\u2016wi \u2212wj\u2016 and \u2016\u03b8i \u2212 \u03b8j\u2016 are small if i \u2194 j [43, 46].", "startOffset": 42, "endOffset": 50}, {"referenceID": 46, "context": "the fixedpoint step) [47].", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "It is an advantage of our methods over the network based bandit [17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 47, "context": "According to the Encapsulating Sum [48], we have", "startOffset": 35, "endOffset": 39}, {"referenceID": 48, "context": "Then the eigenvalues of A \u2297 B [49], where \u2297 is the Kronecker Product, are", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "They do not put the `2 constraint on the fixed-point variable W [30, 47].", "startOffset": 64, "endOffset": 72}, {"referenceID": 46, "context": "They do not put the `2 constraint on the fixed-point variable W [30, 47].", "startOffset": 64, "endOffset": 72}, {"referenceID": 2, "context": "{0, 1} , where a = 1 means sending the positive intervention, while a = 0 indicates no intervention [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 43, "context": "To verify the performance of our method, we use a dataset from a mobile health study, called HeartSteps [44], to approximate the generative model.", "startOffset": 104, "endOffset": 108}, {"referenceID": 43, "context": "interventions), which are adapted to users\u2019 ongoing status, such as suggesting users to take a walk after long sitting [44], or to do some exercises after work.", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "A trajectory of T tuples D = {(si, ai,ri) | i = 1, \u00b7 \u00b7 \u00b7 , T} are generated for each user [1, 3].", "startOffset": 90, "endOffset": 96}, {"referenceID": 2, "context": "A trajectory of T tuples D = {(si, ai,ri) | i = 1, \u00b7 \u00b7 \u00b7 , T} are generated for each user [1, 3].", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": "8 The value of \u03b3 specifies different RL methods: (a) \u03b3 = 0 means the contextual bandit [1]; (b) 0 < \u03b3 < 1 is the discounted reward RL, which is first compared in the online actor-critic setting for mHealth.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Such process is called drawing warm start trajectory (WST) via the micro-randomized trials [4, 44], and T0 is the length of the WST.", "startOffset": 91, "endOffset": 98}, {"referenceID": 43, "context": "Such process is called drawing warm start trajectory (WST) via the micro-randomized trials [4, 44], and T0 is the length of the WST.", "startOffset": 91, "endOffset": 98}, {"referenceID": 0, "context": "To achieve a more practical dataset compared with [1, 3, 4], we come up with a method to generate N users (i.", "startOffset": 50, "endOffset": 59}, {"referenceID": 2, "context": "To achieve a more practical dataset compared with [1, 3, 4], we come up with a method to generate N users (i.", "startOffset": 50, "endOffset": 59}, {"referenceID": 3, "context": "To achieve a more practical dataset compared with [1, 3, 4], we come up with a method to generate N users (i.", "startOffset": 50, "endOffset": 59}, {"referenceID": 0, "context": "There are three online actor-critic RL methods for the comparison: (a) Separate-RL, which is an extension of the online actor-critic contextual bandit in [1] to the online actor-critic reinforcement learning.", "startOffset": 154, "endOffset": 157}, {"referenceID": 7, "context": "MDPs), for example measuring how much alcohol users have in a fixed time period in the alcohol use study [8, 9].", "startOffset": 105, "endOffset": 111}, {"referenceID": 8, "context": "MDPs), for example measuring how much alcohol users have in a fixed time period in the alcohol use study [8, 9].", "startOffset": 105, "endOffset": 111}, {"referenceID": 48, "context": "Since for any matrices A \u2208 Rl\u00d7k and D \u2208 Rm\u00d7n, the Kronecker product has the property (A\u2297D) = (AT \u2297DT) [49].", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "In the wake of the vast population of smart device users worldwide, mobile health (mHealth) technologies are hopeful to generate positive and wide influence on people\u2019s health. They are able to provide flexible, affordable and portable health guides to device users. Current online decision-making methods for mHealth assume that the users are completely heterogeneous. They share no information among users and learn a separate policy for each user. However, data for each user is very limited in size to support the separate online learning, leading to unstable policies that contain lots of variances. Besides, we find the truth that a user may be similar with some, but not all, users, and connected users tend to have similar behaviors. In this paper, we propose a network cohesion constrained (actor-critic) Reinforcement Learning (RL) method for mHealth. The goal is to explore how to share information among similar users to better convert the limited user information into sharper learned policies. To the best of our knowledge, this is the first online actor-critic RL for mHealth and first network cohesion constrained (actor-critic) RL method in all applications. The network cohesion is important to derive effective policies. We come up with a novel method to learn the network by using the warm start trajectory, which directly reflects the users\u2019 property. The optimization of our model is difficult and very different from the general supervised learning due to the indirect observation of values. As a contribution, we propose two algorithms for the proposed online RLs. Apart from mHealth, the proposed methods can be easily applied or adapted to other health-related tasks. Extensive experiment results on the HeartSteps dataset demonstrates that in a variety of parameter settings, the proposed two methods obtain obvious improvements over the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}