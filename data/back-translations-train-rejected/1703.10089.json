{"id": "1703.10089", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Position-based Content Attention for Time Series Forecasting with Sequence-to-sequence RNNs", "abstract": "In this paper, we study the use of recurrent neural networks (RNNs) for modeling and forecasting time series. We first illustrate the fact that standard sequence-to-sequence RNNs neither capture well periods in time series nor handle well missing values, even though many real life times series are periodic and contain missing values. We then propose an extended attention mechanism that can be deployed on top of any RNN and that is designed to capture periods and make the RNN more robust to missing values. We show the effectiveness of this novel model through extensive experiments with multiple univariate and multivariate datasets.", "histories": [["v1", "Wed, 29 Mar 2017 15:11:16 GMT  (248kb,D)", "http://arxiv.org/abs/1703.10089v1", null], ["v2", "Mon, 21 Aug 2017 12:36:58 GMT  (192kb,D)", "http://arxiv.org/abs/1703.10089v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["yagmur g cinar", "hamid mirisaee", "parantapa goswami", "eric gaussier", "ali ait-bachir", "vadim strijov"], "accepted": false, "id": "1703.10089"}, "pdf": {"name": "1703.10089.pdf", "metadata": {"source": "META", "title": "Time Series Forecasting using RNNs: an Extended Attention Mechanism to Model Periods and Handle Missing Values", "authors": ["Yagmur Gizem Cinar", "Hamid Mirisaee"], "emails": ["YAGMUR.CINAR@IMAG.FR", "HAMID.MIRISAEE@IMAG.FR", "PARANTAPA.GOSWAMI@IMAG.FR", "ERIC.GAUSSIER@IMAG.FR", "A.AIT-BACHIR@COSERVIT.COM", "STRIJOV@CCAS.RU"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. How do RNNs behave on time series? A study of periodicity and missing values", "text": "We will focus first on univariate time series, although most of the elements we are discussing are applied directly to multivariate time series (we will consider multivariate time series in a second step).As already mentioned, the time series consists in predicting future values from past observed values. The time span of past values we are looking at here is called history, whereas the time span of future values to be predicted is called the predictive horizon (in multi-step prediction, which we are looking at here as T > 1).The prediction is modeled as a regression-like problem, where the goal is to learn the relationship y = r (x) where y + 1,."}, {"heading": "2.1. RNNs and periodicity", "text": "To illustrate how RNNs behave w.r.t. periods in time series, we maintained two periodic time series, which are fully described in Section 5, each representing (a) the electrical load in Poland over the period of 10 years (1 / 1 / 2002-7 / 31 / 2012). The first time series, called PSE, has two main periods, one daily and one weekly, while the second time series, called PW, has a yearly period. Figure 1 (left and middle) shows the auto-correlation (chatfield, 2016) for each time series and shows these different periods (note that the original designation indicates that we used the dataset as it is, without any change. We have maintained the above described RNN, with (RNN-A) and without (RNN) the attention mechanism (in the latter case, the context is taken to keep the hidden state)."}, {"heading": "2.2. RNNs and missing values", "text": "Turning now to the problem of missing values, this time using a degraded version of the above-mentioned time series, in which 15% of the original values are missing, either in consecutive sequences (to create gaps in the data) or randomly (we refer the reader again to Section 5 for more details on these data sets). As already mentioned, a standard technique for treating such missing values in RNNNs, called padding, is to repeat the hidden state of the last observed value over the missing values. Another standard approach to time data is to reconstruct missing values using interpolation. Among the existing interpolation methods, we experimented with three methods from different categories, namely linear interpolation, nonlinear spline interpolation, and kernel interpolation (Meijering, 2002). We measured their reconstruction ability on all four data sets we used for experiments (described in Section 5.1) in relation to MSE, calculated between the interpolated and original data sets."}, {"heading": "3. Extended attention mechanism", "text": "Here we present two simple enhancements to the attention mechanism that allow you to model periods and better handle missing values in RNNs."}, {"heading": "3.1. Modeling periodicity", "text": "With a historical quantity T and a predictive horizon T \u2032, the possible periods on which we can rely for the predictive range in the quantity {1, \u00b7 \u00b7 \u00b7, T + T \u2032 \u2212 1}. Here, we explicitly model all possible periods within a real vector that we will call \u03c4, the dimension T + T \u2032 \u2212 1. This vector is used to encode the meaning of all possible periods and to decrease or increase the meaning of the corresponding input in order to predict the current issue. To this end, we modify the original attention mechanism to reweight the attention weights as follows: eij, \u03c4 = v T a tanh (Wasi \u2212 1 + Uahj) \u00b7 (\u03c4T \u0445 (i, j)) (5), where \u2206 (i, j) and RT + T \u2032 \u2212 1 is a binary vector that is 1 on the dimension (i \u2212 j) and 0 elsewhere we will refer to this model as NN."}, {"heading": "3.2. Handling missing values", "text": "Provided their proportion is not too important, otherwise the prediction task is less relevant, missing values in time series can easily be identified by taking into account that the most frequent interval between successive points corresponds to the sampling rate of the time series. Points that do not exist in the expected intervals are then considered missing gaps. However, this strategy works well in practice, provided that the sampling rate of the time series does not change much over time. Therefore, it is possible to identify missing values in time series and then use fill or interpolation techniques to represent them. However, filled or interpolated inputs should not be treated as standard inputs as they are less reliable than other inputs. Furthermore, it is possible to use fill or interpolation techniques if the size of a gap, i.e. successive missing values, is important, the more distant missing value is interpolated from the values observed (the last in the case of adding, the last and the less trusted ones) of the interpolated ones."}, {"heading": "4. Multivariate Extension", "text": "Since each variable in aK multivariate time series may have its own periods, we propose here to apply the RNN with the extended attention mechanism described above to each variable k, 1 \u2264 k \u2264 K, of the time series. Therefore, we construct context vectors for each variable k on the basis of the following equations: \u03b1 (k) ij = exp (e (k) ij) \u2211 T j \u2032 = 1 exp (e (k) ij \u2032) (8), where ekij is given by equations. 3, 5, 6 or 7. The context vector for the ith output of the kth variable is then defined by c (k) i = 1 \u03b1 (k) ij h (k) ij h (k) j, where h (k) j is the encoder hidden state for the kth variable."}, {"heading": "5. Experiments", "text": "In this section, we will first describe the data sets used in this study and explain the experimental settings before presenting the results obtained with the proposed methods."}, {"heading": "5.1. Datasets and settings", "text": "We have retained four widely used and publicly available (data sets) described in Table 1. The values for the size of the history have been set to include the known periods of the data sets. Of course, they can be adjusted by cross-validation if one does not want to identify the potential periods by verifying the autocorrelation curves. In general, the forecast horizon should reflect the nature of the data and the application that is being considered, of course, with a balance between long forecast horizon and forecast quality. To this end, the prediction procedures of PSE, PW, AQ and HPC are selected as 8 hours, 1 week, 6 hours and 8 hours, respectively. In Table 1, one can also find the sampling rate for each dataset. Note that for PW we have selected Greater Warsaw, which covers only one recording station. For the univariate experiments, we chose the maximum temperature series from PW, from AQ the output parameters we chose the global SNOGB (SNO8.2) and from SNOGB (SNO8.2)."}, {"heading": "2 http://deeplearning.net/software/theano/", "text": "3 https: / / lasagne.readthedocs.iothe hyperparameters are the suggested values, whether we assess the proposed robust values, whether we introduce the suggested methods with the missing values. Within the series {1, 32, 64, 128, 256, 512} we will maintain the mini-batch size of 64 as it works best. Then, in the second level, we will adjust all the hyperparameters shown in Table 2 and select the top-4 settings according to the RNN model with standard attention mechanism. Note that in all these top-4 settings the learning rate, the regulation coefficient and the type of regulation are the same (shown in the bold page Table 2). In other words, the top-4 settings differ in the number of attention units and the number of RNN units, all other hyperparameters that are the same."}, {"heading": "5.2. Overall results on univariate time series", "text": "Figure 5 and Figure 6 illustrate the univariate experiments with the interpolation and padding methods, each using the MSE and SMAPE as evaluation benchmarks. Nevertheless, we focus our discussion on MSE, as it is the metric results on which the problem is optimized. However, the same conclusion could be drawn from Figure 5 for the AQ dataset, where the three proposed models, i.e. RNN-\u03c4 and RNN-\u03c4\u00b5-1 / 2, provide better MSE results than the other methods. However, the same conclusion could be drawn from Figure 5 for the same dataset. Furthermore, the NPC datasets, RNN-2 presumed in Section 3, tend to be better than RNN-\u03c4\u00b5-1 for these interpolated data. We can see this fact in Figure 6, where the SMSE and SMAPE results are represented for the added data."}, {"heading": "5.3. RNN-\u03c4 and periodicity", "text": "As already mentioned, RNN-\u03c4 aims to determine the periodicity of the data, which happens via the \u03c4 vector described in Section 3.1. Here, we show the effectiveness of this approach by illustrating how the attention weights of the original attention model behave compared to those of the RNN-\u03c4. Obviously, we expect RNN-\u03c4 to impose higher weights on the input factors corresponding to the periods. To illustrate this point, we change the PSE dataset, which has two periods, weekly and daily (see Section 2). To observe how RNN-\u03c4 behaves, we calculate all attention weights of all test examples and of all forecast horizons. Figure 7 (left) shows the average attention weights of PSE with RNN-A and RNN-\u03c4. As one can observe, RNN-\u03c4 fails to detect both periods effectively (1 and 7) during these two days, by assigning them to both effective weights."}, {"heading": "5.4. RNN-\u03c4\u00b5 and missing values", "text": "The RNN-\u03c4\u00b5-1 / 2 models are designed to capture periodicity and prevent the attention mechanism from assigning high weights to the missing instances. As already mentioned, RNN-\u03c4\u00b5-2 with interpolated data is overall the best method per formation. To demonstrate the effectiveness of this method in dealing with the missing values, we examined the attention weights it assigns to the missing instances. For each data set, we averaged the attention weights of RNN-\u03c4\u00b5-2 and compared them with those of RNN-A. Figure 8 illustrates the average attention weight and confidence intervals for all data sets. As can be observed, RNN-\u03c4\u00b5-2 is attention weights lower than those of RNN-A. On average, the attention weights that RNN-\u03c4\u00b5 attaches to missing values are unequal to the 48% of the missing values, which indicates that the missing values of RN-\u00b5 can be much smaller than the missing values of RN-\u00b5."}, {"heading": "5.5. Results on multivariate time series", "text": "As mentioned in Section 4, the proposed methods can be extended to multivariate time series. Here, we show how this extension can effectively surpass the most advanced methods and the standard attention mechanism. To illustrate this, we select all four global time series from HPC, namely global active power, global reactive power, voltage and global intensity. We predict the first one, which is important for example for monthly electricity bills. For AQ, we have selected the four variables associated with real sensors (and exclude nominal sensors), namely C6H6 (GT), NO2 (GT), CO (GT) and NOx (GT), and predict the first one, C6H6 (GT), as it is the most important for NMHC-related air pollution (Vito et al., 2009). Note that this differs from the univariate cases where we focus on the nominal sensor PT08.S2 (NO2) to observe the effects of varying amounts and abnormalities."}, {"heading": "6. Related Work", "text": "The notion of stochasticity in time series modeling and prediction was introduced a long time ago (Yule, 1927). Since then, various stochastic models have been developed, including autoregressive approaches (ARM, 1931) and changing averages (MA), which have been combined in a more general and effective framework known as autoregressive gradient patterns (ARM), or autoregressive integrated averages (ARM) when differentiation is included in modeling (Box & 1968). Vector ARIMA or VARM (Tiao & Box, 1981) is the multivariate extension of the universal ARIMA models, in which each time series with a vector. Neural networks are considered a promising tool for time series (Zhang, 2001; Crone et al) due to their data-driven and self-adaptive nature."}, {"heading": "7. Conclusion", "text": "In this work, we investigated the capabilities of RNNs for modelling and predicting time series. We used state-of-the-art RNNs based on bidirectional LSTM encoder decoders with attention mechanisms and illustrated their shortcomings in capturing the periodicity of the data and in dealing with properly missing values. To alleviate this, we proposed two architectural modifications to the traditional attention mechanism: one to learn and exploit the periodicity in the temporal data (RNN-\u03c4) and a second to handle both random missing values and long gaps in the data (RNN-\u03c4\u00b51 / 2). We expanded the entire framework for multivariate time series. The experiments we conducted over multiple univariate and multivariate time series show the effectiveness of these modifications. RNN-\u03c4 and NN-NN-original time series are not better than NN-1 and NN-2, but better than NN-actual location and NN-2."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Some recent advances in forecasting and control", "author": ["Box", "George EP", "Jenkins", "Gwilym M"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "Box et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Box et al\\.", "year": 1968}, {"title": "The analysis of time series: an introduction", "author": ["Chatfield", "Chris"], "venue": "CRC press,", "citeRegEx": "Chatfield and Chris.,? \\Q2016\\E", "shortCiteRegEx": "Chatfield and Chris.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Recurrent networks and narma modeling", "author": ["Connor", "Jerome", "Atlas", "Les E", "Martin", "Douglas R"], "venue": "In NIPS,", "citeRegEx": "Connor et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Connor et al\\.", "year": 1991}, {"title": "Predicting performance and quantifying corporate governance risk for latin american adrs and banks", "author": ["Creamer", "Germ\u00e1n G", "Freund", "Yoav"], "venue": null, "citeRegEx": "Creamer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Creamer et al\\.", "year": 2004}, {"title": "Advances in forecasting with neural networks? empirical evidence from the nn3 competition on time series prediction", "author": ["Crone", "Sven F", "Hibon", "Michele", "Nikolopoulos", "Konstantinos"], "venue": "International Journal of Forecasting,", "citeRegEx": "Crone et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Crone et al\\.", "year": 2011}, {"title": "25 years of time series forecasting", "author": ["De Gooijer", "Jan G", "Hyndman", "Rob J"], "venue": "International journal of forecasting,", "citeRegEx": "Gooijer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gooijer et al\\.", "year": 2006}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Dynamic least squares support vector machine", "author": ["Fan", "Yugang", "Li", "Ping", "Song", "Zhihuan"], "venue": "In Intelligent Control and Automation,", "citeRegEx": "Fan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2006}, {"title": "Applying lstm to time series predictable through timewindow approaches", "author": ["Gers", "Felix A", "Eck", "Douglas", "Schmidhuber", "J\u00fcrgen"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Gers et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2001}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "Journal of machine learning research,", "citeRegEx": "Gers et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2002}, {"title": "Noisy time series prediction using recurrent neural networks and grammatical inference", "author": ["Giles", "C Lee", "Lawrence", "Steve", "Tsoi", "Ah Chung"], "venue": "Machine learning,", "citeRegEx": "Giles et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Giles et al\\.", "year": 2001}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm", "author": ["Hsieh", "Tsung-Jung", "Hsiao", "Hsiao-Fen", "Yeh", "WeiChang"], "venue": "Applied soft computing,", "citeRegEx": "Hsieh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2011}, {"title": "Application of the adaline system to weather forecasting", "author": ["Hu", "Michael Jen-Chao"], "venue": null, "citeRegEx": "Hu and Jen.Chao.,? \\Q1964\\E", "shortCiteRegEx": "Hu and Jen.Chao.", "year": 1964}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["Jaeger", "Herbert", "Haas", "Harald"], "venue": null, "citeRegEx": "Jaeger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jaeger et al\\.", "year": 2004}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Jordan", "Michael I"], "venue": "Technical Report 8604,", "citeRegEx": "Jordan and I.,? \\Q1986\\E", "shortCiteRegEx": "Jordan and I.", "year": 1986}, {"title": "Comparison of arima and random forest time series models for prediction of avian influenza h5n1 outbreaks", "author": ["Kane", "Michael J", "Price", "Natalie", "Scotch", "Matthew", "Rabinowitz", "Peter"], "venue": "BMC bioinformatics,", "citeRegEx": "Kane et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kane et al\\.", "year": 2014}, {"title": "Comparison of arima and random forest time series models for prediction of avian influenza h5n1 outbreaks", "author": ["Kane", "Michael J", "Price", "Natalie", "Scotch", "Matthew", "Rabinowitz", "Peter"], "venue": "BMC bioinformatics,", "citeRegEx": "Kane et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kane et al\\.", "year": 2014}, {"title": "Draw: a recurrent neural network for image generation", "author": ["G Karol", "I Danihelka", "A Graves", "D Rezende", "D. Wierstra"], "venue": null, "citeRegEx": "Karol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karol et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A data-mining approach to predict influent quality", "author": ["Kusiak", "Andrew", "Verma", "Anoop", "Wei", "Xiupeng"], "venue": "Environmental monitoring and assessment,", "citeRegEx": "Kusiak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kusiak et al\\.", "year": 2013}, {"title": "Nonlinear signal processing using neural networks: prediction and system modelling", "author": ["A Laepes", "R. Farben"], "venue": "Technical report,", "citeRegEx": "Laepes and Farben,? \\Q1987\\E", "shortCiteRegEx": "Laepes and Farben", "year": 1987}, {"title": "A review of unsupervised feature learning and deep learning for time-series modeling", "author": ["L\u00e4ngkvist", "Martin", "Karlsson", "Lars", "Loutfi", "Amy"], "venue": "Pattern Recognition Letters,", "citeRegEx": "L\u00e4ngkvist et al\\.,? \\Q2014\\E", "shortCiteRegEx": "L\u00e4ngkvist et al\\.", "year": 2014}, {"title": "Learning to diagnose with lstm recurrent neural networks", "author": ["Lipton", "Zachary C", "Kale", "David C", "Elkan", "Charles", "Wetzell", "Randall"], "venue": "arXiv preprint arXiv:1511.03677,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "A chronology of interpolation: From ancient astronomy to modern signal and image processing", "author": ["Meijering", "Erik"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Meijering and Erik.,? \\Q2002\\E", "shortCiteRegEx": "Meijering and Erik.", "year": 2002}, {"title": "Predicting time series with support vector machines", "author": ["M\u00fcller", "K-R", "Smola", "Alexander J", "R\u00e4tsch", "Gunnar", "Sch\u00f6lkopf", "Bernhard", "Kohlmorgen", "Jens", "Vapnik", "Vladimir"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "M\u00fcller et al\\.,? \\Q1997\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 1997}, {"title": "Application of critical support vector machine to time series prediction. circuits and systems. iscas03", "author": ["T Raicharoen", "C Lursinsap", "P. Sanguanbhoki"], "venue": "In Proceedings of the 2003 International Symposium on,", "citeRegEx": "Raicharoen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Raicharoen et al\\.", "year": 2003}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["Ranzato", "MarcAurelio", "Szlam", "Arthur", "Bruna", "Joan", "Mathieu", "Michael", "Collobert", "Ronan", "Chopra", "Sumit"], "venue": "arXiv preprint arXiv:1412.6604,", "citeRegEx": "Ranzato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2014}, {"title": "Correcting forecasts with multifactor neural attention", "author": ["Riemer", "Matthew", "Vempaty", "Aditya", "Calmon", "Flavio P", "Heath III", "Fenno F", "Hull", "Richard", "Khabiri", "Elham"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Riemer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riemer et al\\.", "year": 2016}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Mike", "Paliwal", "Kuldip K"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Neural networks as forecasting experts: an empirical test", "author": ["Sharda", "Ramesh", "R. Patil"], "venue": "In Proceedings of the International Joint Conference on Neural Networks,", "citeRegEx": "Sharda et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Sharda et al\\.", "year": 1990}, {"title": "The summation of random causes as the source of cyclic processes", "author": ["Slutzky", "Eugen"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Slutzky and Eugen.,? \\Q1937\\E", "shortCiteRegEx": "Slutzky and Eugen.", "year": 1937}, {"title": "Modeling multiple time series with applications", "author": ["Tiao", "George C", "Box", "George EP"], "venue": "journal of the American Statistical Association,", "citeRegEx": "Tiao et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Tiao et al\\.", "year": 1981}, {"title": "Co, {NO2} and {NOx} urban pollution monitoring with on-field calibrated electronic nose by automatic bayesian regularization", "author": ["Vito", "Saverio De", "Piga", "Marco", "Martinotto", "Luca", "Francia", "Girolamo Di"], "venue": "Sensors and Actuators B: Chemical,", "citeRegEx": "Vito et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vito et al\\.", "year": 2009}, {"title": "On periodicity in series of related terms", "author": ["Walker", "Gilbert"], "venue": "Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character,", "citeRegEx": "Walker and Gilbert.,? \\Q1931\\E", "shortCiteRegEx": "Walker and Gilbert.", "year": 1931}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Werbos", "Paul J"], "venue": "Neural networks,", "citeRegEx": "Werbos and J.,? \\Q1988\\E", "shortCiteRegEx": "Werbos and J.", "year": 1988}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["SHI Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "DitYan", "Wong", "Wai-Kin", "Woo", "Wang-chun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xingjian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xingjian et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "S et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S et al\\.", "year": 2015}, {"title": "An investigation of neural networks for linear time-series forecasting", "author": ["Zhang", "Guoqiang Peter"], "venue": "Computers & Operations Research,", "citeRegEx": "Zhang and Peter.,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Peter.", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "Even though our approach is general in the sense that it can be applied to any sequence-to-sequence RNN, we consider in this study bidirectional RNNs (Schuster & Paliwal, 1997) based on Long Short-Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) with peephole connections (Gers et al., 2002), known to perform very well in practice (Graves, 2013), and we also make use of the attention mechanism recently introduced in (Bahdanau et al.", "startOffset": 284, "endOffset": 303}, {"referenceID": 0, "context": ", 2002), known to perform very well in practice (Graves, 2013), and we also make use of the attention mechanism recently introduced in (Bahdanau et al., 2014).", "startOffset": 135, "endOffset": 158}, {"referenceID": 11, "context": "The encoder represents each input xj , 1 \u2264 j \u2264 T as a hidden state \u2212\u2192 hj = f(xj , \u2212\u2212\u2192 hj\u22121), \u2212\u2192 hj \u2208 R, where the function f corresponds here to the non-linear transformation implemented in LSTM with peephole connections (Gers et al., 2002).", "startOffset": 221, "endOffset": 240}, {"referenceID": 0, "context": "More recently, in (Bahdanau et al., 2014), an attention mechanism is used to construct different context vectors ci for different outputs yi (1 \u2264 i \u2264 T \u2032) as a weighted sum of the hidden states of the encoder representing the input history: ci = \u2211T j=1 \u03b1ijhj , where \u03b1ij are the attention weights.", "startOffset": 18, "endOffset": 41}, {"referenceID": 38, "context": "For AQ, we selected the four variables associated to real sensors (thus excluding nominal sensors), namely C6H6(GT), NO2(GT), CO(GT) and NOx(GT) and predict the first one, C6H6(GT), as it is the most important for NMHC-related air pollution (Vito et al., 2009).", "startOffset": 241, "endOffset": 260}, {"referenceID": 6, "context": "Neural networks are regarded as a promising tool for time series prediction (Zhang, 2001; Crone et al., 2011) due to their data-driven and self-adaptive nature, their ability to approximate any continuous function and their inherent non-linearity.", "startOffset": 76, "endOffset": 109}, {"referenceID": 29, "context": "With the advent of SVM, it has been used to formulate time series prediction as a regression estimation using Vapnik\u2019s insensitive loss function and Huber\u2019s loss function (M\u00fcller et al., 1997).", "startOffset": 171, "endOffset": 192}, {"referenceID": 30, "context": "Since then different versions of SVMs are applied for time series prediction and many different SVM forecasting algorithms have been derived (Raicharoen et al., 2003; Fan et al., 2006).", "startOffset": 141, "endOffset": 184}, {"referenceID": 9, "context": "Since then different versions of SVMs are applied for time series prediction and many different SVM forecasting algorithms have been derived (Raicharoen et al., 2003; Fan et al., 2006).", "startOffset": 141, "endOffset": 184}, {"referenceID": 24, "context": "Random forest regression is used for prediction in the field of finance (Creamer & Freund, 2004) and bioinformatics (Kusiak et al., 2013), and are shown to outperform ARIMA (Kane et al.", "startOffset": 116, "endOffset": 137}, {"referenceID": 4, "context": "Early work (Connor et al., 1991) has shown that RNNs (a) are a type of nonlinear autoregressive moving average (NARMA) model and (b) outperform feedforward networks and various types of linear statistical models on time series.", "startOffset": 11, "endOffset": 32}, {"referenceID": 12, "context": "Subsequently, various RNN-based models were developed for different time series, as noisy foreign exchange rate prediction (Giles et al., 2001), chaotic time series prediction in communication engineering (Jaeger & Haas, 2004) or stock price prediction (Hsieh et al.", "startOffset": 123, "endOffset": 143}, {"referenceID": 16, "context": ", 2001), chaotic time series prediction in communication engineering (Jaeger & Haas, 2004) or stock price prediction (Hsieh et al., 2011).", "startOffset": 117, "endOffset": 137}, {"referenceID": 26, "context": "A detailed review can be found in (L\u00e4ngkvist et al., 2014) on the applications of RNNs along with other deep learning based approaches for different time series prediction tasks.", "startOffset": 34, "endOffset": 58}, {"referenceID": 10, "context": "They have furthermore been shown to outperform traditional RNNs on various temporal tasks (Gers et al., 2001; 2002).", "startOffset": 90, "endOffset": 115}, {"referenceID": 31, "context": "More recently, they have been used for predicting the next frame in a video and for interpolating intermediate frames (Ranzato et al., 2014), for forecasting the future rainfall intensity in a region (Xingjian et al.", "startOffset": 118, "endOffset": 140}, {"referenceID": 41, "context": ", 2014), for forecasting the future rainfall intensity in a region (Xingjian et al., 2015), or for modeling clinical data consisting of multivariate time series of observations (Lipton et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 27, "context": ", 2015), or for modeling clinical data consisting of multivariate time series of observations (Lipton et al., 2015).", "startOffset": 94, "endOffset": 115}, {"referenceID": 0, "context": "Adding attention mechanism on the decoder side of an encoder-decoder RNN framework enabled the network to focus on the interesting parts of the encoded sequence (Bahdanau et al., 2014).", "startOffset": 161, "endOffset": 184}, {"referenceID": 22, "context": ", 2015), image generation (Karol et al., 2015), phoneme recognition (Chorowski et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 3, "context": ", 2015), phoneme recognition (Chorowski et al., 2015), heart failure prediction (Choi et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 32, "context": ", 2016), as well as time series prediction (Riemer et al., 2016) and classification (Choi et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 14, "context": "Many studies also apply attention mechanism on external memory (Graves et al., 2014; 2016).", "startOffset": 63, "endOffset": 90}, {"referenceID": 32, "context": "The previous work (Riemer et al., 2016) uses attention mechanism to determine the importance of a factor among other factors that affect time series.", "startOffset": 18, "endOffset": 39}], "year": 2017, "abstractText": "In this paper, we study the use of recurrent neural networks (RNNs) for modeling and forecasting time series. We first illustrate the fact that standard sequence-to-sequence RNNs neither capture well periods in time series nor handle well missing values, even though many real life times series are periodic and contain missing values. We then propose an extended attention mechanism that can be deployed on top of any RNN and that is designed to capture periods and make the RNN more robust to missing values. We show the effectiveness of this novel model through extensive experiments with multiple univariate and multivariate datasets.", "creator": "LaTeX with hyperref package"}}}