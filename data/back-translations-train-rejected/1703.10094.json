{"id": "1703.10094", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Learning Inverse Mapping by Autoencoder based Generative Adversarial Nets", "abstract": "Generative Adversarial Net has shown its great ability in generating samples. The inverse mapping of generator also contains a great value. Some works have been developed to construct the inverse function of generator. However, the existing ways of training the inverse model of GANs have many shortcomings. In this paper, we propose a new approach of training the inverse model of generator by regarding a pre-trained generator as the decoder part of an autoencoder network. This model does not directly minimize the difference between original input and inverse output, but try to minimize the difference between the generated data by using original input and inverse output. This strategy overcome the difficulty in training a inverse model of a non one-to-one function. And the inverse mapping we learned can be directly used in image searching and processing.", "histories": [["v1", "Wed, 29 Mar 2017 15:23:40 GMT  (4411kb,D)", "http://arxiv.org/abs/1703.10094v1", "9 pages, 6 figures"], ["v2", "Tue, 12 Sep 2017 08:46:47 GMT  (895kb,D)", "http://arxiv.org/abs/1703.10094v2", "10 pages, 5 figures"]], "COMMENTS": "9 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junyu luo", "yong xu", "chenwei tang", "jiancheng lv"], "accepted": false, "id": "1703.10094"}, "pdf": {"name": "1703.10094.pdf", "metadata": {"source": "CRF", "title": "Learning Inverse Mapping by Autoencoder based Generative Adversarial Nets", "authors": ["Junyu Luo"], "emails": ["2015141462158@stu.scu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The paper by Deep convolutional adversarial nets [10] shows the great potential in mapping between image space X and latent space Z. This relationship can be used in image processing and search. It presents a major challenge in finding the inverse mapping of generators. Dumoulin [4] and Donahue [5] have a way of learning in the encoder network."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Generative Adversarial Nets", "text": "A generative model aims to generate high-quality artificial data from game theory using an adversarial strategy. [5] The GAN trains a discriminator and a generator at the same time and puts them in competition with each other. The generator aims to approximate the underlying unknown data distribution in order to deceive the discriminator, while the discriminator aims to detect which samples are real or fake. In the end, the generator can learn how to distribute real data."}, {"heading": "2.2 Autoencoder", "text": "AutoEncoder is an artificial neural network used for encoding through unattended leaning [1]. The goal of an AutoEncoder is to learn the characteristics of data and to represent the data based on the extracted characteristics. In AutoEncoder, an encoder and a decoder are trained simultaneously. Encoders take original data as input and the decoder attempts to reconstruct the original data based on the output of the encoder."}, {"heading": "3 Autoencoder based Generative Adversarial Nets", "text": "The idea of AEGAN is inspired by the auto encoder. We define two models G (z; \u03b8g) and D (x; \u03b8d), which represent the generator and discriminator of the original GAN. And we define an inverse generator IG (x; \u03b8ig), which represents the inverse model of the generator. \u03b8g, \u03b8d, \u03b8ig are the parameters of the generator G, discriminator D and the inverse generator IG. In this context, we consider the generator G (z; \u03b8g) as the decoder part of the auto encoder and the desired inverse generator IG (x; successively) as the encoder part."}, {"heading": "3.1 Training the Generator", "text": "The structure of the GANs in this thesis is based on Radford's DCGAN [10]. Details of the training of GANs can be found in Goodfellow's essay [5]. The first training goal is: min G max D V (D, G) = Ex \u0445 pdata (x) [logD (x)] + Ez \u0445 p (z) [log (1 \u2212 D (G (z)))] (1)"}, {"heading": "3.2 Training the Inverse Generator", "text": "To avoid the difficulty of training the encoder directly, we need the value function of IG to minimize the difference between the original output of generator G (z) and the reconstructed output G (G (z)). Therefore, we can generate the same images as z instead of being the same as e.g. This idea avoids the problem that mapping Z to X is not a biprojection. In AEGAN, we use the cross-entropy cost function to reset the difference between G (z) and the reconstructed output G (G (z)). The second training goal is: max IG V (IG) = Ez (z) [G (z) \u0445 logG (G)))) (IG (z)) + (1 \u2212 G (z)."}, {"heading": "4 Experiment Results", "text": "We evaluate the capability of this inverse model using the CelebFaces Attributes Dataset (CelebA) [8], a large FaceAttributes dataset with more than 200K celebrity images. All experiments below are in unattended state."}, {"heading": "4.1 Reconstructing Samples", "text": "Here we compare AEGAN with a directly trained inverse model (this model is reformed from ICGAN [9]. We delete the conditional vector from the original model due to the unattended state) and the contractively based inverse model BiGAN from Donahue [3]. Figure 2 shows the reconstructed results of AEGAN and directly trained inverse model. Figure 3 shows the reconstructed results of BiGAN. Figure 3 uses the different original samples as BiGAN cannot use a pre-trained generator as a basis, so we can fairly compare BiGAN with the generated samples from its own generator. In addition, we use the dHash as the standard for evaluating the similarity of the generated images. We take the average similarity as the final result. As we can see in Table 1, the result of AEGAN is also best in this case."}, {"heading": "4.2 Searching the Similar Images Using AEGAN", "text": "To illustrate the value of AEGAN, we show its ability to search for similar images. In this case, we are comparing only with the general image search algorithm. Since our approach is based on unattended learning, we do not provide additional information for this task. Therefore, it is fair to let our approach compete with them. We compare AEGAN with three general image search algorithms: dHash, pHash and Color Histogram. In addition, we use the Euclidean distance of corresponding z-vectors of images to obtain the similarity. If the distance is smaller, we think that two images are more similiar. We take an image from the original celebA data set and add some other factors to form the 3 test images. Then, we implement 4 different algorithms to find the next images in the first 20,000 images of celebrities. Figure 4 proves that AEGAN is very well suited for this task."}, {"heading": "4.3 Super-Resolution Using the AEGAN", "text": "To prove that our approach has learned the main features of facial images, we propose the third experiment. In this experiment, we take the Gaussian blur images as inputs for the inverse generator and then use the outputs of the inverse generator to reconstruct the original images. We choose the generated data as original examples, as the inverse model aims to learn the inverse image of the generator. From Figure 6, we can see that AEGAN performs well even in super-resolution."}, {"heading": "5 Conclusion And Further Works", "text": "AEGAN uses the idea of the auotoencoder to overcome the difficulty of building an inverse generator model, and the experiments show that the generator's inverse mapping has a very similar function to Word Embedding [6]. This ability can be very helpful in the field of image and video processing, and it is possible to get a universal vector representation of the image when we train the AEGAN approach on large image sets. And, it is worth applying this approach in video processing by first transforming each frame of the video into the corresponding image vector. Furthermore, we can use AEGAN to reform the image-to-image translation approach based on GAN [7]. With AEGAN, the generation part can be formed in an unattended state, and we only need to train the encoder part in a conditional situation. In other words, it is possible to train an image-to-image-to-network GAN when we are monitoring the AEGAN structure."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations & Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Inverting the generator of a generative adversarial network. 2016", "author": ["Antonia Creswell", "Anil Anthony Bharath"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Krhenbhl", "Trevor Darrell"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian J Goodfellow", "Jean Pougetabadie", "Mehdi Mirza", "Bing Xu", "David Wardefarley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio", "Zoubin Ghahramani", "Max Welling"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning distributed representations of concepts", "author": ["G E Hinton"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1986}, {"title": "Image-toimage translation with conditional adversarial networks. 2016", "author": ["Phillip Isola", "Jun Yan Zhu", "Tinghui Zhou", "Alexei A. Efros"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Invertible conditional gans for image editing", "author": ["Guim Perarnau", "Joost Van De Weijer", "Bogdan Raducanu", "Jose M. lvarez"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "The paper of Deep convolutional generative adversarial nets [10] shows the great potential in the mapping between image space X and latent space Z.", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "Dumoulin [4] and Donahue [3] proposed a way of learning encoder network \u2217", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Creswell and Bharath [2] proposed a different idea that can get a good correlation between samples and reconstructions.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "[9], the inverse model are trained by directly minimizing the difference between Encode(x) and (z, y).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "1 Generative Adversarial Nets A generative model aims to generate high quality artificial data by using adversarial strategy from game theory [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "2 Autoencoder AutoEncoder is an artificial neural network used for coding by unsupervised leaning [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "1 Training the Generator First we train the generator using the normal approach of training GANs(The structure of GANs in this paper is based on the DCGAN of Radford [10]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "The detail of training GANs can be found from the paper of Goodfellow [5].", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "We evaluate the ability of this inverse model on CelebFaces Attributes Dataset (CelebA) [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "compare AEGAN with a directly trained inverse model(This model is reformed from ICGAN [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "We delete the conditional vector from the original model because of the unsupervised condition) and the adversarial based inverse model BiGAN of Donahue [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "And the experiments show that the inverse mapping of generator has a very similar function compared with Word Embedding [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "Translation approach based on GAN [7].", "startOffset": 34, "endOffset": 37}], "year": 2017, "abstractText": "Generative Adversarial Net has shown its great ability in generating samples. The inverse mapping of generator also contains a great value. Some works have been developed to construct the inverse function of generator. However, the existing ways of training the inverse model of GANs have many shortcomings. In this paper, we propose a new approach of training the inverse model of generator by regarding a pre-trained generator as the decoder part of an autoencoder network. This model does not directly minimize the difference between original input and inverse output, but try to minimize the difference between the generated data by using original input and inverse output. This strategy overcome the difficulty in training a inverse model of a non one-to-one function. And the inverse mapping we learned can be directly used in image searching and processing.", "creator": "LaTeX with hyperref package"}}}