{"id": "1703.10135", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Tacotron: Towards End-to-End Speech Synthesis", "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given &lt;text, audio&gt; pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.", "histories": [["v1", "Wed, 29 Mar 2017 16:55:13 GMT  (465kb,D)", "http://arxiv.org/abs/1703.10135v1", "Submitted to Interspeech 2017"], ["v2", "Thu, 6 Apr 2017 21:20:34 GMT  (465kb,D)", "http://arxiv.org/abs/1703.10135v2", "Submitted to Interspeech 2017. v2 changed paper title to be consistent with our conference submission (no content change other than typo fixes)"]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["yuxuan wang", "rj skerry-ryan", "daisy stanton", "yonghui wu", "ron j weiss", "navdeep jaitly", "zongheng yang", "ying xiao", "zhifeng chen", "samy bengio", "quoc le", "yannis agiomyrgiannakis", "rob clark", "rif a saurous"], "accepted": false, "id": "1703.10135"}, "pdf": {"name": "1703.10135.pdf", "metadata": {"source": "CRF", "title": "TACOTRON: A FULLY END-TO-END TEXT-TO-SPEECH SYNTHESIS MODEL", "authors": ["Yuxuan Wang", "RJ Skerry-Ryan", "Daisy Stanton", "Yonghui Wu", "Ron J. Weiss", "Navdeep Jaitly", "Zongheng Yang", "Ying Xiao", "Zhifeng Chen", "Samy Bengio", "Quoc Le", "Yannis Agiomyrgiannakis", "Rob Clark", "Rif A. Saurous"], "emails": ["yxwang@google.com", "rjryan@google.com", "rif@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Modern Text-to-Speech (TTS) pipelines are more complex (Taylor, 2009). In addition, it is common for statistical parametric TTS to have a text frontend that includes various linguistic features, a permanent model, an acoustic prediction model, and complex signal processing techniques (Zen et al., 2009; Agiomyrgiannakis, 2015) These components are based on extensive domain expertise and are elaborate in design. They are also trained independently so that errors can be assembled from each component. Therefore, the complexity of modern TTS designs leads to considerable technical effort in building a new system. So there are many benefits of an integrated end-to-end-to-end TTS system that can rely on < Text > pairs with minimal human annotation. First, such a system alleviates the need for cumbersome feature engineering that can involve hay and fragile design decisions."}, {"heading": "2 RELATED WORK", "text": "WaveNet (van den Oord et al., 2016) is a powerful generative audio model. It works well for TTS, but is slow due to its auto-regressive nature at the sample level. It also requires conditioning linguistic features from an existing TTS front-end and is therefore not consistent: it merely replaces the vocoder and the acoustic model. Another recently developed neural model is DeepVoice (Arik et al., 2017), which replaces each component in a typical TTS pipeline with a corresponding neural network. However, each component is independently formed, and it is not trivial to modify the system to train it in end-to-end mode. To our knowledge, Wang et al. (2016) is the earliest work to touch the end-to-end TTS-to-end TTS-to-end TTS with seq2seq with attention."}, {"heading": "3 MODEL ARCHITECTURE", "text": "The backbone of Tacotron is a seq2seq model with attention (Bahdanau et al., 2014; Vinyals et al., 2015). Figure 1 shows the model, which includes an encoder, an attention-based decoder and a post-processing network. At a high level, our model takes signs as input and generates spectrogram frames, which are then converted into waveforms. We will describe these components below."}, {"heading": "3.1 CBHG MODULE", "text": "We first describe a building block called CBHG, illustrated in Figure 2. CBHG consists of a bank of 1-D convolution filters, followed by highway networks (Srivastava et al., 2015) and a bi-directional gated recurrent unit (GRU) (Chung et al., 2014). CBHG is a powerful module for extracting representations from sequences. Initially, the input sequence is entangled with K-sets of 1-D convolution filters, with the k-th set containing Ck filters of width k (i.e. k = 1, 2,., K). These filters explicitly model local and contextual information (similar to modeling of unigrams, bigrams, up to K-grams). Convolution outputs are stacked and further maximized to increase local inventories."}, {"heading": "3.2 ENCODER", "text": "The goal of the encoder is to extract robust sequential text representations. Input to the encoder is a string in which each character is represented as a uniform vector and embedded in a continuous vector. Subsequently, we apply a series of nonlinear transformations to each embedding, collectively referred to as \"premesh.\" In this work, we use a bottleneck layer with dropouts as premesh, which contributes to convergence and improves generalization. A CBHG module transforms the Prenet outputs into the final encoder representation used by the Attention Module. We found that this CBHG-based encoder not only reduces overpass, but also generates fewer error messages than a standard multi-layered RNN encoder (see our linked page with audio examples)."}, {"heading": "3.3 DECODER", "text": "The fact is that we will be able to hide, and that we will be able, we will be able to be able, we will be able to be able, we will be able to be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able, we will be able."}, {"heading": "3.4 POST-PROCESSING NET AND WAVEFORM SYNTHESIS", "text": "As mentioned above, the task of the post-processing mesh is to transform the seq2seq target into a target that can be synthesized into waveforms. As we use Griffin-Lim as a synthesizer, the post-processing mesh learns to predict spectral orders of magnitude sampled on a linear frequency scale. Another motivation of the post-processing mesh is that it can see the complete decoded sequence, although a simpler architecture also works. Unlike seq2seq, which always runs from left to right, it has both forward and backward information to correct the forecast error for each individual image. In this work, we use a CBHG module for the post-processing mesh, although a simpler architecture is likely."}, {"heading": "4 MODEL DETAILS", "text": "Table 1 lists the hyperparameters and network architectures. We use the log magnitude spectrogram with Hann window, 50 ms image length, 12.5 ms image shift, and 2048-point Fourier transformation. We also found the pre-emphasis (0.97) helpful. We use 24 kHz sampling rate for all experiments. We use r = 2 (output layer reduction factor) for the MOS results in this paper, although larger r values (e.g. r = 5) also work well. We use the Adam optimizer (Kingma & Ba, 2015) with learning rate decay starting at 0.0005 and being reduced to 0.0005, 0.0003, and 0.0001 after 500K, 1M, and 2M global steps, respectively."}, {"heading": "5 EXPERIMENTS", "text": "We train Tacotron on an internal North American English dataset containing approximately 24.6 hours of voice data spoken by a professional speaker. Phrases are normalized, e.g. \"16\" is converted to \"sixteen.\""}, {"heading": "5.1 ABLATION ANALYSIS", "text": "We are conducting a few ablation studies to understand the key components in our model. As is common with generative models, it is difficult to compare models based on objective metrics, which often do not correlate well with perception (Theis et al., 2015). We rely mainly on visual comparisons. We encourage readers to listen to the samplers provided. First, we are comparing with a vanilla seq2seq model, and the decoder uses two layers of RNNs in which each layer has 256 GRU cells (we tried to achieve similar results)."}, {"heading": "5.2 MEAN OPINION SCORE TESTS", "text": "We perform opinion tests in which subjects were asked to rate the naturalness of the stimuli in a 5-point Likert Score. MOS tests were crowdsourced by native speakers. 100 invisible phrases were used for the tests and each phrase received 8 ratings. In calculating MOS, we only include ratings using headphones. We compare our model to a parametric (based on LSTM (Zen et al., 2016) and a concatenating system (Gonzalvo et al., 2016), both of which are in production. As shown in Table 2, Tacotron achieves an MOS of 3.82 that exceeds the parametric system. Given the strong baseline and artifacts introduced by Griffin-Lim synthesis, this is a promising result."}, {"heading": "6 DISCUSSIONS", "text": "We have proposed Tacotron, an integrated end-to-end generative TTS model that uses a string as input and outputs the corresponding spectrogram. With a very simple waveform synthesis module, it achieves a 3.82 MOS score in US English, surpassing a production parametric system in terms of naturalness. Tacotron is frame-based, so inference is much faster than auto-regressive methods at the sample level. Unlike previous work, Tacotron does not require linguistic features or complex components such as an HMM aligner, but can be trained from the ground up by random initialization. We perform simple text normalization, although recent advances in learned text normalization (Sproat & Jaitly, 2016) may make this unnecessary in the future. We still need to study many aspects of our model; many early design decisions have remained unchanged. Our output layer, attention module, waveform, and handle function are currently suitable for high-loss enhancement in light."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Heiga Zen and Ziang Xie for constructive discussions and feedback."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Vocaine the vocoder and applications in speech synthesis", "author": ["Yannis Agiomyrgiannakis"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Agiomyrgiannakis.,? \\Q2015\\E", "shortCiteRegEx": "Agiomyrgiannakis.", "year": 2015}, {"title": "Deep voice: Real-time neural text-to-speech", "author": ["Sercan Arik", "Mike Chrzanowski", "Adam Coates", "Gregory Diamos", "Andrew Gibiansky", "Yongguo Kang", "Xian Li", "John Miller", "Jonathan Raiman", "Shubho Sengupta", "Mohammad Shoeybi"], "venue": "arXiv preprint arXiv:1702.07825,", "citeRegEx": "Arik et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arik et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Recent advances in Google real-time HMM-driven unit selection synthesizer", "author": ["Xavi Gonzalvo", "Siamak Tazari", "Chun-an Chan", "Markus Becker", "Alexander Gutkin", "Hanna Silen"], "venue": "In Proc. Interspeech,", "citeRegEx": "Gonzalvo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gonzalvo et al\\.", "year": 2016}, {"title": "Signal estimation from modified short-time fourier transform", "author": ["Daniel Griffin", "Jae Lim"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Griffin and Lim.,? \\Q1984\\E", "shortCiteRegEx": "Griffin and Lim.", "year": 1984}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1610.03017,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "SampleRNN: An unconditional end-to-end neural audio generation model", "author": ["Soroush Mehri", "Kundan Kumar", "Ishaan Gulrajani", "Rithesh Kumar", "Shubham Jain", "Jose Sotelo", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1612.07837,", "citeRegEx": "Mehri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mehri et al\\.", "year": 2016}, {"title": "Char2Wav: End-to-end speech synthesis", "author": ["Jose Sotelo", "Soroush Mehri", "Kundan Kumar", "Jo\u00e3o Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "venue": "In ICLR2017 workshop submission,", "citeRegEx": "Sotelo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sotelo et al\\.", "year": 2017}, {"title": "RNN approaches to text normalization: A challenge", "author": ["Richard Sproat", "Navdeep Jaitly"], "venue": "arXiv preprint arXiv:1611.00068,", "citeRegEx": "Sproat and Jaitly.,? \\Q2016\\E", "shortCiteRegEx": "Sproat and Jaitly.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Text-to-speech synthesis", "author": ["Paul Taylor"], "venue": "Cambridge university press,", "citeRegEx": "Taylor.,? \\Q2009\\E", "shortCiteRegEx": "Taylor.", "year": 2009}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "A\u00e4ron van den Oord", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "WaveNet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "First step towards end-to-end parametric TTS synthesis: Generating spectral parameters with neural attention", "author": ["Wenfu Wang", "Shuang Xu", "Bo Xu"], "venue": "In Proceedings Interspeech,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Statistical parametric speech synthesis", "author": ["Heiga Zen", "Keiichi Tokuda", "Alan W Black"], "venue": "Speech Communication,", "citeRegEx": "Zen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2009}, {"title": "Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices", "author": ["Heiga Zen", "Yannis Agiomyrgiannakis", "Niels Egberts", "Fergus Henderson", "Przemys\u0142aw Szczepaniak"], "venue": "Proceedings Interspeech,", "citeRegEx": "Zen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Modern text-to-speech (TTS) pipelines are complex (Taylor, 2009).", "startOffset": 50, "endOffset": 64}, {"referenceID": 23, "context": "For example, it is common for statistical parametric TTS to have a text frontend extracting various linguistic features, a duration model, an acoustic feature prediction model and a complex signal-processing-based vocoder (Zen et al., 2009; Agiomyrgiannakis, 2015).", "startOffset": 222, "endOffset": 264}, {"referenceID": 1, "context": "For example, it is common for statistical parametric TTS to have a text frontend extracting various linguistic features, a duration model, an acoustic feature prediction model and a complex signal-processing-based vocoder (Zen et al., 2009; Agiomyrgiannakis, 2015).", "startOffset": 222, "endOffset": 264}, {"referenceID": 5, "context": "Moreover, unlike end-to-end speech recognition (Chan et al., 2016) \u2217These authors really like tacos.", "startOffset": 47, "endOffset": 66}, {"referenceID": 22, "context": "or machine translation (Wu et al., 2016), TTS outputs are continuous, and output sequences are usually much longer than those of the input.", "startOffset": 23, "endOffset": 40}, {"referenceID": 16, "context": "In this paper, we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al.", "startOffset": 115, "endOffset": 139}, {"referenceID": 3, "context": ", 2014) with attention paradigm (Bahdanau et al., 2014).", "startOffset": 32, "endOffset": 55}, {"referenceID": 2, "context": "Another recently-developed neural model is DeepVoice (Arik et al., 2017), which replaces every component in a typical TTS pipeline by a corresponding neural network.", "startOffset": 53, "endOffset": 72}, {"referenceID": 14, "context": "Char2Wav (Sotelo et al., 2017) is an independently-developed end-to-end model that can be trained on characters.", "startOffset": 9, "endOffset": 30}, {"referenceID": 13, "context": "However, Char2Wav still predicts vocoder parameters before using a SampleRNN neural vocoder (Mehri et al., 2016), whereas Tacotron directly predicts raw spectrogram.", "startOffset": 92, "endOffset": 112}, {"referenceID": 2, "context": "Another recently-developed neural model is DeepVoice (Arik et al., 2017), which replaces every component in a typical TTS pipeline by a corresponding neural network. However, each component is independently trained, and it\u2019s nontrivial to change the system to train in an end-to-end fashion. To our knowledge, Wang et al. (2016) is the earliest work touching end-to-end TTS using seq2seq with attention.", "startOffset": 54, "endOffset": 329}, {"referenceID": 3, "context": "The backbone of Tacotron is a seq2seq model with attention (Bahdanau et al., 2014; Vinyals et al., 2015).", "startOffset": 59, "endOffset": 104}, {"referenceID": 20, "context": "The backbone of Tacotron is a seq2seq model with attention (Bahdanau et al., 2014; Vinyals et al., 2015).", "startOffset": 59, "endOffset": 104}, {"referenceID": 12, "context": "Figure 2: The CBHG (1-D convolution bank + highway network + bidirectional GRU) module adapted from Lee et al. (2016).", "startOffset": 100, "endOffset": 118}, {"referenceID": 6, "context": ", 2015) and a bidirectional gated recurrent unit (GRU) (Chung et al., 2014) recurrent neural net (RNN).", "startOffset": 55, "endOffset": 75}, {"referenceID": 9, "context": "We further pass the processed sequence to a few fixed-width 1-D convolutions, whose outputs are added with the original input sequence via residual connections (He et al., 2016).", "startOffset": 160, "endOffset": 177}, {"referenceID": 12, "context": "CBHG is inspired from work in machine translation (Lee et al., 2016), where the main differences from Lee et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 6, "context": ", 2015) and a bidirectional gated recurrent unit (GRU) (Chung et al., 2014) recurrent neural net (RNN). CBHG is a powerful module for extracting representations from sequences. The input sequence is first convolved with K sets of 1-D convolutional filters, where the k-th set contains Ck filters of width k (i.e. k = 1, 2, . . . ,K). These filters explicitly model local and contextual information (akin to modeling unigrams, bigrams, up to K-grams). The convolution outputs are stacked together and further max pooled along time to increase local invariances. Note that we use a stride of 1 to preserve the original time resolution. We further pass the processed sequence to a few fixed-width 1-D convolutions, whose outputs are added with the original input sequence via residual connections (He et al., 2016). Batch normalization (Ioffe & Szegedy, 2015) is used for all convolutional layers. The convolution outputs are fed into a multi-layer highway network to extract high-level features. Finally, we stack a bidirectional GRU RNN on top to extract sequential features from both forward and backward context. CBHG is inspired from work in machine translation (Lee et al., 2016), where the main differences from Lee et al. (2016) include using non-causal convolutions, batch normalization, residual connections, and stride=1 max pooling.", "startOffset": 56, "endOffset": 1234}, {"referenceID": 22, "context": "We use a stack of GRUs with vertical residual connections (Wu et al., 2016) for the decoder.", "startOffset": 58, "endOffset": 75}, {"referenceID": 20, "context": "Vinyals et al. (2015)), where a stateful recurrent layer produces the attention query at each decoder time step.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "Vinyals et al. (2015)), where a stateful recurrent layer produces the attention query at each decoder time step. We concatenate the context vector and the attention RNN cell output to form the input to the decoder RNNs. We use a stack of GRUs with vertical residual connections (Wu et al., 2016) for the decoder. We found the residual connections speed up convergence. The decoder target is an important design choice. While we could directly predict raw spectrogram, it\u2019s a highly redundant representation for the purpose of learning alignment between speech signal and text (which is really the motivation of using seq2seq for this task). Because of this redundancy, we use a different target for seq2seq decoding and waveform synthesis. The seq2seq target can be highly compressed as long as it provides sufficient intelligibility and prosody information for an inversion process, which could be fixed or trained. We use 80-band mel-scale spectrogram as the target, though fewer bands or more concise targets such as cepstrum could be used. We use a post-processing network (discussed below) to convert from the seq2seq target to waveform. We use a simple fully-connected output layer to predict the decoder targets. An important trick we discovered was predicting multiple, non-overlapping output frames at each decoder step. Predicting r frames at once divides the total number of decoder steps by r, which reduces model size, training time, and inference time. More importantly, we found this trick to substantially increase convergence speed, as measured by a much faster (and more stable) alignment learned from attention. This is likely because neighboring speech frames are correlated and each character usually corresponds to multiple frames. Emitting one frame at a time forces the model to attend to the same input token for multiple timesteps; emitting multiple frames allows the attention to move forward early in training. A similar trick is also used in Zen et al. (2016) but mainly to speed up inference.", "startOffset": 0, "endOffset": 1989}, {"referenceID": 4, "context": "Since we do not use techniques such as scheduled sampling (Bengio et al., 2015) (we found it to hurt audio quality), the dropout in the pre-net is critical for the model to generalize, as it provides a noise source to resolve the multiple modalities in the output distribution.", "startOffset": 58, "endOffset": 79}, {"referenceID": 13, "context": "It could be used to predict alternative targets such as vocoder parameters, or as a WaveNet-like neural vocoder (van den Oord et al., 2016; Mehri et al., 2016; Arik et al., 2017) that synthesizes waveform samples directly.", "startOffset": 112, "endOffset": 178}, {"referenceID": 2, "context": "It could be used to predict alternative targets such as vocoder parameters, or as a WaveNet-like neural vocoder (van den Oord et al., 2016; Mehri et al., 2016; Arik et al., 2017) that synthesizes waveform samples directly.", "startOffset": 112, "endOffset": 178}, {"referenceID": 0, "context": "We implemented Griffin-Lim in TensorFlow (Abadi et al., 2016) hence it\u2019s also part of the model.", "startOffset": 41, "endOffset": 61}, {"referenceID": 18, "context": "As is common for generative models, it\u2019s hard to compare models based on objective metrics, which often do not correlate well with perception (Theis et al., 2015).", "startOffset": 142, "endOffset": 162}, {"referenceID": 24, "context": "We compare our model with a parametric (based on LSTM (Zen et al., 2016)) and a concatenative system (Gonzalvo et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 7, "context": ", 2016)) and a concatenative system (Gonzalvo et al., 2016), both of which are in production.", "startOffset": 36, "endOffset": 59}], "year": 2017, "abstractText": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-tosequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it\u2019s substantially faster than sample-level autoregressive methods.", "creator": "LaTeX with hyperref package"}}}