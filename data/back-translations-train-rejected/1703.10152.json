{"id": "1703.10152", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Automatic Argumentative-Zoning Using Word2vec", "abstract": "In comparison with document summarization on the articles from social media and newswire, argumentative zoning (AZ) is an important task in scientific paper analysis. Traditional methodology to carry on this task relies on feature engineering from different levels. In this paper, three models of generating sentence vectors for the task of sentence classification were explored and compared. The proposed approach builds sentence representations using learned embeddings based on neural network. The learned word embeddings formed a feature space, to which the examined sentence is mapped to. Those features are input into the classifiers for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on the Argumentative-Zoning (AZ) annotated articles. The results showed that simply averaging the word vectors in a sentence works better than the paragraph to vector algorithm and by integrating specific cuewords into the loss function of the neural network can improve the classification performance. In comparison with the hand-crafted features, the word2vec method won for most of the categories. However, the hand-crafted features showed their strength on classifying some of the categories.", "histories": [["v1", "Wed, 29 Mar 2017 17:33:27 GMT  (21kb)", "http://arxiv.org/abs/1703.10152v1", "13 pages; 5 tables"]], "COMMENTS": "13 pages; 5 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haixia liu"], "accepted": false, "id": "1703.10152"}, "pdf": {"name": "1703.10152.pdf", "metadata": {"source": "CRF", "title": "Automatic Argumentative-Zoning Using Word2vec", "authors": ["Haixia Liu"], "emails": ["khyx3lhi@nottingham.edu.my"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.10 152v 1 [cs.C L] 29 M"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "The word2vec tool is by Mikolov et al. [17] It has recently attracted a lot of attention. With word2vec tool, one can learn from large amounts of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. the idea behind the word embeddings is the distributed representation [20] to place each word into the k-dimensional vector. How these vectors are generated with word2vec tool is the neural probability calculus [21]. The underlying word representations for each word are achieved during the formation of the language model."}, {"heading": "3 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Models", "text": "In this study, sentence embeddings of large text corpus were learned as features to classify sentences into seven categories in the task of AZ. Three models were researched to obtain sentence vectors: averaging of the vectors of words in a sentence, paragraph vectors and specific word vectors. The first model, averaging of word vectors (AV GWV EC), consists in learning the vectors in word order w = < w1, x2,... wn > on average. The main process in this model is to embed the word matrix Ww: Vavgwvec (w) = 1 n-W xiw (4), where Ww is the word embedding for word xi learned by the classical word2vec algorithm [17]. The second model, PARAV EC, aims at the formation of paragraph vectors."}, {"heading": "3.2 Classification and evaluation", "text": "The learned word embeddings are entered into a classifier as features under a monitored machine learning framework. Similar to mood classification using word embeddings [22], where attempts are made to predict each tweet either positively or negatively, the embeddings are used in AZ task to classify each sentence into one of the seven categories. Precision, recall and Fmeasure were calculated to evaluate the classification performance."}, {"heading": "4 Experimental Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Training Dataset", "text": "ACL Collection. ACL Anthology Reference Corpus 3 contains the canonical 10,921 computer-aided linguistic works from which 622,144 sentences were generated after lower-quality sentences were filtered out. MixedAbs Collection contains 6,778 sentences extracted from the titles and summaries of WEB OF SCIENCE 4 publications."}, {"heading": "4.2 Test Dataset", "text": "Argumentative Zoning Corpus (AZ corpus) consists of 80 AZ annotated conference articles of computer linguistics, originally taken from the Cmplg arXiv. 5. After concatenation of subsets, 7,347 labeled sentences were obtained."}, {"heading": "4.3 Training strategy", "text": "To compare the effectiveness of the three models in the AZ task, the three models were trained on the same ACL dataset (introduced in the dataset section).The word2vec were also trained on different parameters, e.g. 3http: / / acl-arc.comp.nus.edu.sg / 4webofknowledge.com 5http: / / www.cl.cam.ac.uk / \u02dc sht25 / AZ corpus.htmlas different feature dimensions. To evaluate the effects from different areas, the first model was built on different corpus.The features of Word embedding based on different models and datasets are listed in Table 2."}, {"heading": "4.4 Parameters", "text": "Inspired by the work of Sadeghian and Sharafat [26] 6, the word-to-vector attribute was conceived as follows: the minimum number of words is 40; the number of parallel threads is 4 and the context window 10."}, {"heading": "4.5 Strategy of dealing with unbalanced data", "text": "In this experiment, the test dataset is an unbalanced dataset. Table 3 shows the distribution of rhetorical categories from the AZ dataset. The categories OWN and OTH clearly outperform other categories. To solve the problem of classifying unbalanced data, synthetic minority sample TEchnique (SMOTE) [29] was performed on the original dataset. A 10-cross-validation scheme was introduced and the results averaged out of 10 iterations."}, {"heading": "4.6 Results of classification for per category", "text": "Table 4 and 5 show the classification performance of different methods. 7The results were examined from the following aspects: If the feature dimension is set to 100 and the training corpus is ACL, the results generated by different models were compared (AVGWVEC, 7Note that it is not fully compatible with the results of Teufel 2002, since the data set is different due to the chaining in this paper. But Teufel's reports could be a reference.PARAVEC and AVGWVEC + BSWE only for the BAS category). Looking at the F measure, AVGWVEC performs better than PARAVEC, but PARAVEC gave better precision results in several categories, such as AIM, TXT and OWN. Results showed that the PARAVEC model is not robust, for example, it performs poorly for the category of BAS."}, {"heading": "5 Discussion", "text": "The classification results showed that the type of word embedding and the training corpus influences the performance of the AZ. As a simple model, AV GWV EC performs better than others, suggesting that averaging the word vectors in a sentence can capture the semantic property of statements. By training specific argumentation corpus embedding, performance can be improved, as can be seen in the case of detecting the BAS status using the BSWE model. The feature dimension does not dominate the results. There is no significant difference between the results generated by 300-dimensional characteristics and 100-dimensional characteristics. Training corpus influences the results. ACL + AZ, which surpasses the other, indicates that the topics of the training corpus are important factors for argumentative zoning."}, {"heading": "6 Conclusion", "text": "The results showed that word embeddings are effective at classifying sentences from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic characteristics of statements and are easier to obtain than handmade features. To improve sentence classification for a particular category, the integration of a word-specific embeddedness strategy helps. The size of the feature pool does not play too much of a role in the results, nor does the vocabulary size. In comparison, the area of the training corpus influences the classification performance."}], "references": [{"title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies", "author": ["D.R. Radev", "H. Jing", "M. Budzikowska"], "venue": "Proceedings of the 2000 NAACL- ANLP Workshop on Automatic summarization. Association for Computational Linguistics, 2000, pp. 21\u201330.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Identifying topics by position", "author": ["C.-Y. Lin", "E. Hovy"], "venue": "Proceedings of the fifth conference on Applied natural language processing. Association for Computational Linguistics, 1997, pp. 283\u2013290.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Summarizing scientific articles: experiments with relevance and rhetorical status", "author": ["S. Teufel", "M. Moens"], "venue": "Computational linguistics, vol. 28, no. 4, pp. 409\u2013445, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Argumentative zoning: Information extraction from scientific text", "author": ["S. Teufel"], "venue": "Ph.D. dissertation, Citeseer, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Advances in natural language processing", "author": ["J. Hirschberg", "C.D. Manning"], "venue": "Science, vol. 349, no. 6245, pp. 261\u2013266, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing for online applications: Text retrieval, extraction and categorization", "author": ["P. Jackson", "I. Moulinier"], "venue": "John Benjamins Publishing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Ranking with recursive neural networks and its application to multi-document summarization", "author": ["Z. Cao", "F. Wei", "L. Dong", "S. Li", "M. Zhou"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentiment analysis: Capturing favorability using natural language processing", "author": ["T. Nasukawa", "J. Yi"], "venue": "Proceedings of the 2nd international conference on Knowledge capture. ACM, 2003, pp. 70\u201377.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and trends in information retrieval, vol. 2, no. 1-2, pp. 1\u2013135, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Predicting the future with social media", "author": ["S. Asur", "B. Huberman"], "venue": "Web Intelligence and Intelligent Agent Technology (WI- IAT), 2010 IEEE/WIC/ACM International Conference on, vol. 1. IEEE, 2010, pp. 492\u2013499.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Document-word co-regularization for semi-supervised sentiment analysis", "author": ["V. Sindhwani", "P. Melville"], "venue": "Data Mining, 2008.  ICDM\u201908. Eighth IEEE International Conference on. IEEE, 2008, pp. 1025\u20131030.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine learning, vol. 42, no. 1-2, pp. 177\u2013196, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A survey on automatic text summarization", "author": ["D. Das", "A.F. Martins"], "venue": "Literature Survey for the Language and Statistics II course at CMU, vol. 4, pp. 192\u2013195, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "A multiclass-based classification strategy for rhetorical sentence categorization from scientific papers", "author": ["D.H. Widyantoro", "M.L. Khodra", "B. Riyanto", "A. Aziz"], "venue": "FormaMente: Rivista internazionale di ricerca sul futuro digitale, no. 3-2014, p. 223, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u2013 1554, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification", "author": ["P. Wang", "B. Xu", "J. Xu", "G. Tian", "C.-L. Liu", "H. Hao"], "venue": "Neurocomputing, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 801\u2013809.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations", "author": ["M.J. Hinton", "Geoffrey", "D. Rumelhart"], "venue": "1986.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1986}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu", "B. Qin"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, vol. 1, 2014, pp. 1555\u20131565.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "A study on sentiment computing and classification of sina weibo with word2vec", "author": ["B. Xue", "C. Fu", "Z. Shaobin"], "venue": "Big Data (BigData Congress), 2014 IEEE International Congress on. IEEE, 2014, pp. 358\u2013363.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Chinese comments sentiment classification based on word2vec and svm perf", "author": ["D. Zhang", "H. Xu", "Z. Su", "Y. Xu"], "venue": "Expert Systems with Applications, vol. 42, no. 4, pp. 1857\u20131863, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1857}, {"title": "Support vector machines and word2vec for text classification with semantic features", "author": ["J. Lilleberg", "Y. Zhu", "Y. Zhang"], "venue": "Cognitive Informatics & Cognitive Computing (ICCI* CC), 2015 IEEE 14th International Conference on. IEEE, 2015, pp. 136\u2013140.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Borderline oversampling for imbalanced data classification", "author": ["H.M. Nguyen", "E.W. Cooper", "K. Kamei"], "venue": "International Journal of Knowledge Engineering and Soft Data Paradigms, vol. 3, no. 1, pp. 4\u201321, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Smote: synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of artificial intelligence research, pp. 321\u2013357, 2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Unlike document summarization from news circles, where the task is to identify centroid sentences [1] or to extract the first few sentences of the", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "paragraphs [2], summarization of scientific articles involves extra text processing stage [3].", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "paragraphs [2], summarization of scientific articles involves extra text processing stage [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Rhetorical sentence classification, also known as argumentative zoning (AZ) [4], is a process of assigning rhetorical status to the extracted sentences.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "[3] introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "With the development of artificial intelligence, machine learning and computational linguistics, Natural Language Processing (NLP) has become a popular research area [5, 6].", "startOffset": 166, "endOffset": 172}, {"referenceID": 5, "context": "NLP covers the applications from document retrieval, text categorization [7], document summarization [8] to sentiment analysis [9, 10].", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "NLP covers the applications from document retrieval, text categorization [7], document summarization [8] to sentiment analysis [9, 10].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "NLP covers the applications from document retrieval, text categorization [7], document summarization [8] to sentiment analysis [9, 10].", "startOffset": 127, "endOffset": 134}, {"referenceID": 8, "context": "NLP covers the applications from document retrieval, text categorization [7], document summarization [8] to sentiment analysis [9, 10].", "startOffset": 127, "endOffset": 134}, {"referenceID": 9, "context": "Those applications are targeting different types of text resources, such as articles from social media [11] and scientific publications [3].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Those applications are targeting different types of text resources, such as articles from social media [11] and scientific publications [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "From machine learning prospective, text can be analysed via supervised [3], semi-supervised [12] and unsupervised [13] algorithms.", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "From machine learning prospective, text can be analysed via supervised [3], semi-supervised [12] and unsupervised [13] algorithms.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "From machine learning prospective, text can be analysed via supervised [3], semi-supervised [12] and unsupervised [13] algorithms.", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "Although the hand-crafted features have shown the ability for document summarization and sentiment analysis [14, 10], there are not enough efficient features to capture the semantic relations between words, phrases and sentences.", "startOffset": 108, "endOffset": 116}, {"referenceID": 8, "context": "Although the hand-crafted features have shown the ability for document summarization and sentiment analysis [14, 10], there are not enough efficient features to capture the semantic relations between words, phrases and sentences.", "startOffset": 108, "endOffset": 116}, {"referenceID": 2, "context": "[3] have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "features, qualifying adjectives and meta-discourse features [15] to explore AZ task.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "With the advent of neural networks [16], it is possible for computers to learn feature representations automatically.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "Recently, word embedding technique [17] has been widely used in the NLP community.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "There are plenty of cases where word embedding and sentence representations have been applied to short text classification [18] and paraphrase detection [19].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "There are plenty of cases where word embedding and sentence representations have been applied to short text classification [18] and paraphrase detection [19].", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "[17] has gained a lot attention recently.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The idea behind word embeddings is to use distributed representation [20] to map each word into k-dimension vector.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "[17] introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Word2vec has been applied to sentiment analysis [22, 23, 24] and text classification [25].", "startOffset": 48, "endOffset": 60}, {"referenceID": 21, "context": "Word2vec has been applied to sentiment analysis [22, 23, 24] and text classification [25].", "startOffset": 48, "endOffset": 60}, {"referenceID": 22, "context": "Word2vec has been applied to sentiment analysis [22, 23, 24] and text classification [25].", "startOffset": 48, "endOffset": 60}, {"referenceID": 23, "context": "Word2vec has been applied to sentiment analysis [22, 23, 24] and text classification [25].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "[22] used the concatenation of vectors derived from different convolutional layers to analyze the sentiment statements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Le and Mikolov [27] introduced the concept of word vector representation in a formal way:", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "where Ww is the word embedding for word xi, which is learned by the classical word2vec algorithm [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "(PV-DM) [27], which is an extension of word2vec.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "[22]\u2019s model: Sentiment-Specific Word Embedding (unified model: SSWEu).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Similar to sentiment classification using word embeddings [22], where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "In imbalanced data sets, some classes are significantly outnumbered by other classes [28], which affects the classification results.", "startOffset": 85, "endOffset": 89}, {"referenceID": 26, "context": "To deal with the problem of classification on unbalanced data, synthetic Minority Over-sampling TEchnique (SMOTE) [29] were performed on the original dataset.", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "[3] gave better results.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "In comparison with document summarization on the articles from social media and newswire, argumentative zoning (AZ) is an important task in scientific paper analysis. Traditional methodology to carry on this task relies on feature engineering from different levels. In this paper, three models of generating sentence vectors for the task of sentence classification were explored and compared. The proposed approach builds sentence representations using learned embeddings based on neural network. The learned word embeddings formed a feature space, to which the examined sentence is mapped to. Those features are input into the classifiers for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on the Argumentative-Zoning (AZ) annotated articles. The results showed that simply averaging the word vectors in a sentence works better than the paragraph to vector algorithm and by integrating specific cuewords into the loss function of the neural network can improve the classification performance. In comparison with the hand-crafted features, the word2vec method won for most of the categories. However, the hand-crafted features showed their strength on classifying some of the categories.", "creator": "LaTeX with hyperref package"}}}