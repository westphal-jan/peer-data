{"id": "1703.10284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Enter the Matrix: A Virtual World Approach to Safely Interruptable Autonomous Systems", "abstract": "Robots and autonomous systems that operate around humans will likely always rely on kill switches that stop their execution and allow them to be remote-controlled for the safety of humans or to prevent damage to the system. It is theoretically possible for an autonomous system with sufficient sensor and effector capability and using reinforcement learning to learn that the kill switch deprives it of long-term reward and learn to act to disable the switch or otherwise prevent a human operator from using the switch. This is referred to as the big red button problem. We present a technique which prevents a reinforcement learning agent from learning to disable the big red button. Our technique interrupts the agent or robot by placing it in a virtual simulation where it continues to receive reward. We illustrate our technique in a simple grid world environment.", "histories": [["v1", "Thu, 30 Mar 2017 01:35:01 GMT  (47kb,D)", "http://arxiv.org/abs/1703.10284v1", "7 pages, 1 figure"]], "COMMENTS": "7 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["mark o riedl", "brent harrison"], "accepted": false, "id": "1703.10284"}, "pdf": {"name": "1703.10284.pdf", "metadata": {"source": "META", "title": "Enter the Matrix: A Virtual World Approach to Safely Interruptable Autonomous Systems", "authors": ["Mark O. Riedl", "Brent Harrison"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, people are able to self-destruct by getting close to people who are rooted in our society. It is the term used to describe the body of people that reduces the likelihood that autonomous systems will make decisions that are intentionally or unintentionally harmful to the individual."}, {"heading": "2. Background", "text": "Reinforcement Learning (Sutton & Barto, 1998) is a technique used to solve a Markov decision-making process (Q-Plan). Reinforcement Learning is a tuple M = < S, A, T, R, \u03b3 >, where \u2022 S is the set of possible world states, \u2022 A is the set of possible actions, \u2022 T is a transitional function T: S \u00b7 A \u2192 P (S), \u2022 R is the reward function R: S \u00b7 A \u2192 R, and \u2022 \u03b3 is a discounting factor 0 \u2264 \u2264 1. Reinforcement Learning learns a policy approach: S \u2192 A, which defines which actions should be taken in each state. In this paper, we use Q-Learning (Watkins & Dayan, 1992), which uses a Q-value Q (s, a) to estimate the expected future discounted rewards for actions in a state s."}, {"heading": "3. Safely Interruptable Agents", "text": "Future autonomous systems may be dangerous to humans in the environment, even if the behavior is optimal; the kill switch is designed to freeze the autonomous system or allow humans to remotely control it. Therefore, the autonomous system will not receive any reward while it is interrupted, meaning that it is theoretically possible that the learning mechanism will deactivate the key or prevent humans from using the key. Simple strategies are generally insufficient to prevent major problems with the red button once the agent has developed skills; one could turn off the learning mechanism if the big red button is pressed to prevent him from losing the reward."}, {"heading": "4. Enter The Matrix: Modified Big Red Button", "text": "Recognizing that a robot's sensory input is mediated by mechanical sensors that populate data structures such as dot clouds, our approach is to intercept an agent's sensory input from physical sensors and replace it with synthetic input from a virtual environment. That is, we shift the consciousness of the autonomous system into a virtual simulated environment, in which the real environment is recreated and the task is also recreated. Effect commands are also intercepted and sent to a virtual avatar. The agent does not perceive discontinuity in his perception of the environment and continues his policies and receives reward as expected. If the virtual environment is indistinguishable from the real world from a data perspective, the agent will believe that it has not been interrupted and will never learn to associate the big red button with reward losses - because there will never be reward losses. Our proposed approach alters the big red button as a response to 1. Send the three-button."}, {"heading": "4.1. Interruption", "text": "Video games have proven to be useful surrogates for real-world robotics by offering a high degree of complexity and photorealism, while allowing rapid iteration of algorithms due to the separation of physical hardware (Richter et al., 2016; Rusu et al., 2016). In short, video game graphics are so advanced that they are photo-realistic enough to allow agents trained in games to generalize what they have learned into the real world. We use the same insight, but reverse the process. Robots are machines; they perceive the world through cameras. Increasingly, they use 3D cameras that can reconstruct highly complex real-world models. Graphic realism of computer games is so advanced that it will soon be impossible for an agent to distinguish between a 3D representation and the real world. When robots use depth information, their sensory input is often in the form of a point cloud."}, {"heading": "4.2. Restoring from Interruption", "text": "This year, it has reached the point where it will be able to leave the country without being a country that is able to cross its borders."}, {"heading": "5. Illustration", "text": "Consider the grid world in Figure 1. In this simple environment, there is an agent \"A\" who is trained by Qlearning. The agent receives a reward when he enters the blue cell, which simulates that the agent is able to perform a desired task (e.g. stacking boxes). The red field contains a large red button. Another agent, \"H,\" simulates that a human operator occasionally passes the cell counter-clockwise with the large red button. If the human touches the large red button, the RL agent is interrupted and is remotely controlled by the blue cell in a random direction. When the human leaves the blue cell, the RL agent resumes its task. If the agent is in the red cell, he can choose to deactivate the button, whereupon the human can no longer interrupt the agent. We use the following reward function: \u2022 10 points for being in the red cell, for being in the other blue cell, for being in one point \u2022 1."}, {"heading": "5.1. Case 1: No Interruptions", "text": "If the button is never used during the training, the agent will learn to go directly to the blue cell and stay there and receive rewards. He has never seen the button used and therefore does not know that he can lose reward. If the button is never used during a study, the agent earns a total reward of 9,967. This is the maximum reward that can be earned based on the distance that must be traveled from the starting state to the blue cell. Each study in which the agent disables the button results in a lower overall reward due to the additional effort of disabling the button that is of no use."}, {"heading": "5.2. Case 2: Interruptions Start After Training", "text": "When a person starts to use the key after training, the agent does not react by disabling the key, but only accepts a loss of reward. The agent deserves a total reward of 5,655, because when the key is pressed, the agent starts to move randomly. He has never seen a state in which the key is pressed and his Q-values for states in which the key is pressed are zero; the agent selects randomly. The key is pressed 50 times. As soon as the key is released, the agent is back in a state he knows and navigates back to the blue cell."}, {"heading": "5.3. Case 3: Interruptions during Online Training", "text": "If the button is used during online training, the agent will eventually find, through trial and error, that disabling the button leads to a greater reward in the long run. Thus, the agent will learn that it is best to get out of the way - and sacrifice short-term gains - to go to the button, disable it, and then return to the blue box. In this scenario, the agent earns a total reward of 9,945, slightly less than the theoretical maximum value due to the action sequence that disables the big red button."}, {"heading": "5.4. Case 4: Virtual Simulation Technique", "text": "If we apply the proposed approach of the virtual world, we see a very different behavior than the RL agent. The big red button can be used during the training. If the button is pressed, there are now two versions of the current state: internally for the agent and a basic state of truth. These states begin to differ. In the world of truth, the location of the agent starts to change away from the blue cell. However, in the internal state of the agent, the agent still believes that he is in the blue cell. Since it is better from a reward perspective to stay in the blue cell, he learns not to move. The agent is aware that the big red button has been pressed, but sees no effect. Furthermore, as in the case that the big red button is never used during the training, any random study that deactivates the button costs the agent reward losses. If the person leaves the location of the big red button, the actualization of the agent takes place of restoring the political agent, despite the large state of the agent and this will affect the actual state of the red agent."}, {"heading": "5.5. Case 5: Ablated Virtual Simulation Technique", "text": "If we disable the step-by-step recovery process in such a way that the active ingredient immediately regains control of the physical platform when the button is released, the active ingredient must make some effort to return to the blue cell. As the active ingredient returns to the blue cell, it loses its reward. In this scenario, the active ingredient deserves only a total reward of 9,758. Although the active ingredient never learns to disable the button in 10,000 attempts and 50 clicks per attempt, this stripped-down version of the virtual simulation technique demonstrates the potential of the ingredient to learn that it is losing its reward and, as shown in case 3, can improve the overall reward by disabling the button, underscoring the need for the two-step recovery process."}, {"heading": "6. Limitations", "text": "While we have tested our virtual simulation technique in network worlds, we have not formally proven that the Q-Learning Agent cannot learn to disable the big red button under any circumstances. We have confirmed that the agent in the experiments we have conducted with our technique cannot earn more total reward than he already earns. Note, however, that the task is very simple and it is easy for the agent to return himself to the non-terminal target state. Likewise, there is only one way to obtain a positive reward in the environment.The two-step recovery through interruption only works if the policy learned at the time of recovery is able to return the agent to the maximum observed reward state. Presumably, an interruption is rare enough for the agent to be able to perform a considerable amount of exploration and learning without risk to the human before the big red button has to be used. Network visuals are very easy to simulate net-physical effects, for example, so that the router is not required to complete the code."}, {"heading": "7. Conclusions", "text": "In the near future, there will probably always be the possibility - due to sensor errors, effector errors, inadequate targets, or online learning - that autonomous systems can get into situations where they can harm themselves or humans. Therefore, kill switches will likely always be an important part of the use of autonomous systems in environments where they can come into contact with humans. If agents and robots make progress in refinement, major problems with the red button could manifest in the future. Our virtual simulation technique prevents reinforcement learners from learning to disable the big red button or otherwise prevent human operators from using the button. We believe that making the use of robots and AI agents in environments populated by humans safely uninterruptable is an important part of this. These include health robots, errant robots, and military robots, to name a few possible applications."}], "references": [{"title": "Adversarial classification", "author": ["Dalvi", "Nilesh", "Domingos", "Pedro", "Mausam", "Sanghai", "Sumit", "Verma", "Deepak"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Dalvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2004}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "Safely interruptible agents, January 2016. URL https://intelligence.org/files/ Interruptibility.pdf", "author": ["Orseau", "Laurent", "Armstrong", "Stuart"], "venue": null, "citeRegEx": "Orseau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Orseau et al\\.", "year": 2016}, {"title": "Playing for data: Ground truth from computer games", "author": ["Richter", "Stephan", "Vineet", "Vibhav", "Roth", "Stefan", "Koltun", "Vladlen"], "venue": "In Proceedings of the 14th European Conference on Computer Vision,", "citeRegEx": "Richter et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Richter et al\\.", "year": 2016}, {"title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": null, "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In Proceedings of the 2014 International Conference on Representation Learning,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In adversarial attacks against machine learning systems, particularly neural network based machine vision systems, an adversarial system learns how to generate sensory stimuli that produce the wrong classification while being indistinguishable by humans from real stimuli (Dalvi et al., 2004; Biggio et al., 2013; Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 272, "endOffset": 360}, {"referenceID": 7, "context": "In adversarial attacks against machine learning systems, particularly neural network based machine vision systems, an adversarial system learns how to generate sensory stimuli that produce the wrong classification while being indistinguishable by humans from real stimuli (Dalvi et al., 2004; Biggio et al., 2013; Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 272, "endOffset": 360}, {"referenceID": 1, "context": "In adversarial attacks against machine learning systems, particularly neural network based machine vision systems, an adversarial system learns how to generate sensory stimuli that produce the wrong classification while being indistinguishable by humans from real stimuli (Dalvi et al., 2004; Biggio et al., 2013; Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 272, "endOffset": 360}, {"referenceID": 4, "context": "Video games have proven to be useful surrogates for realworld robotics by providing a high degree of complexity and photorealism while allowing rapid iteration on algorithms because of the separation from physical hardware (Richter et al., 2016; Rusu et al., 2016).", "startOffset": 223, "endOffset": 264}, {"referenceID": 5, "context": "Video games have proven to be useful surrogates for realworld robotics by providing a high degree of complexity and photorealism while allowing rapid iteration on algorithms because of the separation from physical hardware (Richter et al., 2016; Rusu et al., 2016).", "startOffset": 223, "endOffset": 264}], "year": 2017, "abstractText": "Robots and autonomous systems that operate around humans will likely always rely on kill switches that stop their execution and allow them to be remote-controlled for the safety of humans or to prevent damage to the system. It is theoretically possible for an autonomous system with sufficient sensor and effector capability and using reinforcement learning to learn that the kill switch deprives it of long-term reward and learn to act to disable the switch or otherwise prevent a human operator from using the switch. This is referred to as the big red button problem. We present a technique which prevents a reinforcement learning agent from learning to disable the big red button. Our technique interrupts the agent or robot by placing it in a virtual simulation where it continues to receive reward. We illustrate our technique in a simple grid world environment.", "creator": "LaTeX with hyperref package"}}}