{"id": "1703.10316", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Efficient Parallel Translating Embedding For Knowledge Graphs", "abstract": "Knowledge graph embedding aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating embedding methods regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph embedding methods. However, a major limitation of these methods is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great difficulty in practical applications. In this paper, we propose an efficient parallel framework for translating embedding methods, called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating embedding methods, i.e., TransE [3], TransH [17], and a more efficient variant TransE- AdaGrad [10] validate that ParTrans-X can speed up the training process by more than an order of magnitude.", "histories": [["v1", "Thu, 30 Mar 2017 05:20:18 GMT  (145kb,D)", "https://arxiv.org/abs/1703.10316v1", null], ["v2", "Mon, 14 Aug 2017 10:52:29 GMT  (585kb,D)", "http://arxiv.org/abs/1703.10316v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["denghui zhang", "manling li", "yantao jia", "yuanzhuo wang"], "accepted": false, "id": "1703.10316"}, "pdf": {"name": "1703.10316.pdf", "metadata": {"source": "CRF", "title": "E\u0080icient Parallel Translating Embedding For Knowledge Graphs", "authors": ["Denghui Zhang", "Manling Li", "Yantao Jia", "Yuanzhuo Wang", "Xueqi Cheng"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computing Methodologies \u2192 Reasoning about faith and knowledge; KEYWORDS Knowledge Graph Embedding, Translation-based, Parallel ACM Reference format: Denghui Zhang, Manling Li, Yantao Jia, Yuanzhuo Wang, Xueqi Cheng. 2017. E cient Parallel Translating Embedding For Knowledge Graphs. In Proceedings of WI '17, Leipzig, 23 - 26 August 2017, 8 pages. DOI: 10.1145 / 3106426.3106447"}, {"heading": "1 INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2 RELATEDWORK", "text": "In recent years, the number of those who are able to reform has multiplied. (...) In recent years, the number of those who are able to reform and to reform has multiplied. (...) In recent years, the number of those who are able to reform has multiplied. (...) In recent years, the number of those who are able to reform has multiplied. (...) In the last ten years, the number of those who are able to reform has diminished. (...) The number of those who are able to reform has multiplied. (...)"}, {"heading": "3 LAW OF COLLISIONS EMERGING IN KG", "text": "As already mentioned, collisions between processors can occur when they update the same embedding vector, which is ultimately one of the most difficult aspects of parallelizing embedding methods. Therefore, we are investigating the law of collisions that appear in this section. First, we formulate the training data of knowledge graphs in hypergraphs. On the basis of this formulation, collisions are discussed further in the training process."}, {"heading": "3.1 Hypergraph Formulation", "text": "First, we formally model the knowledge diagram as G = (E, R, T), where E is the set of entities with R being the set of relationships, and T is the set of triples (h, r, t) in which h, t, E and r, R, R and T are the number of entities in knowledge graphs and lines for relationships. (E, R, T) in which nodes are connected to a differentiated relationship. (a) in which black nodes represent the entities in knowledge graphs and lines for relationships can be represented as G = (E, R, T) in which e1, e2, e4, e5}, R = {r1, r3} and T = (e1, r2), r2, e4)."}, {"heading": "3.2 Collision Formulation", "text": "In this section, we will check that it is highly possible that a few collisions occur when the subjects are selected for large and sparse quantities of knowledge. (LetXsamp represents the event that there are collisions between the subjects, i.e., the subjects update an equal relationship between the subjects. (Xsamp = 1) This is the prerequisite for no collisions. (Xsamp = 1) These relationships or entities are unlikely to match, i.e. P (Xr el = 0) 1 and P (Xsamp = 1). (Xsamp = 1) The prerequisite for no collisions is that these relationships or entities are unlikely to match, i.e. P (Xr el = 0). (Xent = 0) and P (Xsamp = 1)."}, {"heading": "3.3 Special Insights on Parallelizing TransE", "text": "It is an interesting result that sees itself able to find further parallelism between the two methods, as there are fewer collisions due to the different score functions earlier (h, t), h \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"h\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"h\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" h \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"h\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \""}, {"heading": "4 THE PARTRANS-X FRAMEWORK", "text": "Inspired by the realization that collisions between processors are negligible when a knowledge diagram is large and sparse, a parallel framework for these methods called ParTrans-X is being developed, and we will describe it in detail in this section."}, {"heading": "4.1 Framework Description", "text": "e pseudocode for the implementation of ParTrans-X is shown in algorithm m 1. Since the embedded vectors are updated frequently, they are stored in shared memory and each processor can perform updates to release them. E-training process of ParTrans-X starts with initialization of embedded vectors according to uniform or Bernoulli distribution, where no parallel section is required as it takes constant time. To do this, we can follow the learning process of each epoch in parallel, which is the most time consuming part. Running by p processors in parallel way can decrease the training periods according to p times, i.e. the parallel training epoch is ep-p = epp. To do this, we determine the random sampling seeds [i] by selecting SEED RAND for the i-th processor. Avoid e-random sampling sequences by removing random sequences from each other."}, {"heading": "4.2 Application to Typical translating embedding Methods", "text": "Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Scrk-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score"}, {"heading": "5 EXPERIMENT", "text": "First, we apply ParTrans-X to TransE, TransH and TransE-Adagrad in Section 5.1. In Section 5.2, experimental results show an excessive reduction in training time by ParTrans-X, with scaling performance shown together with a growing number of processors in Section 5.3."}, {"heading": "5.1 Experimental Settings", "text": "The data sets used are two representative data sets WN18 and FB15k, which are subsets of known knowledge diagrams WordNet and Freebase, respectively, and were widely used by the translation of embedding methods [3, 8, 11, 19]. Table 2 shows the statistics of them. Furthermore, \u03c1 n and \u03c3 n are shown without loss of generality, and they are both small on WN18 and FB15k. Furthermore, one can see that the two datasets have inherent properties. Namely, WN18 has only 18 relationships that lead to large collisions between the relationships. On the contrary, FB15k is less unbalanced in the number of units and relations. In order to tackle the KGC problem, experiments are carried out on the link prediction task, which aims to predict the missing units h or t is triple (h, r, t)."}, {"heading": "5.2 Link Prediction Peformance of ParTrans-X", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.3 Scaling Results for Multi-Processors", "text": "In addition, we are conducting a series of experiments to test whether implementations are scaled with increasing number of processors. In essence, we are analyzing two aspects of the experiment results, i.e. training time and link forecasting performance. Figure 4 shows the log log graph of training time in wall clock seconds for the respective number of processors. We can observe that the training time continues to decrease along with the increasing number of Mutli processors on both WN18 and FB15k. While the absolute training time of ParTransE-AdaGrad is shorter for a few processors than ParTransE, which is higher than ParTransH, which is consistent with the previous result. Furthermore, the overall training time of ParTransE-AdaGrad decreases sharply when the number of processors is less than four, it is because the training time of ParTransE-AdaGrad with fewer processors is quite short, increasing the communication time costs with the total time of ParTransE-AdaGrad has a lower impact on the other processors compared to the larger processors."}, {"heading": "6 CONCLUSION", "text": "In this paper, we investigate the law of collisions that occur in knowledge diagrams by modelling training data in hypergraphs. Our central observation is that learning iteration affects only a few embedding methods that are not necessarily bound to others, so the likelihood of collisions between different processors can be negligible. Building on this assumption, we propose an efficient parallel framework for translating embedding methods called ParTrans-X. It takes advantage of the intrinsic scarcity of training data in large knowledge diagrams, allowing embedding vectors to be learned without locking and without causing errors. Experiments confirm that ParTrans-X can accelerate the training process by more than one order of magnitude without affecting embedding performance."}, {"heading": "7 ACKNOWLEDGE", "text": "The work was funded by the National Natural Science Foundation of China (No. 61572469, 61402442, 91646120,61572473, 61402 022), the National Key R & D Program of China (No. 2016QY02D0405, 2016YFB1000902) and the National Grand Fundamental Research 973 Program of China (No. 2013CB329602, 2014CB340401)."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["Martn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Je\u0082rey Dean", "Ma\u008ahieu Devin"], "venue": "Learning on Heterogeneous Distributed Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. AcM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in neural information processing systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Large scale distributed deep networks", "author": ["Je\u0082rey Dean", "Greg S Corrado", "Rajat Monga", "Kai Chen", "Ma\u008ahieu Devin", "\u008boc V Le", "Mark Z Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker"], "venue": "In International Conference on Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "\u008comas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "OpenKN: An open knowledge computational engine for network big data", "author": ["Yantao Jia", "Yuanzhuo Wang", "Xueqi Cheng", "Xiaolong Jin", "Jiafeng Guo"], "venue": "In Advances in Social Networks Analysis and Mining (ASONAM)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Locally Adaptive Translation for Knowledge Graph Embedding", "author": ["Yantao Jia", "Yuanzhuo Wang", "Hailun Lin", "Xiaolong Jin", "Xueqi Cheng"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Slow learners are fast", "author": ["John Langford", "Alexander J Smola", "Martin Zinkevich"], "venue": "In International Conference on Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "Science", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Twenty- Ninth AAAI Conference on Arti\u0080cial Intelligence", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "YAGO3: A Knowledge Base from Multilingual Wikipedias", "author": ["Farzaneh Mahdisoltani", "Joanna Biega", "Fabian Suchanek"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Delay-tolerant algorithms for asynchronous distributed online learning", "author": ["H.B. Mcmahan", "M. Streeter"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "WordNet: a lexical database for English", "author": ["George A Miller"], "venue": "Commun. ACM 38,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "E\u0081cient Learning of Entity and Predicate Embeddings for Link Prediction in Knowledge Graphs", "author": ["Pasquale Minervini", "Claudia d\u2019Amato", "Nicola Fanizzi", "Floriana Esposito"], "venue": "In URSW@ ISWC", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "An overview of gradient descent optimization algorithms", "author": ["Sebastian Ruder"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "AAAI - Association for the Advancement of Arti\u0080cial Intelligence", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Deep learning with Elastic Averaging SGD", "author": ["Sixin Zhang", "Anna Choromanska", "Yann Lecun"], "venue": "Science", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": ", TransE [3], TransH [19], and a more e\u0081cient variant TransE- AdaGrad [11] validate that ParTrans-X can speed up the training process by more than an order of magnitude.", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": ", TransE [3], TransH [19], and a more e\u0081cient variant TransE- AdaGrad [11] validate that ParTrans-X can speed up the training process by more than an order of magnitude.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": ", TransE [3], TransH [19], and a more e\u0081cient variant TransE- AdaGrad [11] validate that ParTrans-X can speed up the training process by more than an order of magnitude.", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": ", Freebase [2], WordNet [14], YAGO [12], OpenKN [7], and have played a pivotal role in supporting many applications, such as link prediction, question answering, etc.", "startOffset": 11, "endOffset": 14}, {"referenceID": 13, "context": ", Freebase [2], WordNet [14], YAGO [12], OpenKN [7], and have played a pivotal role in supporting many applications, such as link prediction, question answering, etc.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": ", Freebase [2], WordNet [14], YAGO [12], OpenKN [7], and have played a pivotal role in supporting many applications, such as link prediction, question answering, etc.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": ", Freebase [2], WordNet [14], YAGO [12], OpenKN [7], and have played a pivotal role in supporting many applications, such as link prediction, question answering, etc.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "\u008ce methods represent entities and relations as the embedding vectors by regarding relations as translations from head entities to tail entities, such as TransE [3], TransH [19], TransR[11], etc.", "startOffset": 160, "endOffset": 163}, {"referenceID": 17, "context": "\u008ce methods represent entities and relations as the embedding vectors by regarding relations as translations from head entities to tail entities, such as TransE [3], TransH [19], TransR[11], etc.", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "\u008ce methods represent entities and relations as the embedding vectors by regarding relations as translations from head entities to tail entities, such as TransE [3], TransH [19], TransR[11], etc.", "startOffset": 184, "endOffset": 188}, {"referenceID": 2, "context": "When d is 100 and ep is 1000, it will take 78 minutes for TransE to learn the embeddings of FB15k2, which is a subset of Freebase with 483,142 training triples, and has been widely used as experimental dataset in knowledge graph embedding methods [3, 8, 11, 19].", "startOffset": 247, "endOffset": 261}, {"referenceID": 7, "context": "When d is 100 and ep is 1000, it will take 78 minutes for TransE to learn the embeddings of FB15k2, which is a subset of Freebase with 483,142 training triples, and has been widely used as experimental dataset in knowledge graph embedding methods [3, 8, 11, 19].", "startOffset": 247, "endOffset": 261}, {"referenceID": 10, "context": "When d is 100 and ep is 1000, it will take 78 minutes for TransE to learn the embeddings of FB15k2, which is a subset of Freebase with 483,142 training triples, and has been widely used as experimental dataset in knowledge graph embedding methods [3, 8, 11, 19].", "startOffset": 247, "endOffset": 261}, {"referenceID": 17, "context": "When d is 100 and ep is 1000, it will take 78 minutes for TransE to learn the embeddings of FB15k2, which is a subset of Freebase with 483,142 training triples, and has been widely used as experimental dataset in knowledge graph embedding methods [3, 8, 11, 19].", "startOffset": 247, "endOffset": 261}, {"referenceID": 4, "context": ", 75% persons do not have nationalities in Freebase [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 14, "context": "Pasquale[15] proposed TransE-AdaGrad to speed up the training process by leveraging adaptive learning rates.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "To avoid collisions, some methods [9] lock embedding vectors, which will slow the training process greatly as there are so many vectors.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "On the contrary, updating vectors without locks leads to high e\u0081ciency, but should be based on speci\u0080c assumptions [4, 16].", "startOffset": 115, "endOffset": 122}, {"referenceID": 15, "context": "On the contrary, updating vectors without locks leads to high e\u0081ciency, but should be based on speci\u0080c assumptions [4, 16].", "startOffset": 115, "endOffset": 122}, {"referenceID": 2, "context": ", TransE [3], TransH [19], and a more e\u0081cient variant TransE-AdaGrad, and experiments validate the e\u0082ectiveness of ParTrans-X on two widely used datasets.", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": ", TransE [3], TransH [19], and a more e\u0081cient variant TransE-AdaGrad, and experiments validate the e\u0082ectiveness of ParTrans-X on two widely used datasets.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "A signi\u0080cant work is TransE [3], which heralds the start of translating embedding methods.", "startOffset": 28, "endOffset": 31}, {"referenceID": 17, "context": "Moreover, TransH [19] assumes that it is the projections of entities to a relation-speci\u0080c hyperplane that satisfy the translation constraint, i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Furthermore, TransR [11] employs rotation transformation to project the entities to a relation-speci\u0080c space, i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": ", paths [10], margins [8].", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": ", paths [10], margins [8].", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "Recently, a method TransE-AdaGrad [15] was proposed to reduce the training time of TransE by employing AdaGrad [6], an variant of SGD, to adaptively modify the learning rate.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "Recently, a method TransE-AdaGrad [15] was proposed to reduce the training time of TransE by employing AdaGrad [6], an variant of SGD, to adaptively modify the learning rate.", "startOffset": 111, "endOffset": 114}, {"referenceID": 16, "context": "\u008ce major obstacle to parallel SGD is the collisions between updates of different processors for the same parameter [17], to overcome which there are two main brunches of methods.", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "For example, Hogwild! [16] is a lock-free scheme works well for sparse data, which means that there is only a small part of parameters to update by each iteration of SGD.", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "Downpour SGD [4] mainly employ DistBelief [4] framework, which divides the training data into a number of subsets, then the model replicas run independently on each of these subsets, and do not communicate with each other.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "Downpour SGD [4] mainly employ DistBelief [4] framework, which divides the training data into a number of subsets, then the model replicas run independently on each of these subsets, and do not communicate with each other.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "Inspired by this, TensorFlow [1] splits a computation graph into a subgraph for every worker and communication takes place using Send/Receive node pairs.", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "Motivated by training large-scale convolutional neural networks for image classi\u0080cation, Elastic Averaging SGD (EASGD) [20] reduces the amount of communication between local workers and the master to allow the parameters of local workers to \u0083uctuate further from the center ones.", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": ", Delay-tolerant Algorithms for SGD [13] adapts not only to the sequence of gradients, but also to the precise update delays that occur, inspired by AdaGrad.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "constructed by substituting one entity h\u2032 \u2208 E or t \u2032 \u2208 E for h or t respectively, contributing to a corrupted triple (h\u2032, r , t) or (h, r , t \u2032), which is just simply denoted by (h\u2032, r , t \u2032) following [3].", "startOffset": 202, "endOffset": 205}, {"referenceID": 2, "context": "Each loop is done by \u0080rstly normalizing the entity embedding vectors following [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "\u008ce datasets employed are two representative datasets WN18 and FB15k, which are subsets of well-known knowledge graphs WordNet and Freebase respectively, and have been widely used by translating embedding methods [3, 8, 11, 19].", "startOffset": 212, "endOffset": 226}, {"referenceID": 7, "context": "\u008ce datasets employed are two representative datasets WN18 and FB15k, which are subsets of well-known knowledge graphs WordNet and Freebase respectively, and have been widely used by translating embedding methods [3, 8, 11, 19].", "startOffset": 212, "endOffset": 226}, {"referenceID": 10, "context": "\u008ce datasets employed are two representative datasets WN18 and FB15k, which are subsets of well-known knowledge graphs WordNet and Freebase respectively, and have been widely used by translating embedding methods [3, 8, 11, 19].", "startOffset": 212, "endOffset": 226}, {"referenceID": 17, "context": "\u008ce datasets employed are two representative datasets WN18 and FB15k, which are subsets of well-known knowledge graphs WordNet and Freebase respectively, and have been widely used by translating embedding methods [3, 8, 11, 19].", "startOffset": 212, "endOffset": 226}, {"referenceID": 2, "context": "Similar to the se\u008aing in [3], the task returns a list of candidate entities from the knowledge graph.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "To evaluate the performance of link prediction, we adopt Mean Rank and Hits@10 under \u201cRaw\u201d and \u201cFilter\u201d se\u008aings as evaluation measure following [3].", "startOffset": 144, "endOffset": 147}], "year": 2017, "abstractText": "Knowledge graph embedding aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating embedding methods regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph embedding methods. However, a major limitation of these methods is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great di\u0081culty in practical applications. In this paper, we propose an e\u0081cient parallel framework for translating embedding methods, called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating embedding methods, i.e., TransE [3], TransH [19], and a more e\u0081cient variant TransEAdaGrad [11] validate that ParTrans-X can speed up the training process by more than an order of magnitude.", "creator": "LaTeX with hyperref package"}}}