{"id": "1703.10339", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Finding News Citations for Wikipedia", "abstract": "An important editing policy in Wikipedia is to provide citations for added statements in Wikipedia pages, where statements can be arbitrary pieces of text, ranging from a sentence to a paragraph. In many cases citations are either outdated or missing altogether.", "histories": [["v1", "Thu, 30 Mar 2017 07:48:31 GMT  (982kb,D)", "https://arxiv.org/abs/1703.10339v1", null], ["v2", "Mon, 24 Apr 2017 18:28:09 GMT  (983kb,D)", "http://arxiv.org/abs/1703.10339v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.SI", "authors": ["besnik fetahu", "katja markert", "wolfgang nejdl", "avishek anand"], "accepted": false, "id": "1703.10339"}, "pdf": {"name": "1703.10339.pdf", "metadata": {"source": "CRF", "title": "Finding News Citations for Wikipedia", "authors": ["Besnik Fetahu", "Katja Markert", "Wolfgang Nejdl", "Avishek Anand"], "emails": ["fetahu@L3S.de", "nejdl@L3S.de", "anand@L3S.de", "markert@cl.uni-heidelberg.de", "Permissions@acm.org."], "sections": [{"heading": null, "text": "In this paper, we address the problem of finding and updating news quotes for statements on entity pages. We propose a two-step monitored approach to this problem. In the first step, we construct a classifier to determine whether statements require a news quote or other types of quotation (web, book, journal, etc.); in the second step, we develop a news quotation algorithm for Wikipedia statements that recommends appropriate quotations from a given news collection. In addition to IR techniques that use the statement to query the news collection, we also formalize three characteristics of an appropriate quote, namely: (i) the quote should include the Wikipedia statement, (ii) the statement should be at the center of the quote, and (iii) the quote should come from an authoritative source. We perform a comprehensive evaluation of both steps using 20 million articles from a real news collection. Our results are promising and show that we can accomplish this task with high precision and on a large scale."}, {"heading": "1. INTRODUCTION", "text": "In fact, this is one of the most popular websites that people around the world can rely on, even if they do not visit the Wikipedia website directly. Therefore, it is essential that their content is accurate and reliable. Unlike traditional encyclopaedias, Wikipedia is not primarily written by experts. Also, the articles can be found in the top 10 most visited websites that involve the Internet. Permission to make digital copies of all or part of Wikipedia is granted without charge, that copies are made for profit or commercial advantage, and that copies of this notice are on the first page. Copyrights for components of this work owned by others than ACM must be realized."}, {"heading": "2. PROBLEM DEFINITION AND APPROACH OUTLINE", "text": "In this section we describe the terminology and problem definition for finding news quotes for Wikipedia."}, {"heading": "2.1 Terminology and Problem Definition", "text": "We operate on a specific snapshot of Wikipedia W, where the text in each Wikipedia page is divided into sections e \u2192 W, which are designated by the respective page (e). In addition, entity pages are organized into a type structure, which is a directed acyclicgraph (DAG), which is elicited by the Wikipedia categories. This is routinely exploited by knowledge bases such as YAGO (e.g. Barack Obama is a person) and we use this type structure, in which each page e belongs to a series of types T (e). However, we change the original YAGO type structure to make it deeply consistent, as in section 4.3.2.1.1 Quotes and Wikipedia-Statements-Quotes-Quotes-Quotes-Quotes-Quotes-Quotes-Quotations-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Quotation-Qu"}, {"heading": "2.2 Approach Overview", "text": "Figure 1 provides an overview of our approach. For an organization, we extract entity and type structure, as well as its statements, and finally perform the steps of categorizing statements and discovering citations. In the first step, we predict the citation category of a Wikipedia statement using supervised machine learning. We train a multi-class classification model in which the classes correspond to the citation categories c.Citation Discovery-FC. In the second step, we find evidence of news articles for all news reports. We retrieve news articles from candidates in a news collection N using standard methods of retrieving information, using s as our query, and classify each candidate as either an appropriate quote for s or not."}, {"heading": "3. WIKIPEDIA GROUND-TRUTH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Ground-Truth: Wikpedia News Statements", "text": "From a Wikipedia snapshot W (2015-07-01), we extract all statements and quotes related to this statement. 5 We extract 6.9 million statements with 8.8 million quotations, of 1.65 million units and 668k section types. Quotations are categorized by Wikipedia editors into one of the categories c. Sometimes, however, editors of do5As can add other clauses to a statement, sometimes quotes serve only as proof of part of the statement. However, at this level of granularity, we do not differentiate, but assume that all associated quotes support the entire statement. Do not categorize a quote as a message when they should do. In W, its top 3 news domains serve BBC, NYTimes, Guardian, often in categories other than news.6 Most such violations by editors occur when quoting news under the Web category, which is often a catch basin for any kind of quotations."}, {"heading": "3.2 Wikipedia News Collection", "text": "From the news reports, we extract the quoted news articles and construct the Wikipedia news collection NW, the 6So the quote http: / / news.bbc.co.uk / 1 / hi / uk politics / 7433479.s\\ tm was categorized as web by the unit Liam Byrne, although the more specific news category would have been appropriate. Serves as our basic truth for the citation discovery task. We define Nt'NW as the set of articles quoted from statements originating from units of type t. By Ns, we denote the set of articles originating from s.From the collection of news statements, we have 1.88 million citations on news articles (see above). We have successfully trawled through 1.5 million articles. The remaining 19% of the citations refer to non-existent articles (dead links, shifted content, etc.) In addition, some of the successfully crawled URLs refer to the index pages. This can be noted if we look at the length of the articles (3 letters in the illustration)."}, {"heading": "4. STATEMENT CATEGORIZATION", "text": "7http: / / tika.apache.orgIn the task of statement categorization, we get a statement s and the entity e from which it is extracted. We calculate characteristics that use the language style of s and the type and section structure of e to categorize s into one of the citation categories c. We learn a multi-class classifier (Section 4.3) with classes that correspond to citation categories c, and optimize for predicting message messages. Table 2 shows an overview of the feature list."}, {"heading": "4.1 Statement Language-Style", "text": "We hypothesize that Wikipedia news quotation statements are similar to the language style of news articles because they paraphrase frequently quoted news articles. Different genres (such as news, recipes, sermons, FAQs, fiction..) differ in their linguistic properties because the different functions they perform influence the linguistic form. [5] For example, we expect news reports (which largely focus on past events) to contain more past verbs than a recipe that gives verbs instructions in the imperative. We use functions that have been successful in automatic genre classification, including structural features about parts of speech, as well as lexical interface sources [18]. Part of speech density. Frequency of speech parts (POS) tags determined by the Stanford Tagger allows us to capture some of the structural properties of the text. For example, news statements can be characterized by a high number of past tendencies and proper names."}, {"heading": "4.2 Entity-Structure Based Features", "text": "We take advantage of the entity structure of e and calculate the probability of statements that have a message indication based on their types T (e) and sections in which they appear. A good indicator of the probability that a statement requires a message indication is the entity type to which it belongs and the section type in which it appears. For example, news statements have a higher density in the \"Early Life and Career\" section, as these tend to be more reflected in messages. To avoid matching, we filter out entity types with less than 10 statements. Likewise, we filter out sections with less than 10 statements, and in which they belong to the same citation category. We calculate the conditional probability that a message indication is given for an entity type T (e) in relation to an entity type T (e)."}, {"heading": "4.3 Learning Framework", "text": "A model that is formed at all levels is unlikely to work. For example, the types Location and Politician represent two highly divergent groups in terms of the structure of the entity that the models predict with greater accuracy. We only take types that have more than 1000 entity types, leading to 672 types. Types go from very broad types such as (Owl: Thing) to very specific types such as Series _ A _ Players. To use specialization and generalization in a principled manner, we only take types that have more than 1000 entity types, leading to 672 types. Types go from very broad types such as (Owl: Thing) to very specific types such as Series _ A _ Players. We are transforming the YAGO type taxonomy that we are transforming into an optimal level."}, {"heading": "5. CITATION DISCOVERY", "text": "5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5."}, {"heading": "5.1 Query Construction", "text": "It has been shown that in similar cases where the query corresponds to a sentence or paragraph, QC approaches are necessary to increase the accuracy of IR models. Henzinger et al. [13] propose several QCA1Base approaches, which weights the query terms based on the tf-idf scale. We experimented with different QCA1Base approaches from [13] and their effects on finding news articles in NW. We found that QCA1Base performs best and uses them in the rest of the paper. In QCA1Base, the terms extracted from the statements are weighted on the basis of tf-idf, with tf and idf calculating the top articles taking into account the other statements."}, {"heading": "5.2 Textual Entailment Features", "text": "Since the quotation is intended to approximate the content of the statement, ideally the quoted news article should contain the statement completely, i.e. the statement should be derivable from the news article. Recognition of textual entanglement has been the study of extensive research methods used in baseline entanglement systems over the last 10 years; cf [8] for an overview. A complete treatment of the entanglement requires comprehensive worldwide knowledge and follow-up rules; we limit ourselves here to much simpler lexical and syntactical similarity methods used in baseline entanglement systems, and leave the enhancements to future work areas. We use the retrieval model as a pre-filter to find candidate news articles as quotes for s. The retrieval model also provides us with two possible features for the learning model: First, a suitable Score for query scare-are-are-are-are-scare-are-scare-are-are-scare-scare-are-are-are-scare-are-are-are-are-are-are-are-scare-are-are-are-are-are-are-are-are-scare-are-are-are-are-are-are-are-are-are-are-are-are-scare-are-are-are-are-are-scare-are-are-are-are-scare-are-are-are-are-scare-are-are-are-are-scare-are-are-scare-are-are-are-are-scare-are-are-are-scare-are-are-scare-are-are-are-are-scare-are-are-are-scare-are-are-scare-scare-are-are-are-are-are-are-are-are-scare-are-are-scare-are-scare-are-are-are-scare-are-are-are-are-scare-scare-are-are-scare-are-scare-are-are-scare-are-are-are-scare-are-are-are-scare-are-are-scare-are-are-scare-scare-"}, {"heading": "5.3 Centrality Features", "text": "As described above, we calculate similarity characteristics between s and sentences in ni. However, some sentences in ni are more central than others. Therefore, we find the calculated characters between the pairs < s, [\u03c31i, \u03c32i,..., \u03c3ji] > do not have a uniform weight. First, we construct a graph G = (V, E) of ni, where V corresponds to the sentences of ni, where the edges in E are weighted with the Jaccard similarity between two sentences. In this case, the calculation of centrality is similar for all texts f."}, {"heading": "5.4 News-Domain Authority Features", "text": "Wikipedia's editorial policy clearly distinguishes between more and less established news media, preferring the former (see Introduction). We therefore calculate the authority of news domains w.r.t. The authority of news domains is unequally distributed across different types and sections. For types like politicians, the authority of domains like BBC is higher than for types like athletes, where a domain specializing in sports news is more likely to prevail. We grasp the authority of news domains as follows: p (D | t) = E-W-T (e) is the authority of domains like BBC higher than for types like athletes, where a domain specializing in sports news is more likely to prevail. We grasp the authority of news domains as follows: p (D | t) = E-W-T (e) is the authority of domains created by Wikipedia."}, {"heading": "6. STATEMENT CATEGORIZATION EVALUATION", "text": "Since we consider a type taxonomy, we have a hierarchy of models. Each statement belongs to a unit that is in turn a child of a type (node) in the hierarchy. Consequently, we construct each model from training instances (instructions) that are its children. We focus on two aspects of our approach (i) the performance of models at different depth and (ii) the performance of different feature classes. Detailed results for the task of statement categorization and the corresponding ground truth data are provided in the publication URL11.11http: / / l3s.de / \u02dc fetahu / cikm2016 /"}, {"heading": "6.1 Experimental Setup", "text": "Setup. We look at 672 entity types from our Yago taxonomy, for which we learn individual SC models. We look at types with more than 1000 entity instances. The degree of granularity in the YAGO taxonomy has a maximum depth of 20, while the root type is owl: thing that contains all possible entities. Train / test. We learn the SC models by using up to 90% of entity instances of a type t as a training set, and the rest of 10% for evaluation. We use layered samples to select entities of type t and its subtypes for the traction and test set. We train and test SC models over 6 million statements from 1.3 million entities. Metrics. We evaluate the performance of SC with precision P, R and F1. A statement is correctly categorized if the predicted category corresponds to the truth."}, {"heading": "6.2 Results and Discussion", "text": "This year it will be able to introduce the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainteecnlcsrteeSe."}, {"heading": "7. CITATION DISCOVERY EVALUATION", "text": "In this section, we evaluate the task of citation detection for news stories. We perform a comprehensive evaluation of approximately 22k news stories and discover quotes from a real-life collection of 20 million articles over a two-years.7.1 Explanation and news collection We limit ourselves to the subset of news reports with quotes from news articles in NW from 2013 to 2015.The resulting set contains 22k news reports with 27k news articles in NW.13 We also refer to this timeline of news articles in NW with NW13 \u2212 15. Since it is easier to find the correct quote from this preselected collection than the realistic scenario of finding a citation among all possible news items, we have also collected all English news articles from the period [2013-08, 2015-08] from the GDelt Project14. We call the resulting high-coverage NG.We merge NG with NW13 \u2212 15 and call the resulting data set NW13 = 1, the statement contains NW13 = 14G \u2212 15."}, {"heading": "7.2 Evaluation Strategies", "text": "Evaluation strategy E1: In this scenario, we only consider for each news item the pairs < s, n > in which n-Ns are considered correct and all other possible quotes as incorrect, which allows fully automatic evaluation, but is only a lower limit for FC, since there may be additional articles that are relevant to s but do not exist in Ns. Therefore, we also consider a variant E1 + FP in which we consider n-Ns as additional correct quotes if the similarity (on the Jaccard similarity) to one of the articles in Ns greater than 0.8. Evaluation strategy E2: E2 evaluates the true performance of FC. In this case, apart from pre-existing quotes for s from Ns, we use crowd sourcing to evaluate appropriateness as quotations of articles n-N-N / Ns."}, {"heading": "7.3 Experimental Setup", "text": "We use the top 100 retrieved news articles for a statement as candidate quotes from which we perform feature extraction and learn our SC models.Learning Setup. We learn classifiers specific to entity types for a total of 83 types. We limit ourselves to types that have news statements in the 2013-2015 date range and with at least 100 entity instances. From our set of 22k statements, we randomly draw statements from each entity type if they have more than 1,000 instances, otherwise we take all statements. Training and test data consist of the pairs < s, ni > where s is a news statement, and ni is one of the top 100 citation candidates we retrieve from N. We share training and test data per statement where each s and all its candidates are fully included in the training or test set.Learning Approach."}, {"heading": "7.4 Results and Discussion", "text": "The results of this study show that most of them are not purely a problem, but a problem that has arisen in recent years."}, {"heading": "8. PIPELINE EVALUATION", "text": "For evaluating the two tasks in a pipeline scenario, we randomly sampled 1000 statements from all categories and ran the process of quoting discovery through both steps. Each statement is associated with multiple entity types, as they are extracted from e, where T (e) is a set of types. For the statement categorization task we perform the evaluation based on our soil truth; for quoting discovery we evaluate the proposed citations as in the evaluation strategy E2. Note that here in the evaluation pair we have a news article (which we propose) and a resource that can be of any kind, including book, web, journalism. Explanation categorization. We set up the statement categorization as majority choice categorization. For each statement and the type specific classifier SC we predict the category that has the majority of votes. Unlike the statement categorization in Section 4, where the original task is aimed at what types are performed accurately, we can cite the citations."}, {"heading": "9. RELATED WORK", "text": "They examine what types of sources are cited most frequently, i.e. primary, secondary and tertiary sources as defined in Wikipedia3. They conclude that news is one of the most frequently cited sources in the secondary type, while they see a growing trend of primary sources due to their persistence on the Web, which contradicts the guidelines to favor secondary sources. Luyt and Tan [15] analyze a subset of pages with historical entities and show that citations are tilted toward a specific group of sources. [12, 15] emphasize the importance of quotations in Wikipedia as a means to ensure the quality of entity websites. Wikipedia quality and al. [2] suggest an approach to predicting quality deficiencies in Wikipedia pages."}, {"heading": "10. CONCLUSIONS", "text": "In this paper, we define and attempt to solve the problem of automatic detection of news quotes for Wikipedia. We define two tasks - sentence categorization and discovery of quotes - to find the right news quote for a given Wikipedia statement. For the task of sentence categorization, we learn a multi-class classifier to predict whether a statement requires a news explanation. For the problem of finding news quotes, we first find the likely candidates using a retrieval model of a real news collection followed by a binary classification for the best placed candidates. We find that categorization of citations is a difficult problem to perform well due to the lack of context for the NLP-based attributes. However, the Wikipedia page and its type structure provide important clues for an accurate classification. On the other hand, we perform well in the task of detecting citations with 67% precision (for top categories)."}, {"heading": "11. REFERENCES", "text": "[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20. [2] M. Anderka, B. Stein, and N. Lipka. Predicting quality errors in user-generated content: the case of wikipedia, Ireland, 2013. [4] K. Balog, H. Ramampiaro, N. Takhirov, and K. N\u00f8rv. Multi-step classification. In 36th ACM SIGIR, Ireland, 2013. [4] K. Balog, H. Ramampiaro, and K. Breiagan classification approaches to cumulative citation commendation. In OAIR, Lisbon, Portugal, 2013. [5] D. Variation across speech and writing."}], "references": [{"title": "Predicting quality flaws in user-generated content: the case of wikipedia", "author": ["M. Anderka", "B. Stein", "N. Lipka"], "venue": "The 35th ACM SIGIR, Portland, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Cumulative citation recommendation: classification vs", "author": ["K. Balog", "H. Ramampiaro"], "venue": "ranking. In 36th ACM SIGIR, Dublin, Ireland", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "and K", "author": ["K. Balog", "H. Ramampiaro", "N. Takhirov"], "venue": "N\u00f8rv\u030aag. Multi-step classification approaches to cumulative citation recommendation. In OAIR, Lisbon, Portugal", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Variation across speech and writing", "author": ["D. Biber"], "venue": "Cambridge University Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, 3", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Recognizing textual entailment: Models and applications", "author": ["I. Dagan", "D. Roth", "M. Sammons", "F.M. Zanzotto"], "venue": "Synthesis Lectures on Human Language Technologies, 6(4):1\u2013220", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "How much is wikipedia lagging behind news? In Proceedings of the ACM Web Science Conference", "author": ["B. Fetahu", "A. Anand", "A. Anand"], "venue": "WebSci 2015, Oxford, United Kingdom, June 28 - July 1", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated news suggestions for populating wikipedia entity pages", "author": ["B. Fetahu", "K. Markert", "A. Anand"], "venue": "24th CIKM, Melbourne, Australia", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C.D. Manning"], "venue": "43rd ACL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Getting to the source: where does wikipedia get its information from? In 9th WikiSym", "author": ["H. Ford", "S. Sen", "D.R. Musicant", "N. Miller"], "venue": "Hong Kong, China", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Query-free news search", "author": ["M.R. Henzinger", "B. Chang", "B. Milch", "S. Brin"], "venue": "12th WWW, Budapest, Hungary", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "A dependency-based word subsequence kernel", "author": ["R.J. Kate"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Improving wikipedia\u2019s credibility: References and citations in a sample of history articles", "author": ["B. Luyt", "D. Tan"], "venue": "JASIST, 61(4)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "the sum of all human knowledge\u201d: A systematic review of scholarly research on the content of wikipedia", "author": ["M. Mesgari", "C. Okoli", "M. Mehdi", "F.\u00c5. Nielsen", "A. Lanam\u00e4ki"], "venue": "JASIST, 66(2)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Textrank: Bringing order into text", "author": ["R. Mihalcea", "P. Tarau"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Stable classification of text genres", "author": ["P. Petrenz", "B. Webber"], "venue": "Computational Linguistics, 37(2):385\u2013393", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Using word-sense disambiguation methods to classify web queries by intent", "author": ["E. Pitler", "K.W. Church"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "and B", "author": ["R. Prasad", "N. Dinesh", "A. Lee", "E. Miltsakaki", "L. Robaldo", "A.K. Joshi"], "venue": "L. Webber. The penn discourse treebank 2.0. In LREC. Citeseer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatically generating wikipedia articles: A structure-aware approach", "author": ["C. Sauper", "R. Barzilay"], "venue": "47th ACL", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "The web library of babel: evaluating genre collections", "author": ["S. Sharoff", "Z. Wu", "K. Markert"], "venue": "LREC. Citeseer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Heideltime: High quality rule-based extraction and normalization of temporal expressions", "author": ["J. Str\u00f6tgen", "M. Gertz"], "venue": "5th SemEval, Stroudsburg, PA, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "SIGDAT, pages 63\u201370. ACL", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 14, "context": "A substantial number of reliability studies have compared Wikipedia against other reference works (such as the Encyclopedia Britannica or drug package information) or subjected them to expert review: The exhaustive survey in [16] concludes that the results of these studies have overall been favourable to Wikipedia when it comes to accuracy of facts, although some works (especially on medical articles) found errors of omission.", "startOffset": 225, "endOffset": 229}, {"referenceID": 7, "context": "For current and recent events, news citations are one of the most-used sources [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "\u2022 Sections \u2022 Anchors \u2022 Text \u2022 Categories typeOf\u2028 Politician Obama was born on August 4, 1961,[4] .", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": ") differ in their linguistic properties as the different functions they fulfill influence linguistic form [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "We use features that were successful in automatic genre classification including structural features via parts-of-speech as well as lexical surface cues [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "We use discourse connectives to annotate the statements s with explicit discourse relations based on an approach proposed by Pitler and Church [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "The annotations belong to the categories {temporal, contingency, comparison, expansion}, following the Penn Discourse Treebank annotation [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "As surface lexical features have been shown to be efficient in genre recognition [22], we compute n\u2013gram (up to n=3) language models with Kneser-Ney smoothing (LM) from news articles Nt and compute the score \u03b8(s,Nt).", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "Similarly, we compute topic models using the LDA framework [6], where the score is the jaccard similarity between s and the topic terms from Nt.", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "This proved to be more scalable than state-of-the-art extractors like HeidelTime [23] and Stanford\u2019s CoreNLP [11] module", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "This proved to be more scalable than state-of-the-art extractors like HeidelTime [23] and Stanford\u2019s CoreNLP [11] module", "startOffset": 109, "endOffset": 113}, {"referenceID": 5, "context": "Finally, we opt for Random Forests (RF) [7] as our supervised machine learning model.", "startOffset": 40, "endOffset": 43}, {"referenceID": 11, "context": "[13] propose several QC approaches that weigh query terms based on the tf\u2013idf score.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We experimented with different QC approaches from [13] and their impact on finding news articles in N .", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "The recognition of textual entailment has been the study of extensive research in the last 10 years; cf [8] for an overview.", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "For this purpose, we consider the tree kernel similarity measure proposed in [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "We first compute the dependency parse trees of s and \u03c3 i using the Stanford tagger [24], and then compute the tree kernel, K(s, \u03c3 i ).", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "We refer the reader to [14] for details.", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Similarly, we compute LDA topic models [6] for entity types, specifically from articles in Nt.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "We compute centrality of a sentence in ni through the TextRank approach introduced in [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "We therefore measure relative entity frequency of e in ni based on an approach described in [10].", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "We learn the FC models as supervised binary classification models using random forests RF[7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "[12] analyze the citation behavior of Wikipedia editors with respect to their adherence to the citation guidelines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Luyt and Tan [15] analyze a subset of history entity pages and show that citations are biased towards a specific group of sources.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "[12, 15] emphasize the importance of citations in Wikipedia as a means to ensure the quality of entity pages.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[12, 15] emphasize the importance of citations in Wikipedia as a means to ensure the quality of entity pages.", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "[2] propose an approach to predict quality flaws in Wikipedia pages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Sauper and Barzilay [21] propose an approach to automatically generate complete entity pages for a specific entity type.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "in [10] proposed an approach for suggesting news articles for a Wikipedia entity and entity section.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "This work differs from [21, 10] as we do not add content or suggest news articles to a complete section in an entity page, but rather provide citations to already existing statements.", "startOffset": 23, "endOffset": 31}, {"referenceID": 8, "context": "This work differs from [21, 10] as we do not add content or suggest news articles to a complete section in an entity page, but rather provide citations to already existing statements.", "startOffset": 23, "endOffset": 31}, {"referenceID": 2, "context": "[4, 3] propose approaches that find entity mentions in the document collection and rank them according to how central the entity is in the respective documents.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[4, 3] propose approaches that find entity mentions in the document collection and rank them according to how central the entity is in the respective documents.", "startOffset": 0, "endOffset": 6}], "year": 2017, "abstractText": "An important editing policy in Wikipedia is to provide citations for added statements in Wikipedia pages, where statements can be arbitrary pieces of text, ranging from a sentence to a paragraph. In many cases citations are either outdated or missing altogether. In this work we address the problem of finding and updating news citations for statements in entity pages. We propose a two-stage supervised approach for this problem. In the first step, we construct a classifier to find out whether statements need a news citation or other kinds of citations (web, book, journal, etc.). In the second step, we develop a news citation algorithm for Wikipedia statements, which recommends appropriate citations from a given news collection. Apart from IR techniques that use the statement to query the news collection, we also formalize three properties of an appropriate citation, namely: (i) the citation should entail the Wikipedia statement, (ii) the statement should be central to the citation, and (iii) the citation should be from an authoritative source. We perform an extensive evaluation of both steps, using 20 million articles from a real-world news collection. Our results are quite promising, and show that we can perform this task with high precision and at scale.", "creator": "LaTeX with hyperref package"}}}