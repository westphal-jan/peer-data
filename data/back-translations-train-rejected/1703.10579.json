{"id": "1703.10579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Evaluating Complex Task through Crowdsourcing: Multiple Views Approach", "abstract": "With the popularity of massive open online courses, grading through crowdsourcing has become a prevalent approach towards large scale classes. However, for getting grades for complex tasks, which require specific skills and efforts for grading, crowdsourcing encounters a restriction of insufficient knowledge of the workers from the crowd. Due to knowledge limitation of the crowd graders, grading based on partial perspectives becomes a big challenge for evaluating complex tasks through crowdsourcing. Especially for those tasks which not only need specific knowledge for grading, but also should be graded as a whole instead of being decomposed into smaller and simpler subtasks. We propose a framework for grading complex tasks via multiple views, which are different grading perspectives defined by experts for the task, to provide uniformity. Aggregation algorithm based on graders variances are used to combine the grades for each view. We also detect bias patterns of the graders, and debias them regarding each view of the task. Bias pattern determines how the behavior is biased among graders, which is detected by a statistical technique. The proposed approach is analyzed on a synthetic data set. We show that our model gives more accurate results compared to the grading approaches without different views and debiasing algorithm.", "histories": [["v1", "Thu, 30 Mar 2017 17:25:47 GMT  (722kb)", "http://arxiv.org/abs/1703.10579v1", "8 pages, 13 figures, the paper is accepted by ICCSE 2016"]], "COMMENTS": "8 pages, 13 figures, the paper is accepted by ICCSE 2016", "reviews": [], "SUBJECTS": "cs.AI cs.HC", "authors": ["lingyu lyu", "mehmed kantardzic"], "accepted": false, "id": "1703.10579"}, "pdf": {"name": "1703.10579.pdf", "metadata": {"source": "CRF", "title": "Evaluating Complex Task through Crowdsourcing: Multiple Views Approach", "authors": ["Lingyu Lyu", "Mehmed Kantardzic"], "emails": ["l0lv0002@louisville.edu", "mehmed.kantardzic@louisville.edu"], "sections": [{"heading": null, "text": "In this context, it should be noted that the case is an accident."}, {"heading": "2.1 Literature Review", "text": "In fact, it is the case that you will be able to follow the rules that you have set yourself and then put them into practice."}, {"heading": "2.2 Vancouver Algorithm", "text": "The Vancouver algorithm [2] measures the assessment accuracy of each student by comparing the grades assigned by the student with the grades given by other students in the set. It gives more weight to the input of students with higher measured accuracy. The Vancouver algorithm proceeds in an iterative manner and uses consensus grades to estimate the gradation variance of each user and uses the information about the user variance to calculate more precise consensus grades. The approach can be formalized as: We name after U the set of points to be graded and after S the set of points to be graded (the submissions). We leave G = (T, E) the variance of the user variance to calculate more precise consensus grades as: We name after U the set of points to be graded and after S the graded points (the submissions) to estimate the gradation variance of each user. We leave G = (T, E) the information about the user variance to formulate more precise assessment grades as: We name after U the set of points to be graded and use consensus grades as: We name after U the set of points to be graded and use consensus grades (the submissions)."}, {"heading": "3.1 Modified Vancouver Algorithm", "text": "The enhancement of the Vancouver algorithm is still based on the same principle of the Vancouver algorithm, the main difference being that multiple views are taken into account. Details of the modified Vancouver approach can be seen in Algorithm 1. S is the number of submissions that need to be graded, and U is the totality of gradations. are the lists of messages associated with the submission and grading. The number of views and observed views are used as input for the algorithm as well as the assessment curve. Instead of propagating just one deviation, as in the basic Vancouver algorithm, different deviations for multiple views of each grader are taken into account in the extended approach. After obtaining consensus grades, it is necessary to combine the assessment grades in the overall grade for each submission.The combination methods depend on the instructions of the experts assigned to the consensus."}, {"heading": "3.2 De-biasing", "text": "The gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for all gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradfor the gradations for the gradfor the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradations for the gradfor the gradations for the gradations for the gradgradations for the gradations for the gradfor the gradfor the gradations for the gradations"}, {"heading": "3.3 Grading Complex Task through Crowdsourcing", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die r"}, {"heading": "4.1 Data Set", "text": "To compare the proposed approach with the methodology used in [2] Vancouver, we used the same synthetic data set. 50 graders and 50 submissions were considered, with each grade evaluating 6 submissions. To better understand the process, we defined two different views for the task. For each view, we assumed: The true quality of Qi of each item i has a normal distribution with standard deviation 1. Each grade j had a characteristic deviation vj, and we allowed the grade qij assigned by j to be equal to Qi + \u2206 ij, where Qi is the true quality, and Qij has a normal distribution with mean 0 and deviation vj. We assumed that the deviations {vj} j were distributed according to a gamma distribution with scale 0.4 and form factors k = 2, 3."}, {"heading": "4.2 Models", "text": "Three different models are analyzed and designated as follows: AVG, DM1 and DM2.AVG model represent the average model. It only calculates the averages for each submission submitted in order to obtain the consensus grade. It acts as the base model. DM1 is the model with different views defined by an expert, but without the step of detecting distortion patterns. Therefore, there is no deprivation for the observed grades. DM1 is essentially the Vancouver method in [2], but with extension to different perspectives. The DM2 model uses both different views of tasks and the deprivation of grades. For the complex task, different views are defined before the grades are retrieved from the grades. After the observed grades are obtained, distortion patterns are detected, deprivation patterns are applied, and finally general consensus grades are achieved."}, {"heading": "4.3 Evaluation Metrics", "text": "Our goal is to bring the consensus notes as close as possible to the true marks given by an expert. As a result, the consensus notes are calculated as follows: coefficient correlation (\u03c1), standard deviation (), and mean square error (RMSE) between true notes and consensus notes as weights. The metrics are calculated as follows: (,) (,) = () () (,) (7), where, the coefficient correlation of random variables (,) and (,) (consensus notes) is the default deviation from and,, the mean of and (,) the coefficient correlation of (,) and (,). is the default deviation from (,) and (,) is the default deviation from ()."}, {"heading": "4.4 Experimental Results", "text": "The synthetic data is simulated by 100 passes, and the coefficient correlation, standard deviation and RMSE are then shown as an average over these 100 passes. Figure 3 shows the evaluation metrics calculated for view 1 of the first 20 simulation runs, with form factor k = 2 of the gamma distribution for the three models. To show the difference between DM1 and DM2, we show a more detailed illustration in Figure 4. The result of the view2 is quite similar to view 1. The results are summarized in Table 1. Specifically, the overall result is calculated from the aggregation of the 100 passes, rather than the results from the final view.To better understand the effect of the distortion patterns on the results, we perform the following experiment by: Increasing the percentage of distorted degrees (graders that have distortion patterns) for each view. We increase the number of distorted degrees for view 1 from 9 to the view of 24 and the consensus values of M2 to 28."}, {"heading": "4.5 Detailed Experimental Results Analysis and Discussion", "text": "In fact, most of them are able to survive on their own."}], "references": [{"title": "Tuned models of peer assessment in MOOCs", "author": ["C. Piech", "J. Huang", "Z. Chen", "C. Do", "A. Ng", "D. Koller"], "venue": "ArXiv preprint arXiv:1307-2579,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "CrowdGrader: A tool for crowd-sourcing the evaluation of homework assignments", "author": ["L. Alfaro", "M. Shavlovsky"], "venue": "Proceedings of the 45th ACM technical symposium on Computer science education:", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Peer review: A flawed process at the heart of the science and journals", "author": ["S. Smith"], "venue": "J R Soc Med. Apr;", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Dear colleague letter: Information to principal investigators (pis) planning to submit proposals to the sensors and sensing systems (sss) program", "author": ["N.S. Foundation"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Telescope time without tears: A distributed approach to peer review", "author": ["M. Merrifield", "D. Saari"], "venue": "Astronomy & Geophysics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Maximum likelihood estimation of observer errorrates using the EM algorithm", "author": ["A. Dawid", "A. Skene"], "venue": "Applied Statistics:", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1979}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological):", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Inferring ground truth from subjective labelling of venus images", "author": ["P. Smyth", "U. Fayyad", "M. Burl", "P. Perona", "P. Baldi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Learning with multiple labels", "author": ["R. Jin", "Z. Ghahramani"], "venue": "In Advances in neural information processing systems:", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in neural information processing", "author": ["J. Whitehill", "T.-F. Wu", "J. Bergsma", "J. Movellan", "P. Ruvolo"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning from crowds", "author": ["V. Raykar", "S. Yu", "L. Zhao", "G. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE PAMI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1984}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D. Karger", "S. Oh", "D. Shah"], "venue": "In Proc. of the 25th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "CrowdForge: Crowdsourcing complex work", "author": ["A. Kittur", "B. Smus", "S. Khamkar", "R.E. Kraut"], "venue": "24th annual ACM symposium on User interface software and technology:", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Map Reduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Crowdsourcing general computation", "author": ["H. Zhang", "E. Horvitz", "R.C. Miller", "D.C. Parkes"], "venue": "Proceedings of ACM CHI Conference on Human Factors,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Platemate: crowdsourcing nutritional analysis from food photographs", "author": ["J. Noronha", "E. Hysen", "H. Zhang", "K.Z. Gajos"], "venue": "Proc. UIST", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Some Thoughts on Grading Systems and Grading Practices", "author": ["J. Terwilliger"], "venue": "Teacher Training in Measurement and Assessment Skills:", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "Grades as valid measures of academic achievement of classroom learning. Clearinghouse: A Journal of Educational Strategies", "author": ["J. Ellen"], "venue": "Issues and Ideas,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Toward ai-enhanced computer-supported peer review in legal education", "author": ["K. Ashley", "I. Goldin"], "venue": "In 24th International Conference on Legal Knowledge and Information Systems (JURIX),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Calibrated peer review - a writing and critical-thinking instructional tool", "author": ["A.A. Russell"], "venue": "Teaching Tips: Innovations in Undergraduate Science Instruction,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "The impact of self-and peer-grading on student learning", "author": ["P.M. Sadler", "E. Good"], "venue": "Educational assessment,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Bayesian bias mitigation for crowdsourcing", "author": ["Wauthier", "M. Jordan"], "venue": "In In Proc. of NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Identifying and accounting for taskdependent bias in crowdsourcing", "author": ["E. Kamar", "A. Kapoor", "E. Horvitz"], "venue": "Third AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Leveraging in-batch annotation bias for crowdsourced active learning", "author": ["H. Zhuang", "J. Young"], "venue": "Proc. ACM WSDM: 243\u2013252. Shanghai,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Traditional ways of grading by instructors or teaching assistants are becoming a big challenge for large classes such as massive open online classes (MOOCs), which are distributed on platforms such as Udacity, Coursera and EdX [1], and the grading processes are really time consuming and tedious.", "startOffset": 227, "endOffset": 230}, {"referenceID": 14, "context": "However, the types of tasks accomplished through MTurk have typically been limited to those that are low in complexity, independent, and require little time and cognitive effort to complete [15].", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "Two main contributions of our approach are: extending the Vancouver algorithm [2], proposed by L.", "startOffset": 78, "endOffset": 81}, {"referenceID": 18, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 68, "endOffset": 75}, {"referenceID": 19, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 68, "endOffset": 75}, {"referenceID": 0, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 221, "endOffset": 226}, {"referenceID": 1, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 221, "endOffset": 226}, {"referenceID": 20, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 226, "endOffset": 233}, {"referenceID": 21, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 226, "endOffset": 233}, {"referenceID": 22, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 226, "endOffset": 233}, {"referenceID": 21, "context": "As stated in [24], \u201cstudents are trained to be competent reviewers and are then given the responsibility of providing their classmates with personalized feedback on expository writing assignments\u201d.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "In [1], three statistical models are developed for estimating true grades in peer grading.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 6, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 7, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 8, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 9, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 10, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 11, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 1, "context": "A system called CrowdGrader is developed in [2] to explore the use of peer feedback in grading assignments.", "startOffset": 44, "endOffset": 47}, {"referenceID": 13, "context": "It introduced a Vancouver algorithm (derived from [14]) which relies on a reputation system to aggregate the peer reviewing grades.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "[23] applied Bayesian data analysis to model a computer supported peer review process in a legal class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In [24], a web-based peer review program called Calibrated Peer Review (CPR) system is introduced to improve student learning.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "As what Richard Smith proposed in his paper, \u201cthe practice of peer review is based on faith in its effects, rather on facts\u201d [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 14, "context": "In [15] a framework was proposed to solve the problem by breaking down the complex task into a sequence of subtasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "This idea inspired by MapReduce [16] systems consists of three steps: partition, map and reduce.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "[17-19] presented the similar approaches to address the problem encountered while crowdsourcing the solutions to complex tasks.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[17-19] presented the similar approaches to address the problem encountered while crowdsourcing the solutions to complex tasks.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "Divide-and-conquer algorithm, which depicts the \u2018decompose, solve and recompose\u2019 structure, is proposed in [18] to solve general problem via crowdsourcing.", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "In [19], a system called PlateMate is introduced to crowdsource nutritional analysis from photographs via MTurk.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "Bias may be caused by personal preference, systematic misleading, and lack of interest [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "Many researches have included biases analysis in crowdsourcing models to improve the approximation accuracy [26-28].", "startOffset": 108, "endOffset": 115}, {"referenceID": 24, "context": "Many researches have included biases analysis in crowdsourcing models to improve the approximation accuracy [26-28].", "startOffset": 108, "endOffset": 115}, {"referenceID": 25, "context": "Many researches have included biases analysis in crowdsourcing models to improve the approximation accuracy [26-28].", "startOffset": 108, "endOffset": 115}, {"referenceID": 23, "context": "In [26], a Bayesian model, named as Bayesian Bias Mitigation for Crowdsourcing (BBMC), is proposed to capture the sources of bias.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Authors in [27] introduced and evaluated probabilistic models that can detect and correct task-dependent biases in crowdsourcing automatically.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "In [1], statistical models have been presented to infer the graders\u2019 biases in peer reviewing process.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "The Vancouver algorithm [2] measures each student\u2019s grading accuracy, by comparing the grades assigned by the student with the grades given to the same submission by other students in the crowd.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "An online algorithm is proposed in [2] to solve the defect.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "This algorithm ensures that higher accuracy leads to higher reputation, and therefore to higher influence on the consensus grades [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "In order to compare the proposed approach with the methodology applied in [2] called Vancouver, we used the same synthetic data set.", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "DM1 is basically Vancouver method in [2], but with extension to different views.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "We compared our method with the Vancouver algorithm in [2] (Alfaro and Shavlovsky, 2014).", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "In research [1], by developing the algorithm for estimating the true grades through statistical models, the authors were able to reduce the RMSE error on their prediction of ground truth by 31% to 33% comparing to baseline model.", "startOffset": 12, "endOffset": 15}], "year": 2016, "abstractText": "With the popularity of massive open online courses (MOOCs), grading through crowdsourcing has become a prevalent approach towards large scale classes. However, for getting grades for complex tasks, which require specific skills and efforts for grading, crowdsourcing encounters a restriction of insufficient knowledge of the workers from the crowd. Due to knowledge limitation of the crowd graders, grading based on partial perspectives becomes a big challenge for evaluating complex tasks through crowdsourcing. Especially for those tasks which not only need specific knowledge for grading, but also should be graded as a whole instead of being decomposed into smaller and simpler sub-tasks. We propose a framework for grading complex tasks via multiple views, which are different grading perspectives defined by experts for the task, to provide uniformity. Aggregation algorithm based on graders\u2019 variances are used to combine the grades for each view. We also detect bias patterns of the graders, and de-bias them regarding each view of the task. Bias pattern determines how the behavior is biased among graders, which is detected by a statistical technique. The proposed approach is analyzed on a synthetic data set. We show that our model gives more accurate results compared to the grading approaches without different views and de-biasing algorithm. Keywords\u2014complex task; crowdsourcing; view; bias pattern; debias; Vancouver algorithm", "creator": "Microsoft\u00ae Word 2013"}}}