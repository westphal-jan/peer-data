{"id": "1703.10631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention", "abstract": "Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable - they should provide easy-to-interpret rationales for their behavior - so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.", "histories": [["v1", "Thu, 30 Mar 2017 18:37:49 GMT  (2711kb,D)", "http://arxiv.org/abs/1703.10631v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jinkyu kim", "john canny"], "accepted": false, "id": "1703.10631"}, "pdf": {"name": "1703.10631.pdf", "metadata": {"source": "CRF", "title": "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention", "authors": ["Jinkyu Kim"], "emails": ["canny}@berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "Self-driving vehicles have made dramatic advances in recent years, and many car sellers have promised large-scale commercialization in a 2-3 year timeframe. These controllers use a variety of approaches, but recent successes [3] suggest that neural networks will be widely used in self-driving vehicles. But neural networks are notoriously cryptic - both the network architecture and the activation of the hidden layer may not have an obvious relationship to the function appreciated by the network. An exception to the rule are visual attention networks [26, 21, 7]. These networks provide spatial attention maps - areas of the image the network pays attention to - that can be displayed in a way that is easy for users to interpret. They provide their attention maps immediately on images that are faded into the network, and in this case on the stream of images from automotive video. As we show from our examples, visual attention maps lie above image areas that have an intuitive influence on the control signals."}, {"heading": "2. Related Works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. End-to-End Learning for Self-driving Cars", "text": "The approach of mediated perception is based on the recognition of human characteristics (i.e., road markings and cars) in a controller with if-then-otherwise rules. Some examples are Urmson et al. [24], Buehler et al. [4], and Levinson et al. [18]. Recently, there has been a growing interest in end-to-end learning vehicle control. Most of these approaches learn a controller by monitoring regression to footage of human drivers. The training data includes video from one or more vehicle cameras, and the control exits (steering and possible acceleration and braking) from the driver. ALVINN (Autonomous Land Vehicle In a Neural Network) [19] was the first attempt to use a neural network to navigate images directly in the direction of the vehicle."}, {"heading": "2.2. Visual Explanation", "text": "In a groundbreaking paper, Zeiler and Fergus [28] use \"deconvolution\" to visualize the layer activation of Convolutionary Networks. LeCun et al. [16] provide textual explanations of images as automatically generated captions. Building on this work, Bojarski et al. [2] developed a richer idea of the \"contribution\" of a pixel to output. However, one difficulty with deconvolution-style approaches is the lack of formal metrics for how network output is affected by spatially expanded features (rather than pixels). Attention-based approaches like ours directly extract areas of the image that do not affect network output (because they are obscured by the attention model), and causal filtering removes further spatially expanded areas of the image. Hendricks et al. [11] form a deep network to generate species-specific explanations without explicitly identifying semantic features. Justin Johnson et Castellale [14] suggests using their localisation of paper objects to fully describe their localisation in aliens, using the form of 27."}, {"heading": "3. Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Preprocessing", "text": "As discussed by Bojarski et al. [3], our model predicts the inverse turning circle u-t (= r \u2212 1t, where rt is the turning circle) at each time step t instead of steering angle commands, which depend on the steering geometry of the vehicle, and also leads to numerical instability in predicting steering angle commands close to zero. The relationship between the inverse turning circle ut and the steering angle command \u03b8t can be approximated by Ackermann steering geometry [20] as follows: V = fsteers (ut) = utdwKs (1 + Kslipvt 2) (1), where the inverse turning angle ut and the steering angle command (m / s) is a steering angle and a speed at time. Ks, Kslip, and dw are vehicle-specific parameters. Ks is a steering ratio between the turning point and the steering angle (s / s) and the steering angle (m)."}, {"heading": "3.2. Encoder: Convolutional Feature Extraction", "text": "We use a Convolutionary Neural Network to extract a series of coded visual feature vectors, which we call Convolutionary feature vectors cube xt. Each feature vector can contain high-level object descriptions that allow the attention model to pay selective attention to certain parts of an input image by selecting a subset of feature vectors.As shown in Figure 1, we use a 5-layer folding network used by Bojarski et al. [3] to learn a model for self-driving cars.As discussed by Lee et al. [17], we omit max-pooling layers to prevent spatial information loss as the strongest activation spreads through the model. We collect a three-dimensional Convolutionary feature cube from the last layer by sliding the preprocessed image through the model, and the output cube used as a textual input in the STM sequence we use in the STM-L."}, {"heading": "3.3. Coarse-Grained Decoder: Visual Attention", "text": "The goal of the soft deterministic attention mechanism \u03c0 ({xt, 1, xt, 2,. \u03b2, L}) is to search for a good context vector yt, which is defined as a combination of revolutionary character vectors that require better predictive accuracy. We use a deterministic soft attention mechanism that is feasible through standard back propagation methods, which have advantages over a hard stochastic attention mechanism that requires amplification. Ourmodel feeds \u03b1 context yt to the system, as discussed in several papers [21, 26]: yt = fflatten (\u00b2): yt = fflatten ({xt, i}) = fflatten ({\u03b1t, i}) where i = {1, 2,."}, {"heading": "3.4. Fine-Grained Decoder: Causality Test", "text": "The final step in our pipeline is a fine-grained decoder, in which we refine a map of attention and recognize local visual distortions. Although an attention map from our coarse-grained decoder provides a probability of significance for a 2D image space, our model must determine specific regions that have a causal effect on predictive power. To this end, we evaluate a decrease in performance when a local visual distribution is masked on an input raw image. We first collect a series of attention weights {\u03b1t} and insert raw images."}, {"heading": "4. Result and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "As explained in Table 1, we obtain two large-format datasets containing over 1,200,000 images (\u2248 16 hours) collected by Comma.ai [8], Udacity [23] and Hyundai Center of Excellence in Integrated Vehicle Safety Systems and Control (HCE) as part of a research assignment. These three datasets contain video clips taken from a single front camera mounted behind the vehicle's windshield. In addition to the video data, we also include a series of time-stamped sensor measurements, such as speed, acceleration, steering angle, GPS position and gyroscope angle. Therefore, these datasets are ideal for self-driving studies. Note that for sensor logs that are not synchronized with the time stamps of the video data, we use estimates of interpolated measurements. Videos are mostly recorded while driving on the highway in clear weather, and also on other roads, such as residential roads (with and without lane markings), and we keep the entire lane switches during the lane."}, {"heading": "4.2. Training and Evaluation Details", "text": "To get a revolutionary feature cube xt, we train the 5-layer CNNs explained in Section 3.2 by using additional 5-layer fully connected layers (i.e. # hidden variables: 1164, 100, 50, and 10, respectively), of which the output predicts the measured inverse turning radius ut. By the way, instead of using additional fully connected layers, we could also get a revolutionary feature cube xt by training the entire network from scratch. In our experiment, we get the 10 x 20 x 64-dimensional revolutionary feature cube, which is then flattened to 200 x 64 and fed by the coarse-grained decoder. Other newer types of expresisive networks can give a performance boost over our CNN configuration. However, exploring other revolutionary architectures would be outside our scope [we experiment with a variety of LSTM layers (1 to 5) of the soft deterministic visual attention model."}, {"heading": "4.3. Effect of Choosing Penalty Coefficient \u03bb", "text": "Our model provides a better way to understand the basics of the model decision by visualizing where and what the model sees to drive a vehicle. Figure 3 shows a sequential input of raw images (with a sampling time of 5 seconds) and the corresponding attention maps (i.e., Mt = fmap ({\u03b1t, i}). We also experiment with three different penalty coefficients \u03bb \u00b2 {0, 10, 20}, encouraging the model to pay attention to wider parts of the image (see differences between the bottom 3 lines in Figure 3), since we have a larger \u03bb. For better visualization, an attention map is overlaid by an input raw image and color-coded; for example, red parts represent where the model pays attention. For quantitative analysis, the prediction performance in relation to mean absolute error (MAE) is explained on the bottom of each figure. We find that our model is actually able to anticipate road markings, such as human road markers, which is unavoidable."}, {"heading": "4.4. Effect of Varying Smoothing Factors", "text": "Let us remember from Section 3.1 that the unique exponential smoothing method [13] is used to reduce the impact of variation in human factors and the impact of measurement noise on two sensor inputs: steering angle and speed. A robust model for autonomous vehicles would provide consistent performance even if some measurements are loud. Figure 4 shows the predictive performance in the form of an average absolute error (MAE) on a test data set by comma.ai. Various smoothing factors \u03b1s [0.01, 0.05, 0.1, 0.3, 0.5, 1.0} are used to assess the effect of using smoothing methods. With the setting \u03b1s = 0.05, our model performs best for the steering estimation task. Unless otherwise specified, we will use \u03b1s as 0.05."}, {"heading": "4.5. Quantitative Analysis", "text": "In Table 2, we compare the prediction performance with alternatives with respect to MAE. We implement alternatives, which include the work of Bojarski et al. [3] which uses without attention an identical CNN base and a fully connected network (FCN). In order to see the contribution of LSTMs, we also test a CNN and an LSTM, which is identical to our but not an attention mechanism. In our model, we test the humidity test with three different values of penalty coefficients and an LSTM, which is identical to our own, but does not use an attention mechanism. In our model, we test the humidity test with three different values of penalty coefficients."}, {"heading": "4.6. Effect of Causal Visual Saliencies", "text": "Figure 5 (A) shows typical examples of an input raw image, an attention network output with false attention sources, and our sophisticated attention thermal imaging map. We observe that our model can produce a simpler and more accurate map of visual highlighting by filtering out false attention apertures. In our experiment, 62% and 58% of all attention apertures are actually false attention sources on Comma.ai [8] and HCE datasets (see Figure 5 (B))."}, {"heading": "5. Conclusion", "text": "We tested with three large real-world driving data containing more than 16 hours of video images, showing that (i) the inclusion of attention does not decrease control accuracy compared to an identical base CNN without attention, (ii) the raw attention highlights interpretable features in the image, and (iii) the causal filtering achieves a useful reduction in explanatory complexity by removing features that do not significantly affect the output."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous 7  Dataset  Model  MAE in degree [SD] Training Testing Comma.ai", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "HCE", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Visualbackprop: visualizing cnns for autonomous driving", "author": ["M. Bojarski", "A. Choromanska", "K. Choromanski", "B. Firner", "L. Jackel", "U. Muller", "K. Zieba"], "venue": "arXiv preprint arXiv:1611.05418,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The DARPA urban challenge: autonomous vehicles in city traffic, volume 56", "author": ["M. Buehler", "K. Iagnemma", "S. Singh"], "venue": "springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The laplacian pyramid as a compact image code", "author": ["P. Burt", "E. Adelson"], "venue": "IEEE Transactions on communications, 31(4):532\u2013540,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1983}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2722\u20132730,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Brain inspired cognitive model with attention for self-driving cars", "author": ["S. Chen", "S. Zhang", "J. Shang", "B. Chen", "N. Zheng"], "venue": "arXiv preprint arXiv:1702.05596,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "A densitybased algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "In Kdd,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Aistats, volume 9, pages 249\u2013256,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "European Conference on Computer Vision, pages 3\u201319. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Forecasting with exponential smoothing: the state space approach", "author": ["R. Hyndman", "A.B. Koehler", "J.K. Ord", "R.D. Snyder"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4565\u20134574,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 609\u2013616. ACM,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards fully autonomous driving: Systems and algorithms", "author": ["J. Levinson", "J. Askeland", "J. Becker", "J. Dolson", "D. Held", "S. Kammel", "J.Z. Kolter", "D. Langer", "O. Pink", "V. Pratt"], "venue": "In Intelligent Vehicles Symposium (IV),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Alvinn, an autonomous land vehicle in a neural network", "author": ["D.A. Pomerleau"], "venue": "Technical report, Carnegie Mellon University, Computer Science Department,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Vehicle dynamics and control", "author": ["R. Rajamani"], "venue": "Springer Science & Business Media,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Action recognition using visual attention", "author": ["S. Sharma", "R. Kiros", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.04119,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Autonomous driving in urban environments: Boss and the urban challenge", "author": ["C. Urmson", "J. Anhalt", "D. Bagnell", "C. Baker", "R. Bittner", "M. Clark", "J. Dolan", "D. Duggins", "T. Galatali", "C. Geyer"], "venue": "Journal of Field Robotics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "End-to-end learning of driving models from large-scale video datasets", "author": ["H. Xu", "Y. Gao", "F. Yu", "T. Darrell"], "venue": "arXiv preprint arXiv:1612.01079,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML, volume 14, pages 77\u201381,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Graying the black box: Understanding dqns", "author": ["T. Zahavy", "N.B. Zrihem", "S. Mannor"], "venue": "arXiv preprint arXiv:1602.02658,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision, pages 818\u2013833. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "These controllers use a variety of approaches but recent successes [3] suggests that neural networks will be widely used in self-driving vehicles.", "startOffset": 67, "endOffset": 70}, {"referenceID": 23, "context": "An exception to the rule is visual attention networks [26, 21, 7].", "startOffset": 54, "endOffset": 65}, {"referenceID": 19, "context": "An exception to the rule is visual attention networks [26, 21, 7].", "startOffset": 54, "endOffset": 65}, {"referenceID": 6, "context": "An exception to the rule is visual attention networks [26, 21, 7].", "startOffset": 54, "endOffset": 65}, {"referenceID": 21, "context": "[24], Buehler et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4], and Levinson et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "ALVINN (Autonomous Land Vehicle In a Neural Network) [19] was the first attempt to use neural network for directly mapping images to navigate the direction of the vehicle.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "[3] demonstrated good performance with convolutional neural networks (CNNs) to directly map images from a front-view camera to steering controls.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[25] proposed an end-to-end egomotion prediction approach that takes raw pixels and prior vehicle state signals as inputs and predicts several a sequence of discretized actions (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] who defined human-interpretable intermediate features such as the curvature of lane, distances to neighboring lanes, and distances from the front-located vehicles.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "In a landmark work, Zeiler and Fergus [28] used \u201ddeconvolution\u201d to visualize layer activations of convolutional networks.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "[16] provides textual explanations of images as automatically-generated captions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] developed a richer notion of \u201dcontribution\u201d of a pixel to the output.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] trains a deep network to generate species specific explanation without explicitly identifying semantic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] proposes DenseCap which uses fully convolutional localization networks for dense captioning, their paper achieves both localizing objects and describing salient regions in images using natural langauge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] proposes a visualization method to interpret the agents action by describing Markov Decision Process model as a directed graph on a t-SNE map.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3], our model predicts the inverse turning radius \u00fbt (= r\u22121 t , where rt is the turning radius) at every timestep t instead of steering angle commands, which depends on the vehicle\u2019s steering geometry and also result in numerical instability when predicting near zero steering angle commands.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "The relationship between the inverse turning radius ut and the steering angle command \u03b8t can be approximated by Ackermann steering geometry [20] as follows:", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "We also normalized pixel values to [0, 1] in HSV colorspace.", "startOffset": 35, "endOffset": 41}, {"referenceID": 11, "context": "We utilize a single exponential smoothing method [13] to reduce the effect of human factors-related performance variation and the effect of measurement noise.", "startOffset": 49, "endOffset": 53}, {"referenceID": 2, "context": "[3] to learn a model for self-driving cars.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[17], we omit max-pooling layers to prevent spatial locational information loss as the strongest activation propagates through the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "model feeds \u03b1 weighted context yt to the system as discuss by several works [21, 26]:", "startOffset": 76, "endOffset": 84}, {"referenceID": 23, "context": "model feeds \u03b1 weighted context yt to the system as discuss by several works [21, 26]:", "startOffset": 76, "endOffset": 84}, {"referenceID": 19, "context": "Note that, our attention mechanism \u03c0({\u03b1t,i}, {xt,i}) is different from the previous works [21, 26], which use the \u03b1 weighted average context yt = \u2211L i=1 \u03b1t,ixt,i.", "startOffset": 90, "endOffset": 98}, {"referenceID": 23, "context": "Note that, our attention mechanism \u03c0({\u03b1t,i}, {xt,i}) is different from the previous works [21, 26], which use the \u03b1 weighted average context yt = \u2211L i=1 \u03b1t,ixt,i.", "startOffset": 90, "endOffset": 98}, {"referenceID": 10, "context": "As we summarize in Figure 1, we use a long shortterm memory (LSTM) network [12] that predicts the inverse turning radius \u00fbt and generates attention weights {\u03b1t,i} at each timestep t conditioned on the previous hidden state ht and a current convolutional feature cube xt.", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "[26] by averaging of the feature slices x0,i at initial time fed through two additional hidden layers: finit,c and finit,h.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26], doubly stochastic regularization can encourage the attention model to at different parts of the image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Thus, we use fmap({\u03b1t,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).", "startOffset": 104, "endOffset": 107}, {"referenceID": 23, "context": "Thus, we use fmap({\u03b1t,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "In our experiment, we use DBSCAN [9], a density-based clustering algorithm that has advantages to deal with a noisy dataset because they group particles together that are closely packed, while marking particles as outliers that lie alone in low-density regions.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "(C) We randomly sample 3D N = 500 particles over the attention map, and (D) we apply a density-based clustering algorithm (DBSCAN [9]) to find a local visual saliency by grouping particles into clusters.", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "5 at hidden state connections and Xavier initialization [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "Our implementation is based on Tensorflow [1] and code will be publicly available upon publication.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "[3], which generated images with artificial shifts and rotations by using two additional off-center images (left-view and right-view) captured by the same vehicle.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "1 that the single exponential smoothing method [13] is used to reduce the effect of human factors variation and the effect of measurement noise for two sensor inputs: steering angle and velocity.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "[3], which used an identical base CNN and a fully-connected network (FCN) without attention.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable they should provide easy-tointerpret rationales for their behavior so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network\u2019s output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network endto-end from images to steering angle. The attention model highlights image regions that potentially influence the network\u2019s output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network\u2019s behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.", "creator": "LaTeX with hyperref package"}}}