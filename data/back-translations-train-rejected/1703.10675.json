{"id": "1703.10675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Applying Ricci Flow to Manifold Learning", "abstract": "Traditional manifold learning algorithms often bear an assumption that the local neighborhood of any point on embedded manifold is roughly equal to the tangent space at that point without considering the curvature. The curvature indifferent way of manifold processing often makes traditional dimension reduction poorly neighborhood preserving. To overcome this drawback we propose a new algorithm called RF-ML to perform an operation on the manifold with help of Ricci flow before reducing the dimension of manifold.", "histories": [["v1", "Fri, 3 Mar 2017 07:27:04 GMT  (53kb,D)", "https://arxiv.org/abs/1703.10675v1", "10 pages, 1 figure"], ["v2", "Mon, 10 Apr 2017 08:01:52 GMT  (53kb,D)", "http://arxiv.org/abs/1703.10675v2", "10 pages, 1 figure"], ["v3", "Tue, 11 Apr 2017 00:46:25 GMT  (53kb,D)", "http://arxiv.org/abs/1703.10675v3", "10 pages, 1 figure"]], "COMMENTS": "10 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yangyang li", "ruqian lu"], "accepted": false, "id": "1703.10675"}, "pdf": {"name": "1703.10675.pdf", "metadata": {"source": "CRF", "title": "Applying Ricci Flow to Manifold Learning", "authors": ["Yangyang Lia", "Ruqian Lua"], "emails": ["liyangyang12@mails.ucas.ac.cn", "rqlu@math.ac.cn"], "sections": [{"heading": null, "text": "Traditional manifold learning algorithms often assume that the local neighborhood of any point on an embedded manifold roughly corresponds to the tangential space at that point, without taking into account the curvature. Indifferent types of manifold processing often make traditional dimension reduction difficult to maintain. To overcome this disadvantage, we propose a new algorithm called RF-ML to perform an operation on the manifold using the Ricci flow before reducing the dimension of the manifold. Keywords: manifold learning, Riemannic curvature, Ricci flow"}, {"heading": "1. Introduction", "text": "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is the preservation of the global geometric structure of manifold ones, such as Isomap [3]; the other is the preservation of the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Property Map [5] et al. Isomap aims to maintain the geodesic distance between two high-dimensional data points, which can be regarded as an extension of multidimensional scaling (MDS) [12]. Local neighborhood maintenance algorithms often approach manifolds with an association of locally linear patches. After local patches are estimated using linear methods such as PCA, global representation is achieved by aligning the local patches together. Traditional manifold learning algorithms often support the assumption that local patches comprise manifold patches."}, {"heading": "1.1. Motivation", "text": "In the present paper, we first analyze the intrinsic curvature of a manifold at each point and then use the Ricci flow to regulate the metric and curvature of the manifold before reducing its dimension. Since the Ricci flow preserves the local structure of the manifold and the Riemannic metric of the manifold is uniform after the Ricci flow (see Section 2), the local relationships between the data points are maintained throughout the algorithm process. In the current presentation, we only look at open Riemannic manifolds with non-negative Ricci curvature. Diverse learning with negative Ricci curvature will be discussed in our next paper."}, {"heading": "2. Basic Knowledge", "text": "We assume that {x1, x2, \u00b7 xN, \u00b7 RD, where the number of data points and D are defined their dimension. < M, x2, \u00b7 xN, \u00b7 RD, lie on a d-dimensional Riemannian formula embedded in all tangential spaces of M, RD) in the high-dimensional Euclidean space RD (d D), where M is defined the manifold itself and g is defined its Riemannian metric, defined as the family of all inner products of M, RD, as the ambient space of (M, g). The directional derivative defined on a Riemannian connection, represented by a d-dimensional Riemannian relationship, is represented by a fourth order Riemannian the Riemannian."}, {"heading": "3. Algorithm", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4. Experiment", "text": "We compare our method with traditional multiple learning algorithms (PCA, Isomap, LLE, LEP, Diffu-Map, LTSA) on four sets of three-dimensional data from [13], including Swiss Roll, Sphere, Ellipsoid and Gauss. The aim of this comparison is to map each data set in two-dimensional space and then analyze the Neighbourgarving Ratio (NPR) [9] of various algorithms. Neighbourgarving Ratio (NPR) [9] is defined as follows: NB = 1KN N N \u2211 i = 1 | N (xi) N (zi) |, (12) whereN (xi) is the set of K-next sample subscriptions of xi, andN (zi) is the set of K-next sample subscriptions of zi."}, {"heading": "5. Conclusions and Future Work", "text": "Our method aims to find out the force of the Ricci flow and to use it to dynamically deform the local curvature to make the curvature of the manifolds even. Extensive experiments have shown that our method is more stable than other traditional, diverse learning algorithms. A limitation of our algorithm is that RF-ML only works for manifolds with non-negative curvature. We will discuss the applicability of the RF-ML algorithm to manifolds with negative Ricci curvature in our next paper."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Key Research and Development Program of China under grant number 2016YFB1000902; National Natural Science Foundation of China Project No.61232015, No.61472412, No.61621003; Beijing Science and Technology Project: Machine Learning based Stomatology; Tsinghua-TencentAMSS Joint Project: WWW Knowledge Structure and its Application."}], "references": [{"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, vol. 290", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "V", "author": ["J. Tenenbaum"], "venue": "de Silva and J. Langford, A global geometric framework for nonlinear dimension reduction Science, vol. 290", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps and spectral technique for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data", "author": ["D.L. Donoho", "C.E. Grimes"], "venue": "Proceedings for the National Academy of Sciences of the United States of America, vol. 100", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Principal manifolds and nonlinear dimension reduction via local tangent space alignment", "author": ["Z. Zhang", "H. Zha"], "venue": "SIAMJ. Scientific Computing, vol. 26", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Niyaogi, Locality preserving projections", "author": ["P. Xiaofei He"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Principal component analysis", "author": ["I.T. Jolliffe"], "venue": "Springer-Verlag, vol. 69", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "Dimensionality reduction of clustered data sets", "author": ["Guido Sanguinetti"], "venue": "IEEE TPAMI, vol", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Riemannian Manifolds", "author": ["J.M. Lee"], "venue": "Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Riemannian manifolds of positive curvature", "author": ["S. Brendle", "R. Schoen"], "venue": "Proceedings of the International Congress of Mathematicians", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Multidimensional Scaling", "author": ["T. Cox", "M. Cox"], "venue": "Statistics for the Social and Behavioral Sciences", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Mani Matlab demo", "author": ["T. Wittman"], "venue": "http://www.math.umn.edu/\u223cwittman/mani/", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Recent developments on the Ricci flow", "author": ["H.D. Cao", "B. Chow"], "venue": "Bull. Amer. Math. Soc, vol. 36", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 1, "context": "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.", "startOffset": 262, "endOffset": 265}, {"referenceID": 2, "context": "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.", "startOffset": 271, "endOffset": 274}, {"referenceID": 5, "context": "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.", "startOffset": 280, "endOffset": 283}, {"referenceID": 4, "context": "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.", "startOffset": 290, "endOffset": 293}, {"referenceID": 3, "context": "In general, traditional manifold learning algorithms can be roughly divided into two classes: one is to preserve the global geometric structure of manifold, such as Isomap [3]; the other one is to preserve the local neighborhood geometric structure, such as LLE [2], LEP [4], LPP [7], LTSA [6], Hessian Eigenmap [5] et al.", "startOffset": 312, "endOffset": 315}, {"referenceID": 10, "context": "Isomap aimed to preserve the geodesic distance between any two high dimensional data points, which can be viewed as an extension of Multidimensional Scaling (MDS) [12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 6, "context": "After the local patches are estimated with linear methods such as PCA [8], the global representation is obtained by aligning the local patches together.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "The relationship between \u2207\u0303 and \u2207 is shown by the Gauss formula [10]: \u2207\u0303XY = \u2207XY +B (X, Y ) .", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "The corresponding relationship between Rm (X, Y, Z,W ) and R\u0303m (X, Y, Z,W ) is shown by Gauss equation [10]:", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Under local coordinate system, the second fundamental form B can be represented by [10]: B (X, Y ) = \u2211D \u03b1=d+1 h \u03b1 (X, Y ) \u03be\u03b1, where \u03be\u03b1 (\u03b1 = d+ 1, \u00b7 \u00b7 \u00b7 , D) is the normal vector field of M and h (X, Y ) is the second fundamental form coefficient.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Ricci flow is defined by the following geometric evolution time dependent PDE [14] \u2202gij \u2202t = \u22122Ricij , where gij = g (\u2202i, \u2202j).", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "In the time interval t \u2208 I the Riemannian metric g (t) satisfies the metric equivalence condition e\u22122Ctg (0) \u2264 g (t) \u2264 eg (0) [14], where |Ric| \u2264 C.", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "In [11] researchers have worked out that the Riemannian manifold of dimension d \u2265 4 can be transformed to a sphere under", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Experiment We compare our method with traditional manifold learning algorithms (PCA, Isomap, LLE, LEP, Diffu-Map, LTSA) on four sets of three dimensional data from [13] including Swiss Roll, Sphere, Ellipsoid and Gaussian .", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "The objective of this comparison is to map each data set to two dimensional space and then to analyze the neighborhood preserving ratio (NPR) [9] of different algorithms.", "startOffset": 142, "endOffset": 145}, {"referenceID": 7, "context": "The neighborhood preserving ratio (NPR) [9] is defined as follows:", "startOffset": 40, "endOffset": 43}], "year": 2017, "abstractText": "Traditional manifold learning algorithms often bear an assumption that the local neighborhood of any point on embedded manifold is roughly equal to the tangent space at that point without considering the curvature. The curvature indifferent way of manifold processing often makes traditional dimension reduction poorly neighborhood preserving. To overcome this drawback we propose a new algorithm called RF-ML to perform an operation on the manifold with help of Ricci flow before reducing the dimension of manifold.", "creator": "LaTeX with hyperref package"}}}