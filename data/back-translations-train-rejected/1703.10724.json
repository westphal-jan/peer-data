{"id": "1703.10724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "N-gram Language Modeling using Recurrent Neural Network Estimation", "abstract": "We investigate the effective memory depth of RNN models by using them for n-gram language model (LM) smoothing.", "histories": [["v1", "Fri, 31 Mar 2017 01:21:40 GMT  (25kb)", "https://arxiv.org/abs/1703.10724v1", "10 pages, including references"], ["v2", "Tue, 20 Jun 2017 01:22:18 GMT  (25kb)", "http://arxiv.org/abs/1703.10724v2", "10 pages, including references"]], "COMMENTS": "10 pages, including references", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ciprian chelba", "mohammad norouzi", "samy bengio"], "accepted": false, "id": "1703.10724"}, "pdf": {"name": "1703.10724.pdf", "metadata": {"source": "CRF", "title": "N-GRAM LANGUAGE MODELING USING RECURRENT NEURAL NETWORK ESTIMATION", "authors": ["Ciprian Chelba", "Mohammad Norouzi", "Samy Bengio"], "emails": ["ciprianchelba@google.com", "mnorouzi@google.com", "bengio@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.10 724v 2 [cs.C L] 20 Jun 2017 Google Tech ReportExperiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found that the LSTM dropout cell is the best model for encoding the n-gram state when compared with feed and vanilla RNN models. If the assumption of sentence dependence is maintained, the LSTM n-gram matches the LM performance for n = 9 and slightly exceeds it for n = 13. If dependencies across sentence boundaries are allowed, the LSTM 13-gram almost corresponds to the perplexity of the unlimited history LSTM LM.LSTM n-gram smoothing also has the desirable property of improving with increasing n-gram order, unlike the cat or Kneser-Ney back estimators of the Lm."}, {"heading": "1 INTRODUCTION", "text": "A statistical language model (LM) estimates the previous probability values P (W) for word strings W in a vocabulary V, the size of which normally runs into tens of thousands or hundreds of thousands. Typically, the string W is divided into sentences or other segments, such as expressions in automatic speech recognition, which are assumed to be conditionally independent; the assumption of independence has certain advantages in practice, but is not necessarily necessary. Since the parameter space of P (wk | w1, w2,..., wk \u2212 1) is too large, the language model is forced to place the context Wk \u2212 1 = w1, w2,.., wk \u2212 1 in an equivalence class determined by a function (Wk \u2212 1)."}, {"heading": "1.1 PERPLEXITY AS A MEASURE OF LANGUAGE MODEL QUALITY", "text": "A commonly used measure of quality for a particular M language model is related to the entropy of the underlying source and was introduced under the name Perplexity (PPL) Jelinek (1997): PPL (W, M) = exp (\u2212 1NN \u2211 k = 1ln [PM (wk | Wk \u2212 1)])) (3) To give intuitive meaning to the confusion, it represents the average number of guesses that the model must make to determine the identity of the next word when passing the test string W = w1... wN from left to right. It can easily be shown that the confusion of a language model that uses the uniform probability distribution of words in the V vocabulary corresponds to the size of the vocabulary; a good language model should, of course, have less confusion, and thus the vocabulary size of the vocabulary model constitutes an upper limit to the confusion."}, {"heading": "1.2 SMOOTHING", "text": "Since the language model is intended to give invisible word strings a non-zero probability (or equivalent to ensure that the cross entropy of the model is not infinite over any test chain), a desirable property is that: P (wk | \u03a6 (Wk \u2212 1) > 0, wk, Wk \u2212 1 (4) is also known as a smoothing requirement. There are currently two predominant approaches to building LMs: 1.2.1 n-GRAM LANGUAGE MODELsThe most common paradigm in language modeling assumes a Markov assumption and uses the (n \u2212 1) gram equivalence classification, i.e. definitions (Wk \u2212 1). = wk \u2212 n + 1, wk \u2212 n + 2,.., wk \u2212 1 = h (5) Much of the work has focused over the years on different smoothing methods text LMs accumulated, i.e. definitions (Wk \u2212 1). = wk \u2212 n + 1, wk \u2212 n + 1, wk \u2212 n + 2, wk \u2212 1 = h (5)."}, {"heading": "1.2.2 NEURAL LANGUAGE MODELS", "text": "In the last few years, a similar development has taken place in the USA as in other countries of the world, namely not only in the USA, but also in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "2 METHOD", "text": "As in the previous step (D) performed by n-gram models, we are able to present the data we use in the next step. (D) As in the previous step (D) performed by us in the second step (D) performed by us in the second step (D) performed by us in the second step (D) performed in the second step (D) performed by us in the second step (D) performed in the second step (D) performed by us in the third step (D) performed in the third step (D) performed by us in the third step (D) performed in the third step (D) performed by us in the third step (D) performed in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D) performed by us in the third step (D)."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 UPENN TREEBANK CORPUS", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "3.2 ONE BILLION WORDS BENCHMARK", "text": "In a second set of experiments, we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al. (2016). For the base model LSTM, we used the implementation of a single machine provided by J\u00f3zefowicz (2016); the variant of the LSTM n-gram was implemented as a small adaptation to this codebase and thus differs from the one in the UPenn Treebank experiments in Section 3.1. We experimented with the LSTM configuration in Table 3 by J\u00f3zefowicz et al. (2016) for both base lines LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): Embedding and projection layer dimensionality of the STM was 128, a layer with state dimensionality of 2048. Training uses Adagrad with gradient clipping by global standard (10.0) and drop."}, {"heading": "4 CONCLUSIONS AND FUTURE WORK", "text": "The LSTM dropout cell was by far the best (R) NN model for encoding the n-gram state. If one maintains the assumption of sentence dependence, the LSTM-n gram corresponds to the LSTM performance for n = 9 and easily surpasses it for n = 13. If one allows dependencies across sentence boundaries, the LSTM-13 gram almost corresponds to the perplexity of the infinite history LSTM LM. We can conclude that the memory of LSTM-LMs appears to be about 9-13 previous words that are not trivial in depth but not so large. Compared to standard n-gram smoothing methods, LSTMs have excellent statistical properties: they improve the n-gram order well beyond the point at which cat or Kneser-Ney smoothing methods saturate."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Oriol Vinyals and Rafa\u0142 J\u00f3zefowicz for their support in introducing LSTM LMs for UPenn Treebank in Abadi & et al. (2015a) and One Billion Words Benchmark in J\u00f3zefowicz (2016), respectively, and Maxim Krikun for thorough code reviews and useful discussions."}], "references": [{"title": "Recurrent neural networks tutorial (language modeling), 2015a", "author": ["M. Abadi"], "venue": "URL https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html. UPenn Treebank language modeling", "citeRegEx": "Abadi,? \\Q2015\\E", "shortCiteRegEx": "Abadi", "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015b", "author": ["M. Abadi"], "venue": "URL http://tensorflow.org/. Software available from tensorflow.org", "citeRegEx": "Abadi,? \\Q2015\\E", "shortCiteRegEx": "Abadi", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Large language models in machine translation", "author": ["T. Brants", "A.C. Popat", "P. Xu", "F.J. Och", "J. Dean"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": "CoRR, abs/1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A bit of progress in language modeling, extended version", "author": ["Joshua Goodman"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Information Extraction From Speech And Text, chapter 8, pp. 141\u2013142", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "Jelinek.,? \\Q1997\\E", "shortCiteRegEx": "Jelinek.", "year": 1997}, {"title": "Single machine implementation of LSTM language model on One Billion Words benchmark using synchronized gradient updates, 2016", "author": ["Rafal J\u00f3zefowicz"], "venue": "URL https://github.com/rafaljozefowicz/lm", "citeRegEx": "J\u00f3zefowicz.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz.", "year": 2016}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "CoRR, abs/1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "In IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Kneser and Ney.,? \\Q1995\\E", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "CoRR, abs/1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Sparse non-negativematrix languagemodeling", "author": ["Joris Pelemans", "Noam Shazeer", "Ciprian Chelba"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Pelemans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pelemans et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "A commonly used quality measure for a given model M is related to the entropy of the underlying source and was introduced under the name of perplexity (PPL) Jelinek (1997):", "startOffset": 157, "endOffset": 172}, {"referenceID": 10, "context": "The two most popular smoothing techniques are probably Kneser & Ney (1995) and Katz (1987), both making use of back-off to balance the specificity of long contexts with the reliability of estimates in shorter n-gram contexts.", "startOffset": 79, "endOffset": 91}, {"referenceID": 6, "context": "Goodman (2001) provides an excellent overview that is highly recommended to any practitioner of language modeling.", "startOffset": 0, "endOffset": 15}, {"referenceID": 6, "context": "Goodman (2001) provides an excellent overview that is highly recommended to any practitioner of language modeling. Approaches that depart from the nested features used in back-off n-gram LMs have shown excellent results at the cost of increasing the number of features and parameters stored by the model, e.g. Pelemans et al. (2016).", "startOffset": 0, "endOffset": 333}, {"referenceID": 2, "context": "This is commonly named a feed-forward architecture for an n-gram LM (FF-NNLM), first introduced by Bengio et al. (2001). An alternative is the recurrent NNLM architecture that feeds the embedding of each word E(wk) one at a time, advancing the state S \u2208 R of a recurrent cell and producing a new output U \u2208 R: [Sk, Uk] = RNN(Sk\u22121, E(wk)) (6) S0 = 0", "startOffset": 99, "endOffset": 120}, {"referenceID": 11, "context": "The recurrent cell RNN(\u00b7) can consist of one or more simple affine/non-linearity layers, often called a vanilla RNN architecture, see Mikolov et al. (2010). The LSTM cell due to Hochreiter & Schmidhuber (1997) has proven very effective at modeling long range dependencies and has become the state-of-the-art architecture for language modeling using RNNs, see J\u00f3zefowicz et al.", "startOffset": 134, "endOffset": 156}, {"referenceID": 11, "context": "The recurrent cell RNN(\u00b7) can consist of one or more simple affine/non-linearity layers, often called a vanilla RNN architecture, see Mikolov et al. (2010). The LSTM cell due to Hochreiter & Schmidhuber (1997) has proven very effective at modeling long range dependencies and has become the state-of-the-art architecture for language modeling using RNNs, see J\u00f3zefowicz et al.", "startOffset": 134, "endOffset": 210}, {"referenceID": 9, "context": "The LSTM cell due to Hochreiter & Schmidhuber (1997) has proven very effective at modeling long range dependencies and has become the state-of-the-art architecture for language modeling using RNNs, see J\u00f3zefowicz et al. (2016). In this work we approximate unlimited history (R)NN models with n-gram models in an attempt to identify the order n at which they become equivalent from a perplexity point of view.", "startOffset": 202, "endOffset": 227}, {"referenceID": 3, "context": "\u2022 the training data can be reduced to n-gram sufficient statistics, and the target distribution presented to the NN n-gram LM in a given context can be a multinomial pmf instead of the one-hot encoding used in on-line training for (R)NN LMs; \u2022 unlike many LSTM LM implementations, back-propagation through time for the LSTM n-gram need not be truncated at the begining of segments used to batch the training data; \u2022 the state in a n-gram LM can be succinctly represented with (n \u2212 1) \u2217 4 bytes storing the identity of the words in the context; this is in stark contrast with the state S \u2208 R for an RNN LM, where s = 1024 or higher, making the n-gram LM much easier to use in decoders such as for ASR/SMT; \u2022 similar to Brants et al. (2007), batches of n-gram contexts can be processed in parallel to estimate a sharded (R)NN n-gram model; this is particularly attractive because it allows scaling both the amount of training data and the NNLM size significantly (100X).", "startOffset": 718, "endOffset": 739}, {"referenceID": 5, "context": "(8), using Adagrad (Duchi et al. (2011)) and gradient norm clipping (Pascanu et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 5, "context": "(8), using Adagrad (Duchi et al. (2011)) and gradient norm clipping (Pascanu et al. (2012)); the model parameters are initialized by sampling from a truncated normal distribution of zero mean and a given standard deviation.", "startOffset": 20, "endOffset": 91}, {"referenceID": 0, "context": "All models were implemented using TensorFlow, see Abadi & et al. (2015b).", "startOffset": 50, "endOffset": 73}, {"referenceID": 16, "context": "wk\u22121 is embedded using the mapping E(w); the resulting vectors are concatenated to form a d \u00b7 (n\u2212 1) dimensional vector that is first fed into a dropout layer Srivastava et al. (2014) and then into an affine layer followed by a tanh non-linearity.", "startOffset": 159, "endOffset": 184}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively.", "startOffset": 141, "endOffset": 164}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively.", "startOffset": 141, "endOffset": 207}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively. Perhaps a bit of a technicality but it is worth pointing out a major difference between error backpropagation through time (BPTT) as implemented in either of the above and the error backpropagation in the LSTM/RNN n-gram LM: Abadi & et al. (2015a) and J\u00f3zefowicz (2016) implement BPTT by segmenting the training data into non-overlapping segments (of length 35 or 20, respectively).", "startOffset": 141, "endOffset": 470}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively. Perhaps a bit of a technicality but it is worth pointing out a major difference between error backpropagation through time (BPTT) as implemented in either of the above and the error backpropagation in the LSTM/RNN n-gram LM: Abadi & et al. (2015a) and J\u00f3zefowicz (2016) implement BPTT by segmenting the training data into non-overlapping segments (of length 35 or 20, respectively).", "startOffset": 141, "endOffset": 492}, {"referenceID": 0, "context": "For our initial set of experiments we used the same data set as in Abadi & et al. (2015a), with exactly the same training/validation/test set partition and vocabulary.", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "For our initial set of experiments we used the same data set as in Abadi & et al. (2015a), with exactly the same training/validation/test set partition and vocabulary. The training data consists of about one million words, and the vocabulary contains ten thousand words; the validation/test data contains 73760/82430 words, respectively (including the end-of-sentence token). The out-of-vocabulary rate on validation/test data is 5.0/5.8%, respectively. As an initial batch of experiments we trained and evaluated back-off n-gram models using Katz and interpolated Kneser-Ney smoothing. We also used the medium setting in Abadi & et al. (2015a) as an LSTM/RNN LM baseline; since the baseline n-gram models are trained under a sentence independence assumption, we also ran the LSTM/RNN LM baseline by resetting the LSTM state at each sentence beginning.", "startOffset": 67, "endOffset": 645}, {"referenceID": 0, "context": "The medium setting for the LSTM LM in Abadi & et al. (2015a) performs significantly better than the KN baseline.", "startOffset": 38, "endOffset": 61}, {"referenceID": 4, "context": "In a second set of experiments we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 4, "context": "In a second set of experiments we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al. (2016). For the baseline LSTM model we used the single machine implementation provided by J\u00f3zefowicz (2016); the LSTM n-gram variant was implemented as a minor tweak on this codebase and is thus different from the one used in the UPenn Treebank experiments in Section 3.", "startOffset": 53, "endOffset": 115}, {"referenceID": 4, "context": "In a second set of experiments we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al. (2016). For the baseline LSTM model we used the single machine implementation provided by J\u00f3zefowicz (2016); the LSTM n-gram variant was implemented as a minor tweak on this codebase and is thus different from the one used in the UPenn Treebank experiments in Section 3.", "startOffset": 53, "endOffset": 216}, {"referenceID": 9, "context": "We experimented with the LSTM configuration in Table 3 of J\u00f3zefowicz et al. (2016) for both baseline LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): embedding and projection layer dimensionality was 128, one layer with state dimensionality of 2048.", "startOffset": 58, "endOffset": 83}, {"referenceID": 9, "context": "We experimented with the LSTM configuration in Table 3 of J\u00f3zefowicz et al. (2016) for both baseline LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): embedding and projection layer dimensionality was 128, one layer with state dimensionality of 2048.", "startOffset": 58, "endOffset": 183}, {"referenceID": 9, "context": "We experimented with the LSTM configuration in Table 3 of J\u00f3zefowicz et al. (2016) for both baseline LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): embedding and projection layer dimensionality was 128, one layer with state dimensionality of 2048. Training used Adagrad with gradient clipping by global norm (10.0) and droput (probability 0.1); back-propagation at the output soft-max layer is done using importance sampling as described in J\u00f3zefowicz et al. (2016) with a set 8192 \u201cnegative\u201d samples.", "startOffset": 58, "endOffset": 502}, {"referenceID": 4, "context": "As expected PPL dit not change significantly because the sentences in the training and test data were randomized, see Chelba et al. (2013); in fact modeling the sentence independence explicitly is slightly beneficial.", "startOffset": 118, "endOffset": 139}, {"referenceID": 0, "context": "We would like to thank Oriol Vinyals and Rafa\u0142 J\u00f3zefowicz for support with the baseline implementation of LSTM LMs for UPenn Treebank in Abadi & et al. (2015a) and One Billion Words Benchmark in J\u00f3zefowicz (2016), respectively.", "startOffset": 137, "endOffset": 160}, {"referenceID": 0, "context": "We would like to thank Oriol Vinyals and Rafa\u0142 J\u00f3zefowicz for support with the baseline implementation of LSTM LMs for UPenn Treebank in Abadi & et al. (2015a) and One Billion Words Benchmark in J\u00f3zefowicz (2016), respectively.", "startOffset": 137, "endOffset": 213}], "year": 2017, "abstractText": "We investigate the effective memory depth of RNN models by using them for n-gram language model (LM) smoothing. Experiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the n-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM n-gram matches the LSTM LM performance for n = 9 and slightly outperforms it for n = 13. When allowing dependencies across sentence boundaries, the LSTM 13-gram almost matches the perplexity of the unlimited history LSTM LM. LSTM n-gram smoothing also has the desirable property of improving with increasing n-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low n-gram orders. Experiments on the One Billion Words benchmark show that the results hold at larger scale: while LSTM smoothing for short n-gram contexts does not provide significant advantages over classic N-gram models, it becomes effective with long contexts (n > 5); depending on the task and amount of data it can match fully recurrent LSTM models at about n = 13. This may have implications when modeling short-format text, e.g. voice search/query LMs. Building LSTM n-gram LMs may be appealing for some practical situations: the state in a n-gram LM can be succinctly represented with (n\u2212 1) \u2217 4 bytes storing the identity of the words in the context and batches of n-gram contexts can be processed in parallel. On the downside, the n-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM.", "creator": "LaTeX with hyperref package"}}}