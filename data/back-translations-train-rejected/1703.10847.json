{"id": "1703.10847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation", "abstract": "In this paper, we present MidiNet, a deep convolutional neural network (CNN) based generative adversarial network (GAN) that is intended to provide a general, highly adaptive network structure for symbolic-domain music generation. The network takes random noise as input and generates a melody sequence one mea- sure (bar) after another. Moreover, it has a novel reflective CNN sub-model that allows us to guide the generation process by providing not only 1D but also 2D conditions. In our implementation, we used the intended chord of the current bar as a 1D condition to provide a harmonic context, and the melody generated for the preceding bar previously as a 2D condition to provide sequential information. The output of the network is a 16 by 128 matrix each time, representing the presence of each of the 128 MIDI notes in the generated melody sequence of that bar, with the smallest temporal unit being the sixteenth note. MidiNet can generate music of arbitrary number of bars, by concatenating these 16 by 128 matrices. The melody sequence can then be played back with a synthesizer. We provide example clips showing the effectiveness of MidiNet in generating harmonic music.", "histories": [["v1", "Fri, 31 Mar 2017 10:59:58 GMT  (719kb,D)", "http://arxiv.org/abs/1703.10847v1", "6 pages"], ["v2", "Tue, 18 Jul 2017 08:07:36 GMT  (2062kb,D)", "http://arxiv.org/abs/1703.10847v2", "8 pages, Accepted to ISMIR (International Society of Music Information Retrieval) Conference 2017"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["li-chia yang", "szu-yu chou", "yi-hsuan yang"], "accepted": false, "id": "1703.10847"}, "pdf": {"name": "1703.10847.pdf", "metadata": {"source": "CRF", "title": "MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions", "authors": ["Li-Chia Yang", "Szu-Yu Chou", "Yi-Hsuan Yang"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Music Generation, Deep Learning, Generative Networks"}, {"heading": "1 Introduction", "text": "In a GAN, a generator model and a discrimination model, both are used simultaneously under the motto \"The Diversity of the Generation.\" The generator model aims to generate artificial data that can transform a certain random noise into something that seems realistic to humans. It can be an image, a passage of text or an audio track, depending on the training data. The majority of the existing work has focused on generating images using GANs [3-5].To model the temporal nature of music, we are based on recursive neural networks (RNNs)."}, {"heading": "2 Methods", "text": "The core of MidiNet is a modified DCGAN structure (Deep Convolutional Generative Adversarial Network) [3], which is designed to learn symbolic audio representation in a specified period of time."}, {"heading": "2.1 Generator model", "text": "When designing CNNs for image-related tasks, it is common to use 2D convolutions filters for the Learning feature [3,13]. In contrast, it is sometimes good enough to use 1D convolutions filters because audio data is sequential [14, 15]. Therefore, in the last transposed convolutions layer, we use filters of the form h-by-1, where h indicates the number of MIDI notes viewed by the network. Similarly, the series of transposed convolutions converts a vector of random values into a 2D matrix of the desired form h-by-w as output, corresponding to the occurrence of different notes in the w-time units. Similarly, the size of the filters in the first transposed convolutions layer is set to 1-by-w.The generator model is shown in Figure 1.2. Generally, a melody sequence of notes can be defined as an expression of a particular musical idea."}, {"heading": "2.2 Discriminator model", "text": "The discriminator model is a typical CNN with two sinuous layers and two dense layers. It is trained to distinguish between the 2D representation (i.e. a h-for-w matrix) of a real (i.e. authentic) melody sequence of a bar and the generated (i.e. artificial) melody sequence of a bar."}, {"heading": "2.3 1D condition", "text": "According to the ideas of Conditional Generative Adverse Networks (CGANs) [12], we add 1D conditions not only to the input layers of the discriminator model and the generator model, but also to all intermediate layers. Generally, a 1D condition is represented as a n-dimensional vector. To add it to an intermediate layer of the form a-by-b, we project the conditioning vector into the same matrix shape to obtain a tensor of the form a-by-b-n, and link it to the intermediate layer in the feature map axis. This is illustrated by the bright orange blocks in Figure 1."}, {"heading": "2.4 2D condition: reflective CNN", "text": "Since the generation result of our GAN is a 2D matrix of time and frequency information, it is plausible to condition each entry of the 2D matrix, resulting in a 2D-conditioned matrix. In contrast to the 1D case, it is not easy to convert a 2D matrix into a matrix of a different size. Without such a transformation, we will not be able to condition the intermediate layers of the generator and discriminator model like CNN. However, in order to reasonably assign the 2D state to the intermediate layers, we propose to train another CNN that has almost the same architecture (i.e. the number of layers, the number of filters in each layer, and the size of these filters) as CNN of the generator model. However, while CNN takes a vector of random numbers as input and uses the desired h-for-w matrix as output, the new CNN filter takes the exact input direction of the matrix - it takes by w-for-h input."}, {"heading": "3 Implementation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "Although it is possible to use other means to condition the MidiNet, in our current implementation we use an accordion label for the 1D condition and the melody condition for the 2D condition as described in the last paragraph. Therefore, we need a collection of midi and accordion chords that contain chords other than the 24 base chords."}, {"heading": "3.2 Training of the MidiNet", "text": "Each midi-tab in our training set has 8 bars, so it is possible to condition the generation of the melody sequence of a bar through the melody sequences of the previous 7 bars, using 7 conditional matrices, but this is only possible for the last bar in each midi-tab. However, in order to get the best possible benefit from all bars and also for reasons of simplicity, we only took into account the previous bar for the 2D state in our implementation. Accordingly, when creating the MidiNet, we sampled 2 consecutive bars each time, using the second bar for the real melody and the first bar for the 2D condition.We assumed that the discriminator model should be able to distinguish a real melody from an artificial one, without knowing the melody melody melody of a previous bar. Therefore, we used only the 1D state for the discriminator model. In contrast, we used both the current state of the 1D and the 2D state of the generator as well as the 2D state of the D-D generator."}, {"heading": "4 Results", "text": "In Figure 2 (a) only the state of the 1D chords is used, while in Figure 2 (b) both the 1D and 2D conditions are used. In both cases, MidiNet produces melodies that also occur in the chords, demonstrating the effectiveness of the state of the 1D chords. Furthermore, we see that MidiNet can generate melodies with some repetitive tones when using the 2D state of the previous bar, thus improving the connection between adjacent bars. To evaluate the aesthetic quality and convenience of the production result, we need a listening study involving human listeners. It can also be formulated as a turning test that asks the listener to distinguish between real and generated melodies. For example, while the listening study is still in progress, we provide audio samples of the synthesized generation online for a subjective evaluation. Such audio samples can be found in http: / / rich.gigigio.us _ 14b that the structure of the school is not altered."}, {"heading": "5 Discussion and Conclusion", "text": "The design of MidiNet was inspired by the way people composed music - we first have a chord sequence in mind and then filled the melody line one bar at a time. 4 We also wanted to keep the structure of MidiNet flexible so that it can absorb other musical insights and insights from music theory research so that it can be used in real-time applications. However, it is also possible to additionally condition the MidiNet with the melody of the \"next bar\" to fill the melody line back and forth in an iterative fashion. Step forward, we plan to consider in our future work a higher hierarchy of music - the song structures such as intro, verse and chorus (see Figure 3) - by creating longer music or even a complete song."}], "references": [{"title": "and Y", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville"], "venue": "Bengio, \u201cGenerative adversarial nets,\u201d in Advances in neural information processing systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "NIPS 2016 tutorial: Generative adversarial networks,", "author": ["I.J. Goodfellow"], "venue": "arXiv preprint arXiv:1701.00160,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "and S", "author": ["A. Radford", "L. Metz"], "venue": "Chintala, \u201cUnsupervised representation learning with deep convolutional generative adversarial networks,\u201d arXiv preprint arXiv:1511.06434", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional generative adversarial nets for convolutional face generation,\u201d Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester", "author": ["J. Gauthier"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "and X", "author": ["T. Salimans", "I.J. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford"], "venue": "Chen, \u201cImproved techniques for training GANs,\u201d in Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "C-RNN-GAN: Continuous recurrent neural networks with adversarial training,", "author": ["O. Mogren"], "venue": "arXiv preprint arXiv:1611.09904,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "DeepBach: a steerable model for bach chorales generation,", "author": ["G. Hadjeres", "F. Pachet"], "venue": "arXiv preprint arXiv:1612.01010,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "and D", "author": ["A. Roberts", "J. Engel", "C. Hawthorne", "I. Simon", "E. Waite", "S. Oore", "N. Jaques", "C. Resnick"], "venue": "Eck, \u201cInteractive musical improvisation with Magenta,\u201d Neural Information Processing Systems (NIPS)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding temporal structure in music: Blues improvisation with lstm recurrent networks,", "author": ["D. Eck", "J. Schmidhuber"], "venue": "Neural Networks for Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "and P", "author": ["N. Boulanger-Lewandowski", "Y. Bengio"], "venue": "Vincent, \u201cModeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription,\u201d arXiv preprint arXiv:1206.6392", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Conditional image generation with pixelCNN decoders,", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Conditional generative adversarial nets,", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "and G", "author": ["A. Krizhevsky", "I. Sutskever"], "venue": "E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Event localization in music auto-tagging,", "author": ["J.-Y. Liu", "Y.-H. Yang"], "venue": "Proceedings of ACM Multimedia,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Deep content-based music recommendation,", "author": ["A. Van den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "and P", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever"], "venue": "Abbeel, \u201cInfoGAN: Interpretable representation learning by information maximizing generative adversarial nets,\u201d in Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "and L", "author": ["M. Arjovsky", "S. Chintala"], "venue": "Bottou, \u201cWasserstein GAN,\u201d arXiv preprint arXiv:1701.07875", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "WaveNet: A generative model for raw audio,", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "vol. 1, MIT press Cambridge", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Rethinking automatic chord recognition with convolutional neural networks,", "author": ["E.J. Humphrey", "J.P. Bello"], "venue": "in Proc. International Conference on Machine Learning and Applications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Music Emotion Recognition", "author": ["Y.-H. Yang", "H.H. Chen"], "venue": "CRC Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Generative adversarial networks (GANs) [1, 2] have garnered tremendous attention in recent years.", "startOffset": 39, "endOffset": 45}, {"referenceID": 1, "context": "Generative adversarial networks (GANs) [1, 2] have garnered tremendous attention in recent years.", "startOffset": 39, "endOffset": 45}, {"referenceID": 2, "context": "To date, the majority of existing work has focused on image generation using GANs [3\u20135].", "startOffset": 82, "endOffset": 87}, {"referenceID": 3, "context": "To date, the majority of existing work has focused on image generation using GANs [3\u20135].", "startOffset": 82, "endOffset": 87}, {"referenceID": 4, "context": "To date, the majority of existing work has focused on image generation using GANs [3\u20135].", "startOffset": 82, "endOffset": 87}, {"referenceID": 5, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 6, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 7, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 8, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 9, "context": "To model the temporal nature of music, a number of previous work on music generation are based on recurrent neural networks (RNNs) [6\u201310].", "startOffset": 131, "endOffset": 137}, {"referenceID": 10, "context": "Although these RNNs can sometimes produce realistic music, the training of an RNN is usually slower, as compared with that of a convolutional neural network (CNN) [11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 4, "context": "Moreover, taking advantage of the idea of conditional adversarial training [5, 11, 12], we propose a novel reflective CNN sub-model that uses the melody sequence generated previously for the preceding bar (which is again represented as a 2D matrix) to condition the generation for the current bar.", "startOffset": 75, "endOffset": 86}, {"referenceID": 10, "context": "Moreover, taking advantage of the idea of conditional adversarial training [5, 11, 12], we propose a novel reflective CNN sub-model that uses the melody sequence generated previously for the preceding bar (which is again represented as a 2D matrix) to condition the generation for the current bar.", "startOffset": 75, "endOffset": 86}, {"referenceID": 11, "context": "Moreover, taking advantage of the idea of conditional adversarial training [5, 11, 12], we propose a novel reflective CNN sub-model that uses the melody sequence generated previously for the preceding bar (which is again represented as a 2D matrix) to condition the generation for the current bar.", "startOffset": 75, "endOffset": 86}, {"referenceID": 4, "context": "We note that, while 1D conditions have been widely used in previous work on GAN [5, 11, 12], the use of a 2D condition has not been attempted before, to the best of our knowledge.", "startOffset": 80, "endOffset": 91}, {"referenceID": 10, "context": "We note that, while 1D conditions have been widely used in previous work on GAN [5, 11, 12], the use of a 2D condition has not been attempted before, to the best of our knowledge.", "startOffset": 80, "endOffset": 91}, {"referenceID": 11, "context": "We note that, while 1D conditions have been widely used in previous work on GAN [5, 11, 12], the use of a 2D condition has not been attempted before, to the best of our knowledge.", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "To stabilize the training updates of the GAN, we made use of techniques such as feature matching and one-sided label smoothing [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "The core of MidiNet is a modified deep convolutional generative adversarial network (DCGAN) structure [3], which is designed to learn the symbolic audio representation in fixed time length.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "In designing CNNs for image-related tasks, the use of 2D convolutional filters for feature learning is common [3,13].", "startOffset": 110, "endOffset": 116}, {"referenceID": 12, "context": "In designing CNNs for image-related tasks, the use of 2D convolutional filters for feature learning is common [3,13].", "startOffset": 110, "endOffset": 116}, {"referenceID": 13, "context": "In contrast, for audio-related tasks, it is sometimes good enough to use 1D convolutional filters, since audio data is sequential [14, 15].", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "In contrast, for audio-related tasks, it is sometimes good enough to use 1D convolutional filters, since audio data is sequential [14, 15].", "startOffset": 130, "endOffset": 138}, {"referenceID": 11, "context": "Following the ideas of the conditional generative adversarial networks (CGANs) [12], we add 1D conditions not only to the input layers of the discriminator model and the generator model, but also to all the intermediate layers.", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Due to the nature of Nash equilibrium of non-convex games, the training of GANs is subject to issues of instability and mode collapsing [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "By means of a deeper analysis of the optimization problem and the model structure, a few techniques have been proposed recently to deal with these issues and accordingly stabilize the training updates of GANs [5,16,17].", "startOffset": 209, "endOffset": 218}, {"referenceID": 15, "context": "By means of a deeper analysis of the optimization problem and the model structure, a few techniques have been proposed recently to deal with these issues and accordingly stabilize the training updates of GANs [5,16,17].", "startOffset": 209, "endOffset": 218}, {"referenceID": 16, "context": "By means of a deeper analysis of the optimization problem and the model structure, a few techniques have been proposed recently to deal with these issues and accordingly stabilize the training updates of GANs [5,16,17].", "startOffset": 209, "endOffset": 218}, {"referenceID": 4, "context": "In our implementation, the feature matching and one-sided label smoothing techniques were employed [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "For future work, we can learn longer-term sequential information by considering the melodies of multiple previous bars, or by using the so-called dilated causal convolutions as proposed in the WaveNet model [18] in our reflective CNN.", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 220, "endOffset": 224}, {"referenceID": 13, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 245, "endOffset": 249}, {"referenceID": 20, "context": "Following the ideas of reinforcement learning [19], we will also be interested in integrating to MidiNet more computational models dealing with different aspects of music information, such as automatic chord recognition [20], music auto-tagging [14], and music emotion recognition [21].", "startOffset": 281, "endOffset": 285}], "year": 2017, "abstractText": "In this paper, we present MidiNet, a deep convolutional neural network (CNN) based generative adversarial network (GAN) that is intended to provide a general, highly adaptive network structure for symbolic-domain music generation. The network takes random noise as input and generates a melody sequence one measure (bar) after another. Moreover, it has a novel reflective CNN sub-model that allows us to guide the generation process by providing not only 1D but also 2D conditions. In our implementation, we used the intended chord of the current bar as a 1D condition to provide a harmonic context, and the melody generated for the preceding bar previously as a 2D condition to provide sequential information. The output of the network is a 16 by 128 matrix each time, representing the presence of each of the 128 MIDI notes in the generated melody sequence of that bar, with the smallest temporal unit being the sixteenth note. MidiNet can generate music of arbitrary number of bars, by concatenating these 16 by 128 matrices. The melody sequence can then be played back with a synthesizer. We provide example clips showing the effectiveness of MidiNet in generating harmonic music.", "creator": "LaTeX with hyperref package"}}}