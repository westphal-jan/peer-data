{"id": "1703.11008", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", "abstract": "One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous in this \"deep learning\" regime. In order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.", "histories": [["v1", "Fri, 31 Mar 2017 17:56:41 GMT  (29kb)", "http://arxiv.org/abs/1703.11008v1", "16 pages, 1 table"], ["v2", "Thu, 19 Oct 2017 03:39:56 GMT  (244kb,D)", "http://arxiv.org/abs/1703.11008v2", "14 pages, 1 table, 2 figures. Corresponds with UAI camera ready and supplement. Includes additional references and related experiments"]], "COMMENTS": "16 pages, 1 table", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gintare karolina dziugaite", "daniel m roy"], "accepted": false, "id": "1703.11008"}, "pdf": {"name": "1703.11008.pdf", "metadata": {"source": "CRF", "title": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", "authors": ["Gintare Karolina Dziugaite"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.11 008v 1 [cs.L G] 3Contents1. Introduction 1 2. Preparatory work 4 3. Optimization of the PAC-Bayes boundary 6 4. Experiments 8 5. Results 9 6. Related work 10 7. Conclusions and future work 13 receipts 13 references 14 A. Network symmetries 15 B. Approaching KL \u2212 1 (q | c) 16"}, {"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to outdo themselves if they do not see themselves in a position to outdo themselves. (...) In fact, it is so that they are able to outdo themselves. (...). (...) In fact, it is so that they are able to outdo themselves. (...). \"\" It is so that they are able to outdo themselves. (...). \"\" \"It is so that they are able to outdo themselves. (...).\" \"It is as if they are able to outdo themselves.\" (...). \"(...).\" (...). (It. (...). (It.). (It.). (It.). (It.). (It. (...). (It.). (It. (...). (It.). (It. (...). (). (It. (...). (It.). (It. (). (It.). (It.). (It.). (It.). (It.). (It.). (It. (It.). (It.). (It.). (It.). (It.). (It.). (It. (It.). (It.). (It.). (It. (It.). (It.). (It.). (It. (It.). (It.). (It.). (It.). (It.). (It. (It.) is. (It.).). (It.). (It. (It. (It.). (. (It.).). (It.). (It.). (It. (It. (It.)"}, {"heading": "2. Preliminaries", "text": "Many of the hypotheses produced by H are H = {hw}."}, {"heading": "3. Optimizing the PAC-Bayes bound", "text": "Let us replace the empirical losses by the family of multicultural distributions with the usual structures of optimization. (We will). We will conceive ourselves as a kind of system in which we will keep to the rules. (We will). (We will not do what we want. (We will do it.). (We will.). (We will.). (We will.). (We will.). (We will.). (We will.). (We will. (.). (We will. (.). (We will. (.). (We will.). (.). (We will.). (.). (We will.). (.). (.). (We will. (.). (.). (. (.). (We will. (.). (.). (We will. (.). (We will. (.). (. (.). (We will. (.). (. (.). (.). (We will. (.). (. (.). (. (.). (We will. (.). (.). (. (.). (.). (. (.). (.). (.). (. (.). (. (.). (.). (.). (.). (. (.).). (. (.). (.). (.). (.). (. (.). (.). (. (.). (. (.). (.). (.). (. (.). (. (.). (.). (.). (. (.). (. (.). (.). (. (.). (.). (. (. (. (.).). (. (.). (. (.). (. (.).). (.). (. (.). (.). (. (. (.).). (. (.). (. (. (. (.). (. (.).).). (. (.). ("}, {"heading": "4. Experiments", "text": "We optimize the PAC-Bayes classification based on the error rates of the stochastic neural networks, which we trace back to a binary classification of MNIST types. (We use the MNIST key figures, which we use on the basis of IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT, IT"}, {"heading": "5. Results", "text": "See Table 1. All SGD-trained networks achieve perfect or near-perfect accuracy on the training data. On true labels, the SNN mean training error increases slightly as the weight distribution widens to minimize KL divergence. The SGD solution is close to the SNN mean measured in terms of SNN covariance. For the random label experiment, the SNN mean training error does not change much in the different architectures, despite the potential for overmatch. This phenomenon is well known, although it is still remarkable. For the random label experiment, the empirical test classification error of 0.508 represents a lack of generalization as expected. The same two patterns also hold for the SNN test error, with slightly higher error rates."}, {"heading": "6. Related work", "text": "As we mentioned in the introduction, we have incorporated the ideas in [HC93] and [LC02] into the modern, deeper learning systems in which the networks have a million parameters, but which rely on one or two orders of magnitude of fewer learning examples. (The objective we are aiming for in the PAC-Bayes region is a random init.) W \"r\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "7. Conclusions and Future work", "text": "These limits are reached by optimizing a target derived from the PAC-Bayes boundary, based on the solution produced by the SGD. (However, when the labels are randomized, optimizing the PAC-Bayes boundary leads to a significant shift in the solution.) Our experiments only look at fully networked feed networks trained on a binary classification problem derived from MNIST. It would be interesting to see if the results extend to multiclass classification, other datasets, and other types of architectures that have the SGD boundaries. In our experiments, we optimize the PAC-Bayes boundaries, based on an SGD solution."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Peter Bartlett, Shai Ben-David, Dylan Foster, Matus Telgarsky and Ruth Urner for their helpful discussions. GKD is supported by an EPSRC scholarship, DMR is supported by an NSERC Discovery Grant, Connaught Award and a scholarship from the U.S. Air Force Office of Scientific Research # FA9550-15-1-0074."}, {"heading": "A. Network symmetries", "text": "It is not as if it were a matter of a way in which the distribution mechanisms of the neural networks of Rd, i.e., stochastic neural networks on RD, which we use for the purposes of understanding the distribution mechanisms of neural networks, which we use for the purposes of PAC-Bayes, it is the KL-Bayes-Bayes boundary, it is the KL junction KL, which is the performance of the stochastic neural networks QL."}, {"heading": "B. Approximating KL\u22121(q|c)", "text": "There is no simple formula for KL \u2212 1 (q | c), but we can approximate it using root finding techniques; for all q (0, 1) and c \u2265 0 we define hq, c (p) = KL (q | | p) \u2212 c. Then h'q, c (p) = 1 \u2212 q 1 \u2212 p \u2212 q p \u2212 q p p p p p. With a sufficiently good first estimate p0 of a root of hq, c (\u00b7) we can obtain improved estimates of a root by Newton's method: pn + 1 = N (pn; q, c), where N (p; q, c) = p \u2212 hq, c (c) h'q, c (p). (25) This suggests the following approximation to KL \u2212 1 (q | c): (1) Let us leave b = q + \u221a c 2. (2) If b \u00b2 1, then return 1. (3), then we return Nk (b), for some small integers k > Our reported results in five steps of the Newton method."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org", "author": ["V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses", "author": ["C. Baldassi", "A. Ingrosso", "C. Lucibello", "L. Saglietti", "R. Zecchina"], "venue": "Phys. Rev. Lett", "citeRegEx": "Baldassi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baldassi et al\\.", "year": 2015}, {"title": "Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes", "author": ["C. Baldassi", "C. Borgs", "J.T. Chayes", "A. Ingrosso", "C. Lucibello", "L. Saglietti", "R. Zecchina"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Baldassi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Baldassi et al\\.", "year": 2016}, {"title": "The impact of the nonlinearity on the VC-dimension of a deep network", "author": ["P.L. Bartlett"], "venue": "Preprint.", "citeRegEx": "Bar17", "shortCiteRegEx": null, "year": 2017}, {"title": "Visual Reconstruction", "author": ["A. Blake", "A. Zisserman"], "venue": "Cambridge, MA, USA: MIT Press", "citeRegEx": "BZ87", "shortCiteRegEx": null, "year": 1987}, {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "author": ["P. Chaudhari", "A. Choromanska", "S. Soatto", "Y. LeCun", "C. Baldassi", "C. Borgs", "J. Chayes", "L. Sagun", "R. Zecchina"], "venue": "In: International Conference on Learning Representations (ICLR). 2017. arXiv:", "citeRegEx": "Cha+17", "shortCiteRegEx": null, "year": 1611}, {"title": "2017", "author": ["L. Dinh", "R. Pascanu", "S. Bengio", "Y. Bengio. Sharp Minima Can Generalize For Deep Nets"], "venue": "arXiv:", "citeRegEx": "Din+17", "shortCiteRegEx": null, "year": 1703}, {"title": "Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights", "author": ["G.E. Hinton", "D. van Camp"], "venue": "Proceedings of the Sixth Annual Conference on Computational Learning Theory. COLT \u201993", "citeRegEx": "Hinton and Camp.,? \\Q1993\\E", "shortCiteRegEx": "Hinton and Camp.", "year": 1993}, {"title": "On Graduated Optimization for Stochastic Non-Convex Problems", "author": ["E. Hazan", "K.Y. Levy", "S. Shalev-Shwartz"], "venue": "Proceedings of the 33rd International Conference on Machine Learning (ICML)", "citeRegEx": "Hazan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2016}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "CoRR abs/1509.01240", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Flat Minima", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "author": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"], "venue": "In: International Conference on Learning Representations (ICLR). 2017. arXiv:", "citeRegEx": "Kes+17", "shortCiteRegEx": null, "year": 1609}, {"title": "Quantitatively tight sample complexity bounds", "author": ["J. Langford"], "venue": "Carnegie Mellon University", "citeRegEx": "Lan02", "shortCiteRegEx": null, "year": 2002}, {"title": "Not) Bounding the True Error", "author": ["J. Langford", "R. Caruana"], "venue": "Advances in Neural Information Processing Systems 14", "citeRegEx": "Langford and Caruana.,? \\Q2002\\E", "shortCiteRegEx": "Langford and Caruana.", "year": 2002}, {"title": "MNIST handwritten digit database", "author": ["Y. LeCun", "C. Cortes", "C.J.C. Burges"], "venue": "http://yann.lecun.com/exdb/mnist/.", "citeRegEx": "LCB10", "shortCiteRegEx": null, "year": 2010}, {"title": "Bounds for Averaging Classifiers", "author": ["J. Langford", "M. Seeger"], "venue": "Tech. rep. CMU-CS-01-102. Carnegie Mellon University", "citeRegEx": "LS01", "shortCiteRegEx": null, "year": 2001}, {"title": "PAC-Bayesian Model Averaging", "author": ["D.A. McAllester"], "venue": "Proceedings of the Twelfth Annual Conference on Computational Learning Theory. COLT \u201999. Santa Cruz, California, USA: ACM,", "citeRegEx": "McAllester.,? \\Q1999\\E", "shortCiteRegEx": "McAllester.", "year": 1999}, {"title": "A Universal Prior for Integers and Estimation by Minimum Description Length", "author": ["J. Rissanen"], "venue": "Ann. Statist", "citeRegEx": "Rissanen.,? \\Q1983\\E", "shortCiteRegEx": "Rissanen.", "year": 1983}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "In: International Conference on Representation Learning (ICLR). 2017. arXiv:", "citeRegEx": "Zha+17", "shortCiteRegEx": null, "year": 1611}], "referenceMentions": [{"referenceID": 15, "context": "Finally, we present a variant of the PAC-Bayes bound due to Langford and Seeger [LS01].", "startOffset": 80, "endOffset": 86}, {"referenceID": 12, "context": "(See also [Lan02].", "startOffset": 10, "endOffset": 17}, {"referenceID": 15, "context": "3 (PAC-Bayes [McA99; LS01]).", "startOffset": 13, "endOffset": 26}, {"referenceID": 14, "context": "We use the MNIST handwritten digits data set [LCB10] as provided in Tensorflow [Aba+15], where the dataset is split into the training set (55000 images) and test set (10000 images).", "startOffset": 45, "endOffset": 52}, {"referenceID": 3, "context": "To calculate the upper bound on the VC dimension of the network, we use an upper bound communicated to us by Bartlett [Bar17] which is itself in O(LW logW ), where L is the number of layers and W is the total number of tunable parameters.", "startOffset": 118, "endOffset": 125}, {"referenceID": 4, "context": "Finally, our algorithm also bears resemblance to graduated optimization, an approach toward non-convex optimization attributed to Blake and Zisserman [BZ87] whereby a sequence of increasingly fine-grained versions of an optimization problem are solved in succession.", "startOffset": 150, "endOffset": 156}], "year": 2017, "abstractText": "One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous in this \u201cdeep learning\u201d regime. In order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hiddenunit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.", "creator": "LaTeX with hyperref package"}}}