{"id": "1704.00103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly", "abstract": "We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat using several standard networks and datasets. We use our method to produce a system that can reliably detect whether an image is a picture of a real scene or not. Our system applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our system is robust to adversarial examples built from currently known attacking approaches.", "histories": [["v1", "Sat, 1 Apr 2017 02:12:40 GMT  (3783kb,D)", "http://arxiv.org/abs/1704.00103v1", null], ["v2", "Tue, 15 Aug 2017 05:59:38 GMT  (3740kb,D)", "http://arxiv.org/abs/1704.00103v2", "Accepted to ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jiajun lu", "theerasit issaranon", "david forsyth"], "accepted": false, "id": "1704.00103"}, "pdf": {"name": "1704.00103.pdf", "metadata": {"source": "CRF", "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly", "authors": ["Jiajun Lu", "Theerasit Issaranon", "David Forsyth"], "emails": ["daf}@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "2. SafetyNet: Spotting Adversarial Examples", "text": "SafetyNet consists of the original classifier and an enemy detector that looks at the internal state of the later layers in the original classifier, as shown in Figure 1. If the enemy detector declares that an example is contradictory, then the sample is rejected."}, {"heading": "2.1. Detecting Adversarial Examples", "text": "We force an attacker to solve a hard, discrete optimization problem. Our hypothesis 1 suggests that different code patterns occur for natural examples and contrary examples. We use an adversary detector that compares a code produced at test date with a collection of examples, which means that an attacker must get the network to produce code acceptable to the detector (which is difficult; Section 3). The adversary detector in SafetyNet uses an RBF-SVM on binary or ternary codes (activation patterns) to find opposite exam readings. We designate a code with c that is acceptable to the detector. The RBF-SVM classifies byf (c) = N for optimization, without the detector being very robust."}, {"heading": "2.2. Attacking Methods", "text": "We use the following standard and strong attacks [2], with different choice of hyper parameters generated to test the robustness of the systems. Each attack looks for a nearby one (x) that changes the class of the example and does not create visual artifacts. We use these methods to produce both type I attack (fool the classifier) and type II attack (fool the classifier and sneak past the detector). Fast Sign method: fellow Goodet al. [9] described this simple method. The applied disturbance is the direction in the image space that yields the highest increase in linearized costs below l \u00b2 standard. It uses a hyper parameter to regulate the distance between opposing and original image. Iterative methods: Kurakin et al al al al al al al al. [14] introduced an iteration version of the fast drawing method, applying it several times with a smaller step size."}, {"heading": "2.3. Type I Attacks Are Detected", "text": "Accuracy: Our SafetyNet can detect enemy samples with high accuracy on CIFAR-10 [12] and ImageNet1000 [4]. For classification networks, we have used a 32-layer ResNet [10] for CIFAR-10 and a VGG19 network [28] for ImageNet-1000. Figure 2 shows the detection accuracy of our binarized RBF-SVM detector on the x5 layer of ResNet for Cifar10 and on the fc7 layer of VG19 for ImageNet-1000. Adversarial samples are generated by Iterative-L2, Iterative-Linf, DeepFool-L2 and FastSign methods. Figure 2 compares our RBF-SVM detection results with the detector subnetwork results of attacks of [16]. The RoCfor our detector for our detector for Cifar-10 and ImageNet-1000 is tested on the adversary, although our result will be shown in Figure 3.3, our detector is activated on the adversary."}, {"heading": "2.4. Rejecting by Classification Confidence", "text": "Our experiments show that there is a trade-off between classification trust and detection ease for classifying dogs. Contrasting examples with high confidence in false classification labels tend to have more abnormal activation patterns, making them easier to detect by detectors. While opposing examples with low confidence in false classification labels are harder to detect, attacks like DeepFool add small and just enough disturbances to change the classification labels, making these counter-examples sometimes difficult to detect. However, these counter-examples could not assign high classification trust to the wrong label. If they perform more iterations and increase false classification trust, our detector could detect them much more easily. Experiments also show that type II attacks on our quantified SVM detector together with the classifier could produce adverse examples with low confidence. All these experiments mean that we can use classification trust as a detection criterion, and increase trust in it."}, {"heading": "2.5. Type II Attacks fail", "text": "An attack of type II involves a search for an enemy example that (a) is labeled incorrectly and (b) is not recognized. We perform the gradient pedigree attacks for Cifar10 and ImageNet-1000 with SVM detector and compare them with detection subnetworks [16]. In our experiments for Cifar-10 and ImageNet-1000 we only investigate type II attacks on the detection subnetwork. As the gradients of the detection subnetwork are better shaped, it should be easier to attack the detection and classification attack of type II. In our experiments for Cifar-10 and ImageNet-1000 we use different gradient pedigree attacks of type II (L0, L2, Fast, DeepFool and Top-5 DeepFool) to attack the detection and classification process simultaneously."}, {"heading": "3. SceneProof", "text": "It is indeed the case that we will be able to manoeuvre ourselves into a situation in which we are able, in which we are able to achieve our objectives."}, {"heading": "4. Theory: Bars and P-domains", "text": "We construct a possible explanation for adversarial examples that successfully explain (a) the phenomenology and (b) why SafetyNet works. We have a network of N levels of ReLUs, and study y (k) i (x), the values at the output of the k'th layer of ReLUs. This is a piecewise linear function of x. Such functions break down the input space into cells at the limits of which the piecewise linear function changes are made (i.e., it is only C0). Now we assume that for some y (k) i (x) p-domains (union of cells) D in the input space there are such that there are no or few examples in the p-domain; (b) the measurement of D under P (X) is small; (c) y (k) i (x) | is large within D and small outside D. We will always use the term \"p-domain\" to refer to domains with these properties."}, {"heading": "5. Discussion", "text": "We have shown that it is difficult to produce an example that (a) is mislabeled and (b) is not recognized as counterproductive by SafetyNet. We have outlined a possible reason why SafetyNet works and is hard to attack. A lot of interesting issues are raised by your work, and we offer many insights into the mechanism that neural networks work. SafetyNet: There could be a better architecture than our SafetyNet, whose objective function is more difficult to optimize. Ideally, an architecture that forces the attacker to solve a stubbornly discrete optimization problem that, of course, does not allow smoothing."}, {"heading": "6. Supporting Materials", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. SceneProof Dataset", "text": "Our SceneProof dataset is processed from NYU Depth v2 raw data, Sintel Synthetic RGBD datasets and Middlebury Stereo datasets; the dataset is in Part I and Part II. Part I contains NYU natural image and depth pairs, along with manipulated unnatural scenes (swap depth, insertion range, predicted depth, scale & shift depth), see Figure 6. It is used to train our classifier and work as test data Part I. Part II contains unnatural scenes manipulated by other methods (set swap depth to zero, sample and then scan both RGBD channels, aggressively compress the JPG RGBD images), and Picture & depth pairs of synthetic datasets and stereo datasets refer to Figure 7. Part II is used as test data to verify the generalization capability of our SceneProof network to unnatural detectors and to test our unnatural reactions."}, {"heading": "6.2. Type II Attacks on Cifar-10 and ImageNet-1000", "text": "In this section you will find the detailed percentages of type II attacks on Cifar-10 in Table 10, and the detailed percentages of type II attacks on ImageNet1000 in Table 11."}], "references": [{"title": "A naturalistic open source movie for optical flow evaluation", "author": ["D.J. Butler", "J. Wulff", "G.B. Stanley", "M.J. Black"], "venue": "A. Fitzgibbon et al. (Eds.), editor, European Conf. on Computer Vision (ECCV), Part IV, LNCS 7577, pages 611\u2013625. Springer-Verlag, Oct.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards evaluating the robustness of neural networks", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Advances in neural information processing systems, pages 2366\u20132374,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Exposing photo manipulation with inconsistent reflections", "author": ["H. Farid"], "venue": "ACM Trans. Graph., 31(1):4,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Photo forensics", "author": ["H. Farid"], "venue": "MIT Press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "How to detect faked photos", "author": ["H. Farid"], "venue": "American Scientist,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Exposing photo manipulation with inconsistent shadows", "author": ["E. Kee", "J.F. O\u2019Brien", "H. Farid"], "venue": "ACM Transactions on Graphics (ToG),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Dense associative memory is robust to adversarial inputs", "author": ["D. Krotov", "J.J. Hopfield"], "venue": "arXiv preprint arXiv:1701.00939,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Y. Liu", "X. Chen", "C. Liu", "D. Song"], "venue": "arXiv preprint arXiv:1611.02770,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "On detecting adversarial perturbations", "author": ["J.H. Metzen", "T. Genewein", "V. Fischer", "B. Bischoff"], "venue": "arXiv preprint arXiv:1702.04267,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["T. Miyato", "S.-i. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "arXiv preprint arXiv:1507.00677,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Universal adversarial perturbations", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "O. Fawzi", "P. Frossard"], "venue": "arXiv preprint arXiv:1610.08401,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574\u20132582,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["P.K. Nathan Silberman", "Derek Hoiem", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427\u2013436,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Security and Privacy (SP), 2016 IEEE Symposium on, pages 582\u2013597. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Transferability in machine learning: from phenomena to blackbox attacks using adversarial samples", "author": ["N. Papernot", "P.D. McDaniel", "I.J. Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "High-resolution stereo datasets with subpixel-accurate ground truth", "author": ["D. Scharstein", "H. Hirschm\u00fcller", "Y. Kitajima", "G. Krathwohl", "N. Ne\u0161i\u0107", "X. Wang", "P. Westling"], "venue": "German Conference on Pattern Recognition, pages 31\u201342. Springer,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "ICLR 2016,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 12, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 17, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 22, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 27, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 43, "endOffset": 54}, {"referenceID": 19, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 43, "endOffset": 54}, {"referenceID": 1, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 43, "endOffset": 54}, {"referenceID": 16, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "imagine a small physical modification that could reliably get a stop sign classified as a go faster sign [24]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "The absence of theory means it is hard to defend against adversarial examples (for example, distillation was proposed as a defense [25], but was later shown to not work [2]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 7, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 33, "endOffset": 36}, {"referenceID": 27, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 210, "endOffset": 214}, {"referenceID": 27, "context": "An important puzzle is that networks that generalize very well remain susceptible to adversarial examples [29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "Another important puzzle is that examples that are adversarial for one network tend to be adversarial for another as well [29, 15, 26].", "startOffset": 122, "endOffset": 134}, {"referenceID": 13, "context": "Another important puzzle is that examples that are adversarial for one network tend to be adversarial for another as well [29, 15, 26].", "startOffset": 122, "endOffset": 134}, {"referenceID": 24, "context": "Another important puzzle is that examples that are adversarial for one network tend to be adversarial for another as well [29, 15, 26].", "startOffset": 122, "endOffset": 134}, {"referenceID": 11, "context": "Some network architectures appear to be robust to adversarial examples [13], which still need more empirical verification.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "At least some adversarial attacks appear to apply to many distinct networks [18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "show how to construct examples that appear to be noise, but are confidently classified as objects [22].", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "However, most adversarial examples \u201clook like\u201d images to humans, such as figure 5 in [29], so they are likely to lie within the support of P (X).", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "One way to build a network that is robust to adversarial examples is to train networks with enhanced training data (adding adversarial samples [17]); this approach faces difficulties, because the dimension of the images and features in networks means an unreasonable quantity of training data is required.", "startOffset": 143, "endOffset": 147}, {"referenceID": 14, "context": "show that, by attaching a detection subnetwork that observes the state of the original classification network, one can tell whether it has been presented with an adversarial example or not [16].", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "However, because the gradients of their detection subnetwork are quite well behaved, the joint system can be attacked easily (in our experiments; [16] does not provide any analysis for attacking on both detectors and classifiers).", "startOffset": 146, "endOffset": 150}, {"referenceID": 14, "context": "[16]); (b) such detectors can be made very difficult to defeat (unlike Metzen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16]; section 3); (c).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16]); (d) transfer attacks work because an example that generates unfamiliar patterns in one network tends to generate unfamiliar patterns in other networks too; (e) transfer attacks could be defended as well (section 3).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This is in sharp contrast to all other known methods [24, 16].", "startOffset": 53, "endOffset": 61}, {"referenceID": 14, "context": "This is in sharp contrast to all other known methods [24, 16].", "startOffset": 53, "endOffset": 61}, {"referenceID": 7, "context": "Fast Sign method: Goodfellow et al [9] described this simple method.", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "[14] introduced an iteration version of the fast sign method, by applying it several times with a smaller step size \u03b1 and clipping all pixels after each iteration to ensure that results stay in the neighborhood of the original image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] introduced the DeepFool adversary, which is able to choose which class an example is switched to.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] described a way to attack a black-box network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Accuracy: Our SafetyNet can detect adversarial samples with high accuracy on CIFAR-10 [12] and ImageNet1000 [4].", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "Accuracy: Our SafetyNet can detect adversarial samples with high accuracy on CIFAR-10 [12] and ImageNet1000 [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "For classification networks, we used a 32-layer ResNet [10] for CIFAR-10 and a VGG19 network [28] for ImageNet-1000.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "For classification networks, we used a 32-layer ResNet [10] for CIFAR-10 and a VGG19 network [28] for ImageNet-1000.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "Figure 2 compares our RBF-SVM detection results with the detector subnetwork results of [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "To facilitate comparison, we follow the conventions of [16], plotting the success of the adversary (i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "A: Results for the detection subnetwork on CIFAR-10 from [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "C: Results for SafetyNet and the detection subnetwork (cnn) of [16] on CIFAR-10, where the detector was trained on L\u221e attack and tested on other attacking methods; SafetyNet generalizes better than detection subnetwork to different adversarial attacking methods.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "We cannot compare to the detection subnetwork of [16], because they do not provide results for ImageNet-1000.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "We perform the gradient descent based Type II attacks for Cifar10 and ImageNet-1000 with SVM detector, and compare to detection subnetwork [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "[16] only investigated type I attacks and has not investigated type II attacks on the detection subnetwork.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 149, "endOffset": 155}, {"referenceID": 5, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 149, "endOffset": 155}, {"referenceID": 18, "context": "We use the raw Kinect captures of LivingRoom and Bedroom from NYU v2 dataset [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "\u201cRegression\u201d methods used in both train and test are: random swaps of depth and image planes; single image predicted depth [5]; rectangle cropped region insertion and random shifted or scaled misaligned depth and image.", "startOffset": 123, "endOffset": 126}, {"referenceID": 25, "context": "\u201cRegression\u201d methods used only in test are: all zero depth values; nearest neighbor down-sample and up-sampled images and depths; low quality JPEG compressed images and depths; Middlebury stereo RGBD dataset [27] and Sintel RGBD dataset [1](which should be classified \u201cfake\u201d because they are renderings).", "startOffset": 208, "endOffset": 212}, {"referenceID": 0, "context": "\u201cRegression\u201d methods used only in test are: all zero depth values; nearest neighbor down-sample and up-sampled images and depths; low quality JPEG compressed images and depths; Middlebury stereo RGBD dataset [27] and Sintel RGBD dataset [1](which should be classified \u201cfake\u201d because they are renderings).", "startOffset": 237, "endOffset": 240}, {"referenceID": 22, "context": "In this case, attackers must probe with inputs and gather outputs, or build another approximate network as in [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "We follow [23, 18] by building attacks on various alternative networks, then transferring these network\u2019s adversarial samples.", "startOffset": 10, "endOffset": 18}, {"referenceID": 16, "context": "We follow [23, 18] by building attacks on various alternative networks, then transferring these network\u2019s adversarial samples.", "startOffset": 10, "endOffset": 18}, {"referenceID": 14, "context": "In contrast to SafetyNet, the detector subnetwork of [16] is generally susceptible to type II attacks in both blackbox and non-blackbox settings.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "8% Sintel RGBD [1] 27.", "startOffset": 15, "endOffset": 18}, {"referenceID": 25, "context": "4% Middlebury RGBD [27] 24.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "tion subnetwork\u2019s classification boundary problem [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "The table compares a VGG19 network (Ori) with the detection subnetwork of [16] (Subnet), and two variants of SafetyNet (Det A, where we have an RBF-SVM on fc7; and Det ABC, where we have an RBF-SVM on each of fc7, fc6 and pool5, and declare an adversary when any detector responds).", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "This is because of quantization process and our SafetyNet works like an example matcher, while detection subnetwork suffers from classification boundary problem [18].", "startOffset": 161, "endOffset": 165}, {"referenceID": 28, "context": "point out, the number of training examples available to a typical modern network is small compared to the relative capacity of deep networks [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "For example, excellent training error is obtainable for randomly chosen image labels [30].", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "demonstrate that, in practice, ReLU layers can have large norm as linear operators, despite weight decay (see [29], sec.", "startOffset": 110, "endOffset": 114}], "year": 2017, "abstractText": "We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat using several standard networks and datasets. We use our method to produce a system that can reliably detect whether an image is a picture of a real scene or not. Our system applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our system is robust to adversarial examples built from currently known attacking approaches.", "creator": "LaTeX with hyperref package"}}}