{"id": "1704.00158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Clustering-based Source-aware Assessment of True Robustness for Learning Models", "abstract": "We introduce a novel validation framework to measure the true robustness of learning models for real-world applications by creating source-inclusive and source-exclusive partitions in a dataset via clustering. We develop a robustness metric derived from source-aware lower and upper bounds of model accuracy even when data source labels are not readily available. We clearly demonstrate that even on a well-explored dataset like MNIST, challenging training scenarios can be constructed under the proposed assessment framework for two separate yet equally important applications: i) more rigorous learning model comparison and ii) dataset adequacy evaluation. In addition, our findings not only promise a more complete identification of trade-offs between model complexity, accuracy and robustness but can also help researchers optimize their efforts in data collection by identifying the less robust and more challenging class labels.", "histories": [["v1", "Sat, 1 Apr 2017 11:58:24 GMT  (3529kb,D)", "http://arxiv.org/abs/1704.00158v1", "Submitted to UAI 2017"]], "COMMENTS": "Submitted to UAI 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ozsel kilinc", "ismail uysal"], "accepted": false, "id": "1704.00158"}, "pdf": {"name": "1704.00158.pdf", "metadata": {"source": "CRF", "title": "Clustering-based Source-aware Assessment of True Robustness for Learning Models", "authors": ["Ozsel Kilinc"], "emails": [], "sections": [{"heading": null, "text": "We are introducing a novel validation system to measure the real robustness of real-world learning models for real-world applications by clustering source-based and source-exclusive partitions in a dataset. We are developing a robustness metric derived from source-aware lower and upper limits of model accuracy, even when data source designations are not readily available. We are clearly demonstrating that even on a well-researched dataset like MNIST, challenging training scenarios can be constructed within the proposed evaluation framework for two separate but equally important applications: i) stricter comparison of learning models and ii) evaluation of the adequacy of datasets. In addition, our results not only promise a more complete identification of target conflicts between model complexity, accuracy and robustness, but can also help researchers optimize their data collection efforts by identifying the less robust and demanding class designations."}, {"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 USING CLUSTERING TO FIND SOURCE DISTRIBUTIONS", "text": "This year, the time has come for us to be able to try to find a solution that we are able to find, that we are able to find a solution."}, {"heading": "3 APPLYING SOURCE-AWARE PARTITIONING", "text": "With source-conscious partitioning, assuming that each of the 5 clusters obtained via k-means or ACOL corresponds to a different source that generates data with a different distribution, training and test subgroups are constructed on the basis of this information, rather than completely randomized sampling. Specifically, with exclusive source-conscious partitioning, one of the 5 clusters is randomly selected for each of the 10-digit classes and all of its samples are added to the test set, while the samples of the remaining 4 clusters are added to the training set, effectively creating a 20% to 80% split. This partitioning ensures that the samples of the selected test cluster for each class are never inserted into the algorithm during training. Random selection of clusters added to the test set is performed independently for each class. As the correlations between clusters of different classes are not known, i.e. we do not know that samples in which clusters of digital-1 are generated by the same source are not generated by repeating samples in the first cluster of the full 5."}, {"heading": "4 PROPOSED ROBUSTNESS METRICS AND FRAMEWORK", "text": "We maintain that the accuracy obtained by means of exclusive source-conscious partitioning, AccX, is a better estimate of the expected accuracy of the trained algorithm when it encounters improbable examples than those we have used for training, i.e. a worst-case scenario, and in the same way the accuracy achieved by means of inclusive source-conscious partitioning, AccI, is a better estimate for the expected accuracy in the best-case scenario. Therefore, we propose to use an interval instead of a single value to define the expected real performance of the learning algorithms, so that: Accuracy interval = I: = [Accuracy Interval, AccI] (1) Since we observe the natural variance of the obtained accuracy with respect to the sampled training and test sets for the 100 repeated attempts, it is more accurate to define this interval as an expected accuracy interval [Accurance, Accurance, Accurance, E = Accurance, that I expect [Accurance, E = E] Accuracy, Accurance, E = E, Accuracy, E = E, Accuracy, Accurance, E = E, Accurance, Accurance, E = E, Accurance, E = E, Accurance, E = E, Accurance, E = E, Accurance [Accurance, E = E] Accurance, E = E = E, Accurance, Accurance, E = E = E = E, Accurance [Accurance, E = E] Accurance, E = E = E = E = E, Accurance, Accurance, E = E = E = E = E, Accurance, E = E = E = E = E, Accurance, E = E = E = E = E, Accurance [Accurance, E = E = E = E = E = E = E = E, Accurance, Accurance, E = E = E = E = E = E = E = E = E, Accurance, Accurance [Accurance, E = E = E = E = E = E = E, Accurance, E = E = E = E = E = E = E = E = E = E = E,"}, {"heading": "5 RESULTS", "text": "In this section we will present detailed analyses of the proposed partitioning algorithms for two separate applications: i) measuring the robustness of learning models and ii) measuring the robustness of data sets."}, {"heading": "5.1 MODEL ROBUSTNESS", "text": "We compare the following learning models. \u2022 SVM: - C = 1, kernel = rbf, gamma = 0.01 \u2022 MLP: - Feedforward 2048 - 50% Dropout - maxnorm (2) - Fedforward 2048 - 50% Dropout - maxnorm (2) - Feedforward 2048 - 50% Dropout - maxnorm (2) - CNN-1: - 32x3x3 - 64x3 - 64x3 - MP2x3 - MP2x2 - 25% Dropout - Feedforward 2048 - 50% Dropout \u2022 CNN-2: - 32x3x3 - 32x3 - 3x3 - MP2x3."}, {"heading": "5.2 DATASET ADEQUACY", "text": "Using source-aware partitioning, it is also possible to analyze whether the number of examples in the training dataset is sufficient to obtain a good generalization of the unlikely examples. This kind of analysis can be useful to help the researcher determine the need to collect more data. To observe the effect of the number of training samples, we reduced the size of the training set while keeping the test set the same as in the experiments with the full training set. For these experiments, we fixed our learning models on CNN and applied the partitioning using the ACOL cluster scheme. To interpret this behavior in terms of model capacity, Figure 7 and Figure 8 give field diagrams of the test accuracy obtained in these experiments. It can be observed that in both AccI and AccX, the expected values increase and decrease with increasing training set size. To interpret this behavior in terms of model capacity, Figure 7 and Figure 8 illustrate the expected accuracy interval of CNN - all three are observed."}, {"heading": "5.2.1 Class Label Specific Dataset Adequacy", "text": "More specifically, one of the classes in our dataset could have more diversity among its samples and form clusters with a wider range of distributions. In such a case, we may need to use more examples of this class to get a good generalization, while other classes are less varied and existing samples are sufficient for the algorithm to perform well even in unlikely test samples. Analyzing the class-specific results of the robustness of the data sets could help the researcher optimize efforts for additional data collection by identifying the less robust classes. Figure 9 and Figure 10 illustrate the class-based expected accuracy and robustness observed on CNN-3 model for three sample classes of MNIST - digit-0, digit-1 and digit-5 - show markedly different characteristics in terms of the increasing number of training samples."}, {"heading": "6 CONCLUSION", "text": "The expected generalization error, which is traditionally calculated using a test set constructed by random subsampling from the entire data set, can be a quick measurement to compare different learning algorithms. However, it is not enough to make a realistic assessment of performance under real conditions in which unlikely input is possible compared to those within the data set. To simulate such conditions, we can partition the data sets taking into account the mutually exclusive distribution of the sources, i.e. data generation processes that exist in the data set. If data sources are not readily identifiable, we can approach this type of information by applying cluster algorithms to the samples of each class so that each cluster corresponds to a different distribution or source of its class. In this paper, we have applied two source-aware partition plans to simulate two extreme cases on MNIST."}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["Arthur", "Vassilvitskii", "D. 2007] Arthur", "S. Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2007}, {"title": "No unbiased estimator of the variance of k-fold cross-validation", "author": ["Bengio", "Grandvalet", "Y. 2004] Bengio", "Y. Grandvalet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2004}, {"title": "Heuristics of instability and stabilization in model selection", "author": ["Breiman et al", "L 1996] Breiman"], "venue": "The annals of statistics,", "citeRegEx": "al. and Breiman,? \\Q1996\\E", "shortCiteRegEx": "al. and Breiman", "year": 1996}, {"title": "Source-aware partitioning for robust crossvalidation", "author": ["Kilinc", "Uysal", "O. 2015] Kilinc", "I. Uysal"], "venue": "In 14th IEEE International Conference on Machine Learning and Applications,", "citeRegEx": "Kilinc et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kilinc et al\\.", "year": 2015}, {"title": "Deep clustering using auto-clustering output layer. CoRR, abs/1702.08648", "author": ["Kilinc", "Uysal", "O. 2017] Kilinc", "I. Uysal"], "venue": null, "citeRegEx": "Kilinc et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kilinc et al\\.", "year": 2017}, {"title": "The mnist database of handwritten digits", "author": ["LeCun et al", "Y. 1998] LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "We introduce a novel validation framework to measure the true robustness of learning models for real-world applications by creating sourceinclusive and source-exclusive partitions in a dataset via clustering. We develop a robustness metric derived from source-aware lower and upper bounds of model accuracy even when data source labels are not readily available. We clearly demonstrate that even on a well-explored dataset like MNIST, challenging training scenarios can be constructed under the proposed assessment framework for two separate yet equally important applications: i) more rigorous learning model comparison and ii) dataset adequacy evaluation. In addition, our findings not only promise a more complete identification of trade-offs between model complexity, accuracy and robustness but can also help researchers optimize their efforts in data collection by identifying the less robust and more challenging class labels.", "creator": "LaTeX with hyperref package"}}}