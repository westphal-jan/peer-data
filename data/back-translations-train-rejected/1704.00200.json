{"id": "1704.00200", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Multimodal Dialogs (MMD): A large-scale dataset for studying multimodal domain-aware conversations", "abstract": "Owing to the success of deep learning techniques for tasks such as Q/A and text-based dialog, there is an increasing demand for AI agents in several domains such as retail, travel, entertainment, etc. that can carry on multimodal conversations with humans employing both text and images within a dialog seamlessly. However, deep learning research is this area has been limited primarily due to the lack of availability of large-scale, open conversation datasets. To overcome this bottleneck, in this paper we introduce the task of multi-modal, domain-aware conversations, and propose the MMD benchmark dataset to- wards this task. This dataset was gathered by working in close coordination with large number of domain experts in the retail domain and consists of over 150K conversation sessions between shoppers and sales agents. With this dataset, we propose 5 new sub-tasks for multimodal conversations along with their evaluation methodology. We also propose two novel multi-modal deep learning models in the encode- attend-decode paradigm and demonstrate their performance on two of the sub-tasks, namely text response generation and best image response selection. These experiments serve to establish baseline performance numbers and open new directions of research for each of these sub-tasks.", "histories": [["v1", "Sat, 1 Apr 2017 17:05:35 GMT  (9353kb,D)", "http://arxiv.org/abs/1704.00200v1", null], ["v2", "Tue, 9 May 2017 07:50:08 GMT  (9353kb,D)", "http://arxiv.org/abs/1704.00200v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amrita saha", "mitesh khapra", "karthik sankaranarayanan"], "accepted": false, "id": "1704.00200"}, "pdf": {"name": "1704.00200.pdf", "metadata": {"source": "CRF", "title": "Multimodal Dialogs (MMD): A large-scale dataset for studying multimodal domain-aware conversations", "authors": ["Amrita Saha", "Mitesh Khapra", "Karthik Sankaranarayanan"], "emails": ["amrsaha4@in.ibm.com", "miteshk@cse.iitm.ac.in", "kartsank@in.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to survive themselves are able to survive themselves, \"he said.\" But it's not that they are able to survive themselves. \"He added,\" It's not that they are able to survive themselves. \"But he also stressed,\" It's not that they are able to survive themselves, but that they are able to survive themselves. \"He emphasized,\" It's not that they are able to survive themselves, as if they were able to survive themselves. \""}, {"heading": "2 The Multimodal Dialogs (MMD) Dataset", "text": "As mentioned in the previous section, a central contribution of this paper is an extensive dataset of two-party dialogs that seamlessly utilizes multimodal data in their statements and contexts, and also demonstrates domain-specific knowledge in their interactions. To this end, in this section, we first describe the methodology used to collect this dataset, and then explain in detail the various subtasks that the dataset has and which raise new research problems."}, {"heading": "2.1 Data Collection Methodology", "text": "The data collection process was carried out by us in close consultation with a team of 20 fashion experts and consisted of two main steps: (i) curating and presenting extensive domain knowledge and (ii) developing a large collection of multimodal conversations, each consisting of a series of interactions that utilize this knowledge."}, {"heading": "2.1.1 Domain Knowledge Curation", "text": "Through our series of interviews with the domain experts, we observed that much of the complexity in a natural conversation in this area stems from the background knowledge that both the expert and the buyer use in their conversation. The domain knowledge of the expert is diverse in nature, varying from knowing which clothes go well with which accessory, to which celebrity currently endorses which type of fashion item, or which type of look is better suited for which occasion. Therefore, the first step in our data collection process was to scale up this domain knowledge of unstructured multimodal content on the Web and present it in a machine-consumable manner. This process included a number of steps as listed below: 1. Crawling over 1 million fashion items from the Web along with their available semi / unstructured information and associated images (s) 2. Parsing different types of domain knowledge from the free text information, and curating them."}, {"heading": "2.1.2 Gathering multimodal dialogs", "text": "During the interviews, the domain experts described in detail different phases of the sales process. For example, a dialogue between the seller and a buyer visiting an e-commerce site would begin with the goal of buying either one or more fashion items by the buyer providing the seller with his purchasing requirements, then the seller searches the corpus and returns with a multimodal response (i.e. a series of images that meet the buyer's limitations and / or a related text). Now, with this response, the buyer gives feedback or modifies his requirements. Through this iterative response and feedback loop, the buyer continues to examine his items of interest by including selected items in his basket. The session continues until he decides not to leave without making a purchase, or culminates with the buyer of one or more items. Note that in different steps during such a conversation, the reaction to the current step of the dialogue is based on a variety of images."}, {"heading": "2.1.3 Qualitative Survey", "text": "In order to ensure that the data set is representative and not distorted by the specific fashion experts interviewed, we conducted a survey of the data set, in which 16 different fashion experts participated, asking them to rate both whether the parts of the dialog are natural sounding and meaningful, and whether the images in the dialog are appropriate; the survey was conducted with a randomly selected set of 760 dialogue sessions, and the experts were asked to give an overall rating between 1 and 5 (of which 5 is the most realistic); two types of errors were documented: (i) minor errors are conversation errors (e.g. grammatical and spelling errors); (ii) severe errors are logical errors (e.g. deductions of errors in generating the image or text response; incorrect understanding of the buyer's question; incorrect fashion recommendations, etc.) As the survey results in Table 5 show, the average rating was 4 out of 5, implying an average of 40 in the dialog, implying only a few errors in an average conversation."}, {"heading": "2.1.4 Discussion", "text": "Note that the first version of the dataset to be published will not include the background knowledge gained from this area, as additional standardization is required, but will be incorporated into a later release. Also, note that to evaluate the image response in Selection / Ranking mode (as described in the next section), a system would also require negative training examples. To support this, we also create negative automatons that simulate a dialogue between a buyer and an agent that always returns wrong image reactions in a dialog context. These false examples in addition to the correct image examples can be used for training models for such rating settings."}, {"heading": "2.2 Tasks", "text": "The proposed MMD data sets consist of multimodal, domain-aware conversations between 2 agents and can therefore be used to evaluate a variety of tasks. We describe each of these tasks and explain the associated technical challenges: 1. Text Response: Given the context of k, the task here is about generating the next text response. 2. Picture Response: Given the context of k, the task is about producing the most relevant image (s). There are 2 typical approaches to achieving this: 2.1 Picture Retrieval: Given the context of images, retrieve and evaluate n images based on their relevance to the given context. 2. Picture Generation: Given a context of k spins, generating that is usually done using generative models (Reed et al.), 2016; Goodfellow et al al."}, {"heading": "3 Models", "text": "To evaluate empirically the feasibility of the two tasks described above, we implement a basic method (and some variations thereof) for each task. These methods are based on the popular hierarchical paradigm of \"encode-participate-decode\" (Serban et al., 2016a), which is typically used for (text) talk systems. We divide the description below into two parts: (i) multimodal encoder, which is common for the two tasks (ii) multimodal decoder, which differs depending on whether we need to generate a text reaction or predict an image reaction."}, {"heading": "3.1 Mutimodal encoder", "text": "As already mentioned, the context for both tasks k contains utterances. Any utterance in the context could be either (i) a pure utterance of the text or (ii) a pure utterance of the image or (iii) a multimodal utterance containing both text and images (as shown in Figure 2) We use a multimodal hierarchical encoder to calculate the representation of the encoder in each of these cases as described below. (a) Text only utterance: Any utterance of the text in the context is made by means of a recursive neural network with GRU (Chung et al., 2014) cells in a process similar to the encoder of the layer as described in (Serban et al., 2016a) This is the level 1 encoder in the hierarchical encoder. (b) Image only utterance: In the current setup, each image or multimodal utterance of the level is the modality encoder."}, {"heading": "3.2 Decoder for generating text responses", "text": "As shown in Fig. 2, we use a standard decoder based on recursive neural networks with GRU cells. Such a decoder has been successfully used for various tasks of generating natural language, including text conversation systems (Serban et al., 2016b). We also implemented a version in which we pair the decoder with an attention model. The attention model learns to pay attention to different time steps of the second-level encoder (this has also been successfully tested in the context of text conversation systems (Yao et al., 2016))."}, {"heading": "3.3 Layer for ranking image responses:", "text": "During the training, we get a set of m images for each context, of which only npos max is relevant for the context. The remaining m \u2212 npos max are selected from the corresponding wrong image reactions in the dataset. We train the model based on a maximum margin loss. Specifically, we calculate the cosinal similarity between the learned image embedding and the encoded representation of the multimodal context. The model is then trained to maximize the margin between the cosinal similarity for the correct image and the wrong images. Fig. 3 shows this if m = 2 and npos max = 1.Note that due to space constraints, we only provide pictorial representations of these models."}, {"heading": "4 Experiments", "text": "In this section we describe the experimental setup to evaluate the following models for the two tasks: \u2022 Hierarchical encoder decoder (Ignore the image context), whose architecture is similar to that proposed in (Serban et al., 2016a) \u2022 The proposed multimodal hierarchical en-coder decoder, where we vary the size of the dialog context. \u2022 The proposed multimodal hierarchical en-coder decoder, taking into account the multimodal expression representation at each step."}, {"heading": "4.1 Evaluating the Text Response Task", "text": "The size of the training, validation and test sets is given in the 6th line of Table 3. We set the following hyperparameters using the validation set: Learning rate \u0442 {1e-2, 1e-3, 4e-4}, RNN hidden unit size and {256, 512}, text embedding size \u0442 {256, 512}, image embedding size \u0442 {256, 512}, batch size \u0442 {32, 64}. The numbers in brackets indicate the different values of each hyperparameter we consider. As an optimization algorithm we used Adam (Kingma and Ba, 2014). The results in Table 6 summarize the BLEU- and NIST-values used as a yardstick for this task."}, {"heading": "4.2 Evaluating Image Response Task", "text": "The size of the training, validation and test kits is specified in line 5 of Table 3. During both training and testing, the model is provided with m = 5 target images, of which only npos max = 1 is relevant, and at test time, the model must rate the images in order of relevance in response to the given context. Hyperparameters of the model have been adjusted in the same way as mentioned above. We use Recall @ top-m as evaluation metrics, where top-m varies from 1 to 3 and the model prediction is considered correct only if the true answer is one of the top-m entries in the ranking. Results are summarized in Table 7. In the supplementary material, we provide examples of the model's results for the two tasks."}, {"heading": "4.3 Discussions", "text": "We make a few observations based on the results. \u2022 For both tasks, the basic model, in which no image information is used, requires roughly the same performance as the models that use this information.This suggests that we need better models to capture the interactions between text and images. Specifically, for the two modalities, we need to learn to present together. \u2022 While BLEU attaches more importance to formulation in calculating similarity, NIST attaches more importance to informative words. The low NIST values suggest that the models need a certain amount of external knowledge to understand the informative words in the domain. \u2022 Additional attention reduces performance. Although this is counterproductive, it suggests that we need better models for multimodal attention. \u2022 Overall, we feel that there is a lot of scope for improvement and that current models only determine the feasibility of the two tasks."}, {"heading": "5 Conclusion", "text": "In this paper, we presented the Multimodal Dialogue (MMD) dataset, which is well suited for studying multimodal domain-aware conversations in a variety of environments, compiled in close collaboration with a group of 20 retail experts and includes over 0.15 million conversations between buyers and sellers that include text and images. Powered by the MMD dataset, we proposed five new subtasks along with their assessment methodologies. We also proposed two multimodal deep learning models that use the \"encode-attend-decode\" paradigm and demonstrated their performance in both generating text responses and selecting the best image responses. The performance indicators achieved demonstrate the feasibility of the subtasks involved and highlight the challenges involved."}, {"heading": "6 Supplementary Materials", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Examples of Text Responses generated by the proposed Multimodal HRED model", "text": "This section lists some of the examples taken from the test set of the model that generates the text reactions of the agent. In each illustration, the last line (red) is the predicted output of the system and directly above it the true response from the original dialog in the dataset. The multimodal content shown above is the context that is fed into the model to generate this response."}, {"heading": "6.2 Examples of Target Images Ranked by the proposed Multimodal HRED model", "text": "Similarly, here we list some of the sample output for the Image Response Selection task taken from the test set; the model is given a multimodal context of a running dialog and a set of target images, which it must rate as responses in order of relevance, given the context; the target images are displayed for each example in the bottom line, and the true positive and true negative responses (according to the dataset) are delimited by a green or red box; the model evaluates each of these images internally based on the likelihood that the image is a reaction, depending on the context; in the images, the target images are sorted according to the cosmic similarity value given by the model, with a higher value of similarity implying a higher confidence that the model in this image represents a relevant response."}, {"heading": "6.3 Examples of dialog sessions from MMD dataset", "text": "In this section, we show some of the sample dialog sessions between a buyer and an agent, where each dialog culminates either in a successful purchase or where the buyer exits without a purchase."}], "references": [{"title": "VQA: visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago,", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Visual dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra."], "venue": "CoRR abs/1611.08669. http://arxiv.org/abs/1611.08669.", "citeRegEx": "Das et al\\.,? 2016", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Guesswhat?! visual object discovery through multi-modal dialogue", "author": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron C. Courville."], "venue": "CoRR abs/1611.08481. http://arxiv.org/abs/1611.08481.", "citeRegEx": "Vries et al\\.,? 2016", "shortCiteRegEx": "Vries et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "Proceedings of the SIGDIAL 2015 Conference, The", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering", "author": ["Tegan Maharaj", "Nicolas Ballas", "Aaron C. Courville", "Christopher Joseph Pal."], "venue": "CoRR abs/1611.07810. http://arxiv.org/abs/1611.07810.", "citeRegEx": "Maharaj et al\\.,? 2016", "shortCiteRegEx": "Maharaj et al\\.", "year": 2016}, {"title": "Imagegrounded conversations: Multimodal context for natural question and response generation", "author": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P. Spithourakis", "Lucy Vanderwende."], "venue": "CoRR", "citeRegEx": "Mostafazadeh et al\\.,? 2017", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2017}, {"title": "Generative adversarial text-to-image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee."], "venue": "Proceedings of The 33rd International Conference on Machine Learning.", "citeRegEx": "Reed et al\\.,? 2016", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan."], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 2-4,", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intel-", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio."], "venue": "CoRR abs/1605.06069.", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "CoRR abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR abs/1506.05869. http://arxiv.org/abs/1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston,", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "An attentional neural conversation model with improved specificity", "author": ["Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Kam-Fai Wong."], "venue": "CoRR abs/1606.01292. http://arxiv.org/abs/1606.01292.", "citeRegEx": "Yao et al\\.,? 2016", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Leveraging video descriptions to learn video question answering", "author": ["Kuo-Hao Zeng", "Tseng-Hung Chen", "Ching-Yao Chuang", "Yuan-Hong Liao", "Juan Carlos Niebles", "Min Sun."], "venue": "CoRR abs/1611.04021. http://arxiv.org/abs/1611.04021.", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "The recent progress with deep learning techniques for other problems at the intersection of NLP and Computer Vision such as image captioning (Vinyals et al., 2015; Xu et al., 2015), video description (Yu et al.", "startOffset": 141, "endOffset": 180}, {"referenceID": 16, "context": "The recent progress with deep learning techniques for other problems at the intersection of NLP and Computer Vision such as image captioning (Vinyals et al., 2015; Xu et al., 2015), video description (Yu et al.", "startOffset": 141, "endOffset": 180}, {"referenceID": 18, "context": ", 2015), video description (Yu et al., 2016), image question an-", "startOffset": 27, "endOffset": 44}, {"referenceID": 0, "context": "swering (Antol et al., 2015), video question answering (Zeng et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 19, "context": ", 2015), video question answering (Zeng et al., 2016; Maharaj et al., 2016), is owed in large part to the availability of large-scale open datasets for their respective tasks.", "startOffset": 34, "endOffset": 75}, {"referenceID": 7, "context": ", 2015), video question answering (Zeng et al., 2016; Maharaj et al., 2016), is owed in large part to the availability of large-scale open datasets for their respective tasks.", "startOffset": 34, "endOffset": 75}, {"referenceID": 12, "context": "Though there has been recent work(Serban et al., 2016b; Yao et al., 2016; Serban et al., 2016a) with different", "startOffset": 33, "endOffset": 95}, {"referenceID": 17, "context": "Though there has been recent work(Serban et al., 2016b; Yao et al., 2016; Serban et al., 2016a) with different", "startOffset": 33, "endOffset": 95}, {"referenceID": 11, "context": "Though there has been recent work(Serban et al., 2016b; Yao et al., 2016; Serban et al., 2016a) with different", "startOffset": 33, "endOffset": 95}, {"referenceID": 6, "context": "conversation datasets (Lowe et al., 2015; Vinyals and Le, 2015; Ritter et al., 2010), the mode of interaction there is limited to text conversations only, rendering them inadequate for multimodal conversation research.", "startOffset": 22, "endOffset": 84}, {"referenceID": 14, "context": "conversation datasets (Lowe et al., 2015; Vinyals and Le, 2015; Ritter et al., 2010), the mode of interaction there is limited to text conversations only, rendering them inadequate for multimodal conversation research.", "startOffset": 22, "endOffset": 84}, {"referenceID": 10, "context": "conversation datasets (Lowe et al., 2015; Vinyals and Le, 2015; Ritter et al., 2010), the mode of interaction there is limited to text conversations only, rendering them inadequate for multimodal conversation research.", "startOffset": 22, "endOffset": 84}, {"referenceID": 0, "context": "The body of work most relevant to ours is Visual QA(Antol et al., 2015) involving a single question and response, and the work of (de Vries et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 9, "context": "contextual GANs(Reed et al., 2016; Goodfellow et al., 2014)).", "startOffset": 15, "endOffset": 59}, {"referenceID": 4, "context": "contextual GANs(Reed et al., 2016; Goodfellow et al., 2014)).", "startOffset": 15, "endOffset": 59}, {"referenceID": 6, "context": "This simplified evaluation protocol of \u201cselecting/ranking\u201d the best response was proposed for the Ubuntu Dialog Corpus (Lowe et al., 2015) and helps provide more control on the experimental setup and evaluate individual parts of", "startOffset": 119, "endOffset": 138}, {"referenceID": 11, "context": "These methods are based on the popular hierarchical encode-attend-decode paradigm (Serban et al., 2016a) typically used for (text) conversation sys-", "startOffset": 82, "endOffset": 104}, {"referenceID": 1, "context": "(a) Text only utterance: Every text utterance in the context is encoded using a recurrent neural network with GRU (Chung et al., 2014) cells in", "startOffset": 114, "endOffset": 134}, {"referenceID": 11, "context": "a process similar to the utterance level encoder described in (Serban et al., 2016a).", "startOffset": 62, "endOffset": 84}, {"referenceID": 13, "context": "We encode each image using a 4096 dimensional representation obtained from the FC7 layer of a VGGNet-16 (Simonyan and Zisserman, 2014) convolutional neural network.", "startOffset": 104, "endOffset": 134}, {"referenceID": 12, "context": "Such a decoder has been used successfully for various natural language generation tasks including text conversation systems (Serban et al., 2016b).", "startOffset": 124, "endOffset": 146}, {"referenceID": 17, "context": "tion systems (Yao et al., 2016)).", "startOffset": 13, "endOffset": 31}, {"referenceID": 11, "context": "In this section we describe the experimental setup used to evaluate the following models on the two tasks: \u2022 Hierarchical Encoder Decoder (ignoring image context), whose architecture is similar to that proposed in (Serban et al., 2016a) \u2022 The proposed Multimodal Hierarchical Encoder Decoder, where we varied the size of the", "startOffset": 214, "endOffset": 236}, {"referenceID": 5, "context": "We used Adam (Kingma and Ba, 2014) as the optimization algorithm.", "startOffset": 13, "endOffset": 34}], "year": 2017, "abstractText": "Owing to the success of deep learning techniques for tasks such as Q/A and text-based dialog, there is an increasing demand for AI agents in several domains such as retail, travel, entertainment, etc. that can carry on multimodal conversations with humans employing both text and images within a dialog seamlessly. However, deep learning research is this area has been limited primarily due to the lack of availability of large-scale, open conversation datasets. To overcome this bottleneck, in this paper we introduce the task of multimodal, domain-aware conversations, and propose the MMD benchmark dataset towards this task. This dataset was gathered by working in close coordination with large number of domain experts in the retail domain and consists of over 150K conversation sessions between shoppers and sales agents. With this dataset, we propose 5 new sub-tasks for multimodal conversations along with their evaluation methodology. We also propose two novel multimodal deep learning models in the encodeattend-decode paradigm and demonstrate their performance on two of the sub-tasks, namely text response generation and best image response selection. These experiments serve to establish baseline performance numbers and open new directions of research for each of these sub-tasks.", "creator": "LaTeX with hyperref package"}}}