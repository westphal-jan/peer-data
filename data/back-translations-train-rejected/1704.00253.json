{"id": "1704.00253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2017", "title": "Building a Neural Machine Translation System Using Only Synthetic Parallel Data", "abstract": "Recent works have proved that synthetic parallel data generated by existing translation models can be an effective solution to various neural machine translation (NMT) issues. In this study, we construct NMT systems using only synthetic parallel data. As an effective alternative to real parallel data, we also present a new type of synthetic parallel corpus. The proposed pseudo parallel data are distinct from previous approaches in that real and synthetic sentences are mixed on both sides of sentence pairs. Experiments on Czech-German and French-German translations demonstrate the efficacy of the proposed pseudo parallel corpus that guarantees not only both balanced and competitive performance for bidirectional translation but also substantial improvement with the aid of a real parallel corpus.", "histories": [["v1", "Sun, 2 Apr 2017 05:54:14 GMT  (236kb,D)", "https://arxiv.org/abs/1704.00253v1", "10 pages, 2 figures, 3 tables"], ["v2", "Mon, 17 Apr 2017 02:58:02 GMT  (231kb,D)", "http://arxiv.org/abs/1704.00253v2", "added small-scale experiments"], ["v3", "Mon, 15 May 2017 09:23:21 GMT  (232kb,D)", "http://arxiv.org/abs/1704.00253v3", "added small-scale experiments and reorganized experimental results"], ["v4", "Sun, 17 Sep 2017 01:58:39 GMT  (269kb,D)", "http://arxiv.org/abs/1704.00253v4", null]], "COMMENTS": "10 pages, 2 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jaehong park", "jongyoon song", "sungroh yoon"], "accepted": false, "id": "1704.00253"}, "pdf": {"name": "1704.00253.pdf", "metadata": {"source": "CRF", "title": "Building a Neural Machine Translation System Using Only Synthetic Parallel Data", "authors": ["Jaehong Park", "Jongyoon Song", "Sungroh Yoon"], "emails": ["sryoon}@snu.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Neural Machine Translation", "text": "Considering a source set x = (x1,.., xm) and its corresponding target set y = (y1,.., yn), the NMT aims to model the conditional probability p (y | x) with a single large neural network. To parameterise the conditional distribution, current studies on NMT use the encoder decoder architecture (Kalchburner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014), after which the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) was introduced and successfully addressed the quality deterioration of NMT when it comes to long input sets (Cho et al., 2014a). In this study, we use the attention-oriented NMT architecture developed by Bahdanau et al. (2014)."}, {"heading": "3 Related Work", "text": "In statistical machine translation (SMT), primarily synthetic bilingual data has been proposed as a means of exploiting monolingual corpora (Ueffing et al., 2007; Wu et al., 2008). Similarly, but inversely, the target monolingual corpora has also been used to build synthetic parallel data (Bertoldi and Federico, 2009; Lambert et al., 2011). The primary objective of this work was to adapt trained SMT models to other domains using relatively abundant monolingual data in the domain. Inspired by the successful application in SMT, efforts have been made to use synthetic parallel data to improve NMT systems (Zhang and Zong, 2016b), penning-sided corpora (Sennrich et al., 2015a) and both sides (Cheng al, 201b) have used synthetic parallel data to improve NMT systems."}, {"heading": "4 Synthetic Parallel Data as an Alternative to Real Parallel Corpus", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Motivation", "text": "As described in the previous section, synthetic parallel data is widely used to enhance the performance of the NMT. In this work, we extend its application by training the NMT exclusively with synthetic data. In certain language pairs or domains where the source-to-target parallel corpora is very rare or even unprepared, the synthetic parallel data-trained model can act as an effective base model. Once the additional parallel corpus for the basic truth is established, the trained model can be improved by retraining or fine-tuning using the real parallel data."}, {"heading": "4.2 Limits of the Previous Approaches", "text": "For a given translation task, we classify the existing pseudo-parallel data into the following groups: (a) the source sentences come from a real corpus and the associated target sentences are synthetic; the corpus can be formed by automatically translating a source-side monolingual corpus into the target language (Zhang and Zong, 2016a, b); or it can be formed from the source-related bilingual data by introducing a pioneer language; in this case, a pioneer translation model is used to translate the pioneer language corpus into the target language; the generated target sentences are paired with the original source sentences; (b) the target sentences are formed from a real corpus consisting of a real corpus."}, {"heading": "4.3 Proposed Mixing Approach", "text": "In order to overcome the limitations of the previously proposed pseudo-parallel data, we propose a new type of synthetic parallel data called PSEUDOmix. Our approach is quite simple: for a given translation task, we first build source and target-related pseudo-parallel data. PSEUDOmix can then be easily produced by mixing it. The overall process of building PSEUDOmix for the French \u2192 German translation task is illustrated in Figure 1. By mixing source and target-related pseudo-parallel data, the resulting corpus includes both real and synthetic examples on both sides of sentence pairs, which is the most obvious feature of PSEUDOmix. By mixing approach, we try to reduce the general discrepancy in the quality of source and target examples of synthetic sentence pairs, thereby increasing reliability as a parallel resource. In the following section, we evaluate the actual benefit of the synthetic composition."}, {"heading": "5 Experiments: Effects of Mixing Real and Synthetic Sentences", "text": "In this section, we analyze the effects of mixed composition in synthetic parallel data. However, mixing pseudo-parallel corpora from different sources inevitably leads to diversity that affects the capacity of the resulting corpus. We isolate this factor by building both source and target-based synthetic corpora from the identical source-target-real parallel corpus. Our experiments are carried out on French (Fr) \u2194 German (De) translation tasks. In the rest of the work, we use notation * to denote the synthetic part of the pseudo-sentence pairs."}, {"heading": "5.1 Data Preparation", "text": "By selecting English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpus (Koehn, 2005), constructing a multi-parallel corpus of Fr-En-De. Subsequently, each of the Fr * De and Fr-De * pseudo-parallel corpus is created from the multi-parallel data using the pivot-language-based translation described in the previous section. For automatic translation, we use a pre-trained and publicly released NMT Model 1 for En \u2192 De, and finally train another NMT model for En \u2192 Fr using the WMT '15 En-Fr parallel corpus (Bojar et al., 2015). A size 5 beam is used to generate synthetic sentences, and finally to match the size of training data, De-DOX and De-X with each other *."}, {"heading": "5.2 Data Preprocessing", "text": "We present each sentence as a sequence of subword units that we have learned from byte-pair encoding (Sennrich et al., 2015b). We remove blank lines and all sentences of more than 50 subword units. To make a fair comparison, all cleaned synthetic parallel data are of the same size. The summary of the last parallel corpora is shown in Table 1."}, {"heading": "5.3 Training and Evaluation", "text": "All networks have 1024 hidden units and 500-dimensional embedding; the vocabulary size is limited to 30K for each language; each model is trained for 10 epochs using stochastic gradient pedigree with Adam (Kingma and Ba, 2014); the minibatch size is 80, and the training kit is shuffled between each epoch; the learning rate is always 2 \u00b7 10 \u2212 4; we use the latest test 2012 for a development kit and the latest test 2011 and the latest test 2013 as test kits; during the test period, beam search is used to find approximately the most likely translation; we use a size 12 beam and normalize the probabilities by the length of applicant sets; the assessment metric is case-sensitive tokenized BLEU (Papineni, a 2002 script that was averaged with three different models)."}, {"heading": "5.4 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.4.1 A Comparison between Pivot-based Approach and Back-translation", "text": "Before opting for the pivot-language-based method of data synthesis, we conduct a preliminary experiment in which we analyze both the pivot-based and the direct retranslation. The model used for direct retranslation was trained with the basic Europarl-Fr-De data from the multi-parallel corpus shown in Table 2. In the most recent test sets in 2012 / 2013, the synthetic corpus generated with the Pivot approach showed higher BLEU (19.11 / 20.45) than the counterpart of the retranslation (18.23 / 19.81) when using a De Fr-NMT model. Although the method of retranslation was effective in many studies (Sennrich et al., 2015a, 2016), its availability is limited in cases with limited resources, which is our main concern. This is due to the poor quality of the retranslation model, which can be used from the limited source-target-parallel resource to-rich decorpus model instead."}, {"heading": "5.4.2 Effects of Mixing Source- and Target-originated Synthetic Data", "text": "Table 2 shows that the bias of synthetic examples in pseudo-parallel corpora leads to an improved quality of bidirectional translation tasks. Given that the source and target classification of a specific synthetic corpus is reversed depending on the direction of the translation, the overall results imply that the target originated corpus exceeds the source data for each translation task. Preference of the target synthetic data over the source-related counterparts was previously examined in SMT by Lambert et al., (2011). In NMT, it can be explained by the deterioration in the quality of the source-related data due to the flawed target language model formed from the synthetic objectives. In contrast, we observe that PSEUDOmix not only produces balanced results for Fr \u2192 De and De \u2192 Fr translation tasks, but also has the best or competitive quality of two synthetic translation sentences for each of the two networks."}, {"heading": "5.4.3 A Comparison with Phrase-based Statistical Machine Translation", "text": "We also evaluate the impact of the proposed mixing strategy on phrase-based statistical machine translation (Koehn et al., 2003). We use Moses (Koehn et al., 2007) and its basic configuration for training, using a 5 gram Kneser-Ney model as the language model. Table 4 shows the translation results of the phrase-based statistical machine translation systems (PBSMT). In all experiments, NMT shows a higher BLEU (2,44-3,38) compared to the PBSMT setting. We speculate that the deep architecture of NMT ensures noise immunity in the synthetic examples. It is also noteworthy that the proposed PSEUDOmix outperforms other synthetic corpora in PBSMT. The results clearly show that the benefits of the mixed composition in synthetic sentence pairs go beyond a specific framework of machine translation."}, {"heading": "6 Experiments: Large-scale Application", "text": "The experiments shown in the previous section confirm the potential of PSEUDOmix as an efficient alternative to real parallel data. However, the state in the previous case is somewhat artificial, as we consciously align the sources of all pseudo-parallel corpora. In this section we come to more practical and large-scale applications of synthetic parallel data. Experiments are carried out on Czech (Cs), German (De) and French (Fr) \u2194 German (De) translation tasks."}, {"heading": "6.1 Application Scenarios", "text": "We analyze the effectiveness of the proposed blending approach in the following application scenarios: (i) Pseudo-only: This setting trains NMT models only using synthetic parallel data without parallel corpus. (ii) Real fine-tuning: Once the NMT model is completed in the pseudo-only method, the model will only be fine-tuned using a parallel corpus of basic truths. The proposed scenarios reflect situations with limited resources when building NMT systems. In real fine-tuning, we fine-tune the best model of the Pseudo-only scenario, which is evaluated on the basis of development."}, {"heading": "6.2 Data Preparation", "text": "We use the parallel corpora from the common translation task of WMT '15 and WMT' 16 (Bojar et al., 2016). Using the same pivot-based technique as the previous task, Cs-De * and Fr-De * corpora are built from the parallel data of WMT '15 Cs-En and Fr-En, respectively. For Cs * -De and Fr * -De, WMT' 16 En-De are used parallel data. We use re-trained NMT models for En \u2192 Cs, En \u2192 De and En \u2192 Fr to generate synthetic sentences. A beam of size 1 is used for fast decryption. For the real fine-tuning scenario, we use real parallel corpora from the Europarliament and News Commentary11 dataset. These direct parallel corpora are obtained from OPUS (Tiedemann, 2012). The size of each set of basic data is considered relative to several pairs of facts in view of the synthesis data."}, {"heading": "6.3 Training and Evaluation", "text": "We use the same experimental settings that we used for the previous case, except for the real-finetuning scenario. In the fine-tuning step, we use the learning rate of 2 \u00b7 10 \u2212 5, which has led to better results. Embedding is fixed during the fine-tuning steps."}, {"heading": "6.4 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4.1 A Comparison with Real Parallel Data", "text": "Table 6 shows the results of the pseudo-only scenario for Cs \u2194 De and Fr \u2194 De tasks. For the baseline comparison, we also present the translation quality of the NMT models trained with the basic truth Europarl + NC11 parallel corpus (a). In Cs \u2194 De, the pseudo-only scenario shows better results compared to the real parallel corpus by up to 3.86-4.43 BLEU compared to the most recent test in 2013. Even for Fr \u2194 De, where the size of the real parallel corpus is relatively large, the best BLEU is the pseudo-parallel corpus by 1.3 (Fr \u2192 De) and 0.49 (De \u2192 Fr) higher than the real parallel corpus. We list the results of the most recent test in 2011 and the most recent test in 2012 in the Annex. From the results, we conclude that synthetic parallel data on a large scale can function as an effective alternative to the real parallel corpus, especially for language pairs with limited resources."}, {"heading": "6.4.2 Results from the Pseudo Only Scenario", "text": "As shown in Table 6, the model learned from the Cs * -De corpus always outperforms the model formed with the Cs-De * corpus. This result differs slightly from the previous case, where the targeted synthetic corpus performs better for each translation task than the source-related data. This is due to the diversity in the source of each pseudo-parallel corpus, which vary in their suitability for the given test set. Table 6 also shows that mixing the Cs * -De corpus with the Cs-De * corpus of poorer quality brings improvements in the resulting PSEUDOmix, with the blending strategy showing most of the gap between the Fr-De-De * and the Fr-De-Pr corpus (3.01 \u2192 0.17) in the Fr-De translation compared to other synthetic parallel corpus *."}, {"heading": "6.4.3 Results from the Real Fine-tuning Scenario", "text": "As shown in Table 6, we note that fine-tuning using parallel data to ground the truth brings significant improvements in the translation quality of all NMT models. Of all the finely tuned models, PSEUDOmix performs best in all experiments. This is particularly encouraging in the case of De \u2192 Fr, where PSEUDOmix has lower BLEU values than the Fr-De * data before being refined. Even in the case where PSEUDOmix shows comparable results to other synthetic corpora in the pseudo-only scenario, it shows higher improvements in translation quality when refined with the real parallel data. These results clearly show the strengths of the proposed PSEUDOmix, which point to both the competitive translation quality itself and a relatively higher potential improvement as a result of refinement using parallel corporations of truth in the ground. These results clearly point to the strengths of the proposed PSEUDOmix, which indicate both the competitive translation quality itself and a relatively higher potential improvement as a result of refinement using parallel corporations of truth in the ground."}, {"heading": "7 Conclusion", "text": "In this paper, we have constructed NMT systems that use synthetic parallel data only. To this end, we propose a novel pseudo-parallel corpus called PSEUDOmix, which mixes synthetic and grounded truth samples on both sides of sentence pairs. Experiments show that the proposed PSEUDOmix not only provides improved results for bidirectional translation, but also significantly improves when refined with parallel data on grounded truth. Our work is significant in that it provides a thorough study of the use of synthetic parallel corpus in resource-poor NMT environments. Without adjustments, the proposed method can also be extended to other learning areas where parallel samples are used. In future work, we plan robust data collection methods that would maximize the quality of mixed synthetic parallel data."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical machine translation with monolingual resources", "author": ["Nicola Bertoldi", "Marcello Federico."], "venue": "Proceedings of the fourth workshop on statistical machine translation. Association for Computational Linguistics, pages 182\u2013189.", "citeRegEx": "Bertoldi and Federico.,? 2009", "shortCiteRegEx": "Bertoldi and Federico.", "year": 2009}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Neural machine translation with pivot languages", "author": ["Yong Cheng", "Yang Liu", "Qian Yang", "Maosong Sun", "Wei Xu."], "venue": "arXiv preprint arXiv:1611.04928 .", "citeRegEx": "Cheng et al\\.,? 2016a", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1606.04596 .", "citeRegEx": "Cheng et al\\.,? 2016b", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1601.01073 .", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multilingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T Yarman Vural", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1606.04164 .", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP. volume 3, page 413.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit. volume 5, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Investigations on translation model adaptation using monolingual data", "author": ["Patrik Lambert", "Holger Schwenk", "Christophe Servan", "Sadaf Abdul-Rauf."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Com-", "citeRegEx": "Lambert et al\\.,? 2011", "shortCiteRegEx": "Lambert et al\\.", "year": 2011}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1312.6026 .", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1511.06709 .", "citeRegEx": "Sennrich et al\\.,? 2015a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1606.02891 .", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann."], "venue": "LREC. volume 2012, pages 2214\u2013 2218.", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Transductive learning for statistical machine translation", "author": ["Nicola Ueffing", "Gholamreza Haffari", "Anoop Sarkar"], "venue": "In Annual Meeting-Association for Computational Linguistics", "citeRegEx": "Ueffing et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ueffing et al\\.", "year": 2007}, {"title": "Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora", "author": ["Hua Wu", "Haifeng Wang", "Chengqing Zong."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1. As-", "citeRegEx": "Wu et al\\.,? 2008", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Bridging neural machine translation and bilingual dictionaries", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.07272 .", "citeRegEx": "Zhang and Zong.,? 2016a", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang and Zong.,? 2016b", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Many studies have found that the pseudo parallel data combined with the real bilingual parallel corpus significantly enhance the quality of NMT models (Sennrich et al., 2015a; Zhang and Zong, 2016b; Cheng et al., 2016b).", "startOffset": 151, "endOffset": 219}, {"referenceID": 27, "context": "Many studies have found that the pseudo parallel data combined with the real bilingual parallel corpus significantly enhance the quality of NMT models (Sennrich et al., 2015a; Zhang and Zong, 2016b; Cheng et al., 2016b).", "startOffset": 151, "endOffset": 219}, {"referenceID": 4, "context": "Many studies have found that the pseudo parallel data combined with the real bilingual parallel corpus significantly enhance the quality of NMT models (Sennrich et al., 2015a; Zhang and Zong, 2016b; Cheng et al., 2016b).", "startOffset": 151, "endOffset": 219}, {"referenceID": 19, "context": "In addition, synthesized parallel data have played vital roles in many NMT problems such as domain adaptation (Sennrich et al., 2015a), zeroresource NMT (Firat et al.", "startOffset": 110, "endOffset": 134}, {"referenceID": 8, "context": ", 2015a), zeroresource NMT (Firat et al., 2016b), and the rare word problem (Zhang and Zong, 2016a).", "startOffset": 27, "endOffset": 48}, {"referenceID": 26, "context": ", 2016b), and the rare word problem (Zhang and Zong, 2016a).", "startOffset": 36, "endOffset": 59}, {"referenceID": 9, "context": "Even in recent approaches such as zero-shot NMT (Johnson et al., 2016) and pivot-based NMT (Cheng et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": ", 2016) and pivot-based NMT (Cheng et al., 2016a), where direct source-to-target bilingual data are not required, the direct parallel corpus brings substantial improvements in translation quality where the pseudo parallel data can also be employed.", "startOffset": 28, "endOffset": 49}, {"referenceID": 10, "context": "To parameterize the conditional distribution, recent studies on NMT employ the encoder-decoder architecture (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 108, "endOffset": 183}, {"referenceID": 6, "context": "To parameterize the conditional distribution, recent studies on NMT employ the encoder-decoder architecture (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 108, "endOffset": 183}, {"referenceID": 22, "context": "To parameterize the conditional distribution, recent studies on NMT employ the encoder-decoder architecture (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 108, "endOffset": 183}, {"referenceID": 0, "context": "Thereafter, the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al.", "startOffset": 36, "endOffset": 79}, {"referenceID": 16, "context": "Thereafter, the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al.", "startOffset": 36, "endOffset": 79}, {"referenceID": 5, "context": ", 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al., 2014a).", "startOffset": 125, "endOffset": 144}, {"referenceID": 0, "context": "Thereafter, the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al., 2014a). In this study, we use the attentional NMT architecture proposed by Bahdanau et al. (2014). In their work, the encoder, which is a bidirectional recurrent neural network, reads the source sentence and generates a sequence of source representations", "startOffset": 37, "endOffset": 308}, {"referenceID": 24, "context": "By applying a self-training scheme, the pseudo parallel data were obtained by automatically translating the source-side monolingual corpora (Ueffing et al., 2007; Wu et al., 2008).", "startOffset": 140, "endOffset": 179}, {"referenceID": 25, "context": "By applying a self-training scheme, the pseudo parallel data were obtained by automatically translating the source-side monolingual corpora (Ueffing et al., 2007; Wu et al., 2008).", "startOffset": 140, "endOffset": 179}, {"referenceID": 1, "context": "In a similar but reverse way, the target-side monolingual corpora were also employed to build the synthetic parallel data (Bertoldi and Federico, 2009; Lambert et al., 2011).", "startOffset": 122, "endOffset": 173}, {"referenceID": 15, "context": "In a similar but reverse way, the target-side monolingual corpora were also employed to build the synthetic parallel data (Bertoldi and Federico, 2009; Lambert et al., 2011).", "startOffset": 122, "endOffset": 173}, {"referenceID": 27, "context": "Sourceside (Zhang and Zong, 2016b), target-side (Sennrich et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 19, "context": "Sourceside (Zhang and Zong, 2016b), target-side (Sennrich et al., 2015a) and both sides (Cheng et al.", "startOffset": 48, "endOffset": 72}, {"referenceID": 4, "context": ", 2015a) and both sides (Cheng et al., 2016b) of the monolingual data have been used to build synthetic parallel corpora.", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": "In their work, the pseudo parallel corpus was employed in fine-tuning the targetspecific attention mechanism of trained multi-way multilingual NMT (Firat et al., 2016a) models, which enabled zero-resource NMT between the source and target languages.", "startOffset": 147, "endOffset": 168}, {"referenceID": 26, "context": "Lastly, synthetic sentence pairs have been utilized to enrich the training examples having rare or unknown translation lexicons (Zhang and Zong, 2016a).", "startOffset": 128, "endOffset": 151}, {"referenceID": 3, "context": ", 2015a) and both sides (Cheng et al., 2016b) of the monolingual data have been used to build synthetic parallel corpora. In their work, the pseudo parallel data combined with a real training corpus significantly enhanced the translation quality of NMT. In Sennrich et al., (2015a), domain adaptation of NMT was achieved by finetuning trained NMT models using a synthetic parallel corpus.", "startOffset": 25, "endOffset": 282}, {"referenceID": 3, "context": ", 2015a) and both sides (Cheng et al., 2016b) of the monolingual data have been used to build synthetic parallel corpora. In their work, the pseudo parallel data combined with a real training corpus significantly enhanced the translation quality of NMT. In Sennrich et al., (2015a), domain adaptation of NMT was achieved by finetuning trained NMT models using a synthetic parallel corpus. Firat et al. (2016b) attempted to build NMT systems without any direct source-to-target parallel corpus.", "startOffset": 25, "endOffset": 410}, {"referenceID": 19, "context": "The corpus can be formed by back-translating a target-side monolingual corpus into the source language (Sennrich et al., 2015a).", "startOffset": 103, "endOffset": 127}, {"referenceID": 8, "context": "Similar to the source-originated case, it can be built from a pivot-target bilingual corpus using a pivot-tosource translation model (Firat et al., 2016b).", "startOffset": 133, "endOffset": 154}, {"referenceID": 26, "context": "In the previous study, Zhang and Zong (2016b) bypassed this issue by freezing the decoder parameters while training with the minibatches of pseudo bilingual pairs made from a source language monolingual corpus.", "startOffset": 23, "endOffset": 46}, {"referenceID": 12, "context": "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora (Koehn, 2005), constructing a multiparallel corpus of Fr-En-De.", "startOffset": 152, "endOffset": 165}, {"referenceID": 13, "context": "Each training corpus is tokenized using the tokenization script in Moses (Koehn et al., 2007).", "startOffset": 73, "endOffset": 93}, {"referenceID": 20, "context": "We represent every sentence as a sequence of subword units learned from byte-pair encoding (Sennrich et al., 2015b).", "startOffset": 91, "endOffset": 115}, {"referenceID": 11, "context": "Each model is trained for 10 epochs using stochastic gradient descent with Adam (Kingma and Ba, 2014).", "startOffset": 80, "endOffset": 101}, {"referenceID": 18, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 17, "context": "The evaluation metric is case-sensitive tokenized BLEU (Papineni et al., 2002) computed with the multi-bleu.", "startOffset": 55, "endOffset": 78}, {"referenceID": 15, "context": "The preference of target-originated synthetic data over the sourceoriginated counterparts was formerly investigated in SMT by Lambert et al., (2011). In NMT, it can be explained by the degradation in quality in the source-originated data owing to the erroneous target language model formed by the synthetic target sentences.", "startOffset": 126, "endOffset": 149}, {"referenceID": 14, "context": "We also evaluate the effects of the proposed mixing strategy in phrase-based statistical machine translation (Koehn et al., 2003).", "startOffset": 109, "endOffset": 129}, {"referenceID": 13, "context": "Moses (Koehn et al., 2007) and its baseline configuration for training.", "startOffset": 6, "endOffset": 26}, {"referenceID": 23, "context": "These direct parallel corpora are obtained from OPUS (Tiedemann, 2012).", "startOffset": 53, "endOffset": 70}, {"referenceID": 19, "context": "This is identical in spirit to the method in Sennrich et al. (2015a) which employs back-translation for data synthesis.", "startOffset": 45, "endOffset": 69}], "year": 2017, "abstractText": "Recent works have shown that synthetic parallel data automatically generated by translation models can be effective for various neural machine translation (NMT) issues. In this study, we build NMT systems using only synthetic parallel data. As an efficient alternative to real parallel data, we also present a new type of synthetic parallel corpus. The proposed pseudo parallel data are distinct from previous works in that ground truth and synthetic examples are mixed on both sides of sentence pairs. Experiments on Czech-German and French-German translations demonstrate the efficacy of the proposed pseudo parallel corpus, which shows not only enhanced results for bidirectional translation tasks but also substantial improvement with the aid of a ground truth real parallel corpus.", "creator": "LaTeX with hyperref package"}}}