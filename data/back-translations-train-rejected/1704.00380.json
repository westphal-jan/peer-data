{"id": "1704.00380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2017", "title": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using Word Embeddings", "abstract": "One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods.", "histories": [["v1", "Sun, 2 Apr 2017 22:36:56 GMT  (141kb,D)", "http://arxiv.org/abs/1704.00380v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["junki matsuo", "mamoru komachi", "katsuhito sudoh"], "accepted": false, "id": "1704.00380"}, "pdf": {"name": "1704.00380.pdf", "metadata": {"source": "CRF", "title": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using Word Embeddings", "authors": ["Junki Matsuo", "Mamoru Komachi", "Katsuhito Sudoh"], "emails": ["matsuo-junki@ed.tmu.ac.jp,", "komachi@tmu.ac.jp", "sudoh@is.naist.jp"], "sections": [{"heading": null, "text": "One of the most important problems when evaluating Machine Translation (MT) is to assess the similarity between translation hypotheses and other forms of reference, especially at the segment level. We propose to use word embeddings to evaluate word embeddings at the segment level. We conducted experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with different translation datasets. Experimental results show that our proposed methods outperform previous methods based on word embeddings."}, {"heading": "1 Introduction", "text": "For example, BLEU (Papineni et al., 2002) has improved MT research over the past decade. However, BLEU has little correlation with human judgment at the segment level since it was originally proposed for system evaluation. Segment evaluation is critical for analyzing MT results to improve system accuracy, but there are few studies that address the problem of segment evaluation of MT results. Another problem in MT evaluation is the evaluation of MT hypotheses that are semantically equivalent to different surfaces from the reference. Thus, BLEU does not consider words that do not match the reference at the segment level. METEOR-Universal (Denkowski and Lavie, 2014) treats word similarities better than the MT hypotheses that are semantically equated with different surfaces from the reference."}, {"heading": "2 Related Work", "text": "Several studies have examined the automatic representation of MT systems. BLEUar Xiv: 170 4.00 380v 1 [cs.C L] 2A pr2 017 (Papineni et al., 2002), the de facto standard automatic MT evaluation metrics, can give inappropriate marks to a translation hypothesis that uses similar but different words, because it only takes into account the accuracy of the n-gram word (Callison-Burch et al., 2006). METEOR-Universal (Denkowski and Lavie, 2014) alleviates the problem of surface conformity by using a thesaurus and a plunger, but it requires external resources, such as WordNet. In this paper, we used a distributed representation of words to evaluate the semantic relationship between the hypothesis and reference sentences, an approach that has the advantage that it can only be implemented with a raw monolingual corpus."}, {"heading": "3 Word-Alignment-Based Sentence Similarity using Word Embeddings", "text": "In this section, we present the word alignment-based sentence similarity (Song and Roth, 2015), which is used as the MT rating yardstick. Song and Roth (2015) propose to use word embedding to align words in a sentence. Their approach shows promising results in STS tasks. In the MT rating, a word in the source language aligns with either a word or phrase in the target language, so a word is unlikely to match the whole sentence. Therefore, we use several heuristics to limit the word alignment between hypothesis and reference sentences. In the following subsections, we present three measures of sentence similarity, all of which use cosine similarity to calculate word similarity. To avoid alignment between unrelated words, we cap the word alignment that is less than a threshold."}, {"heading": "3.1 Average Alignment Similarity", "text": "First, the average Alignment Similarity (AAS) heuristics aligns a word with several words in a sentence pair. It calculates the similarity of words between a hypotheses set and a reference set. AAS is given by evaluating the word similarity of all word combinations in | x | | y |.AAS (x, y) = 1 | x | y | | x | \u2211 i = 1 | y | \u2211 j = 1 \u03c6 (xi, yj) (1) Here x is a hypothesis and y is a reference; and xi and yj represent words in each sentence."}, {"heading": "3.2 Maximum Alignment Similarity", "text": "Secondly, we propose the maximum Alignment Similarity (MAS) heuristics, in which only the word that has the maximum similarity value of each aligned word pair is averaged. MAS itself is by definition an asymmetric value, so we symmetrize it by averaging the value in both directions.MASasym (a, b) = 1 | a | | a | \u2211 i = 1 max j \u03c6 (ai, bj) (2) MAS (x, y) = 12 (MASasym (x, y) + MASasym (y, x))) (3) Here are a and b words in a hypothesis or a reference clause."}, {"heading": "3.3 Hungarian Alignment Similarity", "text": "Thirdly, we introduce the Hungarian Alignment similarity (HAS) to limit the word alignment to 1: 1. HAS formulates the problem of word alignment as a two-part graph concordance, in which the words in a hypothesis and a reference are represented as nodes whose edges have a weight \u03c6 (xi, yi). One-to-one word alignment is achieved by calculating the maximum alignment of the perfect two-part graph. For each word xi contained in a hypotheses set, HAS selects the word h (xi) in a reference sentence y according to the Hungarian method (Kuhn, 1955). HAS (x, y) = 1min (| x |, | y |) | \u2211 i = 1 \u03c6 (xi, h (xi)) (4)."}, {"heading": "4 Experiment", "text": "We report on the results of the MT evaluation in a European-English translation task of the WMT12, WMT13 and WMT15 datasets and the Japanese-English task of the WAT2015 and NTCIR8 datasets. For the WMT datasets, we compared our key figures with BLEU and DREEM from the official result of the WMT15 metric task (Stanojevic \u0301 et al., 2015). For the WAT2015 and NTCIR8 datasets, the three types of proposed methods are compared."}, {"heading": "4.1 Experimental Setting", "text": "We used the data sets WMT12, WMT13 and WMT15 with a total of 137,007 sentences in French, Finnish, German, Czech and Russian translation into English. As Japanese-English translation data sets, WAT2015 contains 600 sentences and NTCIR8 1200 sentences. We measured the correlation between the Human Adequacy Score and each of the evaluation metrics. We used Kendall for segment level evaluation. We used a pre-trained model of word2vec using the Google News corpus to calculate word similarity with our proposed methods.1"}, {"heading": "4.2 Result", "text": "Table 1 shows a breakdown of the correlation values for each language pair in WMT15. MAS shows the best accuracy among all proposed metrics for all language pairs. Its accuracy is better for all language pairs except Czech-English than DREEM's. This result shows that eliminating noisy word embedding by using a threshold or 1: n alignment is important for European-English datasets. Figure 1 shows the maximum correlation of word-line-based methods for WMT datasets with different thresholds. In the case of WMT datasets, MAS has the highest correlation values among the three word-line-based methods. A threshold of 0.2 indicates the maximum correlation for MAS for all WMT datasets. Figure 2 shows the correlation of word-line-based methods for the two Japanese-English datasets 1https: / / code.google.com / MAS has the highest correlation for all WMT datasets in 2015 with the highest WIRT / 2P datasets."}, {"heading": "5 Discussion", "text": "Figure 1 shows that MAS and AAS are more stable than HAS for European-English datasets. This may be because it is relatively easy for the AAS and MAS to perform word embedding using word embedding in translation pairs of similar languages, but HAS suffers more from alignment sparseness than the other methods. In the European-English translation, all word alignment-based methods perform poorly when they do not use word embeds.Unlike the European-English translation task, the Japanese-English translation task has a different tendency. Figure 2 shows the comparison between three types of word alignment-based methods for each threshold. This is partly because word embedding helps evaluate lexically similar word pairs, but does not model syntactical variations. In addition, we note that AAS has reached the highest correlation in Japanese-English datasets. We assume that this is due to the fact that the Japanese-English MASS alignment-MASS data sets are stable during MASS-English datasets."}, {"heading": "6 Conclusion", "text": "In this paper, we presented text-based MT evaluation metrics using distributed word representations. In our experiments, MAS showed a higher correlation with human evaluation than other automatic MT metrics such as BLEU and DREEM for European-English datasets. On the other hand, in Japanese-English datasets, AAS showed a higher correlation with human evaluation than other metrics. These results suggest that appropriate word alignment using word embedding is helpful in evaluating MT results."}], "references": [{"title": "Re-evaluating the Role of BLEU in Machine Translation Research", "author": ["Chris Callison-Burch", "Miles Osborne", "Philipp Koehn."], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics. pages", "citeRegEx": "Callison.Burch et al\\.,? 2006", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "Representation Based Translation Evaluation Metrics", "author": ["Boxing Chen", "Hongyu Guo."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-", "citeRegEx": "Chen and Guo.,? 2015", "shortCiteRegEx": "Chen and Guo.", "year": 2015}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. pages 376\u2013380.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "The Hungarian Method for the Assignment Problem", "author": ["Harold W. Kuhn."], "venue": "Naval Research Logistics Quarterly. pages 83\u201397.", "citeRegEx": "Kuhn.,? 1955", "shortCiteRegEx": "Kuhn.", "year": 1955}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computa-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "ChrF: Character n-gram F-score for Automatic MT Evaluation", "author": ["Maja Popovi\u0107."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 392\u2013395.", "citeRegEx": "Popovi\u0107.,? 2015", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "Unsupervised Sparse Vector Densification for Short Text Similarity", "author": ["Yangqui Song", "Dan Roth."], "venue": "Proceedings of the 2015 Annual Conference of the North American Chapter of the ACL. pages 1275\u20131280.", "citeRegEx": "Song and Roth.,? 2015", "shortCiteRegEx": "Song and Roth.", "year": 2015}, {"title": "Results of the WMT15 Metrics Shared Task", "author": ["Milo\u0161 Stanojevi\u0107", "Amir Kamran", "Philipp Koehn", "Ond\u0159ej Bojar."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 256\u2013273.", "citeRegEx": "Stanojevi\u0107 et al\\.,? 2015", "shortCiteRegEx": "Stanojevi\u0107 et al\\.", "year": 2015}, {"title": "Modifications of Machine Translation Evaluation Metrics by Using Word Embeddings", "author": ["Haozhou Wang", "Paola Merlo."], "venue": "Proceedings of the Sixth Workshop on Hybrid Approaches to Translation (HyTra6). pages 33\u201341.", "citeRegEx": "Wang and Merlo.,? 2016", "shortCiteRegEx": "Wang and Merlo.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "For example, BLEU (Papineni et al., 2002) has improved the MT research in the last decade.", "startOffset": 18, "endOffset": 41}, {"referenceID": 2, "context": "METEOR-Universal (Denkowski and Lavie, 2014) handles word similarities better,", "startOffset": 17, "endOffset": 44}, {"referenceID": 1, "context": "DREEM (Chen and Guo, 2015), another metric that addresses the issue of word similarity, does not require human annotations and uses distributed representations for MT evaluation.", "startOffset": 6, "endOffset": 26}, {"referenceID": 6, "context": "We adopt sentence similarity measures proposed by Song and Roth (2015) for a Semantic Textual Similarity (STS) task.", "startOffset": 50, "endOffset": 71}, {"referenceID": 4, "context": "(Papineni et al., 2002) may assign inappropriate score to a translation hypothesis that uses similar but different words because it considers only word n-gram precision (Callison-Burch et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": ", 2002) may assign inappropriate score to a translation hypothesis that uses similar but different words because it considers only word n-gram precision (Callison-Burch et al., 2006).", "startOffset": 153, "endOffset": 182}, {"referenceID": 2, "context": "METEOR-Universal (Denkowski and Lavie, 2014) alleviates the problem of surface mismatch by using a thesaurus and a stemmer but it needs external resources, such as WordNet.", "startOffset": 17, "endOffset": 44}, {"referenceID": 1, "context": "The previous method most similar to ours is DREEM (Chen and Guo, 2015).", "startOffset": 50, "endOffset": 70}, {"referenceID": 5, "context": "CHRF (Popovi\u0107, 2015) is one such metric that uses character n-grams.", "startOffset": 5, "endOffset": 20}, {"referenceID": 0, "context": ", 2002) may assign inappropriate score to a translation hypothesis that uses similar but different words because it considers only word n-gram precision (Callison-Burch et al., 2006). METEOR-Universal (Denkowski and Lavie, 2014) alleviates the problem of surface mismatch by using a thesaurus and a stemmer but it needs external resources, such as WordNet. In this work, we used a distributed word representation to evaluate semantic relatedness between the hypothesis and reference sentences. This approach has the advantage that it can be implemented only with only a raw monolingual corpus. To address the problem of word n-gram precision, Wang and Merlo (2016) propose to smooth it by word embeddings.", "startOffset": 154, "endOffset": 665}, {"referenceID": 6, "context": "In this section, we introduce word-alignmentbased sentence similarity (Song and Roth, 2015) applied as an MT evaluation metrics.", "startOffset": 70, "endOffset": 91}, {"referenceID": 6, "context": "In this section, we introduce word-alignmentbased sentence similarity (Song and Roth, 2015) applied as an MT evaluation metrics. Song and Roth (2015) propose to use word embeddings to align words in a pair of sentences.", "startOffset": 71, "endOffset": 150}, {"referenceID": 3, "context": "For each word xi included in a hypothesis sentence, HAS chooses the word h(xi) in a reference sentence y by the Hungarian method (Kuhn, 1955).", "startOffset": 129, "endOffset": 141}, {"referenceID": 7, "context": "For the WMT datasets, we compared our metrics with BLEU and DREEM taken from the official score of the WMT15 metric task (Stanojevi\u0107 et al., 2015).", "startOffset": 121, "endOffset": 146}, {"referenceID": 7, "context": "BLEU (Stanojevi\u0107 et al., 2015) 0.", "startOffset": 5, "endOffset": 30}, {"referenceID": 1, "context": "349 DREEM (Chen and Guo, 2015) 0.", "startOffset": 10, "endOffset": 30}], "year": 2017, "abstractText": "One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods.", "creator": "LaTeX with hyperref package"}}}