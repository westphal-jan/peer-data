{"id": "1704.00389", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2017", "title": "Hidden Two-Stream Convolutional Networks for Action Recognition", "abstract": "Analyzing videos of human actions involves understanding the temporal relationships among video frames. CNNs are the current state-of-the-art methods for action recognition in videos. However, the CNN architectures currently being used have difficulty in capturing these relationships. State-of-the-art action recognition approaches rely on traditional local optical flow estimation methods to pre-compute the motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information. Our method is 10x faster than a two-stage approach, does not need to cache flow information, and is end-to-end trainable. Experimental results on UCF101 and HMDB51 show that it achieves competitive accuracy with the two-stage approaches.", "histories": [["v1", "Sun, 2 Apr 2017 23:39:51 GMT  (2895kb,D)", "https://arxiv.org/abs/1704.00389v1", "under review at ICCV 2017"], ["v2", "Sat, 8 Jul 2017 21:48:54 GMT  (2897kb,D)", "http://arxiv.org/abs/1704.00389v2", "Code available atthis https URLUpdate results for HMDB51. Attach supplemental materials"], ["v3", "Sun, 22 Oct 2017 03:53:21 GMT  (3445kb,D)", "http://arxiv.org/abs/1704.00389v3", "Extended journal version, under review. Code available atthis https URL"]], "COMMENTS": "under review at ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["yi zhu", "zhenzhong lan", "shawn newsam", "alexander g hauptmann"], "accepted": false, "id": "1704.00389"}, "pdf": {"name": "1704.00389.pdf", "metadata": {"source": "CRF", "title": "Hidden Two-Stream Convolutional Networks for Action Recognition", "authors": ["Yi Zhu", "Zhenzhong Lan", "Shawn Newsam", "Alexander G. Hauptmann"], "emails": ["yzhu25@ucmerced.edu)", "snewsam@ucmerced.edu)", "lanzhzh@cs.cmu.edu)", "alex@cs.cmu.edu)"], "sections": [{"heading": null, "text": "In fact, it is the case that most people who have lived in the USA in recent years are not able to help themselves. (...) It is not the case that they are able to help themselves. (...) It is not the case that they are able to help themselves. (...) It is not the case that they have to help themselves. (...) It is not the case that they are able to help themselves. (...) It is the case that they have to do it. (...) It is not the case that they have to do it. (...) It is not the case that they are able to help themselves. (...) It is the case. (...) It is the case. (...) It is the case. (...) It is the case. (...). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). \"(It. (). (). (). (). ().\" (). (). (). (). (). (). (). (). (). (). (). (). \"(). (). ().\" (). (). (). \"(). ().\" (). \"(). (). (It. (). (). (It. (). (). (). (). (). (). (). (It. (). (). (). (). (). (It. (). (). (). (). (). (It. (). (). (). (). (). (It. (). (). (). (). (). (). (It. (). (). (). (). (). (). (It. (). (). (). (). (). (). (). (). (). ()."}, {"heading": "II. RELATED WORK", "text": "In fact, most people who work for the rights of women and men have to burden themselves and their rights and obligations."}, {"heading": "III. HIDDEN TWO-STREAM NETWORKS", "text": "We first introduce our unattended optical stream estimation network and introduce best practices in Section III-A. We call it MotionNet. We also conduct a search for the CNN architecture to find the best network in terms of the trade-off between accuracy and efficiency. In Section III-B, we stack the time stream network on MotionNet to enable end-to-end training. We call this stacked network the stacked time stream CNN. We also experiment with multi-task learning in a branched time stream and compare these two design decisions. Finally, in Section III-C, we introduce the hidden two-stream CNNs that combine our stacked time stream with a spatial stream."}, {"heading": "A. Unsupervised Optical Flow Learning", "text": "We treat optical flow estimation as an image reconstruction problem [37]. Basically, it is like hoping to generate the optical flow that allows us to reconstruct an image from the other side. (It is like reconstructing an image from the other side. (It is like using the inverse warping function where T is the inverse warping function.) Ourgoal is to minimize the photometric error between I1 and I.) The intuition is that the estimated flow and the next frame can be used to perfectly reconstruct the current frame, then the network should have learned useful representations of the underlying movements. (MotionNet is a completely expanding part of a network, contradictory and contradictory to a network). [37] Basically, it is like reconstructing an image from the other side. (It is like using the inverse warping function where T is the inverse warping function.) Ourgoal is to minimize the photometric error between I1 and I. The intuition is that the estimated flow and the next frame can be used to perfectly reconstruct the current frame, then the network should have learned useful representations of the underlying movements."}, {"heading": "B. Projecting Motion Features to Actions", "text": "In fact, it is such that most of them will be able to move into another world, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "C. Hidden Two-Stream Networks", "text": "These results are important because they are strong indicators of whether our stacked time stream actually learns complementary motion information or merely information about appearance. In accordance with the test scheme of [10], [11] we randomly sample 25 frames / clips for each video. For each frame / clip, we perform ten-fold data augmentation by cropping the 4 corners and 1 center, rotating them horizontally, and averaging the predictive values (before Softmax operation) over all harvests of the samples. At the end, we merge the results of the two streams with a spatial / temporal scatter ratio of 1: 1.5."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we will first describe the data sets in Section IV-A and the implementation details of our proposed approach in Section IV-B. We will then report on the performance of the stacked time current and the hidden dielectric networks in Section IV-C. We will also analyze the experimental results and then conduct a discussion.6"}, {"heading": "A. Evaluation Datasets", "text": "We are conducting experiments on two widely used action recovery benchmarks, UCF101 [42] and HMDB51 [43]. UCF101 consists of realistic action videos from YouTube. It contains 13, 320 video clips spread across 101 action classes. HMDB51 includes 6, 766 video clips of 51 actions extracted from a wide range of sources such as online videos and movies. Both UCF101 and HMDB51 have a standard stream with three different assessment protocols and we report the average detection accuracy across the three splits.B. Implementation DetailsFor the CNNs we use the Caffe Toolbox [44]. For the TV-L1 optical flow we use the OpenCV GPU implementation protocol and we report the average detection accuracy across the three splits.B. Implementation DetailsFor the CNs, we use the Caffe 44 toolbox."}, {"heading": "C. Results", "text": "In this section we evaluate our proposed MotionNet, the stacked temporal stream CNN, and the hidden two-current CNNs on the first division of UCF101. We report on the accuracy and processing speed of the inference step in frames per second. Results are shown in Table III. Results show that our MotionNet achieves a good balance between accuracy and speed in this setting. In terms of accuracy, accuracy and speed, MotionNet only achieves frame pairs as input. We still store the estimated flows for training and inference for fair comparison. Results show that our MotionNet achieves a good balance between accuracy and speed in this setting. In terms of accuracy, MotionNet is competitive with TV-L1, while we perform much better (4% 12% absolute improvement) than other ways of generative flows, including supervised training using synthetic data."}, {"heading": "V. DISCUSSION", "text": "In this section, we will conduct several studies to examine various aspects of the design of our proposed MotionNet. First, in Section V-B, we will conduct an ablation study to understand the contributions of our specially designed loss functions and operators; then, in Section V-D, we will conduct a search for the CNN architecture to find the best network for generating motion characteristics for action detection; Section V-C will compare the two design options of stacking and branching; in Section V-D, we will demonstrate that other approaches can be used to further improve both the speed and accuracy of our method; in Section V-E, we will describe the limitations of MotionNet and examine whether the use of proxy guidance will help to better estimate motion in Section V-F. Finally, we will examine the impact of different motion estimation models for action detection in Section V-G."}, {"heading": "A. Ablation Studies for MotionNet", "text": "Due to our specially designed loss detection functions and operators, our proposed MotionNet can produce high-quality motion estimates that allow us to achieve promising action detection accuracies. Here, we are conducting an ablation study to understand the contributions of these components, the results of which are shown in Table IV. Small Disp shows the use of a network that focuses on small shifts. Conv Between Deconv means adding an additional convolution between deconvolutions in the expanding part of MotionNet. First, we are investigating the importance of using a network structure that focuses on small shifts. We keep the aspects of the other implementation the same, but use a larger core size and proceed at the beginning of the network. Accuracy drops from 82.71% to 82.22%. This decrease shows that the use of smaller cores with deeper network actually helps us to detect small movements and improve our performance.Second, we are investigating the importance of adding the SSIM loss, so that without 858% SSIM detection accuracy we can lower the resolution of 858%."}, {"heading": "B. CNN Architecture Search", "text": "We conduct a search for the CNN architecture to find the best network for generating motion characteristics for action detection in terms of the trade-off between accuracy and efficiency. Here, we compare four architectures, namely Tiny-MotionNet, MotionNet, VGG16-MotionNet and ResNet50-MotionNet. These architectures all use VGG16 as a time stream CNNs as in [11]. The results can be seen in Table V.Our MotionNet, which achieves the highest action classification accuracy with the second smallest model size. Tiny-MotionNet is 20 times smaller than MotionNet, but its accuracy drops by only 1%. It is noteworthy that although Tiny-MotionNet is 80 times smaller than FlowNet2, it is still 1.5% more accurate. This observation is encouraging for two reasons: (1) It indicates that a very deep network is not necessary to test better motion characteristics for highly graded video understanding tasks on mobile phones, such as the small size of Tiny phones."}, {"heading": "C. Stacking or Branching", "text": ", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\",, \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",,\",, \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\""}, {"heading": "D. Temporal Segment Network as Our Temporal Stream CNN", "text": "Recall from Section II that the performance of our method can be further improved by methods that mitigate the problem of incorrect label assignment. Temporal Segment Networks (TSN) [15] are one such method. It addresses the problem of incorrect label assignment by training the networks together on several sparsely sampled frames / clips. To illustrate, we are replacing the VGG16 architecture with TSN as our time stream CNN. This experiment is more a qualitative illustration than a quantitative evaluation. Apart from the fact that our method can be improved by other methods, we also hope to show that the motion information generated by our MotionNet can be used by various time streams CNNs. Therefore, we have not fine-tuned our MotionNet. Instead, we feed the predicted optical flow of MotionNet toTVL1 MotionNet + TV-L1 MotionNet NTS1 MotionNet + FieldsFieldsFig."}, {"heading": "E. Limitations of MotionNet", "text": "Although our MotionNet outperforms other end-to-end methods [54], [31], [30], [15] for video action detection, we are still far from achieving its full potential, performing 1% worse results than the traditional method of estimating optical flow (TVL1) [24]. In this case, we are performing a visual case comparison between the output of MotionNet and the TV-L1 algorithm, hoping to find out where we could further improve our motion estimation. As we can see in Figure 3, MotionNet often produces louder motion estimates than TV-L1. MotionNet in particular has difficulties in regions that are saturated or have dynamic textures, such as water, sky and mirrors. This difficulty stems from the fact that the constant brightness assumption does not hold. In the second series of Figure 3, for example, true motion shaves a beard, which is better outlined in Figure 1."}, {"heading": "F. Will extra guidance help?", "text": "Given the limitations of MotionNet, we hope to correct these errors by using the predictions of robust flow estimators such as TV-L1 as our proxy soil truth. Therefore, we are investigating whether we can learn better motion estimates when using proxy soil truths to train our MotionNet. We are investigating both TV-L1 [24] and FlowFields [54]. To our knowledge, TV-L1 is one of the most widely used and powerful flow estimators for shareholder recognition, and FlowFields is one of the most accurate flow estimators in the optical flow field 2. The results are shown in Table VIII.As you can see, adding proxy data harms performance. Action detection accuracy drops by about 2% when using TV-L1 as a predictive 3%, [56] This result is probably better when applying the results of the flow world."}, {"heading": "G. Learned Optical Flow", "text": "In this section, we systematically examine the effects of different motion estimation models for action detection. We also show some visual examples to discover possible directions for future improvements.Here, we compare three optical flow models: TV-L1, MotionNet and FlowNet2. To quantify the quality of the flow learned, we test the three models against the well-accepted MPI Sintel benchmark [57]. However, for action detection accuracy, 2For FlowFields achieves the worst action detection tasks. On the contrary, MotionNet and TV-L1 achieve a much higher EPE on Sintel, but they score fairly well on UCF101 split1. This interesting observation means that lower EPE fields do not always lead to higher action detection accuracy."}, {"heading": "VI. COMPARISON TO STATE-OF-THE-ART REAL-TIME APPROACHES", "text": "In this section, we compare our proposed method with current real-time state-of-the-art approaches as shown in Table X3. Of all real-time methods, our hidden two-stream networks achieve the highest accuracy on both benchmarks. We are 2.1% better at UCF101 and 10.4% better at HMDB51 than the previous state of the art. This indicates that our learned motion representation is better than motion vectors [58], [30] and RGB differences [15] in terms of action detection task. We also observe that temporal segment networks are effective practices to capture long-term time relationships and help generate more accurate predictions at the video level.3Generally, the real-time processing requirement is 25 fp.11It is worth noting that our hidden two-stream networks with Tiny-MotionNet have a promising performance compared to Motion Net running at a model size of only 500GB or more than Motion Net running at a model size of only 8GB."}, {"heading": "VII. CONCLUSION", "text": "We have proposed a new framework called Hidden Two-Stream Networks to detect human actions in video. It addresses the problem of capturing the temporal relationships between video images that current CNN architectures struggle with. Unlike current common practice of using traditional local methods to estimate the optical flow to pre-calculate motion information for CNNs, we use an uncontrolled pre-training approach. Our MotionNet network (MotionNet) is computationally efficient and feasible at the end. Experimental results on UCF101 and HMDB51 show that our method is ten times faster than traditional methods while maintaining similar accuracy. In the future, we would like to improve our hidden two-stream stream networks in the following directions: First, we would improve our prediction of the optical flow based on the observation that the smoothness loss has a significant impact on the quality of motion estimates for action detection."}], "references": [{"title": "Action Recognition with Improved Trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "ICCV, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Action Recognition with Stacked Fisher Vectors", "author": ["X. Peng", "C. Zou", "Y. Qiao", "Q. Peng"], "venue": "ECCV, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition", "author": ["Z. Lan", "M. Lin", "X. Li", "A.G. Hauptmann", "B. Raj"], "venue": "CVPR, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling Video Evolution for Action Recognition", "author": ["B. Fernando", "J.O.M.E. Gavves", "A. Ghodrati", "T. Tuytelaars"], "venue": "CVPR, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Generalized Rank Pooling for Activity Recognition", "author": ["A. Cherian", "B. Fernando", "M. Harandi", "S. Gould"], "venue": "CVPR, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning Spatiotemporal Features with 3D Convolutional Networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "ICCV, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale Video Classification with Convolutional Neural Networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks", "author": ["L. Sun", "K. Jia", "D.-Y. Yeung", "B.E. Shi"], "venue": "ICCV, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic Image Networks for Action Recognition", "author": ["H. Bilen", "B. Fernando", "E. Gavves", "A. Vedaldi", "S. Gould"], "venue": "CVPR, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Two-Stream Convolutional Networks for Action Recognition in Videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards Good Practices for Very Deep Two-Stream ConvNets", "author": ["L. Wang", "Y. Xiong", "Z. Wang", "Y. Qiao"], "venue": "arXiv preprint arXiv:1507.02159, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Actions \u0303 Transformations", "author": ["X. Wang", "A. Farhadi", "A. Gupta"], "venue": "CVPR, 2016.  12", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Asynchronous Temporal Fields for Action Recognition", "author": ["G.A. Sigurdsson", "S. Divvala", "A. Farhadi", "A. Gupta"], "venue": "CVPR, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "A Key Volume Mining Deep Framework for Action Recognition", "author": ["W. Zhu", "J. Hu", "G. Sun", "X. Cao", "Y. Qiao"], "venue": "CVPR, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition", "author": ["L. Wang", "Y. Xiong", "Z. Wang", "Y. Qiao", "D. Lin", "X. Tang", "L.V. Gool"], "venue": "ECCV, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Temporal Linear Encoding Networks", "author": ["A. Diba", "V. Sharma", "L.V. Gool"], "venue": "CVPR, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos", "author": ["A. Kar", "N. Rai", "K. Sikka", "G. Sharma"], "venue": "CVPR, 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "ActionFlowNet: Learning Motion Representation for Action Recognition", "author": ["J.Y.-H. Ng", "J. Choi", "J. Neumann", "L.S. Davis"], "venue": "arXiv preprint arXiv:1612.03052, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to Extract Motion from Videos in Convolutional Neural Networks", "author": ["D. Teney", "M. Hebert"], "venue": "ACCV, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "FlowNet: Learning Optical Flow with Convolutional Networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. Husser", "C. Hazrba", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "ICCV, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Videobased Person Re-identification with Accumulative Motion Context", "author": ["H. Liu", "Z. Jie", "K. Jayashree", "M. Qi", "J. Jiang", "S. Yan", "J. Feng"], "venue": "IEEE Trans. Circuits Syst. Video Technol., 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "DenseNet for Dense Flow", "author": ["Y. Zhu", "S. Newsam"], "venue": "ICIP, 2017.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Guided Optical Flow Learning", "author": ["Y. Zhu", "Z. Lan", "S. Newsam", "A.G. Hauptmann"], "venue": "arXiv preprint arXiv:1702.02295, 2017.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "A Duality Based Approach for Realtime TV-L1 Optical Flow", "author": ["C. Zach", "T. Pock", "H. Bischof"], "venue": "29th DAGM conference on Pattern recognition, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Going Deeper into Action Recognition: A Survey", "author": ["S. Herath", "M. Harandi", "F. Porikli"], "venue": "arXiv preprint arXiv:1605.04988, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Action Recognition with Trajectory- Pooled Deep-Convolutional Descriptors", "author": ["L. Wang", "Y. Qiao", "X. Tang"], "venue": "CVPR, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolution-Preserving Dense Trajectory Descriptors", "author": ["Y. Wang", "V. Tran", "M. Hoai"], "venue": "arXiv preprint arXiv:1702.04037, 2017.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "3D Convolutional Neural Networks for Human Action Recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition", "author": ["Y. Zhu", "S. Newsam"], "venue": "ECCV Workshops, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time Action Recognition with Enhanced Motion Vector CNNs", "author": ["B. Zhang", "L. Wang", "Z. Wang", "Y. Qiao", "H. Wang"], "venue": "CVPR, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks", "author": ["E. Ilg", "N. Mayer", "T. Saikia", "M. Keuper", "A. Dosovitskiy", "T. Brox"], "venue": "CVPR, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Beyond Short Snippets: Deep Networks for Video Classification", "author": ["J.Y.-H. Ng", "M. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "CVPR, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term Temporal Convolutions for Action Recognition", "author": ["G. Varol", "I. Laptev", "C. Schmid"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2017.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Local Video Feature for Action Recognition", "author": ["Z. Lan", "Y. Zhu", "A.G. Hauptmann"], "venue": "arXiv preprint arXiv:1701.07368, 2017.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Quantization: Encoding Convolutional Activations with Deep Generative Model", "author": ["Z. Qiu", "T. Yao", "T. Mei"], "venue": "CVPR, 2017.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "Action- VLAD: Learning Spatio-Temporal Aggregation for Action Classification", "author": ["R. Girdhar", "D. Ramanan", "A. Gupta", "J. Sivic", "B. Russell"], "venue": "CVPR, 2017.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2017}, {"title": "Back to Basics: Unsupervised Learning of Optical Flow via Brightness Constancy and Motion Smoothness", "author": ["J.J. Yu", "A.W. Harley", "K.G. Derpanis"], "venue": "arXiv preprint arXiv:1608.05842, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatial Transformer Network", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "NIPS, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "DeMoN: Depth and Motion Network for Learning Monocular Stereo", "author": ["B. Ummenhofer", "H. Zhou", "J. Uhrig", "N. Mayer", "E. Ilg", "A. Dosovitskiy", "T. Brox"], "venue": "CVPR, 2017.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": "CRCV-TR-12-01, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "HMDB: A Large Video Database for Human Motion Recognition", "author": ["H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "ICCV, 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR, 2009.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Next-Flow: Hybrid Multi-Tasking with Next-Frame Prediction to Boost Optical-Flow Estimation in the Wild", "author": ["N. Sedaghat"], "venue": "arXiv preprint arXiv:1612.03777, 2016.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency", "author": ["C. Godard", "O.M. Aodha", "G.J. Brostow"], "venue": "CVPR, 2017.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2017}, {"title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation", "author": ["N. Mayer", "E. Ilg", "P. Husser", "P. Fischer", "D. Cremers", "A. Dosovitskiy", "T. Brox"], "venue": "CVPR, 2016.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Optical Flow Estimation using a Spatial Pyramid Network", "author": ["A. Ranjan", "M.J. Black."], "venue": "CVPR, 2017.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2017}, {"title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset", "author": ["J. Carreira", "A. Zisserman"], "venue": "CVPR, 2017.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2017}, {"title": "Spatio-Temporal Vector of Locally Max Pooled Features for Action Recognition in Videos", "author": ["I.C. Duta", "B. Ionescu", "K. Aizawa", "N. Sebe"], "venue": "CVPR, 2017.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "Spatiotemporal Multiplier Networks for Video Action Recognition", "author": ["C. Feichtenhofer", "A. Pinz", "R.P. Wildes"], "venue": "CVPR, 2017.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2017}, {"title": "Spatiotemporal Pyramid Network for Video Action Recognition", "author": ["Y. Wang", "M. Long", "J. Wang", "P.S. Yu"], "venue": "CVPR, 2017.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2017}, {"title": "Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation", "author": ["C. Bailer", "B. Taetz", "D. Stricker"], "venue": "ICCV, 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep End2End Voxel2Voxel Prediction", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "CVPR Workshops, 2016.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient Two-Stream Motion and Appearance 3D CNNs for Video Classification", "author": ["A. Diba", "A.M. Pazandeh", "L.V. Gool"], "venue": "arXiv preprint arXiv:1608.08851, 2016.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "A Naturalistic Open Source Movie for Optical Flow Evaluation", "author": ["D.J. Butler", "J. Wulff", "G.B. Stanley", "M.J. Black"], "venue": "ECCV, 2012.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient Feature Extraction, Encoding and Classification for Action Recognition", "author": ["V. Kantorov", "I. Laptev"], "venue": "CVPR, 2014.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional Two- Stream Network Fusion for Video Action Recognition", "author": ["C. Feichtenhofer", "A. Pinz", "A. Zisserman"], "venue": "CVPR, 2016.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 135, "endOffset": 138}, {"referenceID": 8, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 140, "endOffset": 143}, {"referenceID": 9, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 219, "endOffset": 223}, {"referenceID": 11, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 225, "endOffset": 229}, {"referenceID": 12, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 231, "endOffset": 235}, {"referenceID": 13, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 304, "endOffset": 308}, {"referenceID": 14, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 310, "endOffset": 314}, {"referenceID": 15, "context": "We have moved from manually designed features [1], [2], [3], [4], [5] to learned convolutional neural network (CNN) features [6], [7], [8], [9]; from encoding appearance information to encoding motion information [10], [11], [12], [13]; and from learning local features to learning global video features [14], [15], [16],", "startOffset": 316, "endOffset": 320}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Even when using GPUs, optical flow calculation has been the major computational bottleneck of the current two-stream approaches [10], which learn to encode appearance and motion information in two separate CNNs.", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "We can perform weak supervision by using the optical flow calculated from traditional methods [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "estimation tasks are very different from models (filters) learned for other image processing tasks such as object recognition [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "Therefore, our optimization goal is more than just minimizing the endpoint errors (EPE) [20], [21], [22], [23].", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "Therefore, our optimization goal is more than just minimizing the endpoint errors (EPE) [20], [21], [22], [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "Therefore, our optimization goal is more than just minimizing the endpoint errors (EPE) [20], [21], [22], [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "Therefore, our optimization goal is more than just minimizing the endpoint errors (EPE) [20], [21], [22], [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "Through a set of specially designed operators and unsupervised loss functions, our new training step can generate optical flow that is similar to that generated by one of the best traditional methods [24].", "startOffset": 200, "endOffset": 204}, {"referenceID": 24, "context": "Significant advances in understanding human activities in video have been achieved over the past few years [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "Initially, traditional handcrafted features such as Improved Dense Trajectories (IDT) [1] dominated the field of video analysis for several years.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "IDT and its improvements [2], [3], [26], [27] are computationally formidable for real applications.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "IDT and its improvements [2], [3], [26], [27] are computationally formidable for real applications.", "startOffset": 30, "endOffset": 33}, {"referenceID": 25, "context": "IDT and its improvements [2], [3], [26], [27] are computationally formidable for real applications.", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "IDT and its improvements [2], [3], [26], [27] are computationally formidable for real applications.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "CNNs [7], [28], [6], which are often several orders of magnitude faster than IDTs, performed much worse than IDTs in the beginning.", "startOffset": 5, "endOffset": 8}, {"referenceID": 27, "context": "CNNs [7], [28], [6], which are often several orders of magnitude faster than IDTs, performed much worse than IDTs in the beginning.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "CNNs [7], [28], [6], which are often several orders of magnitude faster than IDTs, performed much worse than IDTs in the beginning.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "Later on, two-stream CNNs [10], [11] addressed this problem by precomputing the optical flow using traditional optical flow estimation methods [24] and training a separate CNN to encode the pre-computed optical flow.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "Later on, two-stream CNNs [10], [11] addressed this problem by precomputing the optical flow using traditional optical flow estimation methods [24] and training a separate CNN to encode the pre-computed optical flow.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "Later on, two-stream CNNs [10], [11] addressed this problem by precomputing the optical flow using traditional optical flow estimation methods [24] and training a separate CNN to encode the pre-computed optical flow.", "startOffset": 143, "endOffset": 147}, {"referenceID": 28, "context": "the temporal stream) significantly improved the accuracy of CNNs and finally allowed them to outperform IDTs on several benchmark action recognition datasets [29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 29, "context": "[30] proposed to use motion vectors, which can be obtained directly from compressed videos without extra calculation, to replace the more precise optical flow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "The encoded motion vectors lack fine structures, and contain noisy and inaccurate motion patterns, leading to much worse accuracy compared to the more precise optical flow [24].", "startOffset": 172, "endOffset": 176}, {"referenceID": 17, "context": "[18] used optical flow calculated by traditional methods as supervision to train a network to predict optical flow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] which uses the network trained on synthetic data where ground truth flow exists.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31]\u2019s work, they show", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] reduced the dimension of each frame/clip using a CNN and aggregated frame-level information using Long Short Term Memory (LSTM) networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] stated that Ng et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] experimented with sparse sampling and jointly trained on the sparsely sampled frames/clips.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] took a step forward along this line by using the networks of Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] to scan through the whole video, aggregate the features (output of a layer of the network) using pooling methods, and fine-tune the last layer of the network using the aggregated features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Recent approaches [16], [35], [36] evolve to end-to-end learning and are currently the best at incorporating global temporal information.", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "Recent approaches [16], [35], [36] evolve to end-to-end learning and are currently the best at incorporating global temporal information.", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "Recent approaches [16], [35], [36] evolve to end-to-end learning and are currently the best at incorporating global temporal information.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "We treat the optical flow estimation as an image reconstruction problem [37].", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "The inverse warping T is performed using a spatial transformer module [38].", "startOffset": 70, "endOffset": 74}, {"referenceID": 38, "context": "We also explored other techniques in the literature, like adding flow confidence [39] and multiplying with original color images TABLE II ARCHITECTURE OF TINY-MOTIONNET.", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "[31] during expanding, however, we did not observe improvements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Though our network structure is similar to a concurrent work [31], MotionNet is fundamentally different from FlowNet2.", "startOffset": 61, "endOffset": 65}, {"referenceID": 30, "context": "First, we perform unsupervised learning while [31] performs supervised learning for optical flow prediction.", "startOffset": 46, "endOffset": 50}, {"referenceID": 30, "context": "The model footprints of MotionNet and FlowNet2 [31] are 170M and 654M, and the prediction speeds are 370fps and 25fps, respectively.", "startOffset": 47, "endOffset": 51}, {"referenceID": 39, "context": "VGG16 [40] and ResNet50 [41] are popular network architectures from the object recognition field.", "startOffset": 6, "endOffset": 10}, {"referenceID": 40, "context": "VGG16 [40] and ResNet50 [41] are popular network architectures from the object recognition field.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "More specifically, as suggested in [10], we first clip the motions that are larger than 20 pixels to 20 pixels.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Simonyan and Zisserman [10] found that a stack of 10 flow fields achieves a much higher accuracy than only using a single flow field.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "Following the testing scheme of [10], [11], we evenly sample 25 frames/clips for each video.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "Following the testing scheme of [10], [11], we evenly sample 25 frames/clips for each video.", "startOffset": 38, "endOffset": 42}, {"referenceID": 41, "context": "We perform experiments on two widely used action recognition benchmarks, UCF101 [42] and HMDB51 [43].", "startOffset": 80, "endOffset": 84}, {"referenceID": 42, "context": "We perform experiments on two widely used action recognition benchmarks, UCF101 [42] and HMDB51 [43].", "startOffset": 96, "endOffset": 100}, {"referenceID": 43, "context": "For the CNNs, we use the Caffe toolbox [44].", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "For the TV-L1 optical flow, we use the OpenCV GPU implementation [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "\u03bb2 is set as suggested in [20] to make sure the losses are numerically on the same order.", "startOffset": 26, "endOffset": 30}, {"referenceID": 44, "context": "Unless otherwise specified, the spatial model is a VGG16 CNN pretrained on ImageNet challenges [45], and the temporal model is initialized with the snapshot provided by Wang et al.", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "TV-L1 [24] 85.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "FlowNet [20] 55.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "FlowNet2 [31] 79.", "startOffset": 9, "endOffset": 13}, {"referenceID": 45, "context": "NextFlow [46] 72.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "Enhanced Motion Vectors [30] 79.", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": "ActionFlowNet (2 frames)[18] 70.", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": "ActionFlowNet (16 frames)[18] 83.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "Two-Stream CNNs [10] 88.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Very Deep Two-Stream CNNs[11] 90.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "In terms of accuracy, our unsupervised MotionNet is competitive to TV-L1 while performing much better (4% \u223c 12% absolute improvement) than other ways of generating flows, including supervised training using synthetic data (FlowNet [20] and FlowNet2 [31]), and directly getting flows from", "startOffset": 231, "endOffset": 235}, {"referenceID": 30, "context": "In terms of accuracy, our unsupervised MotionNet is competitive to TV-L1 while performing much better (4% \u223c 12% absolute improvement) than other ways of generating flows, including supervised training using synthetic data (FlowNet [20] and FlowNet2 [31]), and directly getting flows from", "startOffset": 249, "endOffset": 253}, {"referenceID": 29, "context": "compressed videos (Enhanced Motion Vectors [30]).", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "ActionFlowNet [18] is what we denote as branched temporal stream.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "Among the two representative works we show, Two-Stream CNNs [10] is the earliest two-stream work and Very Deep Two-Stream CNNs [11] is the one we improve upon.", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "Among the two representative works we show, Two-Stream CNNs [10] is the earliest two-stream work and Very Deep Two-Stream CNNs [11] is the one we improve upon.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "Therefore, Very Deep TwoStream CNNs [11] is the most comparable work.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "We can see that our approach is about 1% worse than Very Deep TwoStream CNNs [11] in terms of accuracy but about 10x faster in terms of speed.", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "Original Spatial [11] 79.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "80 \u2212 Original Temporal [11] 85.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "42 \u2212 ActionFlowNet [18] 83.", "startOffset": 19, "endOffset": 23}, {"referenceID": 46, "context": "Similar observations can be found in [47] for unsupervised depth estimation.", "startOffset": 37, "endOffset": 41}, {"referenceID": 47, "context": "This strategy is designed to smooth the motion estimation [48].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "in [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Hence, we may need to design new operators like the correlation layer [20] or novel architectures [49] to learn motions between adjacent frames in future work.", "startOffset": 70, "endOffset": 74}, {"referenceID": 48, "context": "Hence, we may need to design new operators like the correlation layer [20] or novel architectures [49] to learn motions between adjacent frames in future work.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "Current state-of-the-art on the UCF101, HMDB51, ActivityNet, Sports-1M, and Kinetics datasets all adopt two-stream approaches [15], [16], [50], [51], [36], [52], [53].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "Current state-of-the-art on the UCF101, HMDB51, ActivityNet, Sports-1M, and Kinetics datasets all adopt two-stream approaches [15], [16], [50], [51], [36], [52], [53].", "startOffset": 132, "endOffset": 136}, {"referenceID": 49, "context": "Current state-of-the-art on the UCF101, HMDB51, ActivityNet, Sports-1M, and Kinetics datasets all adopt two-stream approaches [15], [16], [50], [51], [36], [52], [53].", "startOffset": 138, "endOffset": 142}, {"referenceID": 50, "context": "Current state-of-the-art on the UCF101, HMDB51, ActivityNet, Sports-1M, and Kinetics datasets all adopt two-stream approaches [15], [16], [50], [51], [36], [52], [53].", "startOffset": 144, "endOffset": 148}, {"referenceID": 35, "context": "Current state-of-the-art on the UCF101, HMDB51, ActivityNet, Sports-1M, and Kinetics datasets all adopt two-stream approaches [15], [16], [50], [51], [36], [52], [53].", "startOffset": 150, "endOffset": 154}, {"referenceID": 51, "context": "Current state-of-the-art on the UCF101, HMDB51, ActivityNet, Sports-1M, and Kinetics datasets all adopt two-stream approaches [15], [16], [50], [51], [36], [52], [53].", "startOffset": 156, "endOffset": 160}, {"referenceID": 52, "context": "Current state-of-the-art on the UCF101, HMDB51, ActivityNet, Sports-1M, and Kinetics datasets all adopt two-stream approaches [15], [16], [50], [51], [36], [52], [53].", "startOffset": 162, "endOffset": 166}, {"referenceID": 10, "context": "Top Section: We list the conventional spatial and temporal stream scores as in [11] for comparison.", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "We also refer to a concurrent work ActionFlowNet [18]1 to further demonstrate our conclusion because ActionFlowNet is a branched temporal stream but with a different network structure from ours.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "1We thank the authors [18] for providing their experiment results.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "Temporal segment networks (TSN) [15] is one such method.", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "The training and testing implementation details of TSN are the same as in [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 53, "context": "Although our MotionNet outperforms other end-to-end methods [54], [31], [30], [15] for video action recognition, we are still far from exploiting its full potential and perform 1% worse than the traditional optical flow estimation method (TVL1) [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 30, "context": "Although our MotionNet outperforms other end-to-end methods [54], [31], [30], [15] for video action recognition, we are still far from exploiting its full potential and perform 1% worse than the traditional optical flow estimation method (TVL1) [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 29, "context": "Although our MotionNet outperforms other end-to-end methods [54], [31], [30], [15] for video action recognition, we are still far from exploiting its full potential and perform 1% worse than the traditional optical flow estimation method (TVL1) [24].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Although our MotionNet outperforms other end-to-end methods [54], [31], [30], [15] for video action recognition, we are still far from exploiting its full potential and perform 1% worse than the traditional optical flow estimation method (TVL1) [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "Although our MotionNet outperforms other end-to-end methods [54], [31], [30], [15] for video action recognition, we are still far from exploiting its full potential and perform 1% worse than the traditional optical flow estimation method (TVL1) [24].", "startOffset": 245, "endOffset": 249}, {"referenceID": 17, "context": "Supervised optical flow learning methods [18], [23], [55], [56] show that using optical flow from traditional methods (which we call proxy ground truths) can help CNNs to learn to predict motions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "Supervised optical flow learning methods [18], [23], [55], [56] show that using optical flow from traditional methods (which we call proxy ground truths) can help CNNs to learn to predict motions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 54, "context": "Supervised optical flow learning methods [18], [23], [55], [56] show that using optical flow from traditional methods (which we call proxy ground truths) can help CNNs to learn to predict motions.", "startOffset": 53, "endOffset": 57}, {"referenceID": 55, "context": "Supervised optical flow learning methods [18], [23], [55], [56] show that using optical flow from traditional methods (which we call proxy ground truths) can help CNNs to learn to predict motions.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "We explore both TV-L1 [24] and FlowFields [54].", "startOffset": 22, "endOffset": 26}, {"referenceID": 53, "context": "We explore both TV-L1 [24] and FlowFields [54].", "startOffset": 42, "endOffset": 46}, {"referenceID": 56, "context": "This result is counter-intuitive because FlowFields is often much better than TV-L1 in optical flow estimation tasks [57].", "startOffset": 117, "endOffset": 121}, {"referenceID": 56, "context": "of learned flow, we test the three models on the well received MPI-Sintel benchmark [57].", "startOffset": 84, "endOffset": 88}, {"referenceID": 53, "context": "2For FlowFields, we use the binary kindly provided by authors in [54].", "startOffset": 65, "endOffset": 69}, {"referenceID": 57, "context": "Motion Vector + FV Encoding [58] 78.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "ActionFlowNet (2 frames) [18] 70.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "ActionFlowNet (16 frames) [18] 83.", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "C3D (1 Net) [6] 82.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "3 \u2212 C3D (3 Net) [6] 85.", "startOffset": 16, "endOffset": 19}, {"referenceID": 29, "context": "2 \u2212 Enhanced Motion Vector [30] 80.", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "2 \u2212 RGB + Enhanced Motion Vector [30] 86.", "startOffset": 33, "endOffset": 37}, {"referenceID": 55, "context": "4 \u2212 Two-Stream 3DNet [56] 90.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "2 \u2212 RGB Diff [15] 83.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "0 \u2212 RGB + RGB Diff [15] 86.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "8 \u2212 RGB + RGB Diff (TSN) [15] 91.", "startOffset": 25, "endOffset": 29}, {"referenceID": 30, "context": "The color scheme follows the standard flow field color coding in [31].", "startOffset": 65, "endOffset": 69}, {"referenceID": 57, "context": "This indicates that our learned motion representation is better than motion vectors [58], [30] and RGB differences [15] with respect to action recognition", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "This indicates that our learned motion representation is better than motion vectors [58], [30] and RGB differences [15] with respect to action recognition", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "This indicates that our learned motion representation is better than motion vectors [58], [30] and RGB differences [15] with respect to action recognition", "startOffset": 115, "endOffset": 119}, {"referenceID": 46, "context": "We could explore other well-designed smoothness terms like edge aware loss [47] or scale invariant gradient loss [39] to further improve", "startOffset": 75, "endOffset": 79}, {"referenceID": 38, "context": "We could explore other well-designed smoothness terms like edge aware loss [47] or scale invariant gradient loss [39] to further improve", "startOffset": 113, "endOffset": 117}, {"referenceID": 58, "context": "For example, we will also perform joint training of the spatial stream CNN and the stacked temporal stream CNN [59] instead of a simple late fusion.", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Analyzing videos of human actions involves understanding the temporal relationships among video frames. CNNs are the current state-of-the-art methods for action recognition in videos. However, the CNN architectures currently being used have difficulty in capturing these relationships. State-of-the-art action recognition approaches rely on traditional local optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We then plug it into a state-of-the-art action recognition framework called twostream CNNs. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than a two-stage one and maintains similar accuracy. Experimental results on UCF101 and HMDB51 datasets show that our approach significantly outperforms previous best real-time approaches.", "creator": "LaTeX with hyperref package"}}}