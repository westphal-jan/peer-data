{"id": "1704.00405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling", "abstract": "Traditional approaches to Semantic Role Labeling (SRL) depend heavily on manual feature engineering. Recurrent neural network (RNN) with long-short-term memory (LSTM) only treats sentence as sequence data and can not utilize higher level syntactic information. In this paper, we propose Syntax Aware LSTM (SA-LSTM) which gives RNN-LSTM ability to utilize higher level syntactic information gained from dependency relationship information. SA-LSTM also assigns different trainable weights to different types of dependency relationship automatically. Experiment results on Chinese Proposition Bank (CPB) show that, even without pre-training or introducing any other extra semantically annotated resources, our SA-LSTM model still outperforms the state of the art significantly base on Student's t-test ($p&lt;0.05$). Trained weights of types of dependency relationship form a stable and self-explanatory pattern.", "histories": [["v1", "Mon, 3 Apr 2017 02:10:19 GMT  (986kb,D)", "https://arxiv.org/abs/1704.00405v1", null], ["v2", "Thu, 20 Apr 2017 01:55:26 GMT  (268kb,D)", "http://arxiv.org/abs/1704.00405v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["feng qian", "lei sha", "baobao chang", "lu-chen liu", "ming zhang"], "accepted": false, "id": "1704.00405"}, "pdf": {"name": "1704.00405.pdf", "metadata": {"source": "CRF", "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling", "authors": ["Feng Qian", "Lei Sha", "Baobao Chang", "Lu-chen Liu", "Ming Zhang"], "emails": ["chbb}@pku.edu.cn", "cs}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The task of SRL is to recognize arguments of a given predicate in a sentence and assign semantic role designations. As SRL can provide a lot of semantic information and can help understand sentences, many NLP works use SRL information such as machine translation (Xiong et al., 2012; Aziz et al., 2011). Figure 1 shows an example of SRL tasks from the Chinese Proposition Bank 1.0 (Xue and Palmer, 2003). Traditional methods for SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2010) to perform classifications according to manually designed features. Recent work based on recurring neural networks (Collobert and Weston, 2008; Zhou and Xu, 2015) is comparable."}, {"heading": "2 Syntax Aware LSTM", "text": "Compared to the traditional feature engineering method, RNN-LSTM eases the burden of manual feature design and selection. But ar Xiv: 170 4.00 405v 2 [cs.C L] 20 Apr 201 7Most RNN-LSTM-based methods have not managed to use the dependency analysis relationship. Based on biRNN-LSTM, we suggest SA-LSTM, which retains all the benefits of bi-RNN-LSTM while being able to directly model dependency analysis."}, {"heading": "2.1 Conventional bi-LSTM Model for SRL", "text": "In a sentence, each word wt has a property representation xt, which automatically appears as (Wang et al., 2015) did. zt is a property representation for wt, calculated as follows: zt = f (W1xt) (1), where W1-Rn1 \u00b7 n0. n0 is the length of the word character representation. In a sentence, each word has six internal vectors, C-Rn1, gi, gf, go, Ct and ht, represented in Equation 2: C-Rn1 \u00b7 n0 (Wczt + Ucht \u2212 1 + bc) gj = \u03c3 (Wjzt + Ujht \u2212 1 + bj) j-i, f, o-Ct = gi-Ct \u2212 1 ht = go f (Ct) (2), where C-Rnh is the candidate value of the current cell state. g are gates used to control the information flow. Ct is the hidden state of wt. Wx and Urix-Rn-R are used."}, {"heading": "2.2 Syntax Aware LSTM Model for SRL", "text": "The structure of our SA-LSTM is shown in Figure 3. The most significant change we make to the original RNN-LSTM is shown in the shaded range.St is the syntax information entered into the current cell and is calculated as shown in Eq.6: St = f (t \u2212 1 \u2211 i = 0 \u03b1 \u00b7 hi) (6) \u03b1 = 1 If a dependency relationship between wi and wt is 0 Otherwise (7) St is the weighted sum of all hidden state vectors hi originating from previous words wi. To protect the original sentence information from dilution by St (Wu et al., 2016), we add St to the hidden layer vector instead of adding Ct to the cell state."}, {"heading": "2.3 Training Criteria", "text": "To train our model, we use the criterion of maximum probability. To optimize the parameters, the stochastic gradient ascension algorithm is used. Global normalization is applied. In the case of a training pair T = (x, y), where T is the current training pair, x denotes the current training set, and y is the corresponding correct response path. yt = k means that the t-th word has the k-th semantic role designation. Thus, the score of ot is calculated as: s (x, y, \u03b8) = Ni = 1 otyt (11), where Ni is the word number of the current sentence and \u03b8 stands for all parameters. Thus, the probability of a single sentence is log p (y | x, \u03b8) = log exp (s (x, y, \u03b8) = log exp (s) = y \u2032 exp (s, y, \u03b8)."}, {"heading": "3 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experiment setting", "text": "To compare with previous Chinese SRL work, we opt for experiments with CPB 1.0. We also follow the same data settings as previous Chinese SRL work (Xue, 2008; Sun et al., 2009). Presented 1-word embeddings are tested on SA-LSTM and show improvements. We use Stanford parsers (Chen and Manning, 2014) to get dependency analysis information that now supports Universal Dependency Representation in Chinese."}, {"heading": "3.2 Syntax Aware LSTM Performance", "text": "To prove that SA-LSTM achieves more improvements through the new SA-LSTM architecture than through the additionally introduced analysis information, we1Trained by word2vec on Chinese Gigaword Corpus 2All Experiment Code and related files are available on re-questdesign an experiment that takes dependency relationships into account in traditional feature engineering.In the face of a word wt, Ft is the average of all dependency-related xi of previous words wi, as shown in Eq.13: Ft = 1T t \u2212 1 \u2211 i = 0 \u03b1 \u00d7 xi (13), where T is the number of dependency-related words and \u03b1 is a 0-1 variable calculated as in equation.Then Ft is linked to xt to form a new feature representation F1. In this way, we model dependence when exchanging information in a conventional feature engineering manner. Then we feed this new feature representation into ordinary bi-LM-STF1 data representation with a random value of 1.775% and an ST764% as shown in a ST764% of the ST764-M table."}, {"heading": "3.3 Visualization of Trained Weights", "text": "According to Eq.10, the influence of a single type of dependency relationship is multiplied by the type weight \u03b1m. If \u03b1m is 0, the influence of this type of dependency relationship is completely ignored. If the weight is greater, the type of dependency relationship will have more influence on the entire system.As shown in Figure 4, the dependency type dobj gets the highest weight after training, as the red bar shows. According to grammar knowledge, dobj should be an informative relationship for the SRL task, and our system automatically gives dobj the greatest influence. This example also shows that the result of SA-LSTM is highly consistent with the grammar knowledge that SA-LSTM further confirms."}, {"heading": "4 Related works", "text": "Semantic role marking (SRL) was first defined by (Gildea and Jurafsky, 2002). Early work (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL yielded promising results without a large commented SRL corpus. Xue and Palmer established the Chinese Proposition Bank (Xue and Palmer, 2003) to standardize Chinese SRL research. Traditional work such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can incorporate information in feature engineering ways, such as syntactical path properties. However, they suffer from severe manual design stress and data sparity problems. Recent SRL work often uses neural network-based methods. Collobert and Weston (2008) suggested a RONS-based neural network method."}, {"heading": "5 Conclusion", "text": "We propose the Syntax Aware LSTM model for semantic role designation in China. SA-LSTM is able to model dependency information directly in an architectural way. We have experimentally demonstrated that SA-LSTM achieves more improvements through the SA-LSTM architecture than through the input of additional dependency information. We increase the state of the art to 79.64%, which significantly exceeds the state of the art according to the Student t-Test (p < 0.05)."}], "references": [{"title": "Shallow semantic trees for smt", "author": ["Wilker Aziz", "Miguel Rios", "Lucia Specia."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, pages 316\u2013322.", "citeRegEx": "Aziz et al\\.,? 2011", "shortCiteRegEx": "Aziz et al\\.", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP. pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "An empirical study of chinese chunking", "author": ["Wenliang Chen", "Yujie Zhang", "Hitoshi Isahara."], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics, pages 97\u2013104.", "citeRegEx": "Chen et al\\.,? 2006", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Improving chinese semantic role classification with hierarchical", "author": ["Weiwei Ding", "Baobao Chang"], "venue": null, "citeRegEx": "Ding and Chang.,? \\Q2008\\E", "shortCiteRegEx": "Ding and Chang.", "year": 2008}, {"title": "Word based chinese semantic role labeling with semantic chunking", "author": ["Weiwei Ding", "Baobao Chang."], "venue": "International Journal of Computer Processing Of Languages 22(02n03):133\u2013154.", "citeRegEx": "Ding and Chang.,? 2009", "shortCiteRegEx": "Ding and Chang.", "year": 2009}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational linguistics 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Semantic role chunking combining complementary syntactic views", "author": ["Sameer Pradhan", "Kadri Hacioglu", "Wayne Ward", "James H Martin", "Daniel Jurafsky."], "venue": "Proceedings of the Ninth Conference on Computational Natural Language Learning. As-", "citeRegEx": "Pradhan et al\\.,? 2005", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "The importance of syntactic parsing and inference in semantic role labeling", "author": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih."], "venue": "Computational Linguistics 34(2):257\u2013287.", "citeRegEx": "Punyakanok et al\\.,? 2008", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Neural semantic role labeling with dependency path embeddings", "author": ["Michael Roth", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1605.07515 .", "citeRegEx": "Roth and Lapata.,? 2016", "shortCiteRegEx": "Roth and Lapata.", "year": 2016}, {"title": "Capturing argument relationships for chinese semantic role labeling", "author": ["Lei Sha", "Tingsong Jiang", "Sujian Li", "Baobao Chang", "Zhifang Sui."], "venue": "EMNLP. pages 2011\u20132016.", "citeRegEx": "Sha et al\\.,? 2016", "shortCiteRegEx": "Sha et al\\.", "year": 2016}, {"title": "Shallow semantic parsing of chinese", "author": ["Honglin Sun", "Daniel Jurafsky."], "venue": "Proceedings of NAACL 2004. pages 249\u2013256.", "citeRegEx": "Sun and Jurafsky.,? 2004", "shortCiteRegEx": "Sun and Jurafsky.", "year": 2004}, {"title": "Improving chinese semantic role labeling with rich syntactic features", "author": ["Weiwei Sun."], "venue": "Proceedings of the ACL 2010 conference short papers. Association for Computational Linguistics, pages 168\u2013172.", "citeRegEx": "Sun.,? 2010", "shortCiteRegEx": "Sun.", "year": 2010}, {"title": "Chinese semantic role labeling with shallow parsing", "author": ["Weiwei Sun", "Zhifang Sui", "Meng Wang", "Xin Wang."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3. Association for Compu-", "citeRegEx": "Sun et al\\.,? 2009", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Chinese semantic role labeling with bidirectional recurrent neural networks", "author": ["Zhen Wang", "Tingsong Jiang", "Baobao Chang", "Zhifang Sui."], "venue": "EMNLP. pages 1626\u20131631.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "An empirical exploration of skip connections for sequential tagging", "author": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.03167 .", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Modeling the translation of predicate-argument structure", "author": ["Deyi Xiong", "Min Zhang", "Haizhou Li"], "venue": null, "citeRegEx": "Xiong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2012}, {"title": "Labeling chinese predicates with semantic roles", "author": ["Nianwen Xue."], "venue": "Computational linguistics 34(2):225\u2013255.", "citeRegEx": "Xue.,? 2008", "shortCiteRegEx": "Xue.", "year": 2008}, {"title": "Annotating the propositions in the penn chinese treebank", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17. Association for Computational Linguistics, pages 47\u201354.", "citeRegEx": "Xue and Palmer.,? 2003", "shortCiteRegEx": "Xue and Palmer.", "year": 2003}, {"title": "Automatic semantic role labeling for chinese verbs", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "IJCAI. Citeseer, volume 5, pages 1160\u20131165.", "citeRegEx": "Xue and Palmer.,? 2005", "shortCiteRegEx": "Xue and Palmer.", "year": 2005}, {"title": "Multipredicate semantic role labeling", "author": ["Haitong Yang", "Chengqing Zong"], "venue": "In EMNLP", "citeRegEx": "Yang and Zong,? \\Q2014\\E", "shortCiteRegEx": "Yang and Zong", "year": 2014}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Since SRL can give a lot of semantic information, and can help in sentence understanding, a lot of NLP works such as machine translation(Xiong et al., 2012; Aziz et al., 2011) use SRL information.", "startOffset": 136, "endOffset": 175}, {"referenceID": 0, "context": "Since SRL can give a lot of semantic information, and can help in sentence understanding, a lot of NLP works such as machine translation(Xiong et al., 2012; Aziz et al., 2011) use SRL information.", "startOffset": 136, "endOffset": 175}, {"referenceID": 18, "context": "0)(Xue and Palmer, 2003).", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "Traditional methods on SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) to do classification according to manually designed features.", "startOffset": 87, "endOffset": 161}, {"referenceID": 17, "context": "Traditional methods on SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) to do classification according to manually designed features.", "startOffset": 87, "endOffset": 161}, {"referenceID": 12, "context": "Traditional methods on SRL use statistical classifiers such as CRF, MaxEntropy and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) to do classification according to manually designed features.", "startOffset": 87, "endOffset": 161}, {"referenceID": 3, "context": "Recent works based on recurrent neural network (Collobert and Weston, 2008; Zhou and Xu, 2015; Wang et al., 2015) extract features automatically, and outperform traditional methods significantly.", "startOffset": 47, "endOffset": 113}, {"referenceID": 21, "context": "Recent works based on recurrent neural network (Collobert and Weston, 2008; Zhou and Xu, 2015; Wang et al., 2015) extract features automatically, and outperform traditional methods significantly.", "startOffset": 47, "endOffset": 113}, {"referenceID": 14, "context": "Recent works based on recurrent neural network (Collobert and Weston, 2008; Zhou and Xu, 2015; Wang et al., 2015) extract features automatically, and outperform traditional methods significantly.", "startOffset": 47, "endOffset": 113}, {"referenceID": 17, "context": "However, RNN methods treat language as sequence data, so most of them fail to take tree structured parsing information into account, which is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 176, "endOffset": 234}, {"referenceID": 8, "context": "However, RNN methods treat language as sequence data, so most of them fail to take tree structured parsing information into account, which is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 176, "endOffset": 234}, {"referenceID": 7, "context": "However, RNN methods treat language as sequence data, so most of them fail to take tree structured parsing information into account, which is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 176, "endOffset": 234}, {"referenceID": 14, "context": "1 Conventional bi-LSTM Model for SRL In a sentence, each word wt has a feature representation xt which is generated automatically as (Wang et al., 2015) did.", "startOffset": 133, "endOffset": 152}, {"referenceID": 15, "context": "To protect the original sentence information from being diluted(Wu et al., 2016) by St, we add St to hidden layer vector ht instead of adding to cell state Ct, as shown in Equation 9:", "startOffset": 63, "endOffset": 80}, {"referenceID": 17, "context": "We also follow the same data setting as previous Chinese SRL work(Xue, 2008; Sun et al., 2009) did.", "startOffset": 65, "endOffset": 94}, {"referenceID": 13, "context": "We also follow the same data setting as previous Chinese SRL work(Xue, 2008; Sun et al., 2009) did.", "startOffset": 65, "endOffset": 94}, {"referenceID": 1, "context": "We use Stanford Parser(Chen and Manning, 2014) to get dependency parsing information, which now supports Universal Dependency representation in Chinese.", "startOffset": 22, "endOffset": 46}, {"referenceID": 13, "context": "Method F1% Xue(2008) 71.", "startOffset": 11, "endOffset": 21}, {"referenceID": 11, "context": "90 Sun et al.(2009) 74.", "startOffset": 3, "endOffset": 20}, {"referenceID": 11, "context": "90 Sun et al.(2009) 74.12 Yand and Zong(2014) 75.", "startOffset": 3, "endOffset": 46}, {"referenceID": 11, "context": "90 Sun et al.(2009) 74.12 Yand and Zong(2014) 75.31 Wang et al.(2015)(Random Initialized) 77.", "startOffset": 3, "endOffset": 70}, {"referenceID": 10, "context": "09 Sha et al.(2016) 77.", "startOffset": 3, "endOffset": 20}, {"referenceID": 14, "context": "Wang et al. (2015) used bi-LSTM without parsing information and got 77.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Semantic role labeling (SRL) was first defined by (Gildea and Jurafsky, 2002).", "startOffset": 50, "endOffset": 77}, {"referenceID": 6, "context": "Early works(Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL got promising result without large annotated SRL corpus.", "startOffset": 11, "endOffset": 62}, {"referenceID": 11, "context": "Early works(Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL got promising result without large annotated SRL corpus.", "startOffset": 11, "endOffset": 62}, {"referenceID": 18, "context": "Xue and Palmer built the Chinese Proposition Bank(Xue and Palmer, 2003) to standardize Chinese SRL research.", "startOffset": 49, "endOffset": 71}, {"referenceID": 19, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 17, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 5, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 13, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 2, "context": "Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods.", "startOffset": 26, "endOffset": 137}, {"referenceID": 2, "context": ", 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can take parsing information into account in feature engineering way, such as syntactic path feature. However, they suffer from heavy manually feature design workload, and data sparsity problem. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a convolutional neural network method for SRL.", "startOffset": 8, "endOffset": 384}, {"referenceID": 2, "context": ", 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can take parsing information into account in feature engineering way, such as syntactic path feature. However, they suffer from heavy manually feature design workload, and data sparsity problem. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a convolutional neural network method for SRL. Zhou and Xu (2015) proposed bidirectional RNN-LSTM method for English SRL, and Wang et al.", "startOffset": 8, "endOffset": 459}, {"referenceID": 2, "context": ", 2009; Chen et al., 2006; Yang et al., 2014) use feature engineering methods. Traditional methods can take parsing information into account in feature engineering way, such as syntactic path feature. However, they suffer from heavy manually feature design workload, and data sparsity problem. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a convolutional neural network method for SRL. Zhou and Xu (2015) proposed bidirectional RNN-LSTM method for English SRL, and Wang et al. (2015) proposed a bi-RNN-", "startOffset": 8, "endOffset": 538}, {"referenceID": 17, "context": "However, most NN based methods can not utilize parsing information which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 128, "endOffset": 186}, {"referenceID": 8, "context": "However, most NN based methods can not utilize parsing information which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 128, "endOffset": 186}, {"referenceID": 7, "context": "However, most NN based methods can not utilize parsing information which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005).", "startOffset": 128, "endOffset": 186}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al.", "startOffset": 8, "endOffset": 66}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as ours, but in feature engineering way.", "startOffset": 8, "endOffset": 88}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as ours, but in feature engineering way. Roth and Lapata (2016) embed dependency parsing path into feature representations using LSTM.", "startOffset": 8, "endOffset": 177}, {"referenceID": 7, "context": ", 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as ours, but in feature engineering way. Roth and Lapata (2016) embed dependency parsing path into feature representations using LSTM. Sha et al. (2016) use dependency parsing information as feature to do argument relationships classification.", "startOffset": 8, "endOffset": 266}], "year": 2017, "abstractText": "As for semantic role labeling (SRL) task, when it comes to utilizing parsing information, both traditional methods and recent recurrent neural network (RNN) based methods use the feature engineering way. In this paper, we propose Syntax Aware Long Short Time Memory(SALSTM). The structure of SA-LSTM modifies according to dependency parsing information in order to model parsing information directly in an architecture engineering way instead of feature engineering way. We experimentally demonstrate that SA-LSTM gains more improvement from the model architecture. Furthermore, SALSTM outperforms the state-of-the-art on CPB 1.0 significantly according to Student t-test (p < 0.05).", "creator": "LaTeX with hyperref package"}}}