{"id": "1704.00440", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Combining Lexical and Syntactic Features for Detecting Content-dense Texts in News", "abstract": "Content-dense news report important factual information about an event in direct, succinct manner. Information seeking applications such as information extraction, question answering and summarization normally assume all text they deal with is content-dense. Here we empirically test this assumption on news articles from the business, U.S. international relations, sports and science journalism domains. Our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector. We heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features. On manually annotated data, we compare the performance of domain-specific classifiers, trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together. Our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label. Domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer. Domain independent classifiers reproduce better naive crowdsourced judgements. Classification prediction is high across all conditions, around 80%.", "histories": [["v1", "Mon, 3 Apr 2017 06:22:04 GMT  (1880kb,D)", "http://arxiv.org/abs/1704.00440v1", "In submission to JAIR"]], "COMMENTS": "In submission to JAIR", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yinfei yang", "ani nenkova"], "accepted": false, "id": "1704.00440"}, "pdf": {"name": "1704.00440.pdf", "metadata": {"source": "CRF", "title": "Combining Lexical and Syntactic Features for Detecting Content-dense Texts in News\u2217", "authors": ["Yinfei Yang", "Ani Nenkova"], "emails": ["yangyin7@gmail.com", "nenkova@seas.upenn.edu"], "sections": [{"heading": null, "text": "In this manuscript, we have expanded the work further by using much larger samples of New York Times articles for educational purposes and analyzing the performance of a larger number of two-layered classifiers. At this point, we also present a new collection of manually annotated test data for both domain-dependent and general density of text content. We use this collection to provide detailed evaluation of the density classifiers. By publishing this manuscript, we will make the classifiers and data available for use by others. In our paper AAI 2014, we use the term information density to describe the types of text we want to discover. Here, we switch terminology to density of content to avoid confusion with the work at the intersection of cognitive science and computational linguistics."}, {"heading": "1. Introduction", "text": "Others aim to provide background information that relates to an event and is necessary to understand an event but is not newsworthy in itself; others seek to entertain the reader or highlight the author's brilliant command of the language and wit. In this paper, we present the task of determining whether a text is dense in content or not. Content-dense news reports provide important factual information about an event, in a direct and concise manner. Prototypical examples of content-dense texts are news articles that are usually perfect answers to a \"question grounded in a specific event.\" In general news, however, content-dense text is not the norm. We base our analysis on the opening paragraph, known as the lead or lede, which is drawn from news articles in the New York Times."}, {"heading": "2. Corpus", "text": "The data for our experiments comes from the New York Times (NYT) annotated corpus (LDC Catalog No. LDC2008T19), which contains 20-year-old editions of the NYT, along with rich metadata about the newspaper section in which the article appeared, and summaries prepared by information scientists for many of the articles. Headings of the articles are explicitly marked in the corpus, making extracting the relevant text for further analysis straightforward. In our previous proof-of-concept work (Yang & Nenkova, 2014), we selected a subcorpus of articles published in 2005 or 2006 from four different genres (economics, international relations, science, and sport), and given the selection criteria, the data in previous work included significantly fewer articles from science and sports, compared to the other two areas where insight into these two areas could not be given."}, {"heading": "2.1 Training set heuristic", "text": "To automatically mark leads as content-dense or not, we use the manual summaries that accompany many articles in the NYT Corpus. For articles with content-dense leads, the manual summary will be very similar to the lead itself, since this type of lead by definition provides a fact-based summary of the article. For leads that merely try to engage the reader with more creative tools, the manual summary is very different from the lead. Overall, the similarity between lead and manual summary provides a strong indication of the importance and factual, event-oriented nature of the information expressed in the lead. For articles with manual summaries of at least 25 words, we calculate a content-dense score. For each word in the summary, a tuple t (w, pos) is created that contains the word and its part of the speech, and the score is calculated as follows: Score = # of t (w), pos also in Leads # t (1)."}, {"heading": "2.2 Label analysis", "text": "Table 1 shows details of the number of all NYT articles from each of the four domains. The first column shows the number of articles in the NYT from the given domain. The second column shows the number of articles used to train domain-dependent classifiers (we explain the selection process below). Overall, however, only about a third of the articles are associated with manual summaries. The distribution of the content-dense ratings, which are assigned as a function of overlapping with human summaries, is shown in Figure 1. In the division, the distribution of ratings is almost uniform, reflecting the fact that there are articles in this section about important events - mergers, unexpected price changes, product announcements, and litigation - but also non-event-specific analyses of current trends, minor events such as auctions and person-centric pieces about prominent businessmen and women.In sports and science, the distribution of content-dense ratings is clearly oriented towards the non-content-dense domain."}, {"heading": "3. Methodology", "text": "In our previous experiments (Yang & Nenkova, 2014), we found that lexical characteristics are well suited to the task, especially lexical representations, which are determined independently of the training data. In addition, lexicalized syntactic representations also lead to remarkably good results. A number of other representations with which we experimented did not seem to be particularly advantageous for the task. Motivated by these results, we are extensively examining the lexical representations and the unleashed syntactic representation, and exploring ways to combine the predictions of these models to achieve even better accuracy."}, {"heading": "3.1 Features", "text": "This year, it has come to the point where it can only take a year to find a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is capable of finding a solution that is capable of finding a solution. \""}, {"heading": "3.2 Classifier combination", "text": "The three characteristic representations that we have introduced capture domain independent lexical notes for content density, domain-dependent indicators for important events and general style of writing captured by the structure of the sentences in the text. We train a support vector logistic regression classifier with each class of characteristics individually. In addition, in this section we examine two approaches for combining predictions from the three classes of characteristic characteristics. The number of entries in the characteristic vector is equal to the sum of the characteristics of the MRC, mutual information and production representations. Then we train a logistic regression model based on the associated characteristic representations in a characteristic vector. We link the three characteristic representations together in a characteristic vector. The number of entries in the characteristic vector is equal to the sum of the characteristics of the MRC, mutual information and production representations. Then we train a logistic regression model based on the concatenated representation of the characteristic."}, {"heading": "4. Evaluation on automatic annotations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Classifier evaluation", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "4.2 Combining classifiers with different representations", "text": "Here we evaluate various possible combinations of trait types. We compare these possibilities for decision-level combination that we have already established better than functional-level combinations. The motivation to examine trait combinations is that not all traits are available in all applications. In addition, concerns about runtime may make syntactic traits undesirable in certain constellations where syntactic analysis may not be practicable. Mutual information representations also require larger training data for each sphere of interest to calculate the mutual weights for each trait. Therefore, we investigate the effectiveness of combining different trait classes. The multi-layer structure makes decision-level merger easier to add or remove traits. Developers can simply form a classifier based on new traits and then add them to the second layer without compromising existing individual trait classifiers. We show the results from the evaluation of three different trait combinations in first-level MRC + (MRC + independent traits) and MRC (MRC + independent traits)."}, {"heading": "4.3 Is the training data enough?", "text": "We will now discuss the impact of the size of the training set on the performance of the classifier. We will evaluate the relationship between the accuracy of the classifier and the increase in the number of training instances for each domain. We will start with a training set of 100 articles, which will grow to 6,500 cases in the training data, increasing the training set of 100 randomly selected articles in each step. Accuracy will be calculated on the same test set for each domain. As in our previous experiments, a 10x cross validation will be performed for each domain. Among the four genres, there is a special test set, which means that all cross validation sizations will use the same test set. The reported results are an average of the accuracy on the specified test set in each domain. Figure 3 shows the accuracy / size curve for each domain. Among the four genres, the combination of all three characteristics at the decision level exhibits the highest accuracy. Accuracy increases rapidly with the increase in training data, when the number of training items increases further than 2,000, but the decision combination is very precise."}, {"heading": "5. Evaluation on human annotations", "text": "So far, we have found that identifying high-density texts can be very accurate if the lead is identified by intuitive heuristics on the available articles / summary resources. However, we also want to test the models using manually annotated data to ensure that the predictions are actually consistent with the reader's perception of the article. We selected a total of 1,000 articles and divided them into two sentences. For the first set of 400 articles, the authors of the paper commented on the content-dense labels and provided a real-value score for the domain-dependent content density of each text. Subsequently, a second set of 600 articles was selected and commented on Amazon Mechanic Turk (AMT). All commented articles were randomly selected from the NYT data and did not appear in the training data of the classifiers we evaluate here."}, {"heading": "5.1 Human annotated dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Basic set", "text": "The authors of the study agree that the issue is one that has come to the fore in recent years: a problem that has come to a head in recent years, and a problem that has come to a head in recent years. \"The authors of the study agree that it is a problem:\" It is not that it is a problem, but rather a problem that has come to a head in recent years. \"The authors agree that it is a problem that is primarily a problem that needs to be addressed."}, {"heading": "5.1.2 AMT annotation set", "text": "This year, it has reached the point where it is a kind of infinite forest in which people find themselves in the forests of the world."}, {"heading": "5.2 Are leads informative?", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "5.3 Classifier evaluation", "text": "This year is the highest in the history of the country."}, {"heading": "6. Recognizing better summaries", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country,"}, {"heading": "7. Conclusion and future work", "text": "We use article / summary pairs from the NYT corpus to mark a large number of articles heuristically as content-dense when the article's title overlaps strongly with the human summary, and as non-content-dense when the overlap is low. We present experiments with two lexical representations and a syntactical representation. The syntactical representation production rule is the best predictor of lead content density among the three. Corpus-independent lexical representation from a vocabulary defined by the MRC lexicon has proven to be the more useful lexical representation. We compare a feature-combination model and a two-layer combination model at the decision level. The latter performs best in all of our experiments. Our analysis shows that there is wide variation in news domains where the content-dense leads and predictability that can be achieved."}, {"heading": "Appendix A. Reference leads used in AMT annotations", "text": "This year, the time has come for only one person to be able to establish himself in the region."}, {"heading": "Appendix B. Production Rules with Highest Weights", "text": "In this section, we list the production rules with the highest weights for each genre. We also show two examples for each production rule. The examples are extracted from guidelines with the Stanford CoreNLP package."}], "references": [{"title": "Finding highquality content in social media", "author": ["E. Agichtein", "C. Castillo", "D. Donato", "A. Gionis", "G. Mishne"], "venue": "In Proceedings of the 2008 International Conference on Web Search and Data Mining,", "citeRegEx": "Agichtein et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Agichtein et al\\.", "year": 2008}, {"title": "Beyond trending topics: Real-world event identification on twitter", "author": ["H. Becker", "M. Naaman", "L. Gravano"], "venue": null, "citeRegEx": "Becker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2011}, {"title": "Jointly learning to extract and compress", "author": ["T. Berg-Kirkpatrick", "D. Gillick", "D. Klein"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "Early feature stream integration versus decision level combination in a multiple classifier system for text line recognition", "author": ["R. Bertolami", "H. Bunke"], "venue": "In Pattern Recognition,", "citeRegEx": "Bertolami and Bunke,? \\Q2006\\E", "shortCiteRegEx": "Bertolami and Bunke", "year": 2006}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Success with style: Using writing style to predict the success of novels", "author": ["V. Ganjigunte Ashok", "S. Feng", "Y. Choi"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ashok et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ashok et al\\.", "year": 2013}, {"title": "A scalable global model for summarization", "author": ["D. Gillick", "B. Favre"], "venue": "In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,", "citeRegEx": "Gillick and Favre,? \\Q2009\\E", "shortCiteRegEx": "Gillick and Favre", "year": 2009}, {"title": "A global optimization framework for meeting summarization", "author": ["D. Gillick", "K. Riedhammer", "B. Favre", "D. Hakkani-Tur"], "venue": "In 2009 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "Redundancy and reduction: Speakers manage syntactic information density", "author": ["T.F. Jaeger"], "venue": "Cognitive psychology,", "citeRegEx": "Jaeger,? \\Q2010\\E", "shortCiteRegEx": "Jaeger", "year": 2010}, {"title": "A coherence model based on syntactic patterns", "author": ["A. Louis", "A. Nenkova"], "venue": "In Proceedings of 2012 Joint Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Louis and Nenkova,? \\Q2012\\E", "shortCiteRegEx": "Louis and Nenkova", "year": 2012}, {"title": "What makes writing great? first experiments on article quality prediction in the science journalism domain. TACL", "author": ["A. Louis", "A. Nenkova"], "venue": null, "citeRegEx": "Louis and Nenkova,? \\Q2013\\E", "shortCiteRegEx": "Louis and Nenkova", "year": 2013}, {"title": "Chinese native language identification", "author": ["S. Malmasi", "M. Dras"], "venue": "In Proceedings of EACL,", "citeRegEx": "Malmasi and Dras,? \\Q2014\\E", "shortCiteRegEx": "Malmasi and Dras", "year": 2014}, {"title": "Summac: a text summarization evaluation", "author": ["I. Mani", "G. Klein", "D. House", "L. Hirschman", "T. Firmin", "B. Sundheim"], "venue": "Natural Language Engineering,", "citeRegEx": "Mani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2002}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "In Association for Computational Linguistics (ACL) System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Decision level combination of multiple modalities for recognition and analysis of emotional expression", "author": ["A. Metallinou", "S. Lee", "S. Narayanan"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Metallinou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Metallinou et al\\.", "year": 2010}, {"title": "Automatic text summarization of newswire: Lessons learned rom the document understanding conference", "author": ["A. Nenkova"], "venue": "In AAAI,", "citeRegEx": "Nenkova,? \\Q2005\\E", "shortCiteRegEx": "Nenkova", "year": 2005}, {"title": "Can you summarize this? identifying correlates of input difficulty for multi-document summarization", "author": ["A. Nenkova", "A. Louis"], "venue": "ACL", "citeRegEx": "Nenkova and Louis,? \\Q2008\\E", "shortCiteRegEx": "Nenkova and Louis", "year": 2008}, {"title": "Talkers account for listener and channel characteristics to communicate efficiently", "author": ["J.K. Pate", "S. Goldwater"], "venue": "Journal of Memory and Language,", "citeRegEx": "Pate and Goldwater,? \\Q2015\\E", "shortCiteRegEx": "Pate and Goldwater", "year": 2015}, {"title": "Explicit and implicit syntactic features for text classification", "author": ["M. Post", "S. Bergsma"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Post and Bergsma,? \\Q2013\\E", "shortCiteRegEx": "Post and Bergsma", "year": 2013}, {"title": "Multimodal subjectivity analysis of multiparty conversation", "author": ["S. Raaijmakers", "K. Truong", "T. Wilson"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Raaijmakers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raaijmakers et al\\.", "year": 2008}, {"title": "Review of classifier combination methods", "author": ["S. Tulyakov", "S. Jaeger", "V. Govindaraju", "D. Doermann"], "venue": "Machine Learning in Document Analysis and Recognition. Informatica", "citeRegEx": "Tulyakov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tulyakov et al\\.", "year": 2008}, {"title": "Improving data driven wordclass tagging by system combination", "author": ["H. van Halteren", "J. Zavrel", "W. Daelemans"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume", "citeRegEx": "Halteren et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Halteren et al\\.", "year": 1998}, {"title": "The mrc psycholinguistic database: Machine readable dictionary", "author": ["M. Wilson"], "venue": "Behavioural Research Methods, Instruments and Computer,", "citeRegEx": "Wilson,? \\Q1988\\E", "shortCiteRegEx": "Wilson", "year": 1988}, {"title": "Detecting information-dense texts in multiple news domains", "author": ["Y. Yang", "A. Nenkova"], "venue": "In Proceedings of Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Yang and Nenkova,? \\Q2014\\E", "shortCiteRegEx": "Yang and Nenkova", "year": 2014}, {"title": "Towards answering opinion questions: separating facts from opinions and identifying the polarity of opinion sentences", "author": ["H. Yu", "V. Hatzivassiloglou"], "venue": "In Proceedings of the 2003 conference on Empirical methods in natural language processing,", "citeRegEx": "Yu and Hatzivassiloglou,? \\Q2003\\E", "shortCiteRegEx": "Yu and Hatzivassiloglou", "year": 2003}, {"title": "Balding, one of three British brothers who gained international fame as polo stars in the 1930\u2019s, when the sport attracted large crowds and wide press coverage, died on Thursday at his home", "author": ["G. Ivor"], "venue": null, "citeRegEx": "Ivor,? \\Q1930\\E", "shortCiteRegEx": "Ivor", "year": 1930}, {"title": "NP ->NP[the Chinese Government \u2019s] NN[use] PP[of military force] S[to suppress", "author": ["Martin Luther King Jr."], "venue": "Roosevelt had died , or that John F. Kennedy had been shot ,", "citeRegEx": "Jr.,? \\Q1989\\E", "shortCiteRegEx": "Jr.", "year": 1989}], "referenceMentions": [{"referenceID": 8, "context": "Here we switch the terminology to content-dense, to avoid confusion with work in the intersection of cognitive science and computational linguistics that uses the term information density in information-theoretic sense to describe the change in surprise in the linear processing of sentences (Jaeger, 2010; Pate & Goldwater, 2015).", "startOffset": 292, "endOffset": 330}, {"referenceID": 22, "context": "MRC Database(MRC): The MRC Psycholinguistic Database (Wilson, 1988) is an electronic dictionary containing 150,837 words, different subsets of which are annotated for 26 linguistic and psycholinguistic attributes.", "startOffset": 53, "endOffset": 67}, {"referenceID": 22, "context": "In (Wilson, 1988), the words were chosen among those with medium frequency in a large corpus and experiment subjects were asked to rate on a scale the degree to which each word has one of these properties.", "startOffset": 3, "endOffset": 17}, {"referenceID": 15, "context": "The beginning of the article is known to be a strong summary baseline (Mani, Klein, House, Hirschman, Firmin, & Sundheim, 2002; Nenkova, 2005) and many features for identifying important content in articles are based on overlap with the", "startOffset": 70, "endOffset": 142}], "year": 2017, "abstractText": "Content-dense news report important factual information about an event in direct, succinct manner. Information seeking applications such as information extraction, question answering and summarization normally assume all text they deal with is content-dense. Here we empirically test this assumption on news articles from the business, U.S. international relations, sports and science journalism domains. Our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector. We heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features. On manually annotated data, we compare the performance of domain-specific classifiers, trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together. Our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label. Domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer. Domain independent classifiers reproduce better naive crowdsourced judgements. Classification prediction is high across all conditions, around 80%. \u2217. In submission to JAIR 1. We reported our initial work on detection of information-dense text in a paper published at AAAI 2014 (Yang & Nenkova, 2014) In this manuscript we have further extended the work by using much larger samples of New York Times articles for training and by analyzing the performance of a larger number of two-layer classifiers. Here we also introduce a new collection of manually annotated test data, for both domain-dependent and general content-density of texts. We make use of this collection for detailed evaluation of the content-density classifiers. With the publication of this manuscript, we will make the classifiers and data available for use by others In our AAAI 2014 paper, we use the term information-dense to describe the types of text we wish to detect. Here we switch the terminology to content-dense, to avoid confusion with work in the intersection of cognitive science and computational linguistics that uses the term information density in information-theoretic sense to describe the change in surprise in the linear processing of sentences (Jaeger, 2010; Pate & Goldwater, 2015). 1 ar X iv :1 70 4. 00 44 0v 1 [ cs .C L ] 3 A pr 2 01 7 Yinfei Yang, & Ani Nenkova", "creator": "TeX"}}}