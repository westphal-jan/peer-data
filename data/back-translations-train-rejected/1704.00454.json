{"id": "1704.00454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Clustering in Hilbert simplex geometry", "abstract": "Clustering categorical distributions in the probability simplex is a fundamental primitive often met in applications dealing with histograms or mixtures of multinomials. Traditionally, the differential-geometric structure of the probability simplex has been used either by (i) setting the Riemannian metric tensor to the Fisher information matrix of the categorical distributions, or (ii) defining the information-geometric structure induced by a smooth dissimilarity measure, called a divergence. In this paper, we introduce a novel computationally-friendly non-Riemannian framework for modeling the probability simplex: Hilbert simplex geometry. We discuss the pros and cons of those three statistical modelings, and compare them experimentally for clustering tasks.", "histories": [["v1", "Mon, 3 Apr 2017 07:23:37 GMT  (5820kb,D)", "https://arxiv.org/abs/1704.00454v1", "18 pages"], ["v2", "Sun, 30 Apr 2017 15:30:07 GMT  (8419kb,D)", "http://arxiv.org/abs/1704.00454v2", "26 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["frank nielsen", "ke sun"], "accepted": false, "id": "1704.00454"}, "pdf": {"name": "1704.00454.pdf", "metadata": {"source": "CRF", "title": "Clustering in Hilbert simplex geometry", "authors": ["Frank Nielsen", "Ke Sun"], "emails": ["Frank.Nielsen@acm.org", "sunk@ieee.org"], "sections": [{"heading": null, "text": "Keywords: Fisher-Rao geometry, information geometry, Hilbert-Simplex geometry, Finsler geometry, center-based clustering."}, {"heading": "1 Introduction", "text": "Multinomial distribution is an important representation in machine learning, which is often fulfilled in applications [32, 18] as normalized histograms (with non-empty vessels).A multinomial distribution (or categorical distribution) p \u2206 d can be regarded as a point located in the probability simplex \u0445 d (standard simplex) with coordinates p = (\u03bb0p,.., \u03bb d), so that \u03bb i p > 0 and \u2211 d i = 0 \u03bb i p = 1. The open probability simplex \u0445 d sits in Rd + 1 on the hyperplane H \u0445 d: \u0445 d i = 0 xi = 1. We consider the task of clustering a set p = {p1,..., pn} categorical distributions [18] (multinomial structures) of \u0445 d using centric kmeans + + + or centric cluster algorithms [6, 23] based on a dissimilarity, as cultinomial distributions between these three multinomial [18] structures."}, {"heading": "1.1 Paper outline", "text": "The rest of this paper is structured as follows: Section 2 formally introduces the distance measurements of \u2206 d. Section 3 presents the efficient calculation of the Hilbert distance. Section 4 presents algorithms for Hilbert Minimax centers and Hilbert clusters. Section 5 performs an empirical study of clustering multinomial distributions and compares the Rieman geometry, information geometry and Hilbert geometry. E-mail: sunk @ ieee.orgar Xiv: 170 4.00 454v 2 [cs.L G] 30 Apr 201 7Section 6 concludes this work with a summary of the pros and cons of each geometry. Although some content requires prior knowledge of geometric structures, we clearly provide the algorithms so that the general public can still benefit from this work."}, {"heading": "2 Three distances with their underlying geometries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Fisher-Hotelling-Rao geometry", "text": "The Rao distance between two multinomial distributions is [28, 32]: \u03c1FHR (p, q) = 2 arcs (d \u2211 i = 0 \u221a \u03bbip\u03bb i q). (1) It is a metric length distance (which fulfils the symmetrical and triangular axioms of inequality), which is achieved by setting the metric tensor g on the Fisher information matrix (FIM) I of the categorical distribution: I (p) = [gij (p)] withgij (p) = \u03b4ij \u03bbip + 1 \u03bb0p. We call this geometry Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50]. The metric tensor g allows to define an inner product on each tangent Tp of the probability plane Tp + 1 \u03bb0p."}, {"heading": "2.2 Information geometry", "text": "A divergence D is a smooth C3 differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG). A f -divergence is defined for a strict convex function f with f (1) = 0 by: If (p: q) = d + 0 If (p: q) = 0 \u03bbipf (2). The class of f -divergences plays an essential role in information theory, as they have been proven to be the only separable divergences that satisfy the information monotonicity property, which is by obtaining histograms in the lower dimensions of multinomials."}, {"heading": "2.3 Hilbert simplex geometry", "text": "In Hilbert Geometry [25] (HG) we are a geometric domain C (here, C = \u2206 d), and the distance between the two points M, M \u00b2, M \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A, M \u00b2, A, M \u00b2, A, M \u00b2, A, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D,"}, {"heading": "3 Computing Hilbert distance in \u2206d", "text": "Let us start with the simplest case: The 1D probability simplex \u0445 1, the space of the Bernoulli distributions. Each Bernoulli distribution is represented by its activation probability p \u0445 \u0445 1 and corresponds to a point in the interval \u0445 1 = (0, 1)."}, {"heading": "3.1 1D probability simplex of Bernoulli distributions", "text": "By definition, Hilbert-Distance has the following form: (a) (a) (b) (c) (c) (a) (c) (c) (c) (c) (a) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c)) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) (c) ("}, {"heading": "3.2 Arbitrary dimension case", "text": "In view of the fact that we must first calculate the intersection of the line (pq) with the boundary of the ddimensional probability (1) in order to obtain the two intersections p \"and q\" so that p, \"p,\" q \"and q\" are each ordered to the (pq) -dimensional facets. Once this is done, we simply apply the formula in Eq \"to obtain the Hilbert formula (1), which consists of d + 1 depressions with their corresponding (d \u2212 1) -dimensional facets. For the probability simplex\" d, \"we leave ei = (0,.)."}, {"heading": "3.3 Visualizing distance profiles", "text": "Figure 5 shows the distance profile from any point in the probability simplex to a fixed reference point (trinomial) for the following common distance measurement variables [15]: Euclidean distance (metric), CauchySchwarz (CS) divergence, Hellinger distance (metric), Fisher-Rao distance (metric), KL divergence and Hilbert simplicity distance (metric).The Euclidean and Cauchy-Black divergence are truncated to \u2206 2. the Cauchy-Black distance is a projective distance \u03c1CS (\u03bbp, \u03bb \u2032 q) = \u03c1CS (p, q) for each \u03bb, \u03bb \u2032 > 0, see [46]."}, {"heading": "4 Center-based clustering", "text": "We concentrate on comparing the efficiency of Hilbert simplex geometry for clustering multinomials = 0,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00"}, {"heading": "It follows that \u2016x\u2032 \u2212 y\u2032\u20162 \u2264 2(\u2016x\u2032\u20162 + \u2016y\u2032\u20162) since \u2016x\u2032 + y\u2032\u20162 \u2265 0. Let x\u2032 = x\u2212 z and y\u2032 = y \u2212 z so", "text": "This means that we see ourselves as being able to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to feel able, to be able to feel, to feel and to act. This means that we are able to be able, to be able, to be able, to be able to be able, to be able to be able, to be able to be able to be, to be able to be, to be able to change the world."}, {"heading": "4.2.1 Exact smallest enclosing ball in a Hilbert simplex geometry", "text": "Considering a finite theorem {x1,.., xn}, xp = D, we define the smallest enclosing sphere (SEB) in Hilbert simplex geometry as: r 1,..., n \u00b2 \u03c1HG (c, xi). (9) The radius of SEB is r. (10) Consider the equivalent problem of SEB searching in the isometric normalized vector space via the mapping in appendix A. For each simplex point xi corresponds a point vi in the normalized vector space Vd.Figure 10 shows some examples of the exact smallest enclosing sphere in the Hilbert simplex geometry and the corresponding normalized vector space. To calculate the SEB, one can also consider the generic LP type randomized algorithm. (41) We note that an enclosing sphere in the general position is the number of points at the boundary of the sphere with 2, which we are categorical."}, {"heading": "4.2.2 Geodesic bisection approximation heuristic", "text": "In Riemannian geometry, the 1-center can be approximated as finely as desired using a simple geodesic bisection algorithm (9, 5] This algorithm can be extended to HG, as described in algorithm 3: The algorithm first takes a point c0 randomly for the initial center, calculates the most distant point fi (in terms of distance) and proceeds on the geodetic path from c0 to fi by a certain amount to define c1, etc. For an arbitrary distance, we define operator # 2 according to the following principle: p # \u03c1\u03b1q = v = p, q, alez), and proceed on the geodesic path from c0 to fi (p: q, \u03b1) is the geodesic crossing of p and q, and parameterized according to the formula."}, {"heading": "5 Experiments", "text": "We create a data set consisting of a set of clusters in a high-dimensional statistical simplex \u2206 d. Each cluster is independently generated as follows: We first select a random c = (\u03bb0c,..., \u03bb d c) based on the uniform distribution of seeds on \u2206 d. Then we create a random sample p = (\u03bb0,.., \u03bbd) based on on\u03bbi = exp (log \u03bbic + \u03c3 i). We repeat the random samples for each cluster center and ensure that different clusters have almost the same number of samples, and each i follows independently of a standard Gaussian distribution. Let us see the accuracy of the experiments based on p = \u03bbic. Therefore, p is randomly distributed around c. We repeat the generation of random samples for each cluster center and ensure that different clusters have almost the same number of samples."}, {"heading": "6 Conclusion", "text": "We introduced Hilbert metric distance and its underlying non-Riemann geometry for modelling the space of open probability multinomials and experimentally compared this geometry with traditional differential-geometric modelling (either FHR metric connection or dual coupled non-metric affine connection of information geometry [2]) for cluster tasks. The main feature of HG is that it is a metric, non-manifold geometry in which geodesy is straight (Euclidean) line segments. For simple domains, the Hilbert spheres have a fixed combinatory (euclidean) statistical structure, and HG is known to be isometric to a standardised space [24, 22]. This latter isometry makes it easy to generalise the standardised proofs of cluster formation (e.g. k-mean or k-centre)."}, {"heading": "B Hilbert geometry with Finslerian/Riemannian structures", "text": "In belt geometry, each tangent TpM of the d-dimensional manifold M = 1bert pp = 1bert pp = 1bert p = 11bert = 11bert = Rd: TpM'Rd. The inner product of each tangent TpM can be visualized by an ellipsoid shape, a convex symmetric object with an empty interior. Thus, in a Finnish geometry, the belt geometry is generalized by considering generic symmetrical symmetrical objects instead of ellipsoids, and this standard is visualized as a symmetrical object with a non-empty interior. Therefore, Finnish geometry generalizes the belt geometry by considering generic symmetrical symmetrical objects instead of ellipsoids for generating standards in each tangent plane. Any Hilbert geometry, the corresponding domain can be expressed by a finslexical geometry."}], "references": [{"title": "Bregman clustering for separable instances", "author": ["Marcel R Ackermann", "Johannes Bl\u00f6mer"], "venue": "In Scandinavian Workshop on Algorithm Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Information Geometry and Its Applications. Applied Mathematical Sciences", "author": ["Shun-ichi Amari"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Information geometry of divergence functions", "author": ["Shun-ichi Amari", "Andrzej Cichocki"], "venue": "Bulletin of the Polish Academy of Sciences: Technical Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Medians and means in Finsler geometry", "author": ["Marc Arnaudon", "Frank Nielsen"], "venue": "LMS Journal of Computation and Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On approximating the Riemannian 1-center", "author": ["Marc Arnaudon", "Frank Nielsen"], "venue": "Computational Geometry,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In ACM- SIAM symposium on Discrete algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Approximate k-means++ in sublinear time", "author": ["Olivier Bachem", "Mario Lucic", "S. Hamed Hassani", "Andreas Krause"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Scalable and distributed clustering via lightweight coresets", "author": ["Olivier Bachem", "Mario Lucic", "Andreas Krause"], "venue": "arXiv preprint arXiv:1702.08248,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Smaller core-sets for balls", "author": ["Mihai B\u00e2doiu", "Kenneth L. Clarkson"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Optimal core-sets for balls", "author": ["Mihai B\u0103doiu", "Kenneth L Clarkson"], "venue": "Computational Geometry,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Hilbert geometry of polytopes", "author": ["Andreas Bernig"], "venue": "Archiv der Mathematik,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Beyond Mahalanobis metric: Cayley-Klein metric learning", "author": ["Yanhong Bi", "Bin Fan", "Fuchao Wu"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "No dimension-independent core-sets for containment under homothetics", "author": ["Ren\u00e9 Brandenberg", "Stefan K\u00f6nig"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "The Geometry of Geodesics", "author": ["H. Busemann"], "venue": "Pure and Applied Mathematics. Elsevier Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Geometric Modeling in Probability and Statistics", "author": ["Ovidiu Calin", "Constantin Udriste"], "venue": "Mathematics and Statistics. Springer International Publishing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Statistical Decision Rules and Optimal Inference. Translations of mathematical monographs", "author": ["N.N. Cencov"], "venue": "American Mathematical Society,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Finding metric structure in information theoretic clustering", "author": ["Kamalika Chaudhuri", "Andrew McGregor"], "venue": "In COLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Voronoi polytopes for polyhedral norms on lattices", "author": ["Michel Deza", "Mathieu Dutour Sikiri\u0107"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Chentsov\u2019s theorem for exponential families", "author": ["J.G. Dowty"], "venue": "ArXiv e-prints,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Spherical k-means++ clustering", "author": ["Yasunori Endo", "Sadaaki Miyamoto"], "venue": "In Modeling Decisions for Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Hilbert metrics and Minkowski norms", "author": ["Thomas Foertsch", "Anders Karlsson"], "venue": "Journal of Geometry,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["Teofilo F Gonzalez"], "venue": "Theoretical Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1985}, {"title": "On Hilbert\u2019s metric for simplices, volume 1, pages 97\u2013118", "author": ["Pierre De La Harpe"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1991}, {"title": "Spaces of statistical parameters", "author": ["Harold Hotelling"], "venue": "In Bulletin AMS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1930}, {"title": "The Cauchy-Schwarz divergence and Parzen windowing: Connections to graph theory and mercer kernels", "author": ["Robert Jenssen", "Jose C Principe", "Deniz Erdogmus", "Torbj\u00f8rn Eltoft"], "venue": "Journal of the Franklin Institute,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Geometrical Foundations of Asymptotic Inference", "author": ["Robert E. Kass", "Paul W. Vos"], "venue": "Wiley- Interscience,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "Confliction of the convexity and metric properties in f -divergences", "author": ["Mohammadali Khosravifard", "Dariush Fooladivanda", "T Aaron Gulliver"], "venue": "IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Minisum hyperspheres, volume 51", "author": ["Mark-Christoph K\u00f6rner"], "venue": "Springer Science & Business Media,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Visualization and Processing of Tensor Fields: Advances and Perspectives", "author": ["David H Laidlaw", "Joachim Weickert"], "venue": "Springer Science & Business Media,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Learning Riemannian metrics", "author": ["Guy Lebanon"], "venue": "In UAI, pages 362\u2013369,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "Birkhoffs version of Hilberts metric and its applications in analysis", "author": ["Bas Lemmens", "Roger Nussbaum"], "venue": "Handbook of Hilbert Geometry,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Isometries of polyhedral hilbert geometries", "author": ["Bas Lemmens", "Cormac Walsh"], "venue": "Journal of Topology and Analysis,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "A note on divergences", "author": ["Xiao Liang"], "venue": "Neural Computation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Worst-case and smoothed analysis of k-means clustering with Bregman divergences", "author": ["Bodo Manthey", "Heiko R\u00f6glin"], "venue": "Journal of Computational Geometry,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Approximating covering and minimum enclosing balls in hyperbolic geometry", "author": ["Frank Nielsen", "Ga\u00ebtan Hadjeres"], "venue": "In International Conference on Networked Geometric Science of Information,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Classification with mixtures of curved Mahalanobis metrics", "author": ["Frank Nielsen", "Boris Muzellec", "Richard Nock"], "venue": "In IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Large margin nearest neighbor classification using curved Mahalanobis distances", "author": ["Frank Nielsen", "Boris Muzellec", "Richard Nock"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "On approximating the smallest enclosing Bregman balls", "author": ["Frank Nielsen", "Richard Nock"], "venue": "In Proceedings of the twenty-second annual symposium on Computational geometry,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2006}, {"title": "On the smallest enclosing information disk", "author": ["Frank Nielsen", "Richard Nock"], "venue": "Information Processing Letters,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Approximating smallest enclosing balls with applications to machine learning", "author": ["Frank Nielsen", "Richard Nock"], "venue": "International Journal of Computational Geometry & Applications,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Total Jensen divergences: Definition, properties and k-means++ clustering", "author": ["Frank Nielsen", "Richard Nock"], "venue": "arXiv preprint arXiv:1309.7109,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "On clustering histograms with k-means by using mixed \u03b1-divergences", "author": ["Frank Nielsen", "Richard Nock", "Shun-ichi Amari"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "On balls in a polygonal Hilbert geometry", "author": ["Frank Nielsen", "La\u00ebtitia Shao"], "venue": "In 33st International Symposium on Computational Geometry (SoCG 2017),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2017}, {"title": "Fitting the smallest enclosing Bregman ball", "author": ["Richard Nock", "Frank Nielsen"], "venue": "In ECML,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Minimum enclosing polytope in high dimensions", "author": ["Rina Panigrahy"], "venue": "arXiv preprint cs/0407020,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2004}, {"title": "Information and accuracy attainable in the estimation of statistical parameters", "author": ["C Radhakrishna Rao"], "venue": "Bull. Cal. Math. Soc.,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1945}, {"title": "Information and the accuracy attainable in the estimation of statistical parameters", "author": ["C Radhakrishna Rao"], "venue": "In Breakthroughs in statistics,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1992}, {"title": "The geometric stability of Voronoi diagrams in normed spaces which are not uniformly convex", "author": ["Daniel Reem"], "venue": "arXiv preprint arXiv:1212.1094,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Perspectives on projective geometry: A guided tour through real and complex geometry", "author": ["J\u00fcrgen Richter-Gebert"], "venue": "Springer Science & Business Media,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2011}, {"title": "New approximation algorithms for minimum enclosing convex shapes", "author": ["Ankan Saha", "SVN Vishwanathan", "Xinhua Zhang"], "venue": "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "A combinatorial bound for linear programming and related problems", "author": ["Micha Sharir", "Emo Welzl"], "venue": "STACS 92,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1992}, {"title": "Riemann-Finsler geometry with applications to information geometry", "author": ["Zhongmin Shen"], "venue": "Chinese Annals of Mathematics-Series B,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "The Geometry of Hessian Structures", "author": ["Hirohiko Shima"], "venue": "World Scientific,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}, {"title": "The epic story of maximum likelihood", "author": ["Stephen M Stigler"], "venue": "Statistical Science,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2007}, {"title": "Introduction aux g\u00e9om\u00e9tries de hilbert", "author": ["Constantin Vernicos"], "venue": "Se\u0301minaire de the\u0301orie spectrale et ge\u0301ome\u0301trie,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2004}], "referenceMentions": [{"referenceID": 29, "context": "1 Introduction The multinomial distribution is an important representation in machine learning that is often met in applications [32, 18] as normalized histograms (with non-empty bins).", "startOffset": 129, "endOffset": 137}, {"referenceID": 16, "context": "1 Introduction The multinomial distribution is an important representation in machine learning that is often met in applications [32, 18] as normalized histograms (with non-empty bins).", "startOffset": 129, "endOffset": 137}, {"referenceID": 16, "context": ", pn} of n categorical distributions [18] (multinomials) of \u2206d using center-based kmeans++ or k-center clustering algorithms [6, 23] that rely on a dissimilarity measure (loosely called distance or divergence when smooth) between any two categorical distributions.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": ", pn} of n categorical distributions [18] (multinomials) of \u2206d using center-based kmeans++ or k-center clustering algorithms [6, 23] that rely on a dissimilarity measure (loosely called distance or divergence when smooth) between any two categorical distributions.", "startOffset": 125, "endOffset": 132}, {"referenceID": 21, "context": ", pn} of n categorical distributions [18] (multinomials) of \u2206d using center-based kmeans++ or k-center clustering algorithms [6, 23] that rely on a dissimilarity measure (loosely called distance or divergence when smooth) between any two categorical distributions.", "startOffset": 125, "endOffset": 132}, {"referenceID": 25, "context": "1 Fisher-Hotelling-Rao geometry The Rao distance between two multinomial distributions is [28, 32]:", "startOffset": 90, "endOffset": 98}, {"referenceID": 29, "context": "1 Fisher-Hotelling-Rao geometry The Rao distance between two multinomial distributions is [28, 32]:", "startOffset": 90, "endOffset": 98}, {"referenceID": 23, "context": "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].", "startOffset": 62, "endOffset": 78}, {"referenceID": 53, "context": "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].", "startOffset": 62, "endOffset": 78}, {"referenceID": 45, "context": "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].", "startOffset": 62, "endOffset": 78}, {"referenceID": 46, "context": "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].", "startOffset": 62, "endOffset": 78}, {"referenceID": 1, "context": "The geodesics \u03b3(p, q;\u03b1) are defined by the Levi-Civita metric connection [2, 15].", "startOffset": 73, "endOffset": 80}, {"referenceID": 14, "context": "The geodesics \u03b3(p, q;\u03b1) are defined by the Levi-Civita metric connection [2, 15].", "startOffset": 73, "endOffset": 80}, {"referenceID": 25, "context": "unit d-sphere of R by using the square root representation p 7\u2192 \u221ap, see [28].", "startOffset": 72, "endOffset": 76}, {"referenceID": 2, "context": "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).", "startOffset": 89, "endOffset": 92}, {"referenceID": 52, "context": "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).", "startOffset": 156, "endOffset": 167}, {"referenceID": 14, "context": "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).", "startOffset": 156, "endOffset": 167}, {"referenceID": 1, "context": "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).", "startOffset": 156, "endOffset": 167}, {"referenceID": 1, "context": "The class of f -divergences plays an essential role in information theory since they are provably the only separable divergences that satisfy the information monotonicity property [2, 35].", "startOffset": 180, "endOffset": 187}, {"referenceID": 32, "context": "The class of f -divergences plays an essential role in information theory since they are provably the only separable divergences that satisfy the information monotonicity property [2, 35].", "startOffset": 180, "endOffset": 187}, {"referenceID": 1, "context": "That is, by coarse-graining the histograms we obtain lower-dimensional multinomials, say p\u2032 and q\u2032, such that 0 \u2264 If (p\u2032 : q\u2032) \u2264 If (p : q), see [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "The underlying information-geometric structure of KL is characterized by a pair of dual connections [2] \u2207 = \u2207(\u22121) (mixture connection) and \u2207\u2217 = \u2207(1) (exponential connection) that induces two dual geodesics (technically, \u00b11-autoparallel curves [15]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "The underlying information-geometric structure of KL is characterized by a pair of dual connections [2] \u2207 = \u2207(\u22121) (mixture connection) and \u2207\u2217 = \u2207(1) (exponential connection) that induces two dual geodesics (technically, \u00b11-autoparallel curves [15]).", "startOffset": 243, "endOffset": 247}, {"referenceID": 2, "context": "The geometry of f -divergences [3] is the \u03b1-geometry (for \u03b1 = 3 + 2f \u2032\u2032\u2032(1)) with the dual \u00b1\u03b1-connections, where \u2207(\u03b1) = 1+\u03b1 2 \u2207\u2217 + 1\u2212\u03b1 2 \u2207.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "More generally, it was shown how to build a dual information-geometric structure for any divergence [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 24, "context": "For example, we can build a dual structure from the symmetric Cauchy-Schwarz divergence [27]: \u03c1CS(p, q) = \u2212 log \u3008\u03bbp, \u03bbq\u3009 \u221a \u3008\u03bbp, \u03bbp\u3009\u3008\u03bbq, \u03bbq\u3009 .", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "Then the Hilbert metric distance [14] is defined by:", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "It is also called the Hilbert cross-ratio metric distance [24, 33].", "startOffset": 58, "endOffset": 66}, {"referenceID": 30, "context": "It is also called the Hilbert cross-ratio metric distance [24, 33].", "startOffset": 58, "endOffset": 66}, {"referenceID": 48, "context": "Notice that we take the absolute value of the logarithm since the Hilbert distance is a signed distance [52].", "startOffset": 104, "endOffset": 108}, {"referenceID": 30, "context": "When C is the unit ball, HG let us recover the Klein hyperbolic geometry [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "When C is a quadric bounded convex domain, we obtain the Cayley-Klein hyperbolic geometry [12] which can be studied with the Riemannian structure and corresponding metric distance called the curved Mahalanobis distances [39, 38].", "startOffset": 90, "endOffset": 94}, {"referenceID": 36, "context": "When C is a quadric bounded convex domain, we obtain the Cayley-Klein hyperbolic geometry [12] which can be studied with the Riemannian structure and corresponding metric distance called the curved Mahalanobis distances [39, 38].", "startOffset": 220, "endOffset": 228}, {"referenceID": 35, "context": "When C is a quadric bounded convex domain, we obtain the Cayley-Klein hyperbolic geometry [12] which can be studied with the Riemannian structure and corresponding metric distance called the curved Mahalanobis distances [39, 38].", "startOffset": 220, "endOffset": 228}, {"referenceID": 10, "context": "Furthermore, the domain bounday \u2202C need not to be smooth: One may also consider bounded polytopes [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "The shape of balls in polytope-domain HG are Euclidean polytopes [33], as depicted in Figure 3.", "startOffset": 65, "endOffset": 69}, {"referenceID": 42, "context": "Hilbert balls have hexagons shapes in 2D [45], rhombic dodecahedra shapes in 3D, and are polytopes [33] with d(d+ 1) facets in dimension d.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "Hilbert balls have hexagons shapes in 2D [45], rhombic dodecahedra shapes in 3D, and are polytopes [33] with d(d+ 1) facets in dimension d.", "startOffset": 99, "endOffset": 103}, {"referenceID": 42, "context": "When the polytope domain is not a simplex, the combinatorial complexity of balls depends on the center location [45], see Figure 4.", "startOffset": 112, "endOffset": 116}, {"referenceID": 28, "context": "The HG of the probability simplex yields a non-Riemannian metric geometry because at infinitesimal radius value, the balls are polytopes and not ellipsoidal balls (corresponding to squared Mahalanobis distance balls used to visualize metric tensors [31]).", "startOffset": 249, "endOffset": 253}, {"referenceID": 31, "context": "The isometries in Hilbert polyhedral geometries are studied in [34].", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": "In Appendix B, we recall that any Hilbert geometry induces a Finslerian structure that becomes Riemannian iff the boundary is an ellipsoid (yielding the hyperbolic Cayley-Klein geometries [52]).", "startOffset": 188, "endOffset": 192}, {"referenceID": 22, "context": "Let us notice that in Hilbert simplex/polytope geometry, the geodesics are not unique (see Figure 2 of [24]).", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "The KL divergence belongs to the family of \u03b1-divergences [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 30, "context": "Theorem 1 (Balls in a simplicial Hilbert geometry [33]) A ball in a Hilbert simplex geometry has a Euclidean polytope shape with d(d+ 1) facets.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "This property allows one to visualize Riemannian metric tensors [31].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "Thus we conclude that: Lemma 2 ([33]) Hilbert simplex geometry is a non-manifold metric length space.", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "3 Visualizing distance profiles Figure 5 displays the distance profile from any point in the probability simplex to a fixed reference point (trinomial) for the following common distance measures [15]: Euclidean distance (metric), CauchySchwarz (CS) divergence, Hellinger distance (metric), Fisher-Rao distance (metric), KL divergence and the Hilbert simplicial distance (metric).", "startOffset": 195, "endOffset": 199}, {"referenceID": 5, "context": "Nevertheless, using a generalization of the k-means initialization [6] (picking randomly seeds), one can bypass the centroid computation, and yet guarantee probabilistically a good clustering.", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "Since its inception (2007), this k-means++ seeding has been extensively studied [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 40, "context": "We state the general theorem established in [43]: Theorem 2 (Generalized k-means++ performance [43]) Let \u03ba1 and \u03ba2 be two constants such that \u03ba1 defines the quasi-triangular inequality property: D(x : z) \u2264 \u03ba1 (D(x : y) +D(y : z)) ,\u2200x, y, z \u2208 \u2206d, and \u03ba2 handles the symmetry inequality: D(x : y) \u2264 \u03ba2D(y : x),\u2200x, y \u2208 \u2206d.", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "We state the general theorem established in [43]: Theorem 2 (Generalized k-means++ performance [43]) Let \u03ba1 and \u03ba2 be two constants such that \u03ba1 defines the quasi-triangular inequality property: D(x : z) \u2264 \u03ba1 (D(x : y) +D(y : z)) ,\u2200x, y, z \u2208 \u2206d, and \u03ba2 handles the symmetry inequality: D(x : y) \u2264 \u03ba2D(y : x),\u2200x, y \u2208 \u2206d.", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "(5) The ratio ED(\u039b,C) E\u2217 D(\u039b,k) is called the competitive factor [6].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "Since divergences may be asymmetric, one can further consider mixed divergence M(p : q : r) = \u03bbD(p : q) + (1 \u2212 \u03bb)D(q : r) for \u03bb \u2208 [0, 1], and extend the k-means++ seeding procedure and analysis, see [44].", "startOffset": 130, "endOffset": 136}, {"referenceID": 41, "context": "Since divergences may be asymmetric, one can further consider mixed divergence M(p : q : r) = \u03bbD(p : q) + (1 \u2212 \u03bb)D(q : r) for \u03bb \u2208 [0, 1], and extend the k-means++ seeding procedure and analysis, see [44].", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "The KL divergence can be interpreted as a separable Bregman divergence [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "The Bregman kmeans++ performance has been studied in [1, 36], and a competitive factor of O( 1 \u03bc ) is reported using the notion of Bregman \u03bc-similarity (that is suited for data-sets on a compact domain).", "startOffset": 53, "endOffset": 60}, {"referenceID": 33, "context": "The Bregman kmeans++ performance has been studied in [1, 36], and a competitive factor of O( 1 \u03bc ) is reported using the notion of Bregman \u03bc-similarity (that is suited for data-sets on a compact domain).", "startOffset": 53, "endOffset": 60}, {"referenceID": 19, "context": "In [21], spherical k-means++ is studied with respect to the distance dS(x, y) = 1\u2212\u3008x, y\u3009 for any pair of points x, y on the unit sphere.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [33] (Theorem 3.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "Appendix A recalls the construction due to De La Harpe [24] in 1991.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "The farthest first traversal heuristic of Gonzalez [23] has a guaranteed approximation factor of 2 for any metric distance (see Algorithm 2).", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Algorithm 3: Geodesic walk for approximating the Hilbert minimax center, generalizing [9] Data: A set of points p1, \u00b7 \u00b7 \u00b7 , pn \u2208 \u2206d.", "startOffset": 86, "endOffset": 89}, {"referenceID": 49, "context": "This problem has been well-studied in computational geometry: See [53, 13, 48].", "startOffset": 66, "endOffset": 78}, {"referenceID": 12, "context": "This problem has been well-studied in computational geometry: See [53, 13, 48].", "startOffset": 66, "endOffset": 78}, {"referenceID": 44, "context": "This problem has been well-studied in computational geometry: See [53, 13, 48].", "startOffset": 66, "endOffset": 78}, {"referenceID": 49, "context": "By considering the equivalent Hilbert norm polytope with d(d+ 1) facets, we state the result of [53]: Theorem 5 (SEB in Hilbert polytope normed space [53]) A (1 + )-approximation of the SEB in Vd can be computed in O(d n ).", "startOffset": 96, "endOffset": 100}, {"referenceID": 49, "context": "By considering the equivalent Hilbert norm polytope with d(d+ 1) facets, we state the result of [53]: Theorem 5 (SEB in Hilbert polytope normed space [53]) A (1 + )-approximation of the SEB in Vd can be computed in O(d n ).", "startOffset": 150, "endOffset": 154}, {"referenceID": 38, "context": "To compute the SEB, one may also consider the generic LP-type randomized algorithm [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 50, "context": "Let D = d(d+1) 2 denote the varying size of the combinatorial basis: Then we can apply the LP-type framework (we check the axioms of locality and monotonicity [54]) to solve efficiently for the SEBs.", "startOffset": 159, "endOffset": 163}, {"referenceID": 50, "context": "Theorem 6 (Smallest Enclosing Hilbert ball is LP-type [59, 54]) The smallest enclosing Hilbert ball amounts to find the smallest enclosing ball in a vector space with respect to a polytope norm that can be solved using a LP-type randomized algorithm.", "startOffset": 54, "endOffset": 62}, {"referenceID": 39, "context": "The Enclosing Ball Decision Problem [42] (EBDP) asks for a given value r, whether r \u2265 r\u2217 or not.", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "The decision problem amounts to find whether a set {rBV + vi} of translates can be stabbed by a point [42]: That is, whether \u2229i=1(rBV + vi) is empty or not.", "startOffset": 102, "endOffset": 106}, {"referenceID": 39, "context": "At stage i, perform a dichotomic search on [ai, bi] by answering the decision problem for ri = bi\u2212ai 2 , and update the radius range accordingly, see [42].", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "(Geodesics are not unique, see Figure 2 of [24])", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "2 Geodesic bisection approximation heuristic In Riemannian geometry, the 1-center can be arbitrarily finely approximated by a simple geodesic bisection algorithm [9, 5] This algorithm can be extended to HG straightforwardly as detailed in Algorithm 3: The algorithm first picks up a point c0 at random from X for the initial center, computes the farthest point fi (with respect to the distance \u03c1), and walk on the geodesic from c0 to fi by a certain amount to define c1, etc.", "startOffset": 162, "endOffset": 168}, {"referenceID": 4, "context": "2 Geodesic bisection approximation heuristic In Riemannian geometry, the 1-center can be arbitrarily finely approximated by a simple geodesic bisection algorithm [9, 5] This algorithm can be extended to HG straightforwardly as detailed in Algorithm 3: The algorithm first picks up a point c0 at random from X for the initial center, computes the farthest point fi (with respect to the distance \u03c1), and walk on the geodesic from c0 to fi by a certain amount to define c1, etc.", "startOffset": 162, "endOffset": 168}, {"referenceID": 34, "context": "See [37] for an extension and analysis in hyperbolic geometry.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "Furthermore, this iterative algorithm implies a core-set [10] (namely, the set of farthest points visited when iterating the geodesic walks) that is useful for clustering large data-sets [8].", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "Furthermore, this iterative algorithm implies a core-set [10] (namely, the set of farthest points visited when iterating the geodesic walks) that is useful for clustering large data-sets [8].", "startOffset": 187, "endOffset": 190}, {"referenceID": 12, "context": "See [13] for coreset results concerning containment problems with respect to a convex homothetic object (the equivalent Hilbert polytope norm in our case).", "startOffset": 4, "endOffset": 8}, {"referenceID": 44, "context": "Panigrahy [48] described as simple algorithm dubbed MINCON for finding an approximation of the Minimum Enclosing Polytope.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "The algorithm induces a core-set of size O( 1 2 ) although the theorem is challenged in [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "Thus by combining the k-center seeding of Gonzalez [23] with the iteration Lloyd-like batched iterations, we get an efficient k-center clustering algorithm for the FHR and Hilbert metric geometries.", "startOffset": 51, "endOffset": 55}, {"referenceID": 43, "context": "dealing with the Kullback-Leibler divergence, we use the fact that KL is a Bregman divergence, and use the 1-center algorithm described in [47, 40] (approximation in any dimension) and [41] (exact but limited to small dimensions).", "startOffset": 139, "endOffset": 147}, {"referenceID": 37, "context": "dealing with the Kullback-Leibler divergence, we use the fact that KL is a Bregman divergence, and use the 1-center algorithm described in [47, 40] (approximation in any dimension) and [41] (exact but limited to small dimensions).", "startOffset": 139, "endOffset": 147}, {"referenceID": 38, "context": "dealing with the Kullback-Leibler divergence, we use the fact that KL is a Bregman divergence, and use the 1-center algorithm described in [47, 40] (approximation in any dimension) and [41] (exact but limited to small dimensions).", "startOffset": 185, "endOffset": 189}, {"referenceID": 30, "context": "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of \u2206d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of \u2206d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].", "startOffset": 233, "endOffset": 245}, {"referenceID": 47, "context": "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of \u2206d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].", "startOffset": 233, "endOffset": 245}, {"referenceID": 17, "context": "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of \u2206d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].", "startOffset": 233, "endOffset": 245}, {"referenceID": 1, "context": "6 Conclusion We introduced the Hilbert metric distance and its underlying non-Riemannian geometry for modeling the space of multinomials of the open probability simplex, and compared experimentally this geometry with the traditional differential-geometric modelings (either FHR metric connection or dually coupled nonmetric affine connection of information geometry [2]) for clustering tasks.", "startOffset": 366, "endOffset": 369}, {"referenceID": 22, "context": "For simplex domains, the Hilbert balls have fixed combinatorial (Euclidean) polytope structures, and HG is known to be isometric to a normed space [24, 22].", "startOffset": 147, "endOffset": 155}, {"referenceID": 20, "context": "For simplex domains, the Hilbert balls have fixed combinatorial (Euclidean) polytope structures, and HG is known to be isometric to a normed space [24, 22].", "startOffset": 147, "endOffset": 155}], "year": 2017, "abstractText": "Clustering categorical distributions in the probability simplex is a fundamental primitive often met in applications dealing with histograms or mixtures of multinomials. Traditionally, the differentialgeometric structure of the probability simplex has been used either by (i) setting the Riemannian metric tensor to the Fisher information matrix of the categorical distributions, or (ii) defining the informationgeometric structure induced by a smooth dissimilarity measure, called a divergence. In this paper, we introduce a novel computationally-friendly non-Riemannian framework for modeling the probability simplex: Hilbert simplex geometry. We discuss the pros and cons of those three statistical modelings, and compare them experimentally for clustering tasks.", "creator": "LaTeX with hyperref package"}}}