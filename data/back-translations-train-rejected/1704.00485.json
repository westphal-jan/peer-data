{"id": "1704.00485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity Classifiers?", "abstract": "Many datasets have multiple tables connected by key-foreign key dependencies. Data scientists usually join all tables to bring in extra features from the so-called dimension tables. Unlike the statistical relational learning setting, such joins do not cause record duplications, which means regular IID models are typically used. Recent work demonstrated the possibility of using foreign key features as representatives for the dimension tables' features and eliminating the latter a priori, potentially saving runtime and effort of data scientists. However, the prior work was restricted to linear models and it established a dichotomy of when dimension tables are safe to discard due to extra overfitting caused by the use of foreign key features. In this work, we revisit that question for two popular high capacity models: decision tree and SVM with RBF kernel. Our extensive empirical and simulation-based analyses show that these two classifiers are surprisingly and counter-intuitively more robust to discarding dimension tables and face much less extra overfitting than linear models. We provide intuitive explanations for their behavior and identify new open questions for further ML theoretical research. We also identify and resolve two key practical bottlenecks in using foreign key features.", "histories": [["v1", "Mon, 3 Apr 2017 09:16:58 GMT  (4494kb,D)", "https://arxiv.org/abs/1704.00485v1", "10 pages"], ["v2", "Sun, 9 Apr 2017 04:02:56 GMT  (4494kb,D)", "http://arxiv.org/abs/1704.00485v2", "10 pages"], ["v3", "Sun, 4 Jun 2017 19:02:20 GMT  (7283kb,D)", "http://arxiv.org/abs/1704.00485v3", "14 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.DB cs.LG", "authors": ["vraj shah", "arun kumar", "xiaojin zhu"], "accepted": false, "id": "1704.00485"}, "pdf": {"name": "1704.00485.pdf", "metadata": {"source": "CRF", "title": "Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity Classifiers?", "authors": ["Vraj Shah", "Arun Kumar", "Xiaojin Zhu"], "emails": ["arunkk}@eng.ucsd.edu,", "jerryzhu@cs.wisc.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "The setting we focus on is the following: The dataset has a set of tables in the star scheme q with carbonate dependencies (KFKDs); star schemes are ubiquitous in many applications, including retail, insurance, web security and recommendation systems [33, 26, 25]; the fact table that has the target variable is called S. It has the scheme S (SID, Y, XS, FK1,..., FKq); a dimension table is called Ri (i = 1 to q); and it has the scheme Ri (RIDi, XRi); Y is the target variable (class name); XS and XRi are vectors (sequences) of characteristics; RIDi is the primary key of Ri; while FKi is a foreign key characteristic that refers to Ri."}, {"heading": "2.2 Assumptions and Scope", "text": "For reasons of traceability, we make some assumptions in this paper [26]. In particular, we assume that the attributes are categorical. Numerical attributes can be discredited using standard techniques such as binning. We also focus on binary classification, but our ideas are also easily applicable to multi-class objectives. We assume that the foreign key attributes (FKi) are not (primary) keys in the fact table, e.g. the employer does not clearly identify a customer. 3 Finally, we do not examine the problem of \"cold start\" because it is orthogonal to the focus of this paper. [36] Therefore, all attributes have known finite domains, possibly including a special placeholder for \"others,\" to temporarily manage previously invisible values. In our example, this means that both employer and gender have known finite domains. Generally, FKi values can only be derived from the given set of Ri.RIDi values (new values are mapped to each other in \"Fi-Ws\")."}, {"heading": "3. EMPIRICAL STUDY WITH REAL DATA", "text": "We now present our detailed empirical study using real-world data sets on 10 classifiers, including 7 high-capacity classifiers (CART decision tree with Gini, information gain and gain ratio; SVM with RBF and square nuclei; multilayer Perceptron ANN; the \"brain-indeadic\" 1-nearest neighbor) and 3 linear classifiers (naive bayes with reverse selection, logistic regression with L1 regularization and linear SVM). As these additional linear classifiers did not provide new insights, we omitted them for reasons of space."}, {"heading": "3.1 Datasets", "text": "We take the seven real data sets from [26]; these are originally from Kaggle, GroupLens, openflights.org, mtg.upf. edu / node / 1671 and last.fm. Two records have binary targets (flights and Expedia); the others have multi-class ordinary targets. For convenience, we will binarize all targets for this paper by grouping ordinal targets into lower and upper halves (this change has no effect on our general conclusions).The record statistics are provided in Table 1. We briefly describe the task for each data set and explain what the foreign features are. Further details of their schemes, including the list of all features, are already in the public domain (listed in [26]).All of our records, scripts and code are available for download."}, {"heading": "3.2 Methodology", "text": "We compare these two approaches for all 10 classifiers already mentioned to get further insights, which also include a third approach to the decision trees: NoFK, which is essentially the same as JoinAll, but with all the other key features. We used the popular R packages \"rpart\" for deciding trees5 and \"e1071\" for the SVMs. For the ANNs, we used the popular Keras Python library on top of TensorFlow.5Except for the profit ratio."}, {"heading": "3.3 Results", "text": "This year is the highest in the history of the country."}, {"heading": "4. IN-DEPTH SIMULATION STUDY", "text": "We delve deeper into the decision-maker's behavior by using a simulation study in which we vary the properties of the underlying \"true\" distribution of data. We focus on a two-spreadsheet CHP that joins for simplicity. We stamp data sets of different dimensions. We use the decision tree for this study because it has the maximum robustness to avoid CHP. Our simulation study is designed to \"test\" this robustness comprehensively. Note that our simulation methodology is not tied to decision trees; it is generic enough to apply the classifier, as we only use generic terms for error and network variance as defined."}, {"heading": "4.1 Scenario OneXr", "text": "In fact, it is a way in which most people are able to hold on to power, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "4.2 Scenario XSXR", "text": "In contrast to the OneXr scenario, we now create a real distribution that maps X \u2261 [XS, XR] to Y without Bayes error (noise).The exact procedure for sampling examples is as follows: (1) Construct a real probability table (TPT) with entries for all possible values of [XS, XR] and assign a random probability to each entry so that the total probability is 1. (2) For each entry in the TPT, select a Y value at random and append the TPT entry; this ensures that H (Y | X) = 0. (3) Marginalize the TPT to obtain P (XR) and from it, for example nR = DFK tuples for R together with an associated sequential RID value. (4) In the original TPT, the probability of each entry is pushed to 0 if its XR values are not selected."}, {"heading": "4.3 Scenario RepOneXr", "text": "We are now presenting results for a new simulation scenario in which the true distribution is accurately determined using a single characteristic Xr-XR. We are sampling examples such as the method for OneXr mentioned above, except that the tuples of R are now formed by replicating the value of Xr values sampled for a tupel to generate all the other characteristics in XR. That is, XR of an example is exactly the same value as the dR values are repeated. Note that the FD FK \u2192 XR implies that there are at least as many unique FK values as XR values. Thus, by increasing the number of FK values relative to the XR values, we hope to increase the probability that the model will get \"confused\" with NoJoin. Our goal is to see if this is the gap between JoinAll and NoJoin.Figure 7 shows the results for the two experiments on a decision ratio of A (where a tup5x is a high) to 5x."}, {"heading": "5. ANALYSIS AND OPEN QUESTIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Explaining the Results", "text": "We now intuitively explain the surprising behavior of decision trees and RBF-SVM with NoJoin versus JoinAll. First, we ask: Does NoJoin compromise the \"generalization error\"? The generalization error is the difference between test and tensile errors. Tables 2 and 3 already provided the test accuracy. Tables 5 and 6 now provide the tensile accuracy. Obviously, JoinAll versus NoJoin is almost indistinguishable for the decision tree! The only exception is Yelp, which we have already noticed. Note that the absolute generalization error is often high, which is expected for decision trees [17]. For example, the tensile accuracy of Flights is almost 100%, while the test accuracy is only 85%. But the absolute generalization error is orthogonal to our focus; we just note that NoJoin does not significantly increase this difference."}, {"heading": "In other words, discarding foreign features did not significantly affect the generalization error of the decision tree!", "text": "This year it is more than ever before."}, {"heading": "5.2 Open Research Questions", "text": "While our intuitive explanations capture the fine-grained behavior of the decision tree and the RBF-SVM with NoJoin vis-a-vis JoinAll, there are many open questions for further research. Is it possible to quantify the likelihood of incorrect partitioning with a decision tree as a function of data properties? Is it possible to quantify the likelihood of mismatched examples selected by the RBF-SVM? Why does the theory of VC dimensions predict the opposite of observed behavior with these models? How can we quantify their generalizability if memorization is permissible and what forms of memorization are permitted? Answering these questions would provide deeper insights into the effects of KFKD / FD on the generalizability and accuracy of such classifiers. It could also produce more formal mechanisms to characterize them if discarding foreign features goes beyond mere consideration of tuples."}, {"heading": "6. MAKING FK FEATURES PRACTICAL", "text": "We will now discuss two key practical problems caused by the large domains of foreign key features and examine how standard approaches can be adapted to solve them. Unlike previous work on the handling of large-scale regular features [7], foreign key features differ in that they have coarse-grained page information in the form of foreign features, suggesting an alternative way of using such features where possible, rather than always including them."}, {"heading": "6.1 Foreign Key Domain Compression", "text": "This year, it has never been as good as it has been this year."}, {"heading": "6.2 Foreign Key Smoothing", "text": "This is not a cold start problem - the FK values are all still from the fully known DFK. This problem arises because there are not enough examples to cover all DFK during the training. Typically, this problem is handled with some form of smoothing, for example, if the random outline for Naive Bayes was used by adding a pseudo-number of 1 to all frequencies [29]. While similar smoothing techniques for estimating probability were studied using decision trees [32] to take advantage of the best of our knowledge, this problem was not generally handled for classification with decision trees. In fact, popular decision tree implementations in R just crash if a value of FK is not seen during the training! Note that SVMs (or other classifiers operating on numerical characteristics) do not consider this issue due to the most uniform coding of FK."}, {"heading": "7. RELATED WORK", "text": "This year is the highest in the history of the country."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "We think it is high time for the data management community to look beyond building faster or scalable ML systems and help reduce the hassle of data retrieval and feature engineering for ML. Understanding how basic characteristics of data sources, especially schema information, influence ML behavior is a promising step in this direction. Although the idea of safely avoiding connections has been adopted in practice for linear classifiers, in this comprehensive experimental study we show that it works even better for popular high-performance classifiers, which runs counter to the intuition that high-performance classifiers are typically more prone to overtake. We hope that our findings and analyses will stimulate further discussion and new research to simplify data retrieval for ML-based analysis. In the future, we plan to formally analyze the effects of KFKD / FD on high-performance classifiers from a learning theoretical perspective."}, {"heading": "9. REFERENCES", "text": "[1] Personal communications: Facebook friendrecommendation system; LogicBlox retail analytics; MakeMyTrip customer analytics. [2] S. Abiteboul, R. Hull, and V. Vianu, editors. Foundations of Databases: The Logical Level. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1st edition, 1995. [3] H. Almuallim and T. G. Dietterich. Efficient Algorithms for Identifying Relevant Features. Technical report, 1992. [4] M. Anderson, D. Antenucci, V. Bittorf, M. Burgess, M. J. Cafarella, A. Kumar, F. Niu, Y. Park, C. Re'an, and C. Zhang. Brainwash: A Data System for Feature Engineering. In CIDR, 2013. [5] M. Boehm, M. W. Dusenberry, D. Eriksson, A. V. Evfimievski, F. Pansmann."}], "references": [{"title": "editors", "author": ["S. Abiteboul", "R. Hull", "V. Vianu"], "venue": "Foundations of Databases: The Logical Level. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1st edition", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Efficient Algorithms for Identifying Relevant Features", "author": ["H. Almuallim", "T.G. Dietterich"], "venue": "Technical report", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Brainwash: A Data System for Feature Engineering", "author": ["M. Anderson", "D. Antenucci", "V. Bittorf", "M. Burgess", "M.J. Cafarella", "A. Kumar", "F. Niu", "Y. Park", "C. R\u00e9", "C. Zhang"], "venue": "CIDR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "SystemML: Declarative Machine Learning on Spark", "author": ["M. Boehm", "M.W. Dusenberry", "D. Eriksson", "A.V. Evfimievski", "F.M. Manshadi", "N. Pansare", "B. Reinwald", "F.R. Reiss", "P. Sen", "A.C. Surve", "S. Tatikonda"], "venue": "VLDB", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Consistency based feature selection", "author": ["M. Dash", "H. Liu", "H. Motoda"], "venue": "Proceedings of the 4th Pacific-Asia Conference on Knowledge Discovery and Data Mining, Current Issues and New Applications, PAKDK, pages 98\u2013109, London, UK, UK", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Bias of importance measures for multi-valued attributes and solutions", "author": ["H. Deng", "G. Runger", "E. Tuv"], "venue": "Proceedings of the 21st International Conference on Artificial Neural Networks - Volume Part II, ICANN\u201911, pages 293\u2013300, Berlin, Heidelberg", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Principles of Data Integration", "author": ["A. Doan", "A. Halevy", "Z. Ives"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A Unified Bias-Variance Decomposition and its Applications", "author": ["P. Domingos"], "venue": "Proceedings of 17th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Big data integration", "author": ["X.L. Dong", "D. Srivastava"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Towards a Unified Architecture for in-RDBMS Analytics", "author": ["X. Feng", "A. Kumar", "B. Recht", "C. R\u00e9"], "venue": "Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201912", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple feature fusion by subspace learning", "author": ["Y. Fu", "L. Cao", "G. Guo", "T.S. Huang"], "venue": "Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval, CIVR \u201908, pages 127\u2013134, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Introduction to Statistical Relational Learning)", "author": ["L. Getoor", "B. Taskar"], "venue": "The MIT Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature Extraction: Foundations and Applications", "author": ["I. Guyon", "S. Gunn", "M. Nikravesh", "L.A. Zadeh"], "venue": "New York: Springer-Verlag", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "The Elements of Statistical Learning: Data mining", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Inference, and Prediction. Springer-Verlag", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The MADlib Analytics Library or MAD Skills", "author": ["J.M. Hellerstein", "C. R\u00e9", "F. Schoppmann", "D.Z. Wang", "E. Fratkin", "A. Gorajek", "K.S. Ng", "C. Welton", "X. Feng", "K. Li", "A. Kumar"], "venue": "the SQL. In VLDB", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unbiased recursive partitioning: A conditional inference framework", "author": ["T. Hothorn", "K. Hornik", "A. Zeileis"], "venue": "JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS, 15(3):651\u2013674", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Visual search at pinterest", "author": ["Y. Jing", "D. Liu", "D. Kislyuk", "A. Zhai", "J. Xu", "J. Donahue", "S. Tavel"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1889\u20131898, New York, NY, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Multisensor data fusion: A review of the state-of-the-art", "author": ["B. Khaleghi", "A. Khamis", "F.O. Karray", "S.N. Razavi"], "venue": "Information Fusion,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "3rd International Conference for Learning Representations (ICLR)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Toward Optimal Feature Selection", "author": ["D. Koller", "M. Sahami"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Feature Selection in Enterprise Analytics: A Demonstration using an R-based Data Analytics System", "author": ["P. Konda", "A. Kumar", "C. R\u00e9", "V. Sashikanth"], "venue": "VLDB", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "MLbase: A Distributed Machine-learning System", "author": ["T. Kraska", "A. Talwalkar", "J.C. Duchi", "R. Griffith", "M.J. Franklin", "M.I. Jordan"], "venue": "CIDR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Demonstration of Santoku: Optimizing Machine Learning over Normalized Data", "author": ["A. Kumar", "M. Jalal", "B. Yan", "J. Naughton", "J.M. Patel"], "venue": "VLDB", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Generalized Linear Models Over Normalized Data", "author": ["A. Kumar", "J. Naughton", "J.M. Patel"], "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201915", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "To Join or Not to Join? Thinking Twice about Joins before Feature Selection", "author": ["A. Kumar", "J. Naughton", "J.M. Patel", "X. Zhu"], "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Data Integration in Machine Learning", "author": ["Y. Li", "A. Ngom"], "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BTBM)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "GraphLab: A New Framework For Parallel Machine Learning", "author": ["Y. Low", "J.E. Gonzalez", "A. Kyrola", "D. Bickson", "C.E. Guestrin", "J. Hellerstein"], "venue": "UAI", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning", "author": ["T.M. Mitchell"], "venue": "McGraw Hill", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "The Logic of Representing Dependencies by Directed Graphs", "author": ["J. Pearl", "T. Verma"], "venue": "AAAI", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1987}, {"title": "Data management challenges in production machine learning", "author": ["N. Polyzotis", "S. Roy", "S.E. Whang", "M. Zinkevich"], "venue": "Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1723\u20131726, New York, NY, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Tree Induction for Probability-Based Ranking", "author": ["F. Provost", "P. Domingos"], "venue": "Machine Learning, 52(3):199\u2013215", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Database Management Systems", "author": ["R. Ramakrishnan", "J. Gehrke"], "venue": "McGraw-Hill, Inc.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Scaling Factorization Machines to Relational Data", "author": ["S. Rendle"], "venue": "Proceedings of the VLDB Endowment", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Introducing CloudLab: Scientific Infrastructure for Advancing Cloud Architectures and Applications", "author": ["R. Ricci", "E. Eide", "C. Team"], "venue": "; login:: the magazine of USENIX & SAGE, 39(6):36\u201338", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods and Metrics for Cold-start Recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning Linear Regression Models over Factorized Joins", "author": ["M. Schleich", "D. Olteanu", "R. Ciucanu"], "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine Learning: The High Interest Credit Card of Technical Debt", "author": ["D. Sculley", "G. Holt", "D. Golovin", "E. Davydov", "T. Phillips", "D. Ebner", "V. Chaudhary", "M. Young", "J.-F. Crespo", "D. Dennison"], "venue": "SE4ML: Software Engineering for Machine Learning ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Database Systems Concepts", "author": ["A. Silberschatz", "H. Korth", "S. Sudarshan"], "venue": "McGraw-Hill, Inc., New York, NY, USA, 5 edition", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "A Novel Feature Selection Approach: Combining Feature Wrappers and Filters", "author": ["O. Uncu", "I. Turksen"], "venue": " Information Sciences, 177(2)", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1113\u20131120, New York, NY, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "A Method for Implementing a Probabilistic Model as a Relational Database", "author": ["S.K.M. Wong", "C.J. Butz", "Y. Xiang"], "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1995}, {"title": "Crossmine: Efficient classification across multiple database relations", "author": ["X. Yin", "J. Han", "J. Yang", "P.S. Yu"], "venue": "Proceedings of the 2004 European Conference on Constraint-Based Mining and Inductive Databases, pages 172\u2013195, Berlin, Heidelberg", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient Feature Selection via Analysis of Relevance and Redundancy", "author": ["L. Yu", "H. Liu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Understanding Deep Learning Requires Rethinking Generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2017}, {"title": "I/O-Efficient Statistical Computing with RIOT", "author": ["Y. Zhang"], "venue": "In ICDE,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": ", [16, 11, 46]), how to scale ML (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 9, "context": ", [16, 11, 46]), how to scale ML (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 44, "context": ", [16, 11, 46]), how to scale ML (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 3, "context": ", [5, 28]), and how to use database ideas to improve ML tasks (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 26, "context": ", [5, 28]), and how to use database ideas to improve ML tasks (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 20, "context": ", [22, 23]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 21, "context": ", [22, 23]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 2, "context": "However, little work has tackled the pains of sourcing data for ML tasks in the first place, especially, how fundamental data properties affect end-to-end data workflows for ML tasks [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 31, "context": "In particular, real-world relational databases often have many tables connected by database dependencies such as key-foreign key dependencies (KFKDs) [33].", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "Thus, given an ML task, data scientists almost always join multiple tables because they like to obtain more features for ML models [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 36, "context": "Furthermore, recent reports of Google\u2019s production ML systems show that features that yield marginal benefits incur high \u201ctechnical debt\u201d that decreases code mangeability and increases costs [38, 31].", "startOffset": 191, "endOffset": 199}, {"referenceID": 29, "context": "Furthermore, recent reports of Google\u2019s production ML systems show that features that yield marginal benefits incur high \u201ctechnical debt\u201d that decreases code mangeability and increases costs [38, 31].", "startOffset": 191, "endOffset": 199}, {"referenceID": 24, "context": "[26] showed that one can often omit entire tables by exploiting KFKDs in the database schema.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Example (based on [26]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "The analysis in [26] revealed a dichotomy in how safe it is to avoid a join from an accuracy standpoint: in terms of the bias-variance trade-off, avoiding a join is unlikely to increase bias but it might significantly increase variance, since foreign key features often have larger domains than foreign features.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "In [26], the tuple ratio quantifies this behavior; in our example, it is the ratio of the number of labeled customers to the number of employers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "While KFKDs are not the same as FDs [39], assuming features have \u201cclosed\u201d domains, they behave essentially as FDs in the output of the join [26].", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "While KFKDs are not the same as FDs [39], assuming features have \u201cclosed\u201d domains, they behave essentially as FDs in the output of the join [26].", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "However, the results in [26] had a major caveat\u2013they applied only to linear classifiers.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "Surprisingly, our results show that their behavior is the exact opposite! We start by rerunning the experiments on the real-world datasets with KFK joins from [26] for these models.", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "In other words, our work refutes an intuition from the VC dimension-based analysis of [26] and shows that these popular high-capacity classifiers are counter-intuitively comparably or more robust to avoiding joins than linear classifiers, not less.", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": ", the holdout test errors blow up) [26].", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25].", "startOffset": 120, "endOffset": 132}, {"referenceID": 24, "context": "Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25].", "startOffset": 120, "endOffset": 132}, {"referenceID": 23, "context": "Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "Note that our setting is different from the statistical relational learning (SRL) setting, which deals with joins that violate the IID assumption and duplicate labeled examples from S [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 24, "context": "For the sake of tractability, in this paper, we adopt some assumptions from [26].", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "Numeric features can be discretized using standard techniques such as binning [29].", "startOffset": 78, "endOffset": 82}, {"referenceID": 34, "context": "Finally, we also do not study the \u201ccold start\u201d issue because it is orthogonal to the focus of this paper [36].", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "We take the seven real datasets from [26]; these are originally from Kaggle, GroupLens, openflights.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "More details about their schemas, including the list of all features are already in the public domain (listed in [26]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 24, "context": "For Naive Bayes, we used the code from [26], while for logistic regression with L1 regularization, we used the popular R package \u201cglmnet.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "We choose the popular Adam stochastic gradient optimization algorithm [20] with the learning rate tuned using the following grid axis: {10\u22123, 10\u22122, 10\u22121}.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Interestingly, on the Yelp dataset, in which both joins are known to be not safe to avoid with the linear classifiers [26], NoJoin correctly sees a large reduction in accuracy from JoinAll\u2013about 0.", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "This reaffirms the importance of foreign key features; as such, it is known that dropping foreign key features could cause the bias to shoot up with linear classifiers [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 24, "context": "These results are surprising given the more conservative behavior predicted even for the linear classifiers in [26].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "A key benefit of avoiding KFK joins safely is that ML runtimes (including feature selection) could be significantly lowered for the linear classifiers [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "All experiments (except for ANN) were run on CloudLab, which offers free access to physical compute nodes for research [35].", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "Thus, these results corroborate the orders of magnitude speedup reported in [26] for Naive Bayes with backward selection.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Note that our simulation methodology is not tied to decision trees; it is generic enough to be applicable to classifier because we only use standard generic notions of error and net variance as defined in [26].", "startOffset": 205, "endOffset": 209}, {"referenceID": 24, "context": "These scenarios represent opposite extremes for how likely the (test) error is likely to shoot up when XR is discarded and FK is used as a representative [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "We generate 100 different training datasets and measure the average test error and average net variance (as defined in [9]) based on the different models obtained from these 100 runs.", "startOffset": 119, "endOffset": 122}, {"referenceID": 24, "context": "In contrast to these results, [26] reported that for linear models, the errors of NoJoin shot up compared to JoinAll (a gap of nearly 0.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "This is akin to the extra overfitting reported in [26] using the plots of the net variance.", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "For the linear model case, [26] reported that as the skew parameters increased, the gap widened.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Note that the absolute generalization error is often high, which is expected for decision trees [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 43, "context": "While this seems similar to how deep neural networks excel at sheer memorization but still offer good test accuracy [45], the models in our setting are not necessarily memorizing all features \u2013 only the foreign keys.", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "Since FK already encodes all information that XR provides [26], the tree almost always uses FK in its partitioning, often multiple times.", "startOffset": 58, "endOffset": 62}, {"referenceID": 37, "context": "From a data management perspective, there are database dependencies more general than FDs: embedded multi-valued dependencies (EMVDs) and join dependencies (JDs) [39].", "startOffset": 162, "endOffset": 166}, {"referenceID": 37, "context": "How does the presence of such database dependencies among features affect the behavior of ML models? There are also conditional FDs, which satisfy FD-like constraints among subsets of the dataset [39].", "startOffset": 196, "endOffset": 200}, {"referenceID": 5, "context": "In contrast to prior work on handling regular large-domain features [7], foreign key features are distinct in that they have coarsergrained side information available in the form of foreign features.", "startOffset": 68, "endOffset": 71}, {"referenceID": 39, "context": "A standard unsupervised method to construct f is the Random hashing trick [41], i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": ", Laplacian smoothing for Naive Bayes by adding a pseudocount of 1 to all frequency counts [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "While similar smoothing techniques have been studied for probability estimation using decision trees [32], to the best of our knowledge, this issue has not been handled in general for classification using decision trees.", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 35, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 32, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 22, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 41, "context": "It was also studied in [43] but their focus was on devising a new ML algorithm.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "We first demonstrated the feasibility of avoiding joins safely in [26] for linear models.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "In this work, we revisit that idea for higher capacity models and find that they are counter-intuitively more robust than linear models to avoiding joins, not less as the VC dimension-based analysis in [26] suggested.", "startOffset": 202, "endOffset": 206}, {"referenceID": 0, "context": "Embedded multi-valued dependencies (EMVDs) are database dependencies that are more general than functional dependencies [2].", "startOffset": 120, "endOffset": 123}, {"referenceID": 28, "context": "The implication of EMVDs for probabilistic conditional independence in Bayesian networks was originally described by [30] and further explored by [42].", "startOffset": 117, "endOffset": 121}, {"referenceID": 40, "context": "The implication of EMVDs for probabilistic conditional independence in Bayesian networks was originally described by [30] and further explored by [42].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "There is a large body of work on statistical relational learning (SRL) to handle joins that cause duplicates in the fact table [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "The data mining and ML communities have long worked on feature selection methods to improve ML accuracy [14, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 13, "context": "The data mining and ML communities have long worked on feature selection methods to improve ML accuracy [14, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 12, "context": "The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].", "startOffset": 71, "endOffset": 83}, {"referenceID": 42, "context": "The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].", "startOffset": 71, "endOffset": 83}, {"referenceID": 19, "context": "The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].", "startOffset": 71, "endOffset": 83}, {"referenceID": 12, "context": "The conventional wisdom is that even a feature that is redundant might be highly relevant and hence, unavoidable in the mix [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "[40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "[40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.", "startOffset": 235, "endOffset": 238}, {"referenceID": 5, "context": "Scores such as Gini and information gain are known to be biased towards large-domain features in decision tree learning [7] and different approaches have explored alternatives to solve that issue [17].", "startOffset": 120, "endOffset": 123}, {"referenceID": 15, "context": "Scores such as Gini and information gain are known to be biased towards large-domain features in decision tree learning [7] and different approaches have explored alternatives to solve that issue [17].", "startOffset": 196, "endOffset": 200}, {"referenceID": 13, "context": "Unsupervised dimensionality reduction methods such as random hashing or PCA are also popular [15, 29].", "startOffset": 93, "endOffset": 101}, {"referenceID": 27, "context": "Unsupervised dimensionality reduction methods such as random hashing or PCA are also popular [15, 29].", "startOffset": 93, "endOffset": 101}, {"referenceID": 25, "context": "Integrating data and features from different sources for ML and data mining algorithms often requires applying and adapting techniques from the data integration literature [27, 8].", "startOffset": 172, "endOffset": 179}, {"referenceID": 6, "context": "Integrating data and features from different sources for ML and data mining algorithms often requires applying and adapting techniques from the data integration literature [27, 8].", "startOffset": 172, "endOffset": 179}, {"referenceID": 16, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 160, "endOffset": 164}, {"referenceID": 8, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 224, "endOffset": 228}], "year": 2017, "abstractText": "Machine learning (ML) over relational data is a booming area of the database industry and academia. While several projects aim to build scalable and fast ML systems, little work has addressed the pains of sourcing data and features for ML tasks. Real-world relational databases typically have many tables (often, dozens) and data scientists often struggle to even obtain and join all possible tables that provide features for ML. In this context, Kumar et al. showed recently that key-foreign key dependencies (KFKDs) between tables often lets us avoid such joins without significantly affecting prediction accuracy\u2013an idea they called \u201cavoiding joins safely.\u201d While initially controversial, this idea has since been used by multiple companies to reduce the burden of data sourcing for ML. But their work applied only to linear classifiers. In this work, we verify if their results hold for three popular high-capacity classifiers: decision trees, non-linear SVMs, and ANNs. We conduct an extensive experimental study using both real-world datasets and simulations to analyze the effects of avoiding KFK joins on such models. Our results show that these high-capacity classifiers are surprisingly and counter-intuitively more robust to avoiding KFK joins compared to linear classifiers, refuting an intuition from the prior work\u2019s analysis. We explain this behavior intuitively and identify open questions at the intersection of data management and ML theoretical research. All of our code and datasets are available for download from http://cseweb.ucsd.edu/~arunkk/hamlet.", "creator": "LaTeX with hyperref package"}}}