{"id": "1704.00616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection", "abstract": "General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.", "histories": [["v1", "Mon, 3 Apr 2017 14:29:40 GMT  (1536kb,D)", "https://arxiv.org/abs/1704.00616v1", "10 pages, 7 figures, ICCV 2017 submission"], ["v2", "Fri, 26 May 2017 18:40:14 GMT  (6367kb,D)", "http://arxiv.org/abs/1704.00616v2", "10 pages, 7 figures, ICCV 2017 submission"]], "COMMENTS": "10 pages, 7 figures, ICCV 2017 submission", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.HC cs.MM cs.NE", "authors": ["mohammadreza zolfaghari", "gabriel l oliveira", "nima sedaghat", "thomas brox"], "accepted": false, "id": "1704.00616"}, "pdf": {"name": "1704.00616.pdf", "metadata": {"source": "CRF", "title": "Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection", "authors": ["Mohammadreza Zolfaghari", "Gabriel L. Oliveira", "Nima Sedaghat"], "emails": ["zolfagha@cs.uni-freiburg.de", "oliveira@cs.uni-freiburg.de", "nima@cs.uni-freiburg.de", "brox@cs.uni-freiburg.de"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to survive on their own."}, {"heading": "2. Related work", "text": "Many traditional work in the field of action detection has focused on designing features to distinguish between action classes [17, 40, 5, 16], which have been encoded with high-level encodings, such as Word Bags (BoW) [35] or Fisher vector-based encodings [31], to produce a global representation for video and to train a classifier for action marks. Recent research has shown that most of these approaches are not only computationally expensive, but also fail to capture contexts and high-level information techniques. CNN-based approaches have enabled the exchange of handmade features through learned features and the learning of whole tasks end-to-end. Several works have used deep architectures for video classification [24, 37, 41]. Thanks to their local functional representation, deep networks, localized features, and context sequences and contextual quests can be captured and highly relevant video can be accessed from large-area information."}, {"heading": "3. Inputs to the Network", "text": "We rely on three input signals: the raw RGB images, the optical flow and the human pose in the form of the segmentation of human body parts. All inputs are provided as spatio-temporal inputs covering multiple frames."}, {"heading": "3.1. Optical Flow", "text": "We calculate the optical flux using the method of Zach et al. [48], a reliable method of variation that is sufficiently fast. We convert the x component and the y component of the optical flux into a 3-channel RGB image by stacking components and their size on top of each other [29]. The flux and brightness values in the image are multiplied by 16 and quantified to the [0,255] interval [18, 29, 42, 43]."}, {"heading": "3.2. Body Part Segmentation", "text": "Encoder decoder architectures with an up-convolutional part have been successfully used for semantic segmentation tasks [23, 22, 30, 3, 27], depth estimation [20] and optical flow estimation [7]. For this work, we use Fast-Net [27], a network for segmentation of human body parts that will provide information about the body to our action detection network. Figure 2 illustrates the architecture of FastNet. The encoder part of the network is initialized with the VGG network [34]. Skip connections from encoder to decoder part ensure the reconstruction of details in the output to the original input resolution. We trained the Fast-Net architecture based on the J-HMDB [12] and the MPII [1] action detection data set. J-HMDB provides body segmentation masks and joint locations, while MPII only provides common locations. To make body detection masks compatible across the data sets, we only need to apply the common areas."}, {"heading": "4. Action Recognition Network", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Multi-stream Fusion with a Markov Chain", "text": "In order to integrate information from the different inputs, we rely on the model of a multi-stream architecture [33], i.e., each input is fed into a separate Convolutional Network Stream, which is geared towards action classification. The innovation in our approach is the way we combine these streams. In contrast to previous work, we combine functions from the different streams sequentially. Based on the different input modalities, we will apply the evidence for an action class with the conditional flow stream and finally apply a refinement by the RGB streams. We assume that class predictions are conditionally independent of the different input modalities. Consequently, the common probability across all input streams is factored into the individual input streams. In a Markov chain, we have a sequence of inputs X = {X1, X2, XS} if we want to predict the output sequence Y sequences."}, {"heading": "4.2. Network Configuration", "text": "In all data streams we use the C3D architecture [37] as the basic architecture with 17.5 million parameters. The network has 8 three-dimensional folding layers with a core size of 3 x 3 x 3 and step 1, 5 three-dimensional pooling layers with a core size of 2 x 2 x 2 and step 2 and two completely connected layers followed by a Softmax; see figure 5. Each data stream is connected to the next data stream via layer FC6; see figure 4-right. Each data stream takes 16 images as input."}, {"heading": "4.3. Training", "text": "The network weights are calculated using mini-batch stochastic gradient descent (SGD) with a pulse of 0.9 and weight breakdown of 5e \u2212 4. Together, we optimize the entire network without abbreviating the gradients and update the weights of each stream based on the full gradient including the contribution from the following stream. We initialize the learning rate with 1e \u2212 4 and decrease it by a factor of 10 every 2k for J-HMDB and NTU records, 20k for UCF101 and NTU and in several steps for HMDB51. The maximum number of iterations was 20k for J-HMDB, 40k for HMDB51 and 60k for the UCF101 and NTU records. We initialize the weights of all streams using an RGB network pre-trained on the large Sport-1M record, and calculate the videos as clips of 16 frames with an overlap of 8 frames, and feed each clip into the entire network using 112 \u00d7 16 points of the image."}, {"heading": "4.4. Temporal Processing of the Whole Video", "text": "At test date, we feed the architecture with a time window of 16 frames. The step over the video is 8. Each set of inputs is randomly selected for crop operations, i.e. 4 corners and 1 center section for the original image and its horizontal counterpart. We extract values before Softmax normalization in the last stream (Y-RGB). In the case of action classification, the final value of a video is calculated by taking the average of values across all pore windows of a video and 10 intersections per clip. In addition to averaging, we also tested a multi-resolution approach called multigranular (MG), in which we trained separate networks for three different time resolutions, which are composed as (1) 16 consecutive frames, (2) 16 frames from a time window of 32 frames at a sample rate of 2 and (3) 16 frames in a time-limited span interval."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "UCF-101 [36] contains more than 2 million images in more than 13,000 videos, divided into 101 human action classes. HMDB51 [15] contains 6766 videos, which are divided into 51 action classes with at least 101 samples each, and the UCF101 data set also contains a subset for spatio-temporal action detection. HMDB51 [15] contains 6766 videos, which are divided into 51 action classes with at least 101 samples each. [12] The evaluation follows the same protocol used for UCF-101.J-HMDB, and contains a subset of videos from the HMDB data set for which it provides additional annotations, in particular for optical flow and joint localization [12]. Therefore, it is well suited for evaluating the contribution of optical flow, segmentation of body parts, and merging of all queues via a Markov chain. The data set includes 21 human actions. The full dataset has 928 clips and 1838 images. There are 3 Fold and 3 data classes to be used in this data base and the Fold and the Fold and the Data Classes in the GB."}, {"heading": "5.2. Action Classification", "text": "Table 1 shows that fusion with the Markov sequential chain model consistently outperforms nuclear fusion across all datasets; nuclear fusion is shown in Figure 4 and can be considered a strong baseline; it consists of merging the multiple modalities through feature concatenation followed by a set of fully interconnected layers; the network is trained together; adding Pose results in a significant improvement over the two-stream version; this confirms that Pose plays an important role as a complementary modality for action detection tasks. Again, the Markov chain fusion is a great advantage; for the J-HMDB dataset, the truth is available for optical flow and pose and can be provided to the method; although it is not relevant in practice, performing detection with this pose shows how much power is lost due to faulty optical flow and pose; surprisingly, the difference between the results is rather small, showing that the network is not lacking in perfect estimates."}, {"heading": "5.2.1 Comparison with the state-of-the-art", "text": "Table 3 compares the proposed network with the state of the art in the action classification. Unlike Table 1, the comparison no longer shows the direct impact of individual contributions, as this table compares whole systems based on entirely different components, many of which also use other methods of extracting features, such as improved density trajectories (IDT), which generally have a positive impact on results, but make the system more complicated and difficult to control. Our network exceeds the state of the art for J-HMDB, NTU and HMDB51. Even for UCF101 datasets, our approach is state-of-the-art, while not relying on additional craftsmanship features. In two stream cases (RGB + OF), when we replace the 3DCNN network with the TSN approach [43], we obtain a classification accuracy of 94.05% for UCF101 (over 3 splits), which is state of the art when the result is greater than the TSN network we use for detecting the data."}, {"heading": "5.2.2 Ordering of modalities in the Markov chain.", "text": "Table 4 shows an analysis of how the sequence of modalities affects the final classification accuracy, and the proposed sequence clearly has an impact. The proposed sequence starts with the pose and then adds the optical flow and RGB images that performed best, but there are alternative jobs that do not perform much worse. Table 5 quantifies the improvement in accuracy when adding a modality. Of course, each additional modality improves the results."}, {"heading": "5.2.3 Fusion location", "text": "In contrast to the large-scale evaluation by Feichtenhofer et al. [8], we tested only two sites: FC6 and FC7. Table 6 shows a clear difference only in the J-HMDB dataset, where an earlier fusion appears to be advantageous on a level where the characteristics are not yet too abstract. Similarly, the results of the study by Feichtenhofer et al. [8] show that the last wave layer worked best."}, {"heading": "5.2.4 Effect of clip length", "text": "Larger windows clearly improve the accuracy of all datasets; see Table 7. For the J-HMDB dataset (RGB modality), we use a time frame of 4 to 16 frames every 4 frames. The highest accuracy is achieved with a clip size of 16 frames. Based on the J-HMDB minimum video size, 16 is the highest possible time frame to investigate. We also tested several time resolutions for the NTU dataset (Pose Modality). Again, we achieved the best results for the network with the larger clip length as input.The experiments conducted confirm that as the clip lengths increase, we reduce the likelihood of obtaining unrelated parts of an action in a video. In addition, 3D windings with longer sequences can better exploit their ability to capture abstract spatial-time characteristics to detect actions."}, {"heading": "5.3. Action Detection", "text": "To demonstrate the universality of our approach, we also show results on action detection on UCF101 and J-HMDB. Although many of the most powerful action classification methods are not applicable to action detection because they integrate information in a complex way over time, they are too slow or unable to locate the action spatially, which is different for our approach, which is efficient and can be executed over time in a sliding window and provides good spatial localization through the segmentation of the human body. In order to create temporally consistent spatial detection, we link the action boxes over time to the productive action tube [9]; see the supplementary material for details. We use the framework action classification to make predictions at the tube level. Figure 6 schematically outlines the detection methods we present, a number of qualitative action experts who provide imaging tests for the UCJ-MJ and HMASF data attempts."}, {"heading": "6. Conclusion", "text": "We have shown that this sequential fusion significantly outperforms other methods of fusion, as it can take into account signal interdependencies during training, while avoiding over-matching due to very large network models. We have demonstrated the value of providing state-of-the-art performance in all four demanding UCF101, HMDB51, J-HMDB, and NTU RGB + D action classification datasets, without additional handcrafted features. We have also demonstrated the value of reliable pose representation valued over a fast Constitutional Network. Finally, we have shown that the approach is also generalized to spatial and spatio-temporal action detection, where we have also achieved state-of-the-art results."}, {"heading": "7. Acknowledgements", "text": "We acknowledge the support of the ERC Starting Grant VideoLearn and the Freiburg Graduate School of Robotics."}], "references": [{"title": "2d human pose estimation: New benchmark and state of the art analysis", "author": ["M. Andriluka", "L. Pishchulin", "P. Gehler", "B. Schiele"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequential deep learning for human action recognition", "author": ["M. Baccouche", "F. Mamalet", "C. Wolf", "C. Garcia", "A. Baskurt"], "venue": "Proceedings of the Second International Conference on Human Behavior Unterstanding, HBU\u201911, pages 29\u201339,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv: 1511.00561,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "P-CNN: Pose-based CNN Features for Action Recognition", "author": ["G. Ch\u00e9ron", "I. Laptev", "C. Schmid"], "venue": "ICCV,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905) - Volume 1 - Volume 01, CVPR \u201905, pages 886\u2013893, Washington, DC, USA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient twostream motion and appearance 3d cnns for video classification", "author": ["A. Diba", "A.M. Pazandeh", "L.V. Gool"], "venue": "CoRR, abs/1608.08851,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["A. Dosovitskiy", "P. Fischer", "E. Ilg", "P. Hausser", "C. Hazrba", "V. Golkov", "P. v.d. Smagt", "D. Cremers", "T. Brox"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Convolutional two-stream network fusion for video action recognition", "author": ["C. Feichtenhofer", "A. Pinz", "A. Zisserman"], "venue": " IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding action tubes", "author": ["G. Gkioxari", "J. Malik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Jointly learning heterogeneous features for rgb-d activity recognition", "author": ["J.F. Hu", "W. shi Zheng", "J. Lai", "J. Zhang"], "venue": "In Computer Vision and Pattern Recognition (CVPR) (In press),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "What do 15,000 object categories tell us about classifying and localizing actions", "author": ["M. Jain", "J.C. van Gemert", "C.G.M. Snoek"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Towards understanding action recognition", "author": ["H. Jhuang", "J. Gall", "S. Zuffi", "C. Schmid", "M.J. Black"], "venue": "International Conf. on Computer Vision (ICCV), pages 3192\u20133199,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "HMDB: a large video database for human motion recognition", "author": ["H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "Proceedings of the International Conference on Computer Vision (ICCV),", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "On space-time interest points", "author": ["I. Laptev"], "venue": "Int. J. Comput. Vision, 64(2-3):107\u2013123, Sept.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, pages 2278\u20132324,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Action recognition by learning deep multi-granular spatio-temporal video representation", "author": ["Q. Li", "Z. Qiu", "T. Yao", "T. Mei", "Y. Rui", "J. Luo"], "venue": "Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval, ICMR \u201916, pages 159\u2013166, New York, NY, USA,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A hierarchical pose-based approach to complex action understanding using dictionaries of actionlets and motion poselets", "author": ["I. Lillo", "J.C. Niebles", "A. Soto"], "venue": "CoRR, abs/1606.04992,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning depth from single monocular images using deep convolutional neural fields", "author": ["F. Liu", "C. Shen", "G. Lin", "I.D. Reid"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 38(10):2024\u2013 2039,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition, pages 816\u2013833", "author": ["J. Liu", "A. Shahroudy", "D. Xu", "e. B. Wang", "Gang", "J. Matas", "N. Sebe", "M. Welling"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "arXiv preprint arXiv: 1506.04579,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, Nov.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularizing long short term memory with 3d human-skeleton sequences for action recognition", "author": ["B. Mahasseni", "S. Todorovic"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint angles similarities and hog2 for action recognition", "author": ["E. Ohn-Bar", "M.M. Trivedi"], "venue": "Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW \u201913, pages 465\u2013470, Washington, DC, USA,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved dense trajectory with cross streams", "author": ["K. Ohnishi", "M. Hidaka", "T. Harada"], "venue": "Proceedings of the 2016 ACM on Multimedia Conference, MM \u201916, pages 257\u2013261, New York, NY, USA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient deep models for monocular road segmentation", "author": ["G.L. Oliveira", "W. Burgard", "T. Brox"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Combining multiple sources of knowledge in deep cnns for action recognition", "author": ["E. Park", "X. Han", "T.L. Berg", "A.C. Berg"], "venue": "WACV, pages 1\u20138. IEEE Computer Society,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-region two-stream R-CNN for action detection", "author": ["X. Peng", "C. Schmid"], "venue": "ECCV 2016 - European Conference on Computer Vision, Amsterdam, Netherlands, Oct.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), volume 9351 of LNCS, pages 234\u2013241", "author": ["O. Ronneberger", "P.Fischer", "T. Brox"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J. Sanchez", "F. Perronnin", "T.E.J. Mensink", "J. Verbeek"], "venue": "International Journal of Computer Vision,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Ntu rgb+d: A large scale dataset for 3d human activity analysis", "author": ["A. Shahroudy", "J. Liu", "T.-T. Ng", "G. Wang"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "Proceedings of the Ninth IEEE International Conference on Computer Vision - Volume 2, ICCV \u201903, pages 1470\u2013, Washington, DC, USA,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["k. Soomro", "A. Roshan Zamir", "M. Shah"], "venue": "In CRCV-TR-12-01,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), pages 4489\u20134497,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term temporal convolutions for action recognition", "author": ["G. Varol", "I. Laptev", "C. Schmid"], "venue": "CoRR, abs/1604.04494,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "An approach to posebased action recognition", "author": ["C. Wang", "Y. Wang", "A.L. Yuille"], "venue": "Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition,  CVPR \u201913, pages 915\u2013922, Washington, DC, USA,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "Proceedings of the 2013 IEEE International Conference on Computer Vision, ICCV \u201913, pages 3551\u2013 3558, Washington, DC, USA,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Action recognition with trajectory-pooled deep-convolutional descriptors", "author": ["L. Wang", "Y. Qiao", "X. Tang"], "venue": "CVPR, pages 4305\u20134314,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Actionness estimation using hybrid fully convolutional networks", "author": ["L. Wang", "Y. Qiao", "X. Tang", "L. Van Gool"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2708\u20132717,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Temporal segment networks: Towards good practices for deep action recognition", "author": ["L. Wang", "Y. Xiong", "Z. Wang", "Y. Qiao", "D. Lin", "X. Tang", "L. Val Gool"], "venue": "ECCV,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to track for spatio-temporal action localization", "author": ["P. Weinzaepfel", "Z. Harchaoui", "C. Schmid"], "venue": "ICCV 2015 - IEEE International Conference on Computer Vision, pages 3164\u20133172, Santiago, Chile, Dec.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards weaklysupervised action localization", "author": ["P. Weinzaepfel", "X. Martin", "C. Schmid"], "venue": "CoRR, abs/1605.05197,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Fusing multi-stream deep networks for video classification", "author": ["Z. Wu", "Y. Jiang", "X. Wang", "H. Ye", "X. Xue", "J. Wang"], "venue": "CoRR, abs/1509.06086,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast action proposals for human action detection and search", "author": ["G. Yu", "J. Yuan"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1302\u20131311, June", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "A duality based approach for realtime tv-l1 optical flow", "author": ["C. Zach", "T. Pock", "H. Bischof"], "venue": "Proceedings of the 29th DAGM Conference on Pattern Recognition, pages 214\u2013223, Berlin, Heidelberg,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "A key volume mining deep framework for action recognition", "author": ["W. Zhu", "J. Hu", "G. Sun", "X. Cao", "Y. Qiao"], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1991\u20131999, June", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "Actions can span various time intervals, making good use of videos and their temporal context is a prerequisite for solving the task to its full extent [38, 37].", "startOffset": 152, "endOffset": 160}, {"referenceID": 36, "context": "Actions can span various time intervals, making good use of videos and their temporal context is a prerequisite for solving the task to its full extent [38, 37].", "startOffset": 152, "endOffset": 160}, {"referenceID": 11, "context": "[12], multistream architectures have been most popular.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "This trend was initiated by Simonyan and Zisserman [33], who proposed a simple fusion of the action class scores obtained with two separate convolutional networks, where one was trained on raw images and the other on optical flow.", "startOffset": 51, "endOffset": 55}, {"referenceID": 36, "context": "Moreover, we use a network architecture with spatio-temporal convolutions [37].", "startOffset": 74, "endOffset": 78}, {"referenceID": 42, "context": ", temporal segmentation networks [43], the architecture still allows the temporal localization of actions by providing actionness scores of frames using a sliding window over video.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Many traditional works in the field of action recognition focused on designing features to discriminate action classes [17, 40, 5, 16].", "startOffset": 119, "endOffset": 134}, {"referenceID": 39, "context": "Many traditional works in the field of action recognition focused on designing features to discriminate action classes [17, 40, 5, 16].", "startOffset": 119, "endOffset": 134}, {"referenceID": 4, "context": "Many traditional works in the field of action recognition focused on designing features to discriminate action classes [17, 40, 5, 16].", "startOffset": 119, "endOffset": 134}, {"referenceID": 15, "context": "Many traditional works in the field of action recognition focused on designing features to discriminate action classes [17, 40, 5, 16].", "startOffset": 119, "endOffset": 134}, {"referenceID": 34, "context": ", bag of words (BoW) [35] or Fisher vector based encodings [31], to produce a global representation for video and to train a classifier on the action labels.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": ", bag of words (BoW) [35] or Fisher vector based encodings [31], to produce a global representation for video and to train a classifier on the action labels.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "Several works employed deep architectures for video classification [24, 37, 41].", "startOffset": 67, "endOffset": 79}, {"referenceID": 36, "context": "Several works employed deep architectures for video classification [24, 37, 41].", "startOffset": 67, "endOffset": 79}, {"referenceID": 40, "context": "Several works employed deep architectures for video classification [24, 37, 41].", "startOffset": 67, "endOffset": 79}, {"referenceID": 1, "context": "[2] firstly used a 3D CNN to learn spatio-temporal features from video and in the next step they employed an LSTM to classify video sequences.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "More recently, several CNN based works presented efficient deep models for action recognition [6, 29, 37].", "startOffset": 94, "endOffset": 105}, {"referenceID": 28, "context": "More recently, several CNN based works presented efficient deep models for action recognition [6, 29, 37].", "startOffset": 94, "endOffset": 105}, {"referenceID": 36, "context": "More recently, several CNN based works presented efficient deep models for action recognition [6, 29, 37].", "startOffset": 94, "endOffset": 105}, {"referenceID": 36, "context": "[37] employed a 3D architecture to learn spatio-temporal features from videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] proposed a two-stream CNN to capture the complementary information from appearance and motion, each modality in an independent stream.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] investigated the optimal position within a convolution network in detail to combine the separate streams.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28] proposed a gated fusion approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46] presented an adaptive fusion approach, which uses two regularization terms to learn fusion weights.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "In addition to optical flow, some works made use of other modalities like audio [46], warped flow [43], and object information [11] to capture complementary information for video classification.", "startOffset": 80, "endOffset": 84}, {"referenceID": 42, "context": "In addition to optical flow, some works made use of other modalities like audio [46], warped flow [43], and object information [11] to capture complementary information for video classification.", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "In addition to optical flow, some works made use of other modalities like audio [46], warped flow [43], and object information [11] to capture complementary information for video classification.", "startOffset": 127, "endOffset": 131}, {"referenceID": 3, "context": "Thus, this information has been employed for action recognition in several works [4, 19, 39].", "startOffset": 81, "endOffset": 92}, {"referenceID": 18, "context": "Thus, this information has been employed for action recognition in several works [4, 19, 39].", "startOffset": 81, "endOffset": 92}, {"referenceID": 38, "context": "Thus, this information has been employed for action recognition in several works [4, 19, 39].", "startOffset": 81, "endOffset": 92}, {"referenceID": 3, "context": "[4] used pose information to extract high-level features from appearance and optical flow.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "[39] used data mining techniques to obtain a representation for each video and finally, by using a bag-of-words model to classify videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[48], which is a reliable variational method that runs sufficiently fast.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "We convert the x-component and ycomponent of the optical flow to a 3 channel RGB image by stacking components and magnitude of them [29].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "The flow and magnitude values in the image are multiplied by 16 and quantized into the [0,255] interval [18, 29, 42, 43].", "startOffset": 104, "endOffset": 120}, {"referenceID": 28, "context": "The flow and magnitude values in the image are multiplied by 16 and quantized into the [0,255] interval [18, 29, 42, 43].", "startOffset": 104, "endOffset": 120}, {"referenceID": 41, "context": "The flow and magnitude values in the image are multiplied by 16 and quantized into the [0,255] interval [18, 29, 42, 43].", "startOffset": 104, "endOffset": 120}, {"referenceID": 42, "context": "The flow and magnitude values in the image are multiplied by 16 and quantized into the [0,255] interval [18, 29, 42, 43].", "startOffset": 104, "endOffset": 120}, {"referenceID": 22, "context": "Encoder-decoder architectures with an up-convolutional part have been used successfully for semantic segmentation tasks [23, 22, 30, 3, 27], depth estimation [20] and optical", "startOffset": 120, "endOffset": 139}, {"referenceID": 21, "context": "Encoder-decoder architectures with an up-convolutional part have been used successfully for semantic segmentation tasks [23, 22, 30, 3, 27], depth estimation [20] and optical", "startOffset": 120, "endOffset": 139}, {"referenceID": 29, "context": "Encoder-decoder architectures with an up-convolutional part have been used successfully for semantic segmentation tasks [23, 22, 30, 3, 27], depth estimation [20] and optical", "startOffset": 120, "endOffset": 139}, {"referenceID": 2, "context": "Encoder-decoder architectures with an up-convolutional part have been used successfully for semantic segmentation tasks [23, 22, 30, 3, 27], depth estimation [20] and optical", "startOffset": 120, "endOffset": 139}, {"referenceID": 26, "context": "Encoder-decoder architectures with an up-convolutional part have been used successfully for semantic segmentation tasks [23, 22, 30, 3, 27], depth estimation [20] and optical", "startOffset": 120, "endOffset": 139}, {"referenceID": 19, "context": "Encoder-decoder architectures with an up-convolutional part have been used successfully for semantic segmentation tasks [23, 22, 30, 3, 27], depth estimation [20] and optical", "startOffset": 158, "endOffset": 162}, {"referenceID": 6, "context": "flow estimation [7].", "startOffset": 16, "endOffset": 19}, {"referenceID": 26, "context": "For this work, we make use of Fast-Net [27], a network for human body part segmentation, which will provide our action recognition network with body pose information.", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "The encoder part of the network is initialized with the VGG network [34].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "We trained the Fast-Net architecture on the J-HMDB [12] and the MPII [1] action recognition datasets.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "We trained the Fast-Net architecture on the J-HMDB [12] and the MPII [1] action recognition datasets.", "startOffset": 69, "endOffset": 72}, {"referenceID": 32, "context": "To integrate information from the different inputs we rely on the model of a multi-stream architecture [33], i.", "startOffset": 103, "endOffset": 107}, {"referenceID": 36, "context": "In all streams, we use the C3D architecture [37] as the base architecture, which has 17.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "We initialize the weights of all streams with an RGB network pre-trained on the large-scale Sports-1M dataset [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "We used Caffe [13] and an NVIDIA Titan X GPU to run our experiments.", "startOffset": 14, "endOffset": 18}, {"referenceID": 35, "context": "UCF-101 [36] contains more than 2 million frames in more than 13, 000 videos, which are divided into 101 human action classes.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "HMDB51 [15] contains 6766 videos divided into 51 action classes, each with at least 101 samples.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "J-HMDB contains a subset of videos from the HMDB dataset, for which it provides additional annotation, in particular optical flow and joint localization [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 31, "context": "NTU RGB+D is a recent action recognition dataset that is quite large and provides depth and pose ground truth [32].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "TS Fusion [8] 92.", "startOffset": 10, "endOffset": 13}, {"referenceID": 37, "context": "4% LTC [38] 91.", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "8% Two-stream [33] 88.", "startOffset": 14, "endOffset": 18}, {"referenceID": 42, "context": "4% TSN [43] 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "4% CPD [26] 92.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "2% Multi-Granular [18] 90.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "6% M-fusion [28] 89.", "startOffset": 12, "endOffset": 16}, {"referenceID": 48, "context": "9% KVMF [49] 93.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "3% P-CNN [4] 61.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "1% Action tubes [9] 62.", "startOffset": 16, "endOffset": 19}, {"referenceID": 28, "context": "5% TS R-CNN [29] 70.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "5% MR-TS R-CNN [29] 71.", "startOffset": 15, "endOffset": 19}, {"referenceID": 42, "context": "In two stream case (RGB+OF), if we replace the 3DCNN network by the TSN approach [43], we obtain a classification accuracy of 94.", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "Deep LSTM [32] 60.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "7% P-LSTM [32] 62.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "93% HOG\u02c62 [25] 32.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "2% FTP DS [10] 60.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "23% ST-LSTM [21] 69.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "[8], we tested only two locations: FC6 and FC7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], where the last convolutional layer worked best.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "duce action tube [9]; see the supplemental material for details.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "Following recent works on action detection [9, 44, 29], we report video-AP.", "startOffset": 43, "endOffset": 54}, {"referenceID": 43, "context": "Following recent works on action detection [9, 44, 29], we report video-AP.", "startOffset": 43, "endOffset": 54}, {"referenceID": 28, "context": "Following recent works on action detection [9, 44, 29], we report video-AP.", "startOffset": 43, "endOffset": 54}, {"referenceID": 8, "context": "Compared to the recent works [9, 45, 29, 47], our detection framework has two desirable properties: (1) the pose network directly provides a single detection box per person, which causes a large speed-up; (2) the classification Figure 7: Qualitative results on the action detection task.", "startOffset": 29, "endOffset": 44}, {"referenceID": 44, "context": "Compared to the recent works [9, 45, 29, 47], our detection framework has two desirable properties: (1) the pose network directly provides a single detection box per person, which causes a large speed-up; (2) the classification Figure 7: Qualitative results on the action detection task.", "startOffset": 29, "endOffset": 44}, {"referenceID": 28, "context": "Compared to the recent works [9, 45, 29, 47], our detection framework has two desirable properties: (1) the pose network directly provides a single detection box per person, which causes a large speed-up; (2) the classification Figure 7: Qualitative results on the action detection task.", "startOffset": 29, "endOffset": 44}, {"referenceID": 46, "context": "Compared to the recent works [9, 45, 29, 47], our detection framework has two desirable properties: (1) the pose network directly provides a single detection box per person, which causes a large speed-up; (2) the classification Figure 7: Qualitative results on the action detection task.", "startOffset": 29, "endOffset": 44}, {"referenceID": 41, "context": "Actionness [42] - - - - 56.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "4 ActionTubes [9] - - - - 53.", "startOffset": 14, "endOffset": 17}, {"referenceID": 43, "context": "[44] - 63.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] - 74.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] 54.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] 42.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] 54.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] 62.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.", "creator": "LaTeX with hyperref package"}}}