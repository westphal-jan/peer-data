{"id": "1704.00774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality for Increased Model Capacity and Performance With No Computational Overhead", "abstract": "Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, resulting in a significant increase of computational cost. An alternative is the recurrent neural tensor network (RNTN), which increases capacity by employing distinct hidden layer weights for each vocabulary word. The disadvantage of RNTNs is that memory usage scales linearly with vocabulary size, which can reach millions for word-level language models. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations using the Penn Treebank corpus show that r-RNTNs improve language model performance over standard RNNs using only a small fraction of the parameters of unrestricted RNTNs.", "histories": [["v1", "Mon, 3 Apr 2017 19:17:58 GMT  (22kb)", "http://arxiv.org/abs/1704.00774v1", null], ["v2", "Sat, 15 Apr 2017 02:59:14 GMT  (23kb)", "http://arxiv.org/abs/1704.00774v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandre salle", "aline villavicencio"], "accepted": false, "id": "1704.00774"}, "pdf": {"name": "1704.00774.pdf", "metadata": {"source": "CRF", "title": "Restricted Recurrent Neural Tensor Networks", "authors": ["Alexandre Salle", "Aline Villavicencio"], "emails": ["atsalle@inf.ufrgs.br", "avillavicencio@inf.ufrgs.br"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.00 774v 1 [cs.C L] 3A pr2 017ral networks (RNN) usually involves enlarging the hidden layer, resulting in a significant increase in computing costs. An alternative is the recursive neural tensor network (RNTN), which increases capacity by using different hidden layer weights for each vocabulary word. The disadvantage of RNTNs is that memory usage is scaled linearly with the vocabulary size, which can reach millions for speech models at the word level. In this essay, we introduce restricted, recurring neural tensor networks (r-RNTN), which reserve different hidden layer weights for frequent vocabulary, while sharing only a single set of weights for rare words. Perplexit evaluations based on the Penn treebank corpus show that r-RNTN's model is an improvement on the performance of the NNTN parameter compared to the standard RNTN, with only a smaller language parameter."}, {"heading": "1 Introduction", "text": "The task is to estimate the conditional probability of a symbol in view of the sequence of symbols that preceded it. (RNN) It is a natural solution to the sequence of symbols that preceded it. (RNN) It is a form that has conditioned its next output. (RNN) It is a form in which there is a natural sequence of symbols. (RNN) It is a form that refers to the choice of words. (RNN) It is a form that relies on traditional n-grammar methods. (RNN) It is a way in which there is a sequence of terms. (RNN) It is a way in which there is a sequence of concepts in which there is a sequence of concepts.) It is a different way in which there is a sequence of concepts. (RNN) It is a way in which there is a sequence of concepts. (RNN) It is a way in which there is a sequence of concepts."}, {"heading": "2 Related Work", "text": "It is about the question of to what extent it is a matter of a way in which people in a world, in which people in a world, in which people in a world, in which people in a world, in which people in a world, in which people in a world, in a world, in which people in a world, in a world, in a world, in a world, in a world, in a world, in a world, in which people in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in a world, in all, in a world, in a world, in all, in a world, in a world, in all, in a world, in a world, in all, in a world, in a people, in a world, in a world, in all, in a world, in a world, in all, in a world, in a world, in all, in a world, in all, in a world, in a world, in all, in a world, in all, in a world, in a world, in all, in a world, in all, in a people, in a world, in a world, in all, in a world, in all, in all, in a world, in a world, in a world, in all, in all, in all, in a world, in a world, in all, in a people."}, {"heading": "3 Restricted Recurrent Neural Tensor Networks", "text": "To balance expressiveness and computational costs, we propose to limit the size of the recursion tensor in the RNTN so that the memory does not grow linearly with the vocabulary size, while maintaining dedicated matrix representations for a subset of words in the vocabulary. We call these Restricted Recurrent Tensor Networks (rRNTN), which modify eq. (3) as follows: ht = \u03c3 (Whxxt + W f (i (xt)))) hh ht \u2212 1 + b f (i (xt) h) h) (4) 1It should be noted that the r-RNTN can also benefit from factoring and it is something we will explore in the future.Where Whh represents a tensor of K < | V | matrices of size H \u00b7 H, bh is the aK \u00d7 H bias matrix with lines indexed by the function mapping each word."}, {"heading": "4 Materials", "text": "As in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al., 1994) using the same pre-processing as Mikolov et al. (2011), which is divided into training, validation, and test kits containing 929K words, 73K words, and 82K words. Vocabulary is limited to the most common 10,000 words, and all other words are divided into the \"< unk >\" RNropes."}, {"heading": "5 Results", "text": "The results are presented in Figures 1 and 2, and some examples are given in Table 1.Comparing r-RNTN with baseline s-RNN with H = 100, we find that as model capacity increases with K, the test theorem perplexity decreases, which shows that r-RNTN is an effective method of increasing model capacity without additional computational costs. As expected, the f-figure exceeds the pseudo-random fmod mapping of the baseline with smaller K. AsK increases, we see a convergence of the two figures. This can be explained by the fact that in large K, the most common words are rarely observed due to module peration in q. (6), the rare words, frequent words dominate the matrix updates and have approximately clear matrices, as they would do with the f-figure. It is noteworthy that even at small K, the frequency of the NRN is not significantly increased with the NRN-frequency difference."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we proposed to limit the size of recurrent neural tensor networks by assigning common words to different matrices and rare words to common matrices, a model that we call limited recurrent neural tensor networks (rRNTNs). This model was motivated by the need to increase the capacity of the RNN model without increasing computing costs, and at the same time fulfilled the compositional ideas discussed by Baroni and Zamparelli (2010) and Socher et al. (2012), which suggest that some words are better modelled by matrices than by vectors, and vice versa. Both goals were achieved by reducing the size of the recurrent neural tensor network described by Sutskever et al. (2011) by meaningful word tomatrix mapping. The results confirmed our hypothesis that frequent words benefit from richer, dedicated modeling, and this is mirrored by tensors in low perplexes."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Associ-", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Conditional computation in neural networks for faster models", "author": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup."], "venue": "arXiv preprint arXiv:1511.06297 .", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning", "author": ["Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.7362 .", "citeRegEx": "Cho and Bengio.,? 2014", "shortCiteRegEx": "Cho and Bengio.", "year": 2014}, {"title": "The 385+ million word corpus of contemporary american english (1990\u20132008+): Design, architecture, and linguistic insights", "author": ["Mark Davies."], "venue": "International journal of corpus linguistics 14(2):159\u2013190.", "citeRegEx": "Davies.,? 2009", "shortCiteRegEx": "Davies.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Modeling compositionality with multiplicative recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "arXiv preprint arXiv:1412.6577 .", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary A. Marcinkiewicz."], "venue": "Computational Linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1994", "shortCiteRegEx": "Marcus et al\\.", "year": 1994}, {"title": "Empirical evaluation and combination of advanced languagemodeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernock\u00fd."], "venue": "INTERSPEECH. pages 605\u2013608.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee W. Teh."], "venue": "arXiv preprint arXiv:1206.6426 .", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean."], "venue": "arXiv preprint arXiv:1701.06538 .", "citeRegEx": "Shazeer et al\\.,? 2017", "shortCiteRegEx": "Shazeer et al\\.", "year": 2017}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher DManning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "Mikolov et al. (2010) applied RNNs to wordlevel language modeling (we refer to this models as s-RNN), outperforming traditional n-gram methods.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN\u2019s hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform the network\u2019s hidden state in different ways (Sutskever et al., 2011).", "startOffset": 295, "endOffset": 319}, {"referenceID": 0, "context": "larly argue that some words are better modeled by matrices than by vectors (Baroni and Zamparelli, 2010; Socher et al., 2012).", "startOffset": 75, "endOffset": 125}, {"referenceID": 12, "context": "larly argue that some words are better modeled by matrices than by vectors (Baroni and Zamparelli, 2010; Socher et al., 2012).", "startOffset": 75, "endOffset": 125}, {"referenceID": 3, "context": "words among the most frequent words in standard corpora like the Corpus of Contemporary American English (COCA) (Davies, 2009).", "startOffset": 112, "endOffset": 126}, {"referenceID": 7, "context": "By using a RNN, Mikolov et al. (2010) created the language model (which we refer to as s-RNN) given by:", "startOffset": 16, "endOffset": 38}, {"referenceID": 14, "context": "The RNTN proposed by Sutskever et al. (2011) is nearly identical to the s-RNN, but the recurrence matrix in eq.", "startOffset": 21, "endOffset": 45}, {"referenceID": 0, "context": "For instance, Baroni and Zamparelli (2010) argue for nouns", "startOffset": 14, "endOffset": 43}, {"referenceID": 2, "context": "Methods which use conditional computation (Cho and Bengio, 2014; Bengio et al., 2015; Shazeer et al., 2017) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping.", "startOffset": 42, "endOffset": 107}, {"referenceID": 1, "context": "Methods which use conditional computation (Cho and Bengio, 2014; Bengio et al., 2015; Shazeer et al., 2017) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping.", "startOffset": 42, "endOffset": 107}, {"referenceID": 11, "context": "Methods which use conditional computation (Cho and Bengio, 2014; Bengio et al., 2015; Shazeer et al., 2017) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping.", "startOffset": 42, "endOffset": 107}, {"referenceID": 4, "context": "There are many other improvements to sRNNs, the most notable of which is the use of Long Term Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), but these are too numerous to list here and for the most part complementary to our proposed method.", "startOffset": 119, "endOffset": 153}, {"referenceID": 2, "context": "Irsoy and Cardie (2014) used m-RNNs for the task of sentiment classification and obtained equal or better performance than sRNNs.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "We can imagine that predicates and function words transform the meaning of the current hidden state of the RNN through matrix multiplication, whereas nouns, for example, add meaning through vector addition, following Baroni and Zamparelli (2010).", "startOffset": 217, "endOffset": 246}, {"referenceID": 7, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 10, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 8, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 11, "context": "As done in other language modeling work (Mikolov et al., 2011; Mnih and Teh, 2012; Zaremba et al., 2014; Mikolov et al., 2014; Shazeer et al., 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al.", "startOffset": 40, "endOffset": 148}, {"referenceID": 6, "context": ", 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al., 1994) using the same pre-processing as Mikolov et al.", "startOffset": 125, "endOffset": 146}, {"referenceID": 6, "context": ", 2017), we evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank corpus (Marcus et al., 1994) using the same pre-processing as Mikolov et al. (2011). This corpus is split into training, validation, and test sets containing 929K words, 73K words, and 82K words respectively.", "startOffset": 126, "endOffset": 202}, {"referenceID": 13, "context": "While L2 regularization did little to remedy this issue, we were able to overcome it by using Dropout (Srivastava et al., 2014) with p = 0.", "startOffset": 102, "endOffset": 127}, {"referenceID": 0, "context": "This model was motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the compositionality ideas discussed by Baroni and Zamparelli (2010) and Socher et al.", "startOffset": 170, "endOffset": 199}, {"referenceID": 0, "context": "This model was motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the compositionality ideas discussed by Baroni and Zamparelli (2010) and Socher et al. (2012) which suggest that some words are better modeled by matrices rather than vectors, and vice versa.", "startOffset": 170, "endOffset": 224}, {"referenceID": 0, "context": "This model was motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the compositionality ideas discussed by Baroni and Zamparelli (2010) and Socher et al. (2012) which suggest that some words are better modeled by matrices rather than vectors, and vice versa. We achieved both goals by pruning the size of the recurrent neural tensor network described by Sutskever et al. (2011) via sensible word-tomatrix mapping.", "startOffset": 170, "endOffset": 441}], "year": 2017, "abstractText": "Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, resulting in a significant increase of computational cost. An alternative is the recurrent neural tensor network (RNTN), which increases capacity by employing distinct hidden layer weights for each vocabulary word. The disadvantage of RNTNs is that memory usage scales linearly with vocabulary size, which can reach millions for word-level language models. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations using the Penn Treebank corpus show that r-RNTNs improve language model performance over standard RNNs using only a small fraction of the parameters of unrestricted RNTNs.", "creator": "LaTeX with hyperref package"}}}