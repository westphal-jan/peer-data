{"id": "1704.00783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated Volition", "abstract": "I make some basic observations about hard takeoff, value alignment, and coherent extrapolated volition, concepts which have been central in analyses of superintelligent AI systems.", "histories": [["v1", "Mon, 3 Apr 2017 19:45:04 GMT  (5kb)", "http://arxiv.org/abs/1704.00783v1", "3 pages"]], "COMMENTS": "3 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.LG", "authors": ["gopal p sarma"], "accepted": false, "id": "1704.00783"}, "pdf": {"name": "1704.00783.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Gopal P. Sarma"], "emails": ["gopal.sarma@emory.edu"], "sections": [{"heading": null, "text": "It is not the first time that people in a country where they go to another world go to another world, it is the second time that they go to another world. It is the second time that they go to another world. It is the second time that they go to another world. It is the third time that they go to another world. It is the first time that they go to another world. It is the second time that they go to another world. It is the first time that they go to another world. It is the second time that they go to another world. It is the second time that they go to another world. It is the second time that they go to another world. It is the second time that they go to another world. It is the second time that they go to another world. It is the third time that they go to another world. It is the third time that they go to another world."}], "references": [{"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["N. Bostrom"], "venue": "OUP Oxford,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Speculations Concerning the First Ultraintelligent Machine", "author": ["I.J. Good"], "venue": "Advances In Computers, vol. 6, no. 99, pp. 31\u201383, 1965.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1965}, {"title": "The Singularity: A Philosophical Analysis", "author": ["D. Chalmers"], "venue": "Journal of Consciousness Studies, vol. 17, no. 9-10, pp. 7\u201365, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "The Technological Singularity", "author": ["M. Shanahan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Implications of a software-limited singularity", "author": ["C. Shulman", "A. Sandberg"], "venue": "Proceedings of the European Conference of Computing and Philosophy, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Artificial intelligence as a positive and negative factor in global risks", "author": ["E. Yudkowsky"], "venue": "Global Catastrophic Risks (N. Bostrom and M. Cirkovic, eds.), ch. 15, pp. 308\u2013345, Oxford: Oxford University Press, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Should We Fear Supersmart Robots", "author": ["S. Russell"], "venue": "Scientific American, vol. 314, no. 6, pp. 58\u201359, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Coherent Extrapolated Volition", "author": ["E. Yudkowsky"], "venue": "Singularity Institute for Artificial Intelligence, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Coherent extrapolated volition: A meta-level approach to machine ethics", "author": ["N. Tarleton"], "venue": "Machine Intelligence Research Institute, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 1, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 2, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 3, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 4, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 5, "context": "Because the notion of an \u201cintelligence explosion\u201d has been constructed in analogy to a physical explosion [6], it gives rise to an inaccurate mental picture in people\u2019s minds.", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "Russell states 3 design principles which encapsulate the notion of value alignment [7]:", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "A related notion is Yudkowsky\u2019s \u201ccoherent extrapolated volition\u201d [8, 9].", "startOffset": 65, "endOffset": 71}, {"referenceID": 8, "context": "A related notion is Yudkowsky\u2019s \u201ccoherent extrapolated volition\u201d [8, 9].", "startOffset": 65, "endOffset": 71}], "year": 2017, "abstractText": "I make some basic observations about hard takeoff, value alignment, and coherent extrapolated volition, concepts which have been central in analyses of superintelligent AI systems.", "creator": "LaTeX with hyperref package"}}}