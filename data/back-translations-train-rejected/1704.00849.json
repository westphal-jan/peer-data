{"id": "1704.00849", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks", "abstract": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a Wasserstein generative adversarial network (W-GAN) that explicitly takes a VC-related objective into account. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.", "histories": [["v1", "Tue, 4 Apr 2017 01:47:14 GMT  (668kb,D)", "https://arxiv.org/abs/1704.00849v1", "Submitted to INTERSPEECH 2017"], ["v2", "Tue, 11 Apr 2017 04:19:07 GMT  (1577kb,D)", "http://arxiv.org/abs/1704.00849v2", "Submitted to INTERSPEECH 2017"], ["v3", "Thu, 8 Jun 2017 01:08:38 GMT  (1574kb,D)", "http://arxiv.org/abs/1704.00849v3", "Submitted to INTERSPEECH 2017"]], "COMMENTS": "Submitted to INTERSPEECH 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chin-cheng hsu", "hsin-te hwang", "yi-chiao wu", "yu tsao", "hsin-min wang"], "accepted": false, "id": "1704.00849"}, "pdf": {"name": "1704.00849.pdf", "metadata": {"source": "CRF", "title": "Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks", "authors": ["Chin-Cheng Hsu", "Hsin-Te Hwang", "Yi-Chiao Wu", "Yu Tsao", "Hsin-Min Wang"], "emails": ["whm}@iis.sinica.edu.tw,", "yu.tsao@citi.sinica.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "The primary goal of Voice Conversion (VC) is to convert speech from one source speaker to that of a target without changing the linguistic or phonetic content. However, let's consider the case of converting one's own voice to that of another who speaks a different language. Conventional VC techniques would have difficulty dealing with such cases, since most of them require parallel training data in which many pairs of speakers utter the same texts. In this context, we do not need to align frames or explicitly cluster phones or frames. The idea is skeletonized in the probabilistic graphical model (PGM) in Fig. 1. In this model, our attention is focused on finding a good inference model (Fig. 2.1) for the latent variable 2 and a synchronization (Fig. 2) of the real language (Fig.)."}, {"heading": "2. Non-parallel voice conversion via deep generative models", "text": "In fact, it is a real question of distribution, where it is a real question of distribution that applies to the real question of distribution."}, {"heading": "2.1. Modeling speech with a C-VAE", "text": "A C-VAE realizing the PGM in Figure 1a maximizes a variable lower limit of the log probability: log p\u03b8 (x-y) \u2264 \u2212 Jvae (x-y) = \u2212 (Jobs (x-y) + Jlat (x), (3) Jlat (x-z, y), (5) where x-Xs-Xt, DKL is the Kullback-Leibler divergence, (z) Jobs (x-y) = \u2212 Eq\u03c6 (z-x) [log-z, y) [log-Xs-Xt, DKL is the Kullback-Leibler divergence, pizbler (z) is our previous distribution model of z, p\u03b8 (x-z, y) is our synthesis model."}, {"heading": "2.2. Improving speech models with GANs", "text": "Despite the effectiveness of C-UAE, simplification leads to inaccuracies in the synthesis model. This defect stems from the fallible assumption that the observed data is normally distributed and uncorrelated across dimensions, and this assumption gave us an erroneous learning goal that leads to muffled converted voices. Therefore, we are motivated to resort to models that circumvent this defect. We can improve the C-UAE by integrating a GAN target [5] into the decoder. However, a vanilla GAN [6] consists of two components: a generator (synthesizer) that generates a realistic spectrum, and a discriminator that judges whether an input is a true spectrum or a generated one. These two components seek a balance in a min-max game with the Jensen-Shannon divergence DJS as the goal defined as: Jgan (Vgan; x) = 2 DJS + (2) (Elip)."}, {"heading": "2.3. Direct consideration of voice conversion with W-GAN", "text": "One deficiency in the UAE-GAN formulations is that VC is treated indirectly. We simply assume that, if the model is well trained, it naturally equips itself with the ability to convert voices. In contrast, we can directly optimize non-parallel VC loss by renewing DJS with a Waterstone target. (2.3.1) Waterstone-1 distance is defined as: W (p, pt, s) = inf. (p, pt | s) E (x, x). (x, x). (Jobs-1). (Jobs-1). (Jobs-1). (Jobs-2). (p, q). \u2212 Z \u2212 Z \u2212 s. (p, x). (s.)."}, {"heading": "2.3.2. VAW-GAN", "text": "The inclusion of the W-GAN loss (12) in (3) results in our final goal: Jvawgan = \u2212 DKL (q\u03c6 (zn | xn) \u0445 p (zn)) + Ez \u0445 q\u03c6 (z | x) [log p\u03b8 (x | z, y)] + \u03b1 Ex \u0445 p * t [D\u0443 (x)] \u2212 \u03b1 Ez \u0445 q\u03c6 (z | x) [D\u0443 (G\u03b8 (z, yt))] (13), where \u03b1 is a coefficient that emphasizes the W-GAN loss. This goal is shared by all three components: the encoder, the synthesizer and the discriminator. The synthesizer minimizes this loss while the discriminator maximizes it; consequently, the two components must be optimized in alternating order."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. The dataset", "text": "The proposed VC system was evaluated on the basis of the Voice Conversion Challenge 2016 dataset [8]. The dataset was a parallel language corpus, with no frame alignment in the following experiments. We conducted experiments on a subset of 3 speakers. In the intergender experiment, we chose SF1 as source and TM3 as target. In the intra-gender experiment, we chose TF2 as target. We used the first 150 expressions (about 10 minutes) per speaker for training, the following 12 for validation and 25 (of 54) expressions in the official test kit for subjective evaluations."}, {"heading": "3.2. The feature set", "text": "We used the STRAIGHT toolkit [9] to extract language parameters, including the STRAIGHT spectra (SP for short), aperidicity (AP), and pitch contours (F0), and the other experimental settings were the same as in [3], except that we recalled the log-energetically normalized SP (logSP) to the range of [\u2212 1, 1] dimensional values. Note that our system performed frame conversion without re-filtering, and that we used neither contextual nor dynamic characteristics in our experiments."}, {"heading": "3.3. Configurations and hyper-parameters", "text": "The baseline system was the C-VAE system (simply referred to as the UAE) [3], as its performance has been proven to be equivalent to another simple parallel baseline. In our proposed system, the encoder, synthesizer and discriminator were Convolutionary Neural Networks. The phonetic space was 64-dimensional and assumed a normal distribution. The speaker display was warm-coded and its embedding was optimized as part of the generator parameter1."}, {"heading": "3.4. The training and conversion procedures", "text": "First we set \u03b1 to 0 to exclude W-GAN, and trained the UAE to convergence to obtain the base model, then we trained the entire VAW-GAN by setting \u03b1 to 50. Conversion was done frame-by-frame, as shown in Fig. 1c. First, E\u03c6 derived the sound content zn from xs, n Then we specified a loudspeaker identity (integer, the subscript t to yt) that called the loudspeaker display vector y. Then, the synthesizer G\u03b8 generated a conditional output image x by using zn and yt."}, {"heading": "3.5. Subjective evaluations", "text": "Each of the 10 listeners rated the output pairs of the VAW-GAN and the UAE. Intersex and intra-sex VC were evaluated accordingly, and the MOS results for the naturalness shown in Figure 2 show that VAW-GAN significantly exceeds the UAE baseline (p-value 0.01 in paired t-tests), and the results are consistent with the converted spectra shown in Figure 3, where the output spectra from VAW-GAN express richer variability over the frequency axis and therefore reflect clearer voices and improved intelligence. We did not report objective ratings such as mean melceptive coefficients [10, 11] because we found inconsistent results with the subjective ratings. However, a similar inconsistency is common in the VC literature because it is highly likely that these ratings are not consistent with human auditory systems [13], which may be found to be similar to the loudspeaker line [13]."}, {"heading": "4. Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. W-GAN improved spectrum modeling", "text": "As we can see in Fig. 3, the spectral sheaths of the synthetic language of VAW-GAN are more structured, with more observable highs and lows. Spectral structures are the key to speech intelligibility and indirectly contribute to increased MOS. In addition, the more detailed spectral shapes in the radio frequency range reflect a clearer (not muted) voice of synthetic speech."}, {"heading": "4.2. W-GAN as a variance modeling alternative", "text": "In contrast to the UAE, which assumes a Gaussian distribution of observation, W-GAN implicitly models observation through a series of stochastic procedures without prescribing density shapes. In Fig. 4, we can observe that the power spectra of the VAW-GAN system exhibit greater deviations than those of the UAE system. Global deviation (GM) of the VAW-GAN output may not be as good as that of the data, but the higher values indicate that VAW-GAN does not over-centralize the predicted values on average. Since language exhibits a highly different distribution, a more sophisticated analysis of this phenomenon is required."}, {"heading": "4.3. Imperfect speaker modeling in VAW-GAN", "text": "The reason that the similarity of the speaker to the converted voice is not improved reminds us of the fact that both VAE and VAWGAN optimize the same PGM, i.e. the same speaker model. Therefore, modeling a speaker with a global variable may be insufficient. Since modeling a speaker with a framed variable may collide with the phonetic vector z, we may have to resort to other PGMs."}, {"heading": "5. Related work", "text": "One of the most intuitive options is to add an automatic speech recognition module (ASR) to the utterances and continue with explicit alignment or model adjustment [15, 16]. The ASR module provides each frame with a phonetic tag (usually the phonetic states). It is particularly suitable for text-to-speech systems (TTS) because they can easily use these marked frames [17]. Another shortcoming of these approaches is that they require additional mapping to realize cross-language VC. To this end, the INCA-based algorithms [18, 19] have been proposed to iteratively search for frame-by-frame correspondence using converted replacement frames. Another attempt is to create frame clusters for the source and target separately and then set up a mapping between them."}, {"heading": "6. Conclusions", "text": "We have presented a language conversion framework that is capable of incorporating a non-parallel VC criterion directly into the lens function, and the proposed VAW-GAN framework improves the results with more realistic spectral shapes. Experimental results show significantly improved performance compared to the base system."}, {"heading": "7. Acknowledgements", "text": "This work was partially supported by the Taiwanese Ministry of Science and Technology under grant number MOST 105-2221-E-001012-MY3."}, {"heading": "8. References", "text": "[1] D. P. Kingma and M. Welling, \"Auto-encoding variationalbayes.org,\" CoRR, 17 September 2016 / 1312.6114, 2013. Available: http: / / arxiv.org / abs / 1312.6114 [2] M. Arjovsky, S. Chintala, and L. Bottou, \"Wasserstein GAN,\" CoRR, vol. abs / 1701.07875, 2017. [Online] Available: http: / / arxiv.org / abs / 1701.07875 [3] C. Hsu, H. Hwang, Y. Wu, Y. Tsao, and H. Wang \"Voice conversion from non parallel corpora using variational autoencoder,\" in Asia-Pacific Signal and Information Processing Association Annual Summit Conference and, APSIPA 2016, Jeju, South Korea, December 13-16, 2016, pp.6."}], "references": [{"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "CoRR, vol. abs/1312.6114, 2013. [Online]. Available: http://arxiv.org/abs/1312.6114", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "CoRR, vol. abs/1701.07875, 2017. [Online]. Available: http://arxiv.org/abs/1701.07875", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Voice conversion from non-parallel corpora using variational autoencoder", "author": ["C. Hsu", "H. Hwang", "Y. Wu", "Y. Tsao", "H. Wang"], "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016, Jeju, South Korea, December 13-16, 2016. IEEE, 2016, pp. 1\u2013 6. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2016. 7820786", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling and transforming speech using variational autoencoders", "author": ["M. Blaauw", "J. Bonada"], "venue": "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1770\u20131774. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2016-1183", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "H. Larochelle", "O. Winther"], "venue": "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, ser. JMLR Workshop and Conference Proceedings, M. Balcan and K. Q. Weinberger, Eds., vol. 48. JMLR.org, 2016, pp. 1558\u20131566. [Online]. Available: http://jmlr.org/proceedings/papers/v48/larsen16.html", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde- Farley", "S. Ozair", "A.C. Courville", "Y. Bengio"], "venue": "CoRR, vol. abs/1406.2661, 2014. [Online]. Available: http://arxiv.org/abs/1406.2661", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal Transport: Old and New, ser", "author": ["C. Villani"], "venue": "Grundlehren der mathematischen Wissenschaften. Berlin: Springer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "The voice conversion challenge 2016", "author": ["T. Toda", "L. Chen", "D. Saito", "F. Villavicencio", "M. Wester", "Z. Wu", "J. Yamagishi"], "venue": "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1632\u20131636. [Online]. Available: http: //dx.doi.org/10.21437/Interspeech.2016-1066", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A. de Cheveign\u00e9"], "venue": "Speech Commun., no. 3-4, pp. 187\u2013207, 1999.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory", "author": ["T. Toda", "A.W. Black", "K. Tokuda"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Voice conversion using deep neural networks with layer-wise generative training", "author": ["L.-H. Chen", "Z.-H. Ling", "L.-J. Liu", "L.-R. Dai"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 22, pp. 1506\u20131521, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A probabilistic interpretation for artificial neural network-based voice conversion", "author": ["H.-T. Hwang", "Y. Tsao", "H.-M. Wang", "Y.-R. Wang", "S.-H. Chen"], "venue": "Proc. APSIPA, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Locally linear embedding for exemplar-based spectral conversion", "author": ["Y.-C. Wu", "H.-T. Hwang", "C.-C. Hsu", "Y. Tsao", "H.-M. Wang"], "venue": "Proc. INTERSPEECH, in press.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Analysis of the voice conversion challenge 2016 evaluation results", "author": ["M. Wester", "Z. Wu", "J. Yamagishi"], "venue": "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 1637\u2013 1641. [Online]. Available: https://doi.org/10.21437/Interspeech. 2016-1331", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Mapping frames with DNN-HMM recognizer for non-parallel voice conversion", "author": ["M. Dong", "C. Yang", "Y. Lu", "J.W. Ehnes", "D. Huang", "H. Ming", "R. Tong", "S.W. Lee", "H. Li"], "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2015, Hong Kong, December 16-19, 2015. IEEE, 2015, pp. 488\u2013494. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2015.7415320", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Text-independent voice conversion based on state mapped codebook", "author": ["M. Zhang", "J. Tao", "J. Tian", "X. Wang"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2008, March 30 - April 4, 2008, Caesars Palace, Las Vegas, Nevada, USA. IEEE, 2008, pp. 4605\u20134608. [Online]. Available: http://dx.doi.org/10.1109/ ICASSP.2008.4518682", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-parallel training for voice conversion based on adaptation method", "author": ["P. Song", "W. Zheng", "L. Zhao"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26- 31, 2013. IEEE, 2013, pp. 6905\u20136909. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2013.6639000", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "INCA algorithm for training voice conversion systems from nonparallel corpora", "author": ["D. Erro", "A. Moreno", "A. Bonafonte"], "venue": "IEEE Trans. Audio, Speech & Language Processing, vol. 18, no. 5, pp. 944\u2013953, 2010. [Online]. Available: http://dx.doi.org/ 10.1109/TASL.2009.2038669", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "The matching-minimization algorithm, the INCA algorithm and a mathematical framework for voice conversion with unaligned corpora", "author": ["Y. Agiomyrgiannakis"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2016, Shanghai, China, March 20-25, 2016. IEEE, 2016, pp. 5645\u20135649. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2016.7472758", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "A first step towards text-independent voice conversion", "author": ["H. Ney", "D. S\u00fcndermann", "A. Bonafonte", "H. H\u00f6ge"], "venue": "INTERSPEECH 2004 - ICSLP, 8th International Conference on Spoken Language Processing, Jeju Island, Korea, October 4-8, 2004. ISCA, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "On the use of i-vectors and average voice model for voice conversion without parallel data", "author": ["J. Wu", "Z. Wu", "L. Xie"], "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016, Jeju, South Korea, December 13-16, 2016. IEEE, 2016, pp. 1\u20136. [Online]. Available: http://dx.doi.org/10.1109/APSIPA.2016.7820901", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "A KL divergence and dnn-based approach to voice conversion without parallel training sentences", "author": ["F. Xie", "F.K. Soong", "H. Li"], "venue": "Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, N. Morgan, Ed. ISCA, 2016, pp. 287\u2013291. [Online]. Available: http: //dx.doi.org/10.21437/Interspeech.2016-116", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In this paper, we present a specific implementation in which a variational autoencoder (VAE [1]) assumes the inference task and a Wasserstein generative adversarial network (W-GAN [2]) undertakes speech synthesis.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "In this paper, we present a specific implementation in which a variational autoencoder (VAE [1]) assumes the inference task and a Wasserstein generative adversarial network (W-GAN [2]) undertakes speech synthesis.", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "Recent works have proven the viability of speech modeling with VAEs [3, 4].", "startOffset": 68, "endOffset": 74}, {"referenceID": 3, "context": "Recent works have proven the viability of speech modeling with VAEs [3, 4].", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": "For every input (xn,yn), we can sample the latent variable zn using the re-parameterization trick described in [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "We can improve the C-VAE by incorporating a GAN objective [5] into the decoder.", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "A vanilla GAN [6] consists of two components: a generator (synthesizer) G\u03b8 that produces realistic spectrum and a discriminatorD\u03c8 that judges whether an input is a true spectrum or a generated one.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "We can combine the objectives of VAE and GAN by assigning VAE\u2019s decoder as GAN\u2019s generator to form a VAEGAN [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "In contrast, we can directly optimize a non-parallel VC loss by renovating DJS with a Wasserstein objective [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "On the other hand, the Kantorovich-Rubinstein duality [7] of (9) allows us to explicitly approach non-parallel VC:", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The proposed VC system was evaluated on the Voice Conversion Challenge 2016 dataset [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "We used the STRAIGHT toolkit [9] to extract speech parameters, including the STRAIGHT spectra (SP for short), aperiodicity (AP), and pitch contours (F0).", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "The rest of the experimental settings were the same as in [3], except that we rescaled log energy-normalized SP (denoted by logSP en) to the range of [\u22121, 1] dimension-wise.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "The baseline system was the C-VAE system (denoted simply as VAE) [3] because its performance had been proven to be on par with another simple parallel baseline.", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "However, similar inconsistency is common in the VC literature because it is highly likely that those evaluations are inconsistent with human auditory systems [10, 11, 12].", "startOffset": 158, "endOffset": 170}, {"referenceID": 10, "context": "However, similar inconsistency is common in the VC literature because it is highly likely that those evaluations are inconsistent with human auditory systems [10, 11, 12].", "startOffset": 158, "endOffset": 170}, {"referenceID": 11, "context": "However, similar inconsistency is common in the VC literature because it is highly likely that those evaluations are inconsistent with human auditory systems [10, 11, 12].", "startOffset": 158, "endOffset": 170}, {"referenceID": 12, "context": "unreported because we found that it remained about the same as that of [13] (System B in [14]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "unreported because we found that it remained about the same as that of [13] (System B in [14]).", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "One of the most intuitive ways is to apply an automatic speech recognition (ASR) module to the utterances, and proceed with explicit alignment or model adaptation [15, 16].", "startOffset": 163, "endOffset": 171}, {"referenceID": 15, "context": "One of the most intuitive ways is to apply an automatic speech recognition (ASR) module to the utterances, and proceed with explicit alignment or model adaptation [15, 16].", "startOffset": 163, "endOffset": 171}, {"referenceID": 16, "context": "It is particularly suitable for text-to-speech (TTS) systems because they can readily utilize these labeled frames [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "To this end, the INCA-based algorithms [18, 19] were proposed to iteratively seek frame-wise correspondence using converted surrogate frames.", "startOffset": 39, "endOffset": 47}, {"referenceID": 18, "context": "To this end, the INCA-based algorithms [18, 19] were proposed to iteratively seek frame-wise correspondence using converted surrogate frames.", "startOffset": 39, "endOffset": 47}, {"referenceID": 19, "context": "Another attempt is to separately build frame clusters for the source and the target, and then set up a mapping between them [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "Recent advances include [21], in which the authors exploited i-vectors to represent speakers.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "In [22], the authors represented the phonetic space with senone probabilities outputted from an ASR module, and then generated voice by means of a TTS module.", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.", "creator": "LaTeX with hyperref package"}}}