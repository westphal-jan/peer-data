{"id": "1704.00924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "Japanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention", "abstract": "Previous approaches to training syntax-based sentiment classification models required phrase-level annotated corpora, which are not readily available in many languages other than English. Thus, we propose the use of tree-structured Long Short-Term Memory with an attention mechanism that pays attention to each subtree of the parse tree. Experimental results indicate that our model achieves the state-of-the-art performance in a Japanese sentiment classification task.", "histories": [["v1", "Tue, 4 Apr 2017 09:08:46 GMT  (246kb,D)", "http://arxiv.org/abs/1704.00924v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ryosuke miyazaki", "mamoru komachi"], "accepted": false, "id": "1704.00924"}, "pdf": {"name": "1704.00924.pdf", "metadata": {"source": "CRF", "title": "Japanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention", "authors": ["Ryosuke Miyazaki", "Mamoru Komachi"], "emails": ["miyazaki-ryosuke@ed.tmu.ac.jp", "komachi@tmu.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Attentional Tree-LSTM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Tree-Structured Long Short-Term Memory (LSTM)", "text": "Various RNN models for handling sentence representation with regard to syntactic structure have been investigated (Socher et al., 2011, 2012, 2013; Qian et al., 2015; Tai et al., 2015; Zhu and Sobhani, 2015). RNN construct sentence representation from their phrase representations by applying a composition function. Phrase representation can be calculated by recursive adoption of composition functions. These RNN models are essentially identical to relapsing neural models because they are unable to maintain a long history. Tai et al. (2015) addressed this problem by introducing LSTM (Hochreiter and Schmidhuber, 1997) to make RNN less vulnerable to the exploding / disappearing gradient problem. In this paper, we use the Binary Tree LSTM proposed by Tai et al. (2015) as an example of a structured LSTM."}, {"heading": "2.2 Softmax Classifier with Attention", "text": "Due to the lack of phrase-level annotations, the sentence representation may be inaccurate as it cannot spread errors from the root of the tree to the terminals and preterminals in a long sentence. We propose an attention mechanism to address this problem. In addition to a hidden representation, this so-called attention classifier takes attention vector representation aj as input: p (y | hj) = softmax (W (s) [aj hj] + b (a))), (1) aj = ichi aji hi, (2) aji = g (hi, hj), (i \u2032 g (hi \u2032, hj), (3) g (hi, hj) = exp (W (a2) tanh (W (W (a1) [hi hj])), (4) aji = g (hi, hj)."}, {"heading": "2.3 Distant Supervision with Polar Dictionaries", "text": "Unlike the Stanford Sentiment Treebank, which has a polarity at the phrase level, other multilingual datasets contain only annotations at the sentence level. As shown in Section 3, the classification of feelings without an annotated corpus at the phrase level is not adequately learned. Although a polarity-named corpus is difficult to maintain in many languages, polar dictionaries are easy to (semi-) compile automatically. Therefore, we opt for the use of polar dictionaries as an alternative source of sensory information. The key difference from the short phrase and word dictionaries used in Nakagawa et al. (2010) is that the phrase in the training sets corresponding to an entry in the polar dictionaries is commented with corresponding polarity."}, {"heading": "2.4 Learning", "text": "The cost function is a cross-entropy error function between the true class marking distribution t (i.e. a hot distribution for correct marking) and the predicted marking distribution y, at each labelled node: J (\u03b8) = \u2212 m \u2211 k = 1 tk log y \u0394k + \u03bb 2 \u0445 \u03b8 \u0445 22, (5) where m is the number of labelled nodes in the training set1 and \u03bb denotes an L2 regularisation hyper parameter."}, {"heading": "3 Experiments", "text": "We performed a mood classification using a Japanese corpus."}, {"heading": "3.1 Data", "text": "For the analysis of constituencies, we used Ckylark (Oda et al., 2015) with 5,447 positive and 8,117 negative expressions. For word segmentation, we used the Japanese opinion corpus NTCIR (NTCIR-J), which contains 997 positive and 2,400 negative sentences (Seki et al., 2007, 2008). The corpus included two Japanese opinion corpus NTCIR (NTCIR-J), which contains 997 positive and 2,400 negative sentences (Seki et al., 2007, 2008)."}, {"heading": "3.2 Methods", "text": "We compared our method with four baselines. All input word vectors except those for MFS and Tree-CRF were pre-trained by word2vec. We implemented our method LogRes and Tree-LSTM using chainer (Tokui et al., 2015). The following methods were used: 1If the dataset contains only sentence annotations, the yardsticks are adapted to the size of the dataset. 2https: / / code.google.com / archive / p / word2vec / 3KyTea Version-0.4.7. 4http: / / www.cl.ecei.tohoku. ac.jp / index.php? OpenResources / JapaneseSentimentPolarityDictionaryMFS. A na\u00efve baseline, as it always selects the most frequent polarity (which is negative in this case)."}, {"heading": "3.3 Hyperparameters", "text": "We have adapted hyperparameters to each development set of 10-fold cross-validation; the best parameters are shown in Table 1."}, {"heading": "4 Results", "text": "The experimental results are shown in Table 2. RNN accuracy is much lower than that of the MFS baseline; furthermore, Tree-LSTM, which represents improved RNN, is still lower than simple LogRes, although Tree-LSTM achieves state-of-the-art performance based on the phrased Stanford Sentiment Treebank (Tai et al., 2015). In contrast, Tree-LSTM achieves attention-compatible results compared to Tree-CRF. Our Tree-LSTM with attention and polar dictionary achieved the best accuracy. Kokkinos and Potamianos (2017) also investigate an attention model for RNN. Their model feeds only one attention vector into the Softmax classifier, while our method inputs both an attention vector and an RNN vector, as shown in Figure 1."}, {"heading": "5 Discussion", "text": "The results described above suggest that TreeLSTM models without an attention mechanism do not learn sentence representations when phrase-level annotations are not available. However, tree-LSTM models can learn more accurate sentence representations when models receive phrase-level information provided by polar dictionaries. In our model, for example, attention information and polar dictionary information are fed into the Tree-LSTM as phrase-level information. Although Tree-LSTM outperforms Tree-CRF by 1.8 points with attention and a polar dictionary, the accuracy of Tree-LSTM without a polar dictionary is lower than that of Tree-CRF. TreeLSTM with a polar dictionary performs better with attention than Tree-LSTM with attention, showing that the monitored designation for each phrase seems to be important for learning tree-LSTM models."}, {"heading": "6 Conclusion", "text": "We presented a recursive neural network based on Tree-LSTM that uses an attention mechanism and a polar dictionary. In this method, each phrase representation is fed into a classifier to predict the polarity of a phrase based on the structure of the phrase. Dictionary lexical elements are used as monitored labels for each corresponding phrase or word in the same way as remote monitoring. Our experimental results showed that the proposed method outperforms previous methods."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Previous approaches to training syntax-<lb>based sentiment classification models re-<lb>quired phrase-level annotated corpora,<lb>which are not readily available in many<lb>languages other than English. Thus, we<lb>propose the use of tree-structured Long<lb>Short-Term Memory with an attention<lb>mechanism that pays attention to each sub-<lb>tree of the parse tree. Experimental results<lb>indicate that our model achieves the state-<lb>of-the-art performance in a Japanese sen-<lb>timent classification task.", "creator": "TeX"}}}