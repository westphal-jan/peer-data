{"id": "1704.01137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computa- tions, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks - one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar operations, which translates to 1.8x-2.3x performance improvement over a Caffe-based implementation, with &lt; 0.5% loss in accuracy.", "histories": [["v1", "Tue, 4 Apr 2017 18:14:02 GMT  (1209kb,D)", "http://arxiv.org/abs/1704.01137v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["sanjay ganapathy", "swagath venkataramani", "balaraman ravindran", "anand raghunathan"], "accepted": false, "id": "1704.01137"}, "pdf": {"name": "1704.01137.pdf", "metadata": {"source": "CRF", "title": "DYVEDEEP: DYNAMIC VARIABLE EFFORT DEEP NEURAL NETWORKS", "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "emails": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "2 RELATED WORK", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3 DYVEDEEP: DESIGN APPROACH AND DYNAMIC EFFORT KNOBS", "text": "The key idea behind DyVEDeep is to improve the computing efficiency of DNNs by modulating the effort they expend based on the input to be processed. As shown in Figure 1, we achieve this by providing the DNN with dynamic effort mechanisms (\"effort buttons\") that dynamically predict the criticality of groups of calculations with very little effort and skip or approximate them accordingly, improving efficiency with negligible impact on classification accuracy. We identify three such dynamic effort mechanisms in DNNs operating at different levels of granularity, and propose a methodology to adjust the hyperparameters associated with these mechanisms so that variable effort versions of a DNN can be achieved with negligible loss of classification accuracy."}, {"heading": "3.1 SATURATION PREDICTION AND EARLY TERMINATION", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3.2 SIGNIFICANCE-DRIVEN SELECTIVE SAMPLING", "text": "Significant Selective Sampling (SDSS) operates the granularity of each characteristic in the revolutionary layers of the DNN. SDSS uses the spatial locality in neuron activations within each characteristic. For example, in the context of images, adjacent pixels in the input image often take similar values. Since the neuron activations are calculated by pushing the nucleus over the image, the patient locality naturally penetrates the characteristics of the revolutionary layers. This behavior is also observed in deeper layers in the network. Indeed, the saturating nature of the activation function improves locality, since variations in the weighted sum between neighbors are masked both within the same saturation regime. SDSS adopts a two-step process for exploiting the spatial locality within the characteristic. Uniform feature sampling."}, {"heading": "3.3 SIMILARITY-BASED FEATURE MAP APPROXIMATION", "text": "In SDSS, the spatial locality was used in the calculation of the neuron activations themselves. In contrast, in the case of SFMA, the spatial locality is used to approximate calculations that use the attribute as input. Let's consider a revolutionary level where one of the input characteristics has all its neuron activations similar to each other. If a folding operation is performed on this input function by moving the kernel matrix, all entries in the folding output are likely to be close together. Therefore, as shown in Figure 6, we approach the entire folding operation as follows. First, the average value of all neuron activations is calculated in the function. Next, the sum of all weights in the kernel matrix is evaluated."}, {"heading": "3.4 INTEGRATING EFFORT KNOBS", "text": "In order to combine SPET and SDSS, each neuron activation in the uniformly captured features of SDSS is calculated with SPET. However, we do not apply SPET to the neurons that are selectively calculated in SDSS, since they are located in the middle of neurons with large activation values and / or variances and are therefore unlikely to be saturated. SFMA essentially amounts to grouping a set of inputs (within a folding window) on a neuron in a single input, and thus fits directly into the process of evaluating a neuron with SPET / SDSS. In summary, the SPET effort button applies to both revolutionary and fully connected layers of DNNs."}, {"heading": "3.5 HYPER-PARAMETER TUNING", "text": "As described in the previous sections, the dynamic effort regulators together are 6 hyperparameters viz. SPETlThresh, SPETuThresh, Maxactthresh, Delactthresh, Binactthresh and FeaV arthresh. However, these hyperparameters control how aggressively the effort parameters are skipped or approximated, allowing a direct trade-off between the individual neurons to be achieved. If we use a pre-formed network and a training dataset, we systematically determine the DyVEDeep parameters before the DNA model is used. Ideally, we could define these parameters uniquely for each neuron. Each neuron could have its unique SPETlThresh threshold to predict when it is saturated (SPET), or interpret FeaV arthresh threshold."}, {"heading": "4 EXPERIMENTAL METHODOLOGY", "text": "In this section, we describe the methodology used in our experiments to evaluate DyVEDeep.Benchmarks.To evaluate DyVEDeep, we used pre-trained DNN models publicly available on the Caffe Model Zoo (BVLC (a)) Benchmark Repository, enhancing DyVEDeep's ability to adapt to a specific trained network.We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe Network (BVLC (b)) for the CIFAR-10 Dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al. (2012), Overfeat-accurat (Sermanet al.), VGG-16 (Simonyan & Zisserman (2014), and Compressed AlexNet et al. (Han et al. (2015a)))."}, {"heading": "5 RESULTS", "text": "In this section we present the results of our experiments demonstrating the benefits of DyVEDeep.5.1 IMPROVEMENT IN SCALAR OPERATIONS AND EXECUTION TIMEWe first present the reduction in scalar operations and execution time that DyVEDeep achieves in Figure 7. Please note that the Y-axis in Figure 7 represents a normalized scale to represent the benefits in both scalar operations and execution time. We find that across all benchmarks DyVEDeep achieves a significant reduction in the number of operations ranging from 2.1 x -2.6 x. This corresponds to 1.8 x -2.3 x benefits in software execution time. In all of the above cases, the difference in classification accuracy between the base line DNN and DyVEDeep < 0.5%. On average, the runtime of the dynamic strain buttons in DyVEDeep is 5% of the base time."}, {"heading": "5.2 LAYER-WISE AND KNOB-WISE BREAKDOWN OF COMPUTE SAVINGS", "text": "Figure 8a shows the breakdown of runtime savings at different levels of AlexNet, where the runtime of each layer is plotted on the X-axis and the average runtime per layer is normalized to the total DNN runtime on the Y-axis. We achieve a 1.5-fold reduction in runtime in the initial Convolutionary Layers (C1, C2), which increases to 2.6-fold in the deeper Convolutionary Layers (C3-C5). The C1 layer in AlexNet has a core size of 11 \u00d7 11 and operates at a step of 4. Therefore, its performance is less likely to correlate to the correlation that SSDS expects. As there are very few input functions, SFMA is also not very effective. Furthermore, the percentage of neurons saturated in the first layers is relatively small, affecting the effectiveness of SPET. Therefore, we achieve better savings in the deeper Convolutionary Layers compared to the total contribution to each of the initial 831 layers compared to each SMA."}, {"heading": "5.3 VISUALISATION OF EFFORT MAP OF DYVEDEEP", "text": "Figures 10 and 11 illustrate the normalized effort map of DyVEDeep for all features in layer C1 for two sample images (Figure 9) from the CIFAR 10 dataset. We use layer C1 because this is the layer closest to the actual image and allows for better visualization; the normalization is done in terms of the number of operations that would have been performed to compute the neuron if our buttons had not been in place. Darker regions represent more calculations. It is noteworthy that DyVEDeep focuses more effort on exactly the regions of the image that contain the object of interest. We compare this to the activation map of the corresponding features. Here, the darker regions represent activated neurons. This was done to highlight the correlation between the activation values and the effort DyVEDeep exerts on the corresponding neurons. The activation map shows that regions where the activation value of the neurons is higher, which makes it more difficult to deviate the values of the activation values of the higher ones."}, {"heading": "6 CONCLUSION", "text": "Deep Neural Networks have significantly influenced the field of machine learning by enabling state-of-the-art functional accuracy on a wide range of machine learning problems in the fields of image, video, text, language and other modalities, but their large-scale structure makes them computationally intensive and data-intensive, which remains a key challenge. We observe that state-of-the-art DNNs are static, i.e. they perform the same set of calculations on all inputs. However, in many real data sets there is considerable heterogeneity in terms of the computational effort required to classify each input. Taking advantage of this opportunity, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) or DNNs that dynamically modulate their computational effort to determine which calculations are critical to classify a given input."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Adaptive dropout for training deep neural networks. In Advances in Neural Information Processing Systems", "author": ["Lei Jimmy Ba", "Brendan J. Frey"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Ba and Frey.,? \\Q2013\\E", "shortCiteRegEx": "Ba and Frey.", "year": 2013}, {"title": "Conditional computation in neural networks for faster models", "author": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup"], "venue": "CoRR, abs/1511.06297,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Yoshua Bengio"], "venue": "CoRR, abs/1305.2982,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "CoRR, abs/1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun", "Olivier Temam"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Distributed deep learning using synchronous stochastic gradient descent", "author": ["Dipankar Das", "Sasikanth Avancha", "Dheevatsa Mudigere", "Karthikeyan Vaidyanathan", "Srinivas Sridharan", "Dhiraj D. Kalamkar", "Bharat Kaul", "Pradeep Dubey"], "venue": "CoRR, abs/1602.06709,", "citeRegEx": "Das et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "MarcAurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Fei-Fei Li"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "CoRR, abs/1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "E. Culurciello", "Y. LeCun"], "venue": "In CVPR 2011 WORKSHOPS,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1603.08983,", "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks. CoRR, abs/1506.02626, 2015b", "author": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "venue": null, "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Google supercharges machine learning tasks with custom chip: https://cloudplatform.googleblog.com/2016/05/google-supercharges-machine-learning-taskswith-custom-chip.html", "author": ["Norman Jouppi"], "venue": null, "citeRegEx": "Jouppi.,? \\Q2016\\E", "shortCiteRegEx": "Jouppi.", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Alex Krizhevsky"], "venue": "CoRR, abs/1404.5997,", "citeRegEx": "Krizhevsky.,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall F. Tappen", "Marianna Pensky"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Pruning deep neural networks by optimal brain damage", "author": ["Chao Liu", "Zhiyong Zhang", "Dong Wang"], "venue": "INTERSPEECH", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Reno: A high-efficient reconfigurable neuromorphic computing accelerator design", "author": ["Xiaoxiao Liu", "Mengjie Mao", "Beiye Liu", "Hai Li", "Yiran Chen", "Boxun Li", "Yu Wang", "Hao Jiang", "Mark Barnell", "Qing Wu", "Jianhua Yang"], "venue": "In Proceedings of the 52Nd Annual Design Automation Conference,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Fast training of convolutional networks through ffts", "author": ["Micha\u00ebl Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "CoRR, abs/1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Spindle: Spintronic deep learning engine for large-scale neuromorphic computing", "author": ["Shankar Ganesh Ramasubramanian", "Rangharajan Venkatesan", "Mrigank Sharad", "Kaushik Roy", "Anand Raghunathan"], "venue": "In Proceedings of the 2014 International Symposium on Low Power Electronics and Design,", "citeRegEx": "Ramasubramanian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramasubramanian et al\\.", "year": 2014}, {"title": "Virtualizing deep neural networks for memory-efficient neural network design", "author": ["Minsoo Rhu", "Natalia Gimelshein", "Jason Clemons", "Arslan Zulfiqar", "Stephen W. Keckler"], "venue": "CoRR, abs/1602.08124,", "citeRegEx": "Rhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rhu et al\\.", "year": 2016}, {"title": "1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns", "author": ["Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu"], "venue": null, "citeRegEx": "Seide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition", "author": ["Shawn Tan", "Khe Chai Sim"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Tan and Sim.,? \\Q2016\\E", "shortCiteRegEx": "Tan and Sim.", "year": 2016}, {"title": "Axnn: Energyefficient neuromorphic systems using approximate computing", "author": ["Swagath Venkataramani", "Ashish Ranjan", "Kaushik Roy", "Anand Raghunathan"], "venue": "In Proceedings of the 2014 International Symposium on Low Power Electronics and Design,", "citeRegEx": "Venkataramani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venkataramani et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al.", "startOffset": 71, "endOffset": 89}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al.", "startOffset": 90, "endOffset": 108}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al. (2012)) and 1-bit SGD (Seide et al.", "startOffset": 90, "endOffset": 166}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al. (2012)) and 1-bit SGD (Seide et al. (2014)) to alleviate communication overheads are representative examples.", "startOffset": 90, "endOffset": 202}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al. (2012)) and 1-bit SGD (Seide et al. (2014)) to alleviate communication overheads are representative examples. The next set of efforts design specialized hardware accelerators to realize DNNs, trading off programmability, the cost of specialized hardware and design effort for efficiency. A spectrum of architectures ranging from low-power IP cores to large-scale systems have been proposed (Farabet et al. (2011); Chen et al.", "startOffset": 90, "endOffset": 572}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi).", "startOffset": 8, "endOffset": 27}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al.", "startOffset": 8, "endOffset": 262}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)).", "startOffset": 8, "endOffset": 293}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al.", "startOffset": 8, "endOffset": 400}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)).", "startOffset": 8, "endOffset": 478}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al.", "startOffset": 8, "endOffset": 742}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al. (2015b); Liu et al.", "startOffset": 8, "endOffset": 762}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al. (2015b); Liu et al. (2014); Venkataramani et al.", "startOffset": 8, "endOffset": 781}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al. (2015b); Liu et al. (2014); Venkataramani et al. (2014); Anwar et al.", "startOffset": 8, "endOffset": 810}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015); Tan & Sim (2016)).", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015); Tan & Sim (2016)).", "startOffset": 8, "endOffset": 46}, {"referenceID": 14, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al.", "startOffset": 83, "endOffset": 101}, {"referenceID": 4, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), and hardware transparent on-chip memory allocation/management schemes such as virtualized DNNs (Rhu et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 4, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), and hardware transparent on-chip memory allocation/management schemes such as virtualized DNNs (Rhu et al. (2016)) are representative examples.", "startOffset": 102, "endOffset": 236}, {"referenceID": 4, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), and hardware transparent on-chip memory allocation/management schemes such as virtualized DNNs (Rhu et al. (2016)) are representative examples. The second class of efforts design specialized hardware accelerators that realize the key computation kernels in DNNs. A range of architectures targeting low-power mobile devices (Farabet et al. (2011)) to high-performance server clusters (Chen et al.", "startOffset": 102, "endOffset": 468}, {"referenceID": 4, "context": "(2011)) to high-performance server clusters (Chen et al. (2014); Jouppi) have been explored.", "startOffset": 45, "endOffset": 64}, {"referenceID": 4, "context": "(2011)) to high-performance server clusters (Chen et al. (2014); Jouppi) have been explored. The third set of efforts investigate new device technologies whose characteristics intrinsically match the compute primitives present in DNNs. Memristor-based crossbar array architectures (Liu et al. (2015b)) and spintronic neuron designs (Ramasubramanian et al.", "startOffset": 45, "endOffset": 301}, {"referenceID": 4, "context": "(2011)) to high-performance server clusters (Chen et al. (2014); Jouppi) have been explored. The third set of efforts investigate new device technologies whose characteristics intrinsically match the compute primitives present in DNNs. Memristor-based crossbar array architectures (Liu et al. (2015b)) and spintronic neuron designs (Ramasubramanian et al. (2014)) are representative examples.", "startOffset": 45, "endOffset": 363}, {"referenceID": 13, "context": "the model size of DNNs by using mechanisms such as pruning connections (LeCun et al. (1989); Han et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 7, "context": "(1989); Han et al. (2015b); Liu et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 7, "context": "(1989); Han et al. (2015b); Liu et al. (2014)), reducing the precision of computations (Venkataramani et al.", "startOffset": 8, "endOffset": 46}, {"referenceID": 7, "context": "(1989); Han et al. (2015b); Liu et al. (2014)), reducing the precision of computations (Venkataramani et al. (2014); Anwar et al.", "startOffset": 8, "endOffset": 116}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)).", "startOffset": 8, "endOffset": 93}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network.", "startOffset": 8, "endOffset": 180}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold.", "startOffset": 8, "endOffset": 388}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al.", "startOffset": 8, "endOffset": 607}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation.", "startOffset": 8, "endOffset": 632}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function.", "startOffset": 8, "endOffset": 757}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency.", "startOffset": 8, "endOffset": 954}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency. Finally, /citeDBLP:journals/corr/FigurnovVK15 propose perforated CNNs, in which only a subset of the neurons in a feature are evaluated. The neurons to be evaluated for each feature are determined statically at training time. Dynamic Techniques. Dynamic optimizations adapt the computations that are approximated based on the input currently being processed. Dynamic techniques are more powerful than statically optimised DNNs, as they can capture additional input-dependent opportunities for efficiency that static methods lack. Notwithstanding this, very little focus has been devoted to developing dynamic DNN approximation techniques. One of the first efforts in this direction (Bengio (2013)), utilizes stochastic neurons to gate regions within the DNN.", "startOffset": 8, "endOffset": 1762}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency. Finally, /citeDBLP:journals/corr/FigurnovVK15 propose perforated CNNs, in which only a subset of the neurons in a feature are evaluated. The neurons to be evaluated for each feature are determined statically at training time. Dynamic Techniques. Dynamic optimizations adapt the computations that are approximated based on the input currently being processed. Dynamic techniques are more powerful than statically optimised DNNs, as they can capture additional input-dependent opportunities for efficiency that static methods lack. Notwithstanding this, very little focus has been devoted to developing dynamic DNN approximation techniques. One of the first efforts in this direction (Bengio (2013)), utilizes stochastic neurons to gate regions within the DNN. Along similar lines, Ba & Frey (2013) propose Standout, where the dropout probability of each neuron is estimated using a binary belief network.", "startOffset": 8, "endOffset": 1862}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency. Finally, /citeDBLP:journals/corr/FigurnovVK15 propose perforated CNNs, in which only a subset of the neurons in a feature are evaluated. The neurons to be evaluated for each feature are determined statically at training time. Dynamic Techniques. Dynamic optimizations adapt the computations that are approximated based on the input currently being processed. Dynamic techniques are more powerful than statically optimised DNNs, as they can capture additional input-dependent opportunities for efficiency that static methods lack. Notwithstanding this, very little focus has been devoted to developing dynamic DNN approximation techniques. One of the first efforts in this direction (Bengio (2013)), utilizes stochastic neurons to gate regions within the DNN. Along similar lines, Ba & Frey (2013) propose Standout, where the dropout probability of each neuron is estimated using a binary belief network. The dropout mask is computed for the network in one shot, conditioned on the input to the network. Bengio et al. (2015) extends a similar idea, wherein the dropout distribution of each layer is computed based on the output of the preceding layer.", "startOffset": 8, "endOffset": 2089}, {"referenceID": 15, "context": "We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe network (BVLC (b)) for the CIFAR-10 dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al.", "startOffset": 119, "endOffset": 137}, {"referenceID": 15, "context": "We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe network (BVLC (b)) for the CIFAR-10 dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al. (2012)), Overfeat-accurate (Sermanet et al.", "startOffset": 119, "endOffset": 177}, {"referenceID": 15, "context": "We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe network (BVLC (b)) for the CIFAR-10 dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al. (2012)), Overfeat-accurate (Sermanet et al.), VGG-16 (Simonyan & Zisserman (2014)), and compressed AlexNet (Han et al.", "startOffset": 119, "endOffset": 252}, {"referenceID": 12, "context": "), VGG-16 (Simonyan & Zisserman (2014)), and compressed AlexNet (Han et al. (2015a)) for the ImageNet ILSVRC 2012 data set (Deng et al.", "startOffset": 65, "endOffset": 84}, {"referenceID": 8, "context": "(2015a)) for the ImageNet ILSVRC 2012 data set (Deng et al. (2009)).", "startOffset": 48, "endOffset": 67}, {"referenceID": 8, "context": "(2015a)) for the ImageNet ILSVRC 2012 data set (Deng et al. (2009)). The inputs for the ImageNet dataset are generated by using a 224\u00d7 224 center crop of the images in the test set. We randomly selected 5% of the test inputs and used it as a validation set to tune the hyper parameters. We report speedup and classification accuracy results on the remaining 95% of the test inputs. Performance Measurement. We implemented DyVEDeep in C++ within the Caffe deep learning framework (Jia et al. (2014)).", "startOffset": 48, "endOffset": 498}], "year": 2017, "abstractText": "Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks \u2014 one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weightcompressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1\u00d7-2.6\u00d7 reduction in the number of scalar operations, which translates to 1.8\u00d7-2.3\u00d7 performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.", "creator": "LaTeX with hyperref package"}}}