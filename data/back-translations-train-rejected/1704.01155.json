{"id": "1704.01155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks", "abstract": "Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from the expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.", "histories": [["v1", "Tue, 4 Apr 2017 18:56:53 GMT  (763kb,D)", "http://arxiv.org/abs/1704.01155v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CR cs.LG", "authors": ["weilin xu", "david evans", "yanjun qi"], "accepted": false, "id": "1704.01155"}, "pdf": {"name": "1704.01155.pdf", "metadata": {"source": "CRF", "title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks", "authors": ["Weilin Xu", "David Evans", "Yanjun Qi"], "emails": [], "sections": [{"heading": null, "text": "Although deep neural networks (DNNs) have achieved great success in many tasks of computer vision, recent studies have shown that they are susceptible to hostile examples. Such examples, typically generated by adding small but targeted distortions, can often deceive DNN models. Previous studies defending against hostile examples have mostly focused on the refinement of DNN models. They either show limited success or suffer from expensive calculations. We propose a new strategy, feature squeezing, which can be used to harden DNN models by recognizing conflicting examples. Feature squeezing reduces the search space available to an opponent by merging samples that correspond to many different feature vectors in the original space into a single sample. By comparing the prediction of a DNN model to the original input to the squeezed input, feature squeezing detects opposing examples with high accuracy and few false positives."}, {"heading": "1 Introduction", "text": "This year, it has come to the point where it is only a matter of time before a result is achieved."}, {"heading": "2 Background", "text": "This section provides a brief introduction to neural networks, methods to find counterexamples, and proposed defense mechanisms."}, {"heading": "2.1 Neural Networks", "text": "Deep Neural Networks (DNNs) can efficiently learn high-precision models from large corpora of training samples in many areas [11, 9, 15]. Convolutional Neural Networks (CNNs), first popularized by LeCun et al. [13], perform exceptionally well in image classification. A deep CNN can be written as a function g: X \u2192 Y, where X represents the entrance space and Y is the output space representing a categorical set. For a sample x x x x x, g (x) = fL (fL \u2212 1 (. (x)))))). Each fi represents a layer representing a classical prefabricated linear layer, reflective layer, max-pooling layer or a convolutionary layer performing a sliding furnace operation across all positions in an input direction."}, {"heading": "2.2 Generating Adversarial Examples", "text": "A contradictory example is an input created by an adversary with the aim of generating an erroneous output of a target classifier. Since the truth on the ground, at least for image classification tasks, is based on human perception, which is difficult to model or test, research on contradictory examples typically defines an adverse example as a misclassified sample x, which is generated by interfering with a correctly classified sample x by a limited amount. Contradictory examples can be targeted, with the goal of the adversary being to classify x as a specific class z or unaimed. In this case, the adversary's goal is to classify x as a different class from his correct class. Formally, the goal of an adversary is to find an x-shaped solution."}, {"heading": "2.2.1 Fast Gradient Sign Method (L\u221e)", "text": "Goodfellow et al. hypothesized that DNNs are susceptible to adversarial disturbances due to their linear nature (i.e. their linear nature).6 They proposed the Fast Gradient Signs (FGSM) method to efficiently find adversarial examples. To control the cost of the attack, FGSM evidently assumes that the attack strength is the same in each characteristic dimension. This essentially measures the perturbation \u2206 (x, x \u2032) using the L \u221e standard. The severance of the disturbance in each dimension is limited by the same constant parameter, \u03b5, which is also used as the disturbance set, provided that the maximum permissible disturbance is most effective. As an untargeted attack, the disturbance is calculated directly by the use of gradient vector of a loss function."}, {"heading": "2.3 Defensive Techniques", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2.4 Detecting Adversarial Examples", "text": "A few recent studies [14, 7, 5] have focused on the detection of counter-examples, and the strategies they have explored can be divided into three groups: sample statistics, detector training, and prediction inconsistency. Our method is both far less expensive and much more accurate than the best previous methods (see Section 6.2 for comparison).Sample statistics and legitimate samples that are unable to detect individual examples that are less useful in practice than the maximum discrepancy and energy distance than the statistical distance measurements. Their experimental results show that the test method requires a large group of counter-examples and legitimate examples, and that they are unable to detect individual examples."}, {"heading": "3 Feature Squeezing Methods", "text": "Although the term feature squeezing is fairly general, we focus on two simple squeezing methods: reducing the color depth of images (Section 3.1) and using smoothing to reduce variation between pixels (Section 3.2). In later sections, we look at the effects of each squeezing method on classification accuracy (Section 4) and robustness to hostile input (Section 5)."}, {"heading": "3.1 Color Depth", "text": "A neural network as a differentiable model assumes that the input space is continuous. However, modern computer architectures only support discrete representations as an approximation of continuous natural data. A standard digital image is represented by an array of pixels, each of which is normally represented as a number representing a particular color.Common image representations use color bit depths that lead to irrelevant characteristics, so we assume that reducing the bit depth can reduce the opposing possibilities without compromising classification accuracy. Two common representations we focus on here due to their use in our test datasets are 8-bit grayscale images and 24-bit shades. A grayscale image provides 28 = 256 possible values for each pixel. An 8-bit value represents the intensity of a pixel where 0 is completely black, 255 is completely white. Other numbers represent different grayscale images. The grayscale images can be expanded by 8-scale, and the color scale can be separated by three: green."}, {"heading": "3.1.1 Squeezing Color Bits", "text": "While people generally prefer greater bit depth because it brings the displayed image closer to the natural image, large color depths are often not necessary to interpret the images (for example, people have no problem recognizing black-and-white images).We examine the bit depth achieved with two popular image classification datasets: MNIST and CIFAR-10.MNIST. The MNIST datasets contain 70,000 images of handwritten digits (0 to 9), of which 60,000 images are used as training data and the remaining 10,000 images are used for testing.Each image is 28 \u00d7 28 pixels, and each pixel is encoded as 8-bit grayscale.D. Figure 2 shows the first example of each class in the MNIST dataset."}, {"heading": "3.1.2 Implementation", "text": "We assume that the image pixel values have been scaled in the range of [0.1]. Therefore, we design our bit depth reduction to maintain the original value scale while squeezing out unnecessary precision. We use a simple nonlinear approach to reduce the bit depth evenly on each channel. Section 3.2 discusses some alternatives that may be worth exploring. The reduction is implemented using the Python Numpy library, using the code shown in Figure 5. The input x is a pixel matrix in the size of width x height, which represents an 8-bit grayscale image scaled in the range of [0.1] (or a three-dimensional tensor of width x height x x x 3, which represents a 24-bit true color image).The parameter bit depth is the target bit depth, which is an integer selected from [1.7]. The function returns an image with the same scale, but with its bitdepth reduced to bitrate."}, {"heading": "3.2 Spatial Smoothing", "text": "Spatial smoothing (also known as blurring) is a group of techniques commonly used in image processing to reduce image noise. Common methods are Gaussian smoothing and the median smoothing method we employ.Median smoothing is a nonlinear noise suppression technology [28]. As we report in Section 5.2, it is particularly effective in mitigating conflicting examples generated by the L0 attack.The median filter passes a sliding window over each pixel of the image in which the middle pixel is replaced by the mean value of the adjacent pixels within the window. It does not reduce the number of pixels in the image, but distributes pixel values over nearby pixels. The median filter is essential in squeezing features out of the sample by making adjacent pixels more similar. The size of the window is a configurable parameter ranging from 1 to image size."}, {"heading": "3.3 Other Squeezing Methods", "text": "Our results in this paper are limited to these two simple squeezing methods, which are surprisingly effective for our test data sets. However, we believe that many other squeezing methods are possible, and further experiments are worthwhile in order to find the most effective squeezing methods. [12] One area that needs to be explored is lossy compression techniques. Kurakin et al. investigated the effectiveness of the JPEG format in mitigating the opposing examples [12]. Their experiment shows that a very low JPEG quality (e.g. 10 out of 100) is capable of destroying the hostile interference generated by FGSM at a maximum of 30% of the successful opposing examples. Another possible direction is to reduce dimensions. Turk and Pentland's early work pointed out, for example, that many pixels are irrelevant features in face recognition and that facial images cannot be projected to the accuracy of legitimate input."}, {"heading": "4 Accuracy", "text": "Before considering the effects of feature squeezing on enemy input, we confirm our intuition that image characteristics can be squeezed without compromising the accuracy of the classifier. We check whether the two feature squeezing methods, bit depth reduction and median smoothing, are able to maintain model accuracy on legitimate inputs, supporting our theory that many irrelevant features are present in images as they are used in typical image classification tasks."}, {"heading": "4.1 Color Depth", "text": "MNIST. We train an MNIST model using the original training data and test its accuracy by adjusting the bit depth of the test images. Table 3 shows the classification accuracy of the legitimate samples of the MNIST data set. Reducing the bit depth of the input images from 8 bits to 1 reduces the accuracy of the test samples from 99.30% to 99.24%. That is, over the 10,000 images, the most aggressive squeezing increases the total number of false predictions by 6. The binary filter corrects 13 predictions (Table 1, and makes 19 false predictions (Table 2). For two of the images (which are most contained on the right of Table 1), the DNN model makes different predictions about the original and binary filtered images, but both are faulty. 1CIFAR-10. Table 4 shows the accuracy of the trained models on the CIFAR-10 dataset as we vary the bit depth. The vertical training axis does not show the depth of the first Bit on the 9Bit used."}, {"heading": "4.2 Spatial Smoothing", "text": "MNIST. Spatial smoothing has a greater impact on legitimate accuracy than decreasing bit depth (the accuracy of the results is shown in Figure 9a in Section 5.2).The original accuracy of legitimate input is maintained with the window size smaller than 5 x 5, but falls off quickly with larger window sizes and is less than 0.5 at 8 x 8. We choose the window size as 3 x 3, which causes only a slight drop in accuracy from 99.30% to 99.10% (20 out of 10,000 correctly classified test images are incorrectly classified after smoothing 3 x 3).We believe that this is an acceptable loss when taking into account the increased robustness to the opponent (Section 5.2).CIFAR-10. In addition to square windows, we also try rectangular window shapes in medium smoothing based on CIFAR-10: K \u00d7 K and K \u00d7 1, with values from K to 8. The three window shapes have different, square window shapes based on the general K and K-based on the accuracy, not the K-based on the K-based on the general accuracy."}, {"heading": "5 Robustness", "text": "The previous section showed that images used in classification tasks contain many irrelevant features that can be squeezed without diminishing accuracy. This section assesses the effectiveness of feature squeezing against adversary examples. We assume a powerful adversary has full access to a trained and defined DNN classification model, but no ability to influence that model. The goal of the adversary is to find input that is misclassified by the model. We focus on gradient-based white-box evasion attacks against a trained and defined DNN classifier, and consider both un-targeted and targeted attacks. At the moment, we do not consider an adversary that adapts to our methods. Observant readers will notice that our squeezing methods provide new opportunities for adversaries, some of which we will discuss in Section 7.2. For this reason, we do not advocate the use of feature squeezing for itself to harden DNs; instead, we consider it a match for our detection."}, {"heading": "5.1 Color Depth", "text": "The resolution of a certain bit depth is defined as the shortest distance between two values on a given filter layer, which we can assume instead as the filter layer that can be distinguished. For example, the resolution of an 8-bit color depth is 1 on the [0,255] scale and 0.00390625 on the [0,1] scale of sensitive filters = 784 Effective examples reduce resolution and reduce the possibility of an opponent having to find effective perturbations. This should be effective in mitigating L2 and L attacks, since the goal of the opponent is to produce small and imperceptible perturbations. As the resolution is reduced, such perturbations have no effect. We rate the binary filter as a way to mitigate the FGSM inputs, because it is the most powerful and it hardly reduces accuracy on the legitimate examples."}, {"heading": "5.2 Spatial Smoothing", "text": "The disturbances mentioned in the JSMA (L0) are similar to those in the JSMA, although they were introduced deliberately rather than randomly. It is assumed that the opposing strength of an L0 opponent limits the number of pixels selected for JSMA. Therefore, it is not surprising that the number of changes to each individual pixel is as high as it is."}, {"heading": "5.3 Combining with Adversarial Training", "text": "The most successful previous defense against enemy examples is enemy training (Section 2.3). To evaluate the effectiveness of combining our feature squeezing method with adversarial training, we combined it with the adversarial training of Cleverhans [17]. The goal is to minimize the mean loss of legitimate examples and the adversarial ones generated with FGSM on the fly = 0.3. The model is trained in 100 epochs, which is the same as the original MNIST model in this work process. Figure 10 shows that the bit depth reduction per se significantly exceeds the adversarial training method."}, {"heading": "6 Detecting Adversarial Inputs", "text": "We will consider a simple strategy for detecting adversarial inputs that builds on our feature squeezing methods: to compare the prediction of the model on the original sample with its prediction on the sample after squeezing. Predicting a legitimate example and its3We would also test retraining with the JSMA opponent on CIFAR-10, but will not be able to do this experiment as the time adversarial training with JSMA on a larger model is prohibitively expensive.0 5001000050000200050050000000000000000000.0 0.5 2.0Non-adversarial (10,000) 1 1 1 1 0 2 0.30501001001500020005000000000.50000500000000000.0 JSMA050000200020002000200020025000000000000.0-1000000000000.0-10000000000.0"}, {"heading": "6.1 Experimental Results", "text": "This means that the prediction will be very different. We use test images from MNIST and explore both the L \u221e opponent FGSM and the L0 opponent JSM. We use the L1 standard to measure the difference between the original prediction vector and the filtered prediction. The higher the hit rate, the higher the difference between the original prediction value and the filtered examples for an input x. As our accuracy results confirm (Section 4), such results should be small as the prediction cannot be changed by squeezing. On the other hand, the L1 points on adversarial inputs should be greater than feature squeezing."}, {"heading": "6.2 Comparison with Other Methods", "text": "We can only make rough comparisons between our results and the detection performance of other published methods, although Table 5 compares the results of the experimental method using the drastically different experimental structures and methods for measuring detection rates. Two previous papers conducted their experiments using the same MNIST dataset, including the detection of the integrated outlier class [7] and the best result from Feinman et al. [5], where a logistic regression model is trained to detect conflicting examples using both the estimate of nuclear density and the uncertainty of the Baysian neural network as characteristics. Table 5 summarizes their experimental results and compares them with us. It is important to note that the results in Table 5 are not directly comparable, due to differences in the experimental arrangements. First, the basic classifiers used in the three studies differ slightly from the DNN model architecture and the accuracy of the input data sets from the second to the second set."}, {"heading": "7 Discussion", "text": "The effectiveness of feature squeezing seems surprising, as it is so simple and inexpensive compared to other proposed defenses. In this section, we'll speculate on why it works and discuss how opponents can adapt to feature squeezing."}, {"heading": "7.1 Why Squeezing Works", "text": "Developing a theory of enemy examples remains an illusionary goal, but our intuition is that the efficacy of the squeeze is due to how it reduces the search space of the possible perturbations available to an adversary. Bit depth reduction. The original model with 8-bit depth inputs is sensitive to the tiny perturbations even at the slightest bit. By essentially eliminating some of the lower bits, we shrink the feature space and force the adversary to produce larger perturbations. Since the characteristics we effectively eliminate are not relevant for classification, this has little impact on the accuracy of legitimate samples. Bit depth reduction is effective in mitigating the adverse examples generated by FGSM, but this leaves an open question as to how well our method would work against advanced adversaries. So far, researchers have focused on improving the attack method to reduce the amount of perturbations that are necessary to produce contradictory examples."}, {"heading": "7.2 Adversarial Adaptation", "text": "Our threat model only takes into account static adversaries that use advanced methods to find opposing examples, but do not adapt to directly attack our feature squeeze method. However, in practice, an adversary can exploit feature squeezing to attack a DNN model. For example, when confronted with binary squeeze, an adversary can construct an image by setting all pixel intensity values to close to 0.5. This image is completely gray to human eyes, but by setting pixel values to either 0.499 or 0.501, it can result in an arbitrary 1-bit filtered image after squeezing. Fortunately, it is possible to detect such an attack by comparing the difference between the original and the squeezed inputs, as the difference would exceed normal thresholds. As seen in Section 6, opposing examples can be accurately detected by the differences between model output and the squeezed inputs when comparing the counterpart to the counterpart."}, {"heading": "8 Conclusion", "text": "Previous defense methods have been mathematically expensive and have proven to be prone to small advances in adversary techniques. In this paper, we propose Feature Squeezing, a new and generic method to make DNN models robust to enemy input while maintaining high accuracy in legitimate examples. Feature Squeezing is inexpensive and easy to implement, and can be compiled with any defense strategy based on the model, such as enemy training. By eliminating unnecessary points in feature space, Feature Squeezing reduces the level of freedom available to an adversary. It can be used in a simple framework to detect adversary examples with high accuracy by comparing the predictions of a model based on feature squeeze input with its predictions on the original input. Availability The source code of our feature squeezing methods will soon be available at http / Made.org along with the data from all our experiments."}, {"heading": "Acknowledgements", "text": "This work was funded by grants from the National Science Foundation and the Air Force Office of Scientific Research, as well as a gift from Google. We thank Nicolas Papernot and the other authors of Cleverhans for providing such a useful tool to the research community."}], "references": [{"title": "Defensive Distillation is not Robust to Adversarial Examples", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1607.04311,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Towards Evaluating the Robustness of Neural Networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "In IEEE Symposium on Security and Privacy (Oakland),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Large-scale Malware Classification using Random Projections and Neural Networks", "author": ["George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Detecting Adversarial Samples from Artifacts", "author": ["Reuben Feinman", "Ryan R Curtin", "Saurabh Shintre", "Andrew B Gardner"], "venue": "arXiv preprint arXiv:1703.00410,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "On the (Statistical) Detection of Adversarial Examples", "author": ["Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1702.06280,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint arXiv:1412.5068,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "DeepSpeech: Scaling up End-to-end Speech Recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "others"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Improving Neural Networks by Preventing Co-adaptation of Feature Detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Adversarial Examples in the Physical World", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "In International Conference on Learning Representations (ICLR) Workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Gradient-based Learning Applied to Document Recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "On Detecting Adversarial Perturbations", "author": ["Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff"], "venue": "arXiv preprint arXiv:1702.04267,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "cleverhans v1.0.0: an adversarial machine learning library", "author": ["Nicolas Papernot", "Ian Goodfellow", "Ryan Sheatsley", "Reuben Feinman", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "The Limitations of Deep Learning in Adversarial Settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In IEEE European Symposium on Security and Privacy (EuroS&P),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Towards the Science of Security and Privacy in Machine Learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Arunesh Sinha", "Michael Wellman"], "venue": "arXiv preprint arXiv:1611.03814,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In IEEE Symposium on Security and Privacy (Oakland),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Deep Face Recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Intriguing Properties of Neural Networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Face Recognition using Eigenfaces", "author": ["Matthew A Turk", "Alex P Pentland"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "A Theoretical Framework for Robustness of (Deep) Classifiers Under Adversarial Noise", "author": ["Beilun Wang", "Ji Gao", "Yanjun Qi"], "venue": "In International Conference on Learning Representations (ICLR) Workshop,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Adversarial Feature Selection against Evasion Attacks", "author": ["Fei Zhang", "Patrick PK Chan", "Battista Biggio", "Daniel S. Yeung", "Fabio Roli"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Deep Neural Networks (DNNs) perform exceptionally well on many artificial intelligence tasks, including security-sensitive applications like malware classification [15, 4] and face recognition [22].", "startOffset": 164, "endOffset": 171}, {"referenceID": 19, "context": "Deep Neural Networks (DNNs) perform exceptionally well on many artificial intelligence tasks, including security-sensitive applications like malware classification [15, 4] and face recognition [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 4, "context": "The maliciously generated inputs are called \u201cadversarial examples\u201d [6, 24] and are normally crafted by using an optimization procedure to search for small, but effective, artificial perturbations.", "startOffset": 67, "endOffset": 74}, {"referenceID": 20, "context": "The maliciously generated inputs are called \u201cadversarial examples\u201d [6, 24] and are normally crafted by using an optimization procedure to search for small, but effective, artificial perturbations.", "startOffset": 67, "endOffset": 74}, {"referenceID": 9, "context": "Deep Neural Networks (DNNs) can efficiently learn highly-accurate models from large corpora of training samples in many domains [11, 9, 15].", "startOffset": 128, "endOffset": 139}, {"referenceID": 7, "context": "Deep Neural Networks (DNNs) can efficiently learn highly-accurate models from large corpora of training samples in many domains [11, 9, 15].", "startOffset": 128, "endOffset": 139}, {"referenceID": 11, "context": "[13], perform exceptionally well on image classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24] first observed that DNN models are vulnerable to adversarial perturbation and used the Limited-memory BFGS (L-BFGS) to find adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Subsequent papers have explored other strategies to generate adversarial manipulations, including using gradient information [6], saliency maps [19], and evolutionary algorithms [16].", "startOffset": 125, "endOffset": 128}, {"referenceID": 16, "context": "Subsequent papers have explored other strategies to generate adversarial manipulations, including using gradient information [6], saliency maps [19], and evolutionary algorithms [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 13, "context": "Subsequent papers have explored other strategies to generate adversarial manipulations, including using gradient information [6], saliency maps [19], and evolutionary algorithms [16].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "Our experiments use implementations of the FGSM and JSMA attacks provided by the Cleverhans library [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "hypothesized that DNNs are vulnerable to adversarial perturbations because of their linear nature [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "3 are typically not considered valid adversarial examples [6] since such high \u03b5 values produce images that are obviously different from the original images.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "[19] proposed the Jacobian-based saliency map approach (JSMA) to search for adversarial examples by only modifying a limited number of input pixels in an image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We use the parameters configured by default in Cleverhans [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "[20] provide a comprehensive summary of recent work on defending against adversarial samples, grouping work into two broad categories: adversarial training and gradient masking, which we discuss further below.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "proposed a theory that unnecessary features are the primary cause of a classifier\u2019s vulnerability to adversarial examples [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "proposed an adversaryaware feature selection model that can improve classifier robustness against evasion attacks [29].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "A straightforward solution is adversarial training, which introduces discovered adversarial examples and the corresponding ground truth labels to the training dataset [6, 24].", "startOffset": 167, "endOffset": 174}, {"referenceID": 20, "context": "A straightforward solution is adversarial training, which introduces discovered adversarial examples and the corresponding ground truth labels to the training dataset [6, 24].", "startOffset": 167, "endOffset": 174}, {"referenceID": 17, "context": "[20], it is essential to include adversarial examples produced by all known attacks in adversarial training, since this defensive training is non-adaptive.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "of the Jacobian matrix [8].", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "Although the trained model behaves more robustly against adversaries, the penalty significantly reduces the capacity of the model and sacrifices accuracy on many tasks [20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "introduced defensive distillation to harden DNN models [21].", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "However, two subsequent studies showed that defensive distillation failed to mitigate a variant of JSMA with a division trick [2] and a black-box attack [18].", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "However, two subsequent studies showed that defensive distillation failed to mitigate a variant of JSMA with a division trick [2] and a black-box attack [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "concluded that methods designed to conceal gradient information are bound to have limited success because of the transferability of adversarial examples [20].", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": "A few recent studies [14, 7, 5] have focused on detecting adversarial examples.", "startOffset": 21, "endOffset": 31}, {"referenceID": 5, "context": "A few recent studies [14, 7, 5] have focused on detecting adversarial examples.", "startOffset": 21, "endOffset": 31}, {"referenceID": 3, "context": "A few recent studies [14, 7, 5] have focused on detecting adversarial examples.", "startOffset": 21, "endOffset": 31}, {"referenceID": 5, "context": "[7] propose a statistical test method for detecting adversarial examples using maximum mean discrepancy and energy distance as the statistical distance measures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "propose detecting adversarial examples using kernel density estimation [5], which measures the distance between an unknown input example and a group of legitimate examples in a manifold space (represented as features in some middle layers of a DNN).", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "[7] and Feinman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] have found that strategies relying on sample statistics gave inferior detection performance compared to other strategies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "proposed attaching a CNN-based detector as a branch off a middle layer of the original DNN [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "[16]) that adds a new \u201cadversarial\u201d class in the last layer of the DNN model [7].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[16]) that adds a new \u201cadversarial\u201d class in the last layer of the DNN model [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "borrowed an idea from dropout [10] and designed a detection technique they called Bayesian neural network uncertainty [5] .", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "borrowed an idea from dropout [10] and designed a detection technique they called Bayesian neural network uncertainty [5] .", "startOffset": 118, "endOffset": 121}, {"referenceID": 4, "context": "Figure 3 hints at why reducing color depth can mitigate adversarial examples generated by the fast gradient sign method (FGSM) [6] (Section 2.", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "The parameter bit depth is the target bit depth which is an integer chosen from [1,7].", "startOffset": 80, "endOffset": 85}, {"referenceID": 16, "context": "It suggests why spatial smoothing can mitigate adversarial examples generated by the Jacobian-based saliency map approach (JSMA) [19] (Section 2.", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "explored the effectiveness of the JPEG format in mitigating the adversarial examples [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "For example, Turk and Pentland\u2019s early work pointed out that many pixels are irrelevant features in the face recognition tasks, and the face images can be projected to a feature space named eigenfaces [26].", "startOffset": 201, "endOffset": 205}, {"referenceID": 17, "context": "We choose \u03b5 from the integers in the [1,20] range, the end points of which are equivalent to 0.", "startOffset": 37, "endOffset": 43}, {"referenceID": 14, "context": "To evaluate the effectiveness of composing our feature squeezing method with adversarial training, we combined it with the adversarial training code from Cleverhans [17].", "startOffset": 165, "endOffset": 169}, {"referenceID": 5, "context": "[7] 99.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] 90.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Two previous works conducted their experiments on the same MNIST dataset, including the integrated outlier class detection [7] and the best result from Feinman et al.", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "[5], where a logistic regression model is trained to detect adversarial examples using both the kernel density estimation and the Baysian neural network uncertainty as features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] and Feinman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] using ROC-AUC scores (on the validation sets), our method has superior performance on both detecting FGSM (99.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] improve the L\u221e attack and claim that it is \u201cso successful that we can change the classification of an image to any desired label by only flipping the lowest bit of each pixel\u201d.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from expensive computation. We propose a new strategy, feature squeezing, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model\u2019s prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.", "creator": "LaTeX with hyperref package"}}}