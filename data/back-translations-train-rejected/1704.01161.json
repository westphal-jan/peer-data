{"id": "1704.01161", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "Finite Sample Analysis for TD(0) with Linear Function Approximation", "abstract": "TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such a result. Works that managed to obtain concentration bounds for online Temporal Difference (TD) methods analyzed modified versions of them, carefully crafted for the analyses to hold. These modifications include projections and step-sizes dependent on unknown problem parameters. Our analysis obviates these artificial alterations by exploiting strong properties of TD(0) and tailor-made stochastic approximation tools.", "histories": [["v1", "Tue, 4 Apr 2017 19:47:52 GMT  (133kb,D)", "http://arxiv.org/abs/1704.01161v1", null], ["v2", "Sun, 2 Jul 2017 10:28:28 GMT  (63kb,D)", "http://arxiv.org/abs/1704.01161v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gal dalal", "bal\\'azs sz\\\"or\\'enyi", "gugan thoppe", "shie mannor"], "accepted": false, "id": "1704.01161"}, "pdf": {"name": "1704.01161.pdf", "metadata": {"source": "CRF", "title": "Finite Sample Analysis for TD(0) with Linear Function Approximation", "authors": ["Gal Dalal", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Gugan Thoppe", "Shie Mannor"], "emails": ["gald@tx.technion.ac.il", "szorenyi.balazs@gmail.com", "gugan.thoppe@gmail.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "The term was coined in [Sutton and Barto, 1998] and describes an iterative process of actualizing a value function V \u03c0 (s) in relation to a given policy \u03c0 on the basis of temporally consecutive samples. The classic version of the algorithm uses a tabular representation, i.e., the entry-based storage of the value per state s. \"However, in many problems, the state space S is too large for such a vanilla approach. The common practice for mitigating this caveat is approximation to the value function using some parameterized families. Linear regression is often used, i.e. the V proposition (s) is considered a prerequisite for an efficient implementation of TD (0)."}, {"heading": "1.1 Existing Literature", "text": "The first step in the right direction has been taken."}, {"heading": "1.2 Our Contribution", "text": "Our work is the first to set a limit on the convergence rate of TD (0) in its original, unaltered form. In fact, it is the first to set a concentration limit for an unaltered online TD algorithm of any kind. In fact, the existing convergence rates only apply to online TD algorithms with changes such as projections and increments depending on unknown problem parameters; alternatively, they only apply to the average of iterates. We believe that the key components of our approach to avoid these changes are i) that the n-th iterate is at worst only O (n) away from the solution; and ii) based on this, we show that after a few additional steps all subsequent iterates are close to the solution. We believe that this approach is not limited to TD (0) alone. In addition, we provide the first expected decay rate of the actual TD (0) iterate. It applies to a general family of increments that are not limited to most summarized works."}, {"heading": "2 Problem Setup", "text": "We consider the problem of policy evaluation for a Markov decision-making process (MDP) >. An MDP is defined by the 5-fold (S, A, P, R, Z, Z, Z), where S is the composition of states, A is the set of fractions, P = P (s) is the transition core, R (s, a, s) is the reward function, and R (0, 1) is the discount factor. In each step of time, the process in some states is \"S\" an action taken by a \"A,\" the system transition to the next state is \"S\" according to the transition core \"P,\" and an immediate reward is \"r\" according to R (s, a, s). Let the policy \"A\" be a stationary mapping of states to actions. \"The summary of the associated Markov chain is ergodic and uni-chain, let's be the induced distribution."}, {"heading": "3 Main Result", "text": "Our main result is the following: O-notation hides problem-related constants and polylogarithmic terms (then there are no further results)."}, {"heading": "4 Proof of Theorem 1", "text": "This section outlines the analysis carried out for Theorem 1. All proofs are listed in Appendix B."}, {"heading": "4.1 Outline of Approach", "text": "We compare the TD (0) iterations with suitable solutions of their limiting ODE method with the method Variation of Parameters (VoP) [Lakshmikantham and Deo, 1998]. Since the solutions of the ODE are continuous functions of time, we first define a linear interpolation {\u00da (t)} of {\u03b8n}. Let us leave t0 = 0. Let us leave tn + 1 = tn + \u03b1n and let\u03b8 (\u03c4) = {\u03b8n if it happens = tn, \u03b8n + \u03c4 \u2212 tn if it happens (tn, tn + 1). (5) The limiting ODE for (2) is \u0432 (t) = h (\u03b8 (t) = b \u2212 A\u03b8 (t) = \u2212 A (t) \u2212 \u0445n if it is not fast enough. (6) Let us be prepared that the solution for the above mentioned ODE iteration is not fast enough."}, {"heading": "4.2 Preliminaries", "text": "Using the results of Chapter 6, [Hirsch et al., 2012], the solution \u03b8 (t, s, u0), t (t, s, u0), t (t, s), t (t, s, u0), t (t, s, u0), t (t, s, u0), t (t), t (t), t (t), t (t), t (t), t (t), t (t), t (t), t (t), t (t), t (t), t (t), t), t (t), t (t), t (t), t (t), t (t)."}, {"heading": "4.3 Part I \u2013 Initial Possible Divergence", "text": "In this section we show that the TD (0) -Iterate is in an O (n) -ball around the Earth. We emphasize that this is one of the results that enables us to achieve more than the existing literature. Previously, the distance of the initial iterate from the Earth was limited by various assumptions that are often justified by an artificial projection step that we can avoid. Let Rwc (n): = 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0 + 0 + 0 + 1 + 0 + 1 + 0 + 1 + 1 + 1 + 1 + 1 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 1 1 + 1 1 + 1 1 1 + 1 1 + 1 1 1 + 1 1 + 1 1 1 + 1 1 + 1 1 1 + 1 + 1 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 +"}, {"heading": "4.4 Part II \u2013 Rate of Convergence", "text": "The goal is to get an estimate of the probability of the event (n0, n1): \"We do this by doing the TD (0) -1-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0"}, {"heading": "5 Proof of Theorem 3", "text": "s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2) and the application of a subtle trick by Kamal [2010]. Building on that approach, our most important steps are: determining a \"nice\" Liapunov function V of the limiting ODE of the TD (0) method; and then applying a conditional expectation that is suitable to get rid of the linear noise concepts in the relationship between V (\u03b8n) and V (\u03b8n + 1)."}, {"heading": "6 Discussion", "text": "In this paper, we have reached the first concentration limit for an unmodified version of the famous TD (0); in fact, it is the first to provide the convergence rate of an unmodified online TD algorithm of any kind. Using the nonlinear analysis presented in [Thoppe and Borkar, 2015], we believe it can be extended to a broader family of functional approximators, such as neural networks. In addition, future work can be extended to more general familial learning rates, including the frequently used adaptive ones. Building on Remark 5, we believe that a stronger expectation limit may apply to TD (0), which could allow tighter concentration limits for TD (0) to be achieved even at generic increments."}, {"heading": "B Supplementary Material for Proof of Theorem 1", "text": "The proof of Lemma 4, and the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 11, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 13, the proof of Lemma 13, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 13, the proof of Lemma 6, the proof of Lemma 13 of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 6, the proof of Lemma 13 of Lemma 6, the evidence of Lemma 13 of the evidence of Lemma 13 of the Lemma 13 of the Lemma 13, the evidence of the Lemma 11 the evidence of the Lemma 11 of the evidence of the Lemma 11, the evidence of Lemma 11 of the evidence of the Lemma 11, the evidence of the Lemma 11 of the evidence of the"}, {"heading": "C Supplementary Material for Proof of Theorem 3", "text": "Note that the matrices (A > + A) and (A > A + > KmI) are symmetrical. Furthermore, asA is positively defined, the above matrices are also positively defined. Therefore, their minimum and maximum eigenvalues are strictly positive. Lemma 16. For n > 0, allow it (A >). Let me be so that I (A + A >) \u2212 n (A + A >) \u2212 n \u00b2 (A + 4K2m I). Then for all k, nso that n \u00b2 k (A + 4Km)."}], "references": [{"title": "Dynamic Programming and Optimal Control", "author": ["D.P. Bertsekas"], "venue": "Vol II. Athena Scientific, fourth edition,", "citeRegEx": "Bertsekas.,? \\Q2012\\E", "shortCiteRegEx": "Bertsekas.", "year": 2012}, {"title": "Stochastic approximation: a dynamical systems viewpoint", "author": ["Vivek S Borkar"], "venue": null, "citeRegEx": "Borkar.,? \\Q2008\\E", "shortCiteRegEx": "Borkar.", "year": 2008}, {"title": "The ode method for convergence of stochastic approximation and reinforcement learning", "author": ["Vivek S Borkar", "Sean P Meyn"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Borkar and Meyn.,? \\Q2000\\E", "shortCiteRegEx": "Borkar and Meyn.", "year": 2000}, {"title": "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes", "author": ["Max Fathi", "Noufel Frikha"], "venue": "Electron. J. Probab.,", "citeRegEx": "Fathi and Frikha.,? \\Q2013\\E", "shortCiteRegEx": "Fathi and Frikha.", "year": 2013}, {"title": "Concentration bounds for stochastic approximations", "author": ["Noufel Frikha", "St\u00e9phane Menozzi"], "venue": "Electron. Commun. Probab.,", "citeRegEx": "Frikha and Menozzi.,? \\Q2012\\E", "shortCiteRegEx": "Frikha and Menozzi.", "year": 2012}, {"title": "Differential equations, dynamical systems, and an introduction to chaos", "author": ["Morris W Hirsch", "Stephen Smale", "Robert L Devaney"], "venue": "Academic press,", "citeRegEx": "Hirsch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hirsch et al\\.", "year": 2012}, {"title": "On the convergence, lock-in probability, and sample complexity of stochastic approximation", "author": ["Sameer Kamal"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Kamal.,? \\Q2010\\E", "shortCiteRegEx": "Kamal.", "year": 2010}, {"title": "On td (0) with function approximation: Concentration bounds and a centered variant with exponential convergence", "author": ["Nathaniel Korda", "LA Prashanth"], "venue": "In ICML,", "citeRegEx": "Korda and Prashanth.,? \\Q2015\\E", "shortCiteRegEx": "Korda and Prashanth.", "year": 2015}, {"title": "Method of variation of parameters for dynamic systems", "author": ["Vangipuram Lakshmikantham", "Sadashiv Deo"], "venue": null, "citeRegEx": "Lakshmikantham and Deo.,? \\Q1998\\E", "shortCiteRegEx": "Lakshmikantham and Deo.", "year": 1998}, {"title": "Finite-sample analysis of lstd", "author": ["Alessandro Lazaric", "Mohammad Ghavamzadeh", "R\u00e9mi Munos"], "venue": "In ICML-27th International Conference on Machine Learning,", "citeRegEx": "Lazaric et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lazaric et al\\.", "year": 2010}, {"title": "Finite-sample analysis of proximal gradient td algorithms", "author": ["Bo Liu", "Ji Liu", "Mohammad Ghavamzadeh", "Sridhar Mahadevan", "Marek Petrik"], "venue": "In UAI,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703", "author": ["Warren B Powell"], "venue": null, "citeRegEx": "Powell.,? \\Q2007\\E", "shortCiteRegEx": "Powell.", "year": 2007}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S Sutton"], "venue": "Machine learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A convergent o (n) temporal-difference algorithm for off-policy learning with linear function approximation", "author": ["Richard S Sutton", "Hamid R Maei", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["Richard S Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesv\u00e1ri", "Eric Wiewiora"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["Richard S Sutton", "A Rupam Mahmood", "Martha White"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Temporal difference learning and td-gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "A concentration bound for stochastic approximation via alekseev\u2019s formula", "author": ["Gugan Thoppe", "Vivek S Borkar"], "venue": null, "citeRegEx": "Thoppe and Borkar.,? \\Q2015\\E", "shortCiteRegEx": "Thoppe and Borkar.", "year": 2015}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "Tsitsiklis and Roy,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1997}, {"title": "Convergence results for some temporal difference methods based on least squares", "author": ["Huizhen Yu", "Dimitri P Bertsekas"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Yu and Bertsekas.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Bertsekas.", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": "The term has been coined in [Sutton and Barto, 1998], describing an iterative process of updating an estimate of a value function V (s) with respect to a given policy \u03c0 based on temporally-successive samples.", "startOffset": 28, "endOffset": 52}, {"referenceID": 11, "context": "The term has been coined in [Sutton and Barto, 1998], describing an iterative process of updating an estimate of a value function V (s) with respect to a given policy \u03c0 based on temporally-successive samples. The classical version of the algorithm uses a tabular representation, i.e., entry-wise storage of the value estimate per each state s \u2208 S. However, in many problems the state-space S is too large for such a vanilla approach. The common practice to mitigate this caveat is to approximate the value function using some parameterized family. Often, linear regression is used, i.e., V (s) \u2248 \u03b8>\u03c6(s). This allows for an efficient implementation of TD(0) even on large state-spaces and has shown to perform well in a variety of problems Tesauro [1995], Powell [2007].", "startOffset": 29, "endOffset": 754}, {"referenceID": 11, "context": "This allows for an efficient implementation of TD(0) even on large state-spaces and has shown to perform well in a variety of problems Tesauro [1995], Powell [2007]. More recently, TD(0) has become prominent in many state-of-the-art RL solutions when combined with deep neural network architectures, as an integral part of fitted value iteration [Mnih et al.", "startOffset": 151, "endOffset": 165}, {"referenceID": 1, "context": "In [Borkar, 2008], a concentration bound is given for generic SA algorithms.", "startOffset": 3, "endOffset": 17}, {"referenceID": 7, "context": "In [Korda and Prashanth, 2015], concentration bounds for TD(0) with mixing-time consideration have been given.", "startOffset": 3, "endOffset": 30}, {"referenceID": 0, "context": "Following that, a key result by Borkar and Meyn [2000] paved the path to a unified and convenient tool for convergence analyses of Stochastic Approximation (SA), and hence of TD algorithms.", "startOffset": 32, "endOffset": 55}, {"referenceID": 0, "context": "Following that, a key result by Borkar and Meyn [2000] paved the path to a unified and convenient tool for convergence analyses of Stochastic Approximation (SA), and hence of TD algorithms. This tool is based on the Ordinary Differential Equation (ODE) method. Essentially, that work showed that under the right conditions, the SA trajectory follows the solution of a suitable ODE, often referred to as its limiting ODE; thus, it eventually converges to the solution of the limiting ODE. Several usages of this tool in RL literature can be found in [Sutton et al., 2009a,b, 2015]. As opposed to the case of asymptotic convergence analysis of TD algorithms, very little is known on their finite sample behavior. We now briefly discuss the few existing results on this topic. In [Borkar, 2008], a concentration bound is given for generic SA algorithms. Recent works [Kamal, 2010, Thoppe and Borkar, 2015] obtain better concentration bounds via tighter analyses. The results in these works are conditioned on the event that the n0\u2212th iterate lies in some a-priori chosen bounded region containing the desired equilibria; this, therefore, is the caveat in applying them to TD(0). In [Korda and Prashanth, 2015], concentration bounds for TD(0) with mixing-time consideration have been given. However, unlike in our work, a strong requirement for all their high probability bounds is that the iterates need to lie in some a-priori chosen bounded set; this is ensured there via projections (personal communication). Additionally, their results require the learning rate to be set based on prior knowledge about system dynamics, which, as argued in the paper, is problematic; alternatively, they apply to average of iterates. An additional work by Liu et al. [2015] considered the gradient TD algorithms GTD(0) and GTD2, which were first introduced in [Sutton et al.", "startOffset": 32, "endOffset": 1758}, {"referenceID": 14, "context": "A MDP is defined by the 5-tuple (S,A , P,R, \u03b3) [Sutton, 1988], where S is the set of states, A is the set of", "startOffset": 47, "endOffset": 61}, {"referenceID": 0, "context": "It is known that A is positive definite [Bertsekas, 2012] and that (2) converges to \u03b8\u2217 := A\u22121b [Borkar, 2008].", "startOffset": 40, "endOffset": 57}, {"referenceID": 1, "context": "It is known that A is positive definite [Bertsekas, 2012] and that (2) converges to \u03b8\u2217 := A\u22121b [Borkar, 2008].", "startOffset": 95, "endOffset": 109}, {"referenceID": 7, "context": "Theorem 1, Korda and Prashanth [2015] requires the TD(0) step-sizes to satisfy: \u03b1n = fn(\u03bb) for some function fn, where \u03bb is as above.", "startOffset": 11, "endOffset": 38}, {"referenceID": 1, "context": "In [Borkar, 2008], on which much of the existing RL literature is based on, the square summability assumption is due to the Gronwall inequality.", "startOffset": 3, "endOffset": 17}, {"referenceID": 8, "context": "In contrast, in our work, we use the Variation of Parameters Formula [Lakshmikantham and Deo, 1998] for comparing the SA trajectory to appropriate trajectories of the limiting ODE; it is a stronger tool than Gronwall inequality.", "startOffset": 69, "endOffset": 99}, {"referenceID": 6, "context": "The expectation bound in Theorem 1, Korda and Prashanth [2015] again requires the stepsize sequence be scaled as in Remark 1.", "startOffset": 36, "endOffset": 63}, {"referenceID": 6, "context": "The expectation bound in Theorem 1, Korda and Prashanth [2015] again requires the stepsize sequence be scaled as in Remark 1. Theorem 2 there obviates this, but it applies to average of iterates. In contrast, our expectation bound applies directly to the TD(0) iterates and does not need any scaling of the above kind. Moreover, our result applies to a broader family of stepsizes; see Remark 4. Our expectation bound when compared to that of Theorem 2, Korda and Prashanth [2015] is of the same order (even though theirs is for average of iterates).", "startOffset": 36, "endOffset": 481}, {"referenceID": 8, "context": "1 Outline of Approach We compare the TD(0) iterates {\u03b8n} with suitable solutions of its limiting ODE using the Variation of Parameters (VoP) method [Lakshmikantham and Deo, 1998].", "startOffset": 148, "endOffset": 178}, {"referenceID": 5, "context": "Using results from Chapter 6, [Hirsch et al., 2012], it follows that the solution \u03b8(t, s, u0), t \u2265 s, of (6) satisfies the relation \u03b8(t, s, u0) = \u03b8 \u2217 + e(u0 \u2212 \u03b8\u2217) .", "startOffset": 30, "endOffset": 51}, {"referenceID": 8, "context": "The key idea then is to use the VoP method [Lakshmikantham and Deo, 1998] and express \u03b8\u0304(t) as a perturbation of \u03b8(t) due to two factors: the discretization error and the martingale difference noise.", "startOffset": 43, "endOffset": 73}, {"referenceID": 6, "context": "The expectation bound is due to an inductive argument and an application of a subtle trick from Kamal [2010]. Building on the approach there, our key steps are: identifying a \u201cnice\" Liapunov function V of the TD(0) method\u2019s limiting ODE; and then using conditional expectation suitably to get rid of the linear noise terms in the relation between V (\u03b8n) and V (\u03b8n+1).", "startOffset": 96, "endOffset": 109}, {"referenceID": 20, "context": "Specifically, using the non-linear analysis presented in [Thoppe and Borkar, 2015], we believe it can be extended to a broader family of function approximators, e.", "startOffset": 57, "endOffset": 82}, {"referenceID": 8, "context": "From the above two relations and the VoP formula [Lakshmikantham and Deo, 1998], the desired result follows.", "startOffset": 49, "endOffset": 79}], "year": 2017, "abstractText": "TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such a result. Works that managed to obtain concentration bounds for online Temporal Difference (TD) methods analyzed modified versions of them, carefully crafted for the analyses to hold. These modifications include projections and step-sizes dependent on unknown problem parameters. Our analysis obviates these artificial alterations by exploiting strong properties of TD(0) and tailor-made stochastic approximation tools.", "creator": "LaTeX with hyperref package"}}}