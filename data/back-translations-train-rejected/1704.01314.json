{"id": "1704.01314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF", "abstract": "We present a character-based model for joint segmentation and POS tagging for Chinese. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and lower-than-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our model is accurate and robust across datasets in different sizes, genres and annotation schemes. We obtain state- of-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and POS tagging.", "histories": [["v1", "Wed, 5 Apr 2017 08:58:44 GMT  (31kb)", "https://arxiv.org/abs/1704.01314v1", "8 pages plus 2 pages references and 1 page appendix, 3 figures, submitted to EMNLP 2017"], ["v2", "Thu, 6 Apr 2017 22:53:37 GMT  (31kb)", "http://arxiv.org/abs/1704.01314v2", "8 pages plus 2 pages references and 1 page appendix, 3 figures, submitted to EMNLP 2017"], ["v3", "Tue, 12 Sep 2017 09:29:15 GMT  (32kb)", "http://arxiv.org/abs/1704.01314v3", "10 pages plus 1 page appendix, 3 figures, IJCNLP 2017"]], "COMMENTS": "8 pages plus 2 pages references and 1 page appendix, 3 figures, submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yan shao", "christian hardmeier", "j\\\"org tiedemann", "joakim nivre"], "accepted": false, "id": "1704.01314"}, "pdf": {"name": "1704.01314.pdf", "metadata": {"source": "CRF", "title": "Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF", "authors": ["Yan Shao"], "emails": ["joakim.nivre}@lingfil.uu.se", "jorg.tiedemann@helsinki.fi"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.01 314v 3 [cs.C L] 12 September 2017Common Segmentation and POS Marking for Chinese. The bidirectional RNN CRF architecture for general sequence marking will be adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and characteristics at the sign level. The proposed model will be comprehensively evaluated and compared with a state-of-the-art marker in CTB5, CTB9 and UD Chinese. Experimental results suggest that our model is accurate and robust across datasets of different sizes, genres and annotation schemes."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to set out in search of a solution that originates in the real world."}, {"heading": "2 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural Network Architecture", "text": "Our basic model is an adaptation of the BiRNNCRF. As shown in Figure 1, the Chinese characters are represented as vectors and fed into the bidirectional recurring layers. Character representations are described in detail in the following sections. For the recurring layer, we use GRU as a basic recurring unit, as it has similar functionality but fewer parameters compared to LSTM (Chung et al., 2014). Dropout (Srivastava et al., 2014) is applied to the outputs of the bidirectional recurring layers, the outputs are concatenated and passed to the CRF layer of first order, and the optimal sequence of combinatorial labels is predicted at the end. There is a post-processing step to retrieve both segmentation and POS tags from the combinatorial tags."}, {"heading": "2.2 Tagging Scheme", "text": "According to the work of Kruengkrai et al. (2009a), the tags used, which specify the word boundaries, are B, I, E, S, which represent a character at the beginning, within, at the end of a word, or as a word with a letter. CRF layer models conditional values over all possible combinatorial markups taking into account input characters. Including transition values between successive labels, the op-1 https: / / github.com / yanshao9798 / tagger (to) (hot) (summer) time sequences can be efficiently obtained via the Viterbi algorithm for both training and decryption. The temporal complexity of the Viterbi algorithm is linear in relation to the sentence length n as O (k2n), where k is constant and corresponds to the total number of combinatorial labels. Efficiency can be improved by reducing and combining the OS with some redundant tags."}, {"heading": "2.3 Character Representations", "text": "We propose three different approaches to effectively represent Chinese characters as vectors for the neural network."}, {"heading": "2.3.1 Concatenated N-gram", "text": "The dominant character-based neural models assume that larger text ranges, such as words and n-grams, can be represented by the order of the characters they consist of. For example, the vector representation Vm, n of a span cm, n is achieved by passing the vector representations vi of the characters ci to a function f as: Vm, n = f (vm, vm + 1,..., vn) (1), where f is usually an RNN (Ling et al., 2015) or a CNN (dos Santos and Zadrozny, 2014). Instead of relying entirely on the BiRNN to extract context-free character representations, we encode rich local information in the character vectors about the use of the incrementally linked n-gram representation, as shown in Figure 2."}, {"heading": "2.3.2 Radicals and Orthographical Features", "text": "Chinese characters are logograms. Unlike alphabetic languages, there is a rich information encoding in the graphic components. For example, the Chinese characters that share the same part (gold) are all somehow related to metals, such as (silver), (iron), (needle), etc. The split part is known as a radical, which acts as a semantic indicator. Therefore, we are investigating the effectiveness of using the information below the character level for our task. Radicals are initially presented as randomly initialized vectors and linked as parts of the character representations. Radicals are traditionally used as indices in Chinese dictionaries. In our approach, they are retrieved via the Unicode representation of Chinese characters, since the characters that share the same radical are grouped together. They are organized in accordance with the categorization in Kangxi dictionaries."}, {"heading": "2.3.3 Pre-trained Character Embeddings", "text": "The context-free vector representations of individual characters introduced in Section 2.3.1 can be replaced by pre-trained character embeddings retrieved from large corporations. We use GloVe (Pennington et al., 2014) to train our character embeddings on Wikipedia2 and the freely available Sogou News Corpora (SogouCS).3 We use randomly initialized vectors to represent characters that are not included in the embed vocabulary. Pre-trained embeddings of higher n-grams are not used in this essay."}, {"heading": "2.4 Ensemble Decoding", "text": "In the final decoding phase, we use ensemble decoding, a simple method of averaging to mitigate the deviations caused by random weight initialization of the neural network. In the CRF decoder, the final sequence of the combinatorial tags y is determined by the conditional values S (yi | xi) and the transition values T (yi, yj) taking into account the input sequence x. Instead of calculating the optimal sequence with respect to the values obtained from a single model, both the conditional values and the transition values are averaged over four models with identical parameter settings, which are trained independently: y \u0445 = argmax y \u0445 L (x) p (y | x; \u00af {S}, \u00af {T}) (2) Ensemble decoding is applied only to the most powerful model according to the characteristic experiments in the final test phase of this work."}, {"heading": "3 Implementation", "text": "Our neural networks are implemented with the library TensorFlow 1.2.0 (Abadi et al., 2016). We group the sets of similar length into the same buckets and the sets in the same buckets are padded to the same length accordingly. We construct sub-computerized diagrams for each bucket. The training and tagging speed of our neural network on GPU devices can be drastically improved thanks to the bucket model. Training time is proportional to both the size of the training set and the number of POS days.Table 1 shows the hyper parameters applied. We use a set of parameters for all experiments on different datasets. The weights of the 2https: / / dumps.wikimedia.org / 3http: / www.sogou.com / labs / resource / cs.phpneural networks, including randomly initialized embedding, are traced back with the Glorot and Bengio (the 2010 tag scheme = 1)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We use three different sets of data for our experiments, namely Chinese Treebank (Xue et al., 2005) 5.0 (CTB5) and 9.0 (CTB9) together with the Chinese section in Universal Dependencies (UD Chinese) (Nivre et al., 2016) of version 1.4.CTB5 is the most commonly used data set for joint segmentation and POS marking in previous research. It consists of newswire data. We follow the traditional division of the data set as in Jiang et al. (2008); Kruengkrai et al. (2009a); Zhang and Clark (2010). CTB9 consists of source code in different genres, CTB5 is a subset of it. We split CTB9 by using the partition of CTB7 in Wang et al. (2011); we expand the training, development and testing sets of CTB5 by converting 80% of the new data into CTB9."}, {"heading": "4.2 Experimental Results", "text": "Both Segmentation (Seg) and Joint Segmentation and POS Marking (Seg & Tag) are evaluated in our experiments.4 We use Word-Level Recall (R), Precision (P) and F1-Score (F) as evaluation metrics. A series of feature experiments are performed on the development sets to evaluate the effectiveness of the proposed approaches for vector representations of the characters. Finally, the most powerful model according to the Feature Experiment is applied to the test sets in the form of single and ensemble pieces and downloaded with ZPar.4The evaluation script is available from: http: / / people.sutd.edu.sg / yue zhang / doc / doc / joint files / evaluate.py."}, {"heading": "4.2.1 Feature Experiments", "text": "The mentionlcihsrc\u00fcehsrcS rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the r"}, {"heading": "4.2.2 Final Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "4.3 Tagging Speed", "text": "Our common segmentation and POS tagger is very efficient for GPU devices and can be used practically for processing very large files. Memory requirements for decoding are drastically lower than for training, so a large batch size can be used. The tagger needs constant time to create partial billing diagrams and load weights.At bucket size of 10 and batch size of 500, Table 9 shows the tagging speed of the tagger with a single Tesla K80 GPU card and the pre-trained model on CTB5. For comparison, the tagging speed of ZPar is also presented. GPU devices are not supported by ZPar, so the tagging speed is calculated using an Intel Core i7 CPU."}, {"heading": "5 Related Work", "text": "Peng and Dredze (2016) adopt the model for Chinese segmentation and recognition of designated entities in the context of multi-task and multi-domain learning. Dong et al. (2016) instead use a BiLSTM-CRF character-level model that uses radical information for Chinese recognition of designated entities. Ma and Sun (2016) use a similar architecture, but feed Chinese characters in pairs as edge embeddings. Their model is based on chunking, segmentation, and POS tagging.Zheng et al. (2013) use common Chinese segmentation and POS tagging via predicting combinatory segmentation and POS tags. They use the adaptation of the neural network presented in Collobert et al, which was introduced in Tolobert et al languages. (2011), which extracts only local features in a context window."}, {"heading": "6 Conclusion", "text": "We adapt the BiRNN CRF model for sequence marking in NLP to the common Chinese segmentation and POS marking by predicting the combinatorial markers of word boundaries and POS markers. Chained N-grams and character characteristics are used, together with conventional pre-formed character embeddings, as vector representations for Chinese characters. Functional experiments suggest that concatenated Ngrams contribute significantly to this. However, both radicals and graphical characteristics are less effective as subcharacter information, and how to integrate the information more effectively at the subcharacter level will be investigated further in the future. The proposed model will be comprehensively evaluated on CTB5, CTB9 and UD-Chinese. Despite the fact that different character representation approaches are sensitive to data size and tagging schemes, we use a set of subcharacter parameters and universal feature test settings based on CTB9 and UD-Chinese. Despite the fact that our data sets react sensitively to data size and tagging schemes, we will use a set of subcharacter parameters and universal character settings based test settings so that our model is robust across the data sets that the data sets are very short."}, {"heading": "Acknowledgments", "text": "We acknowledge the computing resources provided by CSC in Helsinki and Sigma2 in Oslo through NeIC-NLPL (www.nlpl.eu), which is supported by the Chinese Scholarship Council (CSC) (No. 201407930015)."}], "references": [{"title": "TensorFlow: A system for largescale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606 .", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(August):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Characterbased LSTM-CRF with radical-level features for Chinese named entity recognition", "author": ["Chuanhai Dong", "Jiajun Zhang", "Chengqing Zong", "Masanori Hattori", "Hui Di."], "venue": "International Conference on Computer Processing of Oriental", "citeRegEx": "Dong et al\\.,? 2016", "shortCiteRegEx": "Dong et al\\.", "year": 2016}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of The 31st International Conference on Machine Learning", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Joint Chinese word segmen", "author": ["toshi Isahara"], "venue": null, "citeRegEx": "Isahara.,? \\Q2009\\E", "shortCiteRegEx": "Isahara.", "year": 2009}, {"title": "Finding function in form", "author": ["Tiago Luis"], "venue": null, "citeRegEx": "Luis.,? \\Q2015\\E", "shortCiteRegEx": "Luis.", "year": 2015}, {"title": "A new recurrent neural CRF for learning non-linear edge features", "author": ["Shuming Ma", "Xu Sun."], "venue": "arXiv preprint arXiv:1611.04233 .", "citeRegEx": "Ma and Sun.,? 2016", "shortCiteRegEx": "Ma and Sun.", "year": 2016}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, page 10641074.", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing", "author": ["Hwee Tou Ng", "Jin Kiat Low."], "venue": "Barcelona, Spain, pages 277\u2013", "citeRegEx": "Ng and Low.,? 2004", "shortCiteRegEx": "Ng and Low.", "year": 2004}, {"title": "Universal dependencies v1: A multilingual", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "Multi-task multi-domain representation learning for sequence tagging", "author": ["Nanyun Peng", "Mark Dredze."], "venue": "arXiv preprint arXiv:1608.02689 .", "citeRegEx": "Peng and Dredze.,? 2016", "shortCiteRegEx": "Peng and Dredze.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Chinese morphological analysis with character-level POS tagging", "author": ["Mo Shen", "Hongxiao Liu", "Daisuke Kawahara", "Sadao Kurohashi."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A stacked sub-wordmodel for joint Chinese word segmentation and part-of-speech tagging", "author": ["Weiwei Sun."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Associ-", "citeRegEx": "Sun.,? 2011", "shortCiteRegEx": "Sun.", "year": 2011}, {"title": "Radical-enhanced Chinese character embedding", "author": ["Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "International Conference", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data", "author": ["Yiou Wang", "Jun\u2019ichi Kazama", "Yoshimasa Tsuruoka", "Wenliang Chen", "Yujie Zhang", "Kentaro Torisawa"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "CHARAGRAM: Embedding words and sentences via character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "arXiv preprint arXiv:1607.02789 .", "citeRegEx": "Wieting et al\\.,? 2016", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "The Penn Chinese treebank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer."], "venue": "Natural language engineering 11(02):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Joint word segmentation and POS tagging using a single perceptron", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Columbus, Ohio, pages 888\u2013896.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "A fast decoder for joint word segmentation and POS-tagging using a single discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Zhang and Clark.,? 2010", "shortCiteRegEx": "Zhang and Clark.", "year": 2010}, {"title": "Deep learning for Chinese word segmentation and POS tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, USA, pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008).", "startOffset": 93, "endOffset": 134}, {"referenceID": 25, "context": "Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008).", "startOffset": 93, "endOffset": 134}, {"referenceID": 13, "context": "Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a joint model which predicts the combinatory labels of segmentation boundaries and POS tags at the character level.", "startOffset": 94, "endOffset": 364}, {"referenceID": 11, "context": ", 2001) as the output interface for sentence-level optimisation (BiRNN-CRF) achieves state-of-the-art accuracies on various sequence tagging tasks (Huang et al., 2015; Ma and Hovy, 2016) and outperforms the traditional linear statistical models.", "startOffset": 147, "endOffset": 186}, {"referenceID": 2, "context": "RNNs with gated recurrent cells, such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014) are capable of capturing long dependencies and retrieving rich global information.", "startOffset": 138, "endOffset": 156}, {"referenceID": 26, "context": "(Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "For the recurrent layer, we employ GRU as the basic recurrent unit as it has similar functionalities but fewer parameters compared to LSTM (Chung et al., 2014).", "startOffset": 139, "endOffset": 159}, {"referenceID": 18, "context": "Dropout (Srivastava et al., 2014) is applied to the outputs of the bidirectional recurrent layers.", "startOffset": 8, "endOffset": 33}, {"referenceID": 23, "context": "For instance, only the functional word \u7684 can be tagged as DEG in Chinese Treebank (Xue et al., 2005).", "startOffset": 82, "endOffset": 100}, {"referenceID": 16, "context": "We employ GloVe (Pennington et al., 2014) to train our character embeddings on Wikipedia and the freely available Sogou News Corpora (SogouCS).", "startOffset": 16, "endOffset": 41}, {"referenceID": 0, "context": "0 library (Abadi et al., 2016).", "startOffset": 10, "endOffset": 30}, {"referenceID": 7, "context": "Adagrad (Duchi et al., 2011) with mini-batches is employed for optimisation with the initial learning rate \u03b70 = 0.", "startOffset": 8, "endOffset": 28}, {"referenceID": 23, "context": "We employ three different datasets for our experiments, namely Chinese Treebank (Xue et al., 2005) 5.", "startOffset": 80, "endOffset": 98}, {"referenceID": 14, "context": "0 (CTB9) along with the Chinese section in Universal Dependencies (UD Chinese) (Nivre et al., 2016) of version 1.", "startOffset": 79, "endOffset": 99}, {"referenceID": 21, "context": "We split CTB9 by referring to the partition of CTB7 in Wang et al. (2011). We extend the training, development and test sets from CTB5 by adding 80% of the new data in CTB9 to training and 10% each to development and test.", "startOffset": 55, "endOffset": 74}, {"referenceID": 25, "context": "We retrained a ZPar model on CTB5 that reproduces the evaluation scores reported in Zhang and Clark (2010). We also modified the source code so that it is applicable to CTB9 and UD Chinese.", "startOffset": 84, "endOffset": 107}, {"referenceID": 24, "context": "Compared to CTB, the words in UD Chinese are more fine-grained and the average word length is shorter, which makes it easier for the tagger to correctly segment the OOV words as Zhang et al. (2016) show that the longer words are more difficult to be segmented correctly.", "startOffset": 178, "endOffset": 198}, {"referenceID": 17, "context": "18 Shen et al. (2014) 98.", "startOffset": 3, "endOffset": 22}, {"referenceID": 12, "context": "Peng and Dredze (2016) adopt the model for Chinese segmentation and named entity recognition in the context of multi-task and multi-domain learning.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Dong et al. (2016) employ a character level BiLSTM-CRF model that utilises radicallevel information for Chinese named entity recognition.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Dong et al. (2016) employ a character level BiLSTM-CRF model that utilises radicallevel information for Chinese named entity recognition. Ma and Sun (2016) use a similar architecture but feed the Chinese characters pairwise as edge embeddings instead.", "startOffset": 0, "endOffset": 156}, {"referenceID": 4, "context": "Dong et al. (2016) employ a character level BiLSTM-CRF model that utilises radicallevel information for Chinese named entity recognition. Ma and Sun (2016) use a similar architecture but feed the Chinese characters pairwise as edge embeddings instead. Their model is applied respectively to chunking, segmentation and POS tagging. Zheng et al. (2013) model joint Chinese segmentation and POS tagging via predicting the combinatory segmentation and POS tags.", "startOffset": 0, "endOffset": 351}, {"referenceID": 4, "context": "They employ the adaptation of the feed forward neural network introduced in Collobert et al. (2011) that only extracts local features in a context window.", "startOffset": 76, "endOffset": 100}, {"referenceID": 4, "context": "They employ the adaptation of the feed forward neural network introduced in Collobert et al. (2011) that only extracts local features in a context window. A perceptron-style training algorithm is employed for sentence level optimisation, which is the same as the training algorithm of the BiRNNCRF model. Their proposed model is not evaluated on CTB5 and therefore difficult to be compared with our system. Kong et al. (2015) apply segmental recurrent neural networks to joint segmentation and POS tagging but the evaluation results are substantially below the state-of-the-art on CTB5.", "startOffset": 76, "endOffset": 426}, {"referenceID": 12, "context": "The results show that radical-enhanced embeddings outperform both skip-ngram and continues bag-of-word (Mikolov et al., 2013) in word2vec.", "startOffset": 103, "endOffset": 125}], "year": 2017, "abstractText": "We present a character-based model for joint segmentation and POS tagging for Chinese. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and sub-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our model is accurate and robust across datasets in different sizes, genres and annotation schemes. We obtain stateof-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and POS tagging.", "creator": "LaTeX with hyperref package"}}}