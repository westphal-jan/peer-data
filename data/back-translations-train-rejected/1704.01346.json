{"id": "1704.01346", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity", "abstract": "We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02% with human annotations.", "histories": [["v1", "Wed, 5 Apr 2017 10:07:22 GMT  (23kb)", "http://arxiv.org/abs/1704.01346v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jeremy ferrero", "frederic agnes", "laurent besacier", "didier schwab"], "accepted": false, "id": "1704.01346"}, "pdf": {"name": "1704.01346.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Didier Schwab"], "emails": ["jeremy.ferrero@imag.fr", "frederic@compilatio.net", "laurent.besacier@imag.fr", "didier.schwab@imag.fr"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.01 346v 1 [cs.C L] 5A prWe present our submitted systems for Semantic Textual Similarity (STS) Track 4 on SemEval-2017. Given two Spanish-English sentences, each system must have its semantic similarity with a score between 0 and 5. In our paper we use syntax-based, dictionary-based, context-based and MT-based methods. We also combine these methods in an unsupervised and supervised way. Our best result is 1st place on Track 4a with a correlation of 83.02% to human annotations."}, {"heading": "1 Introduction", "text": "CompiLIG is a collaboration between Compilatio1 - a company with a particular interest in the detection of cross-lingual plagiarism - and the LIG Research Group on Natural Language Processing (GETALP). Semantic text similarity detection is an important step in the detection of cross-lingual plagiarism, and evaluation campaigns in this new area are rare. For the first time, the SemEval STS task (Agirre et al., 2016) has been expanded to include a Spanish-English sub-task. This year, the sub-task was renewed as part of Track 4 (divided into two sub-groups: Track 4a and Track 4b), with one sentence in Spanish and one sentence in English, the aim is to achieve semantic text similarity according to a score from 01 www.compatio.netto 5, where 0 means no similarity and 5 means full semantic similarity."}, {"heading": "2 Cross-Language Textual Similarity Detection Methods", "text": "2.1 Cross-Language Character N-Gram (CL-CnG) CL-CnG aims to measure the syntactical similarity between two texts. It is based on Mcnamee and Mayfield (2004) work used in gathering information. It compares two texts under their N-gram vector representation. The main advantage of this method is that it does not require translation between source and target text. After several tests on the previous data set to find the best n, we opt for the CL-C3G implementation of Potthast et al (2011). Let's leave Sx and Sy two sentences in two different languages. First, the alphabet of these sentences is standardized to the ensemble construct \u2211 = {a \u2212 z, 0 \u2212 9,} so that only spaces and alphanumeric characters are preserved. All other diacritical characters or symbols are deleted and the entire text is brought into a lower form."}, {"heading": "2.2 Cross-Language Conceptual Thesaurus-based Similarity (CL-CTS)", "text": "CL-CTS (Gupta et al., 2012; Pataki, 2012) aims to measure the semantic similarity between two vectors of concepts.The model consists in presenting texts as word bags (or concepts) in order to compare them. Also, the method does not require explicit translation, since the matching is performed with internal links in the used \"ontology.\" Let's create a set of words in length n, the n words of the definition are defined by wi as: S = {w1, w2, w3,..., wn} (1) Sx and Sy are two sentences in two different languages. A set of words S \u2032 from each sentence S is built by filtering stopwords and using a function that returns all of its possible translations for a given word. These translations are given collectively by a linked lexical resource, DBNary (Se \u00b2 rasset, 2015), and by transboundary word beds."}, {"heading": "2.3 Cross-Language Word Embedding-based Similarity", "text": "CL-WES (Ferrero et al., 2017) consists in a cosinal similarity of distributed sentence representations, which are determined by the weighted sum of each word vector in a sentence. As in the previous section, each word vector is syntactically and frequently weighted. If Sx and Sy are two sentences in two different languages, then CL-WES forms their (bilingual) common representation vectors Vx and Vy and applies a cosinal similarity between them. A distributed representation V of a sentence S results in the following: V = n \u2211 i = 1, wi \u0445 S (vector (wi)."}, {"heading": "2.4 Translation + Monolingual Word Alignment (T+WA)", "text": "The last method used is a two-step process. First, we translate the Spanish sentence into English using Google Translate (i.e. we translate the two sentences into the same language), then we align both utterances. We use the monolingual alignment 4 of Sultan et al. (2015) with the enhancement of Brychcin and Svoboda (2016), which won the cross-lingual task in 2016 (Agirre et al., 2016). Since this enhancement was not published by the original authors, we propose to share our re-implementation on GitHub5. If Sx and Sy are two sentences in the same language, then we try to measure their similarity with the following formula: J (Sx, Sy) = \u03c9 (Ay) \u2022 (Sx) \u2022 (Sy) \u2022 (Sy) \u2022 (Sy) 3 https: / / / / Silling.com / Silling.com function (Silling.com) \u2022 (Silling.com / Silling.com function \u2022 \u2022 \u2022 (Sx) \u2022 (Sy) \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 Silling.com \u2022 \u2022 (Silling.com) \u2022 (Silling.com / Silling.com function \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 (Silling.com)"}, {"heading": "2.5 System Combination", "text": "These methods are syntax-based, dictionary-based, context-based, and MT-based and therefore potentially complement each other. Therefore, we also combine them in an unattended and supervised manner. Our unattended fusion is an average of the results of each method. For supervised fusion, we revise fusion as a regression problem and experiment with all available methods in Weka 3.8.0 (Hall et al., 2009)."}, {"heading": "3 Results on SemEval-2016 Dataset", "text": "Table 1 reports on the results of the proposed systems on the STS lingual evaluation dataset SemEval-2016. The dataset, annotations and evaluation systems were presented in the STS task description SemEval-2016 (Agirre et al., 2016), so we will not go into further detail here. Bold lines represent the methods that achieve the best mean in each system category (best method alone, unattended and supervised fusion) and the values for the monitored systems are obtained by 10-fold cross-validation."}, {"heading": "4 Runs Submitted to SemEval-2017", "text": "First, it is important to mention that our results are recalculated linearly to a real evaluated space [0; 5]. Run 1: Best Method Alone. Our first run is based only on the best method during our tests (see Table 1), i.e. on the Cross-Language Conceptual Thesaurus-based Similarity (CL-CTS) model as described in Section 2.2.Run 2: Fusion by Average. Our second run is a fusion on average to three methods: CL-C3G, CL-CTS and T + WA, all described in Section 2.Run 3: M5 \"Model Tree. Unlike the two previous runs, the third run is a monitored system. We selected the system that scored the best in our SemEval 2016 tests (see Table 1), which is the M5 \u2032 m del tree model (Wang and Witten, 1997) (called M5P in Weka 3.8.0 (Hall et, a decision system)."}, {"heading": "5 Results of the 2017 evaluation and Discussion", "text": "Data set, annotations and evaluation systems are presented in SemEval-2017 STS task sheet (Agirre et al., 2017). We can see in Table 2 that our systems work well on SNLI6 (Bowman et al., 2015) (Track 4a), where we ranked first with more than 83% of the correlation with human annotations. Conversely, the correlations on the WMT corpus (Track 4b) are strangely low. This difference is remarkable in the results of all participating teams (Agirre et al., 2017) 7. This could be explained by the fact that the WMT corpus of 6 http: / / nlp.stanford.edu / projects / snli / 7The best value for this track is 34%, while for the other tracks it is about 85%. Only one annotator, while the SNLI corpus has been commented by many."}, {"heading": "6 Conclusion", "text": "We described our submission to the SemEval 2017 Semantic Textual Similarity Task on Track 4 (SpEn cross-lingual sub-task) and achieved our best results using an M5 \u2032 model tree combination of different text similarity recognition techniques. This approach worked for the SNLI corpus (4a - ranked 1st with more than 83% correlation with human annotations), which corresponds to a real-life cross-language plagiarism scenario. We also questioned the validity of the WMT corpus (4b) by submitting our own manual annotations and having low correlations with SemEvals."}], "references": [{"title": "SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "Janyce Wiebe."], "venue": "Proceedings of the 10th International", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "I\u00f1igo LopezGazpio", "Lucia Specia."], "venue": "Proceedings of the 11th InternationalWorkshop on Semantic", "citeRegEx": "Agirre et al\\.,? 2017", "shortCiteRegEx": "Agirre et al\\.", "year": 2017}, {"title": "MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP", "author": ["Alexandre Berard", "Christophe Servan", "Olivier Pietquin", "Laurent Besacier."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation", "citeRegEx": "Berard et al\\.,? 2016", "shortCiteRegEx": "Berard et al\\.", "year": 2016}, {"title": "CONDOR, a new parallel, constrained extension of Powell\u2019s UOBYQA algorithm: Experimental results and comparison with the Journal of Computational and Applied", "author": ["Frank Vanden Berghen", "Hugues Bersini"], "venue": null, "citeRegEx": "Berghen and Bersini.,? \\Q2005\\E", "shortCiteRegEx": "Berghen and Bersini.", "year": 2005}, {"title": "A Large Annotated Corpus for Learning Natural Language Inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "UWB at SemEval-2016 Task 1: Semantic textual similarity using lexical, syntactic, and semantic information", "author": ["Tomas Brychcin", "Lukas Svoboda."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016). San Diego, CA, USA, pages 588\u2013594.", "citeRegEx": "Brychcin and Svoboda.,? 2016", "shortCiteRegEx": "Brychcin and Svoboda.", "year": 2016}, {"title": "A Multilingual, Multi-style and Multi-granularity Dataset for Cross-language Textual Similarity Detection", "author": ["J\u00e9r\u00e9my Ferrero", "Fr\u00e9d\u00e9ric Agn\u00e8s", "Laurent Besacier", "Didier Schwab."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation", "citeRegEx": "Ferrero et al\\.,? 2016", "shortCiteRegEx": "Ferrero et al\\.", "year": 2016}, {"title": "Using Word Embedding for Cross-Language Plagiarism Detection", "author": ["J\u00e9r\u00e9my Ferrero", "Laurent Besacier", "Didier Schwab", "Fr\u00e9d\u00e9ric Agn\u00e8s."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Ferrero et al\\.,? 2017", "shortCiteRegEx": "Ferrero et al\\.", "year": 2017}, {"title": "DLS@CU: Sentence similarity from word alignment and semantic vector composition", "author": ["Md Arafat Sultan", "Steven Bethard", "Tamara Sumner."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Sultan et al\\.,? 2015", "shortCiteRegEx": "Sultan et al\\.", "year": 2015}, {"title": "Induction of model trees for predicting continuous classes", "author": ["Yong Wang", "Ian H. Witten."], "venue": "Proceedings of the poster papers of the European Conference on Machine Learning. Prague, Czech Republic, pages 128\u2013137.", "citeRegEx": "Wang and Witten.,? 1997", "shortCiteRegEx": "Wang and Witten.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "For the first time, SemEval STS task (Agirre et al., 2016) was extended with", "startOffset": 37, "endOffset": 58}, {"referenceID": 5, "context": "Last year, among 26 submissions from 10 teams, the method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervised system (SVM regression with RBF kernel) based on word alignment algorithm presented in Sultan et al.", "startOffset": 93, "endOffset": 121}, {"referenceID": 5, "context": "Last year, among 26 submissions from 10 teams, the method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervised system (SVM regression with RBF kernel) based on word alignment algorithm presented in Sultan et al. (2015).", "startOffset": 94, "endOffset": 247}, {"referenceID": 2, "context": "We use the MultiVec (Berard et al., 2016) toolkit for computing and managing word embeddings.", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "We use the MultiVec (Berard et al., 2016) toolkit for computing and managing word embeddings. The corpora used to build the embeddings are Europarl and Wikipedia sub-corpus, part of the dataset of Ferrero et al. (2016). For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.", "startOffset": 21, "endOffset": 219}, {"referenceID": 3, "context": "The 12 POS weights and the value \u03b1 are optimized with Condor (Berghen and Bersini, 2005) in the same", "startOffset": 61, "endOffset": 88}, {"referenceID": 6, "context": "way as in Ferrero et al. (2017). Condor applies a Newton's method with a trust region algorithm to determinate the weights that optimize a desired output score.", "startOffset": 10, "endOffset": 32}, {"referenceID": 7, "context": "CL-WES (Ferrero et al., 2017) consists in a cosine similarity on distributed representations of sentences, which are obtained by the weighted sum of each word vector in a sentence.", "startOffset": 7, "endOffset": 29}, {"referenceID": 2, "context": "We make this method publicly available through MultiVec (Berard et al., 2016) toolkit.", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": "(2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al., 2016).", "startOffset": 103, "endOffset": 124}, {"referenceID": 5, "context": "We reuse the monolingual aligner of Sultan et al. (2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 3, "context": "(2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al.", "startOffset": 31, "endOffset": 59}, {"referenceID": 0, "context": "The dataset, the annotation and the evaluation systems were presented in the SemEval-2016 STS task description paper (Agirre et al., 2016), so we do not re-detail them here.", "startOffset": 117, "endOffset": 138}, {"referenceID": 9, "context": "We have selected the system that obtained the best score during our tests on SemEval-2016 evaluation dataset (see Table 1), which is the M5 m del tree (Wang and Witten, 1997) (called", "startOffset": 151, "endOffset": 174}, {"referenceID": 9, "context": "The first implementation of model trees, M5, was proposed by Quinlan (1992) and the approach was refined and improved in a system called M5 by Wang and Witten (1997). To learn the model, we use all the methods described in section 2 as features.", "startOffset": 143, "endOffset": 166}, {"referenceID": 1, "context": "Dataset, annotation and evaluation systems are presented in SemEval-2017 STS task description paper (Agirre et al., 2017).", "startOffset": 100, "endOffset": 121}, {"referenceID": 4, "context": "We can see in Table 2 that our systems work well on SNLI (Bowman et al., 2015) (track 4a), on which we ranked 1 with more than 83% of correlation with human annotations.", "startOffset": 57, "endOffset": 78}, {"referenceID": 1, "context": "difference is notable on the scores of all participating teams (Agirre et al., 2017).", "startOffset": 63, "endOffset": 84}], "year": 2017, "abstractText": "We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of SpanishEnglish sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1 on track 4a with a correlation of 83.02% with human annotations.", "creator": "LaTeX with hyperref package"}}}