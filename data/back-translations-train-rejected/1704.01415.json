{"id": "1704.01415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "Multi-Label Learning with Global and Local Label Correlation", "abstract": "It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missing-label cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data.", "histories": [["v1", "Tue, 4 Apr 2017 12:50:25 GMT  (413kb,D)", "http://arxiv.org/abs/1704.01415v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["yue zhu", "james t kwok", "zhi-hua zhou"], "accepted": false, "id": "1704.01415"}, "pdf": {"name": "1704.01415.pdf", "metadata": {"source": "CRF", "title": "Multi-Label Learning with Global and Local Label Correlation", "authors": ["Yue Zhu", "James T. Kwok", "Zhi-Hua Zhou"], "emails": ["zhouzh}@lamda.nju.edu.cn", "jamesk@cse.ust.hk"], "sections": [{"heading": null, "text": "Existing approaches either assume that the label correlations are global and shared by all instances, or that the label correlations are local and shared only by a collection of data. In the real world, in both cases, some label correlations may be globally applicable and some may be shared only by a local group of instances. In addition, it is also a common case that only partial labels are observed, making the exploitation of label correlations much more difficult. That is, it is difficult to estimate the label correlations when many labels are missing. In this paper, we propose a new multi-label GLOCAL approach that addresses both full label and mislabel cases, taking advantage of global and local label correlations simultaneously by learning latent label representation and optimizing label diversity."}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of us are able to survive on our own by going in search of our own identity. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...)"}, {"heading": "2. Related Work", "text": "Most studies focus on global correlations. Based on the degree of label correlations used, it can be divided into three categories: (i) first order; (ii) second order; and (iii) high order. However, for the second order, label correlations are not taken into account, and the label problem is converted into several independent label categories. For example, BR [3] creates a classifier for each label independently; for the second order, pairwise label relationships are taken into account. CLR [7] transforms the label problem into the Pacific label ranking problem; for the high order strategy, all other labels imposed on each label are labeled. CC [15] transforms the label-label problem."}, {"heading": "3. The Proposed Approach", "text": "In multi-level learning, one instance can be associated with multiple class names. Let C = {c1,..., cl} be the class caption set of l markups. We designate the characteristic vector of an instance with x-X Rd and designate the basic truth caption vector with y-Y markups, where [y] j = 1 if x is with class name cj, and \u2212 1 otherwise. As mentioned in Section 1, instances in the training data can be partially labeled, i.e., some labels may be missing. We adopt the general setting that both positive and negative labels may be missing [8, 21, 22]. The observed label vector is designated with y, where [y] j = 0 if class label cj is not labeled (i.e. it is missing), and [y] j = [y] j = [y] otherwise. Given the training data D = {(xi, i} ni = y, our goal is to learn [j = y], if it is a class label [i.e.] y = y]."}, {"heading": "3.1. Basic Model", "text": "Let Y = [y-1,. \u2212 n], y-n], as discussed in Section 1, let Y value be low. Let its value be k < l. Thus, Y value can be written as low-grade decay UV, with U value Rl \u00b7 k and V value Rk \u00b7 n. Intuitively, V represents the latent names, which are more compact and semantically abstract than the original names, while matrix U projects the original names onto the latent label space. Generally, the names are only partially observed. Let the observed label matrix Y = [y1,.., yn] contain the labels {\u2212 1, 0, 1} l \u00b7 n, and the labels X-value."}, {"heading": "3.2. Global and Local Manifold Regularizers", "text": "It is an essential component in multilateral education. (F > 0). (F)..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.3. Learning by Alternating Minimization", "text": "Problem (4) can be solved by alternating minimization (algorithm 1). In each iteration, we update one of the variables in {Z, U, V, W} with gradient descent and leave the others fixed. Specifically, the MANOPT toolbox [2] is used to implement gradient descent with line search in the Euclidean space for updating U, V, W and on the manifolds for updating Z."}, {"heading": "3.3.1. Updating Zm", "text": "Fixed with U, V, W reduces problem (4) Tomin Zm \u03bb3nm ntr (F > 0 ZmZ > mF0) + \u03bb4tr (F > mZmZ > mFm) s.t. diag (ZmZ > m) = 1, (5) for each m \u00b2 {1,.., g}. Due to the constraint diag (ZmZ > m) = 1 there is no solution in closed form, and we will solve it with projected gradient. Zm is: Zm = \u03bb3nm n UW > XX > WU > Zm + \u03bb4UW > XmX > mWU > Zm.In order to satisfy the constraint diag (ZmZ > m) = 1, we project each line Zm onto the standard sphere after each update: zm, j \u2190 zm, j / \u06444UW > XmX > Zmj > Zmj, to satisfy the constraint."}, {"heading": "3.3.2. Updating V", "text": "When Zm's and U, W are fixed, the problem (4) of Tomin V-J-J-J-Y (Y-UV), 2F + \u03bb-V-W > X-2F + \u03bb2-V-2F decreases. (6) Note that each column of V is independent of each other, and therefore V can be solved column by column. Leave ji and vi with the column of J and V. The optimization problem for vi can be written as follows: min vi-Diag (ji) yi-Diag (ji) Uvi-2 + \u03bb vi-W > xi-2 + \u03bb2 \u0445 vi-2.If we set the gradient vi to 0, we get the following closed solution of vi: vi = (U > Diag (ji) U + (\u03bb + \u03bb2) I) \u2212 1 (E > W > xi + U > Diag (ji) yi).This includes the calculation of a matrix inverse for each iv. If this is expensive, we can use the V instead."}, {"heading": "3.3.3. Updating U", "text": "When Zm's and V, W are fixed, problem (4) Tomin reduces U-J-T (Y-UV), 2F + \u03bb2, U-2F + g-m = 1 (F > 0 ZmZ > mF0) + \u03bb4tr (F > mZmZ > mFm) (7) Again, we use gradient descend, and the gradient w.r.t. U is: \"U = (J\" (UV \u2212 Y)), V > + \u03bb2U + g-m = 1ZiZ > i U (B > 3nm n W > XmX > mW + \u03bb4W > XX > W)."}, {"heading": "3.3.4. Updating W", "text": "When Zm's and U, V are fixed, Problem (4) decreases tomin values (W > 0 ZmZ > mF0) + \u03bb4tr (F > mZmZ > mFm) (8) The gradient w.r.t. W is: \"W\" = \"X\" (X > W \u2212 V >) + \"W + g\" m = 1 (V3nm n XX > + V4XmX > m) WU > ZmZ > mU."}, {"heading": "4. Experiments", "text": "In this section, extensive experiments with text and image data sets will be conducted, and the performance of both complete and missing cases will be discussed."}, {"heading": "4.1. Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. Data sets", "text": "On text, eleven Yahoo datasets (Arts, Business, Computers, Education, Entertainment, Health, Leisure, References, Science, Social and Society) and the Enron dataset 2 are used. On images, the Corel5k3 and Image3 datasets are used. Subsequently, each dataset is identified by its first three letters. 4 Detailed information on the datasets is given in Table 1. For each dataset, we randomly select 60% of the instances for training and the rest for testing."}, {"heading": "4.1.2. Baselines", "text": "In the GLOCAL algorithm, we use the kmeans clustering algorithm to divide the data into local groups. Eqn. (1) \"s solution is used to heat U, V and W. Zm's are randomly initialized. GLOCAL is compared to the following state-of-the-art multi-label learning algorithms: 1http: / / www.kecl.ntt.co.jp / as / members / ueda / yahoo.tar 2http: / mulan.sourceforge.net / datasets-mlc.html 3http: / / cse.seu.edu.cn / people / zhangml / files / Image.rar 4\" Society \"is referred to as\" Soci \"to distinguish it from\" Social. \"1. BR [3], which uses a binary linear LDC package (using the LIBLINEAR package [6])."}, {"heading": "4.1.3. Performance Evaluation", "text": "Let p be the number of test cases, C + i, C \u2212 i are the sets of positive and negative labels associated with the ith instance, and Z + j, Z \u2212 j are the sets of positive and negative instances associated with the jth label. input x indicates that rankf (x, y) is the rank of the label y in the predicted label ranking (sorted in descending order). For performance evaluation, we use the following popular indicators in multi-label learning [24]: 1. Ranking loss (Rkl): This is the fraction that a negative label is rated higher than a positive label."}, {"heading": "4.2. Learning with Full Labels", "text": "In this experiment, all elements of the labelling matrix are observed, but the performance of the test data is shown in Table 2. As expected, BR is worst because it treats each label independently, without taking into account labelling correlations. MLLOC only takes into account local labelling correlations and LEML only uses the low structure. Although ML-LRC uses both the low structure and labelling correlations, only global labelling correlations are taken into account. As a result, GLOCAL is the best overall as it models both global and local labelling correlations. To show the sample correlations learned from GLOCAL, we use two local groups extracted from the image data. Figure 1 shows that the local labelling correlation varies from group to group and differs from the global correlation. For group 1, \"sunset\" is highly correlated with \"desert\" and \"sea.\" (Figure 1 (This can be seen from Figure 1) (a)."}, {"heading": "4.3. Learning with Missing Labels", "text": "In this experiment, sample \u03c1% of the elements in the label matrix as observed, the rest as missing. Note that BR and MLLOC can only process datasets with complete labels. Therefore, we first use MAXIDE [21], a matrix completion algorithm for transductive multi-label learning, to fill in the missing labels before they can be applied. We use MBR for MAXIDE + BR and MMLLOC for MAXIDE + MLLOC. Tables 4 and 5 show the results on the training and test data in a respectable manner. 5As can be seen, performance increases with more closely observed entries in general. Overall, GLOCAL performs best on different labels, considering both global and local label correlations with diverse regulation. In contrast, MBR and MLLOC perform label recovery and learning separately. Furthermore, MLLOC does not take local label correlations as well as local label relations, although MLOCBR and MLLOC are not often correlated to them."}, {"heading": "4.4. Convergence", "text": "In this section we will empirically examine the convergence of GLOCAL. Figure 2 shows the objective value w.r.t. The number of iterations for the complete label case. Due to the lack of space, the results are displayed only on the Arts, Business, Enron and Image datasets. As you can see, the target converges quickly in some iterations. A similar phenomenon can also be observed on the other datasets. Table 6 shows the time results when learning with missing labels (with \u03c1 = 70). GLOCAL and LEML form a classifier for all labels together and can also take advantage of the low structure of the model or label matrix during the training, so that they are fastest. However, in order to adjust the tables on one page, we do not report the standard deviation. MBR, which performs worst, is also not shown."}, {"heading": "4.5. Sensitivity to Parameters", "text": "In this experiment, we examine the influence of parameters, including the number of clusters g, the regularization parameters \u03bb3 and \u03bb4 (corresponding to the manifold regularizer for global and local label correlations), the regularization parameter \u03bb2 for the Frobenius norm regularizer, and the dimensionality k of the latent representation. We vary one parameter while fixing the others in their best settings."}, {"heading": "4.5.1. Varying the Number of Clusters g", "text": "Figure 3 shows the impact on the Enron dataset. If there is only one cluster, no local label correlation is taken into account. If there are more clusters, performance improves as more local label correlations are taken into account. If too many clusters are used, very few instances are placed in each cluster, and the local label correlations cannot be reliably estimated."}, {"heading": "4.5.2. Influence of Label Manifold Regularizers (\u03bb3 and \u03bb4)", "text": "Figures 4 and 5 show their effects on the Enron dataset. At \u03bb3 = 0, only local label correlations are taken into account, and the performance is poor. As \u03bb3 increases, the performance improves. However, with a very high \u03bb3, the performance deteriorates with the dominance of the global label correlations. A similar phenomenon can be observed for \u03bb4."}, {"heading": "4.5.3. Varying the Latent Representation Dimensionality k", "text": "Figure 6 shows the impact of different k on the Enron dataset. If k is too small, the latent representation cannot capture enough information. As k increases, performance improves. If k is too large, the low structure is not fully utilized and performance begins to deteriorate."}, {"heading": "4.5.4. Influence of \u03bb2", "text": "Figure 7 shows the effects of a variation of \u03bb2 on the Enron dataset. As you can see, GLOCAL is not sensitive to this parameter."}, {"heading": "5. Conclusion", "text": "In this paper, we have proposed a new approach that learns correlations between multiple labels while simultaneously restoring the missing labels, training the classifier and exploiting both global and local label correlations by learning a latent label representation and optimizing label complexity. Compared to the previous work, GLOCAL is the first company to take advantage of both global and local label correlations, learning the Lapland matrix directly without requiring any other knowledge of label correlations. As a result, label editions and label correlations match best globally and locally. In addition, GLOCAL offers a unified solution for both full label learning and multi-label relational learning without labels. Experimental results show that our approach can be state-of-the-art multi-mark learning approaches to learning both complete labels and missing labels. In our case, we treat the labels as symmetrical in many situations."}, {"heading": "Acknowledgment", "text": "This research was supported by NSFC (61333014), 111 Project (B14020) and the Collaborative Innovation Center of Novel Software Technology and Industrialization."}], "references": [{"title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Manopt, a Matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning multi-label scene classification", "author": ["M. Boutell", "J. Luo", "X. Shen", "C. Brown"], "venue": "Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Network-based classification of breast cancer metastasis", "author": ["H.-Y. Chuang", "E. Lee", "Y.-T. Liu", "D. Lee", "T. Ideker"], "venue": "Molecular Systems Biology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Spectral graph theory, volume 92", "author": ["F. Chung"], "venue": "American Mathematical Soc.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Multilabel classification via calibrated label ranking", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "E. Men\u0107\u0131a", "K. Brinker"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Transduction with matrix completion: Three birds with one stone", "author": ["A. Goldberg", "B. Recht", "J. Xu", "R. Nowak", "X. Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Multi-label learning by exploiting label correlations locally", "author": ["S.-J. Huang", "Z.-H. Zhou"], "venue": "In Proceedings of the 26th AAAI Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Extracting shared subspace for multi-label classification", "author": ["S. Ji", "L. Tang", "S. Yu", "J. Ye"], "venue": "In Proceedings of the 14th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Non-negative laplacian embedding", "author": ["D. Luo", "C. Ding", "H. Huang", "T. Li"], "venue": "In Proceedings of the 9th IEEE International Conference on Data Mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Laplacian Support Vector Machines Trained in the Primal", "author": ["S. Melacci", "M. Belkin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Submodular multi-label learning", "author": ["J. Petterson", "T. Caetano"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Automatically learning document taxonomies for hierarchical classification", "author": ["K. Punera", "S. Rajan", "J. Ghosh"], "venue": "Proceedings of the 14th International Conference on World Wide Web,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Classifier chains for multi-label classification", "author": ["J. Read", "B. Pfahringer", "G. Holmes", "E. Frank"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles", "author": ["A. Subramanian", "P. Tamayo", "V. Mootha", "S.M.B. Ebert", "M. Gillette", "A. Paulovich", "S. Pomeroy", "T. Golub", "E. Lander"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Semantic annotation and retrieval of music and sound effects", "author": ["D. Turnbull", "L. Barrington", "D. Torres", "C. Lanckriet"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Parametric mixture models for multi-labeled text", "author": ["N. Ueda", "K. Saito"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Image annotation using multi-label correlated green\u2019s function", "author": ["H. Wang", "H. Huang", "C. Ding"], "venue": "In Proceedings of the 12th International Conference on Computer", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Learning low-rank label correlations for multi-label classification with missing labels", "author": ["L. Xu", "Z. Wang", "Z. Shen", "Y. Wang", "E. Chen"], "venue": "In Proceedings of the 14th IEEE International Conference on Data Mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "author": ["M. Xu", "R. Jin", "Z.-H. Zhou"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Large-scale multi-label learning with missing labels", "author": ["H.-F. Yu", "P. Jain", "P. Kar", "I. Dhillon"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Multi-label learning by exploiting label dependency", "author": ["M.-L. Zhang", "K. Zhang"], "venue": "In Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A review on multi-label learning algorithms", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "For example, a scene image can be annotated with several tags [3], a document may", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "belong to multiple topics [18], and a piece of music may be associated with different genres [17].", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "belong to multiple topics [18], and a piece of music may be associated with different genres [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "Thus, multi-label learning has attracted a lot of attention in recent years [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Current studies on multi-label learning try to incorporate label correlations of different orders [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "However, existing approaches mostly focus on global label correlations shared by all instances [7, 10, 15].", "startOffset": 95, "endOffset": 106}, {"referenceID": 9, "context": "However, existing approaches mostly focus on global label correlations shared by all instances [7, 10, 15].", "startOffset": 95, "endOffset": 106}, {"referenceID": 14, "context": "However, existing approaches mostly focus on global label correlations shared by all instances [7, 10, 15].", "startOffset": 95, "endOffset": 106}, {"referenceID": 8, "context": "On the other hand, certain label correlations are only shared by a local data subset [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 13, "context": "Some approaches learn the label hierarchies by hierarchical clustering [14] or Bayesian network structure learning [23].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Some approaches learn the label hierarchies by hierarchical clustering [14] or Bayesian network structure learning [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "Others estimate label correlations by the co-occurrence of labels in training data [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "[21] and Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] considered using the low-rank structure on the instance-label mapping.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "A more direct approach to model the label dependency approximates the label matrix as a product of two low-rank matrices [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 23, "context": "Based on the degree of label correlations used, it can be divided into three categories [24]: (i) first-order; (ii) second-order; and (iii) high-order.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "For example, BR [3] trains a classifier for each label independently.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "For example, CLR [7] transforms the multi-label learning problem into the pairwise label ranking problem.", "startOffset": 17, "endOffset": 20}, {"referenceID": 14, "context": "For example, CC [15] transforms the multi-label learning problem into a chain of binary classification problems, with the ground-truth labels encoded into the features.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "However, MLLOC [9] demonstrates that sometimes label correlations may only be shared by a local data subset.", "startOffset": 15, "endOffset": 18}, {"referenceID": 20, "context": "MAXIDE [21] is based on fast low-rank matrix completion, and has strong theoretical guarantees.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "LEML [22] also relies on a low-rank structure, and works in an inductive setting.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "ML-LRC [20] adopts a low-rank structure to capture global label correlations, and addresses the missing labels by introducing a supplementary label matrix.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "Manifold regularization [1] exploits instance similarity by forcing the predicted values on similar instances to be similar.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "We adopt the general setting that both positive and negative labels can be missing [8, 21, 22].", "startOffset": 83, "endOffset": 94}, {"referenceID": 20, "context": "We adopt the general setting that both positive and negative labels can be missing [8, 21, 22].", "startOffset": 83, "endOffset": 94}, {"referenceID": 21, "context": "We adopt the general setting that both positive and negative labels can be missing [8, 21, 22].", "startOffset": 83, "endOffset": 94}, {"referenceID": 11, "context": "The manifold regularizer \u2211 i,j Sij\u2016fi,:\u2212fj,:\u20162 should have a small value [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "The manifold regularizer can be equivalently written as tr(F> 0 L0F0) [11], where L0 = D0\u2212S0 is the Laplacian matrix of S0.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": ", gene pathways [16] and networks [4] in bioinformatics applications) or clustering.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": ", gene pathways [16] and networks [4] in bioinformatics applications) or clustering.", "startOffset": 34, "endOffset": 37}, {"referenceID": 18, "context": "In multi-label learning, one rudimentary approach is to compute the correlation coefficient between two labels by cosine distance [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "This constraint also enables us to obtain a normalized Laplacian matrix [5] of Lm.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "Specifically, the MANOPT toolbox [2] is utilized to implement gradient descent with line search on the Euclidean space for the update of U ,V ,W , and on the manifolds for the update of Z.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "BR [3], which trains a binary linear SVM (using the LIBLINEAR package [6]) for each label independently; 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "BR [3], which trains a binary linear SVM (using the LIBLINEAR package [6]) for each label independently; 2.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "MLLOC [9], which exploits local label correlations by encoding them into the instance\u2019s feature representation; 3.", "startOffset": 6, "endOffset": 9}, {"referenceID": 21, "context": "LEML [22], which learns a linear instance-to-label mapping with low-rank structure, and implicitly takes advantage of global label correlation; 4.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "ML-LRC [20], which learns and exploits low-rank global label correlations for multi-label classification with missing labels.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "For performance evaluation, we use the following popular metrics in multi-label learning [24]:", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "we first use MAXIDE [21], a matrix completion algorithm for transductive multi-label learning, to fill in the missing labels before they can be applied.", "startOffset": 20, "endOffset": 24}], "year": 2017, "abstractText": "It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missinglabel cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data.", "creator": "TeX"}}}