{"id": "1704.01419", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Linear Ensembles of Word Embedding Models", "abstract": "This paper explores linear methods for combining several word embedding models into an ensemble. We construct the combined models using an iterative method based on either ordinary least squares regression or the solution to the orthogonal Procrustes problem.", "histories": [["v1", "Wed, 5 Apr 2017 13:38:01 GMT  (126kb,D)", "http://arxiv.org/abs/1704.01419v1", "Nodalida 2017"]], "COMMENTS": "Nodalida 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["avo murom\\\"agi", "kairit sirts", "sven laur"], "accepted": false, "id": "1704.01419"}, "pdf": {"name": "1704.01419.pdf", "metadata": {"source": "CRF", "title": "Linear Ensembles of Word Embedding Models", "authors": ["Avo Murom\u00e4gi", "Kairit Sirts", "Sven Laur"], "emails": ["avom@ut.ee", "kairit.sirts@ut.ee", "swen@math.ut.ee"], "sections": [{"heading": null, "text": "We evaluate the proposed approaches in Estonian - a morphologically complex language for which the available corpora for embedding words is relatively small. We compare both combined models with each other and with the input models for embedding words using synonyms and analogy tests. Results show that regression using the ordinary smallest squares performs poorly in our experiments, while the use of orthogonal procrusts to combine several word embedding models in an ensemble model results in 7-10% relative improvements compared to the average results of the initial models in synonym tests and 19-47% in analogy tests."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Combining word embedding models", "text": "A word embedding model is a matrix W, R | V | \u00b7 d, where | V | is the number of words in the model lexicon and d is the dimensionality of the vectors. Each line in the matrix W is the continuous representation of a word in a vector space. Given the r embedding of models W1,..., Wr, we want to combine them to a target model Y. We define a linear objective function that is the sum of the linear regression optimization goals: J = r, i = 1, Y \u2212 WiPi, 2, (1) where P1,..., Pr are transformation matrices containing W1,.., Wr or in the common vector space containing Y. We use an iterative algorithm to find matrices P1,.., Pr and Y."}, {"heading": "2.1 Solution with the ordinary least squares (SOLS)", "text": "The analytical solution to a linear regression problem Y = PW to find the transformation matrix P, given the input data matrix W and the result Y: P = (W TW) \u2212 1W TY (4) With this formula, we can update all matrices Pi at each iteration. The problem with this approach is that Y is also unknown and is updated repeatedly in the second step of the iterative algorithm, which could lead the OLS to solutions where both WiPi and Y are optimized toward 0, which is not a sensible solution. To counteract this effect, we scale Y anew at the beginning of each iteration. This is done by scaling the elements of Y so that the variance of each column of Y is equal to 1."}, {"heading": "2.2 Solution to the Orthogonal Procrustes problem (SOPP)", "text": "The orthogonal procrust is a linear regression problem in the transformation of the input matrix W into the output matrix Y using an orthogonal transformation matrix P. The orthogonal constraint is called PPT = PT P = IThe solution of the orthogonal procrusts can be calculated analytically using the singular value decomposition (SVD). First calculation method: S = W TYThen it is diagnosed using SVD: ST S = V DSV T SST = UDSUTLast calculation method: P = UV TThis method must be performed for each Pi during each iteration. This approach is very similar to SOLS. The only difference is the additional orthogonal constraint, which gives this method a potential advantage, as in the translated word embeddings WiPi the lengths of the vectors and the angles between the vectors. In addition, we no longer have to take care of the solution PY, where everything is PY."}, {"heading": "3 Experiments", "text": "We tested both methods on a number of Word2Vec models (Mikolov et al., 2013b) trained on the Estonian Reference Corpus.1 Estonian Reference1http: / / www.cl.ut.ee / korpused / segakorpusCorpus is the largest text corpus available in Estonia. Its size is about 240 million word marks, which may seem a lot, but compared with, for example, the English Gigaword corpus, which is commonly used to train word embeddings for English words and contains more than 4B words, it is quite small. All models were trained using a window size 10 and the skip program architecture. We experimented with models of 6 different embedding sizes: 50, 100, 150, 200, 250 and 300. For each dimensionality we had 10 models available. The number of different words in each model is 816757. During the training, the iterative algorithm was achieved so that the constants were extended to 0.0001."}, {"heading": "4 Results", "text": "We evaluate the quality of the combined models on the basis of synonym and analogy tests."}, {"heading": "4.1 Synonym ranks", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2 Analogy tests", "text": "A famous and typical example of an analogy question is \"a man is to a king as a woman is to a?\" The correct answer to this question is \"a queen.\" For an analogy tuple a: b, x: y (a is to b as x is to y) is expected to be held in an embedding room: wb \u2212 wa + wx, where the vectors w are embedded word. For the above example with \"man,\" \"woman\" and \"queen\" this is called: wking \u2212 wman + wwoman, the vector representations for the three words in the analogy question - wa, wb and wx - the goal is to maximize."}, {"heading": "5 Analysis", "text": "In order to better understand how the words in the combined model space are compared to the original models, we performed two additional analyses. First, we calculated the distribution of the average square errors of the words to see how the translated embeddings are distributed around the word embeddings of the combined model. Second, we investigated how both methods affect the paired similarities of the words."}, {"heading": "5.1 Distribution of mean squared distances", "text": "We have calculated the square Euclidean distance for each word in the vocabulary between the combined model Y and all embedded models; the distance ei j for a jte word and the ith input model is: di j = Yj \u2212 Ti j 2, where Ti = WiPi is the ith translated embedding model; then we found the mean square distance for the jte word by calculation: d j = 1 rr \u2211 i = 0 di jThese distances are plotted on Figure 2; the words on the horizontal axis are ordered by their frequency - the most common words that come first. We show these results for models with 100 dimensions, but the results with other embedded texts were similar. Note that the distances for rarer words are similarly small for both SOLS and SOPP methods; however, the distribution of the distances for frequent words is quite different - while the distances for both methods increase punctuality."}, {"heading": "5.2 Word pair similarities", "text": "In this analysis, we investigated how the cosinal similarities between word pairs in the combined model change compared to their similarities in the input models. We randomly selected a total of 1000 word pairs from the vocabulary. For each word pair, we calculated the following values: \u2022 cosinal similarities under the combined model; \u2022 maximum and minimum cosinal similarities in the source models Wi; \u2022 mean cosinal similarities over the output models Wi.The results are plotted in the ascending order of their similarities in the combined model Y. These results are obtained using word embedding sizes of size 100, using different embedding sizes that reveal the same patterns. In the figures, the word pairs on the horizontal axis are ordered in the ascending order of their similarities in the combined model Y. The figures show that 1) the words that are similar in the source models Wi are even more similar than in the combined model Y; and 2) distant words in the initial models are classified in the SOO-PP, although these are explained by the SOO-PP and SPP trends."}, {"heading": "6 Discussion and future work", "text": "Of the two linear methods used to combine the models, SOPP consistently performed better in both bed tests. Although, as shown in Figures 2 and 3, the word beddings of the aligned starting models were closer to the embeddings of the SOLS combination model, this seemingly better fit is at the expense of distorting the relationships between the individual words bedbeds. Thus, we have demonstrated that adding the orthogonal condition to the goal of linear transformation is important to maintain the quality of the translated word embeddings, both in the context of producing model sets and in other contexts in which one embedding space may be relevant, such as working with semantic time series or multilingual embeddings."}, {"heading": "7 Conclusions", "text": "Although model ensembles have often been used to improve the results of various natural language processing tasks, the ensembles of word embedding models have rarely been studied to date. Our main contribution in this paper was to combine several word embedding models trained on the same dataset into an overall ensemble using linear transformation and to experimentally demonstrate the usefulness of this approach. We experimented with two linear methods to combine the input embedding models - the usual solution to the linear regression problem and the orthogonal procrustes, which adds an additional orthogonal constraint to the objective function of the smallest squares. Experiments with synonym and analogy tests on Estonia showed that the combination with orthogonal Procrustes models was consistently better than the ordinary smallest squares, meaning that the preservation of the distances and angles between vectors and orthogonal constraints is crucial for the model combination."}, {"heading": "Acknowledgments", "text": "We thank Alexander Tkachenko for providing the prepared input models and analogy test questions, as well as the anonymous reviewers for their helpful suggestions and comments."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Diachronic word embeddings reveal statistical laws of semantic change", "author": ["Jure Leskovec", "Dan Jurafsky"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Hamilton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Statistically significant detection of linguistic change", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Scott Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bilingual word embeddings from parallel and non-parallel corpora for crosslanguage text classification", "author": ["Mogadala", "Rettinger2016] Aditya Mogadala", "Achim Rettinger"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter", "citeRegEx": "Mogadala et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mogadala et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "A generalized solution of the orthogonal procrustes problem", "author": ["Peter H. Sch\u00f6nemann"], "venue": null, "citeRegEx": "Sch\u00f6nemann.,? \\Q1966\\E", "shortCiteRegEx": "Sch\u00f6nemann.", "year": 1966}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Learning Word Meta-Embeddings", "author": ["Yin", "Sch\u00fctze2016] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "Various methods have been proposed to train word embeddings from unannoted text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Al-Rfou et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014), most well-known of them being perhaps Word2Vec (Mikolov et al.", "startOffset": 88, "endOffset": 204}, {"referenceID": 0, "context": "Various methods have been proposed to train word embeddings from unannoted text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Al-Rfou et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014), most well-known of them being perhaps Word2Vec (Mikolov et al.", "startOffset": 88, "endOffset": 204}, {"referenceID": 11, "context": "Various methods have been proposed to train word embeddings from unannoted text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Al-Rfou et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014), most well-known of them being perhaps Word2Vec (Mikolov et al.", "startOffset": 88, "endOffset": 204}, {"referenceID": 10, "context": "First of them is based on the standard ordinary least squares solution (OLS) for linear regression, the second uses the solution to the orthogonal Procrustes problem (OPP) (Sch\u00f6nemann, 1966), which essentially also solves the OLS but adds the orthogonality constraint that keeps the angles between vectors and their distances unchanged.", "startOffset": 172, "endOffset": 190}, {"referenceID": 2, "context": "Mikolov et al. (2013a) optimized the linear regression with stochastic gradient descent to learn linear transformations between the embeddings in two languages for machine translation.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Mikolov et al. (2013a) optimized the linear regression with stochastic gradient descent to learn linear transformations between the embeddings in two languages for machine translation. Mogadala and Rettinger (2016) used OPP to translate embeddings between two languages to perform cross-lingual document classification.", "startOffset": 0, "endOffset": 215}, {"referenceID": 1, "context": "Hamilton et al. (2016) aligned a series of embedding models with OPP to detect changes in word meanings over time.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Hamilton et al. (2016) aligned a series of embedding models with OPP to detect changes in word meanings over time. The same problem was addressed by Kulkarni et al. (2015) who aligned the embedding models using piecewise linear regression based on a set of nearest neighboring words for each word.", "startOffset": 0, "endOffset": 172}, {"referenceID": 10, "context": "The first is based on the standard least squares solution to the linear regression problem, the second method is known as solution to the Orthogonal Procrustes problem (Sch\u00f6nemann, 1966).", "startOffset": 168, "endOffset": 186}, {"referenceID": 10, "context": "Orthogonal Procrustes is a linear regression problem of transforming the input matrix W to the output matrix Y using an orthogonal transformation matrix P (Sch\u00f6nemann, 1966).", "startOffset": 155, "endOffset": 173}, {"referenceID": 9, "context": "ments (Schnabel et al., 2015).", "startOffset": 6, "endOffset": 29}, {"referenceID": 8, "context": ", 2013b), Glove (Pennington et al., 2014) and dependency based vectors (Levy and Goldberg, 2014) could lead to a model that combines the strengths of all the input models.", "startOffset": 16, "endOffset": 41}, {"referenceID": 4, "context": "As all word embedding systems learn slightly different embeddings, combining for instance Word2Vec (Mikolov et al., 2013b), Glove (Pennington et al., 2014) and dependency based vectors (Levy and Goldberg, 2014) could lead to a model that combines the strengths of all the input models. Yin and Sch\u00fctze (2016) demonstrated that the combination of different word embeddings can be useful.", "startOffset": 100, "endOffset": 309}], "year": 2017, "abstractText": "This paper explores linear methods for combining several word embedding models into an ensemble. We construct the combined models using an iterative method based on either ordinary least squares regression or the solution to the orthogonal Procrustes problem. We evaluate the proposed approaches on Estonian\u2014a morphologically complex language, for which the available corpora for training word embeddings are relatively small. We compare both combined models with each other and with the input word embedding models using synonym and analogy tests. The results show that while using the ordinary least squares regression performs poorly in our experiments, using orthogonal Procrustes to combine several word embedding models into an ensemble model leads to 7-10% relative improvements over the mean result of the initial models in synonym tests and 19-47% in analogy tests.", "creator": "LaTeX with hyperref package"}}}