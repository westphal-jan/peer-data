{"id": "1704.01444", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Learning to Generate Reviews and Discovering Sentiment", "abstract": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.", "histories": [["v1", "Wed, 5 Apr 2017 14:20:28 GMT  (445kb,D)", "http://arxiv.org/abs/1704.01444v1", null], ["v2", "Thu, 6 Apr 2017 09:48:20 GMT  (446kb,D)", "http://arxiv.org/abs/1704.01444v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["alec radford", "rafal jozefowicz", "ilya sutskever"], "accepted": false, "id": "1704.01444"}, "pdf": {"name": "1704.01444.pdf", "metadata": {"source": "META", "title": "Learning to Generate Reviews and Discovering Sentiment", "authors": ["Alec Radford", "Rafal Jozefowicz", "Ilya Sutskever"], "emails": ["<alec@openai.com>."], "sections": [{"heading": "1. Introduction and Motivating Work", "text": "In fact, most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2. Dataset", "text": "As discussed in Jozefowicz et al. (2016), the performance of these datasets is mainly determined by regularization. Interested in high-quality mood representations, we chose the Amazon product rating data set introduced in McAuley et al. (2015) as our training corpus. In deduplicated form, this data set contains over 82 million product ratings from May 1996 to July 2014 totaling over 38 billion training bytes. Due to the size of the data set, we initially divided it into 1000 shards with the same number of ratings and laid back 1 shards for validation and 1 shards for testing."}, {"heading": "3. Model and Training Details", "text": "Many potentially recurrent architectures and hyperparameter settings have been taken into account in preliminary experiments on the dataset. Given the size of the dataset, searching the wide range of possible configurations is quite costly. To facilitate this, we evaluated the generative performance of smaller candidate models after a single pass through the dataset. The model selected for the large experiment is a single-layer multiplicative LSTM (Krause et al., 2016) with 4096 units. We observed multiplicative LSTMs to come together faster than normal LSTMs in the hyperparameter settings studied both in terms of data and the time of the wall clock. The model was trained for a single epoch on mini-batches with 128 partial sequences of length 256 for a total of 1 million weight updates. States were initialized to zero at the beginning of each shard and persisted across updates to simulate full backward propagation and allow the dissemination of information outside of a partial sequence."}, {"heading": "4. Experimental Setup and Results", "text": "Our model processes text as a sequence of UTF-8 encoded bytes (Yergeau, 2003). For each byte, the model updates its hidden state and predicts a probability distribution over the next possible byte. The hidden state of the model serves as an online summary of the sequence that encodes all the information that the model has learned, and that is relevant for predicting the future bytes of the sequence. We are interested in understanding the properties of the learned encoding. \u2022 The process of extracting a feature representation is outlined as follows: \u2022 Since line breaks are used as verification separators in the training data set, all line breaks are replaced by spaces to avoid the model reset state. \u2022 Leading line breaks are removed and replaced by a line break + space to simulate a start token."}, {"heading": "4.1. Review Sentiment Analysis", "text": "Table 1 shows the results of our model on 4 standard text classification datasets; the performance of our model is clearly one-sided; on the MR datasets (Pang & Lee, 2005) and CR datasets (Hu & Liu, 2004), we are improving the state of the art by a significant duplicate; the MR datasets and CR datasets are extracted from Rotten Tomatoes, a film evaluation website, and Amazon product reviews (which almost certainly overlap with our training corpus), suggesting that our model has learned a rich representation of text from a similar domain; on the other two datasets, the subjectivity / objectivity of detection by subjectivity (Pang & Lee, 2004) and opinion polarity (Wiebe et al, 2005) our model has no discernible advantage over other unmonitored representation approaches."}, {"heading": "4.2. Sentiment Unit", "text": "We performed further analyses to understand what representations our model has learned and how they achieve the effectiveness of the data. The benefit of an L1 penalty in the low data regime (see Figure 2) is a clue. L1 regularization is known to reduce the complexity of the sample when there are many irrelevant characteristics (Ng, 2004), which is likely to be the case for our model since it is trained as a language model and not as a supervised trait extractor. By examining the relative contributions of the characteristics on different datasets, we discovered a single unit within the mLSTM that directly corresponds to the mood. In Figure 3, we show the histogram of the final activations of this unit after processing IMDB ratings (Maas et al., 2011), which shows a bimodal distribution with a clear separation between positive and negative ratings. In Figure 4, we visualize the activations of this unit on 6 randomly selected evaluations from a 100-relevant series of evaluations, showing that this unit represents a high-contrasting 99.1% of local assessments, showing that this unit represents a single 0.1% of mood, with a 99.1% of local assessments."}, {"heading": "4.3. Capacity Ceiling", "text": "Encouraged by these results, we were curious to see how well the representation of the model scales to larger datasets. We are trying our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in Zhang et al. (2015) This dataset contains 598,000 ratings, which is an order of magnitude greater than any other dataset we have tested. In visualizing performance as a function of the number of training examples in Figure 5, we observe a \"capacity limit,\" where the testing accuracy of our model improves by just over 1% in an order of magnitude of increase in training data. Using the full dataset, we achieve a test accuracy of 95.22%. This is better than a BoW TFIDF baseline at 93.66%, but slightly worse than the 95.64% of a linear classifier at the top of the 500,000 most common n-grams to length. The observed capacity limit is an interesting phenomenon and a stumbling point for the scaling of our gut."}, {"heading": "4.4. Other Tasks", "text": "In addition to the classification, we also evaluate two other standard tasks: semantic kinship and paraphrase detection. While our model is competitive in Microsoft Research Paraphrase Corpus (Dolan et al., 2004) in Table 3, it performs poorly in the semantic kinship task SICK (Marelli et al., 2014) in Table 4. It is likely that the form and content of the semantic kinship task, which is based on descriptions of images and videos and contains phrases such as \"A sea turtle hunts fish,\" is effectively out of bounds for our model, which was trained only on the text of product reviews."}, {"heading": "4.5. Generative Analysis", "text": "Although the focus of our analysis was on the characteristics of the representation of our model, it is designed as a generative model and we are also interested in its generative abilities. We confirmed that the mood unit is not only used for internal calculations, but also strongly influences the generative process itself. In Table 5 we show that the model generates corresponding positive or negative ratings by simply setting the mood unit to positive or negative. It is interesting to see that such simple manipulation of the representation of the model has a strong influence on its behavior."}, {"heading": "5. Discussion and Future Work", "text": "It is an open question why our model restores the concept of sensation in such a precise, untangled, interpretable, and manipulable way. It is possible that sentiment as a conditioning feature has strong predictive power for speech modeling, which is likely because sentiment is such an important component of verification. Previous work to analyze LSTM language models showed the existence of interpretable units that indicate the position within a line or presence within a quotation (Karpathy et al., 2015). In many ways, the mood unit in this model is only a scaled example of the same phenomenon. The actualization equation of an LSTM could play a role. Elementary operation of its gates can foster axis-oriented representations. Models such as word2vec have also been observed to train small subsets of dimensions that are strongly linked to specific tasks (Li et al., 2016). Our work highlights the sensitivity of perceptual representations of data distribution on which they are trained."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["Blei", "David M", "Ng", "Andrew Y", "Jordan", "Michael I"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Topicrnn: A recurrent neural network with long-range semantic dependency", "author": ["Dieng", "Adji B", "Wang", "Chong", "Gao", "Jianfeng", "Paisley", "John"], "venue": "arXiv preprint arXiv:1611.01702,", "citeRegEx": "Dieng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieng et al\\.", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Hill", "Felix", "Cho", "Kyunghyun", "Korhonen", "Anna"], "venue": "arXiv preprint arXiv:1602.03483,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Minqing", "Liu", "Bing"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["Huang", "Fu Jie", "Boureau", "Y-Lan", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "URL http://prize", "author": ["Hutter", "Marcus. The human knowledge compression contest."], "venue": "hutter1. net, 2006.", "citeRegEx": "Hutter and contest.,? 2006", "shortCiteRegEx": "Hutter and contest.", "year": 2006}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Multiplicative lstm for sequence modelling", "author": ["Krause", "Ben", "Lu", "Liang", "Murray", "Iain", "Renals", "Steve"], "venue": "arXiv preprint arXiv:1609.07959,", "citeRegEx": "Krause et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Su", "Jonathan", "Bradbury", "James", "English", "Robert", "Pierce", "Brian", "Ondruska", "Peter", "Gulrajani", "Ishaan", "Socher", "Richard"], "venue": "CoRR, abs/1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Understanding neural networks through representation erasure", "author": ["Li", "Jiwei", "Monroe", "Will", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1612.08220,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep learning with dynamic computation graphs", "author": ["Looks", "Moshe", "Herreshoff", "Marcello", "Hutchins", "DeLesley", "Norvig", "Peter"], "venue": "arXiv preprint arXiv:1702.02181,", "citeRegEx": "Looks et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Looks et al\\.", "year": 2017}, {"title": "Reexamining machine translation metrics for paraphrase identification", "author": ["Madnani", "Nitin", "Tetreault", "Joel", "Chodorow", "Martin"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entail", "author": ["Marelli", "Marco", "Bentivogli", "Luisa", "Baroni", "Bernardi", "Raffaella", "Menini", "Stefano", "Zamparelli", "Roberto"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Inferring networks of substitutable and complementary products", "author": ["McAuley", "Julian", "Pandey", "Rahul", "Leskovec", "Jure"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "McAuley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Mesnil", "Gr\u00e9goire", "Mikolov", "Tomas", "Ranzato", "Marc\u2019Aurelio", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Adversarial training methods for semi-supervised text classification", "author": ["Miyato", "Takeru", "Dai", "Andrew M", "Goodfellow", "Ian"], "venue": "arXiv preprint arXiv:1605.07725,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Munkhdalai", "Tsendsuren", "Yu", "Hong"], "venue": "arXiv preprint arXiv:1607.04315,", "citeRegEx": "Munkhdalai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai et al\\.", "year": 2016}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Wiebe", "Janyce", "Wilson", "Theresa", "Cardie", "Claire"], "venue": "Language resources and evaluation,", "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "arXiv preprint arXiv:1511.08198,", "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Utf-8, a transformation format of iso 10646", "author": ["Yergeau", "Francois"], "venue": null, "citeRegEx": "Yergeau and Francois.,? \\Q2003\\E", "shortCiteRegEx": "Yergeau and Francois.", "year": 2003}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Characterlevel convolutional networks for text classification", "author": ["Zhang", "Xiang", "Zhao", "Junbo", "LeCun", "Yann"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Selfadaptive hierarchical sentence model", "author": ["Zhao", "Han", "Lu", "Zhengdong", "Poupart", "Pascal"], "venue": "arXiv preprint arXiv:1504.05070,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Representation learning (Bengio et al., 2013) plays a critical role in many modern machine learning systems.", "startOffset": 24, "endOffset": 45}, {"referenceID": 15, "context": "The supervised training of high-capacity models on large labeled datasets is critical to the recent success of deep learning techniques for a wide range of applications such as image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 198, "endOffset": 223}, {"referenceID": 7, "context": "Much of the early research into modern deep learning was developed and validated via this approach (Hinton & Salakhutdinov, 2006) (Huang et al., 2007) (Vincent et al.", "startOffset": 130, "endOffset": 150}, {"referenceID": 32, "context": ", 2007) (Vincent et al., 2008).", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "We refer readers to Bengio et al. (2013) for a detailed review.", "startOffset": 20, "endOffset": 41}, {"referenceID": 28, "context": "These representations, learned by modeling word co-occurrences, increase the data efficiency and generalization capability of NLP systems (Pennington et al., 2014).", "startOffset": 138, "endOffset": 163}, {"referenceID": 1, "context": "Topic modelling can also discover factors within a corpus of text which align to human interpretable concepts such as art or education (Blei et al., 2003).", "startOffset": 135, "endOffset": 154}, {"referenceID": 13, "context": "Inspired by the success of word vectors, Kiros et al. (2015) propose skipthought vectors, a method of training a sentence encoder by predicting the preceding and following sentence.", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "Combining language modelling with topic modelling and fitting a small supervised feature extractor on top has also achieved strong results on in-domain document level sentiment analysis (Dieng et al., 2016).", "startOffset": 186, "endOffset": 206}, {"referenceID": 4, "context": "Hill et al. (2016) also raises concern about current evaluation tasks in their recent work which provides a thorough survey of architectures and objectives for learning unsupervised sentence representations - including the above mentioned skip-thoughts.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "Much previous work on language modeling has evaluated on relatively small but competitive datasets such as Penn Treebank (Marcus et al., 1993) and Hutter Prize Wikipedia (Hutter, 2006).", "startOffset": 121, "endOffset": 142}, {"referenceID": 9, "context": "As discussed in Jozefowicz et al. (2016) performance on these datasets is primarily dominated by regularization.", "startOffset": 16, "endOffset": 41}, {"referenceID": 9, "context": "As discussed in Jozefowicz et al. (2016) performance on these datasets is primarily dominated by regularization. Since we are interested in high-quality sentiment representations, we chose the Amazon product review dataset introduced in McAuley et al. (2015) as a training corpus.", "startOffset": 16, "endOffset": 259}, {"referenceID": 14, "context": "The model chosen for the large scale experiment is a single layer multiplicative LSTM (Krause et al., 2016) with 4096 units.", "startOffset": 86, "endOffset": 107}, {"referenceID": 30, "context": "RNTN (Socher et al., 2013)", "startOffset": 5, "endOffset": 26}, {"referenceID": 16, "context": "CNN (Kim, 2014) DMN (Kumar et al., 2015) LSTM (Wieting", "startOffset": 20, "endOffset": 40}, {"referenceID": 13, "context": "We follow the methodology established in Kiros et al. (2015) by training a logistic regression classifier on top of our model\u2019s representation on datasets for tasks including semantic relatedness, text classification, and paraphrase detection.", "startOffset": 41, "endOffset": 61}, {"referenceID": 30, "context": "The Stanford Sentiment Treebank (SST) (Socher et al., 2013) was created specifically to evaluate more complex compositional models of language.", "startOffset": 38, "endOffset": 59}, {"referenceID": 18, "context": "2% by a 30 model ensemble (Looks et al., 2017).", "startOffset": 26, "endOffset": 46}, {"referenceID": 24, "context": "09% (Miyato et al., 2016).", "startOffset": 4, "endOffset": 25}, {"referenceID": 37, "context": "We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in Zhang et al. (2015). This dataset contains 598,000 examples which is an order of magnitude larger than any other datasets we tested on.", "startOffset": 97, "endOffset": 117}, {"referenceID": 21, "context": ", 2004) in Table 3, it performs poorly on the SICK semantic relatedness task (Marelli et al., 2014) in Table 4.", "startOffset": 77, "endOffset": 99}, {"referenceID": 10, "context": "Previous work analysing LSTM language models showed the existence of interpretable units that indicate position within a line or presence inside a quotation (Karpathy et al., 2015).", "startOffset": 157, "endOffset": 180}, {"referenceID": 17, "context": "Models such as word2vec have also been observed to have small subsets of dimensions strongly associated with specific tasks (Li et al., 2016).", "startOffset": 124, "endOffset": 141}], "year": 2017, "abstractText": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.", "creator": "LaTeX with hyperref package"}}}