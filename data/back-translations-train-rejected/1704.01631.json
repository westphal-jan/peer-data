{"id": "1704.01631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition", "abstract": "End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set.", "histories": [["v1", "Wed, 5 Apr 2017 19:44:23 GMT  (109kb,D)", "http://arxiv.org/abs/1704.01631v1", null], ["v2", "Wed, 19 Apr 2017 16:01:53 GMT  (109kb,D)", "http://arxiv.org/abs/1704.01631v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shubham toshniwal", "hao tang", "liang lu", "karen livescu"], "accepted": false, "id": "1704.01631"}, "pdf": {"name": "1704.01631.pdf", "metadata": {"source": "CRF", "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition", "authors": ["Shubham Toshniwal", "Hao Tang", "Liang Lu", "Karen Livescu"], "emails": ["klivescu}@ttic.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Related Work", "text": "Its application to deep neural networks has been successful in a variety of situations in speech and speech processing [22, 23, 24, 25, 26, 27]. Most previous work combines several losses applied at the last output level of the model, such as common Mandarin letter and phonetic recognition in [26] and common CTC and attention-based training for English ASR [25.] Our work differs from this previous work in that our losses relate to different types of monitoring and are applied to different levels of the model. To our knowledge, the idea of applying low-threshold monitoring at lower levels was first introduced by S\u00f8gaard & Goldberg [28] for natural speech processing tasks and has since been extended by [29]. The work closest to us is the approach of Rao and Sak [30] using phoneme labels to form a multi-accent CTC-based ASR system in a multi-task environment."}, {"heading": "3.1. Baseline Model", "text": "The model is based on the attention encoder decoder RNNs = 2. (1) (1) (1) (1). The speech encoder reads in acoustic properties x = (1). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 The output of a sequence of high-level properties (hidden states) h, which the character decoder does not display in the figure when generating the output character sequence y = (y1,., yK), as in Figure 1 (the attention mechanism and a pyramid-shaped LSTM layer are not shown in the figure).3.1.1. The speech encoder is a deep pyramid-shaped bidirectional memory [31] (BiLSTM). In the first layer, a BiLSTM reads in acoustic properties x and outputs h (1) = (h)."}, {"heading": "3.2. Low-Level Auxiliary Tasks", "text": "As shown in Figure 1, we are exploring several types of auxiliary tasks in our multitaskic approach. We are investigating two types of auxiliary tools for multitaskable learning processes: phonemes and subphonetic states, in which the phoneme-based task is learned at the lowest level of the encoder, while the phoneme transcription losses can be learned at a higher level. We are looking at two types of phoneme transcription losses: phoneme decoder loss: Similar to the character decoder described above, we can add a phoneme decoder to the speech encoder as well. The phoneme decoder has exactly the same mathematical form as the character decoder, but with a phoneme label vocabulary attached to the decoder to the decoder."}, {"heading": "4.1. Model Details and Inference", "text": "We use 256 hidden units in each direction of each layer. The decoder for all tasks is a single-layer LSTM with 256 hidden units. We represent the output symbols of the decoders (both characters and, during training, phonemes) with 256-dimensional embedding vectors. At test time, we use a greedy decoder (beam size = more than the input sequence, due to the sequences of phonemes lasting less than 4 frames each. Anecdotally, these examples seem to correspond to incorrect orientation 1) to generate the character sequence. The character with the maximum posterior probability is selected at each time step and fed into the next time step as input. The decoder stops after the appearance of the \"EOS\" symbol (at the end of a set). We do not use explicit 0.36-language models with a transfer rate of 1MB."}, {"heading": "4.2. Results", "text": "We evaluate performance on the basis of the Word Error Rate (WER). We report on the results of the combined Eval2000 test set and separately on the SWB and CHE subsets. We also report on the character error rates (CER) in the development set.The results of the development set are in Table 1. We refer to the base model as \"Enc-Dec\" and the models with multitask training as \"Enc-Dec + [Auxiliary Task] - [Layer].\" Adding phoneme detection as an auxiliary task on layer 3, either with a separate LSTM decoder with CTC, reduces both the character error rates and the final word error rates. To determine whether the improved performance is a fundamental multitask training effect or specific to the application of the loss at the low level, we compare these results with those of adding the phoneme decoder on the top layer (Enc-Dec + Phonec-Model 4) leading to the application of the worst layer definition of the loss of the phoneme."}, {"heading": "5. Conclusion", "text": "The results from Switchboard and CallHome show consistent improvements over oversight-based base models and support the hypothesis that lower-level oversight is more effective when applied at lower levels of the depth model. We have compared several types of auxiliary tasks, with the best performance achieved by a combination of phoneme decoder and frame-level state loss. Analyses of model training and performance suggest that the addition of auxiliary tasks can either contribute to optimization or generalization. Future work will include studying a broader range of auxiliary tasks and model configurations. For example, it would be interesting to study even deeper models and word-level output, which would allow more options for intermediate tasks and placement of auxiliary losses."}, {"heading": "6. Acknowledgements", "text": "We are grateful to William Chan for helpful discussions and the language group at TTIC, in particular Shane Settle, Herman Kamper, Qingming Tang, and Bowen Shi for sharing their data processing code. This research was supported by a Google Faculty Research Award. 7. References [1] M. Gales and S. Young, \"The application of hidden markov models sin speech recognition,\" Foundations and trends in signal processing, vol. 1, 2008. [2] K. Vesely, A. Ghoshal, L. Burget, and D. Povey, \"Sequence discriminative training of deep neural networks,\" in Interspeech, 2013. [3] D. Povey and B. Kingsbury \"Evaluation of proposed modifications to large scale discriminative training,\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2007. [4] W. Chan, N. Jaitly, Q."}], "references": [{"title": "The application of hidden markov models in speech recognition", "author": ["M. Gales", "S. Young"], "venue": "Foundations and trends in signal processing, vol. 1, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequencediscriminative training of deep neural networks.", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "in Interspeech,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Evaluation of proposed modifications to mpe for large scale discriminative training", "author": ["D. Povey", "B. Kingsbury"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["L. Lu", "X. Zhang", "S. Renals"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng"], "venue": "North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL HLT), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Advances in allneural speech recognition", "author": ["G. Zweig", "C. Yu", "J. Droppo", "A. Stolcke"], "venue": "CoRR, vol. abs/1609.05935, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "European Conference on Computer Vision (ECCV), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G.E. Hinton"], "venue": "Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["M. Johnson", "M. Schuster", "Q.V. Le", "M. Krikun", "Y. Wu", "Z. Chen", "N. Thorat", "F.B. Vi\u00e9gas", "M. Wattenberg", "G. Corrado", "M. Hughes", "J. Dean"], "venue": "CoRR, vol. abs/1611.04558, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Understanding how deep belief networks perform acoustic modelling", "author": ["A.-r. Mohamed", "G.E. Hinton", "G. Penn"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "On the role of nonlinear transformations in deep neural network acoustic models", "author": ["T. Nagamine", "M.L. Seltzer", "N. Mesgarani"], "venue": "Interspeech, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Neural Information Processing Systems (NIPS), 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research (JMLR), 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Multitask sequence to sequence learning", "author": ["M. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "International Conference on Learning Representations (ICLR), 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to parse and translate improves neural machine translation", "author": ["A. Eriguchi", "Y. Tsuruoka", "K. Cho"], "venue": "CoRR, vol. abs/1702.03525, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Joint CTC-attention based end-to-end speech recognition using multi-task learning", "author": ["S. Kim", "T. Hori", "S. Watanabe"], "venue": "CoRR, vol. abs/1609.06773, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "On online attention-based speech recognition and joint Mandarin character-Pinyin training", "author": ["W. Chan", "I. Lane"], "venue": "Interspeech, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["A. S\u00f8gaard", "Y. Goldberg"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL), 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "A joint many-task model: Growing a neural network for multiple NLP tasks", "author": ["K. Hashimoto", "C. Xiong", "Y. Tsuruoka", "R. Socher"], "venue": "CoRR, vol. abs/1611.01587, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-accent speech recognition with hierarchical grapheme based models", "author": ["K. Rao", "H. Sak"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, 1997.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR, vol. abs/1409.2329, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez"], "venue": "International Conference on Machine Learning (ICML), 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1992.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1992}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL), 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR), vol. 12, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "International Conference on Frontiers in Handwriting Recognition (ICFHR), 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For example, traditional ASR systems include components like frame classifiers, phonetic acoustic models, lexicons (which may or may not be learned from data), and language models [1].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "Typically, such approaches involve training initial separate modules, followed by joint fine-tuning using sequence-level losses [2, 3].", "startOffset": 128, "endOffset": 134}, {"referenceID": 2, "context": "Typically, such approaches involve training initial separate modules, followed by joint fine-tuning using sequence-level losses [2, 3].", "startOffset": 128, "endOffset": 134}, {"referenceID": 3, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 4, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 5, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 6, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 7, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 160, "endOffset": 166}, {"referenceID": 8, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 160, "endOffset": 166}, {"referenceID": 3, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 67, "endOffset": 77}, {"referenceID": 8, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 67, "endOffset": 77}, {"referenceID": 9, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 67, "endOffset": 77}, {"referenceID": 10, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 103, "endOffset": 115}, {"referenceID": 11, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 103, "endOffset": 115}, {"referenceID": 13, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 60, "endOffset": 68}, {"referenceID": 14, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 60, "endOffset": 68}, {"referenceID": 15, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 96, "endOffset": 104}, {"referenceID": 16, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 96, "endOffset": 104}, {"referenceID": 17, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 7, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 18, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 3, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 6, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 20, "context": "Multitask training has been studied extensively in the machine learning literature [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 22, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 23, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 24, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 25, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 26, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 25, "context": "Most prior work combines multiple losses applied at the final output layer of the model, such as joint Mandarin character and phonetic recognition in [26] and joint CTC and attention-based training for English ASR [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 24, "context": "Most prior work combines multiple losses applied at the final output layer of the model, such as joint Mandarin character and phonetic recognition in [26] and joint CTC and attention-based training for English ASR [25].", "startOffset": 214, "endOffset": 218}, {"referenceID": 27, "context": "The idea of using low-level supervision at lower levels was, to our knowledge, first introduced by S\u00f8gaard & Goldberg [28] for natural language processing tasks, and has since been extended by [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "The idea of using low-level supervision at lower levels was, to our knowledge, first introduced by S\u00f8gaard & Goldberg [28] for natural language processing tasks, and has since been extended by [29].", "startOffset": 193, "endOffset": 197}, {"referenceID": 29, "context": "The closest work to ours is the approach of Rao and Sak [30] using phoneme labels for training a multi-accent CTC-based ASR system in a multitask setting.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "The dotted line in the character decoder denotes the use of (sampled) model predictions [20] during training (for the phone decoder only the ground-truth prior phone is used in training).", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "The model is based on attention-enabled encoder-decoder RNNs, proposed by [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "The speech encoder is a deep pyramidal bidirectional Long ShortTerm Memory [31] (BiLSTM) network [4].", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "The speech encoder is a deep pyramidal bidirectional Long ShortTerm Memory [31] (BiLSTM) network [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Following [4], we use pyramidal layers to reduces the time resolution of the final state sequence h by a factor of 2 = 8.", "startOffset": 10, "endOffset": 13}, {"referenceID": 32, "context": "CTC Loss: A CTC [33] output layer can also be added to various layers of the speech encoder [30].", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "CTC Loss: A CTC [33] output layer can also be added to various layers of the speech encoder [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "We use the Switchboard corpus (LDC97S62) [34], which contains roughly 300 hours of conversational telephone speech, as our training set.", "startOffset": 41, "endOffset": 45}, {"referenceID": 34, "context": "We use the importance sampling technique described in [35] to reduce this cost.", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "We train all models using Adam [36] with a minibatch size of 64 utterances.", "startOffset": 31, "endOffset": 35}, {"referenceID": 36, "context": "To further control overfitting we: (a) use dropout [37] at a rate of 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "1 on the output of all LSTM layers (b) sample the previous step\u2019s prediction [20] in the character decoder, with a constant probability of 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "1 as in [4].", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "[7] Enc-Dec 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] CTC 38.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Iterated CTC 24.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Our baseline model has better performance than the most similar previous encoder-decoder result [7].", "startOffset": 96, "endOffset": 99}], "year": 2017, "abstractText": "End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set.", "creator": "LaTeX with hyperref package"}}}