{"id": "1704.01770", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2017", "title": "Enabling Smart Data: Noise filtering in Big Data classification", "abstract": "In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.", "histories": [["v1", "Thu, 6 Apr 2017 10:06:52 GMT  (68kb,D)", "https://arxiv.org/abs/1704.01770v1", null], ["v2", "Fri, 28 Jul 2017 11:39:16 GMT  (838kb,D)", "http://arxiv.org/abs/1704.01770v2", null]], "reviews": [], "SUBJECTS": "cs.DB cs.LG", "authors": ["diego garc\\'ia-gil", "juli\\'an luengo", "salvador garc\\'ia", "francisco herrera"], "accepted": false, "id": "1704.01770"}, "pdf": {"name": "1704.01770.pdf", "metadata": {"source": "CRF", "title": "Enabling Smart Data: Noise filtering in Big Data classification", "authors": ["Diego Gar\u0107\u0131a-Gila", "Juli\u00e1n Luengo", "Salvador Gar\u0107\u0131a", "Francisco Herrera"], "emails": ["djgarcia@decsai.ugr.es", "julianlm@decsai.ugr.es", "salvagl@decsai.ugr.es", "herrera@decsai.ugr.es"], "sections": [{"heading": null, "text": "A common problem affecting data quality is the presence of noise, especially in classification problems where label noise refers to the incorrect labeling of training instances and is known to be a very disruptive feature of data. However, in this big data age, the massive growth in data size poses a challenge to traditional anti-noise proposals because they have difficulty handling such a large amount of data. New algorithms need to be proposed to address noise in big data problems, with high-quality and clean data, also known as smart data, being provided. In this paper, two big data pre-processing approaches are proposed to efficiently remove loud examples: homogeneous interaction and a heterogeneous ensemble filter, with particular emphasis on scalability and performance characteristics. The results obtained show that these proposals allow the user to perform smart data classification, smart data classification, smart data classification, smart keywords, smart symbols, smart data classification, smart symbols."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related work", "text": "In this section, we will first present the problem of noise in classification tasks in Section 2.1, then, in Section 2.2, we will present the MapReduce framework commonly used in big data solutions, and finally, in Section 2.3, we will give an insight into smart data."}, {"heading": "2.1. Class noise vs. attribute noise", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "2.2. Big Data. MapReduce and Apache Spark", "text": "The globalization of the big data paradigm is generating a big reaction in terms of technologies that have to deal with the fast-growing rates of data generated [45]. Of all of them, MapReduce is the ground-breaking framework designed by Google in 2003 [9]. The map phase leads to a transformation of the data, and the reduction phase leads to a summary operation. In short, the master node splits the input data and distributes it to the cluster. Then the map transformation is applied to each key value pair in the local data, and the reduction phase leads to a summary operation."}, {"heading": "2.3. From Big Data to Smart Data", "text": "As is the case in any knowledge process, huge amounts of data are analyzed, processed and interpreted in a structured way to generate gains in terms of economic or societal benefits. The first step in this transformation is to implement an integration process that brings together semantics and domains from several major sources. Using ontologies to support integration is a current approach [10, 8], but graph databases are also an option where data is stored in a relative form, such as in health and health."}, {"heading": "3. Towards Smart Data: Noise filtering for Big Data", "text": "In this section, we present the Big Data Framework under Apache Spark to eliminate noisy examples based on the MapReduce paradigm, which demonstrates its ability to withstand major problems in the real world. It is a MapReduce design in which all noise filtering processes are performed in a distributed manner. In Section 3.1, we describe the Spark primitives used to implement the framework. We have developed two algorithms based on ensembles, both of which perform a k-fold on the training data, learn a model for the training part, and clean noisy instances in the test partition. The first is a homogeneous ensemble using Random Forest, Logistic Regression, and NN-based on the simple instances of the ENN (we also have a filterogeneous ensemble based on the use of three different classifiers)."}, {"heading": "3.1. Spark Primitives", "text": "For the implementation of the framework we have used some basic Spark primitives from Spark API. These primitives offer much more complex operations by extending the MapReduce paradigm \u2190. Here we outline the more relevant for the algorithms 2: \u2022 map: Applies a transformation to each element of an RDD. \u2022 Once the operation has been performed on each element, the resulting RDD is returned. \u2022 zipWithIndex: for each element of an RDD, a pair consisting of the element and its index is created, starting at 0. \u2022 join: Return a RDD with all pairs of elements with matching keys between two RDDs. \u2022 filter: Return a new RDD with only those elements that satisfy a predicate. \u2022 union: Return a RDD of pairs as a result of merging two RDDs. \u2022 kFold: Return a RDD: Return a RDD with all pairs of elements with matching keys between two RDDs."}, {"heading": "3.2. Homogeneous Ensemble: HME-BD", "text": "The homogeneous ensemble is inspired by Cross-Validated Committees Filter (CVCF) [44], which removes this filter by partitioning the data into P subsets of the same size. Then, a decision tree such as C4.5 is learned. HME-BD is also based on partitioning the training data. There is an important difference with regard to CVCF: the use of Spark's implementation in a decision tree as a classifier. HME-BD is based on partitioning the training data."}, {"heading": "3.3. Heterogeneous Ensemble: HTE-BD", "text": "This noise filter uses a set of three learning algorithms to identify mislabeled instances in a dataset: a universal decision tree (C4.5), KNN, and a linear machine. It performs a k-fold cross-validation on the training data. For each of the k-parts, three algorithms are trained on the other k-1 parts. Each of the classifiers is used to mark each of the test examples as noisy or clean. At Figure 1: HME-BD Noise Filtering Process FlowChartof the input-fold, each example of the input data has been tagged. Finally, a matching strategy is used and noisy examples are removed. HTE-BD follows the same working scheme as EF. The main difference is the choice of three learning algorithms: \u2022 Instead of a decision tree, we use Sparks implementation of Random Forest."}, {"heading": "3.4. Similarity: ENN-BD", "text": "ENN-BD is a simple filter algorithm that serves as a starting point for comparison purposes. It was designed on the basis of the Processed Nearest Neighbor Algorithm (ENN) [47] and follows a similarity between instances. ENNFigure 2: HTE-BD Noise Filtering Process Flowchart Algorithm 3 ENN-BD Algorithm1: Input: Data an RDD of tuples (label, features) 2: Output: the filtered RDD without noise 3: knnModel \u2190 KNN (1 \"euklidean,\" data) 4: knnPred \u2190 zipWithIndex (Prediction (knnModel, data) 5: joinedData \u2190 join (zipWithIndex (data), knnPred) 6: Filtered data 7: Map instances KNN (1, \"Prediction\" joinedData 8: when label (original) = label (prediction) is then considered as original class (9: otherwise) (11)."}, {"heading": "4. Experimental Results", "text": "This section describes the experimental details and the analyses carried out to show the performance of the three noise filtering methods over four major problems. In Section 4.1 we present the details of the data sets and the parameters used in the methods. We analyze the improvements in accuracy achieved by the proposed framework and the investigation of the cases removed in Section 4.2. Finally, Section 4.3 deals with the calculation times of the proposals."}, {"heading": "4.1. Experimental Framework", "text": "In this context, it should be noted that this is not an isolated case, but a case in which it is an isolated case."}, {"heading": "4.2. Analysis of accuracy performance and removed instances", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.3. Computing times", "text": "In the previous section, we have shown the suitability of the proposed framework in terms of accuracy. In order to form a valid proposal in big data, this framework must also be scalable. This section is dedicated to the representation of the computation times for the two prospective ensemble techniques HME-BD and HTE-BD, as well as the simple similarity method ENN-BD, which is used as a basis. In Table 8, we can see the average running times of the three methods for the four data sets in seconds. Since the noise level is not a factor influencing the computation time, we show the average of the five executions performed for each data set. In Figure 5, we see a graphical representation of this time.The measured times show that the homogeneous interaction of HME-BD is not only the most powerful option in terms of accuracy, but also the most efficient in terms of computation time. HME-BD is about ten times faster than the heterogeneous filter HTE-BD and this filter-BD is the NTE-BD most accurate in terms of the END classification."}, {"heading": "5. Conclusions", "text": "In this paper, we have addressed the issue of noise in big data classification, which is a critical step in converting such raw data into smart data. We have proposed several noise filtering algorithms that have been implemented in a big data framework: Spark. These filtering techniques are based on the creation of classification groups that are executed in the various maps and allow users to handle huge data sets. Various strategies of data partitioning and combinations of classification groups have led to three different approaches. The suitability of these proposed techniques has been analyzed using multiple data sets to investigate the improvement in accuracy, runtime and data reduction rates. In most cases, the homogeneous overall ensemble has proven to be the most appropriate approach, both in terms of improving accuracy and better runtimes. It also shows the best balance between removing and holding sufficient instances, and being one of the most balanced filters in this advanced training unit."}, {"heading": "Acknowledgment", "text": "This work is supported by the Spanish national research project TIN201457251-P and the BBVA project of the Foundation 75 / 2016 BigDaPTOOLS."}], "references": [{"title": "Searching for Exotic Particles in High- Energy Physics with Deep Learning", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature Communications 5 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust supervised classification with mixture models: Learning from data with uncertain labels", "author": ["C. Bouveyron", "S. Girard"], "venue": "Pattern Recognition 42 (11) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning 45 (1) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Identifying Mislabeled Training Data", "author": ["C.E. Brodley", "M.A. Friedl"], "venue": "Journal of Artificial Intelligence Research 11 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Libsvm: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 2 (3) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Data-intensive applications", "author": ["C.P. Chen", "C.-Y. Zhang"], "venue": "challenges, techniques and technologies: A survey on Big Data, Information Sciences 275 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Smart data integration by goal driven ontology learning", "author": ["J. Chen", "D. Dosyn", "V. Lytvyn", "A. Sachenko"], "venue": "in: Advances in Intelligent Systems and Computing, vol. 529", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM 51 (1) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards an automatic analyze and standardization of unstructured data in the context of big and linked data", "author": ["H. Fadili", "C. Jouis"], "venue": "in: 8th International Conference on Management of Digital EcoSystems, MEDES 2016", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "High dimensional classification using features annealed independence rules", "author": ["J. Fan", "Y. Fan"], "venue": "Annals of statistics 36 (6) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Challenges of big data analysis", "author": ["J. Fan", "F. Han", "H. Liu"], "venue": "National Science Review 1 (2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["A. Fern\u00e1ndez"], "venue": "del \u0154\u0131o, V. L\u00f3pez, A. Bawakid, M. J. del Jes\u00fas, J. M. Be\u0144\u0131tez, F. Herrera, Big data with cloud computing: an insight on the computing environment, mapreduce, and programming frameworks, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 4 (5) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification in the presence of label noise: A survey", "author": ["B. Fr\u00e9nay", "M. Verleysen"], "venue": "IEEE Transactions on Neural Networks and Learning Systems 25 (5) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Data Preprocessing in Data Mining", "author": ["S. Gar\u0107\u0131a", "J. Luengo", "F. Herrera"], "venue": "Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Tutorial on practical tips of the most influential data preprocessing algorithms in data mining", "author": ["S. Gar\u0107\u0131a", "J. Luengo", "F. Herrera"], "venue": "Knowledge-Based Systems 98 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "S", "author": ["S. Gar\u0107\u0131a"], "venue": "Ram\u0131\u0301rez-Gallego, J. Luengo, J. M. Be\u0144\u0131tez, F. Herrera, Big data preprocessing: methods and prospects, Big Data Analytics 1 (1) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "S", "author": ["D. Gar\u0107\u0131a-Gil"], "venue": "Ram\u0131\u0301rez-Gallego, S. Gar\u0107\u0131a, F. Herrera, A comparison on scalability for batch big data processing on Apache Spark and Apache Flink, Big Data Analytics 2 (1) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning Spark: Lightning-Fast Big Data Analytics", "author": ["M. Hamstra", "H. Karau", "M. Zaharia", "A. Konwinski", "P. Wendell"], "venue": "OReilly Media", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A journey from big data to smart data", "author": ["F. Iafrate"], "venue": "Advances in Intelligent Systems and Computing 261 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving software quality prediction by noise filtering techniques", "author": ["T.M. Khoshgoftaar", "P. Rebours"], "venue": "Journal of Computer Science and Technology 22 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "3D data management: Controlling data volume", "author": ["D. Laney"], "venue": "velocity, and variety, Tech. rep., META Group ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["N.D. Lawrence", "B. Sch\u00f6lkopf"], "venue": "in: Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Towards a taxonomy of standards in smart data", "author": ["A. Lenk", "L. Bonorden", "A. Hellmanns", "N. Roedder", "S. Jaehnichen"], "venue": "in: Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "D", "author": ["Y. Li", "L.F. Wessels"], "venue": "de Ridder, M. J. Reinders, Classification in the presence of class noise using a probabilistic Kernel Fisher method, Pattern Recognition 40 (12) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Mapreduce is good enough? if all you have is a hammer", "author": ["J. Lin"], "venue": "throw away everything that\u2019s not a nail!, Big Data 1 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Sentiment analysis: Mining opinions", "author": ["B. Liu"], "venue": "sentiments, and emotions, Cambridge University Press", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["J. M\u00e1\u0131llo"], "venue": "Ram\u0131\u0301rez, I. Triguero, F. Herrera, kNN-IS: An Iterative Sparkbased design of the k-Nearest Neighbors classifier for big data, Knowledge- Based Systems 117 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Big Data: A Revolution That Will Transform How We Live", "author": ["V. Mayer-Schonberger", "K. Cukier"], "venue": "Work, and Think, Houghton Mifflin Harcourt", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Mllib: Machine learning in apache spark", "author": ["X. Meng", "J. Bradley", "B. Yavuz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen", "D. Xin", "R. Xin", "M.J. Franklin", "R. Zadeh", "M. Zaharia", "A. Talwalkar"], "venue": "Journal of Machine Learning Research 17 (34) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Rboost: Label noiserobust boosting algorithm based on a nonconvex loss function and the numerically stable base learners", "author": ["Q. Miao", "Y. Cao", "G. Xia", "M. Gong", "J. Liu", "J. Song"], "venue": "IEEE Transactions on Neural Networks and Learning Systems 27 (11) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Data preparation for data mining", "author": ["D. Pyle"], "venue": "Morgan Kaufmann, Los Altos", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1999}, {"title": "C4.5: programs for machine", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1993}, {"title": "Framework for smart health: Toward connected data from big data", "author": ["P. Raja", "E. Sivasankar", "R. Pitchiah"], "venue": "Advances in Intelligent Systems and Computing 343 ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Data discretization: taxonomy and big data challenge, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["S. Ram\u0131\u0301rez-Gallego", "S. Gar\u0107\u0131a", "H. Mouri\u00f1o-Ta\u013a\u0131n", "D. Mart\u0301\u0131nez-Rego", "V. Bol\u00f3n-Canedo", "A. Alonso-Betanzos", "J.M. Be\u0144\u0131tez", "F. Herrera"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Analyzing the presence of noise in multi-class problems: alleviating its influence with the One-vs-One decomposition", "author": ["J.A. S\u00e1ez", "M. Galar", "J. Luengo", "F. Herrera"], "venue": "Knowledge and Information Systems 38 (1) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Apache Spark: Lightning-fast cluster computing", "author": ["A. Spark"], "venue": "http://spark. apache.org/ ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards ultrahigh dimensional feature selection for big data", "author": ["M. Tan", "I.W. Tsang", "L. Wang"], "venue": "Journal of Machine Learning Research 15 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Correcting Noisy Data", "author": ["C.-M. Teng"], "venue": "in: Proceedings of the Sixteenth International Conference on Machine Learning, Morgan Kaufmann Publishers, San Francisco, CA, USA", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1999}, {"title": "S", "author": ["I. Triguero"], "venue": "del \u0154\u0131o, V. L\u00f3pez, J. Bacardit, J. M. Be\u0144\u0131tez, F. Herrera, Rosefw-rf: the winner algorithm for the ecbdl14 big data competition: an extremely imbalanced big data bioinformatics problem, Knowledge-Based Systems 87 ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensemble methods for noise elimination in classification problems", "author": ["S. Verbaeten", "A. Assche"], "venue": "in: 4th International Workshop on Multiple Classifier Systems, vol. 2709 of Lecture Notes on Computer Science, Springer", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards felicitous decision making: An overview on challenges and trends of Big Data", "author": ["H. Wang", "Z. Xu", "H. Fujita", "S. Liu"], "venue": "Information Sciences 367 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Hadoop: The Definitive Guide", "author": ["T. White"], "venue": "O\u2019Reilly Media, Inc.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Asymptotic properties of nearest neighbor rules using edited data", "author": ["D.L. Wilson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics 2 (3) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1972}, {"title": "Knowledge acquisition from databases", "author": ["X. Wu"], "venue": "Ablex Publishing Corp., Norwood, NJ, USA", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1996}, {"title": "Mining with noise knowledge: Error-aware data mining", "author": ["X. Wu", "X. Zhu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics 38 ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2008}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.-Q. Wu", "W. Ding"], "venue": "IEEE Transactions on Knowledge and Data Engineering 26 (1) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "in: Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, NSDI\u201912, USENIX Association, Berkeley, CA, USA", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Class noise elimination approach for large datasets based on a combination of classifiers", "author": ["B. Zerhari"], "venue": "in: Cloud Computing Technologies and Applications (CloudTech), 2016 2nd International Conference on, IEEE", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Analyzing Software Measurement Data with Clustering Techniques", "author": ["S. Zhong", "T.M. Khoshgoftaar", "N. Seliya"], "venue": "IEEE Intelligent Systems 19 (2) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2004}, {"title": "Class Noise vs", "author": ["X. Zhu", "X. Wu"], "venue": "Attribute Noise: A Quantitative Study, Artificial Intelligence Review 22 ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 45, "context": "The current volume of data has exceeded the processing capabilities of classical data mining systems [50] and have created a need for new frameworks for storing and processing this data.", "startOffset": 101, "endOffset": 105}, {"referenceID": 27, "context": "accepted that we have entered the Big Data era [31].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "Big Data is the set of technologies that make processing such large amounts of data possible [7], while most of the classic knowledge extraction methods cannot work in a Big Data environment because they were not conceived for it.", "startOffset": 93, "endOffset": 96}, {"referenceID": 20, "context": "Big Data as concept is defined around five aspects: data volume, data velocity, data variety, data veracity and data value [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases.", "startOffset": 63, "endOffset": 75}, {"referenceID": 30, "context": "In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases.", "startOffset": 63, "endOffset": 75}, {"referenceID": 15, "context": "In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases.", "startOffset": 63, "endOffset": 75}, {"referenceID": 44, "context": "Noise will lead to excessively complex models with deteriorated performance [49], resulting in even larger computing times for less value.", "startOffset": 76, "endOffset": 80}, {"referenceID": 34, "context": "The former is known to be the most disruptive [39, 54].", "startOffset": 46, "endOffset": 54}, {"referenceID": 49, "context": "The former is known to be the most disruptive [39, 54].", "startOffset": 46, "endOffset": 54}, {"referenceID": 12, "context": "Consequently, many recent works, including this contribution, have been devoted to resolving this problem or at least to minimize its effects (see [15] for a comprehensive and updated survey).", "startOffset": 147, "endOffset": 151}, {"referenceID": 47, "context": "While some architectural designs are already proposed in the literature[52], there is no particular algorithm which deals with noise in Big Data classification, nor a comparison of its effect on model generalization abilities or computing times.", "startOffset": 71, "endOffset": 75}, {"referenceID": 2, "context": "The first one is an homogeneous ensemble, named Homogeneous Ensembe for Big Data (HME-BD), which uses a single base classifier (Random Forest [4]) over a partitioning of the training set.", "startOffset": 142, "endOffset": 145}, {"referenceID": 17, "context": "All these techniques have been implemented under the Apache Spark framework [20, 40] and can be downloaded from the Spark\u2019s community repository .", "startOffset": 76, "endOffset": 84}, {"referenceID": 35, "context": "All these techniques have been implemented under the Apache Spark framework [20, 40] and can be downloaded from the Spark\u2019s community repository .", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "We also show that, for the Big Data problems considered, the classifiers also benefit from applying the noise treatment even when no additional noise is induced, since Big Data problems contain implicit noise due to incidental homogeneity, spurious correlations and the accumulation of noisy examples [12].", "startOffset": 301, "endOffset": 305}, {"referenceID": 48, "context": "All these imperfections may harm data interpretation, the design, size, building time, interpretability and accuracy of models, as well as decision making [53, 54].", "startOffset": 155, "endOffset": 163}, {"referenceID": 49, "context": "All these imperfections may harm data interpretation, the design, size, building time, interpretability and accuracy of models, as well as decision making [53, 54].", "startOffset": 155, "endOffset": 163}, {"referenceID": 43, "context": "[48], from the large number of components that comprise a dataset, class labels and attribute values are two essential elements in classification datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "Thus, two types of noise are commonly differentiated in the literature [54, 48]:", "startOffset": 71, "endOffset": 79}, {"referenceID": 43, "context": "Thus, two types of noise are commonly differentiated in the literature [54, 48]:", "startOffset": 71, "endOffset": 79}, {"referenceID": 37, "context": "Class noise includes contradictory examples [42, 39] (examples with identical input attribute values having different class labels) and misclassifications [54] (examples which are incorrectly labeled).", "startOffset": 44, "endOffset": 52}, {"referenceID": 34, "context": "Class noise includes contradictory examples [42, 39] (examples with identical input attribute values having different class labels) and misclassifications [54] (examples which are incorrectly labeled).", "startOffset": 44, "endOffset": 52}, {"referenceID": 49, "context": "Class noise includes contradictory examples [42, 39] (examples with identical input attribute values having different class labels) and misclassifications [54] (examples which are incorrectly labeled).", "startOffset": 155, "endOffset": 159}, {"referenceID": 49, "context": "Missing values are usually considered independently in the literature, so attribute noise is mainly used for erroneous values [54].", "startOffset": 126, "endOffset": 130}, {"referenceID": 49, "context": "Class noise is generally considered more harmful to the learning process, and methods for dealing with class noise are more frequent in the literature [54].", "startOffset": 151, "endOffset": 155}, {"referenceID": 25, "context": ", sentiment analysis polarization [29]), increasing the probability of class noise.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Due to the increasing attention from researchers and practitioners, numerous techniques have been developed to tackle it [15, 54, 16].", "startOffset": 121, "endOffset": 133}, {"referenceID": 49, "context": "Due to the increasing attention from researchers and practitioners, numerous techniques have been developed to tackle it [15, 54, 16].", "startOffset": 121, "endOffset": 133}, {"referenceID": 13, "context": "Due to the increasing attention from researchers and practitioners, numerous techniques have been developed to tackle it [15, 54, 16].", "startOffset": 121, "endOffset": 133}, {"referenceID": 12, "context": "In [15] the mechanisms that generate label noise are examined, relating them to the appropriate treatment procedures that can be safely applied:", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].", "startOffset": 144, "endOffset": 152}, {"referenceID": 23, "context": "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].", "startOffset": 144, "endOffset": 152}, {"referenceID": 31, "context": "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].", "startOffset": 211, "endOffset": 215}, {"referenceID": 29, "context": "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].", "startOffset": 295, "endOffset": 299}, {"referenceID": 1, "context": "Recent proposals exist which that combine these two approaches, which model the noise and give less relevance to potentially noisy instances in the classifier building process [3].", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "\u2022 On the other hand, data level approaches (also called filters) try to develop strategies to cleanse the dataset as a previous step to the fit of the classifier, by either creating ensembles of classifiers [5], iteratively filtering noisy instances [23], computing metrics on the data or even hybrid approaches that combine several of these strategies.", "startOffset": 207, "endOffset": 210}, {"referenceID": 19, "context": "\u2022 On the other hand, data level approaches (also called filters) try to develop strategies to cleanse the dataset as a previous step to the fit of the classifier, by either creating ensembles of classifiers [5], iteratively filtering noisy instances [23], computing metrics on the data or even hybrid approaches that combine several of these strategies.", "startOffset": 250, "endOffset": 254}, {"referenceID": 10, "context": "It is well known that the high dimensionality and example size generate accumulated noise in Big Data problems [12].", "startOffset": 111, "endOffset": 115}, {"referenceID": 40, "context": "MapReduce and Apache Spark The globalization of the Big Data paradigm is generating a large response in terms of technologies that must deal with the rapidly growing rates of generated data [45].", "startOffset": 190, "endOffset": 194}, {"referenceID": 7, "context": "Among all of them, MapReduce is the seminal framework designed by Google in 2003 [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 41, "context": "Apache Hadoop [46] [1] is the most popular open-source framework based on the MapReduce model.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Apache Spark [20, 40] is an open-source framework for Big Data processing built around speed, ease of use and sophisticated analytics.", "startOffset": 13, "endOffset": 21}, {"referenceID": 35, "context": "Apache Spark [20, 40] is an open-source framework for Big Data processing built around speed, ease of use and sophisticated analytics.", "startOffset": 13, "endOffset": 21}, {"referenceID": 24, "context": "The motivation for developing Spark came from the limitations in the MapReduce/Hadoop model [28, 13]:", "startOffset": 92, "endOffset": 100}, {"referenceID": 11, "context": "The motivation for developing Spark came from the limitations in the MapReduce/Hadoop model [28, 13]:", "startOffset": 92, "endOffset": 100}, {"referenceID": 46, "context": "Spark is built on top of a distributed data structure called Resilient Distributed Datasets (RDDs) [51].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Although new promising frameworks for Big Data are emerging, like Apache Flink [14], Apache Spark is becoming the reference in performance [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "This transformation is the difference between \u201cBig\u201d and \u201cSmart\u201d Data [26].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "The usage of ontologies to support the integration is a recent approach [10, 8], but graph databases are also an option where the data is stored in a relational form, as in healthcare domains [36].", "startOffset": 72, "endOffset": 79}, {"referenceID": 6, "context": "The usage of ontologies to support the integration is a recent approach [10, 8], but graph databases are also an option where the data is stored in a relational form, as in healthcare domains [36].", "startOffset": 72, "endOffset": 79}, {"referenceID": 32, "context": "The usage of ontologies to support the integration is a recent approach [10, 8], but graph databases are also an option where the data is stored in a relational form, as in healthcare domains [36].", "startOffset": 192, "endOffset": 196}, {"referenceID": 9, "context": "Even when the integration phase ends, the data is still far from being \u201csmart\u201d: the accumulated noise in Big Data problems creates problems in classical Data Mining techniques, specially when the dimensionality is large [11].", "startOffset": 220, "endOffset": 224}, {"referenceID": 13, "context": "Thus, in order to be \u201csmart\u201d, the data still needs to be cleaned even after its integration, and data preprocessing is the set of techniques utilized to encompass this task [16, 17].", "startOffset": 173, "endOffset": 181}, {"referenceID": 14, "context": "Thus, in order to be \u201csmart\u201d, the data still needs to be cleaned even after its integration, and data preprocessing is the set of techniques utilized to encompass this task [16, 17].", "startOffset": 173, "endOffset": 181}, {"referenceID": 18, "context": "The goal is to evolve from a data-centered organization to a learning organization, where the focus is set on the knowledge extracted instead of struggling with the data management [21].", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "However, Big Data generates great challenges to achieve this since its high dimensionality and large example size imply noise accumulation, algorithmic instability and the massive sample pool is often aggregated from heterogeneous sources [12].", "startOffset": 239, "endOffset": 243}, {"referenceID": 28, "context": "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark\u2019s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a", "startOffset": 181, "endOffset": 185}, {"referenceID": 36, "context": "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark\u2019s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a", "startOffset": 203, "endOffset": 219}, {"referenceID": 33, "context": "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark\u2019s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a", "startOffset": 203, "endOffset": 219}, {"referenceID": 38, "context": "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark\u2019s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a", "startOffset": 203, "endOffset": 219}, {"referenceID": 39, "context": "Homogeneous Ensemble: HME-BD The homogeneous ensemble is inspired by Cross-Validated Committees Filter (CVCF) [44].", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": "Heterogeneous Ensemble: HTE-BD Heterogeneous Ensemble is inspired by Ensemble Filter (EF) [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 26, "context": "\u2022 We use an exact implementation of KNN with the euclidean distance present in Spark\u2019s community repository, kNN-IS [30] \u2022 The linear machine has been replaced by Spark\u2019s implementation of Logistic Regression, which is another linear classifier.", "startOffset": 116, "endOffset": 120}, {"referenceID": 42, "context": "It has been designed based on the Edited Nearest Neighbor algorithm (ENN) [47] and follows a similarity between instances approach.", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "The task is to distinguish between a signal process which produces supersymmetric (SUSY) particles and a background process which does not [2].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "It was further pre-processed and included in the LibSVM dataset repository [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 37, "context": "We carried out experiments on five levels of uniform class noise [42]: for each level of noise, a percentage of the training instances are altered by replacing their actual label by another label from the available classes.", "startOffset": 65, "endOffset": 69}], "year": 2017, "abstractText": "In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.", "creator": "LaTeX with hyperref package"}}}