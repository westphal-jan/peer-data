{"id": "1704.01792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2017", "title": "Neural Question Generation from Text: A Preliminary Study", "abstract": "Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub-spans of the given passage. Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions. In this work, we propose to apply the neural encoder-decoder model to generate meaningful and diverse questions from natural language sentences. The encoder reads the input text and the answer position, to produce an answer-aware input representation, which is fed to the decoder to generate an answer focused question. We conduct a preliminary study on neural question generation from text with the SQuAD dataset, and the experiment results show that our method can produce fluent and diverse questions.", "histories": [["v1", "Thu, 6 Apr 2017 11:44:07 GMT  (244kb,D)", "https://arxiv.org/abs/1704.01792v1", "Submitted to EMNLP 2017"], ["v2", "Sun, 16 Apr 2017 03:27:15 GMT  (338kb,D)", "http://arxiv.org/abs/1704.01792v2", "Submitted to EMNLP 2017"], ["v3", "Tue, 18 Apr 2017 07:54:52 GMT  (337kb,D)", "http://arxiv.org/abs/1704.01792v3", "Submitted to EMNLP 2017"]], "COMMENTS": "Submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qingyu zhou", "nan yang", "furu wei", "chuanqi tan", "hangbo bao", "ming zhou"], "accepted": false, "id": "1704.01792"}, "pdf": {"name": "1704.01792.pdf", "metadata": {"source": "CRF", "title": "Neural Question Generation from Text: A Preliminary Study", "authors": ["Qingyu Zhou", "Nan Yang", "Furu Wei", "Chuanqi Tan", "Hangbo Bao", "Ming Zhou"], "emails": ["qyzhgm@gmail.com", "mingzhou}@microsoft.com", "tanchuanqi@nlsde.buaa.edu.cn", "baohangbo@hit.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The automatic generation of questions from the text of natural language aims to generate questions that serve as input, which has the potential value of educational goals (Heilman, 2011). Conversely, the task of answering questions also has the potential to provide a large corpus of question-answer pairs. Previous work on question-generation mainly uses rigid heuristic rules to turn a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015). However, these methods rely heavily on human-designed rules of transformation and generation, which cannot simply be transferred to other areas. Instead of generating questions from texts, Serban et al. (2016) proposed a novel contribution during the internship at Microsoft Research."}, {"heading": "2 Approach", "text": "In this section we present the NQG framework, which consists of a feature-rich encoder and an anar Xiv: 170 4.01 792v 3 [cs.C L] 18 Apr 201 7Attention-based decoder. Figure 1 gives an overview of our NQG framework."}, {"heading": "2.1 Feature-Rich Encoder", "text": "Within the framework of NQG, we use Gated Recurrent Unit (GRU) (Cho et al., 2014) to build the encoder. To gather more context information, we use bidirectional GRU (BiGRU) to read the input both forward and backwards. Inspired by Chen and Manning (2014); Nallapati et al. (2016), the BiGRU encoder reads not only the sentence words, but also handmade characteristics to create a sequence of word and feature vectors. Specifically, the BiGRU encoder reads the linked word word vector, lexical characteristics that embed vectors and response position indicators that embed vector as input from BiGRU encoders. Specifically, the BiGRU encoder reads the linked word vector, lexical characteristics, and the response position, x = (1, x2, to generate two sequences of xs)."}, {"heading": "2.2 Attention-Based Decoder", "text": "We use an attention-based GRU decoder to decode the sentence and answer information to generate questions. In the decoding step t, the GRU decoder reads the previous word, embeds the wt \u2212 1 and the context vector ct \u2212 1, to calculate the new hidden state st. We use a linear layer with the last hidden state of the decoder, ~ h1, to initialize the decoder. Context vector ct for the current time step t is calculated by the concatenated attention mechanism (Luong et al., 2015), which corresponds to the current decoder state st with each hidden state hi, to obtain an importance score. The importance scores are then normalized to get the current context vector by weighted sum: st = GRU (wt \u2212 1, ct \u2212 1, st \u2212 1) s0 (sh = 1 tanrt) and stanrt (1) (stanrt = 1 tanh)."}, {"heading": "2.3 Copy Mechanism", "text": "To solve the problem of rare and unknown words, Gulcehre et al. (2016) propose to copy rare words from the source set using the pointer mechanism. We use this pointer method in our NQG system. When decoding the word t, the copy switch takes the current decoder state st and the context vector ct as input and generates the probability p to copy a word from the source set: p = \u03c3 (Wst + Uct + b) (9), where \u03c3 is the sigmoid function. We use the attention probability in Eq.4 to decide which word to copy."}, {"heading": "3 Experiments and Results", "text": "We use the SQuAD dataset as training data. SQuAD consists of more than 100K questions asked by Crowd Workers on 536 Wikipedia articles. We extract triple sentence-answer questions to create the training, development and test kits. 1 Since the test kit is not publicly available, we randomly halve the development kit to construct the new development and test kits. The extracted training, development and test kits contain 86,635, 8,965 and 8,964 triples. We present the implementation details in the appendix. We perform several experiments and ablation tests as follows: PCFG-Trans The rules-based System1 has been modified based on the code published by Heilman (2011) \u2212 We modified the code to generate questions based on a predefined word spanning. s2s + att We implement a seq2seq + QQQS + with attention as the basic method."}, {"heading": "3.1 Results and Analysis", "text": "This year, it has come to the point where it will be able to put itself at the top, and in the way in which it has pushed itself to the top."}, {"heading": "4 Conclusion and Future Work", "text": "In this paper, we are conducting a preliminary study on the generation of natural language questions using new network models. We propose to use the neural encoder decoder model to generate answer-focused questions based on natural language phrases. The proposed approach uses a feature rich encoder to encode response position, POS and NER tag information. Experiments demonstrate the effectiveness of our NQG method. In future work, we would like to investigate whether the automatically generated questions can help improve question answer systems."}, {"heading": "B Human Evaluation Examples", "text": "We evaluate the PCFG-Trans Baseline and NQG + + with human judges. The evaluation scheme is in Table 4.The human judges are asked to name the generated questions if they match the given sentence and the answer margin. In Table 5 we provide some sample questions with different results. In the first result 3, the question makes sense and the target answer \"reason\" can be used to answer it in view of the input sentence. In the second result 2, the question is not sufficient to answer the sentence because the answer is a prime number. However, in view of the sentence, a reasonable person gives the targeted answer to the question. In the third result 1 example, the question is completely wrong in view of the sentence and the answer."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of 3rd International Conference for Learning Representations. San Diego.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Towards topicto-question generation", "author": ["Yllias Chali", "Sadid A. Hasan."], "venue": "Comput. Linguist. 41(1):1\u2013", "citeRegEx": "Chali and Hasan.,? 2015", "shortCiteRegEx": "Chali and Hasan.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of EMNLP 2014. Association for Computational Linguistics, Doha, Qatar, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss."], "venue": "Psychological bulletin 76(5):378.", "citeRegEx": "Fleiss.,? 1971", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Aistats. volume 9, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Maxout networks", "author": ["Ian J Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron C Courville", "Yoshua Bengio."], "venue": "ICML (3) 28:1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Automatic factual question generation from text", "author": ["Michael Heilman."], "venue": "Ph.D. thesis, Carnegie Mellon University.", "citeRegEx": "Heilman.,? 2011", "shortCiteRegEx": "Heilman.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP 2015. Association for Computational Linguistics, Lisbon, Portugal, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Foundations of statistical natural language processing, volume 999", "author": ["Christopher D Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Association for Computational Linguistics (ACL) System Demonstrations.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "\u00c7a glar Gul\u00e7ehre", "Bing Xiang."], "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning.", "citeRegEx": "Nallapati et al\\.,? 2016", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3) 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Iulian Vlad Serban", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Model Training We initialize model parameters randomly using a Gaussian distribution with Xavier scheme", "author": ["els. A"], "venue": "(Glorot and Bengio,", "citeRegEx": "A.3,? \\Q2010\\E", "shortCiteRegEx": "A.3", "year": 2010}, {"title": "2015) and simple SGD as our the optimizing algorithms. The training is separated into two phases, the first phase is optimizing the loss function with Adam and the second is with simple SGD", "author": ["Adam (Kingma", "Ba"], "venue": "For the Adam optimizer,", "citeRegEx": ".Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": ".Kingma and Ba", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Automatic question generation from natural language text aims to generate questions taking text as input, which has the potential value of education purpose (Heilman, 2011).", "startOffset": 157, "endOffset": 172}, {"referenceID": 8, "context": "Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015).", "startOffset": 119, "endOffset": 157}, {"referenceID": 1, "context": "Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015).", "startOffset": 119, "endOffset": 157}, {"referenceID": 1, "context": "Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions (Heilman, 2011; Chali and Hasan, 2015). However, these methods heavily rely on human-designed transformation and generation rules, which cannot be easily adopted to other domains. Instead of generating questions from texts, Serban et al. (2016) proposed a neu-", "startOffset": 135, "endOffset": 364}, {"referenceID": 0, "context": "Lastly, the decoder with attention mechanism (Bahdanau et al., 2015) generates an answer specific question of the sentence.", "startOffset": 45, "endOffset": 68}, {"referenceID": 17, "context": "We propose to adapt the recently released Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) as the training and development datasets for the question generation task.", "startOffset": 86, "endOffset": 110}, {"referenceID": 3, "context": "In the NQG framework, we use Gated Recurrent Unit (GRU) (Cho et al., 2014) to build the encoder.", "startOffset": 56, "endOffset": 74}, {"referenceID": 2, "context": "Inspired by Chen and Manning (2014); Nallapati et al.", "startOffset": 12, "endOffset": 36}, {"referenceID": 2, "context": "Inspired by Chen and Manning (2014); Nallapati et al. (2016), the BiGRU encoder not only reads the sentence words, but also handcrafted features, to produce a sequence of word-and-feature vectors.", "startOffset": 12, "endOffset": 61}, {"referenceID": 10, "context": "The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state st with each encoder hidden state hi to get an importance score.", "startOffset": 102, "endOffset": 122}, {"referenceID": 6, "context": "The readout state is passed through a maxout hidden layer (Goodfellow et al., 2013) to predict the next word with a softmax layer over the decoder vocabulary:", "startOffset": 58, "endOffset": 83}, {"referenceID": 7, "context": "To deal with the rare and unknown words problem, Gulcehre et al. (2016) propose using pointing mechanism to copy rare words from source sentence.", "startOffset": 49, "endOffset": 72}, {"referenceID": 16, "context": "NQG+Pretrain Based on NQG+, we initialize the word embedding matrix with pre-trained GloVe (Pennington et al., 2014) vectors.", "startOffset": 91, "endOffset": 116}, {"referenceID": 8, "context": "PCFG-Trans The rule-based system1 modified on the code released by Heilman (2011). We modified the code so that it can generate question based on a given word span.", "startOffset": 67, "endOffset": 82}, {"referenceID": 14, "context": "We report BLEU-4 score (Papineni et al., 2002) as the evaluation metric of our NQG system.", "startOffset": 23, "endOffset": 46}, {"referenceID": 4, "context": "The inter-rater aggreement is measured with Fleiss\u2019 kappa (Fleiss, 1971).", "startOffset": 58, "endOffset": 72}, {"referenceID": 20, "context": "Type of Generated Questions Following Wang and Jiang (2016), we classify the questions into different types, i.", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub-spans of the given passage. Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions. In this work, we propose to apply the neural encoderdecoder model to generate meaningful and diverse questions from natural language sentences. The encoder reads the input text and the answer position, to produce an answer-aware input representation, which is fed to the decoder to generate an answer focused question. We conduct a preliminary study on neural question generation from text with the SQuAD dataset, and the experiment results show that our method can produce fluent and diverse questions.", "creator": "LaTeX with hyperref package"}}}