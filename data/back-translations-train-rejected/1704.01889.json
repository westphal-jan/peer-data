{"id": "1704.01889", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2017", "title": "Conformative Filtering for Implicit Feedback Data", "abstract": "Implicit feedback is the simplest form of user feedback that can be used for item recommendation. It is easy to collect and domain independent. However, there is a lack of negative examples. Existing works circumvent this problem by making various assumptions regarding the unconsumed items, which fail to hold when the user did not consume an item because she was unaware of it. In this paper we propose Conformative Filtering (CoF) as a novel method for addressing the lack of negative examples in implicit feedback. The motivation is that if there is a large group of users who share the same taste and none of them consumed an item, then it is highly likely that the item is irrelevant to this taste. We use Hierarchical Latent Tree Analysis (HLTA) to identify taste-based user groups, and make recommendations for a user based on her memberships in the groups. Experiments on real-world datasets from different domains show that CoF has superior performance compared to other baselines and more than 10% improvement in Recall@5 and Recall@10 is observed.", "histories": [["v1", "Thu, 6 Apr 2017 15:31:48 GMT  (3275kb)", "http://arxiv.org/abs/1704.01889v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["farhan khawar", "nevin l zhang", "jinxing yu"], "accepted": false, "id": "1704.01889"}, "pdf": {"name": "1704.01889.pdf", "metadata": {"source": "CRF", "title": "Conformative Filtering for Implicit Feedback Data", "authors": ["Farhan Khawar", "Nevin L. Zhang", "Jinxing Yu"], "emails": ["fkhawar@cse.ust.hk", "lzhang@cse.ust.hk", "jyuat@cse.ust.hk", "Recall@5", "Recall@10"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.01 889v 1 [cs.I R] 6Implicit feedback is the simplest form of user feedback that can be used for product recommendations. It is easy to collect and domain-independent. However, there is a lack of negative examples. Existing work circumvents this problem by making various assumptions regarding the unconsumed items that do not apply if the user has not consumed a product because he was not aware of it. In this paper, we propose conformative filtering (CoF) as a new method to address the lack of negative examples in the implied feedback. The motivation is that if there is a large group of users who share the same taste and none of them have consumed a product, it is very likely that the product is irrelevant to that taste. We use hierarchical latent tree analysis (HLTA) to identify taste-based user groups and have recommendations for users to perform better on the different sets of data in their @ areas."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top without being able to put itself at the top."}, {"heading": "2 Related Work", "text": "In fact, the fact is that most of them will be able to play by the rules they have established over the years, and that they will be able to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said."}, {"heading": "3 Basics of Latent Tree Models", "text": "In this section, we briefly review the basics of latent tree models. Materials are borrowed from [Zhang and Poon, 2017]. A latent tree model (LTM) is a tree-structured Bayesian network [Pearl, 1988] in which the leaf nodes represent observed variables and the internal nodes represent latent variables. An example is shown in Figure 1 (a). All variables are assumed to be binary in this essay. Model parameters include a marginal distribution for the root Y1 and a conditional distribution for each of the other nodes based on its parent. The product of the distributions defines a common distribution across all variables. By changing the root from Y1 to Y2 in Figure 1 (a), we obtain another model shown in (b). Both models are equivalent in the sense that they represent the same set of distribution variables."}, {"heading": "4 Taste Group Detection", "text": "HLTA can be used to analyze implicit feedback data when we consider the elements as words and a user's usage history as a document. We ran it on the Movielens1M Dataset. Part of the resulting model is shown in Figure 3. The variables on the sheets are binary variables that indicate whether movies are consumed; the other variables are latent variables. Each latent variable divides users into two clusters. Therefore, multiple partitions of users will be obtained [Chen et al., 2012; Liu et al., 2015]. Users in the same clusters tend to consume the same subsets of elements. Thus, multiple tastes are detected, with one being identified by each latent variable. Thus, the movies \"Armageddon,\" \"Golden Eye\" and \"Con Air,\" the relevant abilities of users in the Z13 latent Variable, which show the pattern that the three movies tend to be consumed together, are used."}, {"heading": "5 Conformative filtering", "text": "Suppose we have learned an HLTM m from implicit feedback data, and suppose there is a K-latent variable at the l-level of the model, each with two states s0 and s1. Call the latent variables Zl1,..., ZlK. They give us K-flavor-based user groups Zl1 = s1,..., ZlK = s1, which are sometimes referred to as G1,..., GK for convenience. In this section, we will show how these flavor groups can be used to recommend items."}, {"heading": "5.1 Taste Group Characterization", "text": "A natural way to characterize a user group is to aggregate past behaviors of the group members. We find the problem somewhat complex because our user groups are soft clusters. Let me (i | u, D) be the indicator function that takes a value of 1 if user u item i has previously used up, according to the record D, and 0 otherwise. We determine the preference of a taste group Gk (i.e., Zlk = s1) to an item i as follows: p (i | Gk, D) = Item I (i | u, D) P (Zlk = s1 | u, m). We determine the preferences of a taste group u P (Zlk = s1 | u, m), (1) where P (ZlK = s1 | u, m) the probability is that user u is in the soft cluster Zlk = s1, according to the model m."}, {"heading": "5.2 User Characterization", "text": "A user u is characterized by her membership in the K clusters, i.e., u = (P (Zl1 = s1 | u, m),.., P (ZlK = s1 | u, m). (2) Note that m is a tree structure model. All K probability values can be calculated by spreading messages twice over the tree [Pearl, 1988]. It takes time, linearly in the number of variables in the model and therefore linearly in the number of items."}, {"heading": "5.3 Item Recommendation", "text": "To make recommendations, we first calculate a score for each user-item pair and recommend the items with the highest scores for each user. The score r-ui for a user-item pair (u, i) is calculated from the combination of the taste group characteristics and the affiliation of u in these groups: r-ui = K \u2211 k = 1p (i | Gk, DH) P (Zlk = s1 | u, m). (3) The score is the internal product of two vectors - the user characterization vector u and the iteration vector i = (p (i | G1, DH),..., p (i | GK, DH); the item i is characterized by the preferential values of the K taste groups for i."}, {"heading": "6 Experiments", "text": "We conducted experiments with three publicly available datasets from different areas. Each dataset consisted only of (user, article, timestamp) tuples. Following [Wu et al., 2017], we divided a dataset by time into training, validation, and test subsets. Thus, all training instances came before all test instances, which is more consistent with real-world scenarios than splits without time. We tested several splits of the datasets and the results were similar. Below, we report only the results of the split with 70% of the data for training, 15% for validation, and 15% for testing."}, {"heading": "6.1 Datasets", "text": "Here are the three data sets used in our experiments: \u2022 Movilens1M 2 is a data set that contains the ratings given by users to the movies they watched. \u2022 Amazon baby [McAuley et al., 2015] is a data set that consists of users submitting ratings for the baby products they purchased from Amazon. Following the data set providers \"proposal, we retained only users and articles with more than 5 ratings. 2 https: / / grouplens.org / datasets / movielens / 1m / \u2022 Ta-feng 3 is an implicit feedback supermarket data set that consists of buying events. Where a user buys an item from the supermarket is an event. The first two are explicit feedback data and were converted into implicit feedback data by ignoring the explicit ratings. Then, all of a user's events indicate that an article is consumed by a user (see table 2.)"}, {"heading": "6.2 Setup", "text": "We compared CoF with the four popular baselines: Weighted-User-KNN (W-UkNN), Weighted-itemKNN (W-IkNN), Weighted-Matrix Factorization (WRMF) [Hu et al., 2008] and Bayesian Personalized Ranking Matrix Factorization (BPRMF) [Rendle et al., 2009]. As mentioned in Section 2, WRMF and BPRMF were specifically retrained for the lack of negative examples in implicit feedback. MyMediaLite implementation [Gantner et al., 2011] was applied to all baselines. Parameters of all methods were adjusted based on the validation set. After parameter tuning, we retrained all models on the pull and validation on the test set and tested on the test set. For WRMF and BPRMF, we conducted a network search for the latency factors {10, F, K, 40, K, NK, 10, NK, 10, K, 10, K, K, and NPRMF."}, {"heading": "6.3 Evaluation Measures", "text": "We used two popular metrics to evaluate the quality of the recommended lists for implicit feedback. They are briefly outlined below: \u2022 Recall @ R: It is the share of all items consumed by a user that is placed at the top R positions in a recommended list. \u2022 NDCG: Discounted cumulative gain is defined as DCG = \u2211 P i = 1 Reli log2 (i + 1), with P indicating the total number of recommended items and Reli numbers (0, 1}) whether the item at position i is consumed by the user. NDCG is DCG, normalized by the ideal DCG of list.3 http: / / www.bigdatalab.ac.cn / benchmark / bm / dd? data = Ta-FengWhile Recall @ R was calculated on the top R items, NDCG was calculated across the entire recommendation list, all of the Items were not consumed."}, {"heading": "6.4 Main Results", "text": "The result is in Table 3. Recall @ R is probably the most important measure of implicit feedback. In the real scenario, one can only recommend a few items to a user, and one would hope that the list contains as many items of interest to the user as possible. That's exactly what Recall @ R measures. In terms of Recall @ R, CoF performed drastically better than all baselines. For example, the Recall @ 5 and Recall @ 10 values of CoF are over 10% higher than the baselines of all data sets. Note that the second best method is user-based kNN, which is not a model-based method. If we compare CoF only with model-based alternatives, the improvement is more than 20%. In addition, in Movilens1M, CoF outperforms all competing methods by a wide margin. This shows that when the data is less sparse, CoF is able to extract meaningful information than others."}, {"heading": "6.5 Impact of Parameters", "text": "The first parameter l determines which level of the hierarchical model of HLTA is produced = 33.04% = to use. The larger the l, the less the number of latent factors. The second parameter H denotes the number of recent results of a user used in the characterization of user groups. Although both parameters are selected using validation, it would be interesting to gain some insight into their impact on performance. Figure 4 (a) shows the Recall @ 5 values on Ta-feng as a Table 3: Performance evaluation on test kit: Boldface denotes the best performance, underlines the secondary performance, and parameters include the percentage improvement of CoF over the second best result. CoF outperforms other methods in all but only one test case. Substantial improvement in Recall @ 5 and Recall @ 10 is observed. Ta-feng Recall @ 5 Recall @ 10 Recall @ 20 NDCG CoF (l = 40, 66.0K = 33.04) ="}, {"heading": "7 Conclusion", "text": "The key assumption is that a user with a certain taste would be interested in the items that other users with the same taste have previously consumed. Therefore, the items not consumed by these users are negative examples of this taste. User groups with a certain taste are determined by hierarchical latent tree analyses. The assumption behind CoF is more justified than the assumption behind existing alternative methods. In addition, it has several other advantages. Compared to the userkNN, CoF is model-based, whereas the latter is not. In predicting the interests of a target user, CoF uses earlier behaviors of other users who share at least one taste with the target user, while kNN relies on previous behaviors only of those users who share all tastes with the target user. Compared to matrix factoring (MF), CoF implicitly uses more interpretable latent factors. Furthermore, certain taste groups are only treated by user groups who share all tastes with the target user."}, {"heading": "Acknowledgements", "text": "We would like to thank Peixian Chen and Zhourong Chen for valuable discussions. Research for this article was supported by the Hong Kong Research Grants Council under grant programs 16202515 and 16212516."}, {"heading": "Appendix: Proof of Theorem 1", "text": "(4) Since x1, x2, \u00b7 \u00b7, xN are independent, the Hoeffding inequality [Hoeffding, 1963] results for both sides: t > 0P (X \u2264 E (X) \u2212 t) \u2264 exp (\u2212 2t2N). (5) If we select t = \u221a \u2212 N log (1 \u2212 p) 2, then exp (\u2212 2t2 N) = 1 \u2212 p.Ifm \u2012 log (1 \u2212 q \u2212 p) 2N) n log (1 \u2212 1 / N), then exp (\u2212 2t2 N) = 1 \u2212 p.Ifm \u2012 log (1 \u2212 q \u2212 p) 2N) n log (1 \u2212 1 / N), we haveE (X \u2212 qt), then exp (\u2212 2t2 N) = 1 \u2212 p.Ifm \u2012 p (ED) \u2264 p (1 \u2212 p)."}], "references": [{"title": "on Knowl", "author": ["Gediminas Adomavicius", "YoungOk Kwon. Improving aggregate recommendation diversity using ranking-based techniques. IEEE Trans"], "venue": "and Data Eng., 24(5):896\u2013911,May", "citeRegEx": "Adomavicius and Kwon. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Artificial Intelligence", "author": ["Tao Chen", "Nevin L Zhang", "Tengfei Liu", "Kin Man Poon", "Yi Wang. Model-based multidimensional clustering of categorical data"], "venue": "176(1):2246\u20132269,", "citeRegEx": "Chen et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Progressive em for latent tree models and hierarchical topic detection", "author": ["Chen et al", "2016] Peixian Chen", "Nevin L. Zhang", "Leonard K.M. Poon", "Zhourong Chen"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Mymedialite: A free recommender system library", "author": ["Gantner et al", "2011] Zeno Gantner", "Steffen Rendle", "Christoph Freudenthaler", "Lars Schmidt-Thieme"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "35(12):61\u201370", "author": ["David Goldberg", "David Nichols", "Brian M. Oki", "Douglas Terry. Using collaborative filtering to weave an information tapestry. Commun. ACM"], "venue": "December", "citeRegEx": "Goldberg et al.. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Journal of the American Statistical Association", "author": ["Wassily Hoeffding. Probability inequalities for sums of bounded random variables"], "venue": "58(301):13\u201330,March", "citeRegEx": "Hoeffding. 1963", "shortCiteRegEx": null, "year": 1963}, {"title": "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining", "author": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky. Collaborative filtering for implicit feedback datasets"], "venue": "ICDM \u201908, pages 263\u2013272, Washington, DC, USA,", "citeRegEx": "Hu et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 77\u2013118", "author": ["Yehuda Koren", "Robert Bell. Advances in Collaborative Filtering"], "venue": "Springer US, Boston, MA,", "citeRegEx": "Koren and Bell. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 256\u2013272", "author": ["Tengfei Liu", "Nevin L Zhang", "Peixian Chen. Hierarchical latent tree analysis for topic detection. In Joint European Conference on Machine Learning", "Knowledge Discovery in Databases"], "venue": "Springer,", "citeRegEx": "Liu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine learning", "author": ["Teng-Fei Liu", "Nevin L Zhang", "Peixian Chen", "April Hua Liu", "Leonard KM Poon", "Yi Wang. Greedy learning of latent tree models for multidimensional clustering"], "venue": "98(12):301\u2013330,", "citeRegEx": "Liu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Image-based recommendations on styles and substitutes", "author": ["McAuley et al", "2015] Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton Van Den Hengel"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "In Proceedings of the 5th DELOS Workshop on Filtering and Collaborative Filtering", "author": ["David M. Nichols. Implicit ratings", "filtering"], "venue": "Budapaest: ERCIM, volume 12,", "citeRegEx": "Nichols. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "One-class collaborative filtering", "author": ["Pan et al", "2008] Rong Pan", "Yunhong Zhou", "Bin Cao", "Nathan N. Liu", "Rajan Lukose", "Martin Scholz", "Qiang Yang"], "venue": "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,", "citeRegEx": "Pearl. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "SchmidtThieme. Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Rendle et al", "2009] Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "USA", "author": ["Francesco Ricci", "Lior Rokach", "Bracha Shapira", "Paul B. Kantor. Recommender Systems Handbook. Springer-Verlag New York", "Inc.", "NY New York"], "venue": "1st edition,", "citeRegEx": "Ricci et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Recommender Systems: Introduction and Challenges", "author": ["Francesco Ricci", "Lior Rokach", "Bracha Shapira"], "venue": "pages 1\u201334. Springer US, Boston, MA,", "citeRegEx": "Ricci et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["Wang", "Blei", "2011] ChongWang", "David M. Blei"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Recurrent recommender networks", "author": ["Wu et al", "2017] Chao-Yuan Wu", "Amr Ahmed", "Alex Beutel", "Alexander J. Smola", "How Jing"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2017\\E", "shortCiteRegEx": "al. et al\\.", "year": 2017}, {"title": "February 4-9", "author": ["Nevin L. Zhang", "Leonard K.M. Poon. Latent tree analysis. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence"], "venue": "2017, San Francisco, California, USA., pages 4891\u20134898,", "citeRegEx": "Zhang and Poon. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "Journal of Machine Learning Research, 5(6):697\u2013723", "citeRegEx": "Zhang. 2004", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "Collaborative filtering (CF) [Goldberg et al., 1992] is one commonly used technique to deal with the problem.", "startOffset": 29, "endOffset": 52}, {"referenceID": 7, "context": "Most research work on CF focuses on explicit feedback data, where users provide explicit ratings for items [Koren and Bell, 2015].", "startOffset": 107, "endOffset": 129}, {"referenceID": 11, "context": "In practice, one often encounters implicit feedback data, where users do not explicitly rate items [Nichols, 1997].", "startOffset": 99, "endOffset": 114}, {"referenceID": 16, "context": "A key issue with implicit feedback is how to deal with the lack of negative examples [Ricci et al., 2015].", "startOffset": 85, "endOffset": 105}, {"referenceID": 8, "context": "We use HLTA [Chen et al., 2016; Liu et al., 2014] to identify taste-based user groups.", "startOffset": 12, "endOffset": 49}, {"referenceID": 15, "context": "CoF is similar to user-kNN [Ricci et al., 2010] in that they both predict a user\u2019s preferences based on past behaviors of similar users.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Thus, non-consumption is essentially viewed as disinterest with low confidence [Hu et al., 2008; Pan et al., 2008].", "startOffset": 79, "endOffset": 114}, {"referenceID": 19, "context": "The materials are borrowed from [Zhang and Poon, 2017].", "startOffset": 32, "endOffset": 54}, {"referenceID": 13, "context": "A latent tree model (LTM) is a tree-structured Bayesian network [Pearl, 1988], where the leaf nodes represent observed variables and the internal nodes represent latent variables.", "startOffset": 64, "endOffset": 77}, {"referenceID": 20, "context": ", X5 [Zhang, 2004].", "startOffset": 5, "endOffset": 18}, {"referenceID": 8, "context": "In particular, [Chen et al., 2016; Liu et al., 2014] have developed an algorithm for learning hierarchical latent tree Table 1: Information about the latent variables Z13, that represents the taste for three action-adventure-thriller movies, and Z1147, that represents the taste for three children-animation movies.", "startOffset": 15, "endOffset": 52}, {"referenceID": 1, "context": "Hence, multiple partitions of the users are obtained [Chen et al., 2012; Liu et al., 2015].", "startOffset": 53, "endOffset": 90}, {"referenceID": 9, "context": "Hence, multiple partitions of the users are obtained [Chen et al., 2012; Liu et al., 2015].", "startOffset": 53, "endOffset": 90}, {"referenceID": 13, "context": "All the K posterior probability values can be calculated by propagating messages over the tree twice [Pearl, 1988].", "startOffset": 101, "endOffset": 114}, {"referenceID": 6, "context": "We compared CoF against the four popular baselines: weighted-user-KNN (W-UkNN), weighted-itemKNN (W-IkNN), weighted regularized matrix factorization (WRMF) [Hu et al., 2008], and Bayesian personalized ranking matrix factorization (BPRMF) [Rendle et al.", "startOffset": 156, "endOffset": 173}, {"referenceID": 0, "context": "The choice of H also influences global diversity[Adomavicius and Kwon, 2012] i.", "startOffset": 48, "endOffset": 76}], "year": 2017, "abstractText": "Implicit feedback is the simplest form of user feedback that can be used for item recommendation. It is easy to collect and domain independent. However, there is a lack of negative examples. Existing works circumvent this problem by making various assumptions regarding the unconsumed items, which fail to hold when the user did not consume an item because she was unaware of it. In this paper we propose Conformative Filtering (CoF) as a novel method for addressing the lack of negative examples in implicit feedback. The motivation is that if there is a large group of users who share the same taste and none of them consumed an item, then it is highly likely that the item is irrelevant to this taste. We use Hierarchical Latent Tree Analysis (HLTA) to identify taste-based user groups, and make recommendations for a user based on her memberships in the groups. Experiments on real-world datasets from different domains show that CoF has superior performance compared to other baselines and more than 10% improvement in Recall@5 and Recall@10 is observed.", "creator": "LaTeX with hyperref package"}}}