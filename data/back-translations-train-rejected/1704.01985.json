{"id": "1704.01985", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Recognizing Multi-talker Speech with Permutation Invariant Training", "abstract": "In this paper, we propose a novel technique for direct recognition of multiple speech streams given the single channel of mixed speech, without first separating them. Our technique is based on permutation invariant training (PIT) for automatic speech recognition (ASR). In PIT-ASR, we compute the average cross entropy (CE) over all frames in the whole utterance for each possible output-target assignment, pick the one with the minimum CE, and optimize for that assignment. PIT-ASR forces all the frames of the same speaker to be aligned with the same output layer. This strategy elegantly solves the label permutation problem and speaker tracing problem in one shot. Our experiments on artificially mixed AMI data showed that the proposed approach is very promising.", "histories": [["v1", "Wed, 22 Mar 2017 08:39:32 GMT  (5894kb,D)", "https://arxiv.org/abs/1704.01985v1", "5 pages, 6 figures, InterSpeech2017"], ["v2", "Sun, 4 Jun 2017 08:58:26 GMT  (3498kb,D)", "http://arxiv.org/abs/1704.01985v2", "5 pages, 6 figures, InterSpeech2017"], ["v3", "Fri, 16 Jun 2017 08:29:41 GMT  (3422kb,D)", "http://arxiv.org/abs/1704.01985v3", "5 pages, 6 figures, InterSpeech2017"], ["v4", "Mon, 19 Jun 2017 10:57:38 GMT  (3422kb,D)", "http://arxiv.org/abs/1704.01985v4", "5 pages, 6 figures, InterSpeech2017"]], "COMMENTS": "5 pages, 6 figures, InterSpeech2017", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["dong yu", "xuankai chang", "yanmin qian"], "accepted": false, "id": "1704.01985"}, "pdf": {"name": "1704.01985.pdf", "metadata": {"source": "CRF", "title": "Recognizing Multi-talker Speech with Permutation Invariant Training", "authors": ["Dong Yu", "Xuankai Chang", "Yanmin Qian"], "emails": ["dongyu@ieee.org,", "xuank@sjtu.edu.cn,", "yanminqian@sjtu.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "In recent years, we have repeatedly had to deal with problems affecting people in their countries of origin."}, {"heading": "2. Problem Setup", "text": "In this paper, we assume that a linearly mixed single microphone signal y [n] = \u2211 S s = 1 xs [n] is known, where xs [n], s = 1, \u00b7 \u00b7 \u00b7, S S S are S streams of speech sources. Our goal is to separate and recognize these streams. However, the problem of recognizing all streams is underdetermined because there is an infinite number of possible xs [n] (and thus detection results) combinations that lead to the same y [n].Fortunately, language is not a random signal. It has patterns that we can learn from a series of training pairs y and's, s = 1, \u00b7, S, where it is the transmitter marking sequence for the current. In the case of a single speaker, where S = 1, the learning problem can be cast as a simple verified optimization problem, where entering the model as a characteristic representation of the problem and simply the output of y."}, {"heading": "3. Permutation Invariant Training", "text": "To address the label of ambiguity, we suggest a novel model based on the permutation \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4. Experimental Results", "text": "To evaluate the proposed approach, a series of experiments were conducted on an artificially mixed AMI corpus, focusing on only two mixed scenarios."}, {"heading": "4.1. Experimental data", "text": "The AMI-IHM (close-talk) data are used, containing about 80 hours and 8 hours in training and evaluation sets [27, 28], and the mixed speech of two speakers is artificially generated with the sentences in the corpus. For a better and clearer definition, we defined high-energy (High E) and low-energy (Low E) speakers within each mixed speech of two speakers, thus generating five different SNR conditions (i.e. 0dB, 5dB, 10dB, 15dB, 20dB) based on the energy ratio of the two speakers. We set a rule to make the length of the selected mixed speaker pair comparable, so that most of the speech duration in this new corpus consists of two overlapping speakers. All speaker pairs are randomly selected from two different speakers, and the shorter one is padded with small noises at the front and end to make the same length as the longer speech."}, {"heading": "4.2. Baseline setup", "text": "In this thesis, all neural networks were built using the latest Microsoft Cognitive Toolkit (CNTK) [29] and the decoding systems were built on the basis of Kaldi [30]. We first followed the officially published Kaldi recipe to build an LDA-MLLT-SAT GMM-HMM model, which uses 39-dimensional MFCC features and has approximately 4K bound states and 80K Gaussians. We then used this acoustic model to generate the sensing alignment for neural network training. We trained the DNN and BLSTMRNN baseline systems with the original AMI-HM data. 80-dimensional log banking functions with CMVN were used to train the baselines. DNN layers each contain 2048 sigmoid neurons equipped with BL1 contextual windows."}, {"heading": "4.3. Evaluation on PIT-ASR models", "text": "The experimental results on the proposed PIT-ASR model are described here. All mixed data under the different SNR conditions are summarized for training. The individual senone alignments for the two speakers in each mixed speech utterance are provided by the single-speaker baseline alignment. For compatibility, the alignment of the shorter utterance within the mixed language is filled with the silence state at the front and at the end. The PIT-ASR model consists of 4 bi-directional LSTM layers with 768 memory cells in each layer, and 40-dimensional log filter bank function is used for the PIT-ASR model. The model was designed with 8 parallel expressions in the same minibatch, and the progression was truncated with the threshold of 0.0003 to guarantee training stability. Two outputs of the PIT-ASR model are both used for decoding to obtain the hypotheses for two speakers."}, {"heading": "5. Conclusion", "text": "Our experiments with artificially mixed AMI data showed that the proposed approach is very promising. There are many ways to further improve recognition accuracy. For example, we only investigated functions of the log filter bank. It is well known that a finer frequency resolution can help to better separate speech streams from each other. Furthermore, we only used acoustic information in this work. Further improvement in accuracy can be achieved by feeding information from the speech model back into the speech separation component from the decoder, and by collectively considering all speech streams in the decoding decision. Although we have discussed and evaluated our proposed approach to mixed language with a channel, the technique can be applied to multi-channel conditions and can use amusing results to achieve better results."}, {"heading": "6. References", "text": "[1] D. Yu and L. Deng, Automatic speech recognition: A deep learn-ing approach. Springer, 2014. [2] D. Yu, L. Deng, and G. E. Dahl, \"Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real world speech recognition,\" in NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning, 2010. [3] G. E. Dahl, D. Yu, L. Deng, and A. Acero, \"Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\" IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 30-42, 2012. [4] F. Seide, G. Li, and D. Yu, and D., \"Conversational speech transcription using context-deep neural networks.\""}], "references": [{"title": "Automatic speech recognition: A deep learning approach", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition", "author": ["D. Yu", "L. Deng", "G.E. Dahl"], "venue": "NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks.", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "in INTER- SPEECH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": "ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional neural networks for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "ICASSP, 2015, pp. 4580\u20134584.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional neural networks for LVCSR", "author": ["M. Bi", "Y. Qian", "K. Yu"], "venue": "INTERSPEECH, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional neural networks for noise robust speech recognition", "author": ["Y. Qian", "M. Bi", "T. Tan", "K. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional neural networks for robust speech recognition", "author": ["Y. Qian", "P.C. Woodland"], "venue": "SLT, 2016, pp. 481\u2013488.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Time-frequency convolutional networks for robust speech recognition", "author": ["V. Mitra", "H. Franco"], "venue": "ASRU, 2015, pp. 317\u2013323.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts", "author": ["V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "INTERSPEECH, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "ICASSP, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Feedforward sequential memory networks: A new structure to learn long-term dependency", "author": ["S. Zhang", "C. Liu", "H. Jiang", "S. Wei", "L. Dai", "Y. Hu"], "venue": "arXiv preprint arXiv:1512.08301, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional neural networks with layer-wise context expansion and attention", "author": ["D. Yu", "W. Xiong", "J. Droppo", "A. Stolcke", "G. Ye", "J. Li", "G. Zweig"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "arXiv preprint arXiv:1610.05256, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation", "author": ["D. Yu", "M. Kolbk", "Z.-H. Tan", "J. Jensen"], "venue": "ICASSP, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-talker speech separation and tracing with permutation invariant training of deep recurrent neural networks", "author": ["M. Kolbk", "D. Yu", "Z.-H. Tan", "J. Jensen"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, submitted, 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Factorial hidden markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine learning, vol. 29, no. 2-3, pp. 245\u2013273, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Monaural speech separation and recognition challenge", "author": ["M. Cooke", "J.R. Hershey", "S.J. Rennie"], "venue": "Computer Speech and Language, vol. 24, no. 1, pp. 1\u201315, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep neural networks for single-channel multi-talker speech recognition", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["J.R. Hershey", "Z. Chen", "J.L. Roux", "S. Watanabe"], "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process (ICASSP)., 2016, pp. 31\u201335.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Single-Channel Multi-Speaker Separation Using Deep Clustering", "author": ["Y. Isik", "J.L. Roux", "Z. Chen", "S. Watanabe", "J.R. Hershey"], "venue": "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH), 2016, pp. 545\u2013549.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep attractor network for single-microphone speaker separation", "author": ["Z. Chen", "Y. Luo", "N. Mesgarani"], "venue": "ICASSP, 2017.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Transcribing meetings with the amida systems", "author": ["T. Hain", "L. Burget", "J. Dines", "P.N. Garner", "F. Gr\u00e9zl", "A.E. Hannani", "M. Huijbregts", "M. Karafiat", "M. Lincoln", "V. Wan"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 486\u2013 498, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Proceedings of ASRU, 2013, pp. 285\u2013290.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide", "H. Wang"], "venue": "Microsoft Technical Report MSR-TR-2014\u2013112, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 1, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 2, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 3, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 4, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 5, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 6, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 7, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 8, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 9, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 10, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 11, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 12, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 13, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 14, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 15, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 16, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 17, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 18, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 19, "context": "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple\u2019s Siri and Google Now, where close-talk microphones are commonly used.", "startOffset": 62, "endOffset": 133}, {"referenceID": 20, "context": "Before the deep learning era, the most famous and effective model is the factorial GMM-HMM [21], which outperformed human in the 2006 monaural speech separation and recognition challenge [22].", "startOffset": 91, "endOffset": 95}, {"referenceID": 21, "context": "Before the deep learning era, the most famous and effective model is the factorial GMM-HMM [21], which outperformed human in the 2006 monaural speech separation and recognition challenge [22].", "startOffset": 187, "endOffset": 191}, {"referenceID": 22, "context": "[23, 24, 25, 26, 19, 20].", "startOffset": 0, "endOffset": 24}, {"referenceID": 23, "context": "[23, 24, 25, 26, 19, 20].", "startOffset": 0, "endOffset": 24}, {"referenceID": 24, "context": "[23, 24, 25, 26, 19, 20].", "startOffset": 0, "endOffset": 24}, {"referenceID": 25, "context": "[23, 24, 25, 26, 19, 20].", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "[23, 24, 25, 26, 19, 20].", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "[23, 24, 25, 26, 19, 20].", "startOffset": 0, "endOffset": 24}, {"referenceID": 22, "context": "[23] a deep learning model was developed to recognize the mixed speech directly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24, 25] the multi-talker mixed speech is first separated into multiple streams.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[24, 25] the multi-talker mixed speech is first separated into multiple streams.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "[26] proposed a similar technique called deep attractor network (DANet).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] and Kolbak et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], a simpler yet equally effective technique named permutation invariant training (PIT) was proposed to attack the speaker independent multi-talker speech separation problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Different from [19, 20], we define PIT over the cross entropy (CE) between the true and estimated senone posterior probabilities.", "startOffset": 15, "endOffset": 23}, {"referenceID": 19, "context": "Different from [19, 20], we define PIT over the cross entropy (CE) between the true and estimated senone posterior probabilities.", "startOffset": 15, "endOffset": 23}, {"referenceID": 18, "context": "Interested readers can find additional information in [19, 20] on how training progresses to nowhere when the conventional supervised approach is used for the multi-talker speech separation.", "startOffset": 54, "endOffset": 62}, {"referenceID": 19, "context": "Interested readers can find additional information in [19, 20] on how training progresses to nowhere when the conventional supervised approach is used for the multi-talker speech separation.", "startOffset": 54, "endOffset": 62}, {"referenceID": 18, "context": "To address the label ambiguity problem, we propose a novel model based on the permutation invariant training (PIT) [19, 20].", "startOffset": 115, "endOffset": 123}, {"referenceID": 19, "context": "To address the label ambiguity problem, we propose a novel model based on the permutation invariant training (PIT) [19, 20].", "startOffset": 115, "endOffset": 123}, {"referenceID": 23, "context": "Note that, DPCL [24, 25] and DANet [26] are alternative solutions to the label ambiguity problem when the goal is speech source separation.", "startOffset": 16, "endOffset": 24}, {"referenceID": 24, "context": "Note that, DPCL [24, 25] and DANet [26] are alternative solutions to the label ambiguity problem when the goal is speech source separation.", "startOffset": 16, "endOffset": 24}, {"referenceID": 25, "context": "Note that, DPCL [24, 25] and DANet [26] are alternative solutions to the label ambiguity problem when the goal is speech source separation.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "PIT [19, 20], which is originally designed for speech separation, is extended here to guarantee these properties.", "startOffset": 4, "endOffset": 12}, {"referenceID": 19, "context": "PIT [19, 20], which is originally designed for speech separation, is extended here to guarantee these properties.", "startOffset": 4, "endOffset": 12}, {"referenceID": 26, "context": "The AMI IHM (close-talk) data is used, which contains about 80 hours and 8 hours in training and evaluation sets respectively [27, 28], and the two-talker mixed speech is artificially generated with the sentences in the corpus.", "startOffset": 126, "endOffset": 134}, {"referenceID": 27, "context": "The AMI IHM (close-talk) data is used, which contains about 80 hours and 8 hours in training and evaluation sets respectively [27, 28], and the two-talker mixed speech is artificially generated with the sentences in the corpus.", "startOffset": 126, "endOffset": 134}, {"referenceID": 28, "context": "In this work, all the neural networks were built using the latest Microsoft Cognitive Toolkit (CNTK) [29] and the decoding systems were built based on Kaldi [30].", "startOffset": 101, "endOffset": 105}, {"referenceID": 29, "context": "In this work, all the neural networks were built using the latest Microsoft Cognitive Toolkit (CNTK) [29] and the decoding systems were built based on Kaldi [30].", "startOffset": 157, "endOffset": 161}, {"referenceID": 27, "context": "The performance of these two baselines on the original single-speaker AMI corpus are presented in Table 1, and they are still comparable with other works [28] even without using adapted fMLLR feature.", "startOffset": 154, "endOffset": 158}], "year": 2017, "abstractText": "In this paper, we propose a novel technique for direct recognition of multiple speech streams given the single channel of mixed speech, without first separating them. Our technique is based on permutation invariant training (PIT) for automatic speech recognition (ASR). In PIT-ASR, we compute the average cross entropy (CE) over all frames in the whole utterance for each possible output-target assignment, pick the one with the minimum CE, and optimize for that assignment. PIT-ASR forces all the frames of the same speaker to be aligned with the same output layer. This strategy elegantly solves the label permutation problem and speaker tracing problem in one shot. Our experiments on artificially mixed AMI data showed that the proposed approach is very promising.", "creator": "LaTeX with hyperref package"}}}