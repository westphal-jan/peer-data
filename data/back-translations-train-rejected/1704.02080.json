{"id": "1704.02080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Conversation Modeling on Reddit using a Graph-Structured LSTM", "abstract": "This paper presents a novel approach for modeling threaded discussions on social media using a graph-structured bidirectional LSTM which represents both hierarchical and temporal conversation structure. In experiments with a task of predicting popularity of comments in Reddit discussions, the proposed model outperforms a node-independent architecture for different sets of input features. Analyses show a benefit to the model over the full course of the discussion, improving detection in both early and late stages. Further, the use of language cues with the bidirectional tree state updates helps with identifying controversial comments.", "histories": [["v1", "Fri, 7 Apr 2017 03:27:54 GMT  (2174kb,D)", "http://arxiv.org/abs/1704.02080v1", "Submitted to TACL"]], "COMMENTS": "Submitted to TACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vicky zayats", "mari ostendorf"], "accepted": false, "id": "1704.02080"}, "pdf": {"name": "1704.02080.pdf", "metadata": {"source": "CRF", "title": "Conversation Modeling on Reddit using a Graph-Structured LSTM", "authors": ["Vicky Zayats", "Mari Ostendorf"], "emails": ["vzayats@uw.edu", "ostendor@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "2 Method", "text": "The proposed model is a bidirectional graph LSTM, which characterizes a complete discussion, starting from a tree-structured response network and taking into account the relative order of comments. Each comment in a conversation corresponds to a node in the tree, with its parent being the comment to which it responds, and its children being the responsive comments it orders over time. Each node in the tree is represented by a single recurring neural network unit (RNN), which outputs a vector (embedding) that characterizes the intermediate state of the discussion, analogous to the vector output of an RNN unit, which characterizes the word history in a sentence. In the forward direction, the state vector can be conceived as a summary of the discussion being pursued in a particular branch of the tree, while in the backward direction, the state vector summarizes the complete response tree and the structural state NNN."}, {"heading": "2.1 Graph-structured LSTM", "text": "Each node in the tree is associated with an LSTM unit. Input xt is an embedding that can contain both comment text and local submission contexts associated with the thread structure and timing described in Section 2.2. The node state ht is generated with a modification of the standard LSTM equations to include both hierarchical and temporal structures for each comment. Specifically, we use two forget-me-not gates - one for the previous (or subsequent) hierarchical level and one for the previous (or subsequent) timing level. To describe the update equations, we introduce notation for the hierarchical and temporal structure. In Figure 2, the nodes in the tree are numbered in the order in which the comments are contributed in time. To characterize the graph structure, we have the parents of t and its first child denote."}, {"heading": "2.2 Input Features", "text": "The full model contains two types of characteristics in the input vector, including non-textual characteristics associated with the submission context and textual characteristics of the comment at this node.The characteristics of the submission context are extracted from the graph and metadata associated with the comment, motivated by previous work showing that context factors such as the forum, the time and author of a post are very useful in predicting the popularity of a post. Features of the submission context include: \u2022 Timing: time since the root, time since the parent (in hours), number of subsequent comments and number of previous comments \u2022 Author: a binary indicator of whether the author is the original poster, and the number of comments made by the author in conversation \u2022 Graph location: depth of the comment (removal from the root), and number of siblings \u2022 Graph response: number of children (direct responses to the subroot, the subroot size, the subroot size, the normalization of the subxxx), the number of the faxes, the height of the subroot, the normalization of the subroot."}, {"heading": "2.3 Pruning", "text": "Often the number of comments in a single subtree is large, resulting in high training costs. A large percentage of comments are low in karma and minimally relevant to predicting neighbor karma, and many can be easily identified with simple diagram and timing functions (e.g. having no answers or contributing too late in the discussion). Therefore, we introduce a pre-processing step that identifies comments that are highly likely to have low karma to reduce computing costs. We then assign these nodes to level 0 and trim them out of the tree, but retain a number of nodes that are trimmed for use in a counter-weighted bias term in the update to capture information about the response volume. To detect low karma comments, we train a simple SVM classifier to identify comments at level 0 karma, based on the context characteristics of the submission. If a printed comment is not trimmed to a contiguous diagram (e.g., a tree is not intercepted)."}, {"heading": "2.4 Training", "text": "All model parameters are trained together with the adadelta optimization algorithm (Zeiler, 2012).Text embedding is itized using word2vec Skip-gram embedding (Mikolov et al., 2013).The code is implemented in Theano (Team et al., 2016) and is available at the following address (Gigub URL omitted for the blind).We match the model to different dimensions of the LSTM unit and use performance on the development set as a stop criterion for training."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data", "text": "Reddit2 is a popular discussion platform that consists of a large number of subreddits that focus on different topics and interests. In our study, we experimented with 3 subreddits: askwomen, ascetics and politics. All data consists of discussions that took place between January 1, 2014 and January 31, 2015. Table 1 shows the total amount of data used for each subreddit. For each subreddit, the topics were randomly distributed between training, development (dev) and test kits at a ratio of 6: 2: 2. The performance of the tree pruning classifier on the dev set is shown in Table 2."}, {"heading": "3.2 Task and evaluation metrics", "text": "Reddit Karma has a Zipfian distribution that is heavily distorted in the direction of the low karma comments. Since the rare high karma comments are of the greatest interest in the popularity forecast (Fang et al., 2016), the task is suggested to predict quantified karma (using a nonlinear rule about head-and-tail break in binning) with evaluation2reddit.com, using a macro average of F1 values to predict whether a comment exceeds any other level. Experiments reported here use this framework. Specifically, all comments with a karma smaller than 1 are assigned to level 0, and each subsequent level corresponds to karma smaller or equal to the median karma in the remaining comments based on training data statistics. Each subredit has 8 quantified karma levels based on its karma distribution. There are 7 binary subtasks (does the comment have karma at level j = 1 or equal to median karma in the remaining comments based on training data statistics), and the value metric is F1 (average)."}, {"heading": "3.3 Baseline and Contrast Systems", "text": "We compare the graph LSTM to a node-independent baseline, which is a forward-facing neural network model of input, hidden, and softmax layers. This model is a simplification of the graphLSTM model, where there is no connection between the nodes. The node-independent model characterizes a comment without reference to the text of the comment it responds to, or the comments it responds to. However, the model has information about the size of the response tree via the input contexts. Node-dependent as well as graph-structured models are trained with the same cost function and matched to the same set of hidden layer dimensions. To evaluate the role of each direction (forward or backward) in the diagram-structured model, we also present results by simply using submission context features (graph, timing, author) that represent a strong baseline."}, {"heading": "3.4 Karma Level Prediction", "text": "The results for the average F1 values on the test set are presented in Table 3. In experiments for all subreddits, graph-structured models outperform the corresponding node-independent models, both with and without language characteristics. Language characteristics also lead to greater performance gains when used in the graph-LSTM models. The fact that the forward-oriented graph improves over the interpolated models shows that it is not only the information in the current node that matters to the karma of that node that matters. While the full model outperforms the forward-oriented version for all subreddits, the gain is less than the gain achieved by the forward direction alone compared to the node-independent model, so the forward direction seems to be more important.The karma prediction results (F1 value) at the various levels are shown in Figure 3. While in askmen and askwomen subreddits the overall performance for higher levels decreases, the political subreddit trend rejects to the contrary."}, {"heading": "4 Analysis", "text": "Here we present analyses aimed at a better understanding of the behavior of the graph-structured model and the role of language in predicting it. All analyses are based on the amount of development, taking into account possible scenarios that are exceptions to the simple cases, namely: i) comments that are made early in the discussion and produce large sub-trees that are likely to have high karma, and ii) comments with small sub-trees that typically have low karma. The other two scenarios involve a sub-prediction of karma if only the context of the submission is used. One case is controversial comments that have large sub-trees but do not have high karma due to votes; these tend to have an over-prediction of karma if only the submission context is used; the other two scenarios involve a sub-prediction of karma if only the submission context is used. Early comments that are associated with a few children, and a sub-tree may benefit from lower-tree populations (see below)."}, {"heading": "4.1 Karma Prediction vs. Time", "text": "The first study looked at where the graph-LSTM offers advantages in terms of timing. We plot the average F1 value as a function of the post time in Figure 4. As an approximation for the time we use the quantified number of comments made before the current comment. The charts show that the graph-structured model improves over the node-dependent model during the discussion. Relative gains are greater towards the end of discussions where the node-independent performance is lower. A similar trend can be observed when average F1 is presented as a function of depth in the discussion tree. While the use of text in the graph-LSTM seems to be helpful throughout the discussion, we suspected that there would be several instances where it could help, and these would occur at different times. In fact, 93% of the comments that are overpredicted by the node-independent model without text by more than 2 levels, without text (controversial comments) occur in the first 20% of the discussion, which are less than 2% of the comments at the end of the discussion, but during the first half of the discussion are underestimated."}, {"heading": "4.2 Importance of Responses", "text": "To see how the model benefits from the use of linguistic cues in under-predicted and over-predicted scenarios, we look at the magnitude of errors made by the graph-LSTM model with and without text attributes. In Figure 5, the x-axis indicates the error between the actual karma level and the karma level predicted by the model using context attributes; the negative errors represent the over-predicted comments, and the positive errors represent the under-predicted comments. The y-axis represents the average error between the actual karma level and the karma level predicted by the model both through the context of the template and through language attributes. The x = y identity line does not correspond to any use of language attributes. The results are presented for the political subreddit; other subreddits have similar trends but minor differences for the under-forecast cases. We compare two models - bidirectional and forward graphically structured LSTM forms - to understand the role of the comments on the language and its responses."}, {"heading": "4.3 Language Use Analysis", "text": "To give an insight into what the model predicts about language, we looked at individual words associated with different categories of comments, as well as examples of the different cases of error. For word-level analysis, we classified words in two different ways, which in turn are associated with politics. First, we associate words in comments with zero or positive karma. For each word in the vocabulary, we calculate the probability of a single word that is zero on the level, with a simplified graph structure (one post and one comment), where all inputs were set to zero, except the comment text. The lists of positive karmas and zeros correspond to the 300 words associated with the lowest and highest probability of zero."}, {"heading": "5 Related Work", "text": "The problem of predicting popularity on social media platforms is the subject of several studies. Popularity as defined in terms of response volume is not reflected in the results for shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al., 2012) and retweets on Twitter (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict that karma will be used as popularity (Lakkaraju et al., 2013; Jaech et al., 2016). Popularity prediction is a difficult task in which many factors may play a role, which is why most previous studies have control over certain factors, including topic (Tan et al., 2014; Weninger et al., 2013), timing (Tan et al., 2014; Jaech et al., 2015), and / or comment content (Lakkaraju et al., 2013)."}, {"heading": "6 Conclusion", "text": "In summary, this paper presents a novel approach to modeling social media discussions conducted using a graph-structured bidirectional LSTM, which represents both a hierarchical and a temporal conversation structure. Dissemination of hidden state information in the graph provides a mechanism for displaying context-dependent language, including the history to which a comment responds, and the resulting discussion. Experiments on Reddit discussions show that graph-structured LSTM leads to improved results in predicting the popularity of comments compared to a node-independent model. Analyses show that the model favors predictions about the extent of the discussion and that linguistic cues are particularly important in distinguishing controversial comments from those that are received very positively. Responses from a small number of comments seem to be useful, so it is likely that the bi-directional model would still be useful, with a short-term outlook on predictions of popularity of populations we may anticipate from other platforms, and we may anticipate the popularity of certain types of speech used in the model."}, {"heading": "Acknowledgments", "text": "Omitted for blind review."}], "references": [{"title": "Modeling a retweet network via an adaptive bayesian approach", "author": ["Bin Bi", "Junghoo Cho."], "venue": "Proc. WWW.", "citeRegEx": "Bi and Cho.,? 2016", "shortCiteRegEx": "Bi and Cho.", "year": 2016}, {"title": "Can cascades be predicted? In Proc", "author": ["Justin Cheng", "Lada Adamic", "P Alex Dow", "Jon Michael Kleinberg", "Jure Leskovec."], "venue": "WWW.", "citeRegEx": "Cheng et al\\.,? 2014", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proc. NAACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Learning latent local conversation modes for predicting community endorsement in online discussions", "author": ["Hao Fang", "Hao Cheng", "Mari Ostendorf."], "venue": "Proc. Inter. Workshop on NLP for Social Media.", "citeRegEx": "Fang et al\\.,? 2016", "shortCiteRegEx": "Fang et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with a combinatorial action space for predicting popular Reddit threads", "author": ["Ji He", "Mari Ostendorf", "Xiaodong He", "Jianshu Chen", "Lihong Gao", "Jianfeng andLi", "Li Deng."], "venue": "Proc. EMNLP.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Talking to the crowd: What do people react to in online discussions? In Proc", "author": ["Aaron Jaech", "Victoria Zayats", "Hao Fang", "Mari Ostendorf", "Hannaneh Hajishirzi."], "venue": "EMNLP.", "citeRegEx": "Jaech et al\\.,? 2015", "shortCiteRegEx": "Jaech et al\\.", "year": 2015}, {"title": "What\u2019s in a name? understanding the interplay between titles, content, and communities in social media", "author": ["Himabindu Lakkaraju", "Julian J McAuley", "Jure Leskovec."], "venue": "Proc. ICWSM.", "citeRegEx": "Lakkaraju et al\\.,? 2013", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2013}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proc. Joint Conf. on Lexical and Computational Semantics.", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proc. ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "Proc. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "The effect of wording on message propagation: Topic-and author-controlled natural experiments on twitter", "author": ["Chenhao Tan", "Lillian Lee", "Bo Pang."], "venue": "Proc. ACL, pages 175\u2013186.", "citeRegEx": "Tan et al\\.,? 2014", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Theano: A python framework for fast computa", "author": ["The Theano Development Team", "Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov"], "venue": null, "citeRegEx": "Team et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Team et al\\.", "year": 2016}, {"title": "Characterizing the language of online communities and its relation to community recognition", "author": ["Trang Tran", "Mari Ostendorf."], "venue": "Proc. EMNLP.", "citeRegEx": "Tran and Ostendorf.,? 2016", "shortCiteRegEx": "Tran and Ostendorf.", "year": 2016}, {"title": "An exploration of discussion threads in social news sites: A case study of the Reddit community", "author": ["Tim Weninger", "Xihao Avi Zhu", "Jiawei Han."], "venue": "Proc. IEEE/ACM Inter. Conf. on Advances in Social Networks Analysis and Mining.", "citeRegEx": "Weninger et al\\.,? 2013", "shortCiteRegEx": "Weninger et al\\.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Top-down tree long short-term memory networks", "author": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata."], "venue": "Proc. NAACL-HLT.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Seismic: A self-exciting point process model for predicting tweet popularity", "author": ["Qingyuan Zhao", "Murat A Erdogdu", "Hera Y He", "Anand Rajaraman", "Jure Leskovec."], "venue": "Proc. SIGKDD International Conference on Knowledge Discovery and Data Mining.", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Long short-term memory over recursive structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "Proc. ICML.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "This definition of popularity, which has also been called community endorsement (Fang et al., 2016), is the task of interest in our work on tree-structured modeling of disar X iv :1 70 4.", "startOffset": 80, "endOffset": 99}, {"referenceID": 6, "context": "Previous studies found that the time when the comment/post was published has a big impact on its popularity (Lakkaraju et al., 2013).", "startOffset": 108, "endOffset": 132}, {"referenceID": 12, "context": "Indeed, community style matching is shown to be correlated to comment popularity in Reddit (Tran and Ostendorf, 2016).", "startOffset": 91, "endOffset": 117}, {"referenceID": 6, "context": "Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015).", "startOffset": 102, "endOffset": 164}, {"referenceID": 10, "context": "Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015).", "startOffset": 102, "endOffset": 164}, {"referenceID": 5, "context": "Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015).", "startOffset": 102, "endOffset": 164}, {"referenceID": 3, "context": "structure and timing are important in predicting popularity (Fang et al., 2016), the LSTM units include both hierachical and temporal components to the update, which distinguishes this work from prior treestructured LSTM models.", "startOffset": 60, "endOffset": 79}, {"referenceID": 3, "context": "As in (Fang et al., 2016), but unlike other work (He et al.", "startOffset": 6, "endOffset": 25}, {"referenceID": 4, "context": ", 2016), but unlike other work (He et al., 2016), our model makes use of the full discussion thread in predicting popularity.", "startOffset": 31, "endOffset": 48}, {"referenceID": 3, "context": "(Fang et al., 2016).", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "All model parameters are jointly trained using the adadelta optimization algorithm (Zeiler, 2012).", "startOffset": 83, "endOffset": 97}, {"referenceID": 8, "context": "(Mikolov et al., 2013) trained on all comments from the corresponding subreddit.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "The code is implemented in Theano (Team et al., 2016) and is available at (github URL omitted for blind reviewing).", "startOffset": 34, "endOffset": 53}, {"referenceID": 3, "context": "Since the rare high karma comments are of greatest interest in popularity prediction, (Fang et al., 2016) proposes a task of predicting quantized karma (using a nonlinear head-tail break rule for binning) with evaluation", "startOffset": 86, "endOffset": 105}, {"referenceID": 3, "context": "As shown in (Fang et al., 2016), simply using submission context features (graph, timing, author) gives a strong baseline.", "startOffset": 12, "endOffset": 31}, {"referenceID": 3, "context": "This may be due in part to the lower pruning recall in the politics subreddit, but (Fang et al., 2016) also observe higher performance for high karma levels in the politics subreddit.", "startOffset": 83, "endOffset": 102}, {"referenceID": 1, "context": "Popularity as defined in terms of volume of response has been explored for shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al.", "startOffset": 94, "endOffset": 114}, {"referenceID": 10, "context": ", 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016).", "startOffset": 29, "endOffset": 84}, {"referenceID": 16, "context": ", 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016).", "startOffset": 29, "endOffset": 84}, {"referenceID": 0, "context": ", 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016).", "startOffset": 29, "endOffset": 84}, {"referenceID": 6, "context": "Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al.", "startOffset": 46, "endOffset": 107}, {"referenceID": 5, "context": "Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al.", "startOffset": 46, "endOffset": 107}, {"referenceID": 4, "context": "Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al.", "startOffset": 46, "endOffset": 107}, {"referenceID": 3, "context": ", 2016) or as community endorsement (Fang et al., 2016).", "startOffset": 36, "endOffset": 55}, {"referenceID": 10, "context": "Popularity prediction is a difficult task where many factors can play a role, which is why most prior studies control for specific factors, including topic (Tan et al., 2014; Weninger et al., 2013), timing (Tan et al.", "startOffset": 156, "endOffset": 197}, {"referenceID": 13, "context": "Popularity prediction is a difficult task where many factors can play a role, which is why most prior studies control for specific factors, including topic (Tan et al., 2014; Weninger et al., 2013), timing (Tan et al.", "startOffset": 156, "endOffset": 197}, {"referenceID": 10, "context": ", 2013), timing (Tan et al., 2014; Jaech et al., 2015), and/or comment content (Lakkaraju et al.", "startOffset": 16, "endOffset": 54}, {"referenceID": 5, "context": ", 2013), timing (Tan et al., 2014; Jaech et al., 2015), and/or comment content (Lakkaraju et al.", "startOffset": 16, "endOffset": 54}, {"referenceID": 6, "context": ", 2015), and/or comment content (Lakkaraju et al., 2013).", "startOffset": 32, "endOffset": 56}, {"referenceID": 0, "context": "Studies that do not include such constraints have looked at Twitter retweets (Bi and Cho, 2016) and Reddit karma (He et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 4, "context": "Studies that do not include such constraints have looked at Twitter retweets (Bi and Cho, 2016) and Reddit karma (He et al., 2016; Fang et al., 2016).", "startOffset": 113, "endOffset": 149}, {"referenceID": 3, "context": "Studies that do not include such constraints have looked at Twitter retweets (Bi and Cho, 2016) and Reddit karma (He et al., 2016; Fang et al., 2016).", "startOffset": 113, "endOffset": 149}, {"referenceID": 4, "context": "The work in (He et al., 2016) uses reinforcement learning to identify popular threads to track given the past comment history, so it is learning language cues relevant to high karma but it does not explicitly predict karma.", "startOffset": 12, "endOffset": 29}, {"referenceID": 3, "context": "(Fang et al., 2016).", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Tree LSTMs are a modification of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015).", "startOffset": 116, "endOffset": 194}, {"referenceID": 17, "context": "Tree LSTMs are a modification of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015).", "startOffset": 116, "endOffset": 194}, {"referenceID": 15, "context": "Tree LSTMs are a modification of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015).", "startOffset": 116, "endOffset": 194}, {"referenceID": 7, "context": "Tree LSTMs are a modification of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015).", "startOffset": 116, "endOffset": 194}, {"referenceID": 9, "context": "Some options include summarizing over the children, adding a separate forget gate for each child (Tai et al., 2015), recurrent propagation among siblings (Zhang et al.", "startOffset": 97, "endOffset": 115}, {"referenceID": 15, "context": ", 2015), recurrent propagation among siblings (Zhang et al., 2016), or use of stack LSTMs (Dyer et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 2, "context": ", 2016), or use of stack LSTMs (Dyer et al., 2015).", "startOffset": 31, "endOffset": 50}], "year": 2017, "abstractText": "This paper presents a novel approach for modeling threaded discussions on social media using a graph-structured bidirectional LSTM which represents both hierarchical and temporal conversation structure. In experiments with a task of predicting popularity of comments in Reddit discussions, the proposed model outperforms a node-independent architecture for different sets of input features. Analyses show a benefit to the model over the full course of the discussion, improving detection in both early and late stages. Further, the use of language cues with the bidirectional tree state updates helps with identifying controversial comments.", "creator": "LaTeX with hyperref package"}}}