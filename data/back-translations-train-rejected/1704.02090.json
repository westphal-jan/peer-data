{"id": "1704.02090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Conceptualization Topic Modeling", "abstract": "Recently, topic modeling has been widely used to discover the abstract topics in text corpora. Most of the existing topic models are based on the assumption of three-layer hierarchical Bayesian structure, i.e. each document is modeled as a probability distribution over topics, and each topic is a probability distribution over words. However, the assumption is not optimal. Intuitively, it's more reasonable to assume that each topic is a probability distribution over concepts, and then each concept is a probability distribution over words, i.e. adding a latent concept layer between topic layer and word layer in traditional three-layer assumption. In this paper, we verify the proposed assumption by incorporating the new assumption in two representative topic models, and obtain two novel topic models. Extensive experiments were conducted among the proposed models and corresponding baselines, and the results show that the proposed models significantly outperform the baselines in terms of case study and perplexity, which means the new assumption is more reasonable than traditional one.", "histories": [["v1", "Fri, 7 Apr 2017 05:12:38 GMT  (1481kb,D)", "http://arxiv.org/abs/1704.02090v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yi-kun tang", "xian-ling mao", "heyan huang", "guihua wen"], "accepted": false, "id": "1704.02090"}, "pdf": {"name": "1704.02090.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yi-Kun Tang", "Xian-Ling Mao", "Heyan Huang", "Guihua Wen"], "emails": ["hhy63}@bit.edu.cn", "crghwen@scut.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In recent years, the number of those who are able to survive has multiplied; in recent years, the number of those who are able to survive has multiplied; in recent years, the number of those who are able to survive has multiplied; in recent years, the number of those who are able to survive has multiplied; in recent years, the number of those who are able to survive has multiplied; and in recent years, the number of those who are able to survive has increased."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Topic Modeling", "text": "The existing models are divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts.Unsupervised non-hierarchical topic models are wide study, as LDA [Lead et al., 2003b], Probase-LDA [Yao et al., 2015], TCC [Jayabharathy et al., 2014] and COT [Yao et al., 2016] etc. The most famous is the latent Dirichlet Allocation (LDA). LDA is similar to pLSA [Hofmann, 1999], except that in LDA the distribution of topics as Dirichlet priorities.However, the above models cannot capture the relationship between super- and subtopics."}, {"heading": "2.2 Concept Knowledge Base", "text": "It is therefore easy for mankind to acquire the meaning of an article and to extract the topics of the article, because there is a certain conceptualized knowledge base in a brain. For example, if you see a sentence: \"Microsoft has announced a project called Microsoft Azure Information Protection,\" a human being will never confuse Microsoft as a person or other things, because we know that Microsoft is a concept about software companies. However, machines cannot conceptualize what they are reading, which is a great challenge for machines to understand natural language.Concept knowledge base is a kind of knowledge base that uses taxonomies and ontologies to obtain concepts and extract the relationships between instances and concepts. Therefore, concept knowledge base is a kind of tool to let machines understand natural language.There are many existing concept knowledge databases, such as Probase [Wang et al., 2015; Wu et al., 2012], Freebase and WordNet, etc. Among them, Probase is a type of conceptual language, which many conceptual machines use."}, {"heading": "3 Conceptualization Topic Modeling", "text": "In this section, we show how to integrate the four-level adoption into unattended and monitored topic models to verify the effectiveness of the new adoption. For unattended topic modeling, we choose LDA as the manipulating object because it is the basic component of most existing topic models. For supervised topic modeling, we choose Labeled LDA [Ramage et al., 2009] because it is one of the most representative monitored models."}, {"heading": "3.1 Conceptualization LDA", "text": "In order to integrate the four-layer assumption into LDA, we propose a novel topic model called \u03b2 Conceptualization LDA (CLDA). It models each document as a mixture of underlying topics. Unlike existing topic models, CLDA assumes that each topic is a distribution over concepts and not directly over words, and considers concepts as distributions over words. In addition, the distribution of a concept over words from the concept knowledge base is acquired that are not included in the dictionary of the concept knowledge base, they are considered new concepts. In other words, we define these neologisms as atomic concepts. In CLDA, the distribution of a concept over words from the concept knowledge base, Probase. The graphic model of CLDA is shown in Figure 2.In CLDA, each document consists of a group of words represented as w (d) (w1,..., wNd)."}, {"heading": "3.2 Conceptualization Labeled LDA", "text": "The proposed four-layer Bayesian assumption can be used in most existing topic models, and we have shown that the assumption can be used in the unattended topic model, i.e. in the LDA. In this section, we will further demonstrate the use of the assumption in the supervised topic modeling. The novel model is referred to as conceptualization labeled latent dirichlet allocation (CLLDA) (Ramage et al., 2009), which is a classically supervised topic model, is expanded by the inclusion of conceptualization assumptions. Described latent dirichlet allocation (CLLDA). Labeled LDA allocation (LDA) is very similar to the LDA. Labeled LDA descriptions assume that the topics of each document are limited to their labels. The topic distribution of each document in the Described LDA is generated from a dirichlet distribution whose dimension of the previous parameter is the same as the number of each document."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experiment Setting", "text": "We conducted the experiments with two real-world datasets, one of which, called Conf, contains 2,317 full essays on four conferences (CIKM, SIGIR, SIGKDD and WWW) over three years (2011-2013), and the other, called AP, is a public dataset containing more than 106K full Associated Press articles published in 1989. Both raw datasets contain more than 2 million Probase concepts, which is much larger than the vocabulary. It leads to an imbalance between concept size and vocabulary. In addition, many concepts in Probase Concept Graph are similar to each other, which are associated with the same word. For example, the word microsoft associates concept companies, concept software companies and concept technology companies, which are semantically similar. To solve this problem, we use the concept cluster results provided by Probase to reduce the number of concepts.Overall, Conf includes all of the Conceptual Companies, 4,819 and 12GB."}, {"heading": "4.2 Experiments for CLDA", "text": "USA, USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "4.3 Experiments for CLLDA", "text": "In this experiment, we train CLLDA and LLDA on the Conf dataset. The keywords of each paper are used as labels on the corresponding paper, and the number of labels for the entire dataset is 4760. Case study Table 4 shows top ten words and concepts from five topics we learned at the Conf, where we can easily acquire the concepts under topics. From Table 4, we can learn that concepts are very important for understanding a document. Therefore, we know from the results that the proposed model works better than the baseline, which means that our conceptualization method for modeling topics sounds good, and our assumption is more reasonable that terms like social, user, media are related to these concepts."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a new assumption of a hierarchical, four-layered Bayesian structure for theme modeling, which adds a latent concept layer between theme layer and word layer in the traditional assumption. To verify the effectiveness of the new assumption, we apply this assumption in two representative topic models (LDA and LLDA). Extensive experiments have been conducted on two real data sets. Experimental results show that the proposed assumption works better than the traditional assumption. In the future, we will verify the new, four-layered assumption in more topic models over more data sets. We will also investigate the use of the new assumption in multimedia data such as images and videos."}], "references": [{"title": "Supervised topic models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "Proceeding of the Neural Information Processing Systems(nips)", "citeRegEx": "Blei and McAuliffe. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["David M Blei", "Michael I Jordan", "Thomas L Griffiths", "Joshua B Tenenbaum"], "venue": "pages 17\u201324,", "citeRegEx": "Blei et al.. 2003a", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research, 3:993\u20131022", "citeRegEx": "Blei et al.. 2003b", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D. Blei", "T.L. Griffiths", "M.I. Jordan", "J.B. Tenenbaum"], "venue": "Advances in neural information processing systems, 16:106", "citeRegEx": "Blei et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In AAAI", "author": ["Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji. A novel neural topic model", "its supervised extension"], "venue": "pages 2210\u20132216,", "citeRegEx": "Cao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science, 41(6):391\u2013407", "citeRegEx": "Deerwester et al.. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proc. of Uncertainty in Artificial Intelligence, UAI\u201999, page 21. Citeseer", "citeRegEx": "Hofmann. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Correlated concept based topic updation model for dynamic corpora", "author": ["J Jayabharathy", "S Kanmani", "N Sivaranjani"], "venue": "International Journal of Computer Applications, 89(10):1\u20137", "citeRegEx": "Jayabharathy et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Political issue extraction model: A novel hierarchical topic model that uses tweets by political and non-political authors", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark Carman"], "venue": "Proceedings of NAACL-HLT, pages 82\u201390,", "citeRegEx": "Joshi et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Dolda-a regularized supervised topic model for high-dimensional multi-class regression", "author": ["M\u00e5ns Magnusson", "Leif Jonsson", "Mattias Villani"], "venue": "arXiv preprint arXiv:1602.00260,", "citeRegEx": "Magnusson et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sshlda: a semi-supervised hierarchical topic model", "author": ["Xianling Mao", "Zhaoyan Ming", "Tatseng Chua", "Si Li", "Hongfei Yan", "Xiaoming Li"], "venue": "pages 800\u2013809,", "citeRegEx": "Mao et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Ehllda: A supervised hierarchical topic model", "author": ["Xian-Ling Mao", "Yixuan Xiao", "Qiang Zhou", "Jun Wang", "Heyan Huang"], "venue": "Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, pages 215\u2013226. Springer,", "citeRegEx": "Mao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Jon D Mcauliffe", "David M Blei. Supervised topic models"], "venue": "pages 121\u2013128,", "citeRegEx": "Mcauliffe and Blei. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Mixtures of hierarchical topics with pachinko allocation", "author": ["David Mimno", "Wei Li", "Andrew Mccallum"], "venue": "pages 633\u2013640,", "citeRegEx": "Mimno et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Journal of Child Language", "author": ["Gregory L Murphy. The big book of concepts"], "venue": "31(1):247\u2013253,", "citeRegEx": "Murphy. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchically supervised latent dirichlet allocation", "author": ["A. Perotte", "N. Bartlett", "N. Elhadad", "F. Wood"], "venue": "Neural Information Processing Systems (to appear)", "citeRegEx": "Perotte et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics", "author": ["Yves Petinot", "Kathleen McKeown", "Kapil Thadani. A hierarchical model of web summaries"], "venue": "pages 670\u2013675. ACL,", "citeRegEx": "Petinot et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["Ramage et al", "2009] D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Partially labeled topic models for interpretable text mining", "author": ["D. Ramage", "C.D. Manning", "S. Dumais"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457\u2013465. ACM", "citeRegEx": "Ramage et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical topic models for multi-label document classification", "author": ["T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Arxiv preprint arXiv:1107.2462", "citeRegEx": "Rubin et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Guided htm: Hierarchical topic model with dirichlet forest priors", "author": ["Su-Jin Shin", "IL CHUL MOON"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Shin and MOON. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Journal of the American Statistical Association", "author": ["Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei. Hierarchical dirichlet processes"], "venue": "101(476):1566\u20131581,", "citeRegEx": "Teh et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "An inference approach to basic level of categorization", "author": ["Zhongyuan Wang", "Haixun Wang", "Jirong Wen", "Yanghua Xiao"], "venue": "pages 653\u2013662,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Probase: a probabilistic taxonomy for text understanding", "author": ["Wentao Wu", "Hongsong Li", "Haixun Wang", "Kenny Q Zhu"], "venue": "pages 481\u2013492,", "citeRegEx": "Wu et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating probabilistic knowledge into topic models", "author": ["Liang Yao", "Yin Zhang", "Baogang Wei", "Hongze Qian", "Yibing Wang"], "venue": "pages 586\u2013597,", "citeRegEx": "Yao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Concept over time: the combination of probabilistic topic model with wikipedia knowledge", "author": ["Liang Yao", "Yin Zhang", "Baogang Wei", "Lei Li", "Fei Wu", "Peng Zhang", "Yali Bian"], "venue": "Expert Systems With Applications, 60:27\u201338,", "citeRegEx": "Yao et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of the IEEE International Conference on Computer Vision Workshops", "author": ["Cheng Zhang", "Carl Ek", "Xavi Gratal", "Florian Pokorny", "Hedvig Kjellstrom. Supervised hierarchical dirichlet processes with variational inference"], "venue": "pages 254\u2013261,", "citeRegEx": "Zhang et al.. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 6, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 2, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 25, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 1, "context": ", 2016], Unsupervised hierarchical topic models [Blei et al., 2003a; Teh et al., 2006; Joshi et al., 2016], and their corresponding supervised counterparts [Ramage et al.", "startOffset": 48, "endOffset": 106}, {"referenceID": 21, "context": ", 2016], Unsupervised hierarchical topic models [Blei et al., 2003a; Teh et al., 2006; Joshi et al., 2016], and their corresponding supervised counterparts [Ramage et al.", "startOffset": 48, "endOffset": 106}, {"referenceID": 8, "context": ", 2016], Unsupervised hierarchical topic models [Blei et al., 2003a; Teh et al., 2006; Joshi et al., 2016], and their corresponding supervised counterparts [Ramage et al.", "startOffset": 48, "endOffset": 106}, {"referenceID": 10, "context": ", 2016], and their corresponding supervised counterparts [Ramage et al., 2009; Mao et al., 2012; Magnusson et al., 2016].", "startOffset": 57, "endOffset": 120}, {"referenceID": 9, "context": ", 2016], and their corresponding supervised counterparts [Ramage et al., 2009; Mao et al., 2012; Magnusson et al., 2016].", "startOffset": 57, "endOffset": 120}, {"referenceID": 14, "context": "Concepts can also help people better understand knowledge, as psychologist Gregory Murphy wrote: \u201dConcepts are the glue that holds our mental world together\u201d [Murphy, 2004].", "startOffset": 158, "endOffset": 172}, {"referenceID": 2, "context": "As we known, Latent Dirichlet Allocation (LDA) [Blei et al., 2003b] is the beginning of topic modeling, and is the most important component in all kinds of topic models.", "startOffset": 47, "endOffset": 67}, {"referenceID": 23, "context": "The distribution of each concept over words in our models can be obtained from Probase knowledge base [Wu et al., 2012], which is a universal probabilistic taxonomy concept knowledge base.", "startOffset": 102, "endOffset": 119}, {"referenceID": 2, "context": "Unsupervised non-hierarchical topic models are widely studied, such as LDA [Blei et al., 2003b], Probase-LDA [Yao et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 24, "context": ", 2003b], Probase-LDA [Yao et al., 2015] , TCC [Jayabharathy et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 7, "context": ", 2015] , TCC [Jayabharathy et al., 2014] and COT [Yao et al.", "startOffset": 14, "endOffset": 41}, {"referenceID": 25, "context": ", 2014] and COT [Yao et al., 2016] etc.", "startOffset": 16, "endOffset": 34}, {"referenceID": 6, "context": "LDA is similar to pLSA [Hofmann, 1999], except that in LDA the topic distribution is assumed to have a Dirichlet prior.", "startOffset": 23, "endOffset": 38}, {"referenceID": 3, "context": "To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) [Blei et al., 2004], Hierarchical Dirichlet processes (HDP) [Teh et al.", "startOffset": 112, "endOffset": 131}, {"referenceID": 21, "context": ", 2004], Hierarchical Dirichlet processes (HDP) [Teh et al., 2006], Hierarchical PAM (HPAM) [Mimno et al.", "startOffset": 48, "endOffset": 66}, {"referenceID": 13, "context": ", 2006], Hierarchical PAM (HPAM) [Mimno et al., 2007], PIE [Joshi et al.", "startOffset": 33, "endOffset": 53}, {"referenceID": 8, "context": ", 2007], PIE [Joshi et al., 2016] and Guided HTM [Shin and MOON, 2016] etc.", "startOffset": 13, "endOffset": 33}, {"referenceID": 20, "context": ", 2016] and Guided HTM [Shin and MOON, 2016] etc.", "startOffset": 23, "endOffset": 44}, {"referenceID": 0, "context": "Several modifications of LDA to incorporate supervision have been proposed in the literature, such as Supervised LDA [Blei and McAuliffe, 2007; Mcauliffe and Blei, 2008], Prior-LDA [Rubin et al.", "startOffset": 117, "endOffset": 169}, {"referenceID": 12, "context": "Several modifications of LDA to incorporate supervision have been proposed in the literature, such as Supervised LDA [Blei and McAuliffe, 2007; Mcauliffe and Blei, 2008], Prior-LDA [Rubin et al.", "startOffset": 117, "endOffset": 169}, {"referenceID": 19, "context": "Several modifications of LDA to incorporate supervision have been proposed in the literature, such as Supervised LDA [Blei and McAuliffe, 2007; Mcauliffe and Blei, 2008], Prior-LDA [Rubin et al., 2011], Partially LDA (PLDA) [Ramage et al.", "startOffset": 181, "endOffset": 201}, {"referenceID": 18, "context": ", 2011], Partially LDA (PLDA) [Ramage et al., 2011], NTM [Cao et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": ", 2011], NTM [Cao et al., 2015] and DOLDA [Magnusson et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 9, "context": ", 2015] and DOLDA [Magnusson et al., 2016] etc.", "startOffset": 18, "endOffset": 42}, {"referenceID": 16, "context": "Lots of models, such as hLLDA [Petinot et al., 2011], HSLDA [Perotte et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 15, "context": ", 2011], HSLDA [Perotte et al., 2011], SSHLLDA [Mao et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 10, "context": ", 2011], SSHLLDA [Mao et al., 2012], SHDP [Zhang et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 26, "context": ", 2012], SHDP [Zhang et al., 2013] and EHLLDA [Mao et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 11, "context": ", 2013] and EHLLDA [Mao et al., 2015], have been proposed to solve the problem.", "startOffset": 19, "endOffset": 37}, {"referenceID": 22, "context": "There are many existing concept knowledge bases, such as Probase [Wang et al., 2015; Wu et al., 2012], Freebase and WordNet etc.", "startOffset": 65, "endOffset": 101}, {"referenceID": 23, "context": "There are many existing concept knowledge bases, such as Probase [Wang et al., 2015; Wu et al., 2012], Freebase and WordNet etc.", "startOffset": 65, "endOffset": 101}, {"referenceID": 22, "context": "Therefore, in this paper, we use Probase API [Wang et al., 2015] to get the probability distribution of each concept over words.", "startOffset": 45, "endOffset": 64}], "year": 2017, "abstractText": "Recently, topic modeling has been widely used to discover the abstract topics in text corpora. Most of the existing topic models are based on the assumption of three-layer hierarchical Bayesian structure, i.e. each document is modeled as a probability distribution over topics, and each topic is a probability distribution over words. However, the assumption is not optimal. Intuitively, it\u2019s more reasonable to assume that each topic is a probability distribution over concepts, and then each concept is a probability distribution over words, i.e. adding a latent concept layer between topic layer and word layer in traditional three-layer assumption. In this paper, we verify the proposed assumption by incorporating the new assumption in two representative topic models, and obtain two novel topic models. Extensive experiments were conducted among the proposed models and corresponding baselines, and the results show that the proposed models significantly outperform the baselines in terms of case study and perplexity, which means the new assumption is more reasonable than traditional one.", "creator": "LaTeX with hyperref package"}}}