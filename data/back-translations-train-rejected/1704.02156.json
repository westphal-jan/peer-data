{"id": "1704.02156", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing", "abstract": "We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline model. The overall accuracy however is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.", "histories": [["v1", "Fri, 7 Apr 2017 09:37:36 GMT  (59kb,D)", "https://arxiv.org/abs/1704.02156v1", "To appear in Proceedings of SemEval, 2017"], ["v2", "Wed, 19 Apr 2017 16:03:45 GMT  (60kb,D)", "http://arxiv.org/abs/1704.02156v2", "To appear in Proceedings of SemEval, 2017 (camera-ready)"]], "COMMENTS": "To appear in Proceedings of SemEval, 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rik van noord", "johan bos"], "accepted": false, "id": "1704.02156"}, "pdf": {"name": "1704.02156.pdf", "metadata": {"source": "CRF", "title": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing", "authors": ["Rik van Noord", "Johan Bos"], "emails": ["r.i.k.van.noord@rug.nl", "johan.bos@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "Traditional semantic parsers often use statistical syntactic parsers to derive a syntactic structure on which a meaning representation can be built. Recently, there have been interesting attempts to consider semantic parsing as a translation task and to map a source language (here: English) to a target language (a logical form of some kind). Dong and Lapata (2016) used sequence sequence sequence and sequence-to-tree models of neural translation to generate logical forms from sentences, while Barzdins and Gosko (2016) and Peng et al. (2017) used a similar method for generating AMRs. From a purely engineering point of view, these are interesting attempts to avoid complex models of the semantic parsing process. However, little is known about the performance and fine-tuning of such parsers and whether they can achieve the performance of traditional semantic parsers or whether they can contribute to performance in an ensemble setting, as we strive to achieve these results in the context of EASK 2017."}, {"heading": "2 Neural Semantic Parsing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Datasets", "text": "Our training set consists of the second LDC AMR release (LDC2016E25), which contains 39,620 AMR, and the training set of the Bio AMR Corpus, which contains 5,452 AMR. We use the designated development and test partition of the Bio AMR Corpus, both containing 500 AMR. HTML tags are removed from the sets."}, {"heading": "2.2 Basic Translation Model", "text": "We use a seq2seq neural translation model to translate English sentences into AMRs. This is a Bi-LSTM model with additional attention mechanism, as described in Bahdanau et al. (2014). Similar to Barzdins and Gosko (2016), but unlike Peng et al. (2017), we train the model only at the drawing level. Model specific details are in Table 1. In a pre-processing step, we remove all variables from the AMR and duplicate koreferencing nodes. An example of this is in Figure 1. The variables and koreferencing nodes are restored after the test, using the recovery script from mar Xiv: 170 4.02 156v 2 [cs.C L] 19 Apr 201 7Barzdins and Gosko (2016).1."}, {"heading": "2.3 Improvements", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcnlrVo"}, {"heading": "2.4 CAMR ensemble", "text": "Knowing that our neural semantic parser is unlikely to surpass, but will complement, a modern AMR parser, our strategy is to use an ensemble-based approach, consisting of the commercially available CAMR parser (Wang et al., 2015) and our neural semantic parser, which is similar to Barzdin's and Gosko's (2016) implementation, selecting the AMR that achieves the highest paired match value over the other sets of AMRs generated for one set (Cai and Knight, 2013). Since CAMR is not deterministic, we can ultimately train multiple models on the same dataset. The best ensemble on the test data consisted exclusively of three biomedical models, two bio + DC models and one DC set."}, {"heading": "3 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Results on Test Set", "text": "Table 2 shows the results of all the improvement methods tested in isolation on the test set of biomedical data. Increasing the amount of data helps only slightly, while the supercharacters are responsible for the largest increase in performance. This shows that in our models we do not necessarily only need to use letter or word input, but that a combination of the two could be optimal. The best result was achieved by combining the different methods, and this model was then used to parse the assessment data. Table 3 shows the results of the retraining of CAMR to different data sets and an ensemble of these models. Adding our seq2seq model to the ensemble only yielded very little increase in performance."}, {"heading": "3.2 Official Results", "text": "In Table 4, we see the detailed results of the best seq2seq model and the best ensemble on the evaluation data using the scripts from Damonte et al. (2017).3 While CAMR has similar values for the 3Unofficial Score for seq2seq negation; due to an error, all: polarity nodes were removed in the official template. This did not affect the final F-Score. Test data, but the score of the seq2seq model is reduced by 0.04. Interestingly, seq2seq performs equally well without any literal clarification, while there is no separate module to deal with it."}, {"heading": "3.3 Comparison with CAMR", "text": "Although CAMR far exceeded our neural semantic parser per sentence length, the seq-to-seq model resulted in a better AMR, based on the smatch value, for 108 of the 500 AMRs evaluated. If the CAMR + seq2seq ensemble was somehow able to always select the best AMR, it received an F score of 0.601, an increase of 2.2% instead of the current 0.2%. This suggests that the current method of combining neural semantic parsers with existing parsers is far from optimal, but that neural methods provide complementary information. Another way to include this information would be to select the most appropriate parser based on the input set. A classifier that uses the properties of the set could be trained to assign a parser to each individual (to be parsed) sentence. Figure 3 shows the performance of the neural semantic parser and the CAMR ensemble per sentence length of the set length, but we can see that CAMR is significantly higher when the sentence length is longer."}, {"heading": "4 Conclusion and Future Work", "text": "We were able to reproduce the results of the character level models for neural semantic parsers, as proposed by Barzdins and Gosko (2016). In addition, we showed an improvement in their basic setting by using data augmentation, part of the language as an additional input and use of supercharacters. The latter setting showed that a combination of character and word input could be optimal for neural semantic parsers. Despite these improvements, the resulting AMR parser still has not produced any significant improvements in overall performance. Do these results indicate that neural semantic parsers will never compete with more traditional statistical parsers? We don't think so. We feel that we have just scratched the surface of the possibilities that neural setic parsers can offer us, and how they may complement these strategies."}, {"heading": "Acknowledgements", "text": "We thank the two anonymous reviewers for their comments and the Centre for Information Technology of the University of Groningen for their support and providing access to the Peregrine supercomputer cluster. We also used a Tesla K40 GPU, kindly donated by the NVIDIA Corporation. This work was financed by the NWO-VICI scholarship \"Lost in Translation - Found in Meaning\" (288-89-003)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on amr parsing accuracy", "author": ["Guntis Barzdins", "Didzis Gosko."], "venue": "Proceedings of the 10th International Workshop on Semantic Evalu-", "citeRegEx": "Barzdins and Gosko.,? 2016", "shortCiteRegEx": "Barzdins and Gosko.", "year": 2016}, {"title": "The meaning factory at semeval-2016 task 8: Producing amrs with boxer", "author": ["Johannes Bjerva", "Johan Bos", "Hessel Haagsma."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Com-", "citeRegEx": "Bjerva et al\\.,? 2016", "shortCiteRegEx": "Bjerva et al\\.", "year": 2016}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "Bootstrapping pos taggers using unlabelled data", "author": ["Stephen Clark", "James R Curran", "Miles Osborne."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4. Associa-", "citeRegEx": "Clark et al\\.,? 2003", "shortCiteRegEx": "Clark et al\\.", "year": 2003}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Joachim Daiber", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes."], "venue": "Proceedings of the 9th International Conference on Semantic Systems (I-Semantics).", "citeRegEx": "Daiber et al\\.,? 2013", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "An incremental parser for abstract meaning representation", "author": ["Marco Damonte", "Shay B. Cohen", "Giorgio Satta."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Vol-", "citeRegEx": "Damonte et al\\.,? 2017", "shortCiteRegEx": "Damonte et al\\.", "year": 2017}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Addressing the data sparsity issue in neural amr parsing", "author": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:", "citeRegEx": "Peng et al\\.,? 2017", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "A transition-based algorithm for amr parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Dong and Lapata (2016) used sequence-tosequence and sequence-to-tree neural translation models to produce logical forms from sentences, while Barzdins and Gosko (2016) and Peng et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Dong and Lapata (2016) used sequence-tosequence and sequence-to-tree neural translation models to produce logical forms from sentences, while Barzdins and Gosko (2016) and Peng et al.", "startOffset": 142, "endOffset": 168}, {"referenceID": 2, "context": "Dong and Lapata (2016) used sequence-tosequence and sequence-to-tree neural translation models to produce logical forms from sentences, while Barzdins and Gosko (2016) and Peng et al. (2017) used a similar method to produce AMRs.", "startOffset": 142, "endOffset": 191}, {"referenceID": 2, "context": "More specifically, our objectives are (1) try to reproduce the results of Barzdins and Gosko (2016), who used character-level models for neural semantic parsing; (2) improve on their results by employing several novel techniques; and (3) combine the resulting neural semantic parser with an existing off-the-shelf AMR parser to reach state-of-the-art results.", "startOffset": 74, "endOffset": 100}, {"referenceID": 0, "context": "This is a bi-LSTM model with added attention mechanism, as described in Bahdanau et al. (2014). Similar to Barzdins and Gosko (2016), but contrasting with Peng et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 0, "context": "This is a bi-LSTM model with added attention mechanism, as described in Bahdanau et al. (2014). Similar to Barzdins and Gosko (2016), but contrasting with Peng et al.", "startOffset": 72, "endOffset": 133}, {"referenceID": 0, "context": "This is a bi-LSTM model with added attention mechanism, as described in Bahdanau et al. (2014). Similar to Barzdins and Gosko (2016), but contrasting with Peng et al. (2017), we train the model only on character-level input.", "startOffset": 72, "endOffset": 174}, {"referenceID": 1, "context": "Augmentation AMRs, as introduced by Banarescu et al. (2013), are rooted, directed, labeled graphs, in which the different nodes and triples are unordered by definition.", "startOffset": 36, "endOffset": 60}, {"referenceID": 5, "context": "We append the corresponding POS-tag to each word in the sentence (using the C&C POS-tagger by Clark et al. (2003)), creating a new super character for each unique tag.", "startOffset": 94, "endOffset": 114}, {"referenceID": 6, "context": "(2016), using Spotlight (Daiber et al., 2013).", "startOffset": 24, "endOffset": 45}, {"referenceID": 3, "context": "Wikification Our Wikification method is based on Bjerva et al. (2016), using Spotlight (Daiber et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 10, "context": "The ensemble comprises the off-the-shelf parser CAMR (Wang et al., 2015) and our neural semantic parser.", "startOffset": 53, "endOffset": 72}, {"referenceID": 4, "context": "The implementation of this ensemble is similar to Barzdins and Gosko (2016), choosing the AMR that obtains the highest pairwise Smatch (Cai and Knight, 2013) score when compared to the other AMRs generated for a sentence.", "startOffset": 135, "endOffset": 157}, {"referenceID": 2, "context": "The implementation of this ensemble is similar to Barzdins and Gosko (2016), choosing the AMR that obtains the highest pairwise Smatch (Cai and Knight, 2013) score when compared to the other AMRs generated for a sentence.", "startOffset": 50, "endOffset": 76}, {"referenceID": 7, "context": "In Table 4 we see the detailed results of the best seq2seq model and best ensemble on the evaluation data, using the scripts from Damonte et al. (2017).3 While CAMR has similar scores on the", "startOffset": 130, "endOffset": 152}, {"referenceID": 10, "context": "Adding our neural semantic parser to an ensemble including CAMR (Wang et al., 2015), a dependency-based parser, yielded no noteworthy improvements on the overall performance.", "startOffset": 64, "endOffset": 83}, {"referenceID": 2, "context": "We were able to reproduce the results of the character-level models for neural semantic parsing as proposed by Barzdins and Gosko (2016). Moreover, we showed improvement on their basic setting by using data-augmentation, part-ofspeech as additional input, and using super characters.", "startOffset": 111, "endOffset": 137}], "year": 2017, "abstractText": "We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.", "creator": "LaTeX with hyperref package"}}}