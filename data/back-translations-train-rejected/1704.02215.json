{"id": "1704.02215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION", "abstract": "This paper describes our approach to the SemEval 2017 Task 10: \"Extracting Keyphrases and Relations from Scientific Publications\", specifically to Subtask (B): \"Classification of identified keyphrases\". We explored three different deep learning approaches: a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-F 1 -score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15% of the full data, namely, the original development set. When trained on the full data (training+development), our ensemble has a micro-F 1 -score of 0.69. Our code is available from", "histories": [["v1", "Fri, 7 Apr 2017 13:07:15 GMT  (27kb)", "https://arxiv.org/abs/1704.02215v1", null], ["v2", "Mon, 10 Apr 2017 10:31:49 GMT  (27kb,D)", "http://arxiv.org/abs/1704.02215v2", "In revision, changed to pdfTeX output"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["steffen eger", "erik-l\\^an do dinh", "ilia kuznetsov", "masoud kiaeeha", "iryna gurevych"], "accepted": false, "id": "1704.02215"}, "pdf": {"name": "1704.02215.pdf", "metadata": {"source": "CRF", "title": "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION", "authors": ["Steffen Eger", "Erik-L\u00e2n Do Dinh", "Ilia Kuznetsov", "Masoud Kiaeeha", "Iryna Gurevych"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Although scientific experiments are often accompanied by vast amounts of structured data, full-text scientific publications remain one of the most important means of conveying academic knowledge. Given the dynamics of modern research and its ever-accelerating pace, it is crucial to automatically analyze new work in order to have a complete picture of progress in a given domain. Recently, some progress has been made in this direction for the use of fixed domains. However, the creation of a universal open domain system is still 1 example: BioNLP: http: / / 2016.bionlp-st.org / remains a challenge due to significant domain differences between articles originating from different research fields. SemEval 2017 Task 10: ScienceIE (Augenstein et al., 2017) promotes the multi-domain use case and provides source texts from three domains: computer science, materials science and physics. The task consists of three subtasks, namely (A) the identification of key words (B) of classification."}, {"heading": "2 Task and Data", "text": "In the annotation scheme proposed by the task organizers, keyphrases designating a scientific model, algorithm or process are neither classified as PROCESS (P), which also includes methods (e.g. \"backpropagation\"), physical equipment (e.g. \"plasmatic nanosensors,\" electron microscopes \"), and tools (e.g.\" MATLAB \"). TASK (T) acquiinsar Xiv: 170 4.02 215v 2 [cs.C L] 10 Apr 201 7concrete research tasks (e.g.\" powder processing, \"\" dependency parsing \") and research areas (e.g.\" machine learning \"), while MATERIAL (M) contains physical materials (e.g.\" iron, \"nanotube\") and corpora or data sets (e.g. \"the CoNLL-2003 NER pus\"), which are included in the annotation task 3."}, {"heading": "3 Implemented Approaches", "text": "In this section, we describe the individual systems that form the basis of our experiments (see \u00a7 4). Our basic settings for all our systems were as follows: For each individual keyphrase, we have chosen their left context, the right context, and the key terms themselves. We represent each of the three contexts as the concatenation of their word components, which we have chosen in relation to the individual contexts in relation to the individual contexts we have selected in relation to the individual contexts. We consider them as the way in which we have behaved in relation to their respective ways. If necessary, we have the respective contexts with \"empty tokens,\" the correct context in which we find ourselves, then we will merge each token into a D-dimensional word. Choosing http: / / www.sciencedirect.com / 3F1-Scanalogs as a hyperparameter of our modeling."}, {"heading": "4 Submitted Systems", "text": "We set the c-hyperparameter to 4, and draw the left and right context length hyperstone parameters, \"r ('= r) from a discrete, uniform distribution over the multi-set {1, 2, 3, 3, 4, 4, 4, 5). However, the performance measurement was calculated by the task evaluation script. Table 2 shows average, maximum, and minimum performance of the systems we have experimented with. We point to the\" incorrect \"systems (those trained only on the dev-set) with one star. We tested 56 different CNNs - hyper parameters randomly drawn from specific areas; e.g., we draw the number of filters m from a normal distribution N (250 trained only on the dev-set) with one star. Our three systems submitted were simple majority votes (1) the 90 stackers (2) and 56 CNNets, (3) the 90 stackers."}, {"heading": "5 Conclusion", "text": "We present an ensemble-based keyphrase classification system that has achieved nearly the best results in the ScienceIE Subtask (B), but uses only a fraction of the available training data. With complete training data, our approach ranks first. Avoiding the use of expert functions was one of our priorities, but we believe that the inclusion of additional task-neutral information beyond words and word order would benefit system performance. We also experimented with document embedding created from additionally crawled ScienceDirect6 articles. Although the stacker described in \u00a7 3 as a document classifier achieved reasonably high accuracy of 87%, its predictions had little impact on overall outcomes. Manual analysis of system errors shows that the use of part-of-speech tags, syntactical relationships, and simple named entification would most likely increase the performance of our systems."}, {"heading": "Acknowledgments", "text": "This work was supported by the Volkswagen Foundation, FAZIT, DIPF, KDSL and the EU research and innovation programme Horizon 2020 (H2020-EINFRA-2014-2) under grant agreement 654021. It only reflects the views of the authors and the EU is not responsible for the use of the information contained therein. 6 https: / / dev.elsevier.com / api docs.html"}], "references": [{"title": "SemEval 2017 Task 10: ScienceIE Extracting Keyphrases and Relations from Scientific Publications", "author": ["Isabelle Augenstein", "Mrinal Kanti Das", "Sebastian Riedel", "Lakshmi Nair Vikraman", "Andrew McCallum."], "venue": "Proceedings of the International", "citeRegEx": "Augenstein et al\\.,? 2017", "shortCiteRegEx": "Augenstein et al\\.", "year": 2017}, {"title": "Dependency Based Embeddings for Sentence Classification Tasks", "author": ["Alexandros Komninos", "Suresh Manandhar."], "venue": "Proceedings of NAACL-HLT \u201916. ACL, San Diego, CA, USA, pages 1490\u20131500.", "citeRegEx": "Komninos and Manandhar.,? 2016", "shortCiteRegEx": "Komninos and Manandhar.", "year": 2016}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of ACL \u201914. ACL, Baltimore, MD, USA, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP \u201914. ACL, Doha, Qatar, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The ACL RD-TEC 2.0: A Language Resource for Evaluating Term Extraction and Entity Recognition Methods", "author": ["Behrang QasemiZadeh", "Anne-Kathrin Schumann"], "venue": "In Proceedings of LREC \u201916", "citeRegEx": "QasemiZadeh and Schumann.,? \\Q2016\\E", "shortCiteRegEx": "QasemiZadeh and Schumann.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The SemEval 2017 Task 10: ScienceIE (Augenstein et al., 2017) promotes the multi-domain use case, providing source articles from three domains: Computer Science, Material Sciences and Physics.", "startOffset": 36, "endOffset": 61}, {"referenceID": 0, "context": "85 (Cohen\u2019s \u03ba) (Augenstein et al., 2017).", "startOffset": 15, "endOffset": 40}, {"referenceID": 4, "context": "Reviewing similar annotation efforts (QasemiZadeh and Schumann, 2016) already shows that despite the seemingly simple annotation task, usually annotators do not reach high agreement neither on span of annotations nor the class assigned to each span3.", "startOffset": 37, "endOffset": 69}, {"referenceID": 3, "context": "We use the popular Glove embeddings (Pennington et al., 2014) (6B) of dimensions 50, 100, and 300, which largely capture semantic information.", "startOffset": 36, "endOffset": 61}, {"referenceID": 1, "context": "Further we employ the more syntactically oriented 300-dimensional embeddings of Levy and Goldberg (2014), as well as the 300dimensional embeddings of Komninos and Manandhar (2016), which are trained to predict both dependency- and standard window-based context.", "startOffset": 80, "endOffset": 105}, {"referenceID": 1, "context": "Further we employ the more syntactically oriented 300-dimensional embeddings of Levy and Goldberg (2014), as well as the 300dimensional embeddings of Komninos and Manandhar (2016), which are trained to predict both dependency- and standard window-based context.", "startOffset": 150, "endOffset": 180}, {"referenceID": 0, "context": ", corresponding to the column \u201cOverall\u201d in Augenstein et al. (2017), Table 4.", "startOffset": 43, "endOffset": 68}, {"referenceID": 0, "context": ", corresponding to the column \u201cOverall\u201d in Augenstein et al. (2017), Table 4. Setting \u201crel\u201d leads to consistently higher results. E.g., with this flag, we have 72% micro-F1 for our best ensemble (corresponding to column \u201cB\u201d in Augenstein et al. (2017), Table 4), rather than 69% as reported in our Table 2.", "startOffset": 43, "endOffset": 252}], "year": 2017, "abstractText": "This paper describes our approach to the SemEval 2017 Task 10: \u201cExtracting Keyphrases and Relations from Scientific Publications\u201d, specifically to Subtask (B): \u201cClassification of identified keyphrases\u201d. We explored three different deep learning approaches: a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-F1-score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15% of the full data, namely, the original development set. When trained on the full data (training+development), our ensemble has a micro-F1-score of 0.69. Our code is available from https://github. com/UKPLab/semeval2017-scienceie.", "creator": "LaTeX with hyperref package"}}}