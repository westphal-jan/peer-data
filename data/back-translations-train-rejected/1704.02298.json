{"id": "1704.02298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "TransNets: Learning to Transform for Recommendation", "abstract": "Recently, deep learning methods have been shown to improve the performance of recommender systems over traditional methods, especially when review text is available. For example, a recent model, DeepCoNN, uses neural nets to learn one latent representation for the text of all reviews written by a target user, and a second latent representation for the text of all reviews for a target item, and then combines these latent representations to obtain state-of-the-art performance on recommendation tasks. We show that (unsurprisingly) much of the predictive value of review text comes from reviews of the target user for the target item. We then introduce a way in which this information can be used in recommendation, even when the target user's review for the target item is not available. Our model, called TransNets, extends the DeepCoNN model by introducing an additional latent layer representing the target user-target item pair. We then regularize this layer, at training time, to be similar to another latent representation of the target user's review of the target item. We show that TransNets and extensions of it improve substantially over the previous state-of-the-art.", "histories": [["v1", "Fri, 7 Apr 2017 17:13:03 GMT  (236kb,D)", "http://arxiv.org/abs/1704.02298v1", null], ["v2", "Fri, 30 Jun 2017 15:14:22 GMT  (214kb,D)", "http://arxiv.org/abs/1704.02298v2", "Accepted for publication in the 11th ACM Conference on Recommender Systems (RecSys 2017)"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["rose catherine", "william cohen"], "accepted": false, "id": "1704.02298"}, "pdf": {"name": "1704.02298.pdf", "metadata": {"source": "META", "title": "TransNets: Learning to Transform for Recommendation", "authors": ["Rose Catherine William Cohen"], "emails": ["rosecatherinek@cs.cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Using review texts to predict ratings, it has been shown that the performance of recommendation systems greatly improves [4, 22, 24] compared to the methods of Collaborative Filtering (CF), which only use past ratings [18, 33]. Recent advances in deep learning research have made it possible to use neural networks in a variety of areas, including recommendation systems, with impressive results. Most neural recommendation models [3, 10, 16, 21, 40] have focused on the content associated with the user and the subject used to construct their latent representations. Content associated with demographic information associates associated associated associated sociative characteristics, their product preferences, and the like."}, {"heading": "2 PROPOSED METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 CNNs to process text", "text": "The basic building block referred to in the rest of this work as the CNN Text Processor is a Convolutional Neural Network (CNN) [20] that inputs a sequence of words and outputs an n-dimensional vector representation for input, i.e., the CNN Text Processor is a function length: [w1, w2,..., wT] \u2192 Figure 1 reflects the architecture of the CNN Text Processor. In the first layer is a word that embeds the functionality of Xiv: 170 4.02 298v 1 [cs.I R] 7A pr2 017f: M \u2192 Rd maps each word in the review, which is also included in its Msized vocabulary. In the second layer is a word that embeds the functionality of Xiv: 170 4.02 298v 1 [cs.I R] 7A pr2 017f: M \u2192 Rd maps each word in the review also in its voculary-sized ad vector."}, {"heading": "2.2 The DeepCoNN model", "text": "In order to assign the appraisal rA, the userA to itemB, the FP, in which the FP has been located, is able to obtain a representation. () The FP, in which the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP, the FP"}, {"heading": "2.3 Limitations of DeepCoNN", "text": "DeepCoNN model has achieved impressive MSE values that exceed those of previous state-of-the-art models that use review texts, such as the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as collaborative filtering techniques that use only the assessment information such as Matrix Factorization (MF). In the real world, factoring (PMF) is observed that DeepCoNN only achieves its best performance when the text written by the target for the target object is provided. In the real world, recommendation settings always recommend an element before they have experienced it."}, {"heading": "2.4 TransNets", "text": "As we have seen in the case of DeepCoNN, learning with the target review revAB = migratory time is inadvertently dependent on the presence of such reviews during the test period, which is unrealistic. However, as the experiment above has shown, revAB gives an insight into what userA thought about their experience with itemB, and can be an important predictor for the rAB rating. Although it is not available during the test period, revAB is available during the training.TransNet consists of two networks, as shown in the architecture diagram of Figure 3, a target network that processes the target review revAB and a source network that processes the texts of the (userA, itemB) pairs that do not include the common review revAB. Given a review text revAB, the target network uses a CNN text processor that processes the target review revAB and a source network that processes the texts of the (userA, itemB pairs) that do not include the common review revAB."}, {"heading": "2.5 Training TransNets", "text": "TransNet is trained on the basis of 3 steps as shown in algorithm 1. In the first step, the target network parameters designated by \u03b8T, including those of \u0394T and FMT, are updated to minimize L1 loss calculated between the actual rating rAB and the rating r \u0442T predicted from the actual review text revAB. To teach the source network how to approximate the latent representation of the original review texts revAB generated by the target network, in the second step, its parameters designated by \u03b8trans are updated to minimize an L2 loss calculated between the transformed representation, z-L, the user's texts and the object's representation xT of the actual evaluation of the actual review text."}, {"heading": "2.6 Design Decisions and Other Architectural Choices", "text": "In this section we describe some of the decisions we have made in order to analyze and analyze the transformation philosophy. (asl) In this section we are not able to analyze the results of the transformation philosophy. (asl) In this section we are not able to analyze the results of the transformation philosophy. (asl) In this section we are not able to analyze the results of the transformation philosophy. (asl) In the transformation philosophy of the transformation philosophy it is possible to analyze the results of the transformation philosophy in the transformation philosophy. (asl) In the transformation philosophy of the transformation philosophy it is possible to analyze the results of the transformation philosophy of the transformation philosophy of the transformation philosophy. (asl) In the transformation philosophy of the transformation philosophy of the transformation philosophy of the transformation oposopos6."}, {"heading": "2.7 Extended TransNets", "text": "TransNet uses only the text of the ratings and is user / item identity agnostic, i.e. the user and item are fully represented by the review texts, and their identities are not used in the model. However, in most real-world environments, the identities of users and items are known to the recipient system. In such a scenario, it is important to learn a latent representation of users and items, similar to the methods of matrix factorization. The Extended TransNet (TransNet-Ext) model accomplishes this by extending the architecture of TransNet, as shown in Figure 4. The source network now has two embedding matrices, namely A for items that are functions of the form."}, {"heading": "3 EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Datasets", "text": "We evaluate the performance of the approach proposed in this paper using four large data sets. The first, Yelp17, comes from the most recent Yelp dataset challenge3 and contains about 4M reviews and company ratings by about 1M users. The rest are three of the larger records in the most recent version of Amazon Review4 [25, 26], which contain user ratings and ratings for products purchased on amazon.com from May 1996 to July 2014. We use the aggressively deduplicated version of the dataset and also discard entries in which the review text is blank. Statistics of the datasets are in Table 1. The original size of the dataset before discarding blank reviews is given in parentheses, if applicable."}, {"heading": "3.2 Evaluation Procedure and Settings", "text": "Each data set is divided into 1 GPU, validation and test sets in the ratio 80: 10: 10. After training on each 1000 lots of 500 training examples each, MSE is calculated on the validation and test datasets. We report on the MSE obtained on the validation dataset, and were trained / tested on NVIDIA3https. All algorithms, including competing baselines, were implemented in Python using TensorFlow5, an open source software library for numerical calculations, and were trained / tested on NVIDIA3https: / www.yelp.com / dataset _ challenge 4http: / / jmcauley.ucsd.edps / data / amazon 5https: / www.tensor ow.orgGeForce GTX GPUs. Training TransNet on Yelp17 takes approximately 40 minutes for 1 epoch."}, {"heading": "3.3 Competitive Baselines", "text": "We compare our method against the current state of the art, DeepCoNN = 44, DeepCoNN has been comprehensively evaluated against the previous state of the art, such as Hidden Factors as Topics (HFT) model [24], and shown to exceed their performance by a wide margin, we refrag these comparisons in this paper. However, we are considering some variations of DeepCoNn: / / www.tensor owtkietn for their performance, we refrag them to be able to compare these comparisons."}, {"heading": "3.4 Evaluation on Rating Prediction", "text": "As in previous work, we use the Mean Square Error (MSE) metric to assess the performance of the algorithms. Let's N be the total number of data points to test. Then, MSE will be called: MSE = 1 N \u2211 i = 1 (ri \u2212 r \u0432i) 2wo, ri is the Ground Truth Rating and r \u0441i is the predicted rating for the ith Datapoint. The lower MSE indicates better performance. The MSE values of the different baselines are given in Table 2. For each dataset, the best score is highlighted in blue. As can be seen from the table, it is clear that TransNet and its variant TransNet-Ext provide better rating predictions compared to the competing baselines on all datasets (p-value \u2264 0.05). It can also be seen that learning a user and embedding items only contributes to improving performance through the ratings in the form of TransNet-Ext."}, {"heading": "3.6 Finding the most helpful reviews", "text": "Our primary rating of TransNet is quantitative and uses MSE of predicted ratings. We would also like to investigate whether the learned presentation is qualitatively useful - i.e., it captures interesting, high-quality characteristics of the user's rating. One possible use of the learning review would be to give the user information about his predicted reaction to the article, which is more detailed than a rating. In this section, we will show how TransNets could be used to create the most helpful reviews, which are personalized to each user. For example, the most helpful review for a user who is more concerned about the quality of the service and waiting times would be the most helpful review for another user who is sensitive to the price. For a test user P \u2212 itemQ pair, we run the source network with the text of their ratings from the training set to construct zL, which is an approximation for displaying their actual common rating. Candidate reviews are all the reviews predicted by other users in the CrevQ and CVQ training set."}, {"heading": "4 RELATEDWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Recommendation Models", "text": "The idea is that people who stand up for the rights of women and men also stand up for the rights of men and women who stand up for the rights of women and men, the idea is that women and men who stand up for the rights of men and women stand up for the rights of women and men, the idea is that women and men who stand up for the rights of men and women stand up not only for the rights of women and men, but also for the rights of women and men who stand up for the rights of women, the idea is that women and men who stand up for the rights of women stand up for the rights of men and women."}, {"heading": "4.2 Comparison to Related Architectures", "text": "In fact, it is such that most of the people who are able to move are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "5 CONCLUSIONS", "text": "A newer neural network model, DeepCoNN, uses the text of user-written reviews to learn their latent representations, which are then fed into a regression layer for evaluation. However, their performance depends on the user having access to the pair-by-pair evaluation, which is not available in the real world. In this paper, we propose a new model called TransNets, which adds an additional transform layer to DeepCoNN. This additional layer learns to transform the latent representations of user and object into their pair-by-pair review, so that at test points an approximate representation of the target evaluation can be generated and used for predictions. We also showed how TransNets can be expanded to learn the representation of users and objects only from reviews that can be used in addition to the generated review representation. Our experiments showed that TransNets and its enhanced version can significantly improve the state of the art."}], "references": [{"title": "Context-aware Recommender Systems", "author": ["Gediminas Adomavicius", "Alexander Tuzhilin"], "venue": "In Proceedings of the 2008 ACM Conference on Recommender Systems (RecSys", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Learning Distributed Representations from Reviews for Collaborative Filtering", "author": ["Amjad Almahairi", "Kyle Kastner", "Kyunghyun Cho", "Aaron Courville"], "venue": "In Proceedings of the 9th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Ask the GRU: Multi-task Learning for Deep Text Recommendations", "author": ["Trapit Bansal", "David Belanger", "Andrew McCallum"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "TopicMF: Simultaneously Exploiting Ratings and Reviews for Recommendation", "author": ["Yang Bao", "Hui Fang", "Jie Zhang"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Arti cial Intelligence", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Latent Dirichlet Allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Marginalized Denoising Autoencoders for Domain Adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Q. Weinberger", "Fei Sha"], "venue": "In Proceedings of the 29th International Coference on International Conference on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Deep Neural Networks for YouTube Recommendations", "author": ["Paul Covington", "Jay Adams", "Emre Sargin"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems", "author": ["Ali Mamdouh Elkahky", "Yang Song", "Xiaodong He"], "venue": "In Proceedings of the 24th International Conference on World Wide Web (WWW", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Generative Adversarial Nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde- Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["Geo rey E. Hinton", "Oriol Vinyals", "Je rey Dean"], "venue": "In Deep Learning and Representation Learning Workshop", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation 9,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Harnessing Deep Neural Networks with Logic Rules", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Convolutional Matrix Factorization for Document Context-Aware Recommendation", "author": ["Donghyun Kim", "Chanyoung Park", "Jinoh Oh", "Sungyoung Lee", "Hwanjo Yu"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model", "author": ["Yehuda Koren"], "venue": "In Proc. KDD", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Ha ner"], "venue": "Proc. IEEE 86,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Deep Collaborative Filtering via Marginalized Denoising Auto-encoder", "author": ["Sheng Li", "Jaya Kawale", "Yun Fu"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Ratings Meet Reviews, a Combined Approach to Recommend", "author": ["Guang Ling", "Michael R. Lyu", "Irwin King"], "venue": "In Proceedings of the 8th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit. In Association for Computational Linguistics (ACL) System Demonstrations", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text", "author": ["Julian McAuley", "Jure Leskovec"], "venue": "In Proceedings of the 7th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Inferring Networks of Substitutable and Complementary Products", "author": ["Julian McAuley", "Rahul Pandey", "Jure Leskovec"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Image-Based Recommendations on Styles and Substitutes", "author": ["Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton van den Hengel"], "venue": "In Proceedings  of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Distributed Representations of Words and Phrases and Their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Je rey Dean"], "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Recti ed Linear Units Improve Restricted Boltzmann Machines", "author": ["Vinod Nair", "Geo rey E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Deep Content-based Music Recommendation", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Je rey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Generative Adversarial Text to Image Synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Factorization Machines", "author": ["Ste en Rendle"], "venue": "In Proceedings of the 2010 IEEE International Conference on Data Mining (ICDM", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Probabilistic Matrix Factorization", "author": ["Ruslan Salakhutdinov", "Andriy Mnih"], "venue": "In Proceedings of the 20th International Conference on Neural Information Processing Systems (NIPS\u201907). Curran Associates Inc.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Representation Learning of Users and Items for Review Rating Prediction Using Attention-based Convolutional Neural Network. In 3rd International Workshop on Machine Learning Methods for Recommender Systems (MLRec) (SDM \u201917)", "author": ["Sungyong Seo", "Jing Huang", "Hao Yang", "Yan Liu"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Over tting", "author": ["Nitish Srivastava", "Geo rey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "J. Mach. Learn. Res", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Rating-boosted Latent Topics: Understanding Users and Items with Ratings and Reviews", "author": ["Yunzhi Tan", "Min Zhang", "Yiqun Liu", "Shaoping Ma"], "venue": "In Proceedings of the Twenty-Fifth International Joint Conference on Arti cial Intelligence", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre- Antoine Manzagol"], "venue": "J. Mach. Learn. Res", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Collaborative Topic Modeling for Recommending Scienti c Articles", "author": ["Chong Wang", "David M. Blei"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Collaborative Deep Learning for Recommender Systems", "author": ["Hao Wang", "Naiyan Wang", "Dit-Yan Yeung"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Collaborative Denoising Auto-Encoders for Top-N Recommender Systems", "author": ["Yao Wu", "Christopher DuBois", "Alice X. Zheng", "Martin Ester"], "venue": "In Proceedings of the Ninth ACM International Conference onWeb Search and DataMining (WSDM", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Beyond Clicks: Dwell Time for Personalization", "author": ["Xing Yi", "Liangjie Hong", "Erheng Zhong", "Nanthan Nan Liu", "Suju Rajan"], "venue": "In Proceedings of the 8th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Explicit Factor Models for Explainable Recommendation Based on Phrase-level Sentiment Analysis", "author": ["Yongfeng Zhang", "Guokun Lai", "Min Zhang", "Yi Zhang", "Yiqun Liu", "Shaoping Ma"], "venue": "In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Joint Deep Modeling of Users and Items Using Reviews for Recommendation", "author": ["Lei Zheng", "Vahid Noroozi", "Philip S. Yu"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].", "startOffset": 114, "endOffset": 125}, {"referenceID": 19, "context": "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].", "startOffset": 114, "endOffset": 125}, {"referenceID": 21, "context": "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].", "startOffset": 114, "endOffset": 125}, {"referenceID": 15, "context": "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].", "startOffset": 206, "endOffset": 214}, {"referenceID": 30, "context": "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].", "startOffset": 206, "endOffset": 214}, {"referenceID": 2, "context": "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.", "startOffset": 31, "endOffset": 50}, {"referenceID": 8, "context": "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.", "startOffset": 31, "endOffset": 50}, {"referenceID": 13, "context": "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.", "startOffset": 31, "endOffset": 50}, {"referenceID": 18, "context": "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.", "startOffset": 31, "endOffset": 50}, {"referenceID": 37, "context": "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "In that sense, it is a context [1] feature.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "Only a few neural net models [2, 34, 44] have been proposed to date that use review text for predicting the rating.", "startOffset": 29, "endOffset": 40}, {"referenceID": 31, "context": "Only a few neural net models [2, 34, 44] have been proposed to date that use review text for predicting the rating.", "startOffset": 29, "endOffset": 40}, {"referenceID": 41, "context": "Only a few neural net models [2, 34, 44] have been proposed to date that use review text for predicting the rating.", "startOffset": 29, "endOffset": 40}, {"referenceID": 41, "context": "Of these, the most recent model, Deep Cooperative Neural Networks (DeepCoNN) [44] uses neural nets to learn a latent representation for the user from the text of all reviews written by her and a second latent representation for the item from the text of all reviews that were written for it, and then combines these two representations in a regression layer to obtain state-of-the-art performance on rating prediction.", "startOffset": 77, "endOffset": 81}, {"referenceID": 41, "context": "We process text using the same approach as the current state-ofthe-art method for rating prediction, DeepCoNN [44].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "The basic building block, referred to as a CNN Text Processor in the rest of this paper, is a Convolutional Neural Network (CNN) [20] that inputs a sequence of words and outputs a n-dimensional vector representation for the input, i.", "startOffset": 129, "endOffset": 133}, {"referenceID": 24, "context": "The embedding can be any pre-trained embedding like those trained on the GoogleNews corpus using word2vec1[27], or on Wikipedia using GloVe2 [30].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "The embedding can be any pre-trained embedding like those trained on the GoogleNews corpus using word2vec1[27], or on Wikipedia using GloVe2 [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "Following the embedding layer is the Convolutional Layer, adapted to text processing [8].", "startOffset": 85, "endOffset": 88}, {"referenceID": 25, "context": "zj = \u03b1(V1:T \u2217 Kj + bj ) where, bj is its bias, \u2217 is the convolution operation and \u03b1 is a nonlinearity like Recti ed Linear Unit (ReLU) [28] or tanh.", "startOffset": 135, "endOffset": 139}, {"referenceID": 41, "context": "To compute the rating rAB that userA would assign to itemB , the DeepCoNN model of [44] uses two CNN Text Processors side by side as shown in Figure 2.", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "Both outputs are passed through a dropout layer [36].", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "The model then concatenates the two representations as z = [x\u0304A, \u0233B ] and passes it through a regression layer consisting of a Factorization Machine (FM) [32].", "startOffset": 154, "endOffset": 158}, {"referenceID": 29, "context": "This has been shown to give better parameter estimates under sparsity [32].", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "FMs have been used successfully in large scale recommendation services like online news[42].", "startOffset": 87, "endOffset": 91}, {"referenceID": 41, "context": "DeepCoNN [44] also uses L1 loss.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].", "startOffset": 175, "endOffset": 179}, {"referenceID": 36, "context": "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].", "startOffset": 218, "endOffset": 222}, {"referenceID": 37, "context": "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].", "startOffset": 261, "endOffset": 265}, {"referenceID": 15, "context": "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].", "startOffset": 381, "endOffset": 385}, {"referenceID": 30, "context": "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].", "startOffset": 431, "endOffset": 435}, {"referenceID": 16, "context": "r\u0302T = FMT (x\u0304T ) Since the Target Network uses the actual review, its task is similar to sentiment analysis [19, 35].", "startOffset": 108, "endOffset": 116}, {"referenceID": 32, "context": "r\u0302T = FMT (x\u0304T ) Since the Target Network uses the actual review, its task is similar to sentiment analysis [19, 35].", "startOffset": 108, "endOffset": 116}, {"referenceID": 22, "context": "The rest are three of the larger datasets in the latest release of Amazon reviews4 [25, 26] containing reviews and ratings given by users for products purchased on amazon.", "startOffset": 83, "endOffset": 91}, {"referenceID": 23, "context": "The rest are three of the larger datasets in the latest release of Amazon reviews4 [25, 26] containing reviews and ratings given by users for products purchased on amazon.", "startOffset": 83, "endOffset": 91}, {"referenceID": 21, "context": "We report the MSE obtained on the test dataset when the MSE on the validation dataset was the lowest, similar to [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": "All reviews are rst passed through a Stanford Core NLP Tokenizer [23] to obtain the tokens, which are then lowercased.", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "A 64-dimensional word2vec6 [27] embedding using the Skip-gram model is pre-trained on the 50,000 most frequent tokens in each of the training corpora.", "startOffset": 27, "endOffset": 31}, {"referenceID": 41, "context": "We reuse most of the hyperparameter settings reported by the authors of DeepCoNN [44] since varying them did not give any perceivable improvement.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "All optimizations are learned using Adam [17], a stochastic gradient-based optimizer with adaptive estimates, at a learning rate set to 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 41, "context": "We compare our method against the current state-of-the-art, DeepCoNN [44].", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.", "startOffset": 128, "endOffset": 132}, {"referenceID": 36, "context": "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.", "startOffset": 171, "endOffset": 175}, {"referenceID": 37, "context": "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.", "startOffset": 211, "endOffset": 215}, {"referenceID": 30, "context": "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.", "startOffset": 261, "endOffset": 265}, {"referenceID": 41, "context": "(1) DeepCoNN: The model proposed in [44].", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "TheHidden Factors as Topics (HFT) model [24] aims to nd topics in the review text that are correlated", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "A modi ed version of HFT is the TopicMF model [4], where the goal is to match the latent factors learned for the users and items using MF with the topics learned on their joint reviews using a Non-Negative Matrix Factorization, which is then jointly optimized with the rating prediction.", "startOffset": 46, "endOffset": 49}, {"referenceID": 19, "context": "model [22] where the rating is sampled from a Gaussian mixture.", "startOffset": 6, "endOffset": 10}, {"referenceID": 36, "context": "The Collaborative Topic Regression (CTR) model proposed in [39] is a content based approach, as opposed to a context / review based approach.", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "It uses LDA [5] to model the text of documents (scienti c articles), and a combination of MF and content based model for recommendation.", "startOffset": 12, "endOffset": 15}, {"referenceID": 34, "context": "The Rating-boosted Latent Topics (RBLT) model of [37] uses a simple technique of repeating a review r times in the corpus if it was rated r , so that features in higher rated reviews will dominate the topics.", "startOffset": 49, "endOffset": 53}, {"referenceID": 40, "context": "Explicit Factor Models (EFM) proposed in [43] aims to generate explainable recommendations by extracting explicit product features (aspects) and users\u2019 sentiments towards these aspects using phrase-level sentiment analysis.", "startOffset": 41, "endOffset": 45}, {"referenceID": 41, "context": "The most recent model to successfully employ neural networks at scale for rating prediction is the Deep Cooperative Neural Networks (DeepCoNN) [44], which was discussed in detail in Section 2.", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "Prior to that work, [2] proposed two models: Bag-of-Words regularized Latent Factor model (BoWLF) and Language Model regularized Latent Factor model (LMLF), where MF was used to learn the latent factors of users and items, and likelihood of the review text, represented either as a bag-of-words or an LSTM embedding [14], was computed using the item factors.", "startOffset": 20, "endOffset": 23}, {"referenceID": 11, "context": "Prior to that work, [2] proposed two models: Bag-of-Words regularized Latent Factor model (BoWLF) and Language Model regularized Latent Factor model (LMLF), where MF was used to learn the latent factors of users and items, and likelihood of the review text, represented either as a bag-of-words or an LSTM embedding [14], was computed using the item factors.", "startOffset": 316, "endOffset": 320}, {"referenceID": 31, "context": "[34] proposed a CNN based model identical to DeepCoNN, but with attention mechanism to construct the latent representations, the inner product of which gave the predicted ratings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "The Collaborative Deep Learning (CDL) model [40] uses a Stacked De-noising Auto Encoder (SDAE) [38] to learn robust latent representations of items from their content, which is then fed into a CTR model [39] for predicting the ratings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 35, "context": "The Collaborative Deep Learning (CDL) model [40] uses a Stacked De-noising Auto Encoder (SDAE) [38] to learn robust latent representations of items from their content, which is then fed into a CTR model [39] for predicting the ratings.", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "The Collaborative Deep Learning (CDL) model [40] uses a Stacked De-noising Auto Encoder (SDAE) [38] to learn robust latent representations of items from their content, which is then fed into a CTR model [39] for predicting the ratings.", "startOffset": 203, "endOffset": 207}, {"referenceID": 18, "context": "A very similar approach to CDL is the Deep Collaborative Filtering (DCF) method [21] which uses Marginalized De-noising Auto-Encoder (mDA) [7] instead.", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "A very similar approach to CDL is the Deep Collaborative Filtering (DCF) method [21] which uses Marginalized De-noising Auto-Encoder (mDA) [7] instead.", "startOffset": 139, "endOffset": 142}, {"referenceID": 13, "context": "The Convolutional Matrix Factorization (ConvMF) model [16] uses a CNN to process the description associated with the item and feed the resulting latent vectors into a PMF model for rating prediction.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "The Multi-View Deep Neural Net (MV-DNN) model [10] uses a deep neural net to map user\u2019s and item\u2019s content into a shared latent space such that their similarity in that space is maximized.", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "[29] proposed to generate the latent factors of items \u2013 music in this case\u2014 from the content, audio signals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] also proposed a similar technique but adapted to recommending scienti c-articles.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] used a deep neural net to learn a latent representation from video content which is then fed into a deep ranking network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "Collaborative De-noising Auto-Encoder model (CDAE) [41] learns to reconstruct user\u2019s feedback from a corrupted version of the same.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "Student-Teacher models [6, 13] also have two networks: a Teacher Network, which is large and complex, and typically an ensemble of di erent models, is rst trained to make predictions, and a much simpler Student Network, which learns to emulate the output of the Teacher Network, is trained later.", "startOffset": 23, "endOffset": 30}, {"referenceID": 12, "context": "A recently proposed Student-Teacher model in [15] does train both the Student and the Teacher simultaneously.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "TransNets also bear semblance to GANs [12, 31] since both are attempting to generate an output which is similar to realistic data.", "startOffset": 38, "endOffset": 46}, {"referenceID": 28, "context": "TransNets also bear semblance to GANs [12, 31] since both are attempting to generate an output which is similar to realistic data.", "startOffset": 38, "endOffset": 46}], "year": 2017, "abstractText": "Recently, deep learning methods have been shown to improve the performance of recommender systems over traditional methods, especially when review text is available. For example, a recent model, DeepCoNN, uses neural nets to learn one latent representation for the text of all reviews written by a target user, and a second latent representation for the text of all reviews for a target item, and then combines these latent representations to obtain state-of-the-art performance on recommendation tasks. We show that (unsurprisingly) much of the predictive value of review text comes from reviews of the target user for the target item. We then introduce a way in which this information can be used in recommendation, even when the target user\u2019s review for the target item is not available. Our model, called TransNets, extends the DeepCoNN model by introducing an additional latent layer representing the target user-target item pair. We then regularize this layer, at training time, to be similar to another latent representation of the target user\u2019s review of the target item. We show that TransNets and extensions of it improve substantially over the previous state-of-the-art.", "creator": "LaTeX with hyperref package"}}}