{"id": "1704.02312", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "A Constrained Sequence-to-Sequence Neural Model for Sentence Simplification", "abstract": "Sentence simplification reduces semantic complexity to benefit people with language impairments. Previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. For sentence-level studies, sentences after simplification are fluent but sometimes are not really simplified. For word-level studies, words are simplified but also have potential grammar errors due to different usages of words before and after simplification. In this paper, we propose a two-step simplification framework by combining both the word-level and the sentence-level simplifications, making use of their corresponding advantages. Based on the two-step framework, we implement a novel constrained neural generation model to simplify sentences given simplified words. The final results on Wikipedia and Simple Wikipedia aligned datasets indicate that our method yields better performance than various baselines.", "histories": [["v1", "Fri, 7 Apr 2017 17:53:24 GMT  (702kb,D)", "http://arxiv.org/abs/1704.02312v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["yaoyuan zhang", "zhenxu ye", "yansong feng", "dongyan zhao", "rui yan"], "accepted": false, "id": "1704.02312"}, "pdf": {"name": "1704.02312.pdf", "metadata": {"source": "CRF", "title": "A Constrained Sequence-to-Sequence Neural Model for Sentence Simplification", "authors": ["Yaoyuan Zhang", "Zhenxu Ye", "Yansong Feng", "Dongyan Zhao", "Rui Yan"], "emails": ["ruiyan}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work", "text": "In previous studies, researchers have focused on simplification at the sentence level mainly on simplification as a monolingual machine translation problem. Similarly, Coster and Kauchak (2011) expand the PB-SMT model by adding phrase deletions. Wubben et al. (2012) make further efforts by re-translating the best sentences of Moses based on their dissimilarity with the input. More recently, Xu et al. (2016) have proposed a SB-SMT model that performs better than Wubben's system. In general, simplification at the sentence level maintains the semantic meaning and fluency of the language, but does not always guarantee literal simplification."}, {"heading": "3 Neural Generation Model", "text": "We create simplified sentences using a sequence-to-sequence model trained on a parallel corpus, namely English Wikipedia and plain English Wikipedia. We have identified simplified words from the first step, but the default sequence-to-sequence model cannot guarantee the existence of such words. Therefore, we propose a sequence-to-sequence model (Constrained Seq2Seq) in which the given simplified words serve as constraints during sentence generation."}, {"heading": "3.1 Methodology", "text": "Since many efforts have been made to establish word simplification pairs (Horn et al., 2014; Glavas and S-tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on identifying words that require simplification, or on the methods for selecting simpler words to be changed. Instead, we change the words according to this knowledge base and move to the neural sentence generation model, assuming the word substitutions are correct based on previous studies of synonyms. To be more precise, given the simplified words of the first step as constraints, we propose a limited seq2seq model that first identifies the input sentence as a vector and then replaces it with its simpler synonyms. \u2022 Step 2. Given the simplified words of the first step as a condition, we propose a limited seq2seq model that encodes the input word as a vector."}, {"heading": "3.2 Constrained Seq2Seq Model", "text": "Considering the common probabilities of yb and yf, we are: p (yb), p (yb), p (i), p (i), p (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i)."}, {"heading": "3.3 Multi-constrained Seq2Seq", "text": "We have just illustrated how to insert a single constraint word into the process of sequence-to-sequence generation, when in fact there may be more than one constraint word that is simplified before sentence generation, as shown in Table 1. We extend the single constraint into multiple constraints using a multiple Seq2Seq model. Without losing generosity, we define the multiple keywords as ys1, ys2,... and illustrate MultiConstrained Seq2Seq in Figure 2. We first illustrate the situation with two simplified words, i.e. k = 2, namely ys1 and ys2. We generally take the complex word with the least term frequency as the first constraint word ys1 and use the same method as in Section 3.2 to generate the first output set y1 = (y11,..., ys1,.., ys2)."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and Setups", "text": "We evaluate our proposed approach to the parallel corpus from Wikipedia and Simple Wikipedia in English3. We randomly divide the corpus into 123,626 sentence pairs (each pair as a normal sentence and its simplification in parallel) for training, 5,000 sentences for validation and 600 sentences for testing. There may be noises in the record. We filter test samples out if the output is identical to the input without any simplification. We also applied pre-processing with reduction to all samples. Our vocabulary contains 60,000 words, while words outside the vocabulary are assigned to the token \"unk.\" The RNN encoder and decoder of our model both have 1,000 hidden units; the word embedding dimensionality is 620. We use the nodal ta3 This data set is available at http: / / www.cs. pomona.edu / \u02dc dkauchak / simplification (pointer, 2012) to optimize all parameters."}, {"heading": "4.2 Comparison Methods", "text": "In this work, we perform the experiments on the basis of the English Wikipedia and the simple English Wikipedia dataset in order to compare our proposed method with several representative algorithms. Moses. It is a standard phrase-based machine translation model (Koehn et al., 2007). SBMT. SBMT is a syntactic machine translation model (Xu et al., 2016) implemented on the open source toolkit Joshua (Post et al., 2013). The simplification model is optimized for the SARI metric and uses the PPDB dataset (Pavlick et al., 2016) as a rich source for simplification operations. Lexic substitution. This method replaces only the complex words with the simplified word (e) that we use in our model as a forced word (e) quantum quantum quantum quantum quantum quantum quantum quantus quantum quantum quantus quantus quantum quantum quantum quantum quantum quantum sequence quantus quantus quantum sequence quantum quantum sequence quantum quantum quantus quantum quantum quantum quantus quantum quantum quantum quantum sequence quantum quantum quantus quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantus quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantus quantum quantus quantus quantum quantum quantum quantum quantum quantus quantum quantus quantum quantum quantum quantus quantus quantum quantus quantum quantum quantum quantum quantum quantus quantum quantum quantum quantum quantus quantum quantus quantum quantus quantum quantum quantus quantum quantus quantum quantum quantum qu"}, {"heading": "4.3 Evaluation Metrics", "text": "In fact, it is in such a way that the greater number of people who are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to fight, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move, are able to move"}, {"heading": "4.4 Overall Performance", "text": "In this context, it should be noted that this is a very complex and complex matter."}, {"heading": "4.5 Analysis and Case Studies", "text": "In Table 4, we show some typical examples for all systems. Among them, Moses, SBMT, and the Seq2Seq model generate a completely identical sentence with the simple words \"important,\" \"center,\" and \"many\" as in most cases. Lexical Substitution paraphrases the complex words \"key,\" \"hub,\" and \"very much\" with the simple words \"important,\" \"center,\" and \"many.\" As you can see, the article should be changed from \"a\" to \"a\" for the word \"important,\" but Lexical Substitution does not deal with such errors. As for our proposed model, it generates the output sentences conditioned on the simplified word \"center\" and deletes the complex formulation \"very much.\" Taking the sentence generated from the Constrained Seq2Seq model as input, the multistrained Seq2Seq model replaces the rarer words \"key\" by the word \"important.\" It also modifies the sentence substitution of the simple Seq2Seq model, which was... \"more flexible\" in the 1980s and..."}, {"heading": "5 Conclusion", "text": "In this article, we propose a new two-step method for sentence simplification, combining vocabulary simplification and sentence simplification. We are conducting experiments with the parallel records of Wikipedia and Simple Wikipedia and the results show that our methods outperform different baselines with better readability, flexibility and simplicity. In the future, we plan to consider other factors (e.g. sentence length or grammar rules) and incorporate them as limitations in our proposed model."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Simplifying text for language-impaired readers", "author": ["John Carroll", "Guido Minnen", "Darrenz Pearce", "Canning Yvonne", "Devlin Siobhan", "John Tait."], "venue": "Proceedings of EACL. pages 269\u2013270.", "citeRegEx": "Carroll et al\\.,? 1999", "shortCiteRegEx": "Carroll et al\\.", "year": 1999}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning to simplify sentences using wikipedia", "author": ["William Coster", "David Kauchak."], "venue": "Proceedings of the workshop on monolingual text-to-text generation. Association for Computational Linguistics, pages 1\u20139.", "citeRegEx": "Coster and Kauchak.,? 2011", "shortCiteRegEx": "Coster and Kauchak.", "year": 2011}, {"title": "An evaluation of syntactic simplification rules for people with autism", "author": ["Richard Evans", "Constantin Orasan", "Iustin Dornescu."], "venue": "Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR). pages 121\u2013", "citeRegEx": "Evans et al\\.,? 2014", "shortCiteRegEx": "Evans et al\\.", "year": 2014}, {"title": "Simplifying lexical simplification: Do we need simplified corpora", "author": ["Goran Glava\u0161", "Sanja \u0160tajner."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Glava\u0161 and \u0160tajner.,? 2015", "shortCiteRegEx": "Glava\u0161 and \u0160tajner.", "year": 2015}, {"title": "Learning a lexical simplifier using wikipedia", "author": ["Colby Horn", "Cathryn Manduca", "David Kauchak."], "venue": "ACL (2). pages 458\u2013463.", "citeRegEx": "Horn et al\\.,? 2014", "shortCiteRegEx": "Horn et al\\.", "year": 2014}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["J. Peterand Robert P. Fishburne Jr Kincaid", "Richard L. Rogers", "Brad S."], "venue": "Naval Technical Training Command", "citeRegEx": "Kincaid et al\\.,? 1975", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Moses: Open source toolkit", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris C.Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1607.00970.", "citeRegEx": "Mou et al\\.,? 2016", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Backward and forward language modeling for constrained sentence generation", "author": ["Lili Mou", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1512.06612.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Unsupervised lexical simplification for non-native speakers", "author": ["Gustavo H. Paetzold", "Lucia Specia."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press, pages 3761\u20133767.", "citeRegEx": "Paetzold and Specia.,? 2016", "shortCiteRegEx": "Paetzold and Specia.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Simple ppdb: A paraphrase database for simplification", "author": ["Ellie Pavlick", "Chris Callison-Burch."], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. page 143.", "citeRegEx": "Pavlick and Callison.Burch.,? 2016", "shortCiteRegEx": "Pavlick and Callison.Burch.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP. volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Joshua 5.0: Sparser, better, faster, server", "author": ["Matt Post", "Juri Ganitkevitch", "Luke Orland", "Jonathan Weese", "Yuan Cao", "Chris Callison-Burch"], "venue": "In Proceedings of the Eighth Workshop on Statistical Machine Translation", "citeRegEx": "Post et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Post et al\\.", "year": 2013}, {"title": "The impact of lexical simplification by verbal paraphrases for people with and without dyslexia", "author": ["Luz Rello", "Ricardo Baeza-Yates", "Horacio Saggion."], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics. Springer Berlin", "citeRegEx": "Rello et al\\.,? 2013", "shortCiteRegEx": "Rello et al\\.", "year": 2013}, {"title": "Bidirectional recurrent neural networks 11:2673\u20132681", "author": ["Mike Schuster", "Kuldip K. Paliwal"], "venue": null, "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Translating from complex to simplified sentences", "author": ["Lucia Specia."], "venue": "International Conference on Computational Processing of the Portuguese Language. Springer Berlin Heidelberg, pages 30\u201339.", "citeRegEx": "Specia.,? 2010", "shortCiteRegEx": "Specia.", "year": 2010}, {"title": "Joint learning of a dual smt system for paraphrase generation", "author": ["Hong Sun", "Ming Zhou."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers. Association for Computational Linguistics, volume 2.", "citeRegEx": "Sun and Zhou.,? 2012", "shortCiteRegEx": "Sun and Zhou.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A deeper exploration of the standard pb-smt approach to text simplification and its evaluation", "author": ["Sanja \u0160tajner", "Hannah B\u00e9chara", "Horacio Saggion."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "\u0160tajner et al\\.,? 2015", "shortCiteRegEx": "\u0160tajner et al\\.", "year": 2015}, {"title": "Sentence simplification by monolingual machine translation", "author": ["Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers. volume 1, pages 1015\u20131024.", "citeRegEx": "Wubben et al\\.,? 2012", "shortCiteRegEx": "Wubben et al\\.", "year": 2012}, {"title": "Optimizing statistical machine translation for text simplification", "author": ["Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch."], "venue": "Transactions of the Association for Computational Linguistics 4. pages 401\u2013415.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "A monolingual tree-based translation model for sentence simplification", "author": ["Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych"], "venue": "In Proceedings of the 23rd international conference on computational linguistics. Association for Computational Linguistics,", "citeRegEx": "Zhu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "In particular, children, non-native speakers and individuals with language impairments such as Dyslexia (Rello et al., 2013), Aphasic (Carroll et al.", "startOffset": 104, "endOffset": 124}, {"referenceID": 1, "context": ", 2013), Aphasic (Carroll et al., 1999) and Autistic (Evans et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 4, "context": ", 1999) and Autistic (Evans et al., 2014), would benefit from the task which makes sentences easier to understand.", "startOffset": 21, "endOffset": 41}, {"referenceID": 18, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 25, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 3, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 22, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 23, "context": "translation, several studies treate the sentence simplification problem as monolingual translation task and achieve promising results (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Xu et al., 2016).", "startOffset": 134, "endOffset": 230}, {"referenceID": 2, "context": "cently, neural machine translation (NMT) based on sequence-to-sequence model (seq2seq) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) shows more powerful capabilities than traditional SMT systems.", "startOffset": 87, "endOffset": 152}, {"referenceID": 20, "context": "cently, neural machine translation (NMT) based on sequence-to-sequence model (seq2seq) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) shows more powerful capabilities than traditional SMT systems.", "startOffset": 87, "endOffset": 152}, {"referenceID": 0, "context": "cently, neural machine translation (NMT) based on sequence-to-sequence model (seq2seq) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) shows more powerful capabilities than traditional SMT systems.", "startOffset": 87, "endOffset": 152}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al. (2015), Paetzold et al.", "startOffset": 40, "endOffset": 81}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al. (2015), Paetzold et al. (2016) and Pavlick et al.", "startOffset": 40, "endOffset": 105}, {"referenceID": 6, "context": "Recent lexical simplification models by Horn et al. (2014), Glava\u0161 et al. (2015), Paetzold et al. (2016) and Pavlick et al. (2016) have accumulated substantial numbers of synonymous word pairs.", "startOffset": 40, "endOffset": 131}, {"referenceID": 13, "context": "A knowledge base such as PPDB contains millions of paraphrasing word pairs to change between simple words and complex words (Pavlick and Callison-Burch, 2016) Normal Sentence In the last decades of his life, dukas became well known as a teacher of composition, with many famous students.", "startOffset": 124, "endOffset": 158}, {"referenceID": 8, "context": "(2010) use the standard PB-SMT implemented in Moses toolkit (Koehn et al., 2007) to translate the original sentences to the simplified ones.", "startOffset": 60, "endOffset": 80}, {"referenceID": 16, "context": "Specia et al. (2010) use the standard PB-SMT implemented in Moses toolkit (Koehn et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Similarly, Coster and Kauchak (2011) extend the PB-SMT model by adding phrase deletion.", "startOffset": 11, "endOffset": 37}, {"referenceID": 3, "context": "Similarly, Coster and Kauchak (2011) extend the PB-SMT model by adding phrase deletion. Wubben et al. (2012) make a further effort by reranking the Moses\u2019 nbest output based on their dissimilarity to the input.", "startOffset": 11, "endOffset": 109}, {"referenceID": 3, "context": "Similarly, Coster and Kauchak (2011) extend the PB-SMT model by adding phrase deletion. Wubben et al. (2012) make a further effort by reranking the Moses\u2019 nbest output based on their dissimilarity to the input. Most recently, Xu et al. (2016) have proposed a SB-SMT model, achieving better performance than Wubben\u2019s system.", "startOffset": 11, "endOffset": 243}, {"referenceID": 14, "context": "(2015) employ GloVe (Pennington et al., 2014) to generate synonyms for the complex words.", "startOffset": 20, "endOffset": 45}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia. Glava\u0161 et al. (2015) employ GloVe (Pennington et al.", "startOffset": 0, "endOffset": 184}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia. Glava\u0161 et al. (2015) employ GloVe (Pennington et al., 2014) to generate synonyms for the complex words. Instead of using the parallel datasets, their approach only requires a single corpus. Paetzold et al. (2016) propose a new word embeddings model to deal with the limitation that the traditional models do not accommodate ambiguous lexical semantics.", "startOffset": 0, "endOffset": 376}, {"referenceID": 6, "context": "Horn et al. (2014) extract over 30,000 paraphrase rules for lexical simplification by identifying aligned words in English Wikipedia and Simple English Wikipedia. Glava\u0161 et al. (2015) employ GloVe (Pennington et al., 2014) to generate synonyms for the complex words. Instead of using the parallel datasets, their approach only requires a single corpus. Paetzold et al. (2016) propose a new word embeddings model to deal with the limitation that the traditional models do not accommodate ambiguous lexical semantics. Pavlick et al. (2016) release about 4,500,000 simple paraphrase rules by extracting normal paraphrases rules from a bilingual corpus and reranking the simplicity scores of these rules by a supervised model.", "startOffset": 0, "endOffset": 538}, {"referenceID": 6, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 5, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 11, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 13, "context": "Since there has been many efforts working on the establishment of word simplification pairs (Horn et al., 2014; Glava\u0161 and \u0160tajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016), we do not focus on the identification of words that require simplification or the methods of selecting what simpler words to switch.", "startOffset": 92, "endOffset": 198}, {"referenceID": 17, "context": "In our paper, we apply the bi-directional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) with gated recurrent units (GRUs) (Cho et al.", "startOffset": 75, "endOffset": 103}, {"referenceID": 2, "context": "In our paper, we apply the bi-directional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) with gated recurrent units (GRUs) (Cho et al., 2014) for both the backward and for-", "startOffset": 138, "endOffset": 156}, {"referenceID": 24, "context": "(Zeiler, 2012) to optimize all parameters.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "It is a standard phrase-based machine translation model (Koehn et al., 2007).", "startOffset": 56, "endOffset": 76}, {"referenceID": 23, "context": "SBMT is a syntactic-based machine translation model (Xu et al., 2016), which is implemented on the open-source Joshua toolkit (Post et al.", "startOffset": 52, "endOffset": 69}, {"referenceID": 15, "context": ", 2016), which is implemented on the open-source Joshua toolkit (Post et al., 2013).", "startOffset": 64, "endOffset": 83}, {"referenceID": 13, "context": "The simplification model is optimized to the SARI metric and leverages the PPDB dataset (Pavlick and Callison-Burch, 2016) as a rich source of simplification operations.", "startOffset": 88, "endOffset": 122}, {"referenceID": 2, "context": "The sequence-to-sequence model is the state-of-the-art neural machine translation model (Cho et al., 2014) with the attention mechanism applied (Bahdanau et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 0, "context": ", 2014) with the attention mechanism applied (Bahdanau et al., 2014).", "startOffset": 45, "endOffset": 68}, {"referenceID": 7, "context": "Automatic Evaluation To evaluate the performance of different methods for the simplification task, we leverage four automatic evaluation metrics4: Flesch-Kincaid grade level (FK) (Kincaid et al., 1975), SARI (Xu et al.", "startOffset": 179, "endOffset": 201}, {"referenceID": 23, "context": ", 1975), SARI (Xu et al., 2016), BLEU (Papineni et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 12, "context": ", 2016), BLEU (Papineni et al., 2002) and iBLEU (Sun and Zhou, 2012).", "startOffset": 14, "endOffset": 37}, {"referenceID": 19, "context": ", 2002) and iBLEU (Sun and Zhou, 2012).", "startOffset": 18, "endOffset": 38}, {"referenceID": 25, "context": "Several studies indicate that BLEU alone is not really suitable for the simplification task (Zhu et al., 2010; \u0160tajner et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 149}, {"referenceID": 21, "context": "Several studies indicate that BLEU alone is not really suitable for the simplification task (Zhu et al., 2010; \u0160tajner et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 149}, {"referenceID": 23, "context": "Several studies indicate that BLEU alone is not really suitable for the simplification task (Zhu et al., 2010; \u0160tajner et al., 2015; Xu et al., 2016).", "startOffset": 92, "endOffset": 149}, {"referenceID": 22, "context": "Following earlier studies (Wubben et al., 2012; Xu et al., 2016), we asked participants to rate Gram-", "startOffset": 26, "endOffset": 64}, {"referenceID": 23, "context": "Following earlier studies (Wubben et al., 2012; Xu et al., 2016), we asked participants to rate Gram-", "startOffset": 26, "endOffset": 64}], "year": 2017, "abstractText": "Sentence simplification reduces semantic complexity to benefit people with language impairments. Previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. For sentencelevel studies, sentences after simplification are fluent but sometimes are not really simplified. For word-level studies, words are simplified but also have potential grammar errors due to different usages of words before and after simplification. In this paper, we propose a two-step simplification framework by combining both the word-level and the sentence-level simplifications, making use of their corresponding advantages. Based on the twostep framework, we implement a novel constrained neural generation model to simplify sentences given simplified words. The final results on Wikipedia and Simple Wikipedia aligned datasets indicate that our method yields better performance than various baselines.", "creator": "LaTeX with hyperref package"}}}