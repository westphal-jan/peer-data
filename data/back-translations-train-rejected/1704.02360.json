{"id": "1704.02360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities", "abstract": "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC.", "histories": [["v1", "Mon, 10 Apr 2017 12:35:33 GMT  (578kb)", "http://arxiv.org/abs/1704.02360v1", null], ["v2", "Mon, 17 Apr 2017 04:43:37 GMT  (583kb)", "http://arxiv.org/abs/1704.02360v2", "Submitted to INTERSPEECH 2017"], ["v3", "Mon, 22 May 2017 08:11:02 GMT  (583kb)", "http://arxiv.org/abs/1704.02360v3", "Accepted to INTERSPEECH 2017"], ["v4", "Mon, 7 Aug 2017 02:42:01 GMT  (675kb)", "http://arxiv.org/abs/1704.02360v4", "Accepted to INTERSPEECH 2017"]], "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["hiroyuki miyoshi", "yuki saito", "shinnosuke takamichi", "hiroshi saruwatari"], "accepted": false, "id": "1704.02360"}, "pdf": {"name": "1704.02360.pdf", "metadata": {"source": "CRF", "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities", "authors": ["Hiroyuki Miyoshi", "Yuki Saito"], "emails": ["mathma1306@gmail.com", "saruwatari}@ipc.i.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.02 360v 1 [cs.S D] 10 Apr 201 7of context posterior probabilities is proposed. Conventional VC with shared context posterior probabilities predicts target language parameters from the context posterior probabilities, estimated from the source language parameters. Although conventional VC with shared context posterior probabilities can be constructed from non-parallel data, it is difficult to convert speaker individuality such as the phonetic property and speech rate contained in the posterior probabilities, since the initial probabilities posterior are directly used to predict target language parameters. In this thesis we assume that the training data partly contain parallel language data and suggest sequence-to-sequence learning between source and target posterior probabilities. The conversion models perform a nonlinear and variable longitudinal transformation from the source probability sequence to the target text. Furthermore, we propose a common training algorithm for the sequence probability detection modules proposed for the conversion of the conversion, which forms the contextual, in contrast to conventional methods."}, {"heading": "1. Introduction", "text": "It is mainly divided into two types: text-independent VCs and text-dependent VCs. Text-independent VCs directly predict target language parameters from the source parameters and acoustic models such as Gaussel mixture models [4, 5] or deep neural networks. Since the models are often trained with parallel speech data, the conversion quality of the language parameters is typically very accurate. However, parallel data such as mouse mixture models [4, 5] or deep neural networks [5] are not available."}, {"heading": "2. VC Using Shared Context Posterior Probabilities", "text": "Conventional VCs with common posterior probabilities in context [9] contain two modules: speech recognition and speech synthesis. They are trained separately, and voice conversion occurs by concatenating these probabilities. Figure 1 shows an example of posterior probabilities in context. The upper and middle parts of Figure 2 show the details of these processes."}, {"heading": "2.1. Training Stage", "text": "Detection models estimate the context-dependent probability sequence based on the language parameter sequence. Let x = [x 1, \u00b7 \u00b7, x Tx] and y = [y 1, \u00b7 \u00b7, y Ty] be source and target parameter sequences, resp. Let l (x) = [l (x) 1, \u00b7 \u00b7 \u00b7, l (y) be the parameters for frames. Tx and Ty are their frame lengths. Let l (x) = [l (x) 1, \u00b7, l (x) Tx) and l (y) be the context marking sequence (like Quin-Phone) corresponding to x or y. Speaker-independent neural network R (\u00b7) is trained based on language data including x and y, and the training criterion minimizes transverse entropy LC (lx, R (x)). Synthesis models predict target parameter sequences from the context, including x and g (y)."}, {"heading": "2.2. Conversion Stage", "text": "In the conversion, the converted language parameter sequence y-x is predicted by concatenation of speech recognition and speech synthesis, i.e. y-x = G (p-x) = G (R (x)), where p-x is the context probability sequence of x. Note that the image lengths of x, p-x and y-x are equal, i.e. Tx."}, {"heading": "2.3. Problems", "text": "Since the posterior probabilities estimated in speech recognition are used directly for speech synthesis, it is difficult to convert the individuality of the speaker contained in the posterior probabilities, such as speech rate (image length) and phonetic properties (see Fig. 1). Also, improving recognition accuracy does not always improve speech quality in converted language (with the exception of zero recognition errors)."}, {"heading": "3. Proposed VC using Sequence-to-Sequence Learning of Context Posterior Probabilities", "text": "In order to overcome the limitations of the conventional method, we propose an approach for converting posterior probabilities from the source context to posterior probabilities from the target context, using sequenceto sequence learning."}, {"heading": "3.1. Sequence-to-Sequence Learning", "text": "Sequence-to-sequence learning using recursive neural networks (RNNs) [13] can be applied to the problem that source and target sequences have different lengths. An encoder decoder model used here maps a variable-length source sequence to a fixed-length vector and maps the vector to the variable-length target sequence. For each frame, the source-side RNN (encoder) and target-side RNN (decoder) predict the source or target characteristics of the next frame. As discussed below, we do this in order to convert a posterior probability sequence into a target of varying length."}, {"heading": "3.2. Training Stage", "text": "We propose two algorithms to perform the posterior probability conversion: The first algorithm trains the speech reproduction separately! \"# $#% & '(' ()!\" # $% & '() $* + $), -. $/! * 01 / (2 $! * + $), -. $/! * 0! \"# $3 & *! $(\" & (& 4 * 0 / \"* + (, -%) + (/ 01 + (, -) (2' + (& 3401 / (2 $! + & *! $(\" & - + (& 4 * 0nition and synthesis like conventional algorithms) and trains the probability conversion models separately based on the source and target probabilities."}, {"heading": "3.2.1. Training of probability conversion models", "text": "Given the parallel sequences of source and target context posterior probabilities, we train the encoder decoder models C (\u00b7), which convert the source and target sequences, and the loss function to be minimized is as follows: L (ly, p, x, p, y) = LG (p, y, C (p, x)) + LC (ly, C (p, x)). (1) The first term minimizes the conversion error between the predicted and target sequences. The second term minimizes cross entropy by means of ly, which was achieved by training R (\u00b7), and can reduce the detection error contained in p, y. Our preliminary assessment has shown that using this formulation leads to better conversion accuracy than just using the first term. Considering phoneme-to-sequence learning suffers from long-term dependencies, i.e., the error accumulation is carried out in our phoneme-probability conversion approach."}, {"heading": "3.2.2. Jointly training of recognition, synthesis, and conversion", "text": "Since the ultimate goal of the method is to minimize synthesis errors, its preliminary processes (i.e. detection and conversion) must be trained by taking this error into account. We train speech recognition R (\u00b7) to minimize not only detection errors, but also synthesis errors (e.g. reconstruction errors of auto-encoders): The loss function is LC (lx, R (x)) + LG (x, G (R (x))). Language synthesis G (\u00b7) is trained in the conventional way. We train the conversion models to minimize not only conversion errors, but also synthesis errors: The loss function is the sum of equivalent (1) and LG (G (y, C (p (x))))))."}, {"heading": "3.3. Discussion", "text": "Text-dependent C aligns the segment of the input language into a single context (e.g. phoneme, syllable, or unit of words) and generates language characteristics from the context sequence. Although this method can flexibly transform the context sequence (e.g. variable length conversion), it cannot avoid the effect of time quantization by mapping language characteristic segments. Meanwhile, text-independent C with dynamic time distortion (DTW) [4] corresponds to the orientation of language characteristics at the frame level, but limits the transformation of the (implicitly considered) context sequence, e.g. the sequence length is fixed. The latter also corresponds to the conventional method [9, 10] because the posterior probability of the source is used directly for synthesis of the target language. Compared to these methods, since the proposed algorithm performs the conversion at the frame level without forced alignments, it can avoid the effect of time quantization and convert the context flexibly."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setup", "text": "In fact, it is that we are able to assert ourselves, that we are able to establish ourselves in the world, and that we are able to assert ourselves in the world, that we are able to stay in the world, \"he said."}, {"heading": "4.2. Evaluations", "text": "We discuss the effectiveness of the proposed posterior probability conversion. The separately trained modules were used."}, {"heading": "4.2.1. Objective Evaluation", "text": "The difference between the two methods is the time-warping method, i.e. DTW or sequence-to-sequence learning. The results (Fig. 3) clearly show that the proposed VC exceeds conventional VC, we show that spectral distortions caused by DTW can be alleviated by the use of sequence-to-sequence learning."}, {"heading": "4.2.2. Subjective Evaluation", "text": "In order to subjectively evaluate the conventional and proposed VC, we performed a preference AB test to evaluate the converted speech quality. We presented each pair of converted language of the two groups in a random order and let the listeners select the better-sounding speech sample. Similarly, we performed an XAB test on speaker individuality, using natural language as a reference \"X.\" Seven listeners participated in each assessment.The results are in Fig. 4. Although the proposed VC performs better thanks to the posterior probability conversion (Fig. 4 (a)) in terms of the similarity of the speakers, it degrades the speech quality (Fig. 4 (b). It appears that the probability conversion caused a conversion error that missed the phonetic properties of the initial parameters, which probably led to the deteriorated quality."}, {"heading": "4.3. Evaluation of Joint Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1. Joint Training of Recognition and Synthesis", "text": "We investigated the effectiveness of joint training of recognition and synthesis modules compared to traditional separately trained modules [9]. We calculated mel-ceptral distortion in the auto-encoding case, i.e. a reconstruction error in source language parameters due to recognition and synthesis. As shown in Figure 5, the proposed joint training achieved better distortion than traditional separate training. We also performed an AB test on speech quality and an XAB test on speaker similarity in VC case, similar to Figure 4.2.2. As shown in Figure 6, the proposed joint training exceeds conventional separate training in both speaker similarity and speech quality."}, {"heading": "4.3.2. Joint Training of Recognition, Conversion, and Synthesis", "text": "In order to evaluate the common recognition, conversion and synthesis training, we calculated the distortion of the brain of three systems: (1) conventional VCs [9], (2) separately trained modules (equal to \"suggested\" in Section 4.2.1) and (3) jointly trained modules. The results are in Figure 7. Joint training performs better than conventional VCs but worse than separate training. To illustrate this, we show an example of a probability sequence estimated by speech recognition in Figure 8. The separately trained recognition module gives harder probabilities, i.e. the values for all frames are close to 0 or 1. However, we see that joint recognition and synthesis training tends to soften the values. This requires more in-depth investigation, but we suspect that this tendency is one of the reasons."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed to perform speech conversion (UK) by sequence-to-sequence learning of posterior probabilities in context. As conventional UK uses the posterior probabilities of the source language directly to predict the target language, it is difficult to transform the individuality of the speaker contained in the posterior probabilities. To address this problem, we developed sequence-to-sequence conversion models that transform the posterior probability sequence of the source context into a target. Furthermore, we proposed common training algorithms for speech recognition, speech synthesis and posterior probability conversion. Experimental results showed that (1) the proposed algorithms outperformed conventional UK in terms of speaker similarity, and (2) joint recognition and synthesis training outperformed conventional UK in terms of both speaker similarity and speech quality."}, {"heading": "6. References", "text": "[1] A. B. Cain, J.-P. Hosom, X. Niu, J. P. H. van Santen, M. F. Oken, and J. Staehely, \"Improving the intelligibility of dysarthric speech?,\" Speech Communication, vol. 49, no. 9, pp. 743-759, 2007. [2] F. Rudzicz, \"Acoustic transformations to improve the intelligibility of dysarthric speech?,\" in Proc. SLPAT, Edinburgh, Scotland, Jul. 2011, pp. 11-21. [3] S. Aryal and R. Osuna, \"Can voice conversion be used to reduce non-native accents in Proc. ICASSP, Florence, Italy, pp. 7929-7933. [4] T. Toda, A. W. Black, and K. Tokuda,\" voice conversion based on maximum likelihood."}], "references": [{"title": "Improving the intelligibility of dysarthric speech", "author": ["A.B. Kain", "J.-P. Hosom", "X. Niu", "J.P.H. van Santen", "M.F.- Oken", "J. Staehely"], "venue": "Speech Communication, vol. 49, no. 9, pp. 743\u2013759, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Acoustic transformations to improve the intelligibility of dysarthric speech", "author": ["F. Rudzicz"], "venue": "Proc. SLPAT, Edinburgh, Scotland, Jul. 2011, pp. 11\u201321.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "G.-Osuna, \u201cCan voice conversion be used to reduce non-native accents?", "author": ["R.S. Aryal"], "venue": "in Proc. ICASSP, Florence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Voice conversion based on maximum likelihood estimation of spectral parameter trajectory", "author": ["T. Toda", "A.W. Black", "K. Tokuda"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2222\u20132235, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Continuous probabilistic transform for voice conversion", "author": ["Y. Stylianou", "O. Capp\u00e9", "E. Moulines"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 6, no. 2, pp. 131\u2013142, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Voice conversion using artificial neural networks", "author": ["S. Desai", "E.V. Raghavendra", "B. Yegnanarayana", "A.W. Black", "K. Prahallad"], "venue": "Proc. ICASSP, Taipei, Taiwan, Apr. 2009, pp. 3893\u20133896.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral voice conversion for text-tospeech synthesis", "author": ["A. Kain", "M.W. Macon"], "venue": "Proc. ICASSP, Seattle, U.S.A., May 1998, pp. 285\u2013288.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Text-independent voice conversion based on unit selection", "author": ["D. Sunderman", "H. Hoge", "A. Bonafonte", "H. Ney", "A.W. Black", "S. Narayanan"], "venue": "Proc. ICASSP, Toulouse, France, May 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training", "author": ["L. Sun", "K. Li", "H. Wang", "S. Kang", "H. Meng"], "venue": "Proc. ICME, Seattle, U.S.A., Jul. 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Personalized, cross-lingual TTS using phonetic posteriorgrams", "author": ["L. Sun", "S. Kang", "K. Li", "H. Meng"], "venue": "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 322\u2013326.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A KL divergence and DNNbased approach to voice conversion without parallel training sentences", "author": ["F.-L. Xie", "F.K. Soong", "H. Li"], "venue": "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 287\u2013291.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["K. Cho", "D. Bahdanau", "F. Vougares", "H. Schwenk", "Y. Bengio"], "venue": "Proc. EMNLP, Doha, Qatar, Oct. 2014, pp. 1724\u20131734.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3104\u20133112.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Proc. NIPS, Vancouver, Canada, Dec. 2006, pp. 153\u2013160.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Dual learning for machine translation", "author": ["D. He", "Y. Xia", "T. Qin", "L. Wang", "N. Yu", "T. Liu", "W.-Y. Ma"], "venue": "Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 820\u2013828.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "Proc. ICLR, Banff, Canada, Apr. 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Voice conversion from non-parallel corpora using variational auto-encoder", "author": ["C.-C. Hsu", "H.-T. Hwang", "Y.-C. Wu", "Y. Tsao", "H.-M. Wang"], "venue": "Proc. APSIPA ASC, Jeju, Korea, Dec. 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3581\u20133589.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A large-scale Japanese speech database", "author": ["Y. Sagisaka", "K. Takeda", "M. Abe", "S. Katagiri", "T. Umeda", "H. Kuawhara"], "venue": "IC- SLP90, Kobe, Japan, Nov. 1990, pp. 1089\u20131092.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1990}, {"title": "Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT", "author": ["H. Kawahara", "J. Estill", "O. Fujimura"], "venue": "MAVEBA 2001, Firentze, Italy, Sep. 2001, pp. 1\u20136.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Maximum likelihood voice conversion based on GMM with STRAIGHT mixed excitation", "author": ["Y. Ohtani", "T. Toda", "H. Saruwatari", "K. Shikano"], "venue": "Proc. INTERSPEECH, Pittsburgh, U.S.A., Sep. 2006, pp. 2266\u20132269.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A.D. Cheveigne"], "venue": "Speech Communication, vol. 27, no. 3\u20134, pp. 187\u2013207, 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "The NAIST text-to-speech system for the Blizzard Challenge 2015", "author": ["S. Takamichi", "K. Kobayashi", "K. Tanaka", "T. Toda", "S. Nakamura"], "venue": "Proc. Blizzard Challenge workshop, Berlin, Germany, Sep. 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, Jul. 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-task learning deep neural networks for speech feature denoising", "author": ["B. Huang", "D. Ke", "H. Zheng", "B. Xu", "Y. Xu", "K. Su"], "venue": "Proc. INTERSPEECH, Dresden, Germany, Sep. 2015, pp. 2464\u2013 2468.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "First step towards end-to-end parametric TTS synthesis: Generating spectral parameters with neural attention", "author": ["W. Wang", "S. Xu", "B. Xu"], "venue": "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 2243\u20132247.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].", "startOffset": 67, "endOffset": 73}, {"referenceID": 1, "context": "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.", "startOffset": 150, "endOffset": 156}, {"referenceID": 4, "context": "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.", "startOffset": 150, "endOffset": 156}, {"referenceID": 5, "context": "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 7, "context": "Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 8, "context": "VC using shared context posterior probabilities [9] is classified in text-dependent VC, but the conversion unit is frame level.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11].", "startOffset": 101, "endOffset": 109}, {"referenceID": 10, "context": "This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11].", "startOffset": 101, "endOffset": 109}, {"referenceID": 11, "context": "Assuming that the training data partly include parallel speech data (parallel utterances of phrases), we build an encoder-decoder model [12] that converts the posterior probabilities of the source speech parameters into those of the target speech parameters.", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "When we do not build the conversion model or do not have parallel data, conventional VC [9] is available.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Whereas the conventional VC [9] separately trains speech recognition and speech synthesis, our approach jointly trains these modules (like auto-encoding) and the proposed conversion module.", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "We found through experiment that the proposed methods outperform the conventional VC [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 8, "context": "Conventional VC using shared context posterior probabilities [9] contains two modules: speech recognition and speech synthesis.", "startOffset": 61, "endOffset": 64}, {"referenceID": 12, "context": "Sequence-to-sequence learning using recurent neural networks (RNNs) [13] can be applied to the problem that the source and target sequences have different lengths.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Meanwhile, text-independent VC with dynamic time warping (DTW) [4] aligns speech features in a frame level, but it limits the transformation of (implicitly considered) context sequence, e.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech.", "startOffset": 24, "endOffset": 31}, {"referenceID": 9, "context": "The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech.", "startOffset": 24, "endOffset": 31}, {"referenceID": 13, "context": "2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Therefore, we expect that these processes can be extended to the supervised learning of variational auto-encoders [16] that have not only class labels (e.", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": ", context labels) but also the hidden variables [17, 18].", "startOffset": 48, "endOffset": 56}, {"referenceID": 17, "context": ", context labels) but also the hidden variables [17, 18].", "startOffset": 48, "endOffset": 56}, {"referenceID": 8, "context": "Although the conventional VC [9] and proposed VC accept non-parallel speech data and partly included parallel data, we used fully parallel data in this evaluation.", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "We prepared speech data of eight speakers taken from the ATR Japanese speech database [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "The 0th\u2013through\u201324th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters.", "startOffset": 109, "endOffset": 117}, {"referenceID": 20, "context": "The 0th\u2013through\u201324th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters.", "startOffset": 109, "endOffset": 117}, {"referenceID": 21, "context": "We used the STRAIGHT analysis-synthesis system [22] for the parameter extraction and waveform synthesis.", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "To improve training accuracy, speech parameter trajectory smoothing [23] with a 50 Hz cutoff modulation frequency was applied to the spectral parameters in the training data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "We used AdaGrad [24] as the optimization algorithm, setting the learning rate to 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units.", "startOffset": 102, "endOffset": 109}, {"referenceID": 9, "context": "Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units.", "startOffset": 102, "endOffset": 109}, {"referenceID": 24, "context": "The cross-entropy loss was calculated for each group, and the loss function for training the recognition models was the sum of each loss [25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "In the proposed method, F0 was linearly transformed [4] first, and we modified its length using DTW between the source context posterior probability sequence and the converted posterior probability sequence.", "startOffset": 52, "endOffset": 55}, {"referenceID": 25, "context": "This evaluation uses reference phoneme duration for converting posterior probabilities in order to address the sequence length determination problem to which sequence-to-sequence learning is prone [26].", "startOffset": 197, "endOffset": 201}, {"referenceID": 8, "context": "We calculated mel-cepstral distortion between the target and converted speech parameters of the conventional VC [9] and proposed VC.", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "We evaluated the effectiveness of joint training of recognition and synthesis modules, in comparison to conventional separately trained modules [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "To evaluate of the joint training of recognition, conversion, and synthesis, we calculated the mel-cepstral distortion of three systems: (1) conventional VC [9], (2) separately trained modules (equal to \u201dProposed\u201d in Section 4.", "startOffset": 157, "endOffset": 160}], "year": 2017, "abstractText": "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC.", "creator": "LaTeX with hyperref package"}}}