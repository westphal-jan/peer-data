{"id": "1704.02378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Uncovering Group Level Insights with Accordant Clustering", "abstract": "Clustering is a widely-used data mining tool, which aims to discover partitions of similar items in data. We introduce a new clustering paradigm, \\emph{accordant clustering}, which enables the discovery of (predefined) group level insights. Unlike previous clustering paradigms that aim to understand relationships amongst the individual members, the goal of accordant clustering is to uncover insights at the group level through the analysis of their members. Group level insight can often support a call to action that cannot be informed through previous clustering techniques. We propose the first accordant clustering algorithm, and prove that it finds near-optimal solutions when data possesses inherent cluster structure. The insights revealed by accordant clusterings enabled experts in the field of medicine to isolate successful treatments for a neurodegenerative disease, and those in finance to discover patterns of unnecessary spending.", "histories": [["v1", "Fri, 7 Apr 2017 21:31:59 GMT  (1140kb,D)", "http://arxiv.org/abs/1704.02378v1", "accepted to SDM 2017 (oral)"]], "COMMENTS": "accepted to SDM 2017 (oral)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit dhurandhar", "margareta ackerman", "xiang wang"], "accepted": false, "id": "1704.02378"}, "pdf": {"name": "1704.02378.pdf", "metadata": {"source": "CRF", "title": "Uncovering Group Level Insights with Accordant Clustering", "authors": ["Amit Dhurandhar", "Margareta Ackerman", "Xiang Wang"], "emails": ["adhuran@us.ibm.com", "margareta.ackerman@sjsu.edu", "xiangwa@google.com."], "sections": [{"heading": "1 Introduction", "text": "In this context, it should also be mentioned that they are two groups that are able to outdo each other and outdo each other."}, {"heading": "2 New Clustering Framework", "text": "It is not as if we are able to justify the accumulation of e-mails and e-mails. (r) We are not able to prevent the accumulation of e-mails. (r) We are not able to prevent the accumulation of e-mails. (r) We are not able to prevent the accumulation of e-mails. (r) We are not able to prevent the accumulation of e-mails. (r) We are not able to process the accumulation of e-mails. (r) -definition 2.1. (r) -corresponding accumulation of e-mails. (r) We are unable to process the accumulation of e-mails. (r) We are unable to process the accumulation of e-mails. (r) We are unable to process the accumulation of e-mails. (r) We are unable to process the accumulation of e-mails. (r) We are unable to process the accumulation of e-mails. (r) We are unable to process the accumulation of e-mails."}, {"heading": "4 Qualitative Guarantees", "text": "In the case of each clustering paradigm, this property reflects a unique feeling that is unique in terms of the cost of a dataset, but it is also unique in terms of the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the amount of data it collects, the number of data it collects, the number of data it collects, the number of data it collects, and the number of data it collects."}, {"heading": "5 Experiments", "text": "In practice, when we apply two different approaches, the concepts of appropriate clustering lead to insights that provided this attitude. \"The supplementary material is on the first page of the website on which the individual approaches are based.\" The first instance is in the field of medicine, with a dataset representing patients with neurodegenerative disease, each of whom belongs to one of five different treatment groups that are applied depending on the way they are treated. The second instance was in the field of economics, for which we represent two years of transactional data from a large company, each transaction falling into one of 25 categories (such as IT, research, marketing, etc.) In addition, we also conduct experiments on 6 UCI datasets, which the power of our method in detecting higher quality correspondingly clusterings.These datasets provide a benchmark for measuring the quality of clustering achieved by the means of the actors."}, {"heading": "6 Discussion", "text": "In this paper, we propose a novel cluster paradigm for the discovery of cluster-level insights. We propose an algorithm based on the k-means method, which produces corresponding clusters and has been shown to reveal near-optimal solutions to cluster data. In addition, we described two real-life scenarios in which our algorithm significantly outperformed its customized competitors and provided actionable insights. The superior performance of our algorithm was further confirmed by experiments with the 6 UCI datasets. In all cases, our method approached in less than 20 sections. Given the novelty of our framework, this is a silver bullet for innovation - especially in exploring new applications from which group-level insights can benefit. Additional information about costs or penalties can also be included to enable informed action, and our limitations would serve as a starting point from which additional limitations or regulatory terms can be added. A variety of algorithmic challenges could also be addressed in the future, such as addressing other cluster-level insights, such as those related to cluster-level ones that we do better."}, {"heading": "Acknowledgement", "text": "We would like to thank Joydeep Ghosh, Kiri Wagstaff and Charu Aggarwal for their helpful suggestions."}], "references": [{"title": "Clusterability: A theoretical study", "author": ["M. Ackerman", "S. Ben-David"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Weighted clustering", "author": ["M. Ackerman", "S.B.-D.S. Branzei", "D. Loker"], "venue": "In Proceedings of the 26th Conference on Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Data Clustering: Algorithms and Applications", "author": ["C. Aggarwal", "C. Reddy"], "venue": "CRC Press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Adapting kmeans for supervised clustering", "author": ["S. Al-Harbi", "V. Rayward-Smith"], "venue": "Applied Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Approximate clustering without the approximation", "author": ["M.-F. Balcan", "A. Blum", "A. Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Statistical Methods in Molecular Biology", "author": ["H. Bang", "X. Zhou", "H. Epps", "M. Mazumdar"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Constrained Clustering: Advances in Algorithms, Theory, and Applications", "author": ["S. Basu", "I. Davidson", "K. Wagstaff"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Clustering with constraints: Feasibility issues and the k-means algorithm", "author": ["I. Davidson", "S. Ravi"], "venue": "In Siam Conference on Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Using cluster analysis for market segmentation", "author": ["S. Dolnicar"], "venue": "Australasian Journal of Market Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Supervised clustering ? algorithms and benefits", "author": ["C. Eick", "N. Zeidat", "Z. Zhao"], "venue": "In proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Supervised clustering with support vector machines", "author": ["T. Finley", "T. Joachims"], "venue": "In Proceedings of the 22Nd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Supervised k-means clustering", "author": ["T. Finley", "T. Joachims"], "venue": "In Technical Report", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "An overview on subgroup discovery: foundations and applications", "author": ["F. Herrera", "C. Carmona", "P. Gonzlez", "M. Jesus"], "venue": "Know. and Inf. Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Making large-Scale SVM Learning Practical chapter in Advances in Kernel Methods - Support Vector Learning", "author": ["T. Joachims"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Spectral learning", "author": ["S.D. Kamvar", "D. Klein", "C.D. Manning"], "venue": "In IJCAI, page 561?566,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "IEEE Symposium on the Foundations of Computer Science (FOCS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schroedl"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Labels vs. pairwise constraints: A unified view of label propagation and constrained spectral clustering", "author": ["X. Wang", "B. Qian", "I. Davidson"], "venue": "In ICDM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Semisupervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}], "referenceMentions": [{"referenceID": 5, "context": "As one of the most fundamental data mining tools, clustering is employed in a variety of domains that span from biology [6] to marketing [9], applicable in nearly all disciplines where data is utilized.", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "As one of the most fundamental data mining tools, clustering is employed in a variety of domains that span from biology [6] to marketing [9], applicable in nearly all disciplines where data is utilized.", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": ", nr being the sizes of the r smallest groups, then \u2200t \u2208 [0, 1] there exists an (r, t)-accordant clustering iff k \u2208 {1, .", "startOffset": 57, "endOffset": 63}, {"referenceID": 2, "context": "To this end, a wide variety of objective functions and algorithms have been proposed, falling into a fairly large number of distinct clustering paradigms, which vary in the types of input and output required for clustering methods [3].", "startOffset": 231, "endOffset": 234}, {"referenceID": 1, "context": "One such variation allows the user to specify a weight [2] representing the sig-", "startOffset": 55, "endOffset": 58}, {"referenceID": 16, "context": "Perhaps the most popular semi-supervised setting allows certain pairs of instances to be marked as mustlink (ML) or cannot-link (CL) [17, 7].", "startOffset": 133, "endOffset": 140}, {"referenceID": 6, "context": "Perhaps the most popular semi-supervised setting allows certain pairs of instances to be marked as mustlink (ML) or cannot-link (CL) [17, 7].", "startOffset": 133, "endOffset": 140}, {"referenceID": 7, "context": "If such constraints are feasible [8], the final clustering is likely to have semantic value that is of use to the practitioner.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "In the extreme case of supervised clustering [10, 11, 4] the entire dataset is labeled.", "startOffset": 45, "endOffset": 56}, {"referenceID": 10, "context": "In the extreme case of supervised clustering [10, 11, 4] the entire dataset is labeled.", "startOffset": 45, "endOffset": 56}, {"referenceID": 3, "context": "In the extreme case of supervised clustering [10, 11, 4] the entire dataset is labeled.", "startOffset": 45, "endOffset": 56}, {"referenceID": 12, "context": "Our framework is also different from subgroup discovery [13], which mainly tries to find rules in conjunctive form relative to a given target.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "One of the most insightful and widely-used notions of clusterability related to the k-means objective function is the (c, )-property [1, 5, 16], which describes a dataset characterized by a unique clustering that optimizes the objective (see Balcan et.", "startOffset": 133, "endOffset": 143}, {"referenceID": 4, "context": "One of the most insightful and widely-used notions of clusterability related to the k-means objective function is the (c, )-property [1, 5, 16], which describes a dataset characterized by a unique clustering that optimizes the objective (see Balcan et.", "startOffset": 133, "endOffset": 143}, {"referenceID": 15, "context": "One of the most insightful and widely-used notions of clusterability related to the k-means objective function is the (c, )-property [1, 5, 16], which describes a dataset characterized by a unique clustering that optimizes the objective (see Balcan et.", "startOffset": 133, "endOffset": 143}, {"referenceID": 4, "context": "al [5] for a detailed exposition).", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "((c, )-property [5]) A dataset (X, d) satisfies the (c, )-property for objective function \u03a6 if for every k-clustering C of X where \u03a6(C) \u2264 c \u00b7 OPT\u03a6, the relation dist(C, C\u2217) < holds, where C\u2217 is the clustering that optimizes the value of \u03a6.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Specifically, the methods chosen for comparison are as follows: 1) Supervised k-means (Skmeans) [4], 2) SVM based supervised iterative k-means (SSIkmeans) [12], 3) Constrained k-means (COPkmeans) [17], 4) Constrained spectral clustering (CSC) [15, 18] and 5) Semisupervised learning based on Gaussian fields and harmonic functions (GFHF) [19].", "startOffset": 96, "endOffset": 99}, {"referenceID": 11, "context": "Specifically, the methods chosen for comparison are as follows: 1) Supervised k-means (Skmeans) [4], 2) SVM based supervised iterative k-means (SSIkmeans) [12], 3) Constrained k-means (COPkmeans) [17], 4) Constrained spectral clustering (CSC) [15, 18] and 5) Semisupervised learning based on Gaussian fields and harmonic functions (GFHF) [19].", "startOffset": 155, "endOffset": 159}, {"referenceID": 16, "context": "Specifically, the methods chosen for comparison are as follows: 1) Supervised k-means (Skmeans) [4], 2) SVM based supervised iterative k-means (SSIkmeans) [12], 3) Constrained k-means (COPkmeans) [17], 4) Constrained spectral clustering (CSC) [15, 18] and 5) Semisupervised learning based on Gaussian fields and harmonic functions (GFHF) [19].", "startOffset": 196, "endOffset": 200}, {"referenceID": 14, "context": "Specifically, the methods chosen for comparison are as follows: 1) Supervised k-means (Skmeans) [4], 2) SVM based supervised iterative k-means (SSIkmeans) [12], 3) Constrained k-means (COPkmeans) [17], 4) Constrained spectral clustering (CSC) [15, 18] and 5) Semisupervised learning based on Gaussian fields and harmonic functions (GFHF) [19].", "startOffset": 243, "endOffset": 251}, {"referenceID": 17, "context": "Specifically, the methods chosen for comparison are as follows: 1) Supervised k-means (Skmeans) [4], 2) SVM based supervised iterative k-means (SSIkmeans) [12], 3) Constrained k-means (COPkmeans) [17], 4) Constrained spectral clustering (CSC) [15, 18] and 5) Semisupervised learning based on Gaussian fields and harmonic functions (GFHF) [19].", "startOffset": 243, "endOffset": 251}, {"referenceID": 18, "context": "Specifically, the methods chosen for comparison are as follows: 1) Supervised k-means (Skmeans) [4], 2) SVM based supervised iterative k-means (SSIkmeans) [12], 3) Constrained k-means (COPkmeans) [17], 4) Constrained spectral clustering (CSC) [15, 18] and 5) Semisupervised learning based on Gaussian fields and harmonic functions (GFHF) [19].", "startOffset": 338, "endOffset": 342}, {"referenceID": 10, "context": "SSIkmeans was implemented by installing the python interfaces [11] to SVMlight [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "SSIkmeans was implemented by installing the python interfaces [11] to SVMlight [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "3 UCI datasets We also evaluated our methods on 6 UCI datasets used in previous clustering studies [18] namely: a) Glass, b) Heart, c) Ionosphere, d) Breast Cancer, e) Iris and f) Wine.", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "Clustering is a widely-used data mining tool, which aims to discover partitions of similar items in data. We introduce a new clustering paradigm, accordant clustering, which enables the discovery of (predefined) group level insights. Unlike previous clustering paradigms that aim to understand relationships amongst the individual members, the goal of accordant clustering is to uncover insights at the group level through the analysis of their members. Group level insight can often support a call to action that cannot be informed through previous clustering techniques. We propose the first accordant clustering algorithm, and prove that it finds near-optimal solutions when data possesses inherent cluster structure. The insights revealed by accordant clusterings enabled experts in the field of medicine to isolate successful treatments for a neurodegenerative disease, and those in finance to discover patterns of unnecessary spending.", "creator": "LaTeX with hyperref package"}}}