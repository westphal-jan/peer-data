{"id": "1704.02399", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Stein Variational Policy Gradient", "abstract": "Policy gradient methods have been successfully applied to many complex reinforcement learning problems. However, policy gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. We then propose a novel Stein variational policy gradient method (SVPG) which combines existing policy gradient methods and a repulsive functional to generate a set of diverse but well-behaved policies. SVPG is robust to initialization and can easily be implemented in a parallel manner. On continuous control problems, we find that implementing SVPG on top of REINFORCE and advantage actor-critic algorithms improves both average return and data efficiency.", "histories": [["v1", "Fri, 7 Apr 2017 23:24:07 GMT  (4246kb,D)", "http://arxiv.org/abs/1704.02399v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yang liu", "prajit ramachandran", "qiang liu", "jian peng"], "accepted": false, "id": "1704.02399"}, "pdf": {"name": "1704.02399.pdf", "metadata": {"source": "CRF", "title": "Stein Variational Policy Gradient", "authors": ["Yang Liu", "Prajit Ramachandran", "Qiang Liu"], "emails": ["liu301@illinois.edu", "prajitram@gmail.com", "qiang.liu@dartmouth.edu", "jianpeng@illinois.edu"], "sections": [{"heading": null, "text": "Political gradient methods have been successfully applied to many complex amplification problems, but political gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this paper, we present a framework for maximum entropy policy optimization that explicitly promotes parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. Subsequently, we propose a novel Stein method for variational policy gradient (SVPG) that combines existing policy gradient methods and a repellent function to generate a range of different but well-restrained strategies. SVPG is robust against initialization and can be easily implemented in parallel. In continuous control problems, we find that implementing SVPG in addition to REINFORCE and benefit-actor-critic algorithms improves both average return and data efficiency."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2 Preliminaries", "text": "We present the background of reinforcement learning and discuss different methods of policy gradient assessment."}, {"heading": "2.1 Reinforcement Learning", "text": "The role of agents is to learn a policy that maximizes cumulative rewards. Formally, one considers an agent to be active over time."}, {"heading": "2.2 Policy Gradient Estimation", "text": "In policy-based amplification approaches, politics \u03c0 (a; s; \u03b8) has recently been considered very complex and iteratively improved by updating recommendations for optimizing the utility function J (\u03b8); here it is estimated using the notation J (\u03b8) = J (a). There are two main approaches for estimating the political course (SPSA (Spall, 1998), PEPG (Sehnke et al., 2010) and the evolutionary strategy approximations (Hansen, 2006, Mannor et al., 2003). The key idea of such methods is the use of a small number of random disturbances to approximate the gradient values of all parameters."}, {"heading": "3 Stein Variational Policy Gradient", "text": "In this section, we present our main framework: We start with the introduction of a framework to maximize entropy policy for learning higher-level strategies in Section 3.1 and then develop our stone variation gradient in Section 3.2."}, {"heading": "3.1 Maximum Entropy Policy Optimization", "text": "Instead of finding a single policy approach, here we consider the policy parameter \u03b8 as a random variable q and seek a distribution q (\u03b8) q to optimize the expected return. We also introduce a standard parameter distribution q0 to incorporate previous domain knowledge of parameters or provide the regulation of \u03b8. We formulate the optimization of q as the following regularized expected benefit problem: max q {Eq (\u03b8) [J (\u03b8)] \u2212 \u03b1D (q q q0)}, (5) where q maximizes the expected benefit, regulated by relative entropy or kullback conductor (KL) divergence D (q q q0) with a \"previous distribution\" q0, D (q q q0) = Eq (log q0) \u2212 rope form of expected usefulness, regulated by relative entropy or kullback conductor (KL) divergence D (q 0)."}, {"heading": "3.2 Stein Variational Gradient Descent", "text": "The accurate estimation of J (\u03b8) for a single policy often requires a large number of rollout samples. Instead, we hope to use the gradient information that provides a noisy direction in which the policy is updated. To this end, we use the Stein Method of Variable Gradient Descendancy (SVGD) for Bayesian Inference (Liu and Wang, 2016). SVGD is a non-parametric variable inference algorithm that uses efficient deterministic dynamics to transport a set of particles (SVGD)."}, {"heading": "4 Related Work", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the ruuuiuiu the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the ruuuuiu the rf\u00fc the rf\u00fc the ruiuiuiu the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "5 Experiments", "text": "In this section, we design experiments to 1) compare the SVPG versions of the policy gradient algorithms with their original versions in terms of convergence and data efficiency, 2) analyze the characteristics of the policy learned simultaneously from SVPG, 3) investigate the trade-off between exploration and exploitation in SVPG. For comparisons, we implement the SVPG and the original versions of two popular policy gradient algorithms: REINFORCE (Eq (2)) and the advantage actor critic (A2C) (Eq (4)). In the A2C implementations, we use the generalized advantage estimator (GAE) (Schulman et al., 2015b) for critics implemented in the rllab toolkit (Schulman et al., 2015b, Duan et al., 2016). Only the policy parameters are updated by SVPG, while the critics are updated with the normal TD error gradient."}, {"heading": "5.1 Experimental Setting", "text": "All experiments are performed with the OpenAI rllab development toolkit (Duan et al., 2016). Specifically, we name four classic continuous control tasks: Cartpole Swing-Up, Double Pendulums, Cartpole and MountainCar. The maximum length of the trajectories is set to 500. Details on these environments can be found in (Duan et al., 2016) and on the GitHub website. Here we describe the default setting of the political gradient algorithms we test on these control tasks. We used a Gaussian policy with diagonal covariance for all of these tasks. The mean is parameterized by a neural network with three hidden layers (100-50-25 hidden units) and tanh as an activation function. The log standard deviation is parameterized by a vector, as in (Duan et al., 2016, Schulman et al., 2015a)."}, {"heading": "5.2 Convergence and Data Efficiency", "text": "For the two lighter tasks, Mountain Car and Cartpole, almost all algorithms can successfully solve them in about 20 iterations (with the exception of REINFORCE-Joint on Mountain Car); for the two more difficult tasks, the differences between algorithms are significant; for Cartpole SwingUp, REINFORCE-SVPG and A2C-SVPG converge much faster than the corresponding baselines; A2CIndependent is able to achieve the desired performance after more than 800 iterations, while all other baselines cannot converge after 1,000 iterations; for Double Pendulum, we also performed the same training with a different batch size m = 5000 and found similar results; the deviations of the SVPG algorithms are similar to those of the independent versions, but much smaller than those of the joint versions, probably due to the robustness of several agents; these results show that SVPG is able to improve the convergence of policy-gradient algorithms."}, {"heading": "5.3 SVPG learns strong, diverse policies", "text": "We wanted the quality and variety of policies developed by the SVPG. First, we calculated the best test returns of policies learned by all algorithms and counted how many episodes each policy needed to achieve the 95% of its best returns. Second, we compared the quality of all policies learned by the SVPG and the independent versions. It is worth noting that parameter exploration only occurs through random initialization, while parameter exploration is explicitly done through repulsion. Thus, it is possible that the parameter landscape is better explored and that the average returns of all policies are achieved only through random initialization."}, {"heading": "5.4 Exploration and Exploitation", "text": "The above results show that SVPG is better able to explore in the parameter space than the original gradient methods. In SVPG, the temperature hyperparameter \u03b1 controls the trade-off between exploitation, which is driven by the policy gradient, and exploration, which is driven by the repulsion between parameters of different strategies. Lower temperatures lead the algorithm to focus on exploitation, since the first term with the policy gradient becomes much more important than the second repulsive term. Higher temperatures will drive the strategies to more diverse strategies. In other words, in Figure 4, an ablation of different values of the temperature hyperparameter \u03b1 with the batch size m = 5, 000.If the temperature is very high (e.g. \u03b1 = 100), exploration dominates exploitation. In Figure 4, an ablation of different values of the temperature hyperparameter \u03b1 with the batch size m = 5, 000.If the temperature is very low, the exploration method dominates between the two."}, {"heading": "6 Conclusion", "text": "By modeling a distribution of policy parameters with (relative) entropy regularization, this framework explicitly promotes exploration in the parameter space while optimizing the expected benefits of the strategies generated from this distribution. We demonstrated that this framework can be reduced to a Bayesian inference problem, and then proposed the stone variation gradient to allow for the simultaneous exploitation and exploration of multiple strategies. In our experiments, we evaluated the SVPG versions of REINFORCE and A2C on several continuous control tasks and found that they significantly improved the performance of the original strategy gradient methods. We also conducted a comprehensive analysis of SVPG to demonstrate its robustness and ability to generate different strategies. Due to its simplicity, we expect SVPG to provide a generic approach to improve existing strategies. The parallel nature of SVPG also makes it attractive to implement in distributed systems."}], "references": [{"title": "Unifying countbased exploration and intrinsic motivation", "author": ["M. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "Advances in Neural Information Processing Systems, pages 1471\u2013 1479.", "citeRegEx": "Bellemare et al\\.,? 2016", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "Proceedings of the 33rd International Conference on Machine Learning (ICML).", "citeRegEx": "Duan et al\\.,? 2016", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["S. Gu", "T. Lillicrap", "Z. Ghahramani", "R.E. Turner", "S. Levine"], "venue": "arXiv preprint arXiv:1611.02247.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Reinforcement learning with deep energy-based policies", "author": ["T. Haarnoja", "H. Tang", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1702.08165.", "citeRegEx": "Haarnoja et al\\.,? 2017", "shortCiteRegEx": "Haarnoja et al\\.", "year": 2017}, {"title": "The cma evolution strategy: a comparing review", "author": ["N. Hansen"], "venue": "Towards a new evolutionary computation, pages 75\u2013102. Springer.", "citeRegEx": "Hansen,? 2006", "shortCiteRegEx": "Hansen", "year": 2006}, {"title": "Vime: Variational information maximizing exploration", "author": ["R. Houthooft", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, pages 1109\u20131117.", "citeRegEx": "Houthooft et al\\.,? 2016", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397.", "citeRegEx": "Jaderberg et al\\.,? 2016", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "A natural policy gradient", "author": ["S. Kakade"], "venue": "Advances in neural information processing systems, 2:1531\u20131538.", "citeRegEx": "Kakade,? 2002", "shortCiteRegEx": "Kakade", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "author": ["Q. Liu", "D. Wang"], "venue": "Advances In Neural Information Processing Systems, pages 2370\u20132378.", "citeRegEx": "Liu and Wang,? 2016", "shortCiteRegEx": "Liu and Wang", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Maaten", "L. v. d.", "G. Hinton"], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten et al\\.,? 2008", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "The cross entropy method for fast policy search", "author": ["S. Mannor", "R.Y. Rubinstein", "Y. Gat"], "venue": "ICML, pages 512\u2013519.", "citeRegEx": "Mannor et al\\.,? 2003", "shortCiteRegEx": "Mannor et al\\.", "year": 2003}, {"title": "Learning to navigate in complex environments", "author": ["P. Mirowski", "R. Pascanu", "F. Viola", "H. Soyer", "A. Ballard", "A. Banino", "M. Denil", "R. Goroshin", "L. Sifre", "K Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.03673", "citeRegEx": "Mirowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["S. Mohamed", "D.J. Rezende"], "venue": "Advances in neural information processing systems, pages 2125\u20132133.", "citeRegEx": "Mohamed and Rezende,? 2015", "shortCiteRegEx": "Mohamed and Rezende", "year": 2015}, {"title": "Improving policy gradient by exploring under-appreciated rewards", "author": ["O. Nachum", "M. Norouzi", "D. Schuurmans"], "venue": "arXiv preprint arXiv:1611.09321.", "citeRegEx": "Nachum et al\\.,? 2016", "shortCiteRegEx": "Nachum et al\\.", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S Petersen"], "venue": null, "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "ICML, volume 99, pages 278\u2013287.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Information, utility and bounded rationality", "author": ["D.A. Ortega", "P.A. Braun"], "venue": "International Conference on Artificial General Intelligence, pages 269\u2013274. Springer.", "citeRegEx": "Ortega and Braun,? 2011", "shortCiteRegEx": "Ortega and Braun", "year": 2011}, {"title": "Deep exploration via bootstrapped dqn", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": "Advances In Neural Information Processing Systems, pages 4026\u20134034.", "citeRegEx": "Osband et al\\.,? 2016", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Count-based exploration with neural density models", "author": ["G. Ostrovski", "M.G. Bellemare", "Oord", "A. v. d.", "R. Munos"], "venue": "arXiv preprint arXiv:1703.01310.", "citeRegEx": "Ostrovski et al\\.,? 2017", "shortCiteRegEx": "Ostrovski et al\\.", "year": 2017}, {"title": "Evolution strategies as a scalable alternative to reinforcement learning", "author": ["T. Salimans", "J. Ho", "X. Chen", "I. Sutskever"], "venue": "arXiv preprint arXiv:1703.03864.", "citeRegEx": "Salimans et al\\.,? 2017", "shortCiteRegEx": "Salimans et al\\.", "year": 2017}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML, pages 1889\u20131897.", "citeRegEx": "Schulman et al\\.,? 2015a", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438.", "citeRegEx": "Schulman et al\\.,? 2015b", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Parameterexploring policy gradients", "author": ["F. Sehnke", "C. Osendorfer", "T. R\u00fcckstie\u00df", "A. Graves", "J. Peters", "J. Schmidhuber"], "venue": "Neural Networks, 23(4):551\u2013 559.", "citeRegEx": "Sehnke et al\\.,? 2010", "shortCiteRegEx": "Sehnke et al\\.", "year": 2010}, {"title": "Intrinsically motivated reinforcement learning", "author": ["S.P. Singh", "A.G. Barto", "N. Chentanez"], "venue": "NIPS, volume 17, pages 1281\u20131288.", "citeRegEx": "Singh et al\\.,? 2004", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "An overview of the simultaneous perturbation method for efficient optimization", "author": ["J.C. Spall"], "venue": "Johns Hopkins apl technical digest, 19(4):482\u2013492.", "citeRegEx": "Spall,? 1998", "shortCiteRegEx": "Spall", "year": 1998}, {"title": "Reinforcement driven information acquisition in nondeterministic environments", "author": ["J. Storck", "S. Hochreiter", "J. Schmidhuber"], "venue": "Proceedings of the international conference on artificial neural networks, Paris, volume 2, pages 159\u2013164. Citeseer.", "citeRegEx": "Storck et al\\.,? 1995", "shortCiteRegEx": "Storck et al\\.", "year": 1995}, {"title": "Muticore-tsne", "author": ["D. Ulyanov"], "venue": "https://github. com/DmitryUlyanov/Muticore-TSNE.", "citeRegEx": "Ulyanov,? 2016", "shortCiteRegEx": "Ulyanov", "year": 2016}, {"title": "Accelerating t-sne using treebased algorithms", "author": ["L. Van Der Maaten"], "venue": "Journal of machine learning research, 15(1):3221\u20133245.", "citeRegEx": "Maaten,? 2014", "shortCiteRegEx": "Maaten", "year": 2014}, {"title": "Sample efficient actor-critic with experience", "author": ["Z. Wang", "V. Bapst", "N. Heess", "V. Mnih", "R. Munos", "K. Kavukcuoglu", "N. de Freitas"], "venue": "replay. arXiv preprint arXiv:1611.01224", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["R.J. Williams", "J. Peng"], "venue": "Connection Science, 3(3):241\u2013268.", "citeRegEx": "Williams and Peng,? 1991", "shortCiteRegEx": "Williams and Peng", "year": 1991}, {"title": "Information theorythe bridge connecting bounded rational game theory and statistical physics", "author": ["D.H. Wolpert"], "venue": "Complex Engineered Systems, pages 262\u2013 290. Springer.", "citeRegEx": "Wolpert,? 2006", "shortCiteRegEx": "Wolpert", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "In this work, we introduce the Stein variational policy gradient (SVPG) method, a new policy optimization method that leverages a recent Stein variational gradient descent method (Liu and Wang, 2016) to allow simultaneous exploitation and exploration of multiple policies.", "startOffset": 179, "endOffset": 199}, {"referenceID": 31, "context": "In our experiments, we implementing SVPG on top of existing policy gradient methods, including REINFORCE (Williams, 1992) and advantageous actor critic, improves", "startOffset": 105, "endOffset": 121}, {"referenceID": 26, "context": "Instead of computing the finite difference for each individual parameter, rollout efficient methods such as SPSA (Spall, 1998), PEPG (Sehnke et al.", "startOffset": 113, "endOffset": 126}, {"referenceID": 24, "context": "Instead of computing the finite difference for each individual parameter, rollout efficient methods such as SPSA (Spall, 1998), PEPG (Sehnke et al., 2010) and evolutionary strategy approximations (Hansen, 2006, Mannor et al.", "startOffset": 133, "endOffset": 154}, {"referenceID": 21, "context": "In particular, the following stochastic finite difference approximation has recently been shown to be highly effective over several complex domains (Salimans et al., 2017):", "startOffset": 148, "endOffset": 171}, {"referenceID": 31, "context": "Policy gradient algorithms, such as the well-known REINFORCE (Williams, 1992), estimate the gradient \u2207\u03b8J(\u03b8) from rollout samples generated by \u03c0(a|s; \u03b8) using the likelihood ratio trick.", "startOffset": 61, "endOffset": 77}, {"referenceID": 21, "context": "Learning upper level policies are also discussed in (Salimans et al., 2017), in which the q(\u03b8) distribution is often assumed to be follow certain parametric form, such as a Gaussian distribution, whose parameters are optimized to maximize the expected reward (their method does not use entropy regularization and hence is non-Bayesian).", "startOffset": 52, "endOffset": 75}, {"referenceID": 22, "context": "For example, TRPO (Schulman et al., 2015a) enforces a relative entropy constraint to make sure the current policy stays close to a previous policy.", "startOffset": 18, "endOffset": 42}, {"referenceID": 3, "context": "Recently (Haarnoja et al., 2017) proposed to leverage the maximum entropy principle on policies and derived a deep energy-based policy optimization method.", "startOffset": 9, "endOffset": 32}, {"referenceID": 9, "context": "For this purpose, here we use the Stein variational gradient descent (SVGD) for Bayesian inference (Liu and Wang, 2016).", "startOffset": 99, "endOffset": 119}, {"referenceID": 9, "context": "Liu and Wang (2016) showed that this functional optimization yields a closed form solution,", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "Trust region policy optimization (Schulman et al., 2015a) optimizes its policy by establishing a trust region centered at its current policy.", "startOffset": 33, "endOffset": 57}, {"referenceID": 13, "context": "Asynchronous advantage actor-critic (A3C) (Mnih et al., 2016) runs multiple agents asynchronously in parallel to train an actor-critic architecture.", "startOffset": 42, "endOffset": 61}, {"referenceID": 15, "context": "Recent work (Nachum et al., 2016) has explicitly attempted to explore \u2019under-appreciated\u2019 rewards through importance sampling of trajectories over the current policy.", "startOffset": 12, "endOffset": 33}, {"referenceID": 17, "context": "SVPG is compatible with these reward shaping (Ng et al., 1999) procedures since it does not impose any constraints on the parameters and gradients.", "startOffset": 45, "endOffset": 62}, {"referenceID": 15, "context": "Nair et al. (2015) developed a system called GORILA that deployed multiple actors in parallel to collect experience.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Mnih et al. (2016) extended this idea by sampling different exploration hyperparameters for the greedy policy of different agents.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "of parameters was also explored by Osband et al. (2016) to encourage deep exploration.", "startOffset": 35, "endOffset": 56}, {"referenceID": 23, "context": "In the A2C implementations, we use the generalized advantage estimator (GAE) (Schulman et al., 2015b) for critic, which is implemented in the rllab toolkit (Schulman et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 9, "context": "For SVGD, we follow Liu and Wang (2016) to use the Gaussian RBF kernel k(\u03b8, \u03b8) = exp(\u2212||\u03b8\u2212\u03b8||2/h) with the bandwidth h chosen to bemed/ log(n+1) where med denotes the median of pairwise distances between the particles {\u03b8i}.", "startOffset": 20, "endOffset": 40}, {"referenceID": 1, "context": "All the experiments are constructed with the OpenAI rllab development toolkit (Duan et al., 2016).", "startOffset": 78, "endOffset": 97}, {"referenceID": 1, "context": "Details of these environments can be found in (Duan et al., 2016) and on the GitHub website1.", "startOffset": 46, "endOffset": 65}, {"referenceID": 1, "context": "For A2C, we set \u03bb = 1 for the generalized advantage estimation as suggested in (Duan et al., 2016).", "startOffset": 79, "endOffset": 98}, {"referenceID": 8, "context": "We used ADAM (Kingma and Ba, 2014) to adjust the step-size of gradient descent in all algorithms.", "startOffset": 13, "endOffset": 34}], "year": 2017, "abstractText": "Policy gradient methods have been successfully applied to many complex reinforcement learning problems. However, policy gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. We then propose a novel Stein variational policy gradient method (SVPG) which combines existing policy gradient methods and a repulsive functional to generate a set of diverse but wellbehaved policies. SVPG is robust to initialization and can easily be implemented in a parallel manner. On continuous control problems, we find that implementing SVPG on top of REINFORCE and advantage actor-critic algorithms improves both average return and data efficiency.", "creator": "LaTeX with hyperref package"}}}