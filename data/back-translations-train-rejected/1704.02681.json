{"id": "1704.02681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Pyramid Vector Quantization for Deep Learning", "abstract": "This paper explores the use of Pyramid Vector Quantization (PVQ) to reduce the computational cost for a variety of neural networks (NNs) while, at the same time, compressing the weights that describe them. This is based on the fact that the dot product between an N dimensional vector of real numbers and an N dimensional PVQ vector can be calculated with only additions and subtractions and one multiplication. This is advantageous since tensor products, commonly used in NNs, can be re-conduced to a dot product or a set of dot products. Finally, it is stressed that any NN architecture that is based on an operation that can be re-conduced to a dot product can benefit from the techniques described here.", "histories": [["v1", "Mon, 10 Apr 2017 01:17:43 GMT  (238kb)", "http://arxiv.org/abs/1704.02681v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["vincenzo liguori"], "accepted": false, "id": "1704.02681"}, "pdf": {"name": "1704.02681.pdf", "metadata": {"source": "CRF", "title": "Pyramid Vector Quantization for Deep Learning", "authors": ["Vincenzo Liguori"], "emails": ["enzo@ocean-logic.com"], "sections": [{"heading": null, "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to play, to play, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "II. PYRAMID VECTOR QUANTIZATION", "text": "A pyramid vector quantizer [8] (PVQ) is based on the cubic grid points located on the surface of an N-dimensional pyramid. (Unlike better known forms of vector quantization, which require complex iterative procedures to find the optimal quantified vector, it has a simple coding algorithm. (The pair of integers N and K, together with (1), completely define the surface of an N-dimensional pyramid specified here with P (N, K). (In this work, a certain type of PVQ is used, known as the PVQ product.) Here, a vector y and K, together with (1), are used to define the surface of an N-dimensional pyramid specified here with P (N, K). (N)"}, {"heading": "III. DOT PRODUCT", "text": "We will now look at the point product between a PVQ vector (2) and a PVQ vector (N, K) with radius r and a N-dimensional vector x-dimensional vector x-dimensional vector x-dimensional vector x-x-x = r-y-i-i-i-i (3).The author has shown in [9] that i = 0N-1y-i-xi can be calculated with exactly K-1 additions and / or subtractions and does not contain any multiplications for any y-i-P (N, K).Since we consider all PVQ vectors to be calculated offline, the scaling factor f = r-1 additions and / or subtractions 2 \u2265 0 can also be pre-calculated and considered as a single value. In this case, the point product between a PVQ vector (2) and a vector factor 1 is multiplied by additions and 1 / 1 subtractions."}, {"heading": "IV. NEURAL NETWORKS", "text": "It is enough to say that the number of applied nominations in the US and in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the"}, {"heading": "V. FURTHER OPTIMIZATIONS", "text": "D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). ("}, {"heading": "VI. WEIGHTS COMPRESSION", "text": "This year, it has reached the point where it is only half as much as it is half."}, {"heading": "VII. EXPERIMENTS", "text": "The author conducted a series of experiments that included some simple NN models, such as the PSR-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A"}, {"heading": "VIII. HARDWARE CONSIDERATIONS", "text": "rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "IX. CONCLUSION AND FUTURE WORK", "text": "This paper has shown how to use PVQ to quantify NN weights vectorially and then use the properties of PVQ-encoded vectors to simplify the NN conclusion. Specifically: \u2022 PVQ encoding of weights significantly reduces the number of bits required per weight. \u2022 If the NN is built with activation functions such as ReLU and nonlinearity such as Maxpool, it can only be closed with addition and subtraction. \u2022 If the NN is built with activation functions such as Bsign (x), then it can be closed largely by adding and subtracting binary values. \u2022 This reduction in computing costs and storage is very important for implementations of hardware for low power-consumption embedded systems. Future work will focus on what can be done during the training to improve the results after PVQ encoding as well as post-quantization optimization steps."}], "references": [{"title": "Gradient-Based Learning Applied to Document Recognition\u201dProceedings of the IEEE 86", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "W.J.Dally \u201cLearning both Weightsand Connections for Efficient Neural Networks", "author": ["S. Han", "J. Pool", "J. Tran"], "venue": "arXiv : 1506.02626v3,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A highly Scalable Restricted Boltzmann Machine FPGA Implementation", "author": ["S.K. Kim", "L.C. McAfee", "P.L. McMahon", "K. Olukotun"], "venue": "International Conference on Field Programmable Logic and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "XNOR-Net: ImageNet Classification", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "Using Binary Convolutional Neural Networks\u201d, arXiv:1603.05279v3,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "On the capacity of neural networks with binary weights", "author": ["I. Kocher", "R. Monasson"], "venue": "J. Phys. A: Math. Gen", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "A Pyramid Vector Quantizer", "author": ["T.R. Fischer"], "venue": "IEEE Transactions on Information Theory, Vol. IT-32,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Vector Quantization for Machine Vision", "author": ["V. Liguori"], "venue": "arXiv: 1603.09037,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Neural networks for machine learning", "author": ["Geoffrey Hinton"], "venue": "Coursera, video lectures,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A study of non-blocking switching networks\",Bell System", "author": ["C. Charles"], "venue": "Technical Journal", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1953}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional Neural Networks (CNNs)[1] are the current prevailing implementation of artificial neural networks.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Such methods have relied on pruning or simplifying a network [2] as well as quantization of the weights (in [3] to 16 bits).", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Such methods have relied on pruning or simplifying a network [2] as well as quantization of the weights (in [3] to 16 bits).", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Quantization has been pushed to the extreme case of binary activation functions and binary (+1,-1) weights [4][5][6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "Although the basic idea of binarized weights NNs is not new [7], in their modern incarnations they have reached a level of sophistication that has practical implication for the real world use of NNs.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "A pyramid vector quantizer[8] (PVQ) is based on the cubic lattice points that lie on the surface of an N-dimensional pyramid.", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "The paper[8] also includes simple algorithms to calculate the number of points N p(N , K ) on the surface of the Ndimensional pyramid.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "The author has shown in [9] that \u2211", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "It also appear to be the case for published work like [3] (see fig.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Note the difference from binarized networks such as [4] or [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "If in a particular layer of a binary PVQ net N=K, then the total number of binary additions and subtractions will be the same as in [4] but each individual weight doesn\u2019t need to be +/-1.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "In fact, as already mentioned, [8] describes an algorithm to map a point on a hyper-pyramid P(N,K) to an integer.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Unfortunately, the algorithm given in [8] is not very practical, especially when it comes to the inverse process (i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "However, an important advantage remains in mapping a PVQ vector to an integer, as mentioned in section II: unlike the methods that will be discussed below that result in a string of bits of unpredictable size, the method described in [8] requires log2(N p(N ,K )) bits for any \u0175\u2208P(N , K) .", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "\u2022 As seen also in [4][6], the first layer is the hardest to quantize.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "In order to overcome this problem, Geoff Hinton in [14] suggests to use a \u201cStraight Through Estimator\u201d (STE) which, essentially, consists of imposing a derivative for (17) by definition: d d x bsign(x )=1 (18) Unfortunately Keras does not support user defined activation functions and its derivatives.", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "\u2022 For layers where N/K~=1, the number of addition and subtractions of binary values is the same as in a binarized net as in [4][6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Some parallel architectures, as suggested in section IV in [9] can be more practical for binary PVQ nets.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "For example, the crossbar there described is clearly simpler for binary value and it can be implemented with Clos networks [15] either in full or in a time-multiplexing fashion.", "startOffset": 123, "endOffset": 127}], "year": 2017, "abstractText": "This paper explores the use of Pyramid Vector Quantization (PVQ) to reduce the computational cost for a variety of neural networks (NNs) while, at the same time, compressing the weights that describe them. This is based on the fact that the dot product between an N dimensional vector of real numbers and an N dimensional PVQ vector can be calculated with only additions and subtractions and one multiplication. This is advantageous since tensor products, commonly used in NNs, can be re-conduced to a dot product or a set of dot products. Finally, it is stressed that any NN architecture that is based on an operation that can be re-conduced to a dot product can benefit from the techniques described here.", "creator": "Writer"}}}