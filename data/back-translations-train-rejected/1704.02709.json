{"id": "1704.02709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments", "abstract": "We introduce an approach to implicit semantic role labeling (iSRL) based on a recurrent neural semantic frame model that learns probability distributions over sequences of explicit semantic frame arguments. On the NomBank iSRL test set, the approach results in better state-of-the-art performance with much less reliance on manually constructed language resources.", "histories": [["v1", "Mon, 10 Apr 2017 04:48:53 GMT  (185kb,D)", "http://arxiv.org/abs/1704.02709v1", null], ["v2", "Thu, 5 Oct 2017 10:32:17 GMT  (184kb,D)", "http://arxiv.org/abs/1704.02709v2", "IJCNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["quynh ngoc thi do", "steven bethard", "marie-francine moens"], "accepted": false, "id": "1704.02709"}, "pdf": {"name": "1704.02709.pdf", "metadata": {"source": "CRF", "title": "Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments", "authors": ["Quynh Ngoc Thi Do", "Steven Bethard", "Marie-Francine Moens"], "emails": ["quynhngocthi.do@cs.kuleuven.be", "bethard@cis.uab.edu", "sien.moens@cs.kuleuven.be"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Neural Semantic Frame Model", "text": "We rely on the fact that current explicit SRL sys-ar Xiv: 170 4.02 709v 1 [cs.C L] 10 Apr 201 7tems achieve high performance on verbal predicates and execute an explicit SRL system on unlabeled data. We then construct a predictive recursive semantic frame model (PRNSFM) from these predicted frames and roles.Our PRNSFM considers semantic frames as semantic frames as a sequence: a predicate followed by the arguments in the order in which they appear in the text and are terminated by a special EOS symbol. The task of the model is to take a predicate and zero or more arguments and predict the next argument in the sequence or EOS if no further arguments follow."}, {"heading": "2.1 Model 1: 1-1 LSTM Model", "text": "Model 1 treats the word and the semantic designation as a single unit in both input and output layers. Phelps: A0 would be a single value. Our model consists of three layers (see Figure 1 (a)): The embedding layer is a matrix of size | Vin | \u00b7 dthat maps each unit of the input layer into a d-dimensional vector. The matrix is randomly initialized and updated during network training. the long-term memory layer (LSTM) consists of m LSTM units that take the output of the embedding layer, xt, as input and generate output by pressing 0 \u2264 t \u2264 T: it = sigmoid (Wixt + Uiht \u2212 1 + bi) C \u0442t = tanh (Wcxt + Ucht \u2212 1 + bc) ft = sigmoid (Wfxt + Ufht \u2212 1 + Ufht \u2212 1 + bf), wt: the ability to embed is the ability to embed."}, {"heading": "2.2 Model 2: 2-1 LSTM Model", "text": "Model 2 considers the word and the semantic label as two different units in the input layer. Phelps: A0, for example, is considered as two separate values Phelps and A0. As shown in Figure 1 (b), we use two different embedding levels, one for word values and one for semantic labels, and the two embedding vectors are concatenated before they are passed to the LSTM layer. LSTM and Softmax layers are then the same as in Model 1. For example, the embedding layer for word values can be initialized with pre-trained word embedding (Mikolov et al., 2013; Pennington et al., 2014); the embedding layer for labels is randomly initialized."}, {"heading": "2.3 Selectional Preferences", "text": "To support an iSRL model, we extract selective preferences q from the PRNSFM, which specify how likely a word is to be an argument of a semantic frame. We define the selective preference as P (w: l | p), where the probability of a word p is the l argument of the frame f. Our goal is to predict P (w: l | p) sequentially. For each sequence q in the set of all possible argument sequences predicted by the model before t (St), we select a threshold k and generate argmaxk, s, which have the highest probability of being the next argument. Formally: S0 = [p: PRED]} St + 1 = {[q, wt: p: p]."}, {"heading": "3 Implicit Semantic Role Labeling", "text": "If we only use an explicit SRL system, we extract implicit preferences from our PRNSFM context as follows: For each triple of a nominal predicate np, a word candidate w and a label l, the selective preference value of w is implicitly argumentp: PREDw 1: l 1w: lw: lP 1P 2P wP 11 P 12P w1P 21 P22P w2P w12w 2: l 11 w: l 11 w: l 12: l 21 w 22: l 22w: lw: lw: lw: lP: lP w22 P w22Time 0: S 0 = [p: PRED] Time 1: [p: PRED, w 1: l 1], [p: PRED 2: l 2: l 2] Time 2: S 2 = [p: PRED, w 11: l 11]."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Building PRNSFM", "text": "Semantic Role Labeling We use the full pipeline of MATE1 (Bjo \ufffd rkelund et al., 2010) as an explicit SRL system. The system is retrained on the CoNLL 2009 training part. Uncommented data Uncommented data includes Wikipedia2, Reuters3 and Brown4. Data set for PRNSFM The first 15 million short and medium (less than 100 words) sentences from the uncommented data were automatically commented by the SRL system. The annotated results were then used together with the gold standard CoNLL 2009 SRL training data to train the PRNSFM. To evaluate how well the system acquires knowledge from unmarked data, we train PRNSFM only on the gold standard CoNLL 2009 training data. Neural network training and inference parameters were selected using an evaluation of the CoNLL 2009 development set."}, {"heading": "4.2 Evaluation", "text": "We follow the evaluation framework in Gerber and Chai (2010); Laparra and Rigau (2013); Schenk and Chiarcos (2016) 5: The method is evaluated using cube coefficients based on the evaluation share of the nominal iSFL data."}, {"heading": "4.3 Results", "text": "Table 1 shows the current state of the art and performance of our PRNSFM-based models. We include a base system (Skip-gram) that is trained on the same unlabeled and labeled data as the PRNSFM, but handles predictions and reasons1https: / / code.google.com / archive / p / mate-tools / 2http: / / corpus.byu.edu / wiki / 3http: / / about.reuters.com / researchandstandards / corpus / 4https: / / catalog.ldc.upenn.edu / ldc99t42 5 Following Schenk and Chiarcos (2016), we do not perform the alternative assessment of Gerber and Chai (2012), which evaluates systems on the iSRL Training Kit, as the iSRL Training Kit overlaps the CoNLL 2009 Training Kit, on which MATE explicitly evaluates wheelchair training features of the F1 Precision Model 2."}, {"heading": "5 Conclusion and Future Work", "text": "Building on our evaluations of NomBank's iSRL dataset, our best model improves the state of the art while reducing the language resources needed. In future work, we plan to evaluate how our predictive models can be applied to other tasks. [6] Calculating statistical significance is difficult because item-level predictions from other systems are not publicly available. As an overly conservative estimate, we perform a t-test on the 10 predicate level F1 values (which are available in published papers and are omitted in this draft due to space constraints), resulting in p = 0.28 for Gerber and Chai (2010), p = 0.46 for Laparra and Rigau (2013), and especially p = 0.058 for Schenk and Chiarcos (2016)."}], "references": [{"title": "Efficient estimation of word", "author": ["frey Dean"], "venue": null, "citeRegEx": "Dean.,? \\Q2013\\E", "shortCiteRegEx": "Dean.", "year": 2013}, {"title": "Inducing Implicit Arguments from Comparable Texts: A Framework and its Applications", "author": ["Michael Roth", "Anette Frank."], "venue": "Computational Linguistics 41:625\u2013664.", "citeRegEx": "Roth and Frank.,? 2015", "shortCiteRegEx": "Roth and Frank.", "year": 2015}, {"title": "Semeval-2010 task 10: Linking events and their participants in discourse", "author": ["Josef Ruppenhofer", "Caroline Sporleder", "Roser Morante", "Collin Baker", "Martha Palmer."], "venue": "Proceedings of the 5th International Workshop on Semantic Evalua-", "citeRegEx": "Ruppenhofer et al\\.,? 2010", "shortCiteRegEx": "Ruppenhofer et al\\.", "year": 2010}, {"title": "Unsupervised learning of prototypical fillers for implicit semantic role labeling", "author": ["Niko Schenk", "Christian Chiarcos."], "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Schenk and Chiarcos.,? 2016", "shortCiteRegEx": "Schenk and Chiarcos.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "However, many predicates, especially nominal ones, may bear arguments in discourse (Ruppenhofer et al., 2010; Gerber et al., 2009).", "startOffset": 83, "endOffset": 130}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012).", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories.", "startOffset": 0, "endOffset": 187}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories.", "startOffset": 0, "endOffset": 213}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories. Schenk and Chiarcos (2016) aimed at a simpler strategy that would not rely on manual iSRL annotations, requiring only an existing corpus of explicit SRL annotations: induce prototypical roles from large amounts of explicit SRL annotations and distributed word representations.", "startOffset": 0, "endOffset": 628}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories. Schenk and Chiarcos (2016) aimed at a simpler strategy that would not rely on manual iSRL annotations, requiring only an existing corpus of explicit SRL annotations: induce prototypical roles from large amounts of explicit SRL annotations and distributed word representations. However, the model performance was almost 10 points lower than the state-of-the-art Laparra and Rigau (2013).", "startOffset": 0, "endOffset": 987}, {"referenceID": 3, "context": "We propose an iSRL approach that works in the low-resource setting of Schenk and Chiarcos (2016), but improves state-of-the-art performance by predicting selectional preferences learned through a recurrent neural semantic frame model that learns probability distributions over sequences of explicit semantic frame arguments.", "startOffset": 70, "endOffset": 97}, {"referenceID": 4, "context": "Our models were trained for 120 epochs using the AdaDelta optimization algorithm (Zeiler, 2012).", "startOffset": 81, "endOffset": 95}, {"referenceID": 3, "context": "We follow the evaluation setting in Gerber and Chai (2010); Laparra and Rigau (2013); Schenk and Chiarcos (2016)5: the method is evaluated on the evaluation portion of the nominal iSRL data by Dice coefficient metrics.", "startOffset": 86, "endOffset": 113}, {"referenceID": 3, "context": "edu/ldc99t42 5 Following Schenk and Chiarcos (2016), we do not perform the alternative evaluation of Gerber and Chai (2012) that evaluates systems on the iSRL training set, since the iSRL training set overlaps with the CoNLL 2009 explicit semantic role training set on which MATE is trained.", "startOffset": 25, "endOffset": 52}, {"referenceID": 3, "context": "edu/ldc99t42 5 Following Schenk and Chiarcos (2016), we do not perform the alternative evaluation of Gerber and Chai (2012) that evaluates systems on the iSRL training set, since the iSRL training set overlaps with the CoNLL 2009 explicit semantic role training set on which MATE is trained.", "startOffset": 25, "endOffset": 124}, {"referenceID": 3, "context": "8 Schenk and Chiarcos (2016) 33.", "startOffset": 2, "endOffset": 29}, {"referenceID": 3, "context": "Schenk and Chiarcos (2016), like our approach, uses only an explicit SRL system, but both of our models strongly outperform their result.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "058 for Schenk and Chiarcos (2016).", "startOffset": 8, "endOffset": 35}], "year": 2017, "abstractText": "We introduce an approach to implicit semantic role labeling (iSRL) based on a recurrent neural semantic frame model that learns probability distributions over sequences of explicit semantic frame arguments. On the NomBank iSRL test set, the approach results in better state-of-the-art performance with much less reliance on manually constructed language resources.", "creator": "LaTeX with hyperref package"}}}