{"id": "1704.02712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation", "abstract": "Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.", "histories": [["v1", "Mon, 10 Apr 2017 05:07:38 GMT  (163kb,D)", "http://arxiv.org/abs/1704.02712v1", "CVPR 2017"]], "COMMENTS": "CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zheng xu", "mario a t figueiredo", "xiaoming yuan", "christoph studer", "tom goldstein"], "accepted": false, "id": "1704.02712"}, "pdf": {"name": "1704.02712.pdf", "metadata": {"source": "CRF", "title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation", "authors": ["Zheng Xu", "M\u00e1rio A. T. Figueiredo", "Xiaoming Yuan", "Christoph Studer", "Tom Goldstein"], "emails": ["xuzh@cs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is one of the most important phases of optimisation in the history of the country. (...) It is the question of whether it is one of the greatest challenges in history. (...) It is the question of whether it is one of the greatest challenges in the history of the country. (...) It is the question of whether it is one of the greatest challenges in history. (...) It is the question of whether it is one of the greatest challenges in history. (...) It is the question of whether it is one of the most difficult challenges in history. (...) It is the question of whether it is one of the greatest challenges in history. (...) It is the question of whether it is one of the future. (...) It is the question of whether it is one of the future. (...) It is the question of whether it is the future. (... It is the question of whether it is the future."}, {"heading": "1.1. Overview & contributions", "text": "In this paper, we examine adaptive parameter selections for the relaxed ADMM that collectively and automatically adjust both the penalty parameter \u03c4k and the relaxation parameter \u03b3k. In Section 3, we address theoretical questions about the convergence of ADMM with non-constant penalty and relaxation parameters. In Section 4, we discuss practical methods for selecting these parameters. In Section 6, we apply the proposed ARADMM to various problems in the areas of machine learning, computer vision and image processing. Finally, in Section 7, we compare ARADMM with other ADMM variants and examine the usefulness of the proposed approach to regression, classification and image processing problems in the real world."}, {"heading": "2. Related work", "text": "In computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33] and image processing [42, 21], ADMM is widely used to solve such problems [2, 21, 51, 50] and has recently found applications in neural networks [56, 45], tensor composition [18, 35, 52], structure of motion [19] and other visual problems. The convergence rate of unrelaxed ADMM is established under mild conditions for convex problems [25, 26]. The convergence rate of O (1 / k2) is discussed in [17, 21, 27, 46], where at least one of the functions is either strongly convex or smooth."}, {"heading": "3. Convergence theory", "text": "We study the conditions under which ADMM converges with adaptive penalty and relaxation parameters, using the methods of varying inequality (VI) set out in [24, 25, 26]. Our results measure convergence using the primary and dual \"residuals,\" defined as asrk = b \u2212 Auk \u2212 Bvk and dk = \u03c4kATB (vk \u2212 vk \u2212 1). (6) These residuals have been observed to approach zero when the algorithm approaches a true solution [2]. Therefore, it is important to know that the method converges in the sense that the residuals converge to zero as k \u2192 & & & & ltm & lt.Subsequently, we prove that the relaxed fit in tol > 0 is stop tolerance [2]."}, {"heading": "4. ARADMM: Adaptive relaxed ADMM", "text": "In [51] methods for selecting the spectral step size for vanilla ADMM were discussed. Here, we modify the adaptive ADMM framework in two important ways: First, we discuss the selection of penalty parameters in the presence of the relaxation term. Second, we discuss adaptive methods for the automatic selection of relaxation parameters. The proposed method works by adopting a local linear model for the dual optimization problem and then selecting an optimal step size based on this assumption. A backup method is used to ensure that bad step sizes are not selected if these linearity assumptions do not hold."}, {"heading": "4.1. Dual interpretation of relaxed ADMM", "text": "We derive our adaptive step size rules by examining the close relationship between the relaxed ADMM and the relaxed Douglas-Rachford splitting (DRS) (6, 5, 15).The dual of the general restricted problem (1) is the conjugate f, defined as f (y) = supx < x, y > \u2212 f (x) [41].The relaxed DRS algorithm solves (10) by generating two sequences, (k) k \u00b2 N and (k) k \u00b2 N, defined as f \u00b2 (y) = supx < x, y > \u2212 f (x) [41].The relaxed DRS algorithm solves (10) by generating two sequences, (k) k \u00b2 N and (k) k \u00b2 k \u00b2 n, defined as f \u00b2 k (y) = supx < x, y > \u2212 f (x)."}, {"heading": "4.2. Spectral adaptive stepsize rule", "text": "Adaptive step size rules of the \"spectral\" type were originally proposed by Barzilai and Borwein for simple gradient deviations on smooth problems and dramatically exceed constant step sizes in many applications. Spectral step size methods work by modelling the object's gradient as a linear function and then selecting the optimal step size for this simplified linear model.Spectral methods have recently been used to determine the penalty parameter for the unrelaxed ADMM in [51]. Inspired by this work, we derive spectral step size rules using a linear model / approximate value for h."}, {"heading": "4.3. Estimation of stepsizes", "text": "We now propose a simple method for adapting a linear model to the dual objective terms, so that the formulas in Section 4.2 can be used to obtain increments. Once these linear models are formed, the optimal penalty parameter and relaxation concept can be calculated by (15) and (16), and the optimal step size selection is then calculated by (15) and (16), thanks to the equivalence of relaxed ADMM and DRS. Below, the optimal step size selection is represented by (k) 1 / 2 and (k) 1 / 2."}, {"heading": "4.4. Safeguarding", "text": "Spectral step-size methods for simple degree deviation are paired with a traceability line search to ensure convergence in case the linear model assumptions break down and an unstable step size is generated. ADMM methods have no analogy to traceability. Instead, we use the correlation criterion proposed in [51] to verify the validity of local linear assumptions, and rely only on the adaptive model when the assumptions are deemed valid. (20) If the model assumptions (14) work perfectly, the vectors h, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k"}, {"heading": "4.5. Applying convergence guarantee", "text": "Our convergence theory requires either assumption 1 or assumption 2 to be met, indicating that convergence is guaranteed under \"limited adaptability\" for both penalty and relaxation parameters. These conditions can be guaranteed by explicitly adding constraints to the step size selection in ARADMM. Algorithm 1 Adaptive relaxed ADMM (ARADMM) Input: initialized v0, \u03bb0, \u03c40, and k0 = 01: while convergence does not pass through (7) and k < maxiter do 2: Perform relaxed ADMM, as in (2) - (5) 3: if mod (k, Tf) = 1 then 4: \u03bb \u0441k + 1 = perck + 1 (b \u2212 Auk + 1 \u2212 Bvk) 5: Calculate spectral steps as in (2) with (18) - (3) 3: if mod (k, Tf) then 1 = k + 1."}, {"heading": "4.6. ARADMM algorithm", "text": "The complete adaptive relaxed ADMM (ARADMM) is represented in algorithm 1. We propose to update the step size only every Tf = 2 iterations. We propose a fixed protection threshold cor = 0.2, which is used in all experiments in section 6. The effort for the adaptive scheme is modest and requires only a few internal product calculations."}, {"heading": "5. Proofs of convergence theorems", "text": "We now prove that the relaxed ADMM converges under assumption 1 or 2. Lety = (u v) k (v) k (v + m + p). (24) We use yk = (uk, vk) T and zk = (uk, vk, \u03bbk) T to denote iterates, and y \u00b2 (u \u00b2, v \u00b2) T and z \u00b2 (u \u00b2, v \u00b2) T and z \u2212 k) T to denote optimal solutions. Let us use z + k = (\u0430, vk + k) T to denote iterates, and y \u00b2 (u \u00b2, v \u00b2) T = (u \u00b2, and z \u00b2 k \u2212 k) T to denote optimal solutions."}, {"heading": "5.1. Convergence with adaptivity", "text": "We are now ready to present our most important convergence results. Proof of Theorem 1 is presented here in full form and uses Lemma 2 to generate a contraction argument. Proof of Theorem 2 is extremely similar and is found in the supplementary material.Theorem 1. assumption 1 then applies to the iteration zk = (uk, vk, \u03bbk) T generated by ADMM satisfylim k \u00b2. (33) Assumption 1 implies that the iteration zk = 0. (32) Assumption 1 implies that the iterations k2 \u2212 k \u00b2 (1 + 2k). (1 + 2k)."}, {"heading": "6. Applications", "text": "We focus on the following statistical and image processing problems that cannot be differentiated: Linear regression with the LIBSVM page."}, {"heading": "7. Experiments", "text": "The proposed AADMM is implemented as shown in algorithm 1. We have also implemented vanilla ADMM, (non-adaptive) relaxed ADMM, ADMM with residual balancing (RB) and adaptive ADMM (AADMM) for comparison. Relaxation parameter for non-adaptive relaxed ADMM is set to \u03b3k = 1.5 as proposed in [6].The parameters of RB and AADMM are selected as in [24, 2, 51].The initial penalty \u03c40 = 1 / 10 and initial relaxation \u03b30 = 1 are used for all problems except the canonical QP problem, where the initial parameters are set to the geometric mean of the maximum and minimum inherent value of matrix Q as proposed in [40]. For each problem, the same randomly generated initial variables v0, \u03bb0 are applied to ADMM and its variants."}, {"heading": "7.1. Convergence results", "text": "Table 1 shows the convergence rate of the ADMM and its variants for the applications described in Section 6. Additional experimental results, including the table of more test cases, the convergence curves and visual results of image recovery, and robust PCAs for facial decomposition, can be found in the supplementary material. Relaxed ADMM often performs better than vanilla ADMM, but does not compete with adaptive methods such as RB, AADMM, and ARADMM. The proposed ARADMM performs best in all test cases."}, {"heading": "7.2. Sensitivity to initialization", "text": "We study the sensitivity of the different ADMM variants to the initial penalty (\u03c40) and the initial relaxation parameters (\u03b30). Fig. 1 represents iteration counts for a wide range of values of \u03c40, \u03b30, for the elastic net regression with synthetic datasets. In the left and middle diagrams, we fix one of \u03c40, \u03b30 and vary the other. The number of iterations required for convergence is plotted because the algorithm parameters vary. In the right diagram, we use a grid search to determine the optimal \u03c40 for different10 -4 10 -3 10 -2 10 -1 10 -10 perative methods of convergence, since the initial penalty parameters are Vanilla ADMM Relaxed ADMM Residual Balance Adaptive MMMMM1 1.1 1.2 1.4 1.6 1.7 1.8 1.8 ADADADM ADMADM 2 10010 110 3 Initial Relaxing Parameter 1.MADMADM 1.MADMADM 1.1.5"}, {"heading": "7.3. Sensitivity to safeguarding", "text": "With cor = 0, the calculated adaptive parameters based on curvature estimates are always accepted, and with cor = 1, the parameters are never changed. AADMM is insensitive to cor and is suitable for a wide range of cor [0,1, 0,4] for various applications, except for unpacking SVM and RPCA. Although tuning such \"hyperparameters\" can improve the performance of ARADMM for some applications, the fixed cor = 0,2 performs well in all our experiments (seven applications and over fifty test cases, a full list is in the supplement material). The proposed ARADMM is fully automated and works without parameter tuning."}, {"heading": "8. Conclusion", "text": "We have proposed an adaptive method for co-ordinating the penance and relaxation parameters of a relaxed ADMM without user supervision. We have analysed adaptive relaxed ADMM schemes and provided conditions for which convergence is guaranteed. Experiments with a wide range of benchmarks for machine learning, computer vision and image processing have shown that the proposed adaptive method outperforms (often significantly) other ADMM variants without user supervision or parameter setting. The new adaptive method improves the applicability of a relaxed ADMM by enabling fully automated solvers that exhibit rapid convergence and can be used by non-experienced users."}, {"heading": "Acknowledgments", "text": "TG and ZX were supported by the US Office of Naval Research under grant N00014-17-1-2078 and by the US National Science Foundation (NSF) under grant CCF-1535902. MF was partially supported by Fundac and Cie Ncia e Tecnologia under grant UID / EEA / 5008 / 2013. XY was supported by the General Research Fund of the Hong Kong Research Grants Council under grant HKBU-12313516. CS was partially supported by Xilinx Inc. and by US NSF under grant ECCS-1408006, CCF-1535897 and CAREER CCF1652065."}], "references": [{"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J. Borwein"], "venue": "IMA J. Num. Analysis, 8:141\u2013148,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. and Trends in Mach. Learning, 3:1\u2013122,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Faster convergence rates of relaxed Peaceman-Rachford and ADMM under regularity assumptions", "author": ["D. Davis", "W. Yin"], "venue": "arXiv preprint arXiv:1407.5210,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D. Bertsekas"], "venue": "Mathematical Programming, 55(1- 3):293\u2013318,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics, 32(2):407\u2013499,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 2790\u20132797. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.- J. Lin"], "venue": "Journal of machine learning research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Generalized alternating direction method of multipliers: new theoretical insights and applications", "author": ["E.X. Fang", "B. He", "H. Liu", "X. Yuan"], "venue": "Mathematical Programming Computation, 7(2):149\u2013187,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Barzilai-Borwein method", "author": ["R. Fletcher"], "venue": "Optimization and control with applications, pages 235\u2013256. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, 2(1):17\u201340,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1976}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 23(6):643\u2013660,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Optimal parameter selection for the alternating direction method of multipliers: quadratic problems", "author": ["E. Ghadimi", "A. Teixeira", "I. Shames", "M. Johansson"], "venue": "IEEE Trans. Autom. Control, 60:644\u2013658,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear convergence and metric selection in Douglas-Rachford splitting and ADMM", "author": ["P. Giselsson", "S. Boyd"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Sur l\u2019approximation, par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisationdualit\u00e9 d\u2019une classe de probl\u00e9mes de Dirichlet non lin\u00e9aires", "author": ["R. Glowinski", "A. Marroco"], "venue": "ESAIM: Mod\u00e9lisation Math\u00e9matique et Analyse Num\u00e9rique, 9:41\u201376,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1975}, {"title": "Fast alternating linearization methods for minimizing the sum of two convex functions", "author": ["D. Goldfarb", "S. Ma", "K. Scheinberg"], "venue": "Mathematical Programming, 141(1-2):349\u2013382,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust low-rank tensor recovery: Models and algorithms", "author": ["D. Goldfarb", "Z. Qin"], "venue": "SIAM Journal on Matrix Analysis and Applications, 35(1):225\u2013253,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Shapefit and shapekick for robust, scalable structure from motion", "author": ["T. Goldstein", "P. Hand", "C. Lee", "V. Voroninski", "S. Soatto"], "venue": "European Conference on Computer Vision, pages 289\u2013304. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive primal-dual splitting methods for statistical learning and image processing", "author": ["T. Goldstein", "M. Li", "X. Yuan"], "venue": "Advances in Neural Information Processing Systems, pages 2080\u20132088,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast alternating direction optimization methods", "author": ["T. Goldstein", "B. O\u2019Donoghue", "S. Setzer", "R. Baraniuk"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Unwrapping ADMM: efficient distributed computing via transpose reduction", "author": ["T. Goldstein", "G. Taylor", "K. Barabin", "K. Sayre"], "venue": "AISTATS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale image classification with trace-norm regularization", "author": ["Z. Harchaoui", "M. Douze", "M. Paulin", "M. Dudik", "J. Malick"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3386\u20133393. IEEE,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Alternating direction method with self-adaptive penalty parameters for monotone variational inequalities", "author": ["B. He", "H. Yang", "S. Wang"], "venue": "Jour. Optim. Theory and Appl., 106(2):337\u2013 356,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "On the o(1/n) convergence rate of the Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM Journal on Numerical Analysis, 50(2):700\u2013709,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "On non-ergodic convergence rate of Douglas-Rachford alternating direction method of multipliers", "author": ["B. He", "X. Yuan"], "venue": "Numerische Mathematik, 130:567\u2013577,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerated alternating direction method of multipliers", "author": ["M. Kadkhodaie", "K. Christakopoulou", "M. Sanjabi", "A. Banerjee"], "venue": "ACM SIGKDD, pages 497\u2013506,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient L1 regularized logistic regression", "author": ["S.-I. Lee", "H. Lee", "P. Abbeel", "A. Ng"], "venue": "AAAI, volume 21, page 401,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain generalization and adaptation using low rank exemplar svms", "author": ["W. Li", "Z. Xu", "D. Xu", "D. Dai", "L.V. Gool"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "NIPS, pages 612\u2013620,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "ACM SIGKDD, pages 547\u2013556,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning", "author": ["R. Liu", "Z. Lin", "Z. Su"], "venue": "ACML, pages 116\u2013132,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization", "author": ["C. Lu", "J. Feng", "Y. Chen", "W. Liu", "Z. Lin", "S. Yan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse modeling for image and vision processing", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "Foundations and Trends R  \u00a9 in Computer Graphics and Vision, 8(2-3):85\u2013283,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Informative feature selection for object recognition via sparse PCA", "author": ["N. Naikal", "A.Y. Yang", "S.S. Sastry"], "venue": "2011 International Conference on Computer Vision, pages 818\u2013 825. IEEE,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "A general analysis of the convergence of ADMM", "author": ["R. Nishihara", "L. Lessard", "B. Recht", "A. Packard", "M. Jordan"], "venue": "ICML,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust camera location estimation by convex programming", "author": ["O. Ozyesil", "A. Singer"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2674\u20132683,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Alternating direction method of multipliers for strictly convex quadratic programs: Optimal parameter selection", "author": ["A. Raghunathan", "S. Di Cairano"], "venue": "American Control Conf., pages 4324\u20134329,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Analysis", "author": ["R. Rockafellar"], "venue": "Princeton University Press,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1970}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D: Nonlinear Phenomena, 60(1):259\u2013268,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1992}, {"title": "Fast optimization methods for l1 regularization: A comparative study and two new approaches", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "ECML, pages 286\u2013297. Springer,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast ADMM algorithm for distributed optimization with adaptive penalty", "author": ["C. Song", "S. Yoon", "V. Pavlovic"], "venue": "arXiv preprint arXiv:1506.08928,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Training neural networks without gradients: A scalable ADMM approach", "author": ["G. Taylor", "R. Burmeister", "Z. Xu", "B. Singh", "A. Patel", "T. Goldstein"], "venue": "ICML,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster alternating direction method of multipliers with a worst-case o (1/n) convergence rate. 2016", "author": ["W. Tian", "X. Yuan"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization", "author": ["J. Wright", "A. Ganesh", "S. Rao", "Y. Peng", "Y. Ma"], "venue": "Advances in neural information processing systems, pages 2080\u20132088,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 31:210\u2013227,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse reconstruction by separable approximation", "author": ["S. Wright", "R. Nowak", "M. Figueiredo"], "venue": "IEEE Trans. Signal Processing, 57:2479\u20132493,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "An empirical study of ADMM for nonconvex problems", "author": ["Z. Xu", "S. De", "M.A.T. Figueiredo", "C. Studer", "T. Goldstein"], "venue": "NIPS workshop on nonconvex optimization,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive ADMM with spectral penalty parameter selection", "author": ["Z. Xu", "M.A. Figueiredo", "T. Goldstein"], "venue": "AISTATS,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "Non-negative factorization of the occurrence tensor from financial contracts", "author": ["Z. Xu", "F. Huang", "L. Raschid", "T. Goldstein"], "venue": "NIPS workshop on tensor methods,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting low-rank structure for discriminative sub-categorization", "author": ["Z. Xu", "X. Li", "K. Yang", "T. Goldstein"], "venue": "BMVC, Swansea, UK, September 7-10, 2015,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1794\u20131801. IEEE,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Linearized augmented lagrangian and alternating direction methods for nuclear norm minimization", "author": ["J. Yang", "X. Yuan"], "venue": "Mathematics of Computation, 82(281):301\u2013329,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient training of very deep neural networks for supervised hashing", "author": ["Z. Zhang", "Y. Chen", "V. Saligrama"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1487\u20131495,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301\u2013320,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 47, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 53, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 7, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 35, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 48, "endOffset": 63}, {"referenceID": 46, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 22, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 52, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 30, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 81, "endOffset": 97}, {"referenceID": 3, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 134, "endOffset": 140}, {"referenceID": 2, "context": "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].", "startOffset": 134, "endOffset": 140}, {"referenceID": 15, "context": "ADMM was first introduced in [16] and [12], and has found", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "ADMM was first introduced in [16] and [12], and has found", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "edu applications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].", "startOffset": 157, "endOffset": 164}, {"referenceID": 20, "context": "edu applications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].", "startOffset": 157, "endOffset": 164}, {"referenceID": 5, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 24, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 25, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 9, "context": "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.", "startOffset": 77, "endOffset": 92}, {"referenceID": 39, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 13, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 31, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 179, "endOffset": 187}, {"referenceID": 33, "context": "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the \u201clinearized\u201d ADMM [32, 34].", "startOffset": 179, "endOffset": 187}, {"referenceID": 23, "context": "For nonrelaxed ADMM, the authors of [24] propose methods that modulate the penalty parameter so that the primal and dual residuals (i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "This \u201cresidual balancing\u201d approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 43, "context": "This \u201cresidual balancing\u201d approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44].", "startOffset": 127, "endOffset": 131}, {"referenceID": 50, "context": "In [51], a spectral penalty parameter method is proposed that uses the local curvature of the objective to achieve fast convergence.", "startOffset": 3, "endOffset": 7}, {"referenceID": 47, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 53, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 7, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 46, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 22, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 35, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 52, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 30, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 63, "endOffset": 94}, {"referenceID": 6, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 56, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 42, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 8, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 32, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 113, "endOffset": 131}, {"referenceID": 41, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 154, "endOffset": 162}, {"referenceID": 20, "context": "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].", "startOffset": 154, "endOffset": 162}, {"referenceID": 1, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 20, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 50, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 49, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 57, "endOffset": 72}, {"referenceID": 55, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 129, "endOffset": 137}, {"referenceID": 44, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 129, "endOffset": 137}, {"referenceID": 17, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 160, "endOffset": 172}, {"referenceID": 34, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 160, "endOffset": 172}, {"referenceID": 51, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 160, "endOffset": 172}, {"referenceID": 18, "context": "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.", "startOffset": 196, "endOffset": 200}, {"referenceID": 24, "context": "The O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26].", "startOffset": 105, "endOffset": 113}, {"referenceID": 25, "context": "The O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26].", "startOffset": 105, "endOffset": 113}, {"referenceID": 16, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 20, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 26, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 45, "context": "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.", "startOffset": 44, "endOffset": 60}, {"referenceID": 9, "context": "For the general relaxed ADMM formulation, a O(1/k) convergence rate is provided under mild conditions [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].", "startOffset": 69, "endOffset": 80}, {"referenceID": 37, "context": "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].", "startOffset": 69, "endOffset": 80}, {"referenceID": 14, "context": "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].", "startOffset": 69, "endOffset": 80}, {"referenceID": 39, "context": "For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14].", "startOffset": 86, "endOffset": 94}, {"referenceID": 13, "context": "For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14].", "startOffset": 86, "endOffset": 94}, {"referenceID": 37, "context": "The authors of [38] suggest a grid search and semidefinite programming based method to determine the optimal relaxation and penalty parameters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "Adaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51].", "startOffset": 98, "endOffset": 106}, {"referenceID": 50, "context": "Adaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51].", "startOffset": 98, "endOffset": 106}, {"referenceID": 5, "context": "For the relaxation parameter, it has been suggested in [6] that over-relaxation (\u03b3 \u2208 (1, 2)) may accelerate convergence and \u03b3 = 1.", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].", "startOffset": 77, "endOffset": 89}, {"referenceID": 24, "context": "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].", "startOffset": 77, "endOffset": 89}, {"referenceID": 25, "context": "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].", "startOffset": 77, "endOffset": 89}, {"referenceID": 1, "context": "It has been observed that these residuals approach zero as the algorithm approaches a true solution [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "where tol > 0 is the stopping tolerance [2].", "startOffset": 40, "endOffset": 43}, {"referenceID": 50, "context": "Spectral stepsize selection methods for vanilla ADMM were discussed in [51].", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].", "startOffset": 144, "endOffset": 154}, {"referenceID": 4, "context": "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].", "startOffset": 144, "endOffset": 154}, {"referenceID": 14, "context": "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].", "startOffset": 144, "endOffset": 154}, {"referenceID": 40, "context": "with f\u2217 denoting the Fenchel conjugate of f , defined as f\u2217(y) = supx\u3008x, y\u3009 \u2212 f(x) [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 40, "context": "where \u03b3k is a relaxation parameter, and \u2202f(x) denotes the subdifferential of f evaluated at x [41].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "Adaptive stepsize rules of the \u201cspectral\u201d type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].", "startOffset": 143, "endOffset": 146}, {"referenceID": 10, "context": "Adaptive stepsize rules of the \u201cspectral\u201d type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].", "startOffset": 235, "endOffset": 243}, {"referenceID": 48, "context": "Adaptive stepsize rules of the \u201cspectral\u201d type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].", "startOffset": 235, "endOffset": 243}, {"referenceID": 50, "context": "Spectral methods were recently used to determine the penalty parameter for the non-relaxed ADMM in [51].", "startOffset": 99, "endOffset": 103}, {"referenceID": 50, "context": "Note this is the same \u201coptimal\u201d penalty parameter proposed for non-relaxed ADMM in [51].", "startOffset": 83, "endOffset": 87}, {"referenceID": 50, "context": "The estimation of \u03b1\u0302k and \u03b2\u0302k for the dual components \u0125(\u03bb\u0302k) and \u011d(\u03bbk) at the k-th iteration of primal ADMM has been described in [51].", "startOffset": 130, "endOffset": 134}, {"referenceID": 50, "context": "For a detailed derivation of these formulas, see [51].", "startOffset": 49, "endOffset": 53}, {"referenceID": 50, "context": "Rather, we adopt the correlation criterion proposed in [51] to test the validity of the local linear assumption, and only rely on the adaptive model when the assumptions are deemed valid.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 56, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 29, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 42, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 32, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 20, "context": "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.", "startOffset": 50, "endOffset": 73}, {"referenceID": 56, "context": "Linear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "Linear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving", "startOffset": 166, "endOffset": 174}, {"referenceID": 54, "context": "ADMM has been applied to solve low rank least squares problems [55, 53]", "startOffset": 63, "endOffset": 71}, {"referenceID": 52, "context": "ADMM has been applied to solve low rank least squares problems [55, 53]", "startOffset": 63, "endOffset": 71}, {"referenceID": 52, "context": "LRLS has been used to formulate exemplar classifiers and discover visual subcategories [53].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "where z is the SVM dual variable, Q is the kernel matrix, c is a vector of labels, e is a vector of ones, and C > 0 [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Consensus `1-regularized logistic regression ADMM has become an important tool for solving distributed optimization problems [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 21, "context": "Unwrapped SVM The unwrapped formulation of SVM [22], which can be used in distributed computing environments via \u201ctranspose reduction\u201d tricks, applies ADMM to the primal form of SVM to solve", "startOffset": 47, "endOffset": 51}, {"referenceID": 41, "context": "Total variation image denoising (TVID) Total variation image denoising is often performed by solving [42]", "startOffset": 101, "endOffset": 105}, {"referenceID": 46, "context": "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].", "startOffset": 102, "endOffset": 114}, {"referenceID": 36, "context": "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].", "startOffset": 102, "endOffset": 114}, {"referenceID": 38, "context": "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].", "startOffset": 102, "endOffset": 114}, {"referenceID": 5, "context": "5 as suggested in [6].", "startOffset": 18, "endOffset": 21}, {"referenceID": 23, "context": "The parameters of RB and AADMM are selected as in [24, 2, 51].", "startOffset": 50, "endOffset": 61}, {"referenceID": 1, "context": "The parameters of RB and AADMM are selected as in [24, 2, 51].", "startOffset": 50, "endOffset": 61}, {"referenceID": 50, "context": "The parameters of RB and AADMM are selected as in [24, 2, 51].", "startOffset": 50, "endOffset": 61}, {"referenceID": 39, "context": "The initial penalty \u03c40 =1/10 and initial relaxation \u03b30 =1 are used for all problems except the canonical QP problem, where initial parameters are set to the geometric mean of the maximum and minimum eigenvalues of matrix Q, as proposed for quadratic problems in [40].", "startOffset": 262, "endOffset": 266}, {"referenceID": 23, "context": "As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence.", "startOffset": 16, "endOffset": 24}, {"referenceID": 50, "context": "As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence.", "startOffset": 16, "endOffset": 24}], "year": 2017, "abstractText": "Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a BarzilaiBorwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.", "creator": "LaTeX with hyperref package"}}}