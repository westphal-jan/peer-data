{"id": "1704.02789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Parsimonious Random Vector Functional Link Network for Data Streams", "abstract": "The theory of random vector functional link network (RVFLN) has provided a breakthrough in the design of neural networks (NNs) since it conveys solid theoretical justification of randomized learning. Existing works in RVFLN are hardly scalable for data stream analytics because they are inherent to the issue of complexity as a result of the absence of structural learning scenarios. A novel class of RVLFN, namely parsimonious random vector functional link network (pRVFLN), is proposed in this paper. pRVFLN features an open structure paradigm where its network structure can be built from scratch and can be automatically generated in accordance with degree of nonlinearity and time-varying property of system being modelled. pRVFLN is equipped with complexity reduction scenarios where inconsequential hidden nodes can be pruned and input features can be dynamically selected. pRVFLN puts into perspective an online active learning mechanism which expedites the training process and relieves operator labelling efforts. In addition, pRVFLN introduces a non-parametric type of hidden node, developed using an interval-valued data cloud. The hidden node completely reflects the real data distribution and is not constrained by a specific shape of the cluster. All learning procedures of pRVFLN follow a strictly single-pass learning mode, which is applicable for an online real-time deployment. The efficacy of pRVFLN was rigorously validated through numerous simulations and comparisons with state-of-the art algorithms where it produced the most encouraging numerical results. Furthermore, the robustness of pRVFLN was investigated and a new conclusion is made to the scope of random parameters where it plays vital role to the success of randomized learning.", "histories": [["v1", "Mon, 10 Apr 2017 10:24:34 GMT  (842kb)", "http://arxiv.org/abs/1704.02789v1", "this paper is submitted for publication in Information Sciences"], ["v2", "Sat, 6 May 2017 11:59:53 GMT  (808kb)", "http://arxiv.org/abs/1704.02789v2", "this paper is submitted for publication in Information Sciences"]], "COMMENTS": "this paper is submitted for publication in Information Sciences", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["mahardhika pratama", "plamen p angelov", "edwin lughofer"], "accepted": false, "id": "1704.02789"}, "pdf": {"name": "1704.02789.pdf", "metadata": {"source": "CRF", "title": "Parsimonious Random Vector Functional Link Network for Data Streams", "authors": ["Mahardhika Pratama", "Plamen P. Angelov", "Edwin Lughofer", "Meng Joo Er"], "emails": [], "sections": [{"heading": null, "text": "The design of neural networks (NNs), since it provides a solid theoretical justification for randomized learning, does not reflect fully developed FLN data. Existing work in RVFLN is hardly scalable for data stream analysis because it is inherent to complexity due to the absence of structural learning scenarios. pRVFLN has an open structure paradigm in which its network structure can be rebuilt from scratch and generated automatically in accordance with the degree of nonlinearity and time-varying property of the system to be modeled. pRVFLN is equipped with complexity reduction scenarios in which inconsistent hidden nodes can be printed and input functions dynamically selected."}, {"heading": "Keywords \u2013 Random Vector Functional Link, Evolving Intelligent System, Online Learning, Randomized", "text": "This year, as never before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in"}, {"heading": "3.1 Online Active Learning Strategy", "text": "This year is the highest in the history of the country."}, {"heading": "3.3 Hidden Node Pruning and Recall Strategy", "text": "This method is convenient to use in pRVFLN because it estimates mutual information between a data cloud and a target system by analyzing its correlation. Therefore, the MCI method can be used to measure the correlation. Although this method was established for the type 1 fuzzy system, and is even used for the type 2 fuzzy system, its effectiveness in dealing with data clouds and a recurring structure is an open question. Unlike the RMI method [36] and the T2RMI method [16], which uses the classic symmetrical uncertainty, the MCI method is used using the MCI method."}, {"heading": "End FOR", "text": "We assign 01,0,2.0 to the same setting as [40]. The standard mean square error (MSE) is used as a cost function, which leads to the following gradient concept: RiemporalitemporalityGqGqyT E1 1 1,) 1 () (In addition, the system error was theoretically limited to [35] and the upper limit is also found. It can also be noted that the GOFS incorporates different feature subgroups into each training observation. A relatively unexplored area of existing online feature selection is a situation in which a limited number of features is accessible to the training process. To update this scenario, we assume that for most B input variables during the training process, this strategy cannot be implemented simply by acquiring arbitrary B input characteristics, because this leads to the same subset of remarkable input variables during the training process."}, {"heading": "Else", "text": "1,, titi end IF"}, {"heading": "End FOR", "text": "As with Algorithm 2, the convergence of this scenario is theoretically proven and the upper limit is found in [40]. It must be borne in mind that the processing process in Algorithm 1, 2 is carried out simply by assigning random weights (0 or 1), which fully reflects the importance of input features. 3.5 Random Learning StratypRVFLN adopts the random parameter learning scenario of the RVFLN, whereby only the output nodesW can be analytically matched to an online learning scenario, while others, namely qAt, can be randomly generated without any tuning process. To begin the discussion, let us recall the output expression of pRVFLN as a consequence: Ri tttemporaliio qAXGy 1,), (~ (22) hint that the pRVFLN has a direct connection from an input layer to the output layer wx as with the original RVFLN."}, {"heading": "3.6 Robustness of RVFL Network", "text": "In fact, most of them are able to survive by themselves if they do not see themselves as able to survive by themselves; most of them are able to survive by themselves, and most of them are able to survive by themselves."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "the theory of random vector functional link network (RVFLN) has provided a breakthrough in<lb>the design of neural networks (NNs) since it conveys solid theoretical justification of randomized<lb>learning. Existing works in RVFLN are hardly scalable for data stream analytics because they are inherent<lb>to the issue of complexity as a result of the absence of structural learning scenarios. A novel class of<lb>RVLFN, namely parsimonious random vector functional link network (pRVFLN), is proposed in this<lb>paper. pRVFLN features an open structure paradigm where its network structure can be built from scratch<lb>and can be automatically generated in accordance with degree of nonlinearity and time-varying property<lb>of system being modelled. pRVFLN is equipped with complexity reduction scenarios where<lb>inconsequential hidden nodes can be pruned and input features can be dynamically selected. pRVFLN<lb>puts into perspective an online active learning mechanism which expedites the training process and<lb>relieves operator\u2019s labelling efforts. In addition, pRVFLN introduces a non-parametric type of hidden<lb>node, developed using an interval-valued data cloud. The hidden node completely reflects the real data<lb>distribution and is not constrained by a specific shape of the cluster. All learning procedures of pRVFLN<lb>follow a strictly single-pass learning mode, which is applicable for an online real-time deployment. The<lb>efficacy of pRVFLN was rigorously validated through numerous simulations and comparisons with state-<lb>of-the art algorithms where it produced the most encouraging numerical results. Furthermore, the<lb>robustness of pRVFLN was investigated and a new conclusion is made to the scope of random parameters<lb>where it plays vital role to the success of randomized learning.", "creator": "Microsoft\u00ae Word 2016"}}}