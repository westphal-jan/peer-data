{"id": "1704.02813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Character-Word LSTM Language Models", "abstract": "We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.", "histories": [["v1", "Mon, 10 Apr 2017 11:42:09 GMT  (39kb,D)", "http://arxiv.org/abs/1704.02813v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lyan verwimp", "joris pelemans", "hugo van hamme", "patrick wambacq"], "accepted": false, "id": "1704.02813"}, "pdf": {"name": "1704.02813.pdf", "metadata": {"source": "CRF", "title": "Character-Word LSTM Language Models", "authors": ["Lyan Verwimp", "Joris Pelemans", "Hugo Van hamme", "Patrick Wambacq"], "emails": ["firstname.lastname@esat.kuleuven.be"], "sections": [{"heading": "1 Introduction", "text": "\"We have the task of voice modeling better than the GRUs, therefore we focus on LSTM-based LMs.\" In this work, we address some of the drawbacks of NN based on LMs (many other types of voice processing). \"We have the task of voice modeling better than the GRUs.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" \"We.\" \"\" \".\" \"\" We. \"\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \".\" \"\" \"We.\". \"\" \"\" \"We..\" \"\" \"\" \"We..\" \"\" \"\" \"\" We.. \"\" \"\" \"We..\" \"\" \"\" \"We..\" \"\" \"\" \"\" We.. \"\" \"\" \"\" We.. \"\" \"\" \"\" We.. \"\" \"\" \"\" We... \"\" \"\" \"\" \"We...\" \"\" \"\" We. \"\" \"\" \""}, {"heading": "2 Related work", "text": "In fact, most people who have lived in the United States in recent years are not able to understand the world and what they are doing. (...) It is not as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...). (...). (...). (). (). (). (). (). (). (). ().). (). (). (). (). (). (). (). (). (). (). (). (). ().). (). (). (). (). (). ().). (). (). ().). (). (). ().).).).)...................................................... (). (). (). (). (). (). (). (). (). (). (). ()."}, {"heading": "3 Character-Word LSTM LMs", "text": "A word-level LSTM LM works as follows: a word encoded as an embed character: et = Ww \u00b7 wt (at the time of step t) is fed into the input layer and multiplied by the embed matrix Ww, resulting in a word, the character et: et = Ww \u00b7 wt (1) The word embed of the current word et will be the input for a series of nonlinear operations in the LSTM layer (we refer to (Zaremba et al., 2015) for more details on the equations of the LSTM cell. In the output layer, probabilities for the next word are calculated based on a softmax function. In our letter-word LSTM LM LM, the only difference with the baseline LM is the calculation of the word embedding of the \"word,\" which is now the result of word and character input, instead of word input Wxc."}, {"heading": "3.1 Order of the characters", "text": "The letters can be added in the order in which they appear in the word (in the experiments this is called \"forward order\"), in reverse order (\"reverse order\") or in both (\"both orders\"). In English and Dutch (and many other languages) suffixes can have significant relationships (such as plurality and verbal conjugation) and connections typically have terminals. Therefore, a stronger emphasis on the end of the word might help to better model these properties."}, {"heading": "3.2 Weight sharing", "text": "Note that in Equation 2, each position in the word is associated with different weights: The weights for the first character c1t, W1c, differ from the weights for the character in the second position, W2c. Given that the entered \"vocabulary\" for characters is always the same, one could argue that the same set of weights Wc could be used for all positions in the word: e > t = [(Ww \u00b7 wt) > (Wc \u00b7 c1t) > (Wc \u00b7 c2t) >... (Wc \u00b7 cnt) >] (3) However, one could also argue for the reverse case (no split weights between characters): For example, an \"s\" at the end of a word often has a specific meaning, such as the indication of a third person's singular verb form of the present (in English), which it does not have elsewhere in the word."}, {"heading": "3.3 Number of parameters", "text": "Given that part of the total embedding is used to model the characters, the actual \"word\" embedding is smaller, which significantly reduces the number of parameters. In a normal word-level LSTM, however, the number of parameters in the embedding matrix V \u00b7 E (4) with V being the vocabulary size and E = Ew the total embedding size / word embedding size is. In our CW model, however, the number of parameters V \u00b7 (E \u2212 n \u00b7 Ec) + n \u00b7 (C \u00b7 Ec) (5) is n the number of characters, Ec the size of the embedding size and C the size of the character vocabulary. However, SinceV is by far the dominant factor, since the reduction in the pure word-level embedding significantly reduces the total number of parameters to be traced. If we divide the time weights, this number becomes even smaller: V \u00b7 (E \u2212 E + Ec) (6)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "We test the performance of CW architectures for a small model and a large model, with hyperparameters based on Zaremba et al. (2015) and Kim et al. (2016). However, the small LSTM consists of 2 layers of 200 hidden units and the large LSTM has 2 layers of 650 hidden units. The total size of the embedded layer always corresponds to the size of the hidden layer. During the first 4 / 6 (small / large model) epochs, the learning rate consists of 1, after which we apply exponential decay: the total size of the embedded layer is always equal to the size of the hidden layer. During the first 4 / 6 (small / large model) epochs, the learning rate is 1, after which we apply an exponential expiration date. The smaller, faster number of epochs drops to 13 / 39 (small / large model we fall."}, {"heading": "4.2 Baseline models", "text": "In our experiments, we compared the CW model with two word-level baselines: one with the same number of hidden units in the LSTM layers (i.e., containing more parameters) and one with approximately the same number of parameters as the CW model (such as Kim et al. (2016)), because we are interested in both reducing the number of parameters and improving performance; for the latter baseline, this means that we change the number of hidden units from 200 to 175 for the small model and from 650 to 475 for the large model, with the other hyperparameters remaining the same. The number of parameters for these models is greater than for all CW models, except when only 1 or 2 chars are added. The size difference between the CW models and the smaller word-level models becomes larger when more characters are added, when the size of the character-embeddings is greater and when the character weights.The size of the word-size of the word-size of the matrix = 1275,000 words is equal to 475TB x in the word = 475x."}, {"heading": "4.3 English", "text": "In fact, it is the case that most of them are in a position to go into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4.4 Dutch", "text": "As we explained in the introduction, we expect that the use of information about the internal structure of the word will help more for languages with richer morphology. Although Dutch is still an analytical language (most grammatical relationships are characterized by separate words or word sequences rather than morphemes), it has a richer morphology than English because the composition is a productive and widespread process and because it has more lexical variations due to inflection (e.g. verb conjugation, adjective diffraction). Results for LSTM LMs trained in Dutch seem to confirm this hypothesis (see Figure 5). The CW model exceeds the word layer variation for both the small model and the large model. The best result for the small model is obtained by adding 2 or 3 characters, giving a perplexity of 67.59 or 67.65, corresponding to a relative improvement of 2,200 characters / 2.200% wty% (2.91% or 2.91% / large model), respectively."}, {"heading": "4.5 Random CW models", "text": "To investigate whether the improvements in the CW models are not caused by the fact that the characters add some kind of noise to the input, we are experimenting with the addition of real noise - random \"character information\" - instead of real characters. Both the number of characters (the length of the random \"word\") and the \"characters\" themselves are generated on the basis of a uniform distribution. Table 4 shows the relative change in perplexity averaged over models to which 1 to 10 characters are added with respect to the word level base model LM and the CW model with real characters. In English, the addition of random information had a negative impact on performance in both the basic and the CW models. In Dutch, on the other hand, the addition of random noise to the word level model resulted in small improvements. However, the random models performed much worse than the CW models. We can conclude that the signs provide meaningful information to the LM."}, {"heading": "4.6 Sharing weights", "text": "We repeat certain experiments with the CW models, but with embedding matrices that are divided across all character positions (see Section 3.2). Note that dividing the weights does not mean that the position information is lost, because for example, the first part of the embedding at character level always corresponds to the character at the first position. Dividing the weights ensures that a character is always mapped to the same embedding, regardless of the position of that character in the word, e.g. both occurrences of \"i\" in \"Bliss\" are represented by the same embedding. This effectively reduces the number of parameters. We compare the performance of the CW models with the weight division with the word level LM and the CW model without weight division. Table 5 lists the relative change in relation to these LMs. CW models with weight division generally improve in relation to a word level, with the exception of the small weight division."}, {"heading": "4.7 Dealing with out-of-vocabulary words", "text": "As we mentioned in the introduction, we expect that by providing information about the surface shape of OOV words (namely their characters), the number of errors caused by these words should be reduced. We conduct the following experiment to verify whether this is actually the case: For the CGN test set, we keep the probabilities of each word in mind during the test. If an OOV word occurs, we check the probability of the target word given by a word-level LM and a CW-LM. The word-level model is a large model of size 475, and the CW model is a large model in which 6 characters of size 25 are used in reverse order (the most powerful CW model in our experiments). We find that in 17,483 cases the CW model assigns a higher probability to the target word after an OOV word, while the opposite only happens in 10,724 cases. This is an indication that the use of line information actually helps to model the OV."}, {"heading": "5 Conclusion and future work", "text": "We investigated a character-word-LSTM language model that combines character and word information by concatenating the respective embeddings, which reduces the size of the LSTM and improves perplexity regarding a baseline word-level LM. The model was tested in English and Dutch, as it has multiple embeddings for the characters in which the characters are added and for the weight distribution of the characters. We can conclude that the CW model exceeds the word-level, whereas it has fewer parameters than the word-level model with the same number of LSTM units. If we compare with a word-level LM that has roughly the same number of parameters, the word-level model LM could trump word formation."}, {"heading": "Acknowledgments", "text": "This research is funded by the Flemish government agency IWT (Project 130041, SCATE)."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Vijay Vasudevan", "Fernanda Vigas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "Software available from tensorflow.org.", "citeRegEx": "Vasudevan et al\\.,? 2015", "shortCiteRegEx": "Vasudevan et al\\.", "year": 2015}, {"title": "Morphology and the lexicon: Lexicalization and productivity", "author": ["Marc Aronoff", "Frank Anshen."], "venue": "Andrew Spencer and Arnold M. Zwicky, editors, The Handbook of Morphology, pages 237\u2013247. Blackwell Publishing.", "citeRegEx": "Aronoff and Anshen.,? 2001", "shortCiteRegEx": "Aronoff and Anshen.", "year": 2001}, {"title": "Improved Transition-Based Parsing by Modeling characters instead of words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 349\u2013359.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Alternative structures for character-level RNNs", "author": ["Piotr Bojanowski", "Armand Joulin", "Tom\u00e1s\u0306 Mikolov"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2015}, {"title": "Joint Learning of Character and Word Embeddings", "author": ["Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huanbo Luan."], "venue": "Conference on Artificial Intelligence (AAAI), pages 1236\u20131242.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Transla", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ieee:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based Neural Machine Translation", "author": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 2:357\u2013361.", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Learning Character-level Representations for Part-of-Speech Tagging", "author": ["C\u0131\u0301cero N. dos Santos", "Bianca Zadrozny"], "venue": "Proceedings of The 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "HAC-models: a novel approach to continuous speech recognition", "author": ["Hugo Van hamme."], "venue": "Proceedings Interspeech, pages 2554\u20132557.", "citeRegEx": "hamme.,? 2008", "shortCiteRegEx": "hamme.", "year": 2008}, {"title": "An on-line NMF model for temporal pattern learning: Theory with application to automatic speech recognition", "author": ["Hugo Van hamme."], "venue": "International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), pages 306\u2013313.", "citeRegEx": "hamme.,? 2012", "shortCiteRegEx": "hamme.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 2342\u20132350.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv:1602.02410.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Mandarin word-character hybrid-input Neural Network Language Model", "author": ["Moonyoung Kang", "Tim Ng", "Long Nguyen."], "venue": "Proceedings Interspeech, pages 1261\u20131264.", "citeRegEx": "Kang et al\\.,? 2011", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proc. Conference on Artificial Intelligence (AAAI), pages 2741\u20132749.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Character-based Neural Machine Translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan Black."], "venue": "arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Quantitative Analysis of Culture Using Millions of Digitized Books", "author": ["Erez Lieberman Aiden."], "venue": "Science, 331(6014):176\u2013182.", "citeRegEx": "Aiden.,? 2011", "shortCiteRegEx": "Aiden.", "year": 2011}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1s\u0306 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s\u0306 Burget", "Jan C\u0306ernock\u00fd", "Sanjeev Khudanpur"], "venue": "Proceedings Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Gated Word-Character Recurrent Language Model", "author": ["Yasumasa Miyamoto", "Kyunghyun Cho."], "venue": "arXiv:1606.01700.v1.", "citeRegEx": "Miyamoto and Cho.,? 2016", "shortCiteRegEx": "Miyamoto and Cho.", "year": 2016}, {"title": "The Spoken Dutch Corpus", "author": ["Nelleke Oostdijk."], "venue": "Overview and first Evaluation. Proceedings Language Resources and Evaluation Conference (LREC), pages 887\u2013894.", "citeRegEx": "Oostdijk.,? 2000", "shortCiteRegEx": "Oostdijk.", "year": 2000}, {"title": "Optimizing the Recognition Lexicon for Automatic Speech Recognition", "author": ["Bert R\u00e9veil."], "venue": "PhD thesis, University of Ghent, Belgium.", "citeRegEx": "R\u00e9veil.,? 2012", "shortCiteRegEx": "R\u00e9veil.", "year": 2012}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training Very Deep Networks", "author": ["Rupesh K. Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Neural Information Processing Systems Conference (NIPS), pages 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Proceedings Interspeech, pages 1724\u20131734.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Neural Language Correction with Character-Based Attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivzhagan", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "arXiv:1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Recurrent Neural Network Regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2015", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Character-level Convolutional Networks for Text Classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCunn."], "venue": "Neural Information Processing Systems Conference (NIPS), pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "The current state of the art are recurrent neural network (RNN) based LMs (Mikolov et al., 2010), and more specifically long short-term memory models (LSTM) (Hochreiter and Schmidhuber, 1997) LMs (Sundermeyer et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 11, "context": ", 2010), and more specifically long short-term memory models (LSTM) (Hochreiter and Schmidhuber, 1997) LMs (Sundermeyer et al.", "startOffset": 68, "endOffset": 102}, {"referenceID": 24, "context": ", 2010), and more specifically long short-term memory models (LSTM) (Hochreiter and Schmidhuber, 1997) LMs (Sundermeyer et al., 2012) and their variants (e.", "startOffset": 107, "endOffset": 133}, {"referenceID": 5, "context": "gated recurrent units (GRU) (Cho et al., 2014)).", "startOffset": 28, "endOffset": 46}, {"referenceID": 5, "context": "gated recurrent units (GRU) (Cho et al., 2014)). LSTMs and GRUs are usually very similar in performance, with GRU models often even outperforming LSTM models despite the fact that they have less parameters to train. However, Jozefowicz et al. (2015) recently showed that for the task of language modeling LSTMs work better than GRUs, therefore we focus on LSTM-based LMs.", "startOffset": 29, "endOffset": 250}, {"referenceID": 21, "context": "(R\u00e9veil, 2012)), we expect that it should benefit more from a LM augmented with formal/morphological information.", "startOffset": 0, "endOffset": 14}, {"referenceID": 12, "context": "entirely has been done for neural machine translation (NMT) by Ling et al. (2015) and Costa-juss\u00e0 and Fonollosa (2016), who replace word-level embeddings with character-level embeddings.", "startOffset": 63, "endOffset": 82}, {"referenceID": 5, "context": "(2015) and Costa-juss\u00e0 and Fonollosa (2016), who replace word-level embeddings with character-level embeddings.", "startOffset": 11, "endOffset": 44}, {"referenceID": 5, "context": "Chung et al. (2016) use a subword-level encoder and a character-level decoder for NMT.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM.", "startOffset": 23, "endOffset": 49}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM. Xie et al. (2016) work on natural language correction and also use an encoder-decoder, but operate for both the encoder and the decoder on the character level.", "startOffset": 23, "endOffset": 156}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM. Xie et al. (2016) work on natural language correction and also use an encoder-decoder, but operate for both the encoder and the decoder on the character level. Character-level word representations can also be generated with convolutional neural networks (CNNs), as Zhang et al. (2015) and Kim et al.", "startOffset": 23, "endOffset": 423}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM. Xie et al. (2016) work on natural language correction and also use an encoder-decoder, but operate for both the encoder and the decoder on the character level. Character-level word representations can also be generated with convolutional neural networks (CNNs), as Zhang et al. (2015) and Kim et al. (2016) have proven for text classification and language", "startOffset": 23, "endOffset": 445}, {"referenceID": 23, "context": "(2016) achieve state-of-the-art results in language modeling for several languages by combining a character-level CNN with highway (Srivastava et al., 2015) and LSTM layers.", "startOffset": 131, "endOffset": 156}, {"referenceID": 15, "context": "Kim et al. (2016) achieve state-of-the-art results in language modeling for several languages by combining a character-level CNN with highway (Srivastava et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Finally, Jozefowicz et al. (2016) also describe character embeddings generated by a CNN, but they test on the 1B Word Benchmark, a data set of an entirely different scale than the one we use.", "startOffset": 9, "endOffset": 34}, {"referenceID": 3, "context": "Chen et al. (2015) and Kang et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Chen et al. (2015) and Kang et al. (2011) work on models combining words and Chinese characters to learn embeddings.", "startOffset": 0, "endOffset": 42}, {"referenceID": 3, "context": "Bojanowski et al. (2015) operate on the character level but use knowledge about the context words in two variants of character-level RNN LMs.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Bojanowski et al. (2015) operate on the character level but use knowledge about the context words in two variants of character-level RNN LMs. Dos Santos and Zadrozny (2014) join word and character representations in a deep neural network for part-of-speech tagging.", "startOffset": 0, "endOffset": 173}, {"referenceID": 19, "context": "However, the models investigated in related work are either not tested on a competitive baseline (Miyamoto and Cho, 2016) or do not perform better than our models (Kim et al.", "startOffset": 97, "endOffset": 121}, {"referenceID": 15, "context": "However, the models investigated in related work are either not tested on a competitive baseline (Miyamoto and Cho, 2016) or do not perform better than our models (Kim et al., 2016).", "startOffset": 163, "endOffset": 181}, {"referenceID": 26, "context": "The word embedding of the current word et will be the input for a series of non-linear operations in the LSTM layer (we refer to (Zaremba et al., 2015) for more details about the equations of the LSTM", "startOffset": 129, "endOffset": 151}, {"referenceID": 25, "context": "model, with hyperparameters based on Zaremba et al. (2015) and Kim et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 15, "context": "(2015) and Kim et al. (2016)).", "startOffset": 11, "endOffset": 29}, {"referenceID": 22, "context": "During training, 25% of the neurons are dropped (Srivastava et al., 2014) for the small model", "startOffset": 48, "endOffset": 73}, {"referenceID": 25, "context": "This data set is small but widely used in related work (among others Zaremba et al. (2015) and Kim et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 15, "context": "(2015) and Kim et al. (2016)),", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "We adopt the same pre-processing as used by previous work (Mikolov et al., 2010) to facilitate comparison, which implies that the dataset contains only lowercase characters (the size of the character vocabulary is 48).", "startOffset": 58, "endOffset": 80}, {"referenceID": 20, "context": "The Dutch data set consists of components g, h, n and o of the Corpus of Spoken Dutch (CGN) (Oostdijk, 2000), containing recordings of meetings, debates, courses, lectures and read text.", "startOffset": 92, "endOffset": 108}, {"referenceID": 15, "context": "with two word-level baselines: one with the same number of hidden units in the LSTM layers (thus containing more parameters) and one with approximately the same number of parameters as the CW model (like Kim et al. (2016) do), because we are interested in both reducing the number of", "startOffset": 204, "endOffset": 222}, {"referenceID": 15, "context": "Kim et al. (2016) generate character-level embeddings with a convolutional neural network and also report results", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "Miyamoto and Cho (2016) only report results for a small model that is trained without dropout, resulting in a baseline perplexity of 115.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "Miyamoto and Cho (2016) only report results for a small model that is trained without dropout, resulting in a baseline perplexity of 115.65. If we train our small model without dropout we get a comparable baseline perplexity (116.33) and a character-word perplexity of 110.54 (compare to 109.05 reported by Miyamoto and Cho (2016)).", "startOffset": 0, "endOffset": 331}, {"referenceID": 15, "context": "(Kim et al., 2016) 100.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "3 (Miyamoto and Cho, 2016) 109.", "startOffset": 2, "endOffset": 26}, {"referenceID": 15, "context": "(Kim et al., 2016) 84.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "Comparison with other character-level LMs (Kim et al., 2016) (we only compare to models without highway layers) and", "startOffset": 42, "endOffset": 60}, {"referenceID": 19, "context": "character-word models (Miyamoto and Cho, 2016) (they do not use dropout and only report results for a small model).", "startOffset": 22, "endOffset": 46}, {"referenceID": 15, "context": "Moreover, related models using a CNN-based character embedding (Kim et al., 2016) do not perform better.", "startOffset": 63, "endOffset": 81}, {"referenceID": 1, "context": "(Aronoff and Anshen, 2001)).", "startOffset": 0, "endOffset": 26}], "year": 2017, "abstractText": "We present a Character-Word Long ShortTerm Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.", "creator": "TeX"}}}