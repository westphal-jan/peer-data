{"id": "1704.02901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs", "abstract": "A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.", "histories": [["v1", "Mon, 10 Apr 2017 15:18:54 GMT  (218kb,D)", "http://arxiv.org/abs/1704.02901v1", "Accepted to CVPR 2017; extended version"], ["v2", "Sun, 6 Aug 2017 18:05:11 GMT  (218kb,D)", "http://arxiv.org/abs/1704.02901v2", "Accepted to CVPR 2017; extended version"], ["v3", "Tue, 8 Aug 2017 09:31:17 GMT  (218kb,D)", "http://arxiv.org/abs/1704.02901v3", "Accepted to CVPR 2017; extended version"]], "COMMENTS": "Accepted to CVPR 2017; extended version", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["martin simonovsky", "nikos komodakis"], "accepted": false, "id": "1704.02901"}, "pdf": {"name": "1704.02901.pdf", "metadata": {"source": "CRF", "title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs", "authors": ["Martin Simonovsky", "Nikos Komodakis"], "emails": ["martin.simonovsky@enpc.fr", "nikos.komodakis@enpc.fr"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2. Related Work", "text": "The first question to ask is whether the second person is really the third person, or whether the third person is a woman, a woman or a man; the second question is whether the third person is a woman or a man; the third question is whether the third person is a woman or a man; the third question is whether the third person is a man or a woman; the third question is whether the third person is a man or a woman; the third question is whether the third person is a woman or a man; the third question is whether the third person is a man or a woman; the third question is whether the third person is a man or a woman."}, {"heading": "3. Method", "text": "We propose a method for performing coils over local graph environments using edge labels (Section 3.1) and show them to generalize regular coils (Section 3.2). Then we present deep networks using our convolution operator (Section 3.3) in the case of point clouds (Section 3.4) and general graphs (Section 3.5)."}, {"heading": "3.1. Edge-Conditioned Convolution", "text": "Let us consider a direct or undirected diagram G = (V, E) in which V is a finite series of vertices and edges (V, V, V, V, V, V). Let us assign the vertices and edges of each vertex and L: E 7, Rs the vertices and edges (also called attributes). These functions can assign the vertices and edges (also called signals or characteristics) to each vertex as matrices X l: V 7, Rdl the vertices and edges (also called signals or attributes) to each vertex, and L: E 7, Rs assign the labels (also called attributes) to each edge."}, {"heading": "3.2. Relationship to Existing Formulations", "text": "Our formulation of folding on graph neighborhoods maintains the key properties of standard folding on regular grids that are useful in the context of CNNs: weight distribution and locality. The weights in the ECC are bound by edge denomination, which is in contrast to binding them by hop distance from a vertex [2], according to neighborly linearization heuristically [28], by being the central vertex or not [21], indiscriminately [14], or not at all [6].In fact, our definition is reduced to that of Duvenaud et al. [14] (up to scaling) in the case of informative edge denominations: equation j - N (i), l \u2212 1 (j) = 1 (i) Xl \u2212 1 (j) Xl \u2212 1 (j) (j), if this definition is given by Duvenaud et al. [14] (up to scaling) (up to the equation), we have a uniform (up to the equation of Kantenj = 1 \u2212 i), if we have an equality of edges (j) \u2212 i (1) \u2212 N \u2212 l."}, {"heading": "3.3. Deep Networks with ECC", "text": "In fact, we will be able to correct the errors that have been mentioned, in the way that we have done them and in the way that we have done them, in the way that we have done them."}, {"heading": "3.4. Application in Point Clouds", "text": "Point clouds are an important 3D data modality resulting from many acquisition techniques, such as laser scanning (LiDAR) or the reconstruction of multiple views. Due to their natural irregularity and rarity, the only way to process point clouds using deep learning is first of all to voxelise them before feeding them into a 3D CNN (whether for classification [26] or segmentation [19] purposes. Such dense representation is very hardware-friendly and easy with the current Deep Learning Frameworks.On the other hand, there are also several disadvantages. Firstly, voxel representation tends to be much more expensive in terms of memory than usually sparse point clouds (we are not aware of any GPU implementation of revolutions on sparse tensors).Secondly, the need to fit them into a fixed 3D grid leads to discrediting of these facts and possibly demonstrating the severity of these facts and the loss of mainstreaming with an alternative."}, {"heading": "3.5. Application in General Graphs", "text": "Many problems can be modeled directly as graphs. In such cases, the graph dataset is already given and only the appropriate graph coarsening scheme needs to be selected. This is by no means trivial, and there is a large amount of literature on this problem. [32] Without any concept of spatial localization of indentations, we resort to established graph coarsening algorithms and use the multi-resolution framework by Shuman et al. [36, 29], which works by repeated downsampling and graphical reduction of the input graph. [37] The downsampling step is based on dividing the graph into two components by the sign of the greatest property vector of the laplaker. This is followed by the cron reduction [13], which also defines the new edge labeling, which is ampled by spectral reduction of the edges [37]. Note that the graph algorithm for the purpose of the pooling method can no longer make us attractive as a smaller one for each of the two reasons."}, {"heading": "4. Experiments", "text": "The proposed method is evaluated in point cloud classification (real data in Section 4.1 and synthetic data in Section 4.2) and based on a standard graph classification benchmark (Section 4.3). In addition, we validate our method and examine its properties at MNIST (Section 4.4)."}, {"heading": "4.1. Sydney Urban Objects", "text": "This point cloud dataset [9] consists of 588 objects in 14 categories (vehicles, pedestrians, signs, and trees) in which 32 ECDAR scans are manually extracted, see Figure 4. It shows that non-ideal scanning conditions with occlusions (holes) and a large variability in view (single point of view) are actually not balanced, making object classification a challenging task. Following the protocol used by the dataset authors, we report on the mean F1 score weighted by class frequency as the datasets are unbalanced. This score is further aggregated via four standard trainings / test splits.Network configuration, our ECC network has 7 parametric layers and 4 levels of graph resolution. Its configuration can be described as C (16) -C (32) -MP (0,25,0,5) C (32) -C (32) -FC) -MP (1,5) -GC (75,64) -APC (75,64) -C."}, {"heading": "4.2. ModelNet", "text": "ModelNet [40] is a large-scale collection of object networks. We evaluate classification performance based on its subsets ModelNet10 (3991 / 908 tensile / test examples in 10 categories) and ModelNet40 (9843 / 2468 tensile / test examples in 40 categories). Synthetic point clouds are generated by evenly scanning 1000 points on network surfaces depending on the face area (a multi-angle acquisition simulation) and converted into a unit table. Network configuration. Our ECC network for ModelNet10 has 7 parametric layers and 3 levels of graph resolution with configuration C (16) -C (32) -MP (2.5 / 32,7,5 / 32) -MP (7,5 / 32,22.5 / 32) -C (64) -GMP-FC (64) -FC (64) -D (0,2) -FC (10)."}, {"heading": "4.3. Graph Classification", "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "4.4. MNIST", "text": "To further validate our method, we applied it to the MNIST classification problem (23], a dataset of 70k grayscale images of handwritten digits displayed on a 2D grid of size 28 x 28. We consider each image to be a point cloud2Also possible for unlabeled ENZYMES and D & D, since our method uses Kron reduction labels for all coarse-meshed graphics presented by default.P with points pi = (x, y, 0) and signal X0 (i) = I (x, y) representing each pixel, x, y [0,.., 27}. Edge labeling and graph coarsening are performed as explained in Section 3.4. We are mainly interested in two questions: Is ECC able to achieve the standard performance on this classic baseline? What kind of representation do we learn? Our ECC network has 5 parametric levels with configuration C (C) -32 (C) -MP (3) -32 (-32) -4 (MP)."}, {"heading": "5. Conclusion", "text": "We have introduced edge-conditioned convolution (ECC), an operation on graph signals that is performed in the spatial range, where filter weights are conditioned on edge markers and dynamically generated for each specific input sample. We have shown that our formulation generalizes standard folding on graphs when edge markers are correctly selected, and experimentally validated this claim on MNIST. We have applied our approach to point cloud classification in a novel way, setting a new state of the art for Sydney datasets. In addition, we have surpassed other learning-based approaches on graph classification datasets NCI1. In the feature work, we want to treat meshes as graphs and not as point clouds. In addition, we plan to address the currently higher GPU memory consumption in the case of large graphs with continuous edge markings, for example by randomized clustering, which could also serve as additional regulation by the GPU Corporation."}, {"heading": "A. Overview", "text": "In the first part, the results of the graph classification are discussed further in the appendix (Section B) and the robustness of the point cloud classification with respect to noise (Section C). In the second part, several extensions of our ECC formulation are examined, in particular with different edge designations for point clouds (Section D), with identity connections (Section E), with degree designations (Section F) and with a learned normalization factor (Section G)."}, {"heading": "B. Details on Graph Classification Benchmark", "text": "In this section, we describe the differences in our network architecture with respect to NCI1 in the main work and discuss the evaluation results for each dataset in detail.NCI1. ECC (83.80%) performs significantly better than convolution methods that are unable to use margins (DCNN [2] 62.61%, PSCN [28] 78.59%). Methods that do not approach the problem like convolutions on graphs, but rather combine deep learning methods, are stronger (Deep WL [41] 80.31%, structure2vec [8] 83.72%, but are still surpassed by ECC. While the WeisfeilerLehman-Graph Graph Graph Graph remains the strongest method (WL), it is fair to conclude that ECC, structure2vec [8] 83.72%, is still surpassed by ECC."}, {"heading": "C. Robustness to Noise", "text": "Real point clouds contain various types of artifacts, such as holes due to occlusion and Gaussian noise due to measurement uncertainty. Figure 6 shows that ECC is extremely robust in point removal and can be made robust against additive Gaussian noise by properly enlarging the training data."}, {"heading": "D. Edge Labels for Point Clouds", "text": "In section 3.4, we define edge markers L (j) like the offset meshes L (j) like the offset meshes L (p). However, we are not able to explore our identities. We examine the meaning of individual elements in the proposed edge marker and evaluate other labels that are invariably used for rotating objects in the vertical axis z (IRz). Table 6 shows that models with isotropic identity (60.7) or without labels (38.9) constitute the meaning of individual elements in the proposed edge marker and other labels, while one of the two coordinate systems is important or even slightly better than our proposed one. However, we believe that this is a property of the specific dataset and cannot necessarily be generalized, with one example being MNIST, where IRz corresponds to full isotropy and the accuracy is reduced to 89.9%."}], "references": [{"title": "Orientation-boosted voxel nets for 3d object recognition", "author": ["N.S. Alvar", "M. Zolfaghari", "T. Brox"], "venue": "CoRR, abs/1604.03351", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Diffusion-convolutional neural networks", "author": ["J. Atwood", "D. Towsley"], "venue": "NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Layer normalization", "author": ["L.J. Ba", "R. Kiros", "G.E. Hinton"], "venue": "CoRR, abs/1607.06450", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H. Kriegel"], "venue": "Proceedings of the 5th IEEE International Conference on Data Mining ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Dynamic filter networks", "author": ["B.D. Brabandere", "X. Jia", "T. Tuytelaars", "L.V. Gool"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "CoRR, abs/1312.6203", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Performance of global descriptors for velodyne-based urban object recognition", "author": ["T. Chen", "B. Dai", "D. Liu", "J. Song"], "venue": "2014 IEEE Intelligent Vehicles Symposium Proceedings, Dearborn, MI, USA, June 8-11, 2014, pages 667\u2013673", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative embeddings of latent variable models for structured data", "author": ["H. Dai", "B. Dai", "L. Song"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised feature learning for classification of outdoor 3d scans", "author": ["M. De Deuge", "A. Quadros", "C. Hung", "B. Douillard"], "venue": "Australasian Conference on Robitics and Automation, volume 2", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "R", "author": ["A.K. Debnath"], "venue": "L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):786\u2013797", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1991}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"], "venue": "NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Distinguishing enzyme structures from non-enzymes without alignments", "author": ["P.D. Dobson", "A.J. Doig"], "venue": "Journal of molecular biology, 330(4):771\u2013783", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Kron reduction of graphs with applications to electrical networks", "author": ["F. D\u00f6rfler", "F. Bullo"], "venue": "IEEE Trans. on Circuits and Systems, 60-I(1):150\u2013163", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["D.K. Duvenaud", "D. Maclaurin", "J. Aguilera-Iparraguirre", "R. Bombarell", "T. Hirzel", "A. Aspuru-Guzik", "R.P. Adams"], "venue": "NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph based convolutional neural network", "author": ["M. Edwards", "X. Xie"], "venue": "BMVC", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Fractional max-pooling", "author": ["B. Graham"], "venue": "CoRR, abs/1412.6071", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks at constrained time cost", "author": ["K. He", "J. Sun"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Point cloud labeling using 3d convolutional neural network", "author": ["J. Huang", "S. You"], "venue": "ICPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "CoRR, abs/1609.02907", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Gated graph sequence neural networks", "author": ["Y. Li", "D. Tarlow", "M. Brockschmidt", "R.S. Zemel"], "venue": "ICLR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Geodesic convolutional neural networks on riemannian manifolds", "author": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"], "venue": "pages 37\u201345", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Voxnet: A 3d convolutional neural network for real-time object recognition", "author": ["D. Maturana", "S. Scherer"], "venue": "IROS", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "All you need is a good init", "author": ["D. Mishkin", "J. Matas"], "venue": "ICLR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "ICML", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "GSPBOX: A toolbox for signal processing on graphs", "author": ["N. Perraudin", "J. Paratte", "D.I. Shuman", "V. Kalofolias", "P. Vandergheynst", "D.K. Hammond"], "venue": "CoRR, abs/1408.5781", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Volumetric and multi-view cnns for object classification on 3d data", "author": ["C.R. Qi", "H. Su", "M. Nie\u00dfner", "A. Dai", "M. Yan", "L.J. Guibas"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "3d is here: Point cloud library (pcl)", "author": ["R.B. Rusu", "S. Cousins"], "venue": "Robotics and Automation (ICRA), 2011 IEEE International Conference on, pages 1\u20134. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Advanced coarsening schemes for graph partitioning", "author": ["I. Safro", "P. Sanders", "C. Schulz"], "venue": "ACM Journal of Experimental Algorithmics, 19(1)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "ICLR", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "The graph neural network model", "author": ["F. Scarselli", "M. Gori", "A.C. Tsoi", "M. Hagenbuchner", "G. Monfardini"], "venue": "IEEE Trans. Neural Networks, 20(1):61\u201380", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "E", "author": ["N. Shervashidze", "P. Schweitzer"], "venue": "J. van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12:2539\u20132561", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "A multiscale pyramid transform for graph signals", "author": ["D.I. Shuman", "M.J. Faraji", "P. Vandergheynst"], "venue": "IEEE Trans. Signal Processing, 64(8):2119\u20132134", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph sparsification by effective resistances", "author": ["D.A. Spielman", "N. Srivastava"], "venue": "SIAM Journal on Computing, 40(6):1913\u20131926", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-view convolutional neural networks for 3d shape recognition", "author": ["H. Su", "S. Maji", "E. Kalogerakis", "E.G. Learned-Miller"], "venue": "ICCV", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparison of descriptor spaces for chemical compound retrieval and classification", "author": ["N. Wale", "I.A. Watson", "G. Karypis"], "venue": "Knowledge and Information Systems, 14(3):347\u2013375", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "and J", "author": ["Z. Wu", "S. Song", "A. Khosla", "X. Tang"], "venue": "Xiao. 3d shapenets for 2.5d object recognition and next-best-view prediction. In CVPR", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep graph kernels", "author": ["P. Yanardag", "S.V.N. Vishwanathan"], "venue": "SIGKDD", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "Convolutional Neural Networks (CNNs) have gained massive popularity in tasks where the underlying data representation has a grid structure, such as in speech processing and natural language understanding (1D, temporal convolutions), in image classification and segmentation (2D, spatial convolutions), or in video parsing (3D, volumetric convolutions) [22].", "startOffset": 352, "endOffset": 356}, {"referenceID": 25, "context": "Point clouds have been mostly ignored by deep learning so far, their voxelization being the only trend to the best of our knowledge [26, 19].", "startOffset": 132, "endOffset": 140}, {"referenceID": 18, "context": "Point clouds have been mostly ignored by deep learning so far, their voxelization being the only trend to the best of our knowledge [26, 19].", "startOffset": 132, "endOffset": 140}, {"referenceID": 8, "context": "To offer a competitive alternative with a different set of advantages and disadvantages, we construct graphs in Euclidean space from point clouds in this work and demonstrate state of the art performance on Sydney dataset of LiDAR scans [9].", "startOffset": 237, "endOffset": 240}, {"referenceID": 38, "context": "\u2022 We reach a competitive level of performance on graph classification benchmark NCI1 [39], outperforming other approaches based on deep learning there.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "[6], who looked into both the spatial and the spectral domain of representation for performing localized filtering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "The spatial locality of filters is then given by smoothness of the spectral filters, in case of [6] modeled as B-splines.", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The main challenge here is dealing with weight sharing among local neighborhoods [6], as the number of vertices adjacent to a particular vertex varies and their ordering is often not well definable.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "[6] assumed fixed graph structure and did not share any weights among neighborhoods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] sum the signal over neighboring vertices followed by a weight matrix multiplication, effectively sharing the same weights among all edges.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Atwood and Towsley [2] share weights based on the number of hops between two vertices.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "Kipf and Welling [21] further approximate the spectral method of [11] and weaken the dependency on the Laplacian, but ultimately arrive at center-surround weighting of neighborhoods.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Kipf and Welling [21] further approximate the spectral method of [11] and weaken the dependency on the Laplacian, but ultimately arrive at center-surround weighting of neighborhoods.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "[28] introduces a heuristic for linearizing selected graph neighborhoods so that a conventional 1D CNN can be used.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Finally, Graph neural networks [34, 24] propagate features across a graph until (near) convergence and exploit edge labels as one of the sources of information as we do.", "startOffset": 31, "endOffset": 39}, {"referenceID": 23, "context": "Finally, Graph neural networks [34, 24] propagate features across a graph until (near) convergence and exploit edge labels as one of the sources of information as we do.", "startOffset": 31, "endOffset": 39}, {"referenceID": 24, "context": "[25] define convolution over patch descriptors around every vertex of a 3D mesh using geodesic distances, formulated in a deep learning architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "voxelize them before feeding them to a 3D CNN, be it for classification [26] or segmentation [19] purposes.", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "voxelize them before feeding them to a 3D CNN, be it for classification [26] or segmentation [19] purposes.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "To this end, we borrow the idea from Dynamic filter networks [5] and define a filter-generating network F l : R 7\u2192 Rdl\u00d7dl\u22121 which given edge label L(j, i) outputs edge-specific weight matrix \u0398ji \u2208 Rdl\u00d7dl\u22121 , see Figure 1.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "The weights in ECC are tied by edge label, which is in contrast to tying them by hop distance from a vertex [2], according to a neighborhood linearization heuristic [28], by being the central vertex or not [21], indiscriminately [14], or not at all [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 27, "context": "The weights in ECC are tied by edge label, which is in contrast to tying them by hop distance from a vertex [2], according to a neighborhood linearization heuristic [28], by being the central vertex or not [21], indiscriminately [14], or not at all [6].", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "The weights in ECC are tied by edge label, which is in contrast to tying them by hop distance from a vertex [2], according to a neighborhood linearization heuristic [28], by being the central vertex or not [21], indiscriminately [14], or not at all [6].", "startOffset": 206, "endOffset": 210}, {"referenceID": 13, "context": "The weights in ECC are tied by edge label, which is in contrast to tying them by hop distance from a vertex [2], according to a neighborhood linearization heuristic [28], by being the central vertex or not [21], indiscriminately [14], or not at all [6].", "startOffset": 229, "endOffset": 233}, {"referenceID": 5, "context": "The weights in ECC are tied by edge label, which is in contrast to tying them by hop distance from a vertex [2], according to a neighborhood linearization heuristic [28], by being the central vertex or not [21], indiscriminately [14], or not at all [6].", "startOffset": 249, "endOffset": 252}, {"referenceID": 13, "context": "[14] (up to scaling) in the case of uninformative edge labels: \u2211 j\u2208N(i) \u0398 l jiX l\u22121(j) = \u0398 \u2211 j\u2208N(i)X l\u22121(j) if \u0398ji = \u0398 l \u2200(j, i) \u2208 E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Note that such equivalence is not possible with none of [2, 21, 14] due to their way of weight tying.", "startOffset": 56, "endOffset": 67}, {"referenceID": 20, "context": "Note that such equivalence is not possible with none of [2, 21, 14] due to their way of weight tying.", "startOffset": 56, "endOffset": 67}, {"referenceID": 13, "context": "Note that such equivalence is not possible with none of [2, 21, 14] due to their way of weight tying.", "startOffset": 56, "endOffset": 67}, {"referenceID": 16, "context": "Therefore, the restriction of ECC to 1-hop neighborhoods N(i) is not a constraint, akin to using small 3\u00d73 filters in normal CNNs in exchange for deeper networks, which is known to be beneficial [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 19, "context": "We use batch normalization [20] after each convolution, which was necessary for the learning to converge.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Interestingly, we had no success with other feature normalization techniques such as data-dependent initialization [27] or layer normalization [3].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "Interestingly, we had no success with other feature normalization techniques such as data-dependent initialization [27] or layer normalization [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 25, "context": "Due to their natural irregularity and sparsity, so far the only way of processing point clouds using deep learning has been to first voxelize them before feeding them to a 3D CNN, be it for classification [26] or segmentation [19] purposes.", "startOffset": 205, "endOffset": 209}, {"referenceID": 18, "context": "Due to their natural irregularity and sparsity, so far the only way of processing point clouds using deep learning has been to first voxelize them before feeding them to a 3D CNN, be it for classification [26] or segmentation [19] purposes.", "startOffset": 226, "endOffset": 230}, {"referenceID": 30, "context": "For a single input point cloud P , a pyramid of downsampled point clouds P (h) is obtained by the VoxelGrid algorithm [31], which overlays a grid of resolution r over the point cloud and replaces all points within a voxel with their centroid (and thus maintains subvoxel accuracy).", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "This is by no means trivial and there exists a large body of literature on this problem [32].", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": "[36, 29], which works by repeated downsampling and graph reduction of the input graph.", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[36, 29], which works by repeated downsampling and graph reduction of the input graph.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "This is followed by Kron reduction [13], which also defines the new edge labeling, enhanced with spectral sparsification of edges [37].", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "This is followed by Kron reduction [13], which also defines the new edge labeling, enhanced with spectral sparsification of edges [37].", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "This is in spirit similar to the effect of fractional max-pooling [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "This point cloud dataset [9] consists of 588 objects in 14 categories (vehicles, pedestrians, signs, and trees) manually extracted from 360\u25e6 LiDAR scans, see Figure 4.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "Triangle+SVM [9] 67.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "1 GFH+SVM [7] 71.", "startOffset": 10, "endOffset": 13}, {"referenceID": 25, "context": "0 VoxNet [26] 73.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "0 ORION [1] 77.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Mean F1 score weighted by class frequency on Sydney Urban Objects dataset [9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 32, "context": "The filter-generating networks F l have configuration FC(16)-FC(32)-FC(dldl\u22121) with orthogonal weight initialization [33] and ReLUs in between.", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "4) against two methods based on volumetric CNNs evaluated on voxelized occupancy grids of size 32x32x32 (VoxNet [26] 73.", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "0 and ORION [1] 77.", "startOffset": 12, "endOffset": 15}, {"referenceID": 39, "context": "ModelNet [40] is a large scale collection of object meshes.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "Illustrative samples of the majority of classes in Sydney Urban Objects dataset, reproduced from [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 39, "context": "Table 2 compares our result to several recent works, based either on volumetric [40, 26, 1, 30] or rendered image representation [38].", "startOffset": 80, "endOffset": 95}, {"referenceID": 25, "context": "Table 2 compares our result to several recent works, based either on volumetric [40, 26, 1, 30] or rendered image representation [38].", "startOffset": 80, "endOffset": 95}, {"referenceID": 0, "context": "Table 2 compares our result to several recent works, based either on volumetric [40, 26, 1, 30] or rendered image representation [38].", "startOffset": 80, "endOffset": 95}, {"referenceID": 29, "context": "Table 2 compares our result to several recent works, based either on volumetric [40, 26, 1, 30] or rendered image representation [38].", "startOffset": 80, "endOffset": 95}, {"referenceID": 37, "context": "Table 2 compares our result to several recent works, based either on volumetric [40, 26, 1, 30] or rendered image representation [38].", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "Following [35], we perform 10-fold cross-validation", "startOffset": 10, "endOffset": 14}, {"referenceID": 39, "context": "3DShapeNets [40] 83.", "startOffset": 12, "endOffset": 16}, {"referenceID": 37, "context": "3 MVCNN [38] \u2014 90.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "1 VoxNet [26] 92 83 ORION [1] 93.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "1 VoxNet [26] 92 83 ORION [1] 93.", "startOffset": 26, "endOffset": 29}, {"referenceID": 29, "context": "8 \u2014 SubvolumeSup [30] \u2014 86.", "startOffset": 17, "endOffset": 21}, {"referenceID": 39, "context": "mean instance accuracy) on ModelNets [40].", "startOffset": 37, "endOffset": 41}, {"referenceID": 38, "context": "NCI1 and NCI109 [39] consist of graph representations of chemical compounds screened for activity against nonsmall cell lung cancer and ovarian cancer cell lines, respectively.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "MUTAG [10] is a dataset of nitro compounds labeled according to whether or not they have a mutagenic effect on a bacterium.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "ENZYMES [4] contains representations of tertiary structure of 6 classes of enzymes.", "startOffset": 8, "endOffset": 11}, {"referenceID": 11, "context": "D&D [12] is a database of protein structures (vertices are amino acids, edges indicate spatial closeness) classified as enzymes and non-enzymes.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "The filter-generating networks F l have configuration FC(64)-FC(dldl\u22121) with orthogonal weight initialization [33] and ReLU in between.", "startOffset": 110, "endOffset": 114}, {"referenceID": 34, "context": "[35] and to four approaches using deep learning as at least one of their components [2, 28, 41, 8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[35] and to four approaches using deep learning as at least one of their components [2, 28, 41, 8].", "startOffset": 84, "endOffset": 98}, {"referenceID": 27, "context": "[35] and to four approaches using deep learning as at least one of their components [2, 28, 41, 8].", "startOffset": 84, "endOffset": 98}, {"referenceID": 40, "context": "[35] and to four approaches using deep learning as at least one of their components [2, 28, 41, 8].", "startOffset": 84, "endOffset": 98}, {"referenceID": 7, "context": "[35] and to four approaches using deep learning as at least one of their components [2, 28, 41, 8].", "startOffset": 84, "endOffset": 98}, {"referenceID": 7, "context": "Characteristics of the graph benchmark datasets, extended from [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "DCNN [2] 62.", "startOffset": 5, "endOffset": 8}, {"referenceID": 27, "context": "10 \u2014 PSCN [28] 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 40, "context": "12 Deep WL [41] 80.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "43 \u2014 structure2vec [8] 83.", "startOffset": 19, "endOffset": 22}, {"referenceID": 34, "context": "22 WL [35] 84.", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "The results demonstrate the importance of exploiting edge labels for convolution-based methods, as the performance of DCNN [2] and ECC without edge labels is distinctly worse, justifying the motivation behind this paper.", "startOffset": 123, "endOffset": 126}, {"referenceID": 22, "context": "To further validate our method, we applied it to the MNIST classification problem [23], a dataset of 70k greyscale images of handwritten digits represented on a 2D grid of size 28\u00d728.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "[11] and better than what is offered by other spectral-based approaches (98.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "2 [6], 94.", "startOffset": 2, "endOffset": 5}, {"referenceID": 14, "context": "96 [15]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "Accuracy on MNIST dataset [23].", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.", "creator": "LaTeX with hyperref package"}}}