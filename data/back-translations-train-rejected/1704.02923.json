{"id": "1704.02923", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Pay Attention to Those Sets! Learning Quantification from Images", "abstract": "Major advances have recently been made in merging language and vision representations. But most tasks considered so far have confined themselves to the processing of objects and lexicalised relations amongst objects (content words). We know, however, that humans (even pre-school children) can abstract over raw data to perform certain types of higher-level reasoning, expressed in natural language by function words. A case in point is given by their ability to learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From formal semantics and cognitive linguistics, we know that quantifiers are relations over sets which, as a simplification, we can see as proportions. For instance, in 'most fish are red', most encodes the proportion of fish which are red fish. In this paper, we study how well current language and vision strategies model such relations. We show that state-of-the-art attention mechanisms coupled with a traditional linguistic formalisation of quantifiers gives best performance on the task. Additionally, we provide insights on the role of 'gist' representations in quantification. A 'logical' strategy to tackle the task would be to first obtain a numerosity estimation for the two involved sets and then compare their cardinalities. We however argue that precisely identifying the composition of the sets is not only beyond current state-of-the-art models but perhaps even detrimental to a task that is most efficiently performed by refining the approximate numerosity estimator of the system.", "histories": [["v1", "Mon, 10 Apr 2017 16:03:31 GMT  (5298kb,D)", "http://arxiv.org/abs/1704.02923v1", "Submitted to Journal Paper, 28 pages, 12 figures, 5 tables"]], "COMMENTS": "Submitted to Journal Paper, 28 pages, 12 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["ionut sorodoc", "sandro pezzelle", "aur\\'elie herbelot", "mariella dimiccoli", "raffaella bernardi"], "accepted": false, "id": "1704.02923"}, "pdf": {"name": "1704.02923.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["1@unitn.it", "2@unitn.it", "3@unitn.it", "5firstname.lastname@unitn.it", "4{mariella.dimiccoli}@cvc.uab.es"], "sections": [{"heading": null, "text": "From formal semantics and cognitive linguistics, we know that quantifiers are relationships over quantities that we can, in simplified terms, consider to be proportions. Thus, most fish code the proportion of fish that are red fish. In this paper, we examine how well current language and visual strategies model such relationships. We show that modern attention mechanisms combined with a traditional linguistic formalization of quantifiers perform best in the task assignment.In addition, we offer insights into the role of \"toxic\" representations in quantification. A \"logical\" strategy to accomplish the task would be to first obtain an estimate of the numerosity for the two quantifiers involved and then compare their cardinality.However, we argue that accurately determining the composition of quantities not only exceeds the current state of the art, but may even have a detrimental effect on a task performed most efficiently by refining the non-endangered numerical system."}, {"heading": "1 Introduction", "text": "It is about the question, if and to what extent the people in the USA, in Europe, in Europe, in the world, in the world, in the USA, in Europe, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in, in the, in the, in, in the, in, in, in, in, in, in the, in, in the, in, in, in, in, in, in, in, in the, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in,"}, {"heading": "2 Related Work", "text": "The problem of algorithmically descriptive logical quantifiers was first addressed by (van Benthem, 1986) using automatons. Following these initial efforts, a lot of work was done in formal semantics of calculation to explore quantifiers in language (see e.g. (Szabolsci, 2010; Keenan and Paperno, 2012) in order to get an overview. Recently, visual semantics has turned to the problem of predicting the quantification of a concept possession pair. However, this line of work only looks at linguistic modality without paying attention to vision.Parallel to formal linguistic models, functional words were examined from a statistical perspective using NN architectures of the 1990s."}, {"heading": "3 Data", "text": "The answer is a quantifier (no, few, some, most or all); the query is a < object, property > pair (e.g. < dog, black >) so that the object and property correspond to the restrictor or scope of the quantifier; the scenario is an image that contains objects that may or may not be of the restrictor type, and that may or may not be expressed by the scope; we refer to the objects that have the required property (e.g. black dogs) as targets; we use quantifiers to represent fixed relationships (operationalized as proportions) between the relevant object sets: | restrictor, scope | restrictor |. Therefore, we assume that none and all are the correct answers for scenarios in which the target objects are set at 0% or 100% of the restrictor, respectively."}, {"heading": "3.1 From COCO ATTRIBUTE to Q-COCO", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is not a country, but a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which"}, {"heading": "3.2 From ImageNet to Q-ImageNet", "text": "In fact, most of us are able to surpass ourselves, both in terms of quality and in terms of the way we move, and in terms of the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move."}, {"heading": "4 Experimental Settings", "text": "For both sets of data, we experiment with four experimental settings that allow us to test the system's behavior under different training conditions.3 To these invisible pairs, we assign a PMI of 0.01 - the lowest PMI for seen pairs is 0.46 (\"cheese, grass\").Uncontrolled (UNC) From the entire set of data points generated, we randomly select a balanced number of cases for each quantifier. In this setting, it is possible to make known scenarios or queries at test time, but scenario query combinations are all invisible. (This setting is essentially the sampled data used to control the bias of the data sets in the previous section.) Invisible Objects (UnsObj) This setting tests the generalization power of our models against scenarios that contain invisible objects. Randomly, we share our list of concepts and choose 70% for training, 30% for testing / validation."}, {"heading": "5 Models", "text": "We are experimenting with seven different models to try to understand the contribution of different mechanisms and architectures in the quantification task. The first two models, \"blind\" BOW and BOW + CNN, are simple baselines from the VQA literature (adapted from (Zhou et al., 2015). They show how a language-only model works via one-hot representations and via a simple concatenation of one-hot speech vectors and CNN image representations. The next two models, \"blind\" LSTM and LSTM + CNN, check the contribution of sequential processing to the task, both in a language-only system and using both modalities. We expect sequential processing to take into account to some extent the composition of the restrictor and application components of the query, while it should not play a relevant role for the visual input, as these are groups of Bounding Boxes in which the sequence is not relevant to the quantification of the Qantification system (we then turn to the Attention Mechanism N clearly)."}, {"heading": "5.1 Vector Representations", "text": "Visual Input For each Bounding Box in each scenario, we extract a visual representation using a Convolutional Neural Network (Simonyan and Zisserman, 2014).We use the VGG-19 model, which was pre-trained for feature extraction based on ImageNet ILSVRC data (Russakovsky et al., 2015) and MatConvNet (Vedaldi and Lenc, 2015).Each Bounding Box is represented by a 4096-dimensional vector extracted from the 7th completely connected layer (fc7). To increase computing efficiency, we then reduce the vectors to 400 dimensions by applying Singular Value Decomposition (SVD).Linguistic input Similarly, each word in a query is represented by a 400-dimensional vector, the 2013 Cordal, with the BOW architecture."}, {"heading": "5.2 Baselines: BOW and BOW+CNN", "text": "As baselines, we consider two models that have demonstrated remarkable accuracy in the VQA task due to their simplicity: BOW and iBOWIMG (Zhou et al., 2015).4 We implement minor adjustments to these models to meet the quantification task described below. \"Blind\" BOW This is a purely linguistic model. The network has an input layer that is the size of the entire vocabulary (in our case, all the concepts and properties in our data sets).The query (e.g. black dog) is initially converted into a vector (BOW) vector (activation of units for black and dog in the input layer), which is then embedded into a \"word feature.\" The combined features are sent to a Softmax layer that predicts the response by assigning appropriate weights to an output layer in which each node corresponds to a visual level."}, {"heading": "5.3 The role of sequential processing: LSTM and LSTM+CNN", "text": "This model receives the linguistic embeddings for each query as input. Subsequently, the input is processed by a two-cell LSTM module, which we hope will simulate the composition of the restrictor and scope components of the query; its output is mapped linearly into a five-dimensional vector, on which a Softmax classifier is applied to enable correct quantification. CNN + LSTM As shown in Figure 6, CNN visual features are processed by an LSTM, combining the output of the last cell (Gist1) with the linguistic information provided by the \"Blind LSTM\" module that processes the query (Gist2).Gist1 and Gist2 are linked to a single vector that uses a softmax classifier to output the quantifier with the highest probability."}, {"heading": "5.4 The role of attention mechanisms: SAN", "text": "Stacked Attention Network (SAN) The Stacked Attention Network (SAN) proposed by (Yang et al., 2015) is motivated by the idea that VQA might be more than one step of reasoning. The model is designed to pay particular attention to the image regions relevant to the query via the attention layer. Figure 8 shows the diagram zooming into the main module of the network: the attention layer. This layer summarizes each visual vector with the linguistic representation and then applies a tanh and softmax function to the result to obtain a weighted average of the initial visual vectors (\"ghst\"). Thus, the core encodes information about both the question and the image. According to the purpose of this architecture, which is to perform a multi-level reasoning, the attention layer is used twice in SAN. As shown in Figure 7, a first pass applies the representation of the query, as predicted by a LSTM module, to the visual input is made to the second input."}, {"heading": "5.5 The role of formal linguistic structure: QMN", "text": "In the neeisrcnlhsrc\u00fceBnlhsrtee\u00fccnlhsrc\u00fc\u00fceegnlhgc\u00fce eeisrf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die r"}, {"heading": "5.6 Putting it all together: QSAN", "text": "Our Quantification Stacked Attention Network (QSAN, Fig. 10) is an adjustment of the SAN model using the linguistically informed structure of the QMN. The system follows two steps, as in QMN.Step 1: the SAN model is newly implemented, with the main difference that the given linguistic information is only the restrictor, e.g. the embedding of the word dog. We refer to this part of the model as the Restrictor-SAN module and its output as the Restrictor-Gist. The network then takes the probabilities obtained from the softmax layer in the Restrictor-SAN module, and uses these probabilities to weight the initial visual vectors. We assume that this process will pay attention to the correct regions of the visual scenario in order to find the restrictor theorem (e.g. the dogs in the picture). As in QMN, the composition of the visual restrictor and the weighting of the scopter is determined by the scanner."}, {"heading": "6 Results and Analysis", "text": "In this section we report on the results of all models described in \u00a7 5 in all experimental environments described in \u00a7 4. Then we zoom into more quantitative and qualitative analyses to better interpret the results."}, {"heading": "6.1 Results", "text": "In fact, most of them are able to survive on their own."}, {"heading": "6.2 Analysis", "text": "To understand the results with QSAN, we perform two types of analysis. The first is that the task of predicting the correct quantifier is more difficult if the scenario contains an increasing number of deflectors that have the same property. We verify this by calculating the total number of cases for deflecting the deflectors (i.e., the number of deflectors)."}, {"heading": "7 Conclusion", "text": "As discussed in Section 1, assigning a quantifier to a scenario involves two steps a) an approximate number estimation mechanism that acts on the relevant quantities in the image; b) a quantification comparison step. The simplest and most logical strategy to learn such a two-step operation would be to divide the task into two subtasks: learning a correlation (a) of raw data to abstract fixed representations and (b) of the latter to quantifiers. The high results obtained in (Sorodoc et al., 2016), which have trained NNNs to quantify synthetic scenarios of colored dots, suggest that NNs should be able to learn the second subtask quite easily. Our own experiments with a flat CNN with only a false layer of abstract images confirm this. However, we know from previous work that object identification, and in particular property identification, is not a solved problem."}], "references": [{"title": "Association for Computational Linguistics", "author": ["San Diego", "California"], "venue": "Neural module networks", "citeRegEx": "Diego and California.,? \\Q2016\\E", "shortCiteRegEx": "Diego and California.", "year": 2016}, {"title": "Entailment above the word", "author": ["M. Vision (ICCV). Baroni", "R. Bernardi", "Do", "N.-Q", "Shan", "C.-c"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "The wacky wide web", "author": ["S. Bernardini", "A. Ferraresi", "E. Zanchetta"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2009}, {"title": "Formal distributional semantics: Introduction", "author": ["G. 238\u2013247. Boleda", "A. Herbelot"], "venue": null, "citeRegEx": "Boleda and Herbelot,? \\Q2016\\E", "shortCiteRegEx": "Boleda and Herbelot", "year": 2016}, {"title": "Salient object detection: A benchmark", "author": ["A. Borji", "M. Cheng", "H. Jiang", "J. Li"], "venue": null, "citeRegEx": "Borji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Borji et al\\.", "year": 2014}, {"title": "Empirical Methods in Natural Language Processing", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "A. Yuille"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Visual turing test", "author": ["D. Representations. Geman", "S. GErman", "N. Hallonquist", "L. Younes"], "venue": null, "citeRegEx": "Geman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2015}, {"title": "Making the V", "author": ["Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": null, "citeRegEx": "Goyal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goyal et al\\.", "year": 2016}, {"title": "Building a shared world: Mapping distributional to", "author": ["A. ArXiv e-prints. Herbelot", "E.M. Vecchi"], "venue": null, "citeRegEx": "Herbelot and Vecchi,? \\Q2015\\E", "shortCiteRegEx": "Herbelot and Vecchi", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick"], "venue": "In Proceedings of CVPR 2017", "citeRegEx": "Johnson et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "Generics, prevalence, and default inferences", "author": ["S. Khemlani", "Leslie", "S.-J.", "S. Glucksberg"], "venue": "Proceedings of the 31st annual conference of the Cognitive Science Society, pages 443\u2013448. Cognitive Science Society Austin, TX.", "citeRegEx": "Khemlani et al\\.,? 2009", "shortCiteRegEx": "Khemlani et al\\.", "year": 2009}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "R.E.J. Bradbury", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "Proceedings of NAACL.", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin", "T.-Y.", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "Proceedings of ECCV (European Conference on Computer Vision).", "citeRegEx": "Lin et al\\.,? 2014a", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "T.-Y.", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "Microsoft COCO: Common Objects in Context.", "citeRegEx": "Lin et al\\.,? 2014b", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence(AAAI).", "citeRegEx": "Ma et al\\.,? 2016", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Malinowski and Fritz,? 2014", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "In International Conference on Computer Vision (ICCV\u201915).", "citeRegEx": "Malinowski et al\\.,? 2015", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Coco attributes: Attributes for people, animals, and objects", "author": ["G. Patterson", "J. Hays"], "venue": "European Conference on Computer Vision.", "citeRegEx": "Patterson and Hays,? 2016", "shortCiteRegEx": "Patterson and Hays", "year": 2016}, {"title": "Be precise or fuzzy: Learning the meaning of cardinals and quantifiers from vision", "author": ["S. Pezzelle", "M. Marelli", "R. Bernardi"], "venue": "In Proceedings of EACL.", "citeRegEx": "Pezzelle et al\\.,? 2017", "shortCiteRegEx": "Pezzelle et al\\.", "year": 2017}, {"title": "Learning and the language of thought", "author": ["S.T. Piantadosi"], "venue": "PhD thesis, Massachusetts Institute of Technologu.", "citeRegEx": "Piantadosi,? 2011", "shortCiteRegEx": "Piantadosi", "year": 2011}, {"title": "Modeling the acquistiion of quantifier semantics: a case study in function word learnability", "author": ["S.T. Piantadosi", "J.B. Tenenbaum", "N.D. Goodman"], "venue": null, "citeRegEx": "Piantadosi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Piantadosi et al\\.", "year": 2012}, {"title": "Grounding linguistic quantifiers in perception: Experiments on numerosity judgments", "author": ["R. Rajapakse", "A. Cangelosi", "K. Conventry", "S. Newstead", "A. Bacon"], "venue": "Proceeding of the 2nd Language and Technology Conference, Poland.", "citeRegEx": "Rajapakse et al\\.,? 2005", "shortCiteRegEx": "Rajapakse et al\\.", "year": 2005}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Advances in Neural Information Processing Systems (NIPS 2015).", "citeRegEx": "Ren et al\\.,? 2015a", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "In International Conference on Machine Learning Deep Learning Workshop.", "citeRegEx": "Ren et al\\.,? 2015b", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Learning to count with deep object features", "author": ["S. Seg\u00fa\u0131", "O. Pujol", "J. Vitria"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 90\u201396.", "citeRegEx": "Seg\u00fa\u0131 et al\\.,? 2015", "shortCiteRegEx": "Seg\u00fa\u0131 et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman,? 2014", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "look, some green circles!: Learning to quantify from image", "author": ["I. Sorodoc", "A. Lazaridou", "G.B.A. H", "S. Pezzelle", "R. Bernardi"], "venue": "In Proceedings of the 5th Workshop on Vision and Language,", "citeRegEx": "Sorodoc et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sorodoc et al\\.", "year": 2016}, {"title": "Emergence of a\u2019visual number sense\u2019in hierarchical generative models", "author": ["I. Stoianov", "M. Zorzi"], "venue": "Nature neuroscience, 15(2):194\u2013196.", "citeRegEx": "Stoianov and Zorzi,? 2012", "shortCiteRegEx": "Stoianov and Zorzi", "year": 2012}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS 2015), volume 28.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Quantification", "author": ["A. Szabolsci"], "venue": "Cambridge University Press.", "citeRegEx": "Szabolsci,? 2010", "shortCiteRegEx": "Szabolsci", "year": 2010}, {"title": "Essays in Logical Semantics", "author": ["J. van Benthem"], "venue": "Reidel Publishing Co,", "citeRegEx": "Benthem,? \\Q1986\\E", "shortCiteRegEx": "Benthem", "year": 1986}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia.", "citeRegEx": "Vedaldi and Lenc,? 2015", "shortCiteRegEx": "Vedaldi and Lenc", "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "In Proceedings of International Conference on Machine Learning (ICML).", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J.L. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for imagequestion answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "In Proceedings of CVPR.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "CoRR, abs/1511.02274.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Salient object subitizing", "author": ["J. Zhang", "S. Ma", "M. Sameki", "S. Sclaroff", "M. Betke", "Z. Lin", "X. Shen", "B. Price", "R.M. ech"], "venue": "In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "Proceedings of CVPR.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Suhkbaatar", "A. Szlam", "R. Fergus"], "venue": "Technical report, arXiv:1512.02167, 2015.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "A well-founded, broad-coverage semantics should therefore jointly model lexical items and functional operators (Boleda and Herbelot, 2016).", "startOffset": 111, "endOffset": 138}, {"referenceID": 13, "context": "One strand of work concentrates on content word representations, and nouns in particular (see for example (Anderson et al., 2013; Lazaridou et al., 2015)), whilst another is interested in approximate sentence representation, as in the Image Captioning (IC) and the Visual Question Answering tasks (VQA) (e.", "startOffset": 106, "endOffset": 153}, {"referenceID": 26, "context": "It has been found that out-of-the-shelf state-of-the-art (SoA) systems perform poorly on the type of questions (Ren et al., 2015b; Antol et al., 2015) which requires exact numerosity estimation, although recent work shows that it might be possible to adapt them to the counting task (Chattopadhyay et al.", "startOffset": 111, "endOffset": 150}, {"referenceID": 30, "context": "The high results obtained in (Sorodoc et al., 2016), who have trained NNs to quantify over synthetic scenarios of coloured dots, suggest that NNs should be able to learn the second subtask quite easily.", "startOffset": 29, "endOffset": 51}, {"referenceID": 43, "context": "(Zhou et al., 2015), for example, have demonstrated that a simple bag-of-word baseline, that concatenates visual and textual inputs, can achieve very decent overall performance on the VQA task.", "startOffset": 0, "endOffset": 19}, {"referenceID": 42, "context": "Part of these results might be due to the language prior that has been discovered in the VQA dataset (Zhang et al., 2016; Johnson et al., 2017) and that has been addressed by either using abstract scenes or by carefully building a dataset of very similar natural images corresponding to different answers (Goyal et al.", "startOffset": 101, "endOffset": 143}, {"referenceID": 10, "context": "Part of these results might be due to the language prior that has been discovered in the VQA dataset (Zhang et al., 2016; Johnson et al., 2017) and that has been addressed by either using abstract scenes or by carefully building a dataset of very similar natural images corresponding to different answers (Goyal et al.", "startOffset": 101, "endOffset": 143}, {"referenceID": 7, "context": ", 2017) and that has been addressed by either using abstract scenes or by carefully building a dataset of very similar natural images corresponding to different answers (Goyal et al., 2016).", "startOffset": 169, "endOffset": 189}, {"referenceID": 33, "context": "(Szabolsci, 2010; Keenan and Paperno, 2012) for an overview).", "startOffset": 0, "endOffset": 43}, {"referenceID": 1, "context": "Recently, distributional semantics has turned to the problem, with (Baroni et al., 2012) demonstrating that some entailment relations hold between quantifier vectors obtained from large corpora, and (Herbelot and Vecchi, 2015) mapping a distributional vector space to a formal space from which the quantification of a concept-property pair can be predicted.", "startOffset": 67, "endOffset": 88}, {"referenceID": 8, "context": ", 2012) demonstrating that some entailment relations hold between quantifier vectors obtained from large corpora, and (Herbelot and Vecchi, 2015) mapping a distributional vector space to a formal space from which the quantification of a concept-property pair can be predicted.", "startOffset": 118, "endOffset": 145}, {"referenceID": 24, "context": "Of particular interest to us, (Rajapakse et al., 2005) aimed at", "startOffset": 30, "endOffset": 54}, {"referenceID": 23, "context": "In the meantime, interesting progress on modelling the acquisition of quantifiers in a Bayesian probabilistic framework has been reported in (Piantadosi et al., 2012; Piantadosi, 2011).", "startOffset": 141, "endOffset": 184}, {"referenceID": 22, "context": "In the meantime, interesting progress on modelling the acquisition of quantifiers in a Bayesian probabilistic framework has been reported in (Piantadosi et al., 2012; Piantadosi, 2011).", "startOffset": 141, "endOffset": 184}, {"referenceID": 28, "context": "(Seg\u00fa\u0131 et al., 2015), for instance, explore the task of counting occurrences of an object in an image using convolutional NNs, and demonstrate that object identification can be learnt as a surrogate of counting.", "startOffset": 0, "endOffset": 20}, {"referenceID": 31, "context": "Stoianov and Zorzi (Stoianov and Zorzi, 2012) show that the ANS emerges as a statistical property of images in deep networks that learn a hierarchical generative model of visual input.", "startOffset": 19, "endOffset": 45}, {"referenceID": 41, "context": "Similarly focusing on the subitising process, (Zhang et al., 2015) address the issue of salient object detection and show how CNN models can discriminate between images with 0 to 4+ salient objects.", "startOffset": 46, "endOffset": 66}, {"referenceID": 4, "context": "As discussed in (Borji et al., 2014), the salient object detection task highly depends on various properties of the images, like the uniformity of the various regions, the complexity of the foreground and background, how close to each other the salient objects are, and how they differ in size.", "startOffset": 16, "endOffset": 36}, {"referenceID": 24, "context": "We also follow (Rajapakse et al., 2005) in their investigation of \u2018vague\u2019 linguistic quantifiers, but we train and evaluate our system on real images rather than toy examples.", "startOffset": 15, "endOffset": 39}, {"referenceID": 30, "context": "To our knowledge, (Sorodoc et al., 2016; Pezzelle et al., 2017) are the only recent attempt to model non-cardinals in a visual quantification task, using neural networks.", "startOffset": 18, "endOffset": 63}, {"referenceID": 21, "context": "To our knowledge, (Sorodoc et al., 2016; Pezzelle et al., 2017) are the only recent attempt to model non-cardinals in a visual quantification task, using neural networks.", "startOffset": 18, "endOffset": 63}, {"referenceID": 21, "context": "(Pezzelle et al., 2017) focus on the difference between the acquisition of cardinals and quantifiers, showing they can be modelled by two different operations within the network, and learning one function per cardinal/quantifier.", "startOffset": 0, "endOffset": 23}, {"referenceID": 30, "context": "Our paper can be seen as extending the work of (Sorodoc et al., 2016) by a) augmenting their list of logical quantifiers (no, some, all) with proportional ones (few, most); b) moving from artificial scenarios with geometric figures to real images; c) most importantly, treating quantifiers as relations between two sets of objects amongst a number of distractors (in contrast, their scenarios only include objects of the same type, e.", "startOffset": 47, "endOffset": 69}, {"referenceID": 25, "context": "Datasets with numerosity annotation COCO-QA (Ren et al., 2015a) was the first dataset of images associated with number questions.", "startOffset": 44, "endOffset": 63}, {"referenceID": 15, "context": "COCO-QA consists of around 123K images extracted from (Lin et al., 2014b), and 118K questions generated automatically from image descriptions.", "startOffset": 54, "endOffset": 73}, {"referenceID": 15, "context": "Starting from (Lin et al., 2014b), (Antol et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 10, "context": "The difficulty of number questions was further highlighted in (Johnson et al., 2017), where the authors introduced CLEVR (Compositional Language and Elementary Visual Reasoning diagnostics), a dataset allowing for an in-depth evaluation of current VQA models on various visual reasoning tasks.", "startOffset": 62, "endOffset": 84}, {"referenceID": 41, "context": "Focusing on the subitising phenomenon, the Salient Object Subitising (SOS) dataset, proposed in (Zhang et al., 2015), contains about 14K everyday images annotated with respect to numerosity of salient objects (from 0 to 4+).", "startOffset": 96, "endOffset": 116}, {"referenceID": 20, "context": "It contains images annotated with both objects (of various categories) and properties (Patterson and Hays, 2016).", "startOffset": 86, "endOffset": 112}, {"referenceID": 17, "context": "Neural Networks for VQA Since the pioneer work by (Malinowski and Fritz, 2014; Geman et al., 2015), many researchers have taken up the VQA challenge.", "startOffset": 50, "endOffset": 98}, {"referenceID": 6, "context": "Neural Networks for VQA Since the pioneer work by (Malinowski and Fritz, 2014; Geman et al., 2015), many researchers have taken up the VQA challenge.", "startOffset": 50, "endOffset": 98}, {"referenceID": 43, "context": "Various LSTM-CNN models have been proposed which differ with regard to the way these two types of features are combined (multimodal pooling): by mere concatenation (Zhou et al., 2015), or by more complex operations like element-wise multiplication (Antol et al.", "startOffset": 164, "endOffset": 183}, {"referenceID": 25, "context": "(Ren et al., 2015a) use an LSTM to jointly model the image and the question: they treat the image as a word appended to the question, and the image is processed by a CNN model, the output of which is frozen during the training process.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "More recently, on the opposite site, a convolutional architecture has been used to learn both types of feature and their interaction (Ma et al., 2016).", "startOffset": 133, "endOffset": 150}, {"referenceID": 38, "context": "(Xu et al., 2015), for instance, introduced an attention-based framework into the problem of image caption generation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 36, "context": "Memory Networks (MNs) have been used to tackle tasks involving reasoning on natural language text (Weston et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 98, "endOffset": 144}, {"referenceID": 32, "context": "Memory Networks (MNs) have been used to tackle tasks involving reasoning on natural language text (Weston et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 98, "endOffset": 144}, {"referenceID": 12, "context": "(Kumar et al., 2016) and recently applied to the VQA challenge in the Dynamic Memory Network (DNM+) (Xiong et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 37, "context": ", 2016) and recently applied to the VQA challenge in the Dynamic Memory Network (DNM+) (Xiong et al., 2016) and Stacked Attention Networks (SANs) (Yang et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 39, "context": ", 2016) and Stacked Attention Networks (SANs) (Yang et al., 2016).", "startOffset": 46, "endOffset": 65}, {"referenceID": 11, "context": "reported by (Khemlani et al., 2009) for low-prevalence and majority predications.", "startOffset": 12, "endOffset": 35}, {"referenceID": 20, "context": "COCO-Attribute (Patterson and Hays, 2016) is a dataset with comprehensive property annotation.", "startOffset": 15, "endOffset": 41}, {"referenceID": 14, "context": "It contains 84K image from MS-COCO (Lin et al., 2014a).", "startOffset": 35, "endOffset": 54}, {"referenceID": 2, "context": "This set is subsequently filtered according to the criterion that the property words must occur at least 150 times in the UkWaC corpus (Baroni et al., 2009): this ensures the quality of the corresponding word embeddings.", "startOffset": 135, "endOffset": 156}, {"referenceID": 15, "context": "We use an association measure based on MS-COCO captions (Lin et al., 2014b), which evaluates the chance of two objects to appear together in a real image.", "startOffset": 56, "endOffset": 75}, {"referenceID": 43, "context": "The first two models, \u2018blind\u2019 BOW and BOW+CNN, are simple baselines from the VQA literature (adapted from (Zhou et al., 2015)).", "startOffset": 106, "endOffset": 125}, {"referenceID": 40, "context": "We then turn to attention mechanisms and adapt the Stacked Attention Network (SAN) of (Yang et al., 2015), hoping that attention will allow the system to focus on relevant sets of individuals when quantifying.", "startOffset": 86, "endOffset": 105}, {"referenceID": 29, "context": "Visual input For each bounding box in each scenario, we extract a visual representation using a Convolutional Neural Network (Simonyan and Zisserman, 2014).", "startOffset": 125, "endOffset": 155}, {"referenceID": 27, "context": "We use the VGG-19 model pre-trained on the ImageNet ILSVRC data (Russakovsky et al., 2015) and the MatConvNet (Vedaldi and Lenc, 2015) toolbox for features extraction.", "startOffset": 64, "endOffset": 90}, {"referenceID": 35, "context": ", 2015) and the MatConvNet (Vedaldi and Lenc, 2015) toolbox for features extraction.", "startOffset": 27, "endOffset": 51}, {"referenceID": 19, "context": "Linguistic input Similarly, each word in a query is represented by a 400-dimension vector built with the Word2Vec CBOW architecture (Mikolov et al., 2013), using the parameters that were shown to perform best in (Baroni et al.", "startOffset": 132, "endOffset": 154}, {"referenceID": 43, "context": "As baselines, we consider two models which have shown remarkable accuracy on the VQA task, given their simplicity: BOW and iBOWIMG (Zhou et al., 2015).", "startOffset": 131, "endOffset": 150}, {"referenceID": 40, "context": "Stacked Attention Network (SAN) The Stacked Attention Network (SAN) proposed by (Yang et al., 2015) is motivated by the idea that VQA might require more than one step of reasoning.", "startOffset": 80, "endOffset": 99}, {"referenceID": 32, "context": "This model is an adaptation of the Memory Network originally proposed by (Sukhbaatar et al., 2015), which achieved state-of-the-art performance in both synthetic question answering and language modelling.", "startOffset": 73, "endOffset": 98}, {"referenceID": 30, "context": "The high results obtained in (Sorodoc et al., 2016), who have trained NNs to quantify over synthetic scenarios of coloured dots, suggest that NNs should be able to learn the second subtask quite easily.", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "Major advances have recently been made in merging language and vision representations. But most tasks considered so far have confined themselves to the processing of objects and lexicalised relations amongst objects (content words). We know, however, that humans (even pre-school children) can abstract over raw data to perform certain types of higherlevel reasoning, expressed in natural language by function words. A case in point is given by their ability to learn quantifiers, i.e. expressions like few, some and all. From formal semantics and cognitive linguistics, we know that quantifiers are relations over sets which, as a simplification, we can see as proportions. For instance, in most fish are red, most encodes the proportion of fish which are red fish. In this paper, we study how well current language and vision strategies model such relations. We show that state-of-theart attention mechanisms coupled with a traditional linguistic formalisation of quantifiers gives best performance on the task. Additionally, we provide insights on the role of \u2018gist\u2019 representations in quantification. A \u2018logical\u2019 strategy to tackle the task would be to first obtain a numerosity estimation for the two involved sets and then compare their cardinalities. We however argue that precisely identifying the composition of the sets is not only beyond current state-of-the-art models but perhaps even detrimental to a task that is most efficiently performed by refining the approximate numerosity estimator of the system.", "creator": "LaTeX with hyperref package"}}}