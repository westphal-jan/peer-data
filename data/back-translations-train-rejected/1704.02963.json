{"id": "1704.02963", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Exploring Word Embeddings for Unsupervised Textual User-Generated Content Normalization", "abstract": "Text normalization techniques based on rules, lexicons or supervised training requiring large corpora are not scalable nor domain interchangeable, and this makes them unsuitable for normalizing user-generated content (UGC). Current tools available for Brazilian Portuguese make use of such techniques. In this work we propose a technique based on distributed representation of words (or word embeddings). It generates continuous numeric vectors of high-dimensionality to represent words. The vectors explicitly encode many linguistic regularities and patterns, as well as syntactic and semantic word relationships. Words that share semantic similarity are represented by similar vectors. Based on these features, we present a totally unsupervised, expandable and language and domain independent method for learning normalization lexicons from word embeddings. Our approach obtains high correction rate of orthographic errors and internet slang in product reviews, outperforming the current available tools for Brazilian Portuguese.", "histories": [["v1", "Mon, 10 Apr 2017 17:37:22 GMT  (42kb,D)", "http://arxiv.org/abs/1704.02963v1", "Published in Proceedings of the 2nd Workshop on Noisy User-generated Text, 9 pages"]], "COMMENTS": "Published in Proceedings of the 2nd Workshop on Noisy User-generated Text, 9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["thales felipe costa bertaglia", "maria das gra\\c{c}as volpe nunes"], "accepted": false, "id": "1704.02963"}, "pdf": {"name": "1704.02963.pdf", "metadata": {"source": "CRF", "title": "Exploring Word Embeddings for Unsupervised Textual User-Generated Content Normalization", "authors": ["Thales Felipe Costa", "Maria das Gra\u00e7as Volpe Nunes"], "emails": ["thales.bertaglia@usp.br", "gracan@icmc.usp.br"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2 Related work", "text": "In fact, it is the case that most people are able to survive themselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt \":\" I don't think they will be able to change the world. \"He added,\" I don't think they will be able to change the world. \"He stressed,\" I don't think they will be able to change the world, and that they will be able to change and change the world. \""}, {"heading": "3 Distributed Representation of Words", "text": "Distributed representations of words in a vector space, also known as word embedding, are able to capture lexical, semantic, syntactic, and even contextual similarities between words (Sridhard, 2015). The Skip-gram model, introduced by (Mikolov et al., 2013a), produced an efficient method for learning high-quality vector representations of words from large amounts of unstructured text data. Unlike previous models, this model does not include dense matrix multiplications - making training optimized and efficient. Skip-gram's model, graphically displayed in Figure 1, attempts to maximize the classification of a word based on another word in the same sentence. Each current word is used as an input to a log-linear classifier and predicts words in a window before and after the current one. Formal, the goal of the Skipgram model was used to do this."}, {"heading": "4 Similarity Measures", "text": "The cosinal distance between two D-dimensional vectors u and v can be used to determine how similar two word embeddings are. It is defined as: cosinal similarity = D-similarity i = 1ui \u00b7 vi. d-equation i = 1 (ui) 2 \u00b7 D-equation i = 1 (vi) 2 (2) 1https: / / radimrehurek.com / gensim / index.html 2http: / / www.cs.cmu.edu / ~ ark / TweetNLP / # posTo illustrate the representational power of word embeddings, the following image shows the 25 closest neighbors (closest cosinal similarity) of the word 'voc\u00ea' (They) obtained from our word embedding model: In Figure 2, the canonical form of the word (bold) is located at the center."}, {"heading": "5 Learning Normalization Lexicons", "text": "\"We have a list of words that we do not know,\" he said. \"We have a list of words that we do not know.\" (\"We\") \"We.\" (\"We\"). \"(\" We \").\" (\"We\"). \"(\" We \").\" (\"We\"). \"(\" We \").\" (\"We\"). \"(\" We \").\" (\"We\"). \"(\" We \"). (\" We \"). (\" We \"). (\" We \"). (\" We \"). (\" (\"We\"). \"(\"). \"(\"). \"(\"). \"(\"). \"(\") (\") (). (\") () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () (() () () () (() () () (() () (() () (() () () ((() () (() () ((() () () (() (() (() ((() () (() (((() () ()) () ((((() () (())) ((((()) (((())) ((((()) (((()) (((((()))) ((("}, {"heading": "5.1 Expanding the Learned Lexicon", "text": "Given the fact that only the K-closest neighbors of a canonical word are added to the lexicon, some very rare noisy words may not be among the top neighbors of a canonical word. In this case, the approach outlined above cannot find correction candidates. To solve this problem, we have added an extension step to the method. If a given noisy wn is not included in the learned lexicon, i.e. wn / \u0442 T, then we find the canonical word of L that is most similar to wn, and add it as a correction candidate. Thus, the extension step can be defined as follows: If wn / \u0418wn T [wn] \u0445 max wc \u0441L lexical similarity (wn, wc) (7) Despite its simplicity, the extension step improved the general correction memory."}, {"heading": "5.2 Representing Context", "text": "This idea could be enough to correct the Internet slang, as the same abbreviations are rarely used for different words. However, ignoring context information makes it impossible to deal with contextual orthographic errors (also known as real word errors or RWEs), which often occur when a particular word is included in the recognized lexicon (i.e. it is not per se a loud word), but is incorrect due to the context in which it appears. In Portuguese, cases of real word errors caused by the absence of a single diacritic are common and can only be corrected by taking into account the context. To do this, we used a language model (LM) (with trigrams) consisting of a Wikipedia sample of 3841834 sentences. We trained our LM using the KenLM framework, in which modified words are used based on the probability of Knesman assessments (not the trim)."}, {"heading": "6 Evaluation", "text": "In order to evaluate the proposed method and to compare it with existing tools for Brazilian Portuguese, two additional product reviews written by users were used. Each sample contains 60 reviews, each error being commented manually by a specialist. The comment takes into account the six categories of noise, those of (Duran et al., 2015), but our technique can only be applied to the correction of orthographic errors and the Internet. Firstly, we have conducted experiments to determine the best word-embedding model. We have introduced both the skip gram and the continuous editing of words (Cbow) to learn the embedding."}, {"heading": "7 Conclusion", "text": "Our approach does not require rules or domain-specific dictionaries, but only a dictionary containing a list of canonical words. It can therefore be easily expanded and adapted to the needs of each specific application. We trained distributed representations using the Skip-gram model on a large body of both Twitter posts and product reviews. The results suggest that the method exceeds the results of existing tools for Brazilian Portuguese for both Internet slang and orthographic error correction. The expansion step, while simple, greatly improved the correction. Future work will need to explore various methods of context representation. Methods for dealing with multi-word expressions, such as acronyms for Internet slang, need to be added to our framework. Although such language constructs are rare in Brazilian Portuguese, they are largely derived from English."}], "references": [{"title": "An improved error model for noisy channel spelling correction", "author": ["Eric Brill", "Robert C. Moore."], "venue": "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL \u201900, pages 286\u2013 293, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Brill and Moore.,? 2000", "shortCiteRegEx": "Brill and Moore.", "year": 2000}, {"title": "Evaluating phonetic spellers for user-generated content in brazilian portuguese", "author": ["Gustavo Augusto de Mendon\u00e7a Almeida", "Lucas Avan\u00e7o", "Magali Sanches Duran", "Erick Rocha Fonseca", "Maria das Gra\u00e7as Volpe Nunes", "Sandra Maria Alu\u00edsio."], "venue": "Computational Processing of the Portuguese Language: 12th International Conference, PROPOR 2016, Tomar, Portugal, July 13-15, 2016, Proceedings, pages 361\u2013373. Springer International Publishing.", "citeRegEx": "Almeida et al\\.,? 2016", "shortCiteRegEx": "Almeida et al\\.", "year": 2016}, {"title": "Some issues on the normalization of a corpus of products reviews in Portuguese", "author": ["Magali Sanches Duran", "Lucas Avan\u00e7o", "Sandra M Aluisio", "Thiago A S Pardo", "M Gra\u00e7as Volpe Nunes."], "venue": "Proceedings of the EACL 2014 9th Web as Corpus Workshop (WAC-9), pages 22\u201327.", "citeRegEx": "Duran et al\\.,? 2014", "shortCiteRegEx": "Duran et al\\.", "year": 2014}, {"title": "A Normalizer for UGC in Brazilian Portuguese", "author": ["Magali Sanches Duran", "Lucas Avan\u00e7o", "M Gra\u00e7as Volpe Nunes."], "venue": "Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 38\u201347.", "citeRegEx": "Duran et al\\.,? 2015", "shortCiteRegEx": "Duran et al\\.", "year": 2015}, {"title": "Automatically constructing a normalisation dictionary for microblogs", "author": ["Bo Han", "Paul Cook", "Timothy Baldwin."], "venue": "EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference, pages 421\u2013432.", "citeRegEx": "Han et al\\.,? 2012", "shortCiteRegEx": "Han et al\\.", "year": 2012}, {"title": "A large corpus of product reviews in portuguese: Tackling out-of-vocabulary words", "author": ["Nathan Hartmann", "Lucas Avan\u00e7o", "Pedro Balage", "Magali Duran", "Maria Das Gra\u00e7as Volpe Nunes", "Thiago Pardo", "Sandra Alu\u00edsio."], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), Reykjavik, Iceland, may. European Language Resources Association (ELRA).", "citeRegEx": "Hartmann et al\\.,? 2014", "shortCiteRegEx": "Hartmann et al\\.", "year": 2014}, {"title": "Social text normalization using contextual graph random walks", "author": ["Hany Hassan", "Arul Menezes."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1577\u20131586.", "citeRegEx": "Hassan and Menezes.,? 2013", "shortCiteRegEx": "Hassan and Menezes.", "year": 2013}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["Kenneth Heafield."], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Heafield.,? 2013", "shortCiteRegEx": "Heafield.", "year": 2013}, {"title": "User-generated content", "author": ["John Krumm", "Nigel Davies", "Chandra Narayanaswami."], "venue": "Ieee Cs, 7:10\u201311.", "citeRegEx": "Krumm et al\\.,? 2008", "shortCiteRegEx": "Krumm et al\\.", "year": 2008}, {"title": "Paraphrasing 4 Microblog Normalization", "author": ["Wang Ling", "Chris Dyer", "W. Alan Black", "Isabel Trancoso."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73\u201384.", "citeRegEx": "Ling et al\\.,? 2013", "shortCiteRegEx": "Ling et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Nips, pages 1\u20139.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "ArXiv Preprint.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A Mathematical Theory of Communication", "author": ["Claude E. Shannon."], "venue": "The Bell System Technical Journal, 27(3):379\u2013423.", "citeRegEx": "Shannon.,? 1948", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Normalization of non-standard words", "author": ["Richard Sproat", "Alan W. Black", "Stanley Chen", "Shankar Kumar", "Mari Ostendorf", "Christopher Richards."], "venue": "Computer Speech & Language, 15(3):287\u2013333.", "citeRegEx": "Sproat et al\\.,? 2001", "shortCiteRegEx": "Sproat et al\\.", "year": 2001}, {"title": "Unsupervised Text Normalization Using Distributed Representations of Words and Phrases", "author": ["K.R. Vivek Sridhard."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 8\u201316.", "citeRegEx": "Sridhard.,? 2015", "shortCiteRegEx": "Sridhard.", "year": 2015}, {"title": "A Log-Linear Model for Unsupervised Text Normalization", "author": ["Yi Yang", "Jacob Eisenstein."], "venue": "Emnlp, (October):61\u201372.", "citeRegEx": "Yang and Eisenstein.,? 2013", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "The growth in recent years of user-generated content (UGC) \u2013 especially the one created by ordinary people \u2013 brings forth a new niche of promising practical applications (Krumm et al., 2008).", "startOffset": 170, "endOffset": 190}, {"referenceID": 3, "context": "Most of the natural language processing tools and techniques are developed from and for texts of standard language (Duran et al., 2015).", "startOffset": 115, "endOffset": 135}, {"referenceID": 13, "context": "In general, NSWs are words which properties and meaning cannot be derived directly from a lexicon (Sproat et al., 2001).", "startOffset": 98, "endOffset": 119}, {"referenceID": 9, "context": "In contrast to standardized language, UGC is often informal, with less adherence to conventions regarding punctuation, spelling, and style (Ling et al., 2013).", "startOffset": 139, "endOffset": 158}, {"referenceID": 6, "context": "Therefore, it is considerably noisy, containing ad-hoc abbreviations, phonetic substitutions, customized abbreviations, and slang language (Hassan and Menezes, 2013).", "startOffset": 139, "endOffset": 165}, {"referenceID": 10, "context": "The vectors explicitly encode many linguistic regularities and patterns, as well as syntactic and semantic word relationships (Mikolov et al., 2013a).", "startOffset": 126, "endOffset": 149}, {"referenceID": 12, "context": "This model consists of two components: a source model and a channel model (Shannon, 1948).", "startOffset": 74, "endOffset": 89}, {"referenceID": 0, "context": "(Brill and Moore, 2000) defined the spelling correction problem as finding argmaxwP (w|s), being s the canonical word, which was sent by the source model, and w the received corrupted word.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "(Yang and Eisenstein, 2013) proposed a model in which the relationship between standard and nonstandard words may be characterized by a log-linear model with arbitrary features.", "startOffset": 0, "endOffset": 27}, {"referenceID": 6, "context": "(Hassan and Menezes, 2013) proposed a method that uses random walks on a contextual similarity bi-", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "(Han et al., 2012) also presented an approach for unsupervised construction of normalization lexicons based on context information.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "(Ling et al., 2013) proposed a supervised learning technique for learning normalization rules from machine translations of a parallel corpus of microblog messages.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Our technique is most similar to (Sridhard, 2015), since we implement an adaptation of the method presented in the mentioned work.", "startOffset": 33, "endOffset": 49}, {"referenceID": 14, "context": "The method proposed by (Sridhard, 2015) aims to learn distributed representations of words to capture the notion of contextual similarity and subsequently learn normalization lexicons from these representations in a completely unsupervised manner.", "startOffset": 23, "endOffset": 39}, {"referenceID": 2, "context": "Regarding Brazilian Portuguese, some studies have been performed considering noises in specific domains, such as reviews of products (Duran et al., 2014), and some tools have been developed specifically for that same domain.", "startOffset": 133, "endOffset": 153}, {"referenceID": 3, "context": "The normalizer described in (Duran et al., 2015) is, as far as we know, the only tool for text normalization available for Brazilian Portuguese.", "startOffset": 28, "endOffset": 48}, {"referenceID": 3, "context": "Since a large part of misspellings found in UGC is phonetically-motivated, (Duran et al., 2015) proposed a phonetic-based speller for correcting such errors.", "startOffset": 75, "endOffset": 95}, {"referenceID": 3, "context": "The results obtained by (Duran et al., 2015) will be further discussed, as they are the main source of comparison for our work.", "startOffset": 24, "endOffset": 44}, {"referenceID": 14, "context": "recently applied to a wide range of NLP tasks with surprising results (Sridhard, 2015).", "startOffset": 70, "endOffset": 86}, {"referenceID": 10, "context": "The Skip-gram model, introduced by (Mikolov et al., 2013a), brought forth an efficient method for learning high-quality vector representations of words from large amounts of unstructured text data.", "startOffset": 35, "endOffset": 58}, {"referenceID": 10, "context": "Figure 1: The Skip-gram model architecture (Mikolov et al., 2013a).", "startOffset": 43, "endOffset": 66}, {"referenceID": 11, "context": "where c is the size of the training context, centered on word wt (Mikolov et al., 2013b).", "startOffset": 65, "endOffset": 88}, {"referenceID": 5, "context": "A complete description of the corpus can be found in (Hartmann et al., 2014).", "startOffset": 53, "endOffset": 76}, {"referenceID": 3, "context": "Next, the sentences were segmented by using the tool provided by (Duran et al., 2015).", "startOffset": 65, "endOffset": 85}, {"referenceID": 6, "context": "We compute its value using an adaptation of the definition presented in (Hassan and Menezes, 2013), as:", "startOffset": 72, "endOffset": 98}, {"referenceID": 14, "context": "We used K = 25, following (Sridhard, 2015) experiments.", "startOffset": 26, "endOffset": 42}, {"referenceID": 14, "context": "score(wn, wc) = n\u00d7 lexical similarity(wn, wc) + (1\u2212 n)\u00d7 cosine similarity(wn, wc) (6) Different from (Sridhard, 2015), our approach considers the cosine similarity at the score function, because some corrections have low lexical similarity but appear in the same context, specially for abbreviations \u2013 for instance, d+ as abbreviation for demais (too much).", "startOffset": 101, "endOffset": 117}, {"referenceID": 7, "context": "We trained our LM using the KenLM framework, which employs modified Kneser-Ney smoothing for estimation (Heafield, 2013).", "startOffset": 104, "endOffset": 120}, {"referenceID": 3, "context": "The annotation considers the six categories of noise proposed by (Duran et al., 2015), but our technique can only be applied to the correction of orthographic errors and internet slang.", "startOffset": 65, "endOffset": 85}, {"referenceID": 3, "context": "icon models with UGCNormal, the tool proposed by (Duran et al., 2015).", "startOffset": 49, "endOffset": 69}], "year": 2017, "abstractText": "Text normalization techniques based on rules, lexicons or supervised training requiring large corpora are not scalable nor domain interchangeable, and this makes them unsuitable for normalizing user-generated content (UGC). Current tools available for Brazilian Portuguese make use of such techniques. In this work we propose a technique based on distributed representation of words (or word embeddings). It generates continuous numeric vectors of high-dimensionality to represent words. The vectors explicitly encode many linguistic regularities and patterns, as well as syntactic and semantic word relationships. Words that share semantic similarity are represented by similar vectors. Based on these features, we present a totally unsupervised, expandable and language and domain independent method for learning normalization lexicons from word embeddings. Our approach obtains high correction rate of orthographic errors and internet slang in product reviews, outperforming the current available tools for Brazilian Portuguese.", "creator": "LaTeX with hyperref package"}}}