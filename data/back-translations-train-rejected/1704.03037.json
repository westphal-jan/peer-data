{"id": "1704.03037", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Learning from Multi-View Structural Data via Structural Factorization Machines", "abstract": "Real-world relations among entities can often be observed and determined by different perspectives/views. For example, the decision made by a user on whether to adopt an item relies on multiple aspects such as the contextual information of the decision, the item's attributes, the user's profile and the reviews given by other users. Different views may exhibit multi-way interactions among entities and provide complementary information. In this paper, we introduce a multi-tensor-based approach that can preserve the underlying structure of multi-view data in a generic predictive model. Specifically, we propose structural factorization machines (SFMs) that learn the common latent spaces shared by multi-view tensors and automatically adjust the importance of each view in the predictive model. Furthermore, the complexity of SFMs is linear in the number of parameters, which make SFMs suitable to large-scale problems. Extensive experiments on real-world datasets demonstrate that the proposed SFMs outperform several state-of-the-art methods in terms of prediction accuracy and computational cost.", "histories": [["v1", "Mon, 10 Apr 2017 19:52:29 GMT  (2674kb)", "http://arxiv.org/abs/1704.03037v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chun-ta lu", "lifang he", "hao ding", "philip s yu"], "accepted": false, "id": "1704.03037"}, "pdf": {"name": "1704.03037.pdf", "metadata": {"source": "META", "title": "Learning from Multi-View Structural Data via Structural Factorization Machines", "authors": ["Chun-Ta Lu", "Lifang He", "Hao Ding", "Philip S. Yu"], "emails": ["clu29@uic.eud", "lifanghescut@gmail.com", "haoding.tourist@gmail.com", "psyu@cs.uic.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar Xiv: 170 4.03 037v 1 [cs.L G] 10 Apr 2Real-World Relations between entities can o en be observed and determined by di erent perspectives / views. For example, a user's decision whether to adopt an element relies on several aspects such as the contextual information of the decision, the element's a ributes, the user's per le and the ratings given by other users. Views presented can have multiple interactions between entities and provide complementary information. In this paper, we present a multitensor-based approach that can preserve the underlying structure of multi-view data in a generic prediction model. Specifically, we propose structural factoring machines (SFMs) that learn the common latent spaces shared by multi-view tensors and automatically adjust the meaning of each view in the prediction model."}, {"heading": "1 INTRODUCTION", "text": "This year, it is time for the Presidency of the Council of the European Union to move into the EU in order to occupy the Presidency of the Council of the European Union."}, {"heading": "2 PRELIMINARIES", "text": "In this section, we begin with a brief introduction to some related concepts and notations in tensor algebra, and then proceed to formulate the problem we are dealing with in multiview learning."}, {"heading": "2.1 Tensor Basics and Notation", "text": "A \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "2.2 Problem Formulation", "text": "In fact, it is the case that the problem is a collection of sub-areas (such as person, company, product) and a series of sub-areas in which the individual sub-areas are interwoven with each other. (...) We have a view as a tupel (...) in which the individual sub-areas are separated from each other. (...) We have the possibility of relating to the totality of sub-areas. (...) We have the possibility of entering into the totality of sub-areas. (...) We have the possibility of entering into the totality of sub-areas. (...) We have the possibility of entering into the totality of sub-areas. (...) We have the possibility of focusing on the totality of individual sub-areas. (...) We have the totality of sub-areas. (...) The totality of the sub-areas in which we are. (...) The totality of the sub-areas in which we are. (...) The totality of the sub-areas in which we are. (...) The totality of the sub-areas in which we are. (...) The totality of the sub-areas in which we are."}, {"heading": "3 METHODOLOGY", "text": "In this section, we will first discuss how to design predictive models for learning from multiple coupled tensors, then derive structural factorization machines (SFMs) that can learn the common latent spaces that are divided into coupled tensors with multiple views, and that automatically adjust the meaning of each view in the predictive model."}, {"heading": "3.1 Predictive Models", "text": "Without loss of universality, we take two views as an example to present our basic design of predictive models (K + K)."}, {"heading": "3.2 Learning Structural Factorization Machines", "text": "In connection with the traditional frameworks for assisted learning, we propose to learn the model parameters by minimizing the following regulated empirical risks: x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.3 E cient Computing with Relational Structures", "text": "In the theory of relativity, we can observe that the characteristic vectors of the same entity are repeated in the pure forma ed matrixX, whereX = [1], \u00b7 \u00b7 \u00b7 X (M)], \u00b7 RI \u00b7 N (m), \u00b7 Rim \u00b7 N (M), the characteristic matrix in m-th mode (M), as an example, in which the parts in the fourth mode (which represents the friends of the user) repeatedly appear in the rst three columns. Clearly, these repetitive pas originate from the relational structure of the same entity. In the following, we will show how the proposed SFM method can make use of the relational structure of each mode, so that learning and prediction can be scaled to predictor variables generated from the relational relationships of high cardinality. We accept the idea of [23] to avoid redundant computing computing over a set of characteristics."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "In order to evaluate the ability and applicability of the proposed SFMs, we include a range of large datasets from different areas. The statistics for each dataset are summarized in Table 2, the scheme of structural views in each dataset is presented in Fig. 4, and the details are as follows: Amazon2: e rst group of datasets are recently released by Amazon.com. is among the largest datasets available that include review texts and metadata of items. Each top product category on Amazon.com has been constructed as an independent dataset in [20]. In this paper, we take a variety of large categories as described in Tabel 2.Each sample in these datasets has review texts and metadata of items. That is, users, items, review texts, categories and links."}, {"heading": "4.2 Comparison Methods", "text": "To demonstrate the effectiveness of the proposed SFMs, we compare a number of state-of-the-art methods. Matrix Factorization (MF) is used to confirm that meta-information is helpful in improving prediction performance. We use the LIBMF implementation [6] for comparison in the experiment. Factorization Machine (FM) [22] is the most advanced method in recommendation systems. We compare its enhancement of higher order [2] with second- and third-order interactions and refer to it as FM-2 and FM-3. PolynomialNetwork (PolyNet) [16] is a recently proposed method that uses the polynomial core for all characteristics. We compare the augmented PolyNet (which adds a constant to the feature vector [3]) to the second order and third core of the structure and refer to it as PolyNet-2. Multi-View Machine (MVM), which is a common interlatorization method between the proposed slatorization models."}, {"heading": "4.3 Experimental Settings", "text": "For each data set, we randomly allocate 50%, 10% and 40% of the samples designated as a training set, validation set and test set. Validation sets are used for setting each model's hyperparameter. Each of the validation and test sets does not overlap with other sets to ensure the reasonableness of the experiment. To ensure simplicity and fair comparison, the dimension of the latent factors R = 20 and the maximum number of epochs is set to 400 in all comparison methods, and Forbenius norm regulators are used to avoid overlapping. e regularization hyperparameters are set from {10 \u2212 5, 10 \u2212 4, \u00b7 \u00b7, 100}. All methods except MF are implemented in TensorFlow, and the parameters are set with the help of the scaling variance initializer [8] and Adam optimizer Vasis [11]. We choose the learning factor {10, the learning factor 5, and the scaling factor from}."}, {"heading": "4.4 Performance Analysis", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "4.5 Computational Cost Analysis", "text": "Next, we examine the calculation costs of comparison methods. e training time (seconds per epoch) required for each dataset is shown in Fig. 5. we can easily find that the proposed SFM requires much lower computing costs for all datasets, especially for the Yelp dataset (about 11% of the computing costs required for training the FM-3). e frequency results from the use of relational structure representation. As shown in Table 2, the number of zeros in the user statement matrix Nz (X) is much greater than the number of zeros in the relational structure representation Nz (B). e number of repeating elements is much higher for the Yelp dataset than for the other dataset, because the addition of all friends of a user statement leads to considerably large repetitive blocks in the simple attribute matrix. Standard ML algorithms such as the comparison methods typically have a linear structure (X) with a high complexity for the use of the Nxity of the FM."}, {"heading": "4.6 Analysis of the Impact of Data Sparsity", "text": "As the experimental results show, the improvement of SFM compared to traditional collaborative Ltering methods (e.g. MF) is significant for data sets that are sparse, mainly because the number of samples is too small to adequately model items and users. We verify this by comparing the performance of comparative methods with MF for users with limited training data. Figure 6 shows the gain of each method compared to MF for users with limited training samples, where G1, G2 and G3 are user groups with [1, 3], [4, 6] and [7, 10] observed samples in the training set. Due to the spatial limitation, we report only the results from two Amazon data sets (Sport and Health), while the observations for the remaining data sets continue to apply. It is evident that the proposed SFM wins most in group G1, where users have extremely few training programs."}, {"heading": "4.7 Sensitivity analysis", "text": "The number of latent factors R is an important hyperparameter for the factorization models. We analyze the latent values of R and report the results in Fig. 7. e Results demonstrate once again that SFM consistently outperforms other methods with different values of R. Unlike results in other related models, which are based on latent factors [26], where predictive errors with a greater R can be steadily reduced, we find that the performance of each method is relatively stable even as R increases. It generally makes sense, since the expressivity of the model is sufficient to describe the information embedded in the data. Although greater R makes the model more meaningful, the available observations regarding the target values are too sparse, but the meta information is rich, only a few factors are needed to process the data well."}, {"heading": "5 CONCLUSIONS", "text": "We develop structural factorization machines (SFMs) that learn the common latent spaces shared in multi-view tensors while automatically adjusting the contribution of each view in the predictive model. We also use the representation of the relational structure to provide an efficient approach to avoid unnecessary computational costs in the repetition of multi-view data. It has been shown that the proposed SFMs surpass state-of-the-art factorization models in terms of predictive accuracy and computational costs in eight large-scale datasets."}, {"heading": "ACKNOWLEDGMENTS", "text": "The work of the Titan X GPU is supported in part by NSF grants IIS-1526499, CNS-1626432, NSFC (61472089, 61503253, 61672357), NSFC-Guangdong Joint Found (U1501254) and the Science Foundation of Guangdong Province (2014A030313556)."}], "references": [{"title": "All-at-once optimization for coupled matrix and tensor factorizations", "author": ["E. Acar", "T.G. Kolda", "D.M. Dunlavy"], "venue": "arXiv preprint arXiv:1105.3422", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Higher-order factorization machines", "author": ["M. Blondel", "A. Fujino", "N. Ueda", "M. Ishihata"], "venue": "Advances in Neural Information Processing Systems, pages 3351\u2013 3359", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["M. Blondel", "M. Ishihata"], "venue": "Fujino, and N.Ueda. Polynomial networks and factorization machines: New insights and e\u0081cient training algorithms. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 850\u2013858", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensor-based multiview feature selection with applications to brain diseases", "author": ["B. Cao", "L. He", "X. Kong", "P.S. Yu", "Z. Hao", "A.B. Ragin"], "venue": "ICDM, pages 40\u201349", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-viewmachines", "author": ["B. Cao", "H. Zhou", "G. Li", "P.S. Yu"], "venue": "ACM International Conference on Web Search and Data Mining, pages 427\u2013436", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Libmf: A library for parallel matrix factorization in shared-memory systems", "author": ["W.-S. Chin", "B.-W. Yuan", "M.-Y. Yang", "Y. Zhuang", "Y.-C. Juan", "C.-J. Lin"], "venue": "\u008ae Journal of Machine Learning Research, 17(1):2971\u20132975", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "\u008ce movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4):19", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving deep into recti\u0080ers: Surpassing human-level performance on imagenet classi\u0080cation", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Dusk: A dual structure-preserving kernel for supervised tensor learning with applications to neuroimages", "author": ["L. He", "X. Kong", "S.Y. Philip", "A.B. Ragin", "Z. Hao", "X. Yang"], "venue": "matrix, 3(1):2", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint community and structural hole spanner detection via harmonic modularity", "author": ["L. He", "C.-T. Lu", "J.Ma", "J. Cao", "L. Shen", "P.S. Yu"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and DataMining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Adam: Amethod for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review, 51(3):455\u2013500", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative \u0080ltering model", "author": ["Y. Koren"], "venue": "Proceedings of the 14th ACM SIGKDD international conference  on Knowledge discovery and data mining, pages 426\u2013434. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Factor in the neighbors: Scalable and accurate collaborative \u0080ltering", "author": ["Y. Koren"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), 4(1):1", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Ratings meet reviews", "author": ["G. Ling", "M.R. Lyu", "I. King"], "venue": "a combined approach to recommend. In Proceedings of the 8th ACM Conference on Recommender systems, pages 105\u2013112. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "On the computational e\u0081ciency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems, pages 855\u2013863", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilinear factorizationmachines formulti-taskmulti-view learning", "author": ["C.-T. Lu", "L. He", "W. Shao", "B. Cao", "P.S. Yu"], "venue": "Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages 701\u2013709. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Item recommendation for emerging online businesses", "author": ["C.-T. Lu", "S. Xie", "W. Shao", "L. He", "P.S. Yu"], "venue": "Proc. 25th Int. Joint Conf. Arti\u0080cial Intell., pages 3797\u20133803", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "SIGKDD, pages 785\u2013794", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "ICDM, pages 995\u20131000", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorization machines with libFM", "author": ["S. Rendle"], "venue": "Intelligent Systems and Technology, 3(3):57", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Scaling factorization machines to relational data", "author": ["S. Rendle"], "venue": "Proceedings of the VLDB Endowment, volume 6, pages 337\u2013348. VLDB Endowment", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["S. Rendle", "L. Schmidt-\u008cieme"], "venue": "In Proceedings of the third ACM international conference on Web search and data mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 650\u2013658. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Coupled group lasso for web-scale CTR prediction in display advertising", "author": ["L. Yan", "W.-j. Li", "G.-R. Xue", "D. Han"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Joint deep modeling of users and items using reviews for recommendation", "author": ["L. Zheng", "V. Noroozi", "P.S. Yu"], "venue": "Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages 425\u2013434. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "Since di\u0082erent views usually provide complementary information [4, 5, 17], how to effectively incorporate information frommultiple structural views is critical to good prediction performance for various machine learning tasks.", "startOffset": 63, "endOffset": 73}, {"referenceID": 4, "context": "Since di\u0082erent views usually provide complementary information [4, 5, 17], how to effectively incorporate information frommultiple structural views is critical to good prediction performance for various machine learning tasks.", "startOffset": 63, "endOffset": 73}, {"referenceID": 16, "context": "Since di\u0082erent views usually provide complementary information [4, 5, 17], how to effectively incorporate information frommultiple structural views is critical to good prediction performance for various machine learning tasks.", "startOffset": 63, "endOffset": 73}, {"referenceID": 20, "context": "Recent works have shown that linear models fail for tasks with very sparse data [21, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 21, "context": "Recent works have shown that linear models fail for tasks with very sparse data [21, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 1, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 20, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 2, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 249, "endOffset": 256}, {"referenceID": 15, "context": "A variety of methods have been proposed to address the data sparsity issue by factorizing the monomials (or feature interactions) with kernel tricks, such as the ANOVA kernels used in FMs [2, 21] and polynominal kernels used in polynominal networks [3, 16].", "startOffset": 249, "endOffset": 256}, {"referenceID": 9, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 164, "endOffset": 172}, {"referenceID": 23, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 164, "endOffset": 172}, {"referenceID": 8, "context": "Matrix/tensor factorization models have been a topic of interest in the areas of structural data analysis, such as community detection [10], collaborative \u0080ltering [13, 24], and neuroimage analysis [9].", "startOffset": 198, "endOffset": 201}, {"referenceID": 24, "context": "Assuming multi-view data have the same underlying lowrank structure (at least in one mode), coupled data analysis such as collective matrix factorization (CMF) [25] and coupled matrix and tensor factorization (CMTF) [1] that jointly factorize multiple matrices (or tensors) has been applied to applications such as clustering and missing data recovery.", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "Assuming multi-view data have the same underlying lowrank structure (at least in one mode), coupled data analysis such as collective matrix factorization (CMF) [25] and coupled matrix and tensor factorization (CMTF) [1] that jointly factorize multiple matrices (or tensors) has been applied to applications such as clustering and missing data recovery.", "startOffset": 216, "endOffset": 219}, {"referenceID": 24, "context": ", the weights of the \u0080\u008aing error of each matrix/tensor in the objective function [25], the weights of di\u0082erent types of latent factors in the predictive models [14], or the regularization hyperparamters of latent factor alignment [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": ", the weights of the \u0080\u008aing error of each matrix/tensor in the objective function [25], the weights of di\u0082erent types of latent factors in the predictive models [14], or the regularization hyperparamters of latent factor alignment [18].", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": ", the weights of the \u0080\u008aing error of each matrix/tensor in the objective function [25], the weights of di\u0082erent types of latent factors in the predictive models [14], or the regularization hyperparamters of latent factor alignment [18].", "startOffset": 230, "endOffset": 234}, {"referenceID": 11, "context": "3 (CP factorization [12]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "Inspired by [5], we construct tensor representation for each view over its entities by X\u0303 = x\u0303 \u25e6 x\u0303 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 x\u0303 \u2208 R1M , where x\u0303(m) = [1;x(m)] \u2208 R1+Im and \u25e6 is the outer product operator.", "startOffset": 12, "endOffset": 15}, {"referenceID": 10, "context": "For the results presented in this paper, we use the Adam optimization algorithm [11] for parameter updates.", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "We adopt the idea from [23] to avoid redundant computing on repeating pa\u008aerns over a set of feature vectors.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "com recently introduced by [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "com has been constructed as an independent dataset in [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "\u008ce ages are split in eight bins as in [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "We use the LIBMF implementation [6] for comparison in the experiment.", "startOffset": 32, "endOffset": 35}, {"referenceID": 21, "context": "Factorization Machine (FM) [22] is the state-of-the-art method in recommender systems.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "We compare with its higher-order extension [2] with up to second-order, and third-order feature interactions, and denote them as FM-2 and FM-3.", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "PolynomialNetwork (PolyNet) [16] is a recently proposedmethod that utilizes polynomial kernel on all features.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "We compare the augmented PolyNet (which adds a constant one to the feature vector [3]) with up to the second-order, and third-order kernel and denote them as PolyNet-2 and PolyNet-3.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Multi-View Machine (MVM) [5] is a tensor factorization based method that explores the latent representation embedded in the full-order interactions among all the modes.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "All the methods except MF are implemented in TensorFlow, and the parameters are initialized using scaling variance initializer [8], and Adam optimizer [11].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "All the methods except MF are implemented in TensorFlow, and the parameters are initialized using scaling variance initializer [8], and Adam optimizer [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "To investigate the performance of comparisonmethods, we adopt mean squared error (MSE) on the test data as the evaluation metrics [19, 27].", "startOffset": 130, "endOffset": 138}, {"referenceID": 26, "context": "To investigate the performance of comparisonmethods, we adopt mean squared error (MSE) on the test data as the evaluation metrics [19, 27].", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "MF usually performswell in practice [15, 22], while in datasets which are extremely sparse, as is shown in our case, MF is unable to learn an accurate representation of users/items.", "startOffset": 36, "endOffset": 44}, {"referenceID": 21, "context": "MF usually performswell in practice [15, 22], while in datasets which are extremely sparse, as is shown in our case, MF is unable to learn an accurate representation of users/items.", "startOffset": 36, "endOffset": 44}, {"referenceID": 1, "context": "\u008ce major di\u0082erence between these twomethods is the choice of kernel applied [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 133, "endOffset": 139}, {"referenceID": 3, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 141, "endOffset": 147}, {"referenceID": 5, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 141, "endOffset": 147}, {"referenceID": 6, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 153, "endOffset": 160}, {"referenceID": 9, "context": "6 is the gain of each method compared with MF for users with limited training samples, where G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set.", "startOffset": 153, "endOffset": 160}, {"referenceID": 25, "context": "In contrast to \u0080ndings in other related models based on latent factors [26]where prediction error can steadily get reducedwith larger R, we observe that the performance of each method is rather stable even with the increasing of R.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 40, "endOffset": 46}, {"referenceID": 2, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 40, "endOffset": 46}, {"referenceID": 3, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 48, "endOffset": 54}, {"referenceID": 5, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 60, "endOffset": 67}, {"referenceID": 9, "context": "G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.", "startOffset": 60, "endOffset": 67}], "year": 2017, "abstractText": "Real-world relations among entities can o\u0089en be observed and determined by di\u0082erent perspectives/views. For example, the decision made by a user on whether to adopt an item relies on multiple aspects such as the contextual information of the decision, the item\u2019s a\u008aributes, the user\u2019s pro\u0080le and the reviews given by other users. Di\u0082erent views may exhibit multi-way interactions among entities and provide complementary information. In this paper, we introduce a multi-tensor-based approach that can preserve the underlying structure of multi-view data in a generic predictive model. Speci\u0080cally, we propose structural factorization machines (SFMs) that learn the common latent spaces shared by multi-view tensors and automatically adjust the importance of each view in the predictive model. Furthermore, the complexity of SFMs is linear in the number of parameters, which make SFMs suitable to large-scale problems. Extensive experiments on real-world datasets demonstrate that the proposed SFMs outperform several state-of-the-art methods in terms of prediction accuracy and computational cost.", "creator": "LaTeX with hyperref package"}}}