{"id": "1704.03073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "abstract": "Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "histories": [["v1", "Mon, 10 Apr 2017 22:29:50 GMT  (1019kb,D)", "http://arxiv.org/abs/1704.03073v1", "12 pages, 5 Figures"]], "COMMENTS": "12 pages, 5 Figures", "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["ivaylo popov", "nicolas heess", "timothy lillicrap", "roland hafner", "gabriel barth-maron", "matej vecerik", "thomas lampe", "yuval tassa", "tom erez", "martin riedmiller"], "accepted": false, "id": "1704.03073"}, "pdf": {"name": "1704.03073.pdf", "metadata": {"source": "CRF", "title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "authors": ["Ivaylo Popov", "Nicolas Heess", "Timothy Lillicrap", "Roland Hafner", "Gabriel Barth-Maron", "Matej Vecerik", "Thomas Lampe", "Yuval Tassa", "Tom Erez", "Martin Riedmiller"], "emails": [], "sections": [{"heading": null, "text": "It is one of the biggest junctions in the history of the European Union, in which the European Commission has applied for the end of the EU accession tour."}, {"heading": "II. RELATED WORK", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "III. BACKGROUND", "text": "In this section, we briefly formalize the learning problem, summarize the DDPG algorithm, and explain its relationship to several other Q function-based reinforcement learning algorithms (RL).The RL problem consists of an agent that interacts with an environment in a sequential manner to maximize the expected sum of rewards. \"At the time of detection, the agent observes the state of the system and produces a control ut = (xt;) according to the guidelines with parameters. This causes the environment to transition to a new state + 1 according to the dynamic xt + 1.\" (\u00b7 xt, ut), and the agent receives a reward rt = r (xt, ut).The goal is to maximize the expected sum of the discounted rewards J. \"(\u03b8) = easing.\" (QPG) that the consequences of the dynamic xt + 1. \"(xt, ut), where the agent receives a reward. (xt, ut), receives a reward J\" receive a reward that is expected to maximize the expected sum (r)."}, {"heading": "IV. TASK AND EXPERIMENTAL SETUP", "text": "The complete task we are looking at in this paper is to take a raw stone off the bar and put it on the ground. This \"composite\" task can be divided into several parts, including the stacks of bricks, both of which are able to turn the arm into a random constellation."}, {"heading": "V. ASYNCHRONOUS DPG WITH VARIABLE REPLAY STEPS", "text": "In fact, it is such that the majority of them are able to survive themselves without a process occurring in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which a process occurs, in which it occurs, in which it occurs, in which it occurs, in which it occurs, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a process, in a"}, {"heading": "VI. COMPOSITE SHAPING REWARDS", "text": "This year it is more than ever before in the history of the city."}, {"heading": "VII. LEARNING FROM INSTRUCTIVE STATES", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "VIII. CONCLUSION", "text": "We have introduced two enhancements to the DDPG algorithm that make it a powerful way to learn robust strategies for complex continuous control tasks. Specifically, we have shown that by decoupling the frequency of network updates from environmental interaction, we can significantly improve data efficiency to a level that in some cases makes the difference between finding a solution or not. In addition, we have presented two methods that help guide the learning process toward good solutions, thereby reducing the pressure on exploration strategies and accelerating learning. The first, compound reward is a recipe for effective reward functions for tasks consisting of a sequence of subtasks."}, {"heading": "A. Reward function", "text": "In this section, we provide further details on the reward functions described in Section VI. For our experiments, we derived these from the state vector of the simulation, but they could also be achieved by instrumentation in the hardware. The reward functions are defined in the following sizes: \u2022 b (1) z: height of the site above brick 2, at the position where sB1 is located when brick 1 is stacked on brick 2. \u2022 sB2, y, z: locations of the site above brick 2, at the location where sB1 is located when brick 1 is stacked on brick 2. \u2022 sP {x, y, z}: x, z positions of the site of the hand - roughly which the fingertips would hit if the fingers were closed. 1) Sparse reward components: With the above conditions, we can define the successful completion of tasks."}], "references": [{"title": "Autonomous helicopter control using reinforcement learning policy search methods", "author": ["J Andrew Bagnell", "Jeff G Schneider"], "venue": "In Robotics and Automation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Relative entropy inverse reinforcement learning", "author": ["A. Boularias", "J. Kober", "J. Peters"], "venue": "In JMLR Workshop and Conference Proceedings Volume", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "A survey on policy search for robotics", "author": ["Marc Peter Deisenroth", "Gerhard Neumann", "Jan Peters"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Deep reinforcement learning for robotic manipulation", "author": ["Shixiang Gu", "Ethan Holly", "Timothy Lillicrap", "Sergey Levine"], "venue": "arXiv preprint arXiv:1610.00633,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Muprop: Unbiased backpropagation for stochastic neural networks", "author": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Tim Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning dexterous manipulation for a soft robotic hand from human demonstrations", "author": ["Abhishek Gupta", "Clemens Eppner", "Sergey Levine", "Pieter Abbeel"], "venue": "In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Reinforcement learning in feedback control", "author": ["Roland Hafner", "Martin Riedmiller"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Neural reinforcement learning controllers for a real robot application", "author": ["Roland Hafner", "Martin A. Riedmiller"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Nicolas Heess", "Gregory Wayne", "David Silver", "Tim Lillicrap", "Tom Erez", "Yuval Tassa"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Neural networks for control systems: A  survey", "author": ["K.J. Hunt", "D. Sbarbaro", "R. \u017bbikowski", "P.J. Gawthrop"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1992}, {"title": "Learning force control policies for compliant manipulation", "author": ["M. Kalakrishnan", "L. Righetti", "P. Pastor", "S. Schaal"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Learning objective functions for manipulation", "author": ["M. Kalakrishnan", "P. Pastor", "L. Righetti", "S. Schaal"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Policy gradient reinforcement learning for fast quadrupedal locomotion", "author": ["Nate Kohl", "Peter Stone"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["Sergey Levine", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["Sergey Levine", "Peter Pastor", "Alex Krizhevsky", "Deirdre Quillen"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Learning cpgbased biped locomotion with a policy gradient method", "author": ["Takamitsu Matsubara", "Jun Morimoto", "Jun Nakanishi", "Masa-aki Sato", "Kenji Doya"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["K. Muelling", "J. Kober", "O. Kroemer", "J. Peters"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Skill learning and task outcome prediction for manipulation", "author": ["P. Pastor", "M. Kalakrishnan", "S. Chitta", "E. Theodorou", "S. Schaal"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA), Shanghai,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Policy gradient methods for robotics", "author": ["Jan Peters", "Stefan Schaal"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Supersizing selfsupervision: Learning to grasp from 50k tries and 700 robot", "author": ["Lerrel Pinto", "Abhinav Gupta"], "venue": "hours. CoRR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "Proceedings of the IEEE International Conference on Neural Networks (ICNN),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1993}, {"title": "Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method", "author": ["Martin A. Riedmiller"], "venue": "In Machine Learning: ECML 2005, 16th European Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Dynamic Movement Primitives -A Framework for Motor Control in Humans and Humanoid Robotics, pages 261\u2013280", "author": ["Stefan Schaal"], "venue": "ISBN 978-4-431-31381-6", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Temporal difference learning and tdgammon", "author": ["Gerald Tesauro"], "venue": "Commun. ACM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Learning robot in-hand manipulation with tactile features", "author": ["Herke van Hoof", "Tucker Hermans", "Gerhard Neumann", "Jan Peters"], "venue": "In 15th IEEE-RAS International Conference on Humanoid Robots, Humanoids", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Neural networks for control. chapter A  Menu of Designs for Reinforcement Learning over Time, pages 67\u201395", "author": ["Paul J. Webros"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1990}, {"title": "Collective robot reinforcement learning with distributed asynchronous guided policy", "author": ["Ali Yahya", "Adrian Li", "Mrinal Kalakrishnan", "Yevgen Chebotar", "Sergey Levine"], "venue": "search. CoRR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "[7, 20, 32, 31, 11, 17]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "[7, 20, 32, 31, 11, 17]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "[7, 20, 32, 31, 11, 17]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 30, "context": "[7, 20, 32, 31, 11, 17]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "[7, 20, 32, 31, 11, 17]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": "[7, 20, 32, 31, 11, 17]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 22, "context": "What is feasible on a simulated system, where hundreds of millions of control steps are possible [23], does not necessarily transfer to real robot applications due to unrealistic learning times.", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "in the form of dynamic movement primitives [30], or in the form of strong teaching signals, e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "kinesthetic teaching of trajectories [24].", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "[18, 5, 39]), but tasks as complex as grasping-and-stacking", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "[18, 5, 39]), but tasks as complex as grasping-and-stacking", "startOffset": 0, "endOffset": 11}, {"referenceID": 38, "context": "[18, 5, 39]), but tasks as complex as grasping-and-stacking", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "On the algorithm side we build on the Deep Deterministic Policy Gradient (DDPG; [20]), a general purpose model-free", "startOffset": 80, "endOffset": 84}, {"referenceID": 33, "context": "A wide variety of techniques have been developed that exploit this idea [34], with a broad distinction often made between valuebased and policy search methods.", "startOffset": 72, "endOffset": 76}, {"referenceID": 2, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 144, "endOffset": 147}, {"referenceID": 25, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 12, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 24, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 36, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 17, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 4, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 38, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 7, "context": "The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces [3] and applications include manipulation [26, 13, 25, 37, 18, 5, 39, 8], locomotion e.", "startOffset": 186, "endOffset": 216}, {"referenceID": 15, "context": "[16, 21], and a range of other challenges such as helicopter flight [1].", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[16, 21], and a range of other challenges such as helicopter flight [1].", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "[16, 21], and a range of other challenges such as helicopter flight [1].", "startOffset": 68, "endOffset": 71}, {"referenceID": 37, "context": "[38, 35, 12, 10].", "startOffset": 0, "endOffset": 16}, {"referenceID": 34, "context": "[38, 35, 12, 10].", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "[38, 35, 12, 10].", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "[38, 35, 12, 10].", "startOffset": 0, "endOffset": 16}, {"referenceID": 21, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 22, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 5, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 6, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 19, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 31, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 30, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 10, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 16, "context": "[22, 23, 6, 7, 20, 32, 31, 11, 17].", "startOffset": 0, "endOffset": 34}, {"referenceID": 9, "context": "[10, 5, 18, 39],", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "[10, 5, 18, 39],", "startOffset": 0, "endOffset": 15}, {"referenceID": 17, "context": "[10, 5, 18, 39],", "startOffset": 0, "endOffset": 15}, {"referenceID": 38, "context": "[10, 5, 18, 39],", "startOffset": 0, "endOffset": 15}, {"referenceID": 17, "context": "One exception are guided policy search methods (GPS) [18, 39].", "startOffset": 53, "endOffset": 61}, {"referenceID": 38, "context": "One exception are guided policy search methods (GPS) [18, 39].", "startOffset": 53, "endOffset": 61}, {"referenceID": 17, "context": "[18] or model-free [39] trajectory optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[18] or model-free [39] trajectory optimization.", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "One recent work [5], closely related to the ideas followed in this paper, provides a proof of concept demonstration that value-based methods using neural network approximators can be used for robotic manipulation in the real world .", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "This work applied a Q-learning approach [7] to a door opening task in which a robotic arm fitted with an unactuated hook needed to reach to a handle and pull a door to a given angle.", "startOffset": 40, "endOffset": 43}, {"referenceID": 26, "context": "[27], including the use of several robots from which experience are gathered in parallel [19, 5, 39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[27], including the use of several robots from which experience are gathered in parallel [19, 5, 39].", "startOffset": 89, "endOffset": 100}, {"referenceID": 4, "context": "[27], including the use of several robots from which experience are gathered in parallel [19, 5, 39].", "startOffset": 89, "endOffset": 100}, {"referenceID": 38, "context": "[27], including the use of several robots from which experience are gathered in parallel [19, 5, 39].", "startOffset": 89, "endOffset": 100}, {"referenceID": 22, "context": "This can be combined with single machine or distributed training depending on whether the bottleneck is primarily one of data collection or also one of network training [23].", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "Finally, the use of demonstration data has played an important role in robot learning, both as a means to obtain suitable cost functions [2, 14, 4, 8] but also to bootstrap and thus speed up learning.", "startOffset": 137, "endOffset": 150}, {"referenceID": 13, "context": "Finally, the use of demonstration data has played an important role in robot learning, both as a means to obtain suitable cost functions [2, 14, 4, 8] but also to bootstrap and thus speed up learning.", "startOffset": 137, "endOffset": 150}, {"referenceID": 3, "context": "Finally, the use of demonstration data has played an important role in robot learning, both as a means to obtain suitable cost functions [2, 14, 4, 8] but also to bootstrap and thus speed up learning.", "startOffset": 137, "endOffset": 150}, {"referenceID": 7, "context": "Finally, the use of demonstration data has played an important role in robot learning, both as a means to obtain suitable cost functions [2, 14, 4, 8] but also to bootstrap and thus speed up learning.", "startOffset": 137, "endOffset": 150}, {"referenceID": 25, "context": "For the latter, kinesthetic teaching is widely used [26, 13, 25, 39].", "startOffset": 52, "endOffset": 68}, {"referenceID": 12, "context": "For the latter, kinesthetic teaching is widely used [26, 13, 25, 39].", "startOffset": 52, "endOffset": 68}, {"referenceID": 24, "context": "For the latter, kinesthetic teaching is widely used [26, 13, 25, 39].", "startOffset": 52, "endOffset": 68}, {"referenceID": 38, "context": "For the latter, kinesthetic teaching is widely used [26, 13, 25, 39].", "startOffset": 52, "endOffset": 68}, {"referenceID": 32, "context": "DPG [33] is a policy gradient algorithm for continuous action spaces that improves the deterministic policy function \u03c0 via backpropagation of the action-value gradient from a learned approximation to the Q-function.", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "DDPG [20] is an improvement of the original DPG algorithm adding experience replay and target networks: Experience is collected into a buffer and updates to \u03b8 and \u03c6 (eqs.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "DDPG bears a relation to several other recent model free RL algorithms: The NAF algorithm [7] which has recently been applied to a real-world robotics problem [5] can be viewed as a DDPG variant where the Q-function is quadratic in the action so that the optimal action can be easily recovered directly from the Q-function, making a separate representation of the policy unnecessary.", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "DDPG bears a relation to several other recent model free RL algorithms: The NAF algorithm [7] which has recently been applied to a real-world robotics problem [5] can be viewed as a DDPG variant where the Q-function is quadratic in the action so that the optimal action can be easily recovered directly from the Q-function, making a separate representation of the policy unnecessary.", "startOffset": 159, "endOffset": 162}, {"referenceID": 21, "context": "DDPG and especially NAF are the continuous action counterparts of DQN [22], a Q-learning algorithm that recently re-popularized the use of experience replay and target networks to stabilize learning with powerful function approximators such as neural networks.", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "NFQ [29] and NFQCA [9] employ the same basic update as DDPG and DQN, however, they are batch algorithms that perform updates less frequently and fully re-fit the Q-function and the policy network after every episode with several hundred iterations of gradient descent", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "NFQ [29] and NFQCA [9] employ the same basic update as DDPG and DQN, however, they are batch algorithms that perform updates less frequently and fully re-fit the Q-function and the policy network after every episode with several hundred iterations of gradient descent", "startOffset": 19, "endOffset": 22}, {"referenceID": 27, "context": "with Rprop [28] and using full-batch updates with the entire replay buffer.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "the stochastic value gradients (SVG) family [11], which", "startOffset": 44, "endOffset": 48}, {"referenceID": 35, "context": "We implement the experiments in a physically plausible simulation in MuJoCo [36] with the simulated arm being closely matched to a real-world Jaco arm1 setup in our lab.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "To gain a better understanding of this interaction we modified the original DDPG algorithm as described in [20] to perform a fixed but configurable number of mini-batch updates per step in the environment.", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "In [20] one update was performed after each new interaction step.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "as in [39, 5]).", "startOffset": 6, "endOffset": 13}, {"referenceID": 4, "context": "as in [39, 5]).", "startOffset": 6, "endOffset": 13}, {"referenceID": 22, "context": "This is inspired by the A3C algorithm proposed in [23], and also analogous to [5, 39].", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "This is inspired by the A3C algorithm proposed in [23], and also analogous to [5, 39].", "startOffset": 78, "endOffset": 85}, {"referenceID": 38, "context": "This is inspired by the A3C algorithm proposed in [23], and also analogous to [5, 39].", "startOffset": 78, "endOffset": 85}, {"referenceID": 14, "context": "We use the Adam optimizer [15] with local non-shared first-order statistics and a single shared instance of second-order statistics.", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "creator": "LaTeX with hyperref package"}}}