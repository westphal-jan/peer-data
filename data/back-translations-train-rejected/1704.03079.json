{"id": "1704.03079", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "WRPN: Training and Inference using Wide Reduced-Precision Networks", "abstract": "For computer vision applications, prior works have shown the efficacy of reducing the numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.", "histories": [["v1", "Mon, 10 Apr 2017 22:54:38 GMT  (122kb)", "http://arxiv.org/abs/1704.03079v1", "Under submission to CVPR Workshop"]], "COMMENTS": "Under submission to CVPR Workshop", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["asit mishra", "jeffrey j cook", "eriko nurvitadhi", "debbie marr"], "accepted": false, "id": "1704.03079"}, "pdf": {"name": "1704.03079.pdf", "metadata": {"source": "CRF", "title": "WRPN: Training and Inference using Wide Reduced-Precision Networks", "authors": ["Asit Mishra", "Jeffrey J Cook", "Eriko Nurvitadhi", "Debbie Marr"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 4.03 079v 1 [cs.L G] 10 Apr 201 7"}, {"heading": "1. Introduction", "text": "A promising approach to providing such an extremely efficient solution is the use of deep numerical precision learning algorithms. Operating in less precise mode reduces computation and data movement and storage requirements. Due to these efficiency benefits, there is much existing work that has suggested low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1]. However, most existing work in low-precision DNNs sacrifices accuracy across baseline networks. Furthermore, most previous work aims to reduce the precision of model parameters (network weights), primarily benefiting the inference step when hardware sizes are small. We observe that activation cards (neuron outputs) occupy more memory than the model parameters for batch size. This observation also holds during batch size."}, {"heading": "2. WRPN Scheme", "text": "While most of them work with reduced precision (e.g. 34, 4, 3), we find that the memory cards require more memory than the battery memory cards. As the battery memory continues to grow, so does the number of battery memories."}, {"heading": "2.1. Hardware friendly quantization scheme", "text": "To quantify tensor values, we first restrict the weights (W) within the range {-1, + 1} and activation values (A) within the range {0.1}. This is followed by a k-bit quantization of the values within the respective interval. Since a k-bit number can represent 2k numbers, we set the quantified weights as round ((((2k \u2212 1 \u2212 1) \u043c W) 2k \u2212 1 and the quantified activation values as round (((2k \u2212 1) \u0445 A) 2k \u2212 1. Onebit is reserved for character bits in the case of weights, hence the use of 2k \u2212 1 for these quantified values. In the case of corresponding affine transformations, the convolution operations (the majority of computing operations in the network) can be performed with quantified values, followed by scaling with floating point constants. This makes our scheme hardware-friendly and can be used in embedded systems."}, {"heading": "3. Conclusions", "text": "In this paper, we introduce the WRPN scheme to reduce the calculation requirements of DNNs. While most previous work has been aimed at reducing the precision of the weights, we find that activations when using mini-batches contribute significantly more to the storage space than weight parameters, thus aggressively reducing the precision of the activation values. In addition, we increase the number of filter cards in a layer and show that extending filters and reducing the precision of network parameters does not sacrifice model accuracy. Our scheme is hardware-friendly, making it practicable for deeply embedded system applications, which is useful for robotics applications."}], "references": [{"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "CoRR, abs/1602.02830,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Multi-modal variational encoder-decoders", "author": ["I.V. Serban", "A.G.O. II", "J. Pineau", "A.C. Courville"], "venue": "CoRR, abs/1612.00377,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Accelerating deep convolutional networks using low-precision and sparsity", "author": ["G. Venkatesh", "E. Nurvitadhi", "D. Marr"], "venue": "CoRR, abs/1610.00324,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["S. Zhou", "Z. Ni", "X. Zhou", "H. Wen", "Y. Wu", "Y. Zou"], "venue": "CoRR, abs/1606.06160,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Trained ternary quantization. CoRR, abs/1612.01064, 2016. 1 Compute cost is the product of the number of FMA operations and the width of the activation and weight operands", "author": ["C. Zhu", "S. Han", "H. Mao", "W.J. Dally"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1].", "startOffset": 176, "endOffset": 182}, {"referenceID": 0, "context": "Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1].", "startOffset": 176, "endOffset": 182}, {"referenceID": 0, "context": "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 12}, {"referenceID": 2, "context": "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "Using minibatches of inputs is typical in training of DNNs and for multi-modal inference [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "This has been reported in prior work as well [4].", "startOffset": 45, "endOffset": 48}], "year": 2017, "abstractText": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.", "creator": "LaTeX with hyperref package"}}}