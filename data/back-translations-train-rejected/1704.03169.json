{"id": "1704.03169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Later-stage Minimum Bayes-Risk Decoding for Neural Machine Translation", "abstract": "For extended periods of time, sequence generation models rely on beam search algorithm to generate output sequence. However, the correctness of beam search degrades when the a model is over-confident about a suboptimal prediction. In this paper, we propose to perform minimum Bayes-risk (MBR) decoding for some extra steps at a later stage. In order to speed up MBR decoding, we compute the Bayes risks on GPU in batch mode. In our experiments, we found that MBR reranking works with a large beam size. Later-stage MBR decoding is shown to outperform simple MBR reranking in machine translation tasks.", "histories": [["v1", "Tue, 11 Apr 2017 06:48:45 GMT  (570kb,D)", "https://arxiv.org/abs/1704.03169v1", null], ["v2", "Thu, 8 Jun 2017 08:02:00 GMT  (930kb,D)", "http://arxiv.org/abs/1704.03169v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raphael shu", "hideki nakayama"], "accepted": false, "id": "1704.03169"}, "pdf": {"name": "1704.03169.pdf", "metadata": {"source": "CRF", "title": "Later-Stage Minimum Bayes-Risk Decoding for Neural Machine Translation", "authors": ["Raphael Shu", "Hideki Nakayama"], "emails": ["shu@nlab.ci.i.u-tokyo.ac.jp,", "nakayama@ci.i.u-tokyo.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "erD rf\u00fc ide rf\u00fc die f \u00fc die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f der die f die f die f der die f die f der die f die f die f der die f die f die f der die f die f die f die f die f die f der die f die f der die f die f die f die f die f die f der die f die f die f der die f die f die f der die f der die f der die f die f die f der die f die f die f der die f die f die f die f die f der die f der die f der die f der die f der die f der die die f e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e"}, {"heading": "2 Related Work", "text": "MBR decoding is commonly used in SMT (Kumar and Byrne, 2004; Gonza \u0301 lez-Rubio et al., 2011; Duh et al., 2011), which also improves translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) used the SMT translation grid to guide NMT decoding with the MBR decision rule, which proves to be better than simply recalculating the N-best results of SMT. One drawback of this approach is that an SMT system must be available and must be decrypted simultaneously with the NMT model. Recently, several studies have been proposed to improve beam search through amplification learning. Li et al. (2017) uses a simplified version of the stakeholder criteria model to decode arbitrary decoding targets."}, {"heading": "3 Minimum Bayes-Risk Decoding", "text": "The MBR deciphering is a technique to find a candidate with the least expected loss (Bickel and Doksum, 1977). According to previous work in SMT (Kumar and Byrne, 2004), the Bayes risk of a candidate y is calculated using an evidence space E: R (y) = \u2211 y \"E\" (y, y \") p (y\" | x). (1) The term \"y\" indicates the discrepancy between two candidates, which is normally calculated by using 1 \u2212 BLEU (y, y \") in machine translation. In this essay, we use smoothed BLEU (LinAlgorithm 1 Later Level MBR Decoding 1: Inputs: B = Bar Size, T = Time Budget 2: Initialize: H\" discarded hypotheses E. \"B\" Finished Candidate3: for t \"1 to T\" 4: Sort H according to Equation 2: Bayes Popp. \""}, {"heading": "4 Later-stage MBR Decoding", "text": "In this section, we propose a simple decoding strategy that searches for low-risk hypotheses after we have completed the beam search. The basic idea is to use the results of the beam search as evidence space to guide the later stage of the MBR decoding. Since the hypotheses discarded by the beam search provide good starting points for finding low-risk hypotheses, we start the later stage of decoding with a selection of discarded hypotheses and not from scratch. To do this, we save B discarded hypotheses located outside the \"beam\" to a hypotheses list H, where B is the number of beam size. After the beam search, we select the best B discarded hypotheses 2 and store them in a proof room E.After collecting the discarded hypotheses in H and the evidence in the E from the beam search, we perform the additional decoding steps for the later MBR."}, {"heading": "4.1 Score Function for Hypothesis Selection", "text": "To achieve this, we select the hypotheses in H to decode them according to a score function calculated for each hypothesis in each additional step calculated as follows: S (y) = 1 | y | log p (y | x) \u2212 \u03b1R (y) \u2212 \u03b2L (y). (2) where the first part of the equation is the average log probability. The risk concept R (y) is calculated by equation. 1.However, short hypotheses are overpunished by converting them with R (y) alone, since they are certainly not similar to the finished hypotheses in E. Therefore, we add a length penalty term L (y) to promote the selection of short hypotheses proportional to the number of remaining steps: L (y) = (T \u2212 t) \u00b7 | (3) In the final step of the MZ, the risks are decoded only by the value L (y) and the final stage of the hypotheses."}, {"heading": "4.2 Fast Computation of Bayes Risk", "text": "Unfortunately, the calculation of the Bayes risk for N candidates requires an assessment of the discrepancy function \u2206 (y, y) by a factor of N \u00b7 N. The calculation is excessively time-consuming for a CPU-based implementation whose bottleneck consists in calculating the following discrepancy matrix. To solve this problem, we have developed two approaches to efficiently calculate the discrepancy matrix on GPU: (1) Calculation of BLEU values in stacks using a sophisticated GPU-based implementation (2) approach the discrepancy values using a neural network. The advantage of the approach is that implementation is independent of the criterion chosen for the discrepancy. \u2212 In practice, we have found that approximation to STPU-based implementation (2) approximates the discrepancy values with a neural network."}, {"heading": "4.3 Dynamically Adjusting Weights", "text": "Similar to Li et al. (2016), we apply the REINFORCE algorithm (Williams, 1992) to learn the optimal weights (\u03b1 and \u03b2) for each input, the difference being that we do not need to discredit weights to create a finite space for action. Instead, we apply the REINFORCE algorithm directly to learn a Gaussian policy of continuous action. The merit of this approach is that we do not need to find the effective range of each weight value beforehand. (8) p = soft plus (f (s; 1) compensate (9)."}, {"heading": "5 Experiments", "text": "We evaluate our proposed decoding strategy for English-Japanese translation tasks using the parallel ASPEC corpus (Nakazawa et al., 2016), which contains 3M training pairs and 1812 sentence pairs in the test set. We tokenize the sentences with \"tokenizer.perl\" for the English side and Kytea (Neubig et al., 2011) for the Japanese side. Vocabulary size is 80k and 40k respectively. In our experiments, we trained an NMT model with a standard architecture (Bahdanau et al., 2014) that includes 1000 units for both embedding and LSTMs."}, {"heading": "5.1 Fast Bayes-Risk Computation", "text": "In this section, we compare the speed between the Bayer GPU-based risk anchors and a standard anchor. A comparison of the reanking speed is shown in Fig. 1. For each input set, we have reordered a list of N-candidate translations. Average reanking time per sentence is given. Results show that both GPU-based approaches are much faster than a standard CPU-based anchor and can therefore be integrated into the beam search. Remarkably, the growth pattern of the average reanking time for GPU-based approaches is linear and not exponential. In our experiment, it was found that reanking candidates with approximate discrepancy values are as good as a normal anchor shown in the middle of Table 1. Training details are provided in supplementary corresponding material."}, {"heading": "5.2 Later-stage MBR decoding", "text": "In Table 1, we compare different decoding strategies in different bar size settings. BLEU values are evaluated using a standard post-processing process.We found that increasing the bar size to a large number does not consistently contribute to the evaluation results. On the contrary, MBR re-ranking improves the values when using a large bar size, but less effectively when using a small bar size. Later MBR decoding outperforms the simple MBR re-ranking in all bar size settings. In addition, we found that the number of candidates in the evidence room largely influences the effectiveness of MBR re-ranking. In our experiments, the amount of evidence is fixed to the same number of bar size. Using further evidence deteriorates the quality of selected candidates."}, {"heading": "6 Conclusion and Future Work", "text": "We calculate the Bayes risk on the GPU to speed up the gradual MBR decoding. Interestingly, we found that simple MBR re-anking is particularly effective on a large beam size. Without MBR re-anking, further increase in beam size does not yield a significant gain. For future work, we intend to create a better evidence room with an alternative neural network to later decode the MBR phase.4We produce the BLEU values using the Kytea tokenizer. The post-processing script can be found at http: / / lotus. kuee.kyoto-u.ac.jp / WAT /."}, {"heading": "A Supplemental Materials", "text": "\"A Note on the Standard Implementation for Computing Bayes RisksAs in the equation for computing R (y), all the terms in the summation are positive numbers, we stop computing the risk of a candidate if the sum is already higher than the lowest risk value of computed candidates. Even with this early stop technique, the standard MBR reranchor can still be calculated very slowly, as shown in Figure 1 of the paper. \u2212 A.2 GPU-based BLEU Computation For the particular discrepancy function based on BLEU, we found that a matrix of N \u00b7 N BLEU values can be efficient on GPU. The trick is to create an M-dimensional count vector ci for each candidate yi, which contains the count of all M uniq n-grams in the candidate space. Another vector g is used to specify the rank of each n-gramm.For example, let a set of n-gramms be {,\" park, \"\" ball, \"in a,\" the candidate space we be. \""}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Mathematical statistics: basic ideas and selected topics", "author": ["PJ Bickel", "KA Doksum"], "venue": null, "citeRegEx": "Bickel and Doksum.,? \\Q1977\\E", "shortCiteRegEx": "Bickel and Doksum.", "year": 1977}, {"title": "Noisy parallel approximate decoding for conditional recurrent language model", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1605.03835 .", "citeRegEx": "Cho.,? 2016", "shortCiteRegEx": "Cho.", "year": 2016}, {"title": "Generalized minimum bayes risk system combination", "author": ["Kevin Duh", "Katsuhito Sudoh", "Xianchao Wu", "Hajime Tsukada", "Masaaki Nagata."], "venue": "IJCNLP. pages 1356\u20131360.", "citeRegEx": "Duh et al\\.,? 2011", "shortCiteRegEx": "Duh et al\\.", "year": 2011}, {"title": "Minimum bayes risk decoding for bleu", "author": ["Nicola Ehling", "Richard Zens", "Hermann Ney."], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics, pages 101\u2013104.", "citeRegEx": "Ehling et al\\.,? 2007", "shortCiteRegEx": "Ehling et al\\.", "year": 2007}, {"title": "Minimum bayes-risk system combination", "author": ["Jes\u00fas Gonz\u00e1lez-Rubio", "Alfons Juan", "Francisco Casacuberta."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.", "citeRegEx": "Gonz\u00e1lez.Rubio et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Rubio et al\\.", "year": 2011}, {"title": "Trainable greedy decoding for neural machine translation", "author": ["Jiatao Gu", "Kyunghyun Cho", "Victor OK Li."], "venue": "arXiv preprint arXiv:1702.02429 .", "citeRegEx": "Gu et al\\.,? 2017", "shortCiteRegEx": "Gu et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Minimum bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William J. Byrne."], "venue": "HLT-NAACL.", "citeRegEx": "Kumar and Byrne.,? 2004", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "A simple, fast diverse decoding algorithm for neural generation", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1611.08562 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning to decode for future success", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1701.06549 .", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Chin-Yew Lin", "Franz Josef Och."], "venue": "ACL. https://doi.org/10.3115/1218955.1219032.", "citeRegEx": "Lin and Och.,? 2004", "shortCiteRegEx": "Lin and Och.", "year": 2004}, {"title": "Aspec: Asian scientific paper excerpt corpus", "author": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."], "venue": "Proceedings of the Ninth International Conference on Language Re-", "citeRegEx": "Nakazawa et al\\.,? 2016", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2016}, {"title": "Pointwise prediction for robust, adaptable japanese morphological analysis", "author": ["Graham Neubig", "Yosuke Nakata", "Shinsuke Mori."], "venue": "ACL. pages 529\u2013533.", "citeRegEx": "Neubig et al\\.,? 2011", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin A. Riedmiller."], "venue": "ICML.", "citeRegEx": "Silver et al\\.,? 2014", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Neural machine translation by minimising the bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne."], "venue": "arXiv preprint arXiv:1612.03791 .", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 2818\u20132826.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 16, "context": "Szegedy et al. (2016) has shown that a neural network can become too confident in a suboptimal prediction.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "To improve beam search, various approaches have been explored recently either by enhancing the scoring method (Li et al., 2016) or using reinforcement learning (Li et al.", "startOffset": 110, "endOffset": 127}, {"referenceID": 10, "context": ", 2016) or using reinforcement learning (Li et al., 2017; Gu et al., 2017).", "startOffset": 40, "endOffset": 74}, {"referenceID": 6, "context": ", 2016) or using reinforcement learning (Li et al., 2017; Gu et al., 2017).", "startOffset": 40, "endOffset": 74}, {"referenceID": 8, "context": "In this work, we try to apply Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004) to guide the decoding algorithm of NMT to find a better candidate.", "startOffset": 64, "endOffset": 87}, {"referenceID": 8, "context": "MBR decoding is widely applied in SMT (Kumar and Byrne, 2004; Gonz\u00e1lez-Rubio et al., 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al.", "startOffset": 38, "endOffset": 108}, {"referenceID": 5, "context": "MBR decoding is widely applied in SMT (Kumar and Byrne, 2004; Gonz\u00e1lez-Rubio et al., 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al.", "startOffset": 38, "endOffset": 108}, {"referenceID": 3, "context": "MBR decoding is widely applied in SMT (Kumar and Byrne, 2004; Gonz\u00e1lez-Rubio et al., 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al.", "startOffset": 38, "endOffset": 108}, {"referenceID": 4, "context": ", 2011), which is also found to improve the translation quality (Ehling et al., 2007).", "startOffset": 64, "endOffset": 85}, {"referenceID": 2, "context": "(2017) extends the noisy, parallel approximate decoding (NPAD) algorithm (Cho, 2016) by adjusting the hidden states in recurrent networks with an agent, which is trained with the deterministic policy gradient algorithm (Silver et al.", "startOffset": 73, "endOffset": 84}, {"referenceID": 14, "context": "(2017) extends the noisy, parallel approximate decoding (NPAD) algorithm (Cho, 2016) by adjusting the hidden states in recurrent networks with an agent, which is trained with the deterministic policy gradient algorithm (Silver et al., 2014).", "startOffset": 219, "endOffset": 240}, {"referenceID": 2, "context": ", 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) utilized the translation lattice of SMT to guide NMT decoding with the MBR decision rule, which is shown to be better than simply rescoring the N-best results of SMT.", "startOffset": 8, "endOffset": 139}, {"referenceID": 2, "context": ", 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) utilized the translation lattice of SMT to guide NMT decoding with the MBR decision rule, which is shown to be better than simply rescoring the N-best results of SMT. A drawback of this approach is that it requires a SMT system to be available and decode simultaneously with the NMT model. Recently, some studies are proposed to enhance beam search with reinforcement learning. Li et al. (2017) utilizes a simplified version of the actorcritic model to decode for arbitrary decoding objectives.", "startOffset": 8, "endOffset": 534}, {"referenceID": 2, "context": ", 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) utilized the translation lattice of SMT to guide NMT decoding with the MBR decision rule, which is shown to be better than simply rescoring the N-best results of SMT. A drawback of this approach is that it requires a SMT system to be available and decode simultaneously with the NMT model. Recently, some studies are proposed to enhance beam search with reinforcement learning. Li et al. (2017) utilizes a simplified version of the actorcritic model to decode for arbitrary decoding objectives. The scoring function is modified to be an interpolation of the log probability and the output of the value function (or Q function). Gu et al. (2017) extends the noisy, parallel approximate decoding (NPAD) algorithm (Cho, 2016) by adjusting the hidden states in recurrent networks with an agent, which is trained with the deterministic policy gradient algorithm (Silver et al.", "startOffset": 8, "endOffset": 784}, {"referenceID": 1, "context": "MBR decoding is a technique to find a candidate with the least expected loss (Bickel and Doksum, 1977).", "startOffset": 77, "endOffset": 102}, {"referenceID": 8, "context": "Following previous work in SMT (Kumar and Byrne, 2004), given an evidence space E, the Bayes risk of a candidate y is computed by:", "startOffset": 31, "endOffset": 54}, {"referenceID": 17, "context": "(2016), we apply the REINFORCE algorithm (Williams, 1992) to learn the optimal weights (\u03b1 and \u03b2) for each input.", "startOffset": 41, "endOffset": 57}, {"referenceID": 9, "context": "Similar to Li et al. (2016), we apply the REINFORCE algorithm (Williams, 1992) to learn the optimal weights (\u03b1 and \u03b2) for each input.", "startOffset": 11, "endOffset": 28}, {"referenceID": 12, "context": "We evaluate our proposed decoding strategy on English-Japanese translation task, using ASPEC parallel corpus (Nakazawa et al., 2016).", "startOffset": 109, "endOffset": 132}, {"referenceID": 13, "context": "perl\u201d for English side, and Kytea (Neubig et al., 2011) for Japanese side.", "startOffset": 34, "endOffset": 55}, {"referenceID": 0, "context": "In our experiments, we trained a NMT model with a standard architecture (Bahdanau et al., 2014), which has 1000 units for both the embeddings and LSTMs.", "startOffset": 72, "endOffset": 95}], "year": 2017, "abstractText": "For extended periods of time, sequence generation models rely on beam search as the decoding algorithm. However, the performance of beam search degrades when the model is over-confident about a suboptimal prediction. In this work, we enhance beam search by performing minimum Bayes-risk (MBR) decoding for some extra steps at a later stage. In our experiments, we found that the conventional MBR reranking is only effective with a large beam size. In contrast, later-stage MBR decoding is shown to work regardless of the choice of beam size, and outperform simple MBR reranking. Additionally, we found that the computation of Bayes risks can be much faster by calculating the discrepancies on GPU in batch mode.", "creator": "LaTeX with hyperref package"}}}