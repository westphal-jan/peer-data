{"id": "1704.03188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Simplified Stochastic Feedforward Neural Networks", "abstract": "It has been believed that stochastic feedforward neural networks (SFNNs) have several advantages beyond deterministic deep neural networks (DNNs): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training large-scale SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNNand approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN-&gt;Simplified-SFNN-&gt;SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CASIA, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, we train a stochastic model of 28 layers and 36 million parameters, where training such a large-scale stochastic network is significantly challenging without using Simplified-SFNN", "histories": [["v1", "Tue, 11 Apr 2017 08:19:00 GMT  (1370kb,D)", "http://arxiv.org/abs/1704.03188v1", "22 pages, 6 figures"]], "COMMENTS": "22 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kimin lee", "jaehyung kim", "song chong", "jinwoo shin"], "accepted": false, "id": "1704.03188"}, "pdf": {"name": "1704.03188.pdf", "metadata": {"source": "CRF", "title": "Simplified Stochastic Feedforward Neural Networks", "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "emails": ["kiminlee@kaist.ac.kr,", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu,", "jinwoos@kaist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to go in search of a suitable place where they can move."}, {"heading": "2 Simple Transformation from DNN to SFNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preliminaries for SFNN", "text": "Stochastic Forward-Looking Neural Network (SFNN) is a hybrid model that has both stochastic binary and deterministic hidden units. First, we introduce SFNN with a stochastic hidden layer (and without deterministic hidden layers) for simplicity. In the course of this paper, we commonly refer to the preload for unit i and the weight matrix of the \"th hidden layer\" by b'i and W. Then, the stochastic hidden layer in SFNN is defined as a binary random vector with N1 units, i.e., h1 (0, 1) N1) N1, drawn under the following distribution: P (h1 | x) = N1 P (h1i = 1 | x), where P (h1i) = x) = unit (W1i x + i i i i i i) = unit (1 i) (1) (1) of the layer above is the input (x) and the input (x)."}, {"heading": "2.2 Simple transformation from sigmoid-DNN and ReLU-DNN to SFNN", "text": "Despite recent advances, the SFNN activation function has still evolved very slowly compared to DNN, which is true of sampling methods: in particular, it is very difficult to train SFNN when the network structure is deeper and broader. To solve these problems, we consider the following approaches: P (y) = EP (h1 + b1) + b2 + b2 (2) Note: The above approximation corresponds to replacing stochastic units by deterrence, so that their hidden activation values are the same as marginal distributions of stochastic units, i.e., SFNN can be approximated using sigmoid activation functions."}, {"heading": "3 Transformation from DNN to SFNN via Simplified-SFNN", "text": "In this section, we propose an advanced method to use the pre-trained parameters of the DNN for the formation of SFNN. In addition, as shown in the previous section, simple parameter transformations from DNN to SFNN generally do not work uniquely, especially for activation functions other than sigmoid. Furthermore, the formation of the DNN does not take advantage of the stochastic regulation effect, which is an important advantage of the SFNN. To solve the problems, we design an intermediate model called Simplified-SFNN. The proposed model is a special form of stochastic neural networks that resembles specific SFNN by simplifying their upper latent units via stochastic units. Subsequently, we establish more rigorous connections between three models: DNN \u2192 Simplified-SFNN \u2192 SFNN, which leads to an efficient training process of the stochastic models by using pre-programmed parameters of the DNN. In our experiments, we evaluate the different tasks for different DNN architectures and strategies."}, {"heading": "3.1 Simplified-SFNN of two hidden layers and non-negative activation functions", "text": "It is therefore assumed that the first and second hidden layer consists of stochastic hidden units and deterministic units. (3) The first layer is defined as a random binary vector with N1 units, i.e. the first layer is defined as a random vector with N1 units. (3) The first layer is defined as a random vector with N1 units. (3) The second layer is defined as a random vector with N1 units. (4) The second layer is defined as a random vector with N1 units. (4) The second layer is defined as a random vector with N1 units. (4) The third layer is defined as a random vector with N1 units. (4) The third layer is defined as a random vector with N1 units. (4) The third layer is defined as a random vector with N1 units."}, {"heading": "3.2 Why Simplified-SFNN ?", "text": "In fact, the fact is that most of the people who have opted for the EU in recent years have opted for a different EU than for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another world, for another EU, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another world, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU, for another EU"}, {"heading": "3.3 Training Simplified-SFNN", "text": "The parameters of Simplified-SFNN can be learned using a variant of the baking propagation algorithm [29] in a similar way to DNN. However, in contrast to DNN, there are two arithmetic problems for Simplified-SFNN: calculating expectations with respect to stochastic units in forward gear and calculating gradients in reverse gear. It can be noted that both problems are insoluble since they require sums over all possible configurations of all stochastic units. First, to handle the problem in forward gear, we use the following Monte Carlo approximation to estimate the expectation: EP (h1 | x) [s (W2jh 1 + b2j)] w 1M 0 m (W2jh) + b2j), where h (m), p (h1 | x) and M (h1) are the number of samples. This random estimator is impartial and has a relatively small deviation [8] because its size does not depend on the W2h."}, {"heading": "4 Extensions of Simplified-SFNN", "text": "In this section we describe how the transfer of network knowledge between SimplifiedSFNN and DNN, i.e. Theorem 1, generalizes to several layers and general activation functions."}, {"heading": "4.1 Extension to multiple layers", "text": "A deeper Simplified-SFNN with L hidden layers can be defined similarly to the case of L = 2. We also establish the knowledge transfer between Simplified-SFNN and DNN with L hidden layers, as explained in the following theorem. Here, we assume that stochastic layers are not consecutive for a simpler representation, but the theorem is generalizable for successive stochastic layers. Theorem 2, we assume that both DNN and Simplified-SFNN with L hidden layers have the same network structure with non-negative activation function f. Given parameters are: \"= 1,..., L\" of DNN and input dataset D, we first select the same layers for Simplified-SFNN and modify them for each \"-th stochastic layer and its upper layer as follows:.\""}, {"heading": "4.2 Extension to general activation functions", "text": "In this section, we describe an advanced version of Simplified-SFNN that can use any activation function. To this end, we modify the definitions of stochastic layers and their upper layers by introducing certain additional terms. If the \"-th hidden layer is stochastical, we modify the original definition (5) slightly as follows: P (h '| x) = N'i = 1 P (h'i | x) with P (h'i = 1 | x) = min {\u03b1'f (W1i x + 1 2), 1}, where f: R is a non-linear (possibly negative) activation function with | f'i (x) | \u2264 1 for all x R. In addition, we redefine the upper layer as follows: h '+ 1 (x) = [f's (x) that we assume the layer x) x.\""}, {"heading": "5 Proofs of Theorems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Proof of Theorem 1", "text": "Let us first consider the first hidden layer, i.e. the second hidden layer. Let us first consider the second hidden layer."}, {"heading": "5.2 Proof of Theorem 2", "text": "For the proof of Theorem 2, we first list the two key lemmas for error spread in SimplifiedSFNN.Lemma 4: \"There is some positive constant B,\" so that it is defined in (7). \"Specified parameters are\" W, \"b,\" b, \"b,\" b, \"b,\" b, \"b,\" b, \"b,\" b, \"b,\" b, \"b,\" b, \"b,\" b, \"b,\" b, \"b,\" b, \"b, b,\" b, \"c,\" c, \"c,\" c, c, c, c, c, \"c, c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, c, c, c, c, c, c, c, c, c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c, c,\" c, \"c,\" c, \"c, c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c, c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,"}, {"heading": "5.3 Proof of Lemma 4", "text": "Based on the assumption that there is a certain constant i, so that | i | < B andh '\u2212 1i (x) = h \"\u2212 1 i (x) + i, i, x.After defining the deterministic standard layer, it follows that it is thath'j (x) = f (x) + b\" \u2212 1 i (x) + b \"\u2212 1 j) = f (x) + 1 i (x) + x\" i \"ij i + b\" j).Since we assume that | f \"(x) + b\" j \"j,\" we can conclude that the constant i (x) \u2212 f (x) ij h \"\u2212 1 (x) + b\" j \"j.\""}, {"heading": "5.4 Proof of Lemma 5", "text": "From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption that there is a constant. (...) From the assumption. (...) From the assumption. (...) From the assumption. (...) (...) From the assumption that there is a constant. (...)"}, {"heading": "6 Experimental Results", "text": "We present several experimental results for both multimodal and classification tasks at MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26]. Softmax and Gauss with the standard deviation of 0.05 are used as output reliability for the classification task or multimodal prediction. In all experiments, we first train a basic model, and the trained parameters are used for further fine-tuning those of Simplified-SFNN."}, {"heading": "6.1 Multi-modal regression", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "6.2 Classification", "text": "We evaluate the regulatory effects of Simplified-SFNN for the classification tasks on CIFAR-10, CIFAR-100 and SVHN. The CIFAR-10 and CIFAR-100 datasets consist of 50,000 training sessions and 10,000 test images. They have 10 and 100 image classes, respectively, the SVHN datasets consist of 73,257 training sessions and 26,032 test images. They consist of a house number 0 to 9 collected by Google Street View. Similar to [39], we process the data with global contrast standardization and ZCA whitening. For these datasets, we design a conventional version of Simplified-SFNN, the conventional neural networks such as Lenet-5 [6], NIN [13] and WRN [39]."}, {"heading": "7 Conclusion", "text": "In order to develop an efficient training method for large-scale SFNN, this paper proposes a new stochastic intermediate model called Simplified-SFNN. We establish the connection between three models, i.e. DNN \u2192 Simplified-SFNN \u2192 SFNN, which naturally leads to an efficient training sequence of the stochastic models using pre-trained parameters and architectures of the DNN. Using several popular DNNs, including Lenet-5, NIN, FCN and WRN, we show how they can be effectively transferred to the corresponding stochastic models for both multimodal and non-multimodal tasks. We believe that our work opens up a new perspective for the formation of stochastic neural networks that would be of greater interest in many related applications."}], "references": [{"title": "The generalized reparameterization gradient", "author": ["F.R. Ruiz", "M.T.R. AUEB", "D. Blei"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "CASIA online and offline Chinese handwriting databases", "author": ["C.L. Liu", "F. Yin", "D.H. Wang", "Q.F. Wang"], "venue": "In International Conference on Document Analysis and Recognition (ICDAR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Cubic convolution interpolation for digital image processing", "author": ["R. Keys"], "venue": "In IEEE transactions on acoustics, speech, and signal processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1981}, {"title": "Learning stochastic feedforward networks", "author": ["R.M. Neal"], "venue": "Department of Computer Science, University of Toronto,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning stochastic feedforward neural networks", "author": ["Y. Tang", "R.R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["T. Raiko", "M. Berglund", "G. Alain", "L. Dinh"], "venue": "arXiv preprint arXiv:1406.2989,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Masters thesis,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Weight uncertainty in neural networks", "author": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "Artificial intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Net2Net: Accelerating Learning via Knowledge Transfer", "author": ["T. Chen", "I. Goodfellow", "J. Shlens"], "venue": "arXiv preprint arXiv:1511.05641,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Networks of spiking neurons: the third generation of neural network models", "author": ["W. Maass"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.R. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.P. David"], "venue": "arXiv preprint arXiv:1511.00363,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Noisy activation functions", "author": ["C. Gulcehre", "M. Moczulski", "M. Denil", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.00391,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "In Neurocomputing: Foundations of research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1988}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1951}, {"title": "Adaptive dropout for training deep neural networks", "author": ["J. Ba", "B. Frey"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "MuProp: Unbiased backpropagation for stochastic neural networks", "author": ["S. Gu", "S. Levine", "I. Sutskever", "A. Mnih"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Y. Bengio", "N. Lonard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": "Aston University,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1994}, {"title": "The toronto face database", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton"], "venue": "Department of Computer Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Mean field theory for sigmoid belief networks", "author": ["L.K. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Artificial intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1996}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Reinforcement learning neural Turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": ", speech recognition [24] and object recognition [11].", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": ", speech recognition [24] and object recognition [11].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 28, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 200, "endOffset": 208}, {"referenceID": 9, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 200, "endOffset": 208}, {"referenceID": 15, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 237, "endOffset": 245}, {"referenceID": 43, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 237, "endOffset": 245}, {"referenceID": 31, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 280, "endOffset": 288}, {"referenceID": 26, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 280, "endOffset": 288}, {"referenceID": 4, "context": "On the other hand, stochastic feedforward neural networks (SFNNs) [5] having random latent units are often necessary in to model the complex stochastic natures of many real-world tasks, e.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": ", structured prediction [8] and image generation [41].", "startOffset": 24, "endOffset": 27}, {"referenceID": 38, "context": ", structured prediction [8] and image generation [41].", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "Furthermore, it is believed that SFNN has several advantages beyond DNN [9]: it has more expressive power for multi-modal learning and regularizes better for large-scale networks.", "startOffset": 72, "endOffset": 75}, {"referenceID": 30, "context": "Certain stochastic neural networks using continuous random units are known to be trainable efficiently using backpropagation with variational techniques and reparameterization tricks [32, 1].", "startOffset": 183, "endOffset": 190}, {"referenceID": 0, "context": "Certain stochastic neural networks using continuous random units are known to be trainable efficiently using backpropagation with variational techniques and reparameterization tricks [32, 1].", "startOffset": 183, "endOffset": 190}, {"referenceID": 4, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 36, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 7, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 33, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 8, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 32, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 31, "context": "However, it is questionable whether a similar strategy works in general, particularly for other unbounded activation functions like ReLU [33] since SFNN has binary, i.", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "The most significant part of our approach lies in providing rigorous network knowledge transferring [22] between Simplified-SFNN and DNN.", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 [6], NIN [13], FCN [2] and WRN [39].", "startOffset": 111, "endOffset": 114}, {"referenceID": 11, "context": "We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 [6], NIN [13], FCN [2] and WRN [39].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 [6], NIN [13], FCN [2] and WRN [39].", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "The multi-layer extension is straightforward via a combination of stochastic and deterministic hidden layers (see [8, 9]).", "startOffset": 114, "endOffset": 120}, {"referenceID": 8, "context": "The multi-layer extension is straightforward via a combination of stochastic and deterministic hidden layers (see [8, 9]).", "startOffset": 114, "endOffset": 120}, {"referenceID": 7, "context": "This random estimator is unbiased and has relatively low variance [8] since one can draw samples from the exact distribution.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "Next, in order to handle the issue in backward pass, [5] proposed Gibbs sampling, but it is known that it often mixes poorly.", "startOffset": 53, "endOffset": 56}, {"referenceID": 36, "context": "[38] proposed a variational learning based on the mean-field approximation, but it has additional parameters making the variational lower bound looser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 140, "endOffset": 146}, {"referenceID": 8, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 140, "endOffset": 146}, {"referenceID": 33, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 228, "endOffset": 239}, {"referenceID": 8, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 228, "endOffset": 239}, {"referenceID": 32, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 228, "endOffset": 239}, {"referenceID": 8, "context": "Although such a strategy looks somewhat \u2018rude\u2019, it was often observed in the literature that it reasonably works well for SFNN [9] and we also evaluate it as reported in Table 1.", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "We evaluate the performance of the simple transformations from DNN to SFNN on the MNIST dataset [6] and the synthetic dataset [36], where the former and the latter are popular datasets used for a classification task and a multi-modal (i.", "startOffset": 96, "endOffset": 99}, {"referenceID": 34, "context": "We evaluate the performance of the simple transformations from DNN to SFNN on the MNIST dataset [6] and the synthetic dataset [36], where the former and the latter are popular datasets used for a classification task and a multi-modal (i.", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "One can train Simplified-SFNN similarly as SFNN: we use Monte Carlo approximation for estimating the expectation and the (biased) estimator of the gradient for approximating backpropagation inspired by [9] (see Section 3.", "startOffset": 202, "endOffset": 205}, {"referenceID": 6, "context": "The loss was minimized using ADAM learning rule [7] with a mini-batch size of 128.", "startOffset": 48, "endOffset": 51}, {"referenceID": 42, "context": "1 For example, if one replaces the first feature maps in the fifth residual unit of Pre-ResNet having 164 layers [45] by stochastic ones, then the corresponding DNN, Simplified-SFNN and SFNN took 1 mins 35 secs, 2 mins 52 secs and 16 mins 26 secs per each training epoch, respectively, on our machine with one Intel CPU (Core i7-5820K 6-Core@3.", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "Here, we trained both stochastic models using the biased estimator [9] with 10 random samples on CIFAR-10 dataset.", "startOffset": 67, "endOffset": 70}, {"referenceID": 14, "context": "More interestingly, Simplified-SFNN consistently outperforms its baseline DNN due to the stochastic regularizing effect, even when we train both models using dropout [16] and batch normalization [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "More interestingly, Simplified-SFNN consistently outperforms its baseline DNN due to the stochastic regularizing effect, even when we train both models using dropout [16] and batch normalization [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "3 Training Simplified-SFNN The parameters of Simplified-SFNN can be learned using a variant of the backpropagation algorithm [29] in a similar manner to DNN.", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "This random estimator is unbiased and has relatively low variance [8] since its accuracy does not depend on the dimensionality of h1 and one can draw samples from the exact distribution.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Next, in order to handle the issue in back pass, we use the following approximation inspired by [9]:", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 95, "endOffset": 98}, {"referenceID": 35, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 128, "endOffset": 132}, {"referenceID": 2, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 140, "endOffset": 143}, {"referenceID": 12, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 165, "endOffset": 169}, {"referenceID": 24, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "Similar to [9], we use 124 individuals with at least 10 facial expressions as data.", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "For this experiments, each pixel of every digit images is binarized using its grey-scale value similar to [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "We remark that both tasks are commonly performed in recent other works to test the multi-modal learning using SFNN [9, 34].", "startOffset": 115, "endOffset": 122}, {"referenceID": 32, "context": "We remark that both tasks are commonly performed in recent other works to test the multi-modal learning using SFNN [9, 34].", "startOffset": 115, "endOffset": 122}, {"referenceID": 3, "context": "The bicubic interpolation [4] is used for re-sizing all images as 32\u00d7 32 pixels.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "For both TFD and MNIST datasets, we use fully-connected DNNs as the baseline models similar to other works [9, 34].", "startOffset": 107, "endOffset": 114}, {"referenceID": 32, "context": "For both TFD and MNIST datasets, we use fully-connected DNNs as the baseline models similar to other works [9, 34].", "startOffset": 107, "endOffset": 114}, {"referenceID": 6, "context": "ADAM learning rule [7] with a mini-batch size of 128.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "For the CASIA dataset, we choose fully-convolutional network (FCN) models [2] as the baseline ones, which consists of convolutional layers followed by a fully-convolutional layer.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "The loss was minimized using ADAM learning rule [7] with a mini-batch size of 128.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "For these datasets, we design a convolutional version of Simplified-SFNN using convolutional neural networks such as Lenet-5 [6], NIN [13] and WRN [39].", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "For these datasets, we design a convolutional version of Simplified-SFNN using convolutional neural networks such as Lenet-5 [6], NIN [13] and WRN [39].", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "All models are trained using dropout [16] and batch normalization [17].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "All models are trained using dropout [16] and batch normalization [17].", "startOffset": 66, "endOffset": 70}], "year": 2017, "abstractText": "It has been believed that stochastic feedforward neural networks (SFNNs) have several advantages beyond deterministic deep neural networks (DNNs): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training large-scale SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN \u2192 Simplified-SFNN \u2192 SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CASIA, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, we train a stochastic model of 28 layers and 36 million parameters, where training such a largescale stochastic network is significantly challenging without using Simplified-SFNN.", "creator": "LaTeX with hyperref package"}}}