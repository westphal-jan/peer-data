{"id": "1704.03242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Automatic Keyword Extraction for Text Summarization: A Survey", "abstract": "In recent times, data is growing rapidly in every domain such as news, social media, banking, education, etc. Due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. Text summarization is emerged as an important research area in recent past. In this regard, review of existing work on text summarization process is useful for carrying out further research. In this paper, recent literature on automatic keyword extraction and text summarization are presented since text summarization process is highly depend on keyword extraction. This literature includes the discussion about different methodology used for keyword extraction and text summarization. It also discusses about different databases used for text summarization in several domains along with evaluation matrices. Finally, it discusses briefly about issues and research challenges faced by researchers along with future direction.", "histories": [["v1", "Tue, 11 Apr 2017 11:20:19 GMT  (1026kb)", "http://arxiv.org/abs/1704.03242v1", "12 pages, 4 figures"]], "COMMENTS": "12 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["santosh kumar bharti", "korra sathya babu"], "accepted": false, "id": "1704.03242"}, "pdf": {"name": "1704.03242.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["prof.ksb}@gmail.com,", "skjena@nitrkl.ac.in"], "sections": [{"heading": null, "text": "Due to the abundance of data, there is a need for an automatic summary that will be able to summarize the data, especially text data in the original document, without losing critical purposes. Text Summary has become an important area of research in the recent past. In this regard, reviewing existing work on the text summary process is useful for conducting further research. In this paper, current literature on automatic keyword extraction and text summary will be presented, as the process of text summary greatly depends on keyword extraction. This literature includes discussion of different methods of keyword extraction and text summary, as well as various databases used for text summary in several areas along with assessment matrices. Finally, topics and challenges that researchers face, along with future guidelines, will be discussed. Keywords: Abstractive Summary, Extractive Summary, Keyword Extraction, Natural Language Processing, Text Summary."}, {"heading": "1. INTRODUCTION", "text": "In the period from 1900 to 1900, ie from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, from 1900 to 1900, 1900 to 1900, from 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900, 1900 to 1900 to 1900, 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900 to 1900, 1900, 1900 to 1900, 1900 to 1900, 1900, 1900, 1900 to 1900, 1900 to 1900, 1900, 1900 to 1900, 1900, 1900, 1900 to 1900, 1900 to 1900, 1900, 1900, 1900, 1900 to 1900, 1900, 1900 to 1900, 1900, 1900, 1900, 19"}, {"heading": "2 AUTOMATIC KEYWORD EXTRACTION: A REVIEW", "text": "Based on previous work on automatic keyword extraction from the text for its summary, extraction systems can be divided into four classes: simple statistical approach, linguistic approach, machine learning approach, and hybrid approach [1] as soon as Figure 1 is available."}, {"heading": "2.1 Simple Statistical Approach", "text": "These strategies are crude, simplistic, and tend not to have training sets. They focus on statistics derived from non-linguistic characteristics of the document, such as the position of a word within the document, the term frequency, and the frequency of the reversed document. These findings are later used to compile a list of keywords. Cohen [15] used n-gram statistical data to automatically find the keyword within the document. Other techniques besides this class include word frequency, term frequency (TF) [16], or term frequency inverse document frequency (TF-IDF) [17], word coincidence [18], and PAT tree [19]. The most important of these is term frequency. In these strategies, the frequency of recurrence is the main criteria that determine whether a word is an akeyword or not. It is extremely unsophisticated and tends to produce statistical results."}, {"heading": "2.2 Linguistics Approach", "text": "This approach uses the linguistic features of words for keyword recognition and extraction in text documents. It includes lexical analysis [20], syntactical analysis [21], discourse analysis [22], etc. Resources used for lexical analysis are an electronic dictionary, tree tagging, WordNet, ngrams, POS patterns, etc. Noun phrases (NP), chunks (parsing) are also used as resources for syntactical analysis."}, {"heading": "2.3 Machine Learning Approach", "text": "Hidden Markov models [23], Support Vectormachine (SVM) [24], naive Bayes (NB) [25], Bagging [21], etc. are commonly used training models in these approaches. In the second phase, the document whose keywords are to be extracted is entered into the model, which then extracts the keywords that best fit the model's formation. One of the most famous algorithms in this approach is the Keyword Extraction Algorithm (KEA) [26]. In this approach, the article lines are first converted into a graph in which each word is treated as an anode, and whenever two words appear in the same sentence, the nodes are joined to an edge every time they appear together. Then, the number of edges connecting the vertices is converted into points and clustered accordingly."}, {"heading": "2.4 Hybrid Approach", "text": "These approaches combine the above two methods or usage euristics, such as position, length, layout characteristics of Table 1: Previous studies on automatic keyword extractionStudy Types of Approach Domain TypesT1 T2 T3 T4 D1 D2 D3 D4 D5 D6 D7Dennis et al. [29], 1967 \u221a Salton et al. [30], 1991 \u221a Cohen et al. [15], 1995 \u221a Chien et al. [19], 1997 \u221a Salton et al. [22], 1997 \u221a Ohsawa et al. [31], 1998 \u221a Hovy et al. [2], 1998 \u221a Fukumoto et al. [32], 1998 \u221a Mani et al. [3], 1999 \u221a Witten et al. [26], 1999 \u221a Frank et al. [25], 1999 \u221a Barzilay et al."}, {"heading": "3. TEXT SUMMARIZATION PROCESS: A REVIEW", "text": "Based on the literature, the text summary process can be divided into five types, based on the number of documents, based on summary usage, based on techniques, based on characteristics of the summary as text and based on levels of the linguistic process [1] as shown in Figure 2."}, {"heading": "3.1 Single Document Text Summarization", "text": "Thomas et al. [5] developed a system for automatic keyword extraction for the text summary in a single e-newspaper article. Marcu et al. [35] developed a discourse-based summary that determines the adequacy of the summary of texts for discourse-based methods in the area of individual news articles."}, {"heading": "3.2 Multiple Document Text Summarization", "text": "When summarizing multiple documents, numerous documents are used as input to perform summaries and deliver a single output document [14] [38] [39] [40] [41] [43] [44]. Mirroshandel et al. [44] presents two different algorithms for time-dependent keyword extraction and text summarization in multi-documents. The first algorithm was a weakly monitored machine learning approach to classifying temporal relationships between events and the second algorithm was expectation maximization (EM) based on an unattended learning approach to time relationship extraction. Min et al. [40] used the information common to document sets that belong to a common category to improve the quality of automatically extracted content in summaries of multiple documents."}, {"heading": "3.3 Query-based Text Summarization", "text": "In this summary technique, a specific part was used to extract the essential keyword from the input documentation to compile the summary of the corresponding document [11] [37] [48] [50] [51]. Fisher et al. [50] developed a query-based summary system that uses a log-linear model to classify each word in a sentence. It uses the property of absence ranking methods in which they take neural query ranking and query-focused ranking into account. Dong et al. [51] developed a query-based summary that uses document capture, time-sensitive queries and Recency Sensitive Queries as attributes for the summary of texts."}, {"heading": "3.4 Extractive Text Summarization", "text": "In this process, Summarizer discovers more critical information (either words or sentences) from input documents to compile the summary of the corresponding document [2] [5] [35] [53] [65] [75] [40]. In this process, it uses statistical and linguistic characteristics of the sentences to determine the most relevant sentences in the given input document. Thomas et al. [5] designed a hybrid model that uses machine learning and simple statistical methods to extract the text from e-newspaper articles. Min et al. [40] used freely available, open, extractive summarization system called SWING to group the text into a multi-page document. They used information common to document sets belonging to an ordinary category as a feature and summarized the concept of category-specific meaning (CSI). They demonstrated that CSI is a valuable metric tool for selecting sentences in the extraction structure."}, {"heading": "3.5 Abstractive Text Summarization", "text": "In this process, a machine must understand the idea of all the input documents and then provide a summary with their individual sentences [34] [37] [52] [56] [58]. It uses linguistic methods to examine and interpret the text and then find the new concepts and expressions to best describe it by generating a new shorter text that conveys the most important information from the original text document. Brandow et al. [56] developed an abstract summary system that analyzes the statistical corpus and extracts the signatures from the corpus. It then assigns weight to all the signatures. Using the extracted signatures, they assign weight to the sentences and select some weighted sentences as a summary. Daume et al. [37] developed an abstract summary system that maps all documents into database-like representations and divides them into four categories: a single person, a single event, several events, and natural disasters."}, {"heading": "3.6 Supervised Learning Based Text Summarization", "text": "Thomas et al. [5] designed a system for automatic keyword extraction for the text summary using the hidden Markov model. The learning process was monitored, human annotated keywords were used to train the model. Mirroshandel et al. [44] used a set of marked data sets to train the system for classifying the temporal relationships between events. Aramaki et al. [75] designated a learning-based extractive text collector that identifies the negative event and also investigates what kind of information is useful for identifying negative events. An SVM classifier is used to distinguish negative events from other events."}, {"heading": "3.7 Unsupervised Learning Based Text Summarization", "text": "There are no predefined guidelines for this technique at the time of training [13] [38] [39] [44] [65]. Mirroshandel etal. [44] proposed a method for time relation extraction based on the algorithm of expectation maximization (EM). Within the EM, they used various techniques such as a greedybest-first search and integer linear programming to eliminate time inconsistency. The EM-based approach was a completely unattended time relation extraction for text summarization. Alguliev et al. [39] developed a learning-based extractive summariser that optimizes three properties: relevance, redundancy, and length. It divided documents into sentences and selected outstanding sentences from the document."}, {"heading": "2. TEXT SUMMARIZATION APPROACH: A REVIEW", "text": "Based on the literature, approaches to summarizing texts can be divided into five types: statistical, machine learning, coherent, graph-based, algebraic, as shown in Figure 3."}, {"heading": "4.1 Statistical Based Approach", "text": "This approach is very simple and is often used to extract keywords from documents. No predefined data set is required for this approach. To extract keywords from documents, it uses several statistical features of the document such as term or word frequency (TF), term frequency-inverse document frequency (TF-IDF), keyword position (POK), etc. as shown in Figures 1 and 3. These statistical features are used to summarize texts [5] [90] [91] [92]."}, {"heading": "4.2 Machine learning Based Approach", "text": "Machine learning is a trait-based approach, for which we need an annotated data set to train the models. There are several popular machine learning approaches, namely Nave Bayes (NB) [93] [94] [95], Decision Trees (DTs) [96] [97], Hidden Markov Model (HMM) [98] [99] [100], Maximum Entropy (ME) [101] [102] [103] [104] [105], Neural Network (NN) [106] [107] [108], Support Vector Machine (SVM) [109] [110] [111], etc., which are used to summarize texts."}, {"heading": "4.3 Coherent Based Approach", "text": "Cohesion relations between text elements: reference, ellipse, substitution, conjunction and lexical cohesion [112]. Lexical chain (LC) [113], WordNet (WN) [8] [114], lexical chain evaluation of a word (LCS) [113], direct lexical chain evaluation of a word (DLCS) [113], lexical chain evaluation of a word (LCSS) [113], Rhetorical structure theory (RST) [115] [116] [9]."}, {"heading": "4.4 Graph Based Approach", "text": "There are two popular graph-based approaches to summarizing texts, namely Hyperlinked Induced Topic Search (HITS) [117] [118] and Google's PageRank (GPR) [117] [33] [119] [120]."}, {"heading": "4.5 Algebraic Approach", "text": "This approach uses algebraic theories, namely matrix, matrix transposing, self-vectors, etc. There are many algorithms used for text summarization using algebraic approaches such as Latent Semantic Analysis (LSA) [121] [122] [123], Meta Latent Semantic Analysis (MLSA) [124] [125], Symmetric Nonnegative Matrix Factorization (SNMF) [126], Semi-DiscreteDecomposition (SLSS) [126], Non-Negative Matrix Factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-DiscreteDecomposition (SDD) [129]."}, {"heading": "3. DATABASES: A REVIEW", "text": "In the literature we have observed, there are seven types of databases used for summarizing texts, namely, Document Understanding Workshop (DUC), MEDLINE, Text Analysis Conference (TAC), Computational Linguistics Scientific Document Summarization Sharpus (CL-SciSumm), TIPSTER Text Summarization Evaluation Conference (SUMMMAC), Topic Detection and Tracking (TDT).The DUC is the international conference for the evaluation of texts. This data collection consists of 50 topics and 25 documentation relevant to each topic from the AQUAINT Corpus."}, {"heading": "4. 6. PERFORMANCE EVALUATION MEASURE: A REVIEW", "text": "In the area of text summary, performance evaluation can be divided into two categories, intrinsic and extrinsic. Intrinsic evaluation assesses the quality of the summary directly on the basis of an analysis of a set of standards, whereas extrinsic evaluation assesses the quality of the summary on the basis of the way it affects the performance of another task. The full structure of the classification on the assessment benchmarks for the text summary is shown in Figure 4. To compare performance, ROUGE uses the ROUGE evaluation software package, which compares various summary results of several summary methods with humanly generated summaries. ROUGE has been applied by the Document Understanding Conference (DUC) for performance evaluation. ROUGE includes five automatic evaluation methods, ROUGEN, ROUGE-L, ROUGE-W, ROUGE-S and ROUGE-SU. Each method recalls the precision and summary of the system proposed between the ROUGE evaluation systems and the G3 reference system."}, {"heading": "7. ISSUES AND CHALLENGES OCCURS IN TEXT SUMMARIZATION", "text": "In the area of text summaries, there are the following research questions and challenges in implementation."}, {"heading": "7.1 Research Issues", "text": "In the case of summaries of multi-document texts, several problems often arise when assessing summaries, such as redundancy, temporal dimension, correlation or sentence order, etc., which makes it very difficult to achieve a high-quality summary. Some other problems arise, such as grammar, cohesion, coherence, which is detrimental to the summary; the quality of summaries varies from system to system, or from person to person; some people believe that some sentences are important for the summary, while others believe that the other sentences are important for the required summary."}, {"heading": "7.2 Implementation Challenges", "text": "To get a high-quality summary, high-quality keywords are required for the text summary. There is no standard for identifying high-quality keywords within or in multiple documents. The extracted keywords vary when applying different approaches to keyword extraction. Multilingual text summary is another challenging task."}, {"heading": "8. CONCLUSION AND FUTURE DIRECTION", "text": "Considerable work has been done in this area in the recent past. Due to the lack of information and standardization, large overlaps in research are a common phenomenon. Since 2012, no comprehensive reviews on automatic keyword extraction and text summary have been published, especially in the Indian context. Therefore, we thought that the survey paper covering recent work on keyword extraction and text summary might encourage the research community to close some important research gaps. This paper also includes the literature review of recent work on text summary from the perspective of automatic keyword extraction, text databases, summary processes, summarization methods and evaluation matrices. Some important research topics in the area of text summary will also be included in the paper.In the future, one can focus on the area of summary: text summary in languages with limited resources, especially in the Indian context such as this text summary, Hindi Tamil, multimedia, etc."}], "references": [{"title": "Automatic keyword extraction from documents using conditional random fields,", "author": ["C. Zhang"], "venue": "Journal of Computational Information Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Automated text summarization and the summarist system,", "author": ["E. Hovy", "C.-Y. Lin"], "venue": "in: Proceedings of a workshop on held at Baltimore, ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Lexrank: graph-based lexical centrality as salience in text summarization,", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Automatic keyword extraction for text summarization in e-newspapers,", "author": ["J.R. Thomas", "S.K. Bharti", "K.S. Babu"], "venue": "Proceedings of the International Conference on Informatics and Analytics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Giarlo, \u201cA comparative analysis of keyword extraction techniques,", "author": ["J. M"], "venue": "citeseer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Using lexical chains for keyword extraction, Information", "author": ["G. Ercan", "I. Cicekli"], "venue": "Processing & Management,\u201d vol", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Graph-based keyword extraction for singledocument summarization,", "author": ["M. Litvak", "M. Last"], "venue": "in: Proceedings of the workshop on Multisource Multilingual Information Extraction and Summarization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Event-based extractive summarization,", "author": ["E. Filatova", "V. Hatzivassiloglou"], "venue": "in: Proceedings of ACL Workshop on Summarization,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Classy query-based multi-document summarization,", "author": ["J.M. Conroy", "J.D. Schlesinger", "J.G. Stewart"], "venue": "in: Proceedings of the 2005 Document Understanding Workshop, Citeseer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Fastsum: fast and accurate query-based multi-document summarization,", "author": ["F. Schilder", "R. Kondadadi"], "venue": "in: Proceedings of the 46th annual meeting of the association for computational linguistics on human language technologies,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Fuzzy clustering for topic analysis and summarization of document collections,", "author": ["R. Witte", "S. Bergler"], "venue": "in: Advances in Artificial Intelligence, Springier,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Themecrowds: Multiresolution summaries of twitter usage,", "author": ["D. Archambault", "D. Greene", "P. Cunningham", "N. Hurley"], "venue": "in: Proceedings of the 3rd international workshop on Search and mining user-generated contents,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Highlights: Language- and domain-independent automatic indexing terms for abstracting,", "author": ["J.D. Cohen"], "venue": "JASIS, vol", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "A statistical approach to mechanized encoding and searching of literary information,", "author": ["H.P. Luhn"], "venue": "IBM Journal of research and development,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1957}, {"title": "Using tf-idf to determine word relevance in document queries,", "author": ["J. Ramos"], "venue": "in: Proceedings of the first instructional conference on machine learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Keyword extraction from a single document using word co-occurrence statistical information,", "author": ["Y. Matsuo", "M. Ishizuka"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Pat-tree-based keyword extraction for Chinese information retrieval,", "author": ["L.-F. Chien"], "venue": "in: ACM SIGIR Forum,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Using lexical chains for text summarization,", "author": ["R. Barzilay", "M. Elhadad"], "venue": "Advances in automatic text summarization,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Improved automatic keyword extraction given more linguistic knowledge,", "author": ["A. Hulth"], "venue": "in: Proceedings of the 2003 conference on Empirical methods in natural language processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Automatic text structuring and summarization,", "author": ["G. Salton", "A. Singhal", "M. Mitra", "C. Buckley"], "venue": "Information Processing & Management,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Text summarization via hidden Markov models,", "author": ["J.M. Conroy", "D.P. O'leary"], "venue": "in: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Keyword extraction using support vector machine,", "author": ["K. Zhang", "H. Xu", "J. Tang", "J. Li"], "venue": "in: Advances in Web-Age Information Management, Springier,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Domain-specific key-phrase extraction,", "author": ["E. Frank", "G.W. Paynter", "I.H. Witten", "C. Gutwin", "C.G. Nevill-Manning"], "venue": "In 16th International Joint Conference on Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Kea: Practical automatic key-phrase extraction,", "author": ["I.H. Witten", "G.W. Paynter", "E. Frank", "C. Gutwin", "C.G. Nevill-Manning"], "venue": "in: Proceedings of the fourth ACM conference on Digital libraries,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Learning to extract key-phrases from text,", "author": ["P. Turney"], "venue": "arXiv preprint cs/0212013", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "The design and testing of a fully automatic indexing searching system for documents consisting of expository text,", "author": ["S.F. Dennis"], "venue": "Information Retrieval: a Critical Review, Washington DC: Thompson Book Company,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1967}, {"title": "Automatic text structuring and retrieval experiments in automatic encyclopedia searching", "author": ["G. Salton", "C. Buckley"], "venue": "in: Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1991}, {"title": "Keygraph: Automatic indexing by co-occurrence graph based on building construction metaphor,", "author": ["Y. Ohsawa", "N.E. Benson", "M. Yachida"], "venue": "in: Research and Technology Advances in Digital Libraries,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "Keyword extraction of radio news using term weighting with an encyclopedia and newspaper articles,", "author": ["F. Fukumoto", "Y. Sekiguchi", "Y. Suzuki"], "venue": "in: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Graph-based ranking algorithms for sentence extraction, applied to text summarization,", "author": ["R. Mihalcea"], "venue": "In Proceedings of the Association for Computational Linguistics on Interactive poster and demonstration sessions. ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Automatic abstracting research at chemical abstracts service, Journal of Chemical Information and Computer Sciences", "author": ["J.J. Pollock", "A. Zamora"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1975}, {"title": "Discourse trees are good indicators of importance in text, Advances in automatic text summarization", "author": ["D. Marcu"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Generating single and multi-document summaries with gistexter", "author": ["S.M. Harabagiu", "F. Lacatusu"], "venue": "in: Document Understanding Conferences,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Robust generic and querybased summarization,", "author": ["H. Saggion", "K. Bontcheva", "H. Cunningham"], "venue": "in: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Exploiting category-specific information for multi-document summarization,", "author": ["Z.L. Min", "Y.K. Chew", "L. Tan"], "venue": "Proceedings of COLING,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Framework for abstractive summarization using text-to-text generation,", "author": ["P.-E. Genest", "G. Lapalme"], "venue": "in: Proceedings of the Workshop on Monolingual Text-To-Text Generation,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Mashechkin, \u201cUsing NMF-based text summarization to improve supervised and unsupervised classification,", "author": ["D. Tsarev", "I.M. Petrovskiy"], "venue": "in: Hybrid Intelligent Systems (HIS),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Uwn: A large multilingual lexical knowledge base,", "author": ["G. de Melo", "G. Weikum"], "venue": "in: Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Webinessence: A personalized web based multi-document summarization and recommendation system,", "author": ["D.R. Radev", "W. Fan", "Z. Zhang"], "venue": "Ann Arbor, vol", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2001}, {"title": "Multi-document summarization using generic relation extraction,", "author": ["B. Hachey"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Cats a topic-oriented multidocument summarization system at duc 2005,", "author": ["A. Farzindar", "F. Rozon", "G. Lapalme"], "venue": "in: Proc. of the Document Understanding Workshop,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2005}, {"title": "Towards recency ranking in web search,", "author": ["A. Dong", "Y. Chang", "Z. Zheng", "G. Mishne", "J. Bai", "R. Zhang", "K. Buchner", "C. Liao", "F. Diaz"], "venue": "in: Proceedings of the third ACM international conference on Web search and data mining,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Generating natural language summaries from multiple on-line sources,", "author": ["D.R. Radev", "K.R. McKeown"], "venue": "Computational Linguistics,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1998}, {"title": "A multilingual news summarizer,", "author": ["H.-H. Chen", "C.-J. Lin"], "venue": "in: Proceedings of the 18th conference on Computational linguistics,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2000}, {"title": "Experiments in single and multi-document summarization using mead,", "author": ["D.R. Radev", "S. Blair-Goldensohn", "Z. Zhang"], "venue": "Ann Arbor, vol", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2001}, {"title": "Newsinessence: A system for domain-independent, real-time news clustering and multi-document summarization,", "author": ["D.R. Radev", "S. Blair-Goldensohn", "Z. Zhang", "R.S. Raghavan"], "venue": "in: Proceedings of the first international conference on Human language technology research,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2001}, {"title": "Automatic condensation of electronic publications by sentence selection,", "author": ["R. Brandow", "K. Mitze", "L.F. Rau"], "venue": "Information Processing & Management, vol", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1995}, {"title": "Information fusion in the context of multi-document summarization,", "author": ["R. Barzilay", "K.R. McKeown", "M. Elhadad"], "venue": "in: Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1999}, {"title": "Automated multi-document summarization in neats,", "author": ["C.-Y. Lin", "E. Hovy"], "venue": "in: Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2002}, {"title": "Sigelman, \u201cTracking and summarizing news on a daily basis with columbia's newsblaster,", "author": ["K.R. McKeown", "R. Barzilay", "D. Evans", "V. Hatzivassiloglou", "J.L. Klavans", "A. Nenkova", "C. Sable", "S.B. Schi man"], "venue": "in: Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2002}, {"title": "Generating indicative-informative summaries with sumum,", "author": ["H. Saggion", "G. Lapalme"], "venue": "Computational linguistics,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2002}, {"title": "The university of Lethbridge text summarizer at duc 2003,", "author": ["Y. Chali", "M. Kolla", "N. Singh", "Z. Zhang"], "venue": "in: the Proceedings of the HLT/NAACL workshop on Automatic Summarization/Document Understanding Conference,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2003}, {"title": "Picking phrases, picking sentences,", "author": ["T. Copeck", "S. Szpakowicz"], "venue": "in: the Proceedings of the HLT/NAACL workshop on Automatic Summarization/ Document Understanding Conference,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2003}, {"title": "The university of Michigan at duc 2004,", "author": ["G. Erkan", "D.R. Radev"], "venue": "in: Proceedings of the Document Understanding Conferences Boston, MA,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "Crl/nyu summarization system at duc-2004,", "author": ["C. Nobata", "S. Sekine"], "venue": "in: Proceedings of DUC,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2004}, {"title": "Erss 2005: Co-reference-based summarization reloaded,", "author": ["R. Witte", "R. Krestel", "S. Bergler"], "venue": "in: DUC 2005 Document Understanding Workshop,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Context-based multi-document summarization using fuzzy co-reference cluster graphs,", "author": ["R.Witte", "R. Krestel", "S. Bergler"], "venue": "in: Proc. of Document Understanding Workshop at HLT-NAACL,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "Msbga: A multidocument summarization system based on genetic algorithm,", "author": ["Y.-X. He", "D.-X. Liu", "D.-H. Ji", "H. Yang", "C. Teng"], "venue": "in: 2006 International Conference on Machine Learning and Cybernetics,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2006}, {"title": "Qcs: A system for querying, clustering and summarizing documents,", "author": ["D.M. Dunlavy", "D.P.O. Leary", "J.M. Conroy", "J.D. Schlesinger"], "venue": "Information Processing & Management, vol", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2007}, {"title": "Enhancing single document summarization by combining ranknet and third-party sources,", "author": ["K.M. Svore", "L. Vanderwende", "C.J. Burges"], "venue": "in: EMNLP-CoNLL,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2007}, {"title": "Personalized pagerank based multidocument summarization,", "author": ["Y. Liu", "X. Wang", "J. Zhang", "H. Xu"], "venue": "in: International Workshop on Semantic Computing and Systems,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2008}, {"title": "Adasum: an adaptive model for summarization,", "author": ["J. Zhang", "X. Cheng", "G. Wu", "H. Xu"], "venue": "in: Proceedings of the 17th ACM conference on Information and knowledge management,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2008}, {"title": "Text2table: Medical text summarization system based on named entity recognition and modality identification,", "author": ["E. Aramaki", "Y. Miura", "M. Tonoike", "T. Ohkuma", "H. Mashuichi", "K. Ohe"], "venue": "in: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2009}, {"title": "Tiara: a visual exploratory text analytic system,", "author": ["F. Wei", "S. Liu", "Y. Song", "S. Pan", "M.X. Zhou", "W. Qian", "L. Shi", "L. Tan", "Q. Zhang"], "venue": "in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2010}, {"title": "An algorithm for pronominal anaphora resolution,", "author": ["S. Lappin", "H.J. Leass"], "venue": "Computational linguistics,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1994}, {"title": "Sarcastic sentiment detection in tweets streamed in real time: a big data approach,", "author": ["S. Bharti", "B. Vachha", "R. Pradhan", "K. Babu", "S. Jena"], "venue": "Digital Communications and Networks,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2016}, {"title": "On helmholtz's principle for documents processing,", "author": ["A. Balinsky", "H.Y. Balinsky", "S.J. Simske"], "venue": "in: Proceedings of the 10th ACM symposium on Document engineering,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2010}, {"title": "Automatic multi document summarization approaches,", "author": ["Y.J. Kumar", "N. Salim"], "venue": "In Journal of Computer Science,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2012}, {"title": "A survey on automatic text summarization. Literature Survey for the Language and Statistics II course", "author": ["D. Das", "A.F. Martins"], "venue": null, "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2007}, {"title": "A survey on automatic text summarization,", "author": ["C.S. Saranyamol", "L. Sindhu"], "venue": "Int. J. Computer. Sci. Inf. Technology,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2014}, {"title": "Automated Knowledge Provider System with Natural Language Query Processing,", "author": ["P. Mukherjee", "B. Chakraborty"], "venue": "IETE Technical Review, vol. 33(5),", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2016}, {"title": "Automatic multi-document summarization based on clustering and nonnegative matrix factorization,", "author": ["S. Park", "B. Cha", "D.U. An"], "venue": "IETE Technical Review,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2010}, {"title": "A literature survey on information extraction and text summarization,", "author": ["Zechner"], "venue": "Computational Linguistics Program,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 1997}, {"title": "Text summarisation in progress: a literature review,", "author": ["E. Lloret", "M. Palomar"], "venue": "Artificial Intelligence Review,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2012}, {"title": "The automatic creation of literature abstract,", "author": ["H.P. Luhn"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 1958}, {"title": "Statistical approach based keyword extraction aid dimensionality reduction,", "author": ["M.R. Murthy", "J.V.R. Reddy", "P.P. Reddy", "S.C. Satapathy"], "venue": "In Proceedings of the International Conference on Information Systems Design and Intelligent Applications. Springer,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2011}, {"title": "Naive (Bayes) at forty: The independence assumption in information retrieval,", "author": ["D.D. Lewis"], "venue": "In Proceedings of tenth European Conference on Machine Learning,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1998}, {"title": "Tackling the poor assumptions of nave Bayes classifiers,", "author": ["J.D.M. Rennie", "L. Shih", "J. Teevan", "D.R. Karger"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2003}, {"title": "Increasing the accuracy of discriminative of multinomial Bayesian classifier in text classification,", "author": ["T. Mouratis", "S. Kotsiantis"], "venue": "In Proceedings of fourth International Conference on Computer Sciences and Convergence Information Technology,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2009}, {"title": "Training a selection function for extraction,", "author": ["C.Y. Lin"], "venue": "In Proceedings of the eighth International Conference on Information and Knowledge Management,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1999}, {"title": "Statistics-based summarization- step one: Sentence compression,", "author": ["K. Knight", "D. Marcu"], "venue": "In Proceeding of the Seventeenth National Conference of the American Association for Artificial Intelligence,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2000}, {"title": "An introduction to hidden Markov model,", "author": ["L. Rabiner", "B. Juang"], "venue": "Acoustics Speech and Signal Processing Magazine, vol", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2003}, {"title": "Script recognition using hidden Markov models,", "author": ["R. Nag", "K.H. Wong", "F. Fallside"], "venue": "In Proceedings of International Conference on Acoustics Speech and Signal Processing,", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 1986}, {"title": "Research of information extraction algorithm based on hidden Markov model,", "author": ["C. Zhou", "S. Li"], "venue": "In Proceedings of second International Conference on Information Science and Engineering,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2010}, {"title": "Using maximum entropy for sentence extraction,", "author": ["M. Osborne"], "venue": "In Proceedings of the AssociaCL-02 Workshop on Automatic Summarization,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2002}, {"title": "Thumbs up? Sentiment classification using machine learning technique,", "author": ["B. Pang", "L. Lee", "S.Vaithyanathan"], "venue": "In Proceedings of the Association for Computational Linguistics conference on Empirical methods in Natural Language Processing,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2002}, {"title": "A maximum entropy approach information extraction from semi-structured and free text,", "author": ["H.L. Chieu", "H.T. Ng"], "venue": "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, American Association for Artificial Intelligence,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2002}, {"title": "A comparison of algorithms for maximum entropy parameter estimation,", "author": ["R. Malouf"], "venue": "In Proceedings of sixth conference on Natural Language Learning,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2002}, {"title": "A hidden state maximum entropy model forward confidence estimation,", "author": ["P. Yu", "J. Xu", "G.L. Zhang", "Y.C. Chang", "F. Seide"], "venue": "In International Conference on Acoustic, Speech and Signal Processing,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2007}, {"title": "Automatic text categorization using neural networks,", "author": ["M.E. Ruiz", "P. Srinivasan"], "venue": "In Proceedings of the Eighth American Society for Information Science/", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 1997}, {"title": "Ntc (neural network categorizer) neural network for text categorization,", "author": ["T. Jo"], "venue": "International Journal of Information Studies,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2010}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "mach. learn. Machine Learning,", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2002}, {"title": "Ntt\u2019s text summarization system for duc-2002,", "author": ["T. Hirao", "Y. Sasaki", "H. Isozaki", "E. Maeda"], "venue": "In Proceedings of the Document Understanding Conference,", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2002}, {"title": "Sentence extraction with support vector machine ensemble,", "author": ["L.N. Minh", "A. Shimazu", "H.P. Xuan", "B.H. Tu", "S. Horiguchi"], "venue": "In Proceedings of the First World Congress of the International Federation for Systems Research,", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2005}, {"title": "Lexical cohesion computed by thesaural relations as an indicator of the structure of text,", "author": ["J. Morris", "G. Hirst"], "venue": "Journal of Computational Linguistics,", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 1999}, {"title": "WordNet::similarity: Measuring the relatedness of concepts,", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi"], "venue": "In Proceedings in Human Language Technology Conference,", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2004}, {"title": "Efficient text summarization using lexical chains,", "author": ["H.G. Silber", "K.F. McCoy"], "venue": "In Proceedings of Fifth International Conference on Intelligent User Interfaces,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2000}, {"title": "Rhetorical structure theory: Towards a function theory of text organization,", "author": ["W.C. Mann", "S.A. Thompson"], "venue": "Text - Interdisciplinary Journal for the Study of Discourse,", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 1988}, {"title": "Evaluation of automatic summarization methods based on rhetorical structure theory,", "author": ["V.R. Uzeda", "T. Pard", "M. Nunes"], "venue": "In Eighth International Conference on Intelligent Systems Design and Applications,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2008}, {"title": "Automatic text summarization based on rhetorical structure theory,", "author": ["L. Chengcheng"], "venue": "In International Conference on Computer Application and System Modeling,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2010}, {"title": "The anatomy of a large-scale hyper textual web search engine,", "author": ["S. Brin", "L. Page"], "venue": "In Proceedings of the Seventh International World Wide Web Conference, Computer Networks and ISDN Systems,", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 1988}, {"title": "Anatomy of a large-scale social search engine,", "author": ["D. Horowitz", "S.D. Kamvar"], "venue": "In Nineteenth International Conference on World Wide Web,", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 2010}, {"title": "Information search and retrieval in microblogs,", "author": ["M. Efron"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 2011}, {"title": "Introduction to latent semantic analysis,", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Journal of Discourse Processes, vol", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 1998}, {"title": "Latent semantic indexing: An overview,", "author": ["B. Rosario"], "venue": "Technical Report of INFOSYS 240, University of California,", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2000}, {"title": "Unsupervised learning by probabilistic latent semantic analysis,", "author": ["T. Hofmann"], "venue": "Journal of Machine Learning,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 2001}, {"title": "Meta latent semantic analysis", "author": ["M. Simina", "C. Barbu"], "venue": "In IEEE International conference on Systems, Man and Cybernetics,", "citeRegEx": "124", "shortCiteRegEx": "124", "year": 2004}, {"title": "Adaptive Bayesian latent semantic analysis", "author": ["J.T. Chien"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 2008}, {"title": "Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization,", "author": ["D. Wang", "T. Li", "S. Zhu", "C. Ding"], "venue": "In Proceedings of the 31 Annual International ACM Special Interest Group on Information Retrieval Conference on Research and Development in Information Retrieval,", "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2008}, {"title": "Automatic generic document summarization based on non-negative matrix factorization", "author": ["J.H. Lee", "S. Park", "C.M. Ahn", "D. Kim"], "venue": "Journal on Information Processing and Management,", "citeRegEx": "127", "shortCiteRegEx": "127", "year": 2009}, {"title": "Using semi-discrete decomposition for topic identification,", "author": ["V. Snasel", "P. Moravec", "J. Pokorny"], "venue": "In Proceedings of the Eighth International Conference on Intelligent Systems Design and Applications,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 2008}, {"title": "Summarization from medical documents: a survey,", "author": ["S. Afantenos", "V. Karkaletsis", "P. Stamatopoulos"], "venue": "Artificial intelligence in medicine,", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 2005}, {"title": "Technology Rourkela, India. His research interest includes opinion mining and sarcasm sentiment detection resume. Email-id: sbharti1984@gmail.com Korra Sathya Babu is working as an Assistant Professor in the Dept. of CSE, National Institute of Technology Rourkela India. His research interest includes Data engineering, Data privacy, Opinion mining and Sarcasm sentiment detection. Email-id: prof.ksb@gmail.com Sanjay Kumar Jena is working as a Professor in the Dept", "author": [], "venue": null, "citeRegEx": "132", "shortCiteRegEx": "132", "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "intervention depending on the model [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Summarization is a process where the most salient features of a text are extracted and compiled into a short abstract of the original document [2].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "Summaries are usually around 17\\% of the original text and yet contain everything that could have been learned from reading the original article [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "On the premise of past work done towards automatic keyword extraction from the text for its summarization, extraction systems can be classified into four classes, namely, simple statistical approach, linguistics approach, machine learning approach, and hybrid approaches [1] as soon in Figure 1.", "startOffset": 271, "endOffset": 274}, {"referenceID": 12, "context": "Cohen [15], utilized n-gram statistical data to discover the keyword inside the document automatically.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Other techniques in- side this class incorporate word frequency, term frequency (TF) [16] or term frequency-", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "inverse document frequency (TF-IDF) [17], word cooccurrences [18], and PAT-tree [19].", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "inverse document frequency (TF-IDF) [17], word cooccurrences [18], and PAT-tree [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "inverse document frequency (TF-IDF) [17], word cooccurrences [18], and PAT-tree [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "It incorporates the lexical analysis [20], syntactic analysis [21], discourse analysis [22], etc.", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "It incorporates the lexical analysis [20], syntactic analysis [21], discourse analysis [22], etc.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "It incorporates the lexical analysis [20], syntactic analysis [21], discourse analysis [22], etc.", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 92, "endOffset": 96}, {"referenceID": 23, "context": "One of the most famous algorithms in this approach is the keyword extraction algorithm (KEA) [26].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "GenEx [27] is another tool in this approach.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "[29], 1967 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[30], 1991 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15], 1995 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19], 1997 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22], 1997 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31], 1998 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], 1998 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[32], 1998 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26], 1999 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25], 1999 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20], 1999 \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27], 1999 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23], 2001 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21], 2003 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17], 2003 \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18], 2004 \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[4], 2004 \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[33], 2004 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24], 2006 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[8], 2007 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9], 2008 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], 2008 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5], 2016 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Based on the literature, text summarization process can be characterized into five types, namely, based on the number of the document, based on summary usage, based on techniques, based on characteristics of summary as text and based on levels of linguistics process [1] as shown in Figure 2.", "startOffset": 267, "endOffset": 270}, {"referenceID": 3, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 140, "endOffset": 143}, {"referenceID": 30, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "[5] designed a system for automatic keyword extraction for text summarization in single document e-Newspaper article.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[35] developed a discourse-based summarizer that determines adequacy for summarizing texts for discoursebased methods in the domain of single news articles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "[40] used the information which is common to document sets belonging to a common category to improve the quality of automatically extracted content in multi-document summaries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 165, "endOffset": 169}, {"referenceID": 33, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 169, "endOffset": 173}, {"referenceID": 40, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 173, "endOffset": 177}, {"referenceID": 41, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 185, "endOffset": 189}, {"referenceID": 41, "context": "[51] developed a query-based summarization that uses document ranking, time-sensitive queries and ranks recency sensitive queries as the features for text summarization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 167, "endOffset": 170}, {"referenceID": 42, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 170, "endOffset": 174}, {"referenceID": 31, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 174, "endOffset": 178}, {"referenceID": 43, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 178, "endOffset": 182}, {"referenceID": 44, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 182, "endOffset": 186}, {"referenceID": 45, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 186, "endOffset": 190}, {"referenceID": 53, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 190, "endOffset": 194}, {"referenceID": 62, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 194, "endOffset": 198}, {"referenceID": 34, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 202, "endOffset": 206}, {"referenceID": 3, "context": "[5] designed a hybrid model based extractive summarizer using machine learning and simple statistical method for keyword extraction from e-Newspaper article.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[40] used freely available, open-source extractive summarization system, called SWING to summarize the text in multi-document.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[35] developed a discoursebased extractive summarizer that uses the rhetorical parsing algorithm to determine discourse structure of the text of given input, determine partial ordering on the elementary and parenthetical units of the text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[65] developed an extractive summarization environment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[75] destined a supervised learning based extractive text summarizer that identifies the negative event and it also investigates what kind of information is helpful for negative event identification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 144, "endOffset": 148}, {"referenceID": 42, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 148, "endOffset": 152}, {"referenceID": 46, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 152, "endOffset": 156}, {"referenceID": 47, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 156, "endOffset": 160}, {"referenceID": 46, "context": "[56] developed an abstractive summarization system that analyses the statistical corpus and extracts the signature words from the corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37] developed an abstractive summarization system that maps all the documents into database-like representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 78, "endOffset": 82}, {"referenceID": 60, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 90, "endOffset": 94}, {"referenceID": 62, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "[5] designed a system for automatic keyword extraction for text summarization using hidden Markov model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 62, "context": "[75] destined a supervised learning based extractive text summarizer that identifies the negative event and also investigates what kind of information is helpful for negative event identification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In this technique, there are no predefined guidelines available at the time of training [13][38][39][44][65].", "startOffset": 88, "endOffset": 92}, {"referenceID": 53, "context": "In this technique, there are no predefined guidelines available at the time of training [13][38][39][44][65].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "These statistical features are used for text summarization [5][90][91][92].", "startOffset": 59, "endOffset": 62}, {"referenceID": 74, "context": "These statistical features are used for text summarization [5][90][91][92].", "startOffset": 66, "endOffset": 70}, {"referenceID": 75, "context": "These statistical features are used for text summarization [5][90][91][92].", "startOffset": 70, "endOffset": 74}, {"referenceID": 76, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 77, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 78, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 86, "endOffset": 90}, {"referenceID": 79, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 113, "endOffset": 117}, {"referenceID": 80, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 81, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 149, "endOffset": 153}, {"referenceID": 82, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 153, "endOffset": 157}, {"referenceID": 83, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 157, "endOffset": 162}, {"referenceID": 84, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 185, "endOffset": 190}, {"referenceID": 85, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 190, "endOffset": 195}, {"referenceID": 86, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 195, "endOffset": 200}, {"referenceID": 87, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 200, "endOffset": 205}, {"referenceID": 88, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 205, "endOffset": 210}, {"referenceID": 89, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 232, "endOffset": 237}, {"referenceID": 90, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 237, "endOffset": 242}, {"referenceID": 91, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 278, "endOffset": 283}, {"referenceID": 92, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 283, "endOffset": 288}, {"referenceID": 93, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 288, "endOffset": 293}, {"referenceID": 94, "context": "Cohesion relations among elements in a text: reference, ellipsis, substitution, conjunction, and lexical cohesion [112].", "startOffset": 114, "endOffset": 119}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 19, "endOffset": 24}, {"referenceID": 5, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 39, "endOffset": 42}, {"referenceID": 96, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 42, "endOffset": 47}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 85, "endOffset": 90}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 136, "endOffset": 141}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 185, "endOffset": 190}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 97, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 283, "endOffset": 288}, {"referenceID": 98, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 288, "endOffset": 293}, {"referenceID": 6, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 293, "endOffset": 296}, {"referenceID": 99, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 121, "endOffset": 126}, {"referenceID": 100, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 126, "endOffset": 131}, {"referenceID": 99, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 160, "endOffset": 165}, {"referenceID": 29, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 165, "endOffset": 169}, {"referenceID": 101, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 169, "endOffset": 174}, {"referenceID": 102, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 174, "endOffset": 179}, {"referenceID": 103, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 118, "endOffset": 123}, {"referenceID": 104, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 123, "endOffset": 128}, {"referenceID": 105, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 128, "endOffset": 133}, {"referenceID": 106, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 172, "endOffset": 177}, {"referenceID": 107, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 177, "endOffset": 182}, {"referenceID": 108, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 234, "endOffset": 239}, {"referenceID": 108, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 281, "endOffset": 286}, {"referenceID": 109, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 328, "endOffset": 333}, {"referenceID": 110, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 411, "endOffset": 416}, {"referenceID": 18, "context": "relevant to each topic from the AQUAINT corpus for queryrelevant multi-document summarization [21].", "startOffset": 94, "endOffset": 98}, {"referenceID": 111, "context": "org/ with journal identifiers [130] and it contains abstracts from more than 3500 journals [131].", "startOffset": 91, "endOffset": 96}, {"referenceID": 112, "context": "The three output summaries comprise: the traditional self-summary of the paper (the abstract), the community summary (the collection of citation sentences \u2018citances\u2019) and a human summary written by a trained annotator [132].", "startOffset": 218, "endOffset": 223}, {"referenceID": 21, "context": "ROUGE includes five automatic evaluation methods, ROUGEN, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "[34], 1975 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[56], 1995 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], 1998 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 42, "context": "[52], 1998 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[35], 1999 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[57], 1999 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[53], 2000 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[54], 2001a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[55], 2001b \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[46], 2001c \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[59], 2002 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[60], 2002 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[36], 2002 \u221a \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[61], 2002 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37], 2003 \u221a \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[62], 2003 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[63], 2003 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[65], 2004 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[10], 2004 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[66], 2004 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11], 2005 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[48], 2005 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[67], 2005 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[68], 2006 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[69], 2006 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13], 2007 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "[70], 2007 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[72], 2007 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12], 2008 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[73], 2008 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[74], 2008 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[75], 2009 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[47], 2009 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[76], 2010 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[51], 2010 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 71, "context": "[87], 2010 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14], 2011 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[41], 2011 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[42], 2011 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[40], 2012 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[43], 2012 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5], 2012 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "In recent times, data is growing rapidly in every domain such as news, social media, banking, education, etc. Due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. Text summarization is emerged as an important research area in recent past. In this regard, review of existing work on text summarization process is useful for carrying out further research. In this paper, recent literature on automatic keyword extraction and text summarization are presented since text summarization process is highly depend on keyword extraction. This literature includes the discussion about different methodology used for keyword extraction and text summarization. It also discusses about different databases used for text summarization in several domains along with evaluation matrices. Finally, it discusses briefly about issues and research challenges faced by researchers along with future direction.", "creator": "Microsoft\u00ae Word 2013"}}}