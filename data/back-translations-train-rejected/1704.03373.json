{"id": "1704.03373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Quality Aware Network for Set to Set Recognition", "abstract": "This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at", "histories": [["v1", "Tue, 11 Apr 2017 15:47:41 GMT  (903kb,D)", "http://arxiv.org/abs/1704.03373v1", "Accepted at CVPR 2017"]], "COMMENTS": "Accepted at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yu liu", "junjie yan", "wanli ouyang"], "accepted": false, "id": "1704.03373"}, "pdf": {"name": "1704.03373.pdf", "metadata": {"source": "CRF", "title": "Quality Aware Network for Set to Set Recognition", "authors": ["Yu Liu", "Junjie Yan", "Wanli Ouyang"], "emails": ["liuyuisanai@gmail.com", "yanjunjie@sensetime.com", "wanli.ouyang@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Related work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3. Quality aware network (QAN)", "text": "In our work, we focus on improving the embedding model of images, which maps an image set S = {I1, I2, \u00b7 \u00b7, IN} to a fixed-dimensional representation, so that image sets with different numbers of images are comparable to each other. Let Ra (S) and RIi represent S and Ii. Ra (S) is determined by all elements in S, so it can be called Ra (S) = F (RI1, RI2, \u00b7 \u00b7 \u00b7, RIN). (1) The challenge is to find an optimized F (\u00b7) that aggregates features from the entire image set to obtain the most discriminatory representation."}, {"heading": "3.1. QAN for image set embedding", "text": "In this essay, feature generation and the aggregation module are implemented by a complete Convolutionary Neural Network called QAN, as in Fig. 2. Two branches are split from the center of the module. In the first branch, a part for quality generation is followed by a composite pooling unit, the aggregation module. And in the second branch, the feature generation part generates the representation of images. Now, we present how an image set flows through QAN. At the beginning of the process, all images are sent into a fully revolutionary network to generate middle representations. Afterwards, QAN is divided into two branches. The first (upper) part for quality generation is a tiny neural network (see Section 3.4 for details), which is used to predict quality results."}, {"heading": "3.2. Training QAN without quality supervision", "text": "The data flow is represented in Figure 2. QAN is designed to generate discriminatory representations for images and sentences belonging to different identities. In image-level image training, after the part of the feature generation, a complete connecting layer is formed, supervised by Softmax loss Lclass. In sentence-level training, the representation of a sentence Ra (S) is automatically supervised by Lveri, which is formulated as follows: Lveri = \"Ra\" (Sa) \u2212 Ra (Sp) - \"Ra\" (Sa) \u2212 Ra (Sn) - \"Ra\" (Sn) - \"Ra\" (Sn) - \"Ra\" (Sa) - \"Ra.\" We define Sa as an anchor, Sp as a positive set and Sn as a negative set. This function minimizes deviations from intra-class samples (RIJ) - \"S,\" while Softmax loss \"RagD\" - \"Raglos\" is defined in previous work as \"We define a positive set,\" Sa \"Sa\" and \"Sp\" as samples."}, {"heading": "3.3. Mechanism for learning quality score", "text": "The automatic gradient of \u00b5. After the repropagation by a specified pooling unit, the gradient of \u00b5i is calculated with respect to the sample Lverican according to Eq. 8, which is the point product of the gradient of Ra (S) and RIi. Thus, if the angle of Ra (S) and RIi (Sneg) belongs to (\u2212 90, 90), the gradient of \u00b5i will be positive after this repropagation process. In contrast, the relative direction of xai is in the opposite side of the gradient of Ra (Sanchor), which obviously makes it a hard sample, so that the quality of \u00b5ai tends to be smaller. Obviously, samples in the \"right\" direction together with the specified gradient of \u00b5i will always be of higher quality, while the parts of RIa (S) and Ra (Sneg) represented in the \"wrong\" direction \"are a hard sample, so that the quality of \u00b5ai in the lower sample will be."}, {"heading": "3.4. Details of quality generation part", "text": "Quality-conscious network (QAN) is a neural folding network. We design different score generation parts that start with different character cards. As an example, we use QAN split for pool 4. As shown in Fig. 4, the spatial output of the Pool4 layer is 512 x 14 x 14. To generate a quality score of 1 x 1, the folding part contains a two-stage pool layer and a final pool layer with grain size 7 x 7. A fully bonded layer is followed by the final pool layer to generate the original quality score. Afterwards, the original values of all images are sent in a set of sigmoid layers and the Group L1 normalization layer to generate the final results \u00b5. In the QAN splitting at pool 3, we add a block that contains three single-stage folding layer and a two-stage pooling layer at the beginning of quality generation."}, {"heading": "4. Experiments", "text": "In this section, we will first examine the importance of the quality value learned from QAN. Then, we will analyze QAN's sensitivity to the feature level. Based on the above knowledge, we will evaluate QAN against two human recognition benchmarks and two unrestricted facial inspection benchmarks. Finally, we will analyze the concept learned from QAN and compare it to a human-determined value."}, {"heading": "4.1. What is learned in QAN?", "text": "Qualitative Analysis We visualize images with their \u00b5 generated by QAN to explore the meaning of \u00b5. Examples of the same person with different qualities are shown in the first two rows of Fig. 5. All images are selected from the test set. The two images in the same column belong to the same person. The upper images are randomly selected from images with quality values higher than 0.8, and the lower images are selected from images whose quality values are lower than the corresponding higher ones. It is easy to see that images with inconsistencies, overlays, blurs or extreme lighting conditions tend to have lower quality values than normal images. The last two lines in Fig. 5 give some examples of other images randomly selected from the test set. They are sorted from left to right by their quality values. We can observe that instances with quality values greater than 0.70 are easy to recognize for humans, while the others are hard. An especially large number of hard images contain two or more bodies in the middle, and we can hardly distinguish which is the right image."}, {"heading": "4.2. Person re-identification", "text": "The experiments are carried out on PRID2011 [9] and iLiDS-VID [33] datasets. PRID2011 contains frames in two views taken at different positions on a street. CameraA has 385 identities, while CameraB has 749 identities, and the two videos have an overlap of 200 persons. Results are reported in terms of cumulative matching characteristics (CMC) table, and the average number is 100. iLIDS-VID dataset has 300 persons, and each person has two sets, which were also captured from different positions. Each person has 23 to 192 image evaluations. Results are reported in terms of cumulative matching characteristics (CMC) table, with each column representing the matching rate in a particular TopN matching."}, {"heading": "4.2.1 Evaluation on common setting", "text": "The results of the evaluation, which follow the \"10-fold cross-validation\" on PRID2011 and iLIDS-VID, are in Table 1 and Table 2. Note that our CNN + AvePool and CNN + Min (cos) baselines are close to or even better than the state of the art when considering the extensive training dataset. Note that most of the leading methods listed in the table take into account both appearance information and spatial and temporal information, while our method only takes into account appearance information. In the PRID2011 dataset, QAN increases the top-1 matching rate by 11.1% and 29.4%, respectively, compared to CNN + AvePool and CNN + Min (cos). In the iLIDS VID dataset, the inherent noise is much stronger than in PRID2011, which significantly affects the accuracy of CNN + Min (cos), as the operator is \"Min (cos)\" more sensitive than \"AvePool and CNN + Min (cos).\""}, {"heading": "4.2.2 Dataset cross evaluation", "text": "In order to prevent our model from exceeding the quality distribution of the test set, we perform a cross-evaluation of the data sets. We extract the set representation of iLIDS-VID and PRID2011 directly with trained QAN without fine-tuning. Afterwards, the QAN representation is evaluated for CMC values. Tables 3 and 4 show the results of the QAN and the two baselines. It can be stated that the QAN is also robust in the cross-dataset setting. It improves the top-1 matching by 15.6% and 8.2% compared to baselines. This result shows that the quality distribution learned by QAN from different data sets can be generalized to other data sets."}, {"heading": "4.3. Unconstrained face verification", "text": "The model is evaluated on YouTube Face Dataset [35] and IARPA Janus Benchmark A (IJB-A) Dataset. YouTube Face contains 3425 videos of 1595 identities. It is difficult because most faces are blurred or have a low resolution. IJB-A Dataset contains 2042 videos of 500 people. Faces in IJB-A have a large variance. Evaluation procedures. We follow the 1: 1 protocol in both benchmarks and evaluate the results based on the characteristic curves of the receiver (ROC). Below the curve (AUC) and accuracy are two important indicators of ROC."}, {"heading": "4.3.1 Results on YouTube Face and IJB-A benchmark", "text": "10 \u2212 310 \u2212 210 \u2212 110 000.10.30.40.50.60.70.80.91False Positive RateT rue Posi tive Rat eQAN _ pool2 Baseline (AvePool) Baseline (MinCos) DeepFace proprietary PEP DDML (combine) Figure 7. Average ROC curves of different methods on YouTube Face Dataset 10 \u2212 3 10 \u2212 2 10 \u2212 1 1000.80.850.90.951False Positive RateT rue Posi tive Rat eBaseline (MinCos) Baseline (AvePool) QAN @ FC & Fix QAN @ Input QAN @ Pool1 QAN @ Pool3 QAN @ Pool4Figure 8. ROC results for score generation part learned by different level of feature.On YouTube Face dataset, it perFace at Fig. 7 and Table 5 that the accuracy and AUC QAN SQAN-SQN-SQN-SQN-01-01 is similar to the QAN-QN-01."}, {"heading": "4.4. Quality by QAN VS. quality by human", "text": "There are no explicit monitoring signals for the cascade score generation unit in the training. Therefore, another problem arises: Is it better to use human-defined scores instead of letting the network learn for itself? In the YouTube Face experiment, we replace the Q (I) quality score with a voluntary score and get the following result in Fig. 9, which is better than the two baseline, but worse than the result of the original QAN. It shows that Q is similar to human thoughts, but more suitable for detection."}, {"heading": "4.5. Diagnosis experiments", "text": "We use YouTube Face to analyze this factor by comparing different configurations.In the first configuration, the part of the weight generation is associated with the image. In the second to fifth configuration, the part of the weight generation is determined after four pooling layers in each block. In the sixth configuration, we connect the part of the weight generation to a fully connected layer. For the final configuration, we fix all parameters before the final fully connected layer in the sixth configuration and update only parameters of the part of the weight generation that is taken as the seventh structure.To minimize the impact of the number of parameters, the total size of different models is limited to the same thing by changing the channel number.The results are shown in Figure 8. It can be noted that the performance of the QAN improves at the outset and reaches the highest accuracy in Pool3."}, {"heading": "5. Conclusion and future work", "text": "In this article, we propose a Quality Aware Network (QAN) for set-to-set recognition, which automatically learns the concept of quality for each sample in a set without a monitored signal and aggregates the most discriminatory samples to produce a set representation. We show theoretically and experimentally that the quality predicted by the network is beneficial for set representation and better than humans. QAN can be considered an attention model that pays attention to high quality elements in an image. However, a low quality image can still have some discriminatory regions. In view of this, in our future work we will explore a fine-grained quality conscious network that pays attention to high quality regions instead of high quality images in an image."}], "references": [{"title": "Approximate nearest subspace search", "author": ["Ronen Basri", "Tal Hassner", "Lihi Zelnik-Manor"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Face recognition based on image sets", "author": ["Hakan Cevikalp", "Bill Triggs"], "venue": "In CVPR\u201910,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Supervised transformer network for efficient face detection", "author": ["Dong Chen", "Gang Hua", "Fang Wen", "Jian Sun"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "An end-to-end system for unconstrained face verification with deep convolutional neural networks", "author": ["Jun-Cheng Chen", "Rajeev Ranjan", "Amit Kumar", "Ching-Hui Chen", "Vishal Patel", "Rama Chellappa"], "venue": "In ICCV Workshops,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Person re-identification by symmetrydriven accumulation of local features", "author": ["Michela Farenzena", "Loris Bazzani", "Alessandro Perina", "Vittorio Murino", "Marco Cristani"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Mdlface: Memorability augmented deep learning for video face recognition", "author": ["Gaurav Goswami", "Romil Bhardwaj", "Richa Singh", "Mayank Vatsa"], "venue": "In Biometrics (IJCB),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Pooling faces: template based face recognition with pooled face images", "author": ["Tal Hassner", "Iacopo Masi", "Jungyeon Kim", "Jongmoo Choi", "Shai Harel", "Prem Natarajan", "Gerard Medioni"], "venue": "In CVPR\u201916 Workshops,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Person re-identification by descriptive and discriminative classification", "author": ["Martin Hirzer", "Csaba Beleznai", "Peter M. Roth", "Horst Bischof"], "venue": "In Proc. Scandinavian Conference on Image Analysis (SCIA),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Sparse approximated nearest points for image set classification", "author": ["Yiqun Hu", "Ajmal S Mian", "Robyn Owens"], "venue": "In CVPR\u201911,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Learning hierarchical representations for face verification with convolutional deep belief networks", "author": ["Gary B. Huang"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Gary B Huang", "Manu Ramesh", "Tamara Berg", "Erik Learned- Miller"], "venue": "Technical report, Technical Report 07-49,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Projection metric learning on grassmann manifold with application to video based face recognition", "author": ["Zhiwu Huang", "Ruiping Wang", "Shiguang Shan", "Xilin Chen"], "venue": "In CVPR\u201915,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid", "Patrick P\u00e9rez"], "venue": "In CVPR\u201910,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Open source biometric recognition", "author": ["Joshua C Klontz", "Brendan F Klare", "Scott Klum", "Anubhav K Jain", "Mark J Burge"], "venue": "In Biometrics: Theory, Applications and Systems (BTAS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Large scale metric learning from equivalence constraints", "author": ["Martin Koestinger", "Martin Hirzer", "Paul Wohlhart", "Peter M Roth", "Horst Bischof"], "venue": "In CVPR\u201912,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["Svetlana Lazebnik", "Cordelia Schmid", "Jean Ponce"], "venue": "In CVPR\u201906,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Eigen-pep for video face recognition", "author": ["Haoxiang Li", "Gang Hua", "Xiaohui Shen", "Zhe Lin", "Jonathan Brandt"], "venue": "ACCV", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Top rank optimization in linear time", "author": ["Nan Li", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Deepreid: Deep filter pairing neural network for person re-identification", "author": ["Wei Li", "Rui Zhao", "Tong Xiao", "Xiaogang Wang"], "venue": "In ICCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning locally-adaptive decision functions for person verification", "author": ["Zhen Li", "Shiyu Chang", "Feng Liang", "Thomas Huang", "Liangliang Cao", "John Smith"], "venue": "In CVPR\u201913,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Multi-manifold deep metric learning for image set classification", "author": ["Jiwen Lu", "Gang Wang", "Weihong Deng", "Pierre Moulin", "Jie Zhou"], "venue": "In CVPR\u201915,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Local fisher discriminant analysis for pedestrian reidentification", "author": ["Sateesh Pedagadi", "James Orwell", "Sergio Velastin", "Boghos Boghossian"], "venue": "In ICCV\u201913,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Yi Sun", "Yuheng Chen", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In NIPS, pages 1988\u20131996,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Deeply learned face representations are sparse, selective, and robust", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In ICCV, pages 1701\u20131708,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Face search at scale: 80 million gallery", "author": ["Dayong Wang", "Charles Otto", "Anil K Jain"], "venue": "arXiv preprint arXiv:1507.07242,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Covariance discriminative learning: A natural and efficient approach to image set classification", "author": ["Ruiping Wang", "Huimin Guo", "Larry S Davis", "Qionghai Dai"], "venue": "In CVPR\u201912,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Person re-identification by video ranking", "author": ["Taiqing Wang", "Shaogang Gong", "Xiatian Zhu", "Shengjin Wang"], "venue": "ECCV", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Person re-identification by discriminative selection in video", "author": ["Taiqing Wang", "Shaogang Gong", "Xiatian Zhu", "Shengjin Wang"], "venue": "ranking. 2016", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Face recognition in unconstrained videos with matched background similarity", "author": ["Lior Wolf", "Tal Hassner", "Itay Maoz"], "venue": "In CVPR\u201911,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Deep recurrent convolutional networks for video-based person re-identification: An end-to-end approach", "author": ["Lin Wu", "Chunhua Shen", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1606.01609,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Learning deep feature representations with domain guided dropout for person re-identification", "author": ["Tong Xiao", "Hongsheng Li", "Wanli Ouyang", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1604.07528,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Neural aggregation network for video face recognition", "author": ["Jiaolong Yang", "Peiran Ren", "Dong Chen", "Fang Wen", "Hongdong Li", "Gang Hua"], "venue": "arXiv preprint arXiv:1603.05474,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Face recognition based on regularized nearest points between image sets", "author": ["Meng Yang", "Pengfei Zhu", "Luc Van Gool", "Lei Zhang"], "venue": "In Automatic Face and Gesture Recognition (FG),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Top-push video-based person re-identification", "author": ["Jinjie You", "Ancong Wu", "Xiang Li", "Wei-Shi Zheng"], "venue": "arXiv preprint arXiv:1604.08683,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Mars: A video benchmark for large-scale person re-identification", "author": ["Liang Zheng", "Zhi Bie", "Yifan Sun", "Jingdong Wang", "Chi Su", "Shengjin Wang", "Qi Tian"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Person reidentification by probabilistic relative distance comparison", "author": ["Wei-Shi Zheng", "Shaogang Gong", "Tao Xiang"], "venue": "In CVPR,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "From point to set: Extend the learning of distance metrics", "author": ["Pengfei Zhu", "Lei Zhang", "Wangmeng Zuo", "David Zhang"], "venue": "In ICCV\u201913,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 23, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 24, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 25, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 27, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 4, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 67, "endOffset": 78}, {"referenceID": 18, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 67, "endOffset": 78}, {"referenceID": 39, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 67, "endOffset": 78}, {"referenceID": 18, "context": "In person re-identification, [20, 37, 41] use features generated by deep convolutional network and obtain state-of-the-art performance.", "startOffset": 29, "endOffset": 41}, {"referenceID": 34, "context": "In person re-identification, [20, 37, 41] use features generated by deep convolutional network and obtain state-of-the-art performance.", "startOffset": 29, "endOffset": 41}, {"referenceID": 38, "context": "In person re-identification, [20, 37, 41] use features generated by deep convolutional network and obtain state-of-the-art performance.", "startOffset": 29, "endOffset": 41}, {"referenceID": 9, "context": "[11] uses convolutional Restricted Boltzmann Machine while deep convolutional neural network is used in [28, 30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[11] uses convolutional Restricted Boltzmann Machine while deep convolutional neural network is used in [28, 30].", "startOffset": 104, "endOffset": 112}, {"referenceID": 27, "context": "[11] uses convolutional Restricted Boltzmann Machine while deep convolutional neural network is used in [28, 30].", "startOffset": 104, "endOffset": 112}, {"referenceID": 23, "context": "Furthermore, [26, 29] use deeper convolutional network and achieved accuracy that even surpasses human performance.", "startOffset": 13, "endOffset": 21}, {"referenceID": 26, "context": "Furthermore, [26, 29] use deeper convolutional network and achieved accuracy that even surpasses human performance.", "startOffset": 13, "endOffset": 21}, {"referenceID": 10, "context": "The accuracy achieved by deep learning on image-based face verification benchmark LFW [12] has been promoted to 99.", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 86, "endOffset": 92}, {"referenceID": 11, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 86, "endOffset": 92}, {"referenceID": 20, "context": "Under these settings, samples in a set distribute in a Hilbert space or Grassmann mainfold so that this issue can be formulated as a metric learning problem [23, 39].", "startOffset": 157, "endOffset": 165}, {"referenceID": 36, "context": "Under these settings, samples in a set distribute in a Hilbert space or Grassmann mainfold so that this issue can be formulated as a metric learning problem [23, 39].", "startOffset": 157, "endOffset": 165}, {"referenceID": 15, "context": "The most famous approach in this kind is the Bag of features [17], which uses histogram to represent the whole set for feature aggregation.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Another classical work is vector of locally aggregated descriptors (VLAD) [14], which aggregates all local descriptors from all samples.", "startOffset": 74, "endOffset": 78}, {"referenceID": 33, "context": "Temporal max/average pooling is used in [36] to integrate all frames\u2019 features generated by recurrent convolutional network.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "The 2nd order statistics is used in [32, 43] in assuming that samples follow Gaussian distribution.", "startOffset": 36, "endOffset": 44}, {"referenceID": 40, "context": "The 2nd order statistics is used in [32, 43] in assuming that samples follow Gaussian distribution.", "startOffset": 36, "endOffset": 44}, {"referenceID": 6, "context": "In [8], original faces in a set are classified into 20 bins based on their pose and quality.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "[38] uses attention mechanism to summarize several sample points to a single aggregated point.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Different from recent works which learn aggregation based on fixed feature [38] or image [8], the QAN learns feature representation and aggregation simultaneously.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Different from recent works which learn aggregation based on fixed feature [38] or image [8], the QAN learns feature representation and aggregation simultaneously.", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "[7] proposed a similar quality aware module named \u201cmemorability based frame selection\u201d which takes \u201cvisual entropy\u201d to be the score of a frame.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Lveri = \u2016Ra(Sa)\u2212Ra(Sp)\u2016\u2212 \u2016Ra(Sa)\u2212Ra(Sn)\u2016 + \u03b4 (4) The loss function above is referred as Triplet Loss in previous works [26].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "Then the 6 aligned scores of each image are averaged and finally normalized to [0, 1] to get the final quality score from human.", "startOffset": 79, "endOffset": 85}, {"referenceID": 7, "context": "Experiments are conducted on PRID2011 [9] and iLiDS-VID [33] datasets.", "startOffset": 38, "endOffset": 41}, {"referenceID": 30, "context": "Experiments are conducted on PRID2011 [9] and iLiDS-VID [33] datasets.", "startOffset": 56, "endOffset": 60}, {"referenceID": 37, "context": "In the first setting, we follow the state-of-the-art method described in [40] and [34].", "startOffset": 73, "endOffset": 77}, {"referenceID": 31, "context": "In the first setting, we follow the state-of-the-art method described in [40] and [34].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "8 CNN+RNN [36] 70 90 95 97 STFV3D [22] 42.", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "6 TDL [40] 56.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "6 eSDC [34] 48.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "4 DVR [34] 40.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "2 LFDA [25] 43.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "9 KISSME [16] 34.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "0 LADF [21] 47.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "1 TopRank [19] 31.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "3% higher than previous best CNN+RNN [36] on PRID2011 and 10% on iLIDSVID.", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "9 CNN+RNN [36] 58 84 91 96 STFV3D [22] 37.", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "9 TDL [40] 56.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "3 eSDC [34] 41.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "1 DVR [34] 39.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "0 LFDA [25] 32.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "6 KISSME [16] 36.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "1 LADF [21] 39.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "8 TopRank [19] 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "6 CNN+RNN [36] 28 57 69 81", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "For face verification, we train our base model on extended version of VGG Face dataset [24], in which we extend the identity number from 2.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "The model is evaluated on YouTube Face Database [35] and IARPA Janus Benchmark A (IJB-A) dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "All faces in training and testing sets are detected and aligned by a multi-task region proposal network as described in [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 35, "context": "06% NAN [38] 95.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "7% FaceNet [26] 95.", "startOffset": 11, "endOffset": 15}, {"referenceID": 26, "context": "39% DeepID2+ [29] 93.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "2% DeepFace-single [30] 91.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "3% EigenPEP [18] 84.", "startOffset": 12, "endOffset": 16}, {"referenceID": 35, "context": "67% NAN [38] 78.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "5% DCNN+metric [4] 78.", "startOffset": 15, "endOffset": 18}, {"referenceID": 28, "context": "1% LSFS [31] 51.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "3% OpenBR [15] 10.", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at GitHub1", "creator": "LaTeX with hyperref package"}}}