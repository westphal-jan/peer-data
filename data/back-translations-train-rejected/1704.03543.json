{"id": "1704.03543", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Leveraging Term Banks for Answering Complex Questions: A Case for Sparse Vectors", "abstract": "While open-domain question answering (QA) systems have proven effective for answering simple questions, they struggle with more complex questions. Our goal is to answer more complex questions reliably, without incurring a significant cost in knowledge resource construction to support the QA. One readily available knowledge resource is a term bank, enumerating the key concepts in a domain. We have developed an unsupervised learning approach that leverages a term bank to guide a QA system, by representing the terminological knowledge with thousands of specialized vector spaces. In experiments with complex science questions, we show that this approach significantly outperforms several state-of-the-art QA systems, demonstrating that significant leverage can be gained from continuous vector representations of domain terminology.", "histories": [["v1", "Tue, 11 Apr 2017 21:21:39 GMT  (27kb)", "http://arxiv.org/abs/1704.03543v1", "Related datasets can be found atthis http URL"]], "COMMENTS": "Related datasets can be found atthis http URL", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["peter d turney"], "accepted": false, "id": "1704.03543"}, "pdf": {"name": "1704.03543.pdf", "metadata": {"source": "CRF", "title": "Leveraging Term Banks for Answering Complex Questions: A Case for Sparse Vectors", "authors": ["Peter D. Turney"], "emails": ["peter.turney@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.03 543v 1 [cs.I R] 11 A systems have proven effective in answering simple questions, they struggle with more complex questions. Our goal is to answer more complex questions more reliably without incurring significant costs in creating knowledge resources to support QA. One readily available knowledge resource is a termbase that lists the key concepts in an area. We have developed an unsupervised learning approach that uses a termbase to manage a QA system by representing the terminological knowledge with thousands of specialized vector spaces. In experiments with complex scientific questions, we show that this approach significantly outperforms several state-of-the-art QA systems, showing that significant benefits can be derived from continuous vector representations of domain domain domain domenterology. In our experiments, we made the surprising discovery that dense, low-dimensional embedding (which are used in many KI systems) is not more effective than the most economical representation for other projects, and that the Dartival spaces were not more exposed to high-dimensional imaging."}, {"heading": "1 Introduction", "text": "The answer to a question about \"why\" and \"why\" in the world is the question about \"how\" in the world \"the answer to a question about\" why \"and\" why \"in the world?\" The answer to a question about \"why\" in the world is the answer to a question about \"why\" in the world? \"The answer to a question about\" why \"in the world is the answer to a question about\" how \"in the world.\" The answer to a question about \"how\" in the world is the answer to a question about \"why\" in the world? \"The answer to a question about\" why \"in the world\" is the answer to a question about \"why\" in the world, \"the\" the \"answer to a question about\" why \"the world,\" the \"the\" answer to a question about \"the world,\" the \"the\" the \"the\" answer to a question about \"the world,\" the \"the\" the \"the\" the \"answer to a question about\" the world, \"the\" the \"the\" the \"the\" the \"answer to a question about\" the \"the\" why \"in the world,\" the \"the\" the \"the\" the \"the\" the \"the\" the \"answer about\" why \"the world,\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" world, \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the world, \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the world,\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the"}, {"heading": "2 Related Work", "text": "The first TREC Quality Assurance Task (Text REtrieval Conference) took place in 1999 (Voorhees, 1999). The task was to answer fact-based, short, open questions involving mainly named companies by retrieving small snippets of text. We now have well-proven IR techniques to answer such questions and research is shifting to more difficult issues. In this section we will discuss related work with scientific exam questions. As embedding is currently popular, our results with sparse vectors may be surprising; hence we will also discuss past work comparing sparse vectors with embedding."}, {"heading": "2.1 Multiple-Choice Science Exam Questions", "text": "This information tends to be unreliable when acquired automatically, or labor-intensive when acquired manually. Multivex requires only a large body of text and a terminology bank for the chosen domain. Khot et al. (2015) compared three different types of Markov logic networks (MLNs) to answer scientific exam questions. They used structured knowledge in the form of if-then rules. Clark et al. (2016) evaluated an interaction of five solvers: Three of the five were corpus-based, but the fourth used if-then rule and the fifth used table. Their ablation study showed that all five solvers performed significantly. Jauhar et al. (2016) represented scientific knowledge in a tabular form, in which rows of facts and columns imposed a parallel structure of types in the lines. The best answer to a question was determined by the rows and columns that best supported one of the ILar decisions."}, {"heading": "2.2 Sparsity and Density", "text": "Dense embedding achieves good results in many tasks (Turney and Pantel, 2010). A more recent approach is to learn embedding with a neural network (Mikolov et al., 2013a; Mikolov et al., 2013b). Baroni et al. (2014) described the classical approach as context counting and the neural network approach as context prediction. Levy et al. (2014b), however, argued that the two approaches learn the same latent structure.Many papers report that dense embedding is better than sparse vectors. For example, Landauer et al. (1997) achieved 64.4% on the TOEFL synonym test with embedding from truncated SVDs."}, {"heading": "3 Multivex", "text": "The input to Multivex is a termbase, a corpus, and a multiple-choice question. The output is the answer to the question. Multivex uses three types of spaces: terminology space, word space, and sentence space. Each term in the term bank maps a line vector in the terminology matrix, a word matrix in the word matrix, and a sentence matrix in the sentence matrix. Table 1 summarizes the spaces. Let q be a question with m possible answers A = {a1,...,..., am} and let T = {t1,...,..., be our termbase with n terms. Multivex evaluates each QA pair < q, ai > in relation to a scientific term tj. The score (q, ai | tj) averages over eight partial ratings, four based on terminology space, two based on sentence space. The final rating for the pair < q, the end text for the source text, we put the source bank, the largest part of the source bank."}, {"heading": "3.1 Term Bank", "text": "The term database consists of 9,356 terms from 52 scientific glossaries. [1] Most of the glossaries come from K-12 websites (kindergartens and grades 1 to 12)."}, {"heading": "3.2 Corpus", "text": "The corpus consists of 280 GB of text (50 billion tokens) collected by a web crawler. All markups were removed from the web pages and the text was split into sentences using the sentence segmentator Stanford CoreNLP. 2 We selected English text by requiring all sentences to contain English stopwords using the SMART stopword list (Salton, 1971).3 The result was 1.75 billion English sentences."}, {"heading": "3.3 Pseudo-Documents", "text": "For each of the 9,356 scientific terms, we searched the corpus for sentences containing the given term. If there were fewer than ten sentences, we dropped the term, leaving 9,009 scientific terms. For each remaining term, we collected a maximum of 50,000 sentences, which formed the pseudo-document for the term. 4"}, {"heading": "3.4 Terminology Space", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3.5 Word Space", "text": "The word space is designed to describe how a word behaves in the context of a given scientific term. For example, the context surrounding the word boundary in sentences about earthquakes differs from the context around the boundary in sentences about the fixed state. The idea is to judge whether the words in a QA pair are used in the QA pair in the same sense as they are used with the given scientific term. That is, the contexts in the QA pair should be similar to the contexts in the pseudo-document for the scientific term. If they are not similar, then the term does not fit well with the QA pair. This can be regarded as a kind of literal confusion. The vector representation of the boundary is modulated by the scientific terms earthquake and the fixed state. By choosing the term, we choose the meaning of the boundary (Reisinger and Mooney, 2010)."}, {"heading": "3.6 Sentence Space", "text": "The aim is to treat the given QA pair as if it were a sentence, and then compare it with the sentences in the sentence space. If the given scientific term is suitable for the given QA pair, then the QA pair should be similar to the sentences in the pseudo-document for the scientific term. \"For example, large earthquakes regularly occur along the San Andreas Fault in California - a huge rupture in the earth marking the boundary between the North American and Pacific tectonic plates.\" Compare this sentence with the question in Figure 1. In this example, the sentence covers the QA pair thoroughly, but we do not assume that a single sentence contains all the information we need to answer a question. Sentence space is used to calculate subpoints that combine information from multiple parts of multiple sentences (see Section 3.7)."}, {"heading": "3.7 Scoring QA Pairs", "text": "The results that we have seen in the USA in recent years in the USA are unbelievable in the way that we have seen them in the USA and in Europe. (tasD) \"We,\" according to the tenor, \"are in a position to be present in the USA, in Europe, in Europe, in Europe, in Europe, in the world, in Europe, in the world, in the USA, in Europe, in the USA, in the USA, in the USA, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "4 Experiments", "text": "In this section, we first test whether Multivex can outperform an IR base system, then replace some of the sparse, high-dimensional vectors in Multivex with dense, low-dimensional vectors to determine the effect of dimensionality and density, measure how much each of the eight sub-assessments contributes to accuracy, and measure how sensitive Multivex is to parameter settings.5 In the following experiments, we use multiplechoice science exam questions at elementary (3rd through 5th grades) and middle school (6th through 8th grades), which are divided into train, development, and test subgroups summarized in Table 3."}, {"heading": "4.1 Comparison with an IR Baseline", "text": "In their experiments with multiple-choice exam questions, Clark et al. (2016) have shown that a Lucene6-based QA system is a strong approach to answering questions that outperforms several more complex algorithms. Therefore, our first experiment compares Multivex with a Lucene-based approach. Table 4 shows the accuracy of Lucene and Multivex in the test questions. The algorithms assign a score to each of the four possible answer decisions, and the choice with the highest score is selected as the best guess. Accuracy is measured by the percentage of correct decisions. If n answer decisions are undecided for the correct score, the algorithm receives a subscore of 1 / n, the expected value of the randomly resolved questions. 5 http: / / allenai.org / data.html. 6 https: / lucene.apache.org / Both algorithms use the same corpus of 1.75 billion sentences, reaching levels higher than those described in section 2.x."}, {"heading": "4.2 SVD Embeddings", "text": "To compare sparse vectors with dense vectors, we modify the first two subvalues (steps 1,1 and 1,2) so that they use dense, low-dimensional SVD matrices (Landauer and Dumais, 1997; Turney and Pantel, 2010). We focus on these two subvalues because step 1 plays a key role in the multivex: searching through the 9,009 scientific terms to pick out the top ten. We leave the other subvalues as they are to make it easier to interpret them. Embedding matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix mat"}, {"heading": "4.3 Word2vec Embeddings", "text": "As another comparison of sparse vectors with dense vectors, this experiment evaluates Word2vec embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b), which is compatible with Google News.8 As with SVD, we apply embeddings to the first two subevaluations (Steps 1.1 and 1.2) and leave the rest of Multivex the same. The embeddings are 300 dimensional.We evaluate two ways to replace the line vectors in the terminology space with Word2vec embeddings. (1) In the face of a term like earthquake, we simply use the Word2vec vector, which corresponds to earthquakes. For multilingual terms, we add the Word2vec vectors for each word in the terminology space. (2) If a term like earthquake is given, we can use the appropriate pseudo document to make an embedding for earthquakes."}, {"heading": "4.4 Ablating Subscores", "text": "Table 5 shows the results of lowering each of the eight sub-evaluations in Multivex. All sub-evaluations contribute to some degree of accuracy, with the exception of the two sub-evaluations in Unigrams (Steps 1.1 and 2.1), which seemed useful in the training and development groups, but are not usable for the test theory.The table suggests that the two sub-evaluations in conjunctions (Steps 1.2 and 2.2) play a key role in Multivex, based on their deltas. Consider the example in Section 3.4 of the Border & Earthquake in Conjunction feature. If this feature has a high tf-idf weight, there must be many sentences in the pseudo-document for earthquakes containing both boundary and earthquake.This means that there is a strong lexical link between question and answer (B) in Figure 1, as the term earthquake occurs."}, {"heading": "4.5 Varying Parameters", "text": "In Section 3.7, we described how Multivex searches through the 9,009 scientific terms. Step 1 selects the first ten terms using the first two partial results. Step 2 reduces the ten to four. Step 3 selects the single best term, resulting in the final output in Step 4.These parameter values were matched to the training sets, with the aim of balancing accuracy and speed. Table 6 examines some alternative values for the parameters and displays the results on the test set. The default settings given in Section 3.7 correspond to the second line in the body of the table. The parameter values have little impact on accuracy (which is a good thing), but they have a big impact on the execution time (as expected). Answering a single question involves four passes through the steps in Section 3.7, one pass for each of the four candidates. With the default parameter settings, Multivex can answer a typical four selection question in five seconds running on a standard desktop computer (iore Intel)."}, {"heading": "5 Trouble with Embeddings", "text": "This hypothesis is supported by the results of experiments with SVD and Word2vec (sections 4.2 and 4.3). Table 5 shows the value of the conjunction characteristics (steps 1.2 and 2.2).The tf-idf-weighted conjunction correspondence is the most important sub-value. Of the 22,767,476 columns in the terminology matrix, 22,505,565 conjunctions (98.8%).The frequency of conjunction characteristics (dfj in section 3.4) ranges from 1 to 4,292, with a median of 1. Conjunction characteristics exhibit a very long tail of rare occurrences. Rare conjunction events provide valuable information to answer scientific questions."}, {"heading": "6 Future Work and Limitations", "text": "Our focus in this paper was on multiple-choice questions, but it should be possible to extend Multivex to direct-answer questions. For example, the sentence matrices could be used to generate a series of direct answers from candidates (see Section 3.6). Multivex is unattended; we expect that a supervised approach would lead to higher test scores. One possibility is to use a supervised approach of deep learning with an attention model to focus on rare events (Li et al., 2016; Zhao et al., 2017). Another limitation is that the features in Multivex are simple unigrams, bigrams, trigrams and conjunctions of unigrams. More complex features, such as parts-of-speech tags and semantic relationships, could complement these basic traits. The success of our termbase suggests that we should look for other low-cost resources that can guide QA systems."}, {"heading": "7 Conclusion", "text": "Multivex is a domain-limited QA system because it requires a domain-specific termbase, but this is a relatively light requirement compared to QA systems that require if-then rules or tables. Most importantly, we can use lexical cohesion with a termbase and some vectors to guide us to the right answer.Multivex differs from many recent work in that it uses sparse, high-dimensional vectors instead of dense, low-dimensional embeddings. Intuition is that word meanings are distributive and general, but facts are intersections of word meanings; facts tend to be rare and specific. Experimental results in Sections 4.2 and 4.3 support these intuitions."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "GeorgianaDinu", "Germ\u00e1nKruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of ACL, pages 238\u2013247.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Joint learning of sentence embeddings for relevance and entailment", "author": ["Petr Baudis", "Silvestr Stanko", "Jan Sedivy."], "venue": "arXiv preprint arXiv:1605.04655.", "citeRegEx": "Baudis et al\\.,? 2016", "shortCiteRegEx": "Baudis et al\\.", "year": 2016}, {"title": "Combining retrieval, statistics, and inference to answer elementary science questions", "author": ["Peter Clark", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Turney", "Daniel Khashabi."], "venue": "Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Clark et al\\.,? 2016", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "Matrix Computations", "author": ["Gene Golub", "Charles Van Loan."], "venue": "JHU Press, 3rd edition.", "citeRegEx": "Golub and Loan.,? 1996", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "Tables as semi-structured knowledge for question answering", "author": ["Sujay Kumar Jauhar", "Peter Turney", "Eduard Hovy."], "venue": "ACL, pages 474\u2013483.", "citeRegEx": "Jauhar et al\\.,? 2016", "shortCiteRegEx": "Jauhar et al\\.", "year": 2016}, {"title": "Question answering via integer programming over semistructured knowledge", "author": ["Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni", "Dan Roth."], "venue": "IJCAI, pages 1145\u20131152.", "citeRegEx": "Khashabi et al\\.,? 2016", "shortCiteRegEx": "Khashabi et al\\.", "year": 2016}, {"title": "Exploring Markov logic networks for question answering", "author": ["Tushar Khot", "Niranjan Balasubramanian", "Eric Gribkoff", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni."], "venue": "Proceedings of EMNLP, volume 5, pages 685\u2013694.", "citeRegEx": "Khot et al\\.,? 2015", "shortCiteRegEx": "Khot et al\\.", "year": 2015}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas Landauer", "Susan Dumais."], "venue": "Psychological Review, 104(2):211\u2013240.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "CoNLL, pages 171\u2013180.", "citeRegEx": "Levy and Goldberg.,? 2014a", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Advances in Neural Information Processing Systems, pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014b", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "The SMART Retrieval", "author": ["Gerard Salton"], "venue": null, "citeRegEx": "Salton.,? \\Q1971\\E", "shortCiteRegEx": "Salton.", "year": 1971}, {"title": "The TREC-8 question answering", "author": ["Ellen Voorhees"], "venue": null, "citeRegEx": "Voorhees.,? \\Q1999\\E", "shortCiteRegEx": "Voorhees.", "year": 1999}], "referenceMentions": [{"referenceID": 6, "context": "tions (Khot et al., 2015; Clark et al., 2016; Jauhar et al., 2016; Khashabi et al., 2016).", "startOffset": 6, "endOffset": 89}, {"referenceID": 2, "context": "tions (Khot et al., 2015; Clark et al., 2016; Jauhar et al., 2016; Khashabi et al., 2016).", "startOffset": 6, "endOffset": 89}, {"referenceID": 4, "context": "tions (Khot et al., 2015; Clark et al., 2016; Jauhar et al., 2016; Khashabi et al., 2016).", "startOffset": 6, "endOffset": 89}, {"referenceID": 5, "context": "tions (Khot et al., 2015; Clark et al., 2016; Jauhar et al., 2016; Khashabi et al., 2016).", "startOffset": 6, "endOffset": 89}, {"referenceID": 12, "context": "The first TREC (Text REtrieval Conference) QA task took place in 1999 (Voorhees, 1999).", "startOffset": 70, "endOffset": 86}, {"referenceID": 6, "context": "Khot et al. (2015) compared three different types of Markov Logic Networks (MLNs) for answering science exam questions.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "(2016) applied ILP to knowledge in a tabular form, using the same tables as Jauhar et al. (2016). Their ILP system performed multi-step inference by chaining together multiple rows from separate tables.", "startOffset": 76, "endOffset": 97}, {"referenceID": 7, "context": "The classical approach to embeddings is to make a word\u2013context cooccurrence matrix and then apply a dimensionality reduction algorithm (Landauer and Dumais, 1997).", "startOffset": 135, "endOffset": 162}, {"referenceID": 0, "context": "Baroni et al. (2014) described the classical approach as context-counting and the neural network approach as contextpredicting.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Baroni et al. (2014) described the classical approach as context-counting and the neural network approach as contextpredicting. However, Levy et al. (2014b) argued the two approaches are learning the same latent structure.", "startOffset": 0, "endOffset": 157}, {"referenceID": 10, "context": "In summary, they reported that \u201cthere is no single method that consistently performs better than the rest\u201d (Levy et al., 2015) and a sparse representation \u201cis superior in some of the more semantic tasks\u201d (Levy and Goldberg, 2014a).", "startOffset": 107, "endOffset": 126}, {"referenceID": 8, "context": ", 2015) and a sparse representation \u201cis superior in some of the more semantic tasks\u201d (Levy and Goldberg, 2014a).", "startOffset": 85, "endOffset": 111}, {"referenceID": 8, "context": ", 2015) and a sparse representation \u201cis superior in some of the more semantic tasks\u201d (Levy and Goldberg, 2014a). Toutanova et al. (2015) show that a sparse \u201cobserved features\u201d model is better than a dense \u201clatent feature\u201d model for knowledge bases and textual inference.", "startOffset": 86, "endOffset": 137}, {"referenceID": 11, "context": "We selected English text by requiring all sentences to contain English stop words, using the SMART stop word list (Salton, 1971).", "startOffset": 114, "endOffset": 128}, {"referenceID": 2, "context": "In their experiments with multiple-choice science exam questions, Clark et al. (2016) show that a QA system based on Lucene is a strong approach to question answering, out-performing several more complex algorithms.", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": "For comparison with past work, Jauhar et al. (2016) achieve 54.", "startOffset": 31, "endOffset": 52}, {"referenceID": 3, "context": "For comparison with past work, Jauhar et al. (2016) achieve 54.9% on the Public Elementary questions, but this result is for the whole set of 855 questions, not the test set. Sachan et al. (2016) achieve 46.", "startOffset": 31, "endOffset": 196}, {"referenceID": 1, "context": "Baudis et al. (2016) achieve 44.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "2) to use dense, low-dimensional SVD embeddings (Landauer and Dumais, 1997; Turney and Pantel, 2010).", "startOffset": 48, "endOffset": 100}], "year": 2017, "abstractText": "While open-domain question answering (QA) systems have proven effective for answering simple questions, they struggle with more complex questions. Our goal is to answer more complex questions reliably, without incurring a significant cost in knowledge resource construction to support the QA. One readily available knowledge resource is a term bank, enumerating the key concepts in a domain. We have developed an unsupervised learning approach that leverages a term bank to guide a QA system, by representing the terminological knowledge with thousands of specialized vector spaces. In experiments with complex science questions, we show that this approach significantly outperforms several state-of-the-art QA systems, demonstrating that significant leverage can be gained from continuous vector representations of domain terminology. In our experiments, we made the surprising discovery that dense, low-dimensional embeddings (used in many AI systems) were not the most effective representation, and that sparse, high-dimensional vector spaces performed better. We discuss the reasons for this, and the implications this may have for other projects that have assumed embeddings are the best continuous representation.", "creator": "LaTeX with hyperref package"}}}