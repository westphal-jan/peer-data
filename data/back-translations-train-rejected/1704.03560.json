{"id": "1704.03560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge", "abstract": "This paper describes Luminoso's participation in SemEval 2017 Task 2, \"Multilingual and Cross-lingual Semantic Word Similarity\", with a system based on ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses on general knowledge that relates the meanings of words and phrases. Our submission to SemEval was an update of previous work that builds high-quality, multilingual word embeddings from a combination of ConceptNet and distributional semantics. Our system took first place in both subtasks. It ranked first in 4 out of 5 of the separate languages, and also ranked first in all 10 of the cross-lingual language pairs.", "histories": [["v1", "Tue, 11 Apr 2017 22:44:35 GMT  (15kb)", "http://arxiv.org/abs/1704.03560v1", "5 pages, accepted to the SemEval workshop at ACL 2017"]], "COMMENTS": "5 pages, accepted to the SemEval workshop at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert speer", "joanna lowry-duda"], "accepted": false, "id": "1704.03560"}, "pdf": {"name": "1704.03560.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rspeer@luminoso.com", "jlowry-duda@luminoso.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.03 560v 1 [cs.C L] 11 Apr 201 7pation in SemEval 2017 Task 2, \"Multilingual and Cross-lingual Semantic Word Similarity,\" with a system based on ConceptNet. ConceptNet is an open, multilingual knowledge graphic that focuses on general knowledge that relates the meaning of words and phrases. Our submission to SemEval was an update of earlier work that builds high-quality multilingual word embedding from a combination of ConceptNet and distribution semantics. Our system took first place in both subtasks, taking first place in 4 of 5 separate languages and first place in all 10 cross-language pairs."}, {"heading": "1 Introduction", "text": "ConceptNet 5 (Speer and Havasi, 2013) is a multilingual cross-domain knowledge graphic that combines words and phrases of natural language (terms) with labeled, weighted edges. Compared to other knowledge graphics, it avoids the attempt to be a large viewer of named entities. It aims primarily to cover the frequently used words and phrases of each language and to present commonly known relationships between the meanings of these terms. The paper describing ConceptNet 5.5 (Speer et al., 2017) showed that it could be used in combination with sources of distributional semantics, in particular the word2vec Google News embeddings (Mikolov et al., 2013) and GloVe 1.2 (Pennington et al., 2014) to produce new embeddings of words in different languages."}, {"heading": "2 Implementation", "text": "The way in which we built our embedding is based on retrofitting (Faruqui et al., 2015) and in particular on their elaboration, which we call \"extended retrofitting\" (Speer et al., 2017). Retrofitting, as originally described, adapts the values of existing word embedding based on a new objective function that also takes into account a knowledge graph. Its output has the same vocabulary as its input. In advanced retrofitting, on the other hand, terms that are only present in the knowledge graph are added to the vocabulary and are also associated with 1data and code are available at http: / / conceptnet.io.vectors."}, {"heading": "2.1 Combining Multiple Sources of Vectors", "text": "As described in the paper ConceptNet 5.5 (Speer et al., 2017), we apply advanced retrofitting separately to multiple embedding sources (such as pre-trained word2vec and GloVe), then adjust the results to a uniform vocabulary and reduce its dimensionality. First, we create a uniform matrix of embedding, M1, as follows: \u2022 Take the ConceptNet subgraph, which consists of nodes of at least 3 degrees. Remove edges that correspond to negative relationships (such as NotUsedFor and Antonym). Remove phrases with 4 or more words. \u2022 Standardize the sources of embedding by folding their terms on a case-by-case basis and normalizing L1 their columns. \u2022 For each source of embedding, apply ex-panded retrofits via this source with the ConceptNet subgraph."}, {"heading": "2.2 Vocabulary Selection", "text": "Advanced Retrofitting produces vectors for all the terms in its knowledge diagram and all the terms in the input embeddings. Some terms outside the ConceptNet diagram have useful embeddings that represent knowledge we would like to retain, but the use of all these terms would be loud and wasteful. To select the vocabulary of our term vectors, we used a heuristics that takes advantage of the fact that the precalculated word2vec and GloVe embeddings we use have sorted their lines (which represent terms) by term frequency. To find suitable terms, we take all the terms that occur in the first 500,000 lines of theword2vec and GloVe inputs, and appear in the first 200,000 lines of at least one of them. We take the union of these terms with the terms described above in the ConceptNet subgraph. The resulting vocabulary we use in the system is the 9.988-ConceptNet plus its additional variants."}, {"heading": "2.3 Dimensionality Reduction", "text": "The concatenated matrixM1 has k columns that represent features that can be redundant with each other. Our next step is to reduce their dimensionality to a smaller number k, which we set to 300, the dimensionality of the largest input matrix. Our goal is to learn a projection of k dimensions to k dimensions that removes the redundancy that comes from concatenating multiple sources of embedding. We take 5% of the rows from M1 to obtain M2, which we will use to find the projection more efficiently, provided that their vectors represent roughly the same distribution as M1. M2 can be approximated with an abbreviated SVD output: M2 \u2248 U2 / 2V T, where the projection is truncated to a k-k diagonal matrix of the largest singular values, and U and V are truncated accordingly to have only these k columns."}, {"heading": "2.4 Don\u2019t Take \u201cOOV\u201d for an Answer", "text": "In fact, the fact is that most of them are able to decide whether they are able or not, will be able to play by the rules."}, {"heading": "3 Results", "text": "In this task, the systems were evaluated using the harmonic mean of their Pearson-and-Spearman correlation with the test set for each language (or language pairs in Subtask 2) and assigned overall scores, with averages for the top 4 languages in Subtask 1 and the top 6 pairs in Subtask 2."}, {"heading": "3.1 The Submitted System: ConceptNet + word2vec + GloVe", "text": "The system we submitted used the retrofitting and merge procedure described above, with ConceptNet 5.5.3 as a knowledge chart and two renowned sources of English word embedding. The first source is word2vec Google News embeddings2, and the second is the GloVe 1.2 embedding, which has been trained on 840 billion Common Crawl3 tokens. Since the embedding of the input is only in English, the vectors in other languages depended entirely on spreading these English embedding via the multilingual links in ConceptNet. This system appears in the results as \"Luminoso-run2.\" Run 1 was similar, but it looked for neighbors in an unpublished version of the ConceptNet chart with fewer edges of DBPedia. The total result of this system in Subtask 1 was 0.743. Its combined score for Subtask 2 (averaged over the six best language pairs) was 754."}, {"heading": "3.2 Variant A: Adding Polyglot Embeddings", "text": "Instead of relying entirely on ConceptNet's knowledge of English, it seemed reasonable to include pre-calculated word embedding in other languages as well. In variant A, we inserted input from the polyglot embedding (Al-Rfou et al., 2013) in German, Spanish, Italian and Farsi as four additional inputs into the retrofitting and merging process.2https: / / code.google.com / archive / p / word2vec / 3 http: / / nlp.stanford.edu / projects / glove / The results of this variant of the test data were markedly lower, and if we evaluate them retrospectively on the test results, their test results are also lower. Overall results are at Subtask 1 at.720 and Subtask 2 at.736."}, {"heading": "3.3 Variant B: Adding Parallel Text from OpenSubtitles", "text": "In variant B, we calculated our own multilingual distribution embeddings from word coincidences in the parallel corpus OpenSubtitles2016 (Lison and Tiedemann, 2016) and used them as a third input besides word2vec and GloVe.For each pair of matching subtitles between the five languages, we combined the language-tagged words into a single set of n words, then added these sparse coincidences by 1 / n into 300-dimensional vectors, resulting in a sparse matrix of word coincidences within and between languages. We submitted Run 2 instead of variant B because Run 2 was simpler and seemed to be slightly better on average. However, when we performed variant B on the basis of published test data, this variant was compared with Run 2 without results."}, {"heading": "3.4 Comparison of Results", "text": "The published results4 show that our system, listed as Luminoso Run2, has the highest overall score for both tasks and for each test set except for the monolingual Farsi set.Table 1 compares the results per language of the system we submitted, the same system without our OOV handling strategies, variants A and B and the basic Nasari system (Camacho-Collados et al., 2016).Variant B performed best at the end, so we will integrate parallel text from OpenSubtitles into the next version of the ConceptNet numberbatch system."}, {"heading": "4 Discussion", "text": "The idea of creating word embeddings from a combination of distribution and relations4 http: / / alt.qcri.org / semeval2017 / task2 / index.php? id = resultsknowldedge has been implemented by many others, including Iacobacci et al. (2015) and various implementations of Retrofitting (Faruqui et al., 2015). ConceptNet is characterized by the great improvement in the evaluation results that occur when used as a source of relational knowledge, suggesting that ConceptNet's unique blend of crowd-sourced, gamified, and expert knowledge provides valuable information that is not only learned from distribution semantics, but translates well to other languages and demonstrates the usefulness of ConceptNet as a \"multilingual glue\" that can combine knowledge in multiple languages into a single representation."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for Computa-", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606 https://arxiv.org/pdf/1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "SemEval2017 Task 2: Multilingual and cross-lingual semantic word similarity", "author": ["Jose Camacho-Collados", "Mohammad Taher Pilehvar", "Nigel Collier", "Roberto Navigli."], "venue": "Proceedings of SemEval. Vancouver, Canada.", "citeRegEx": "Camacho.Collados et al\\.,? 2017", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2017}, {"title": "Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities", "author": ["Jos\u00e9 Camacho-Collados", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "Artificial Intelligence 240:36\u201364.", "citeRegEx": "Camacho.Collados et al\\.,? 2016", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2016}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of NAACL. http://arxiv.org/abs/1411.4166.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "SensEmbed: Learning sense embeddings for word and relational similarity", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "ACL (1). pages 95\u2013105.", "citeRegEx": "Iacobacci et al\\.,? 2015", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225. http://www.aclweb.org/anthology/Q15-1016.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles", "author": ["Pierre Lison", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation.", "citeRegEx": "Lison and Tiedemann.,? 2016", "shortCiteRegEx": "Lison and Tiedemann.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR abs/1301.3781. http://arxiv.org/abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12:1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "2017. ConceptNet 5.5: An open multilingual graph of general knowledge", "author": ["Robert Speer", "Joshua Chin", "Catherine Havasi"], "venue": "San Francisco", "citeRegEx": "Speer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2017}, {"title": "ConceptNet 5: A large semantic network for relational knowledge", "author": ["Robert Speer", "Catherine Havasi."], "venue": "The People\u2019s Web Meets NLP, Springer, pages 161\u2013176.", "citeRegEx": "Speer and Havasi.,? 2013", "shortCiteRegEx": "Speer and Havasi.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "ConceptNet 5 (Speer and Havasi, 2013) is a multilingual, domain-general knowledge graph that", "startOffset": 13, "endOffset": 37}, {"referenceID": 10, "context": "(Speer et al., 2017) showed that it could be used in combination with sources of distributional semantics, particularly the word2vec Google News skip-gram embeddings (Mikolov et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": ", 2017) showed that it could be used in combination with sources of distributional semantics, particularly the word2vec Google News skip-gram embeddings (Mikolov et al., 2013) and GloVe 1.", "startOffset": 153, "endOffset": 175}, {"referenceID": 9, "context": "2 (Pennington et al., 2014), to produce new embeddings with state-of-the-art performance across many word-relatedness eval-", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": "The three data sources are combined using an extension of the technique known as retrofitting (Faruqui et al., 2015).", "startOffset": 94, "endOffset": 116}, {"referenceID": 2, "context": "This task (Camacho-Collados et al., 2017) evaluated systems at their ability to rank pairs of words by their semantic similarity or relatedness.", "startOffset": 10, "endOffset": 41}, {"referenceID": 4, "context": "The way we built our embeddings is based on retrofitting (Faruqui et al., 2015), and in particular, the elaboration of it we call \u201cexpanded retrofitting\u201d (Speer et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 10, "context": ", 2015), and in particular, the elaboration of it we call \u201cexpanded retrofitting\u201d (Speer et al., 2017).", "startOffset": 82, "endOffset": 102}, {"referenceID": 10, "context": "5 paper (Speer et al., 2017), we apply expanded retrofitting separately to multiple sources of embeddings (such as pre-trained word2vec and GloVe), then align the results on a unified vocabulary and reduce its dimensionality.", "startOffset": 8, "endOffset": 28}, {"referenceID": 6, "context": "by Levy et al. (2015) in their SVD process.", "startOffset": 3, "endOffset": 22}, {"referenceID": 1, "context": "Some evaluators, such as Bojanowski et al. (2016), discard all pairs containing an OOV word.", "startOffset": 25, "endOffset": 50}, {"referenceID": 0, "context": "In Variant A, we added inputs from the Polyglot embeddings (Al-Rfou et al., 2013) in German, Spanish, Italian, and Farsi as four additional inputs to the retrofitting-and-merging process.", "startOffset": 59, "endOffset": 81}, {"referenceID": 7, "context": "In Variant B, we calculated our own multilingual distributional embeddings from word cooccurrences in the OpenSubtitles2016 parallel corpus (Lison and Tiedemann, 2016), and used this as a third input alongside word2vec and GloVe.", "startOffset": 140, "endOffset": 167}, {"referenceID": 6, "context": "We then used the SVD-of-PPMI process described by Levy et al. (2015) to convert these sparse cooccurrences into 300-dimensional vectors.", "startOffset": 50, "endOffset": 69}, {"referenceID": 3, "context": "Table 1 compares the results per language of the system we submitted, the same system without our OOV-handling strategies, variants A and B, and the baseline Nasari (Camacho-Collados et al., 2016) system.", "startOffset": 165, "endOffset": 196}, {"referenceID": 4, "context": "(2015) and various implementations of retrofitting (Faruqui et al., 2015).", "startOffset": 51, "endOffset": 73}, {"referenceID": 4, "context": "knowldedge has been implemented by many others, including Iacobacci et al. (2015) and various implementations of retrofitting (Faruqui et al.", "startOffset": 58, "endOffset": 82}], "year": 2017, "abstractText": "This paper describes Luminoso\u2019s participation in SemEval 2017 Task 2, \u201cMultilingual and Cross-lingual Semantic Word Similarity\u201d, with a system based on ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses on general knowledge that relates the meanings of words and phrases. Our submission to SemEval was an update of previous work that builds high-quality, multilingual word embeddings from a combination of ConceptNet and distributional semantics. Our system took first place in both subtasks. It ranked first in 4 out of 5 of the separate languages, and also ranked first in all 10 of the cross-lingual language pairs.", "creator": "LaTeX with hyperref package"}}}