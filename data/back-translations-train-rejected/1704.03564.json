{"id": "1704.03564", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Active classification with comparison queries", "abstract": "We study an extension of active learning in which the learning algorithm may ask the annotator to compare the distances of two examples from the boundary of their label-class. For example, in a recommendation system application (say for restaurants), the annotator may be asked whether she liked or disliked a specific restaurant (a label query); or which one of two restaurants did she like more (a comparison query).", "histories": [["v1", "Tue, 11 Apr 2017 22:55:29 GMT  (127kb,D)", "https://arxiv.org/abs/1704.03564v1", "23 pages (not including references), 1 figure"], ["v2", "Fri, 2 Jun 2017 00:49:37 GMT  (127kb,D)", "http://arxiv.org/abs/1704.03564v2", "23 pages (not including references), 1 figure. The new version contains a minor fix in the proof of Lemma 4.2"]], "COMMENTS": "23 pages (not including references), 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.CG", "authors": ["daniel m kane", "shachar lovett", "shay moran", "jiapeng zhang"], "accepted": false, "id": "1704.03564"}, "pdf": {"name": "1704.03564.pdf", "metadata": {"source": "CRF", "title": "Active classification with comparison queries", "authors": ["Daniel M. Kane", "Shachar Lovett", "Shay Moran", "Jiapeng Zhang"], "emails": ["dakane@ucsd.edu", "slovett@cs.ucsd.edu.", "shaymoran1@gmail.com.", "jpeng.zhang@gmail.com."], "sections": [{"heading": null, "text": "We focus on the half-room class and show that under natural assumptions, such as a large margin or a limited bit description of the input examples, it is possible to reveal all the names of a sample of size n using, say, O (log n) queries. This implies an exponential improvement over classical active learning, where only label queries are allowed. We supplement these results by showing that if one of these assumptions is removed, queries for O (n) are required at worst. Our results result from a new general framework of active learning with additional queries. We identify a combinatorial dimension, the so-called inference dimension, which captures the complexity of the query when each additional query is determined by O (1) examples (such as comparison queries, each determined by the two comparison examples)."}, {"heading": "1 Introduction", "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "1.1 Active learning with additional comparison queries", "text": "Consider a learned concept of the form c (x) = character (f (x)), where f is a real value function (e.g. half spaces, neural networks), and consider two instances x1, x2 such that, for example, f (x1) = 10 and f (x2) = 1000. Both c (x1) and c (x2) are equal to + 1, where f (x2) > f (x1) suggests that x2 is a \"more positive instance\" than x2. In the context of movie classification, of course, this is interpreted as meaning that the person likes the movie x2 better than the movie x1. We call the query \"f (x2) \u2265 f (x1)?\" a comparison query."}, {"heading": "1.1.1 Example: learning half-planes with comparison and label queries", "text": "To be specific, one has to look at the class of half-planes in R2. Here, a comparison query is equivalent to asking which of two samples x1, x2 is closer to the boundary line. Do such queries improve the complexity of the query about standard active learning? It is known that without such queries, the learner essentially has to query all labels (Dasgupta, 2004).The following algorithm demonstrates an exponential improvement if queries are allowed. Later, we will present more general results that generalize to higher dimensions under certain natural conditions, and that such limitations are actually necessary. Interactive learning algorithms with comparison questions in R2 (see Figure 1 for a graph)."}, {"heading": "1.2 Results", "text": "Next, we present and discuss our main findings. 1. Section 1.2.1 is dedicated to our results on half-spaces, 2. Section 1.2.2 is devoted to the conclusion level and how it captures the query complexity of active learning with additional comparative questions, and 3. Section 1.2.3 is focused on the general framework of active learning with additional queries."}, {"heading": "1.2.1 Interactive learning of half spaces with comparison queries", "text": "We start by discussing our results for interactive learning of half-spaces in Rd, when both label queries and comparison queries are permitted. We show that a general algorithm such as we have described for R2 cannot exist if we identify two useful properties that enable such a learning algorithm: limited bit complexity and marginality. We first describe our results in the context of exact recovery. Here, the labels of all n samples must be uncovered by using as few queries as possible. We first show that in the worst case, this requires a query (s) for all d labels. We remember that O (log n) queries are sufficient in R2. Theorem 1.1 (Theorem 4.11, informal version) There are n dots in R3 that require a label and comparison queries for detecting all labels. Our first positive result shows that efficient, exact restoration of labels is possible when the dots are low."}, {"heading": "1.2.2 Inference dimension", "text": "Our results for learning are based on a common combinatorial property, which we describe next. (Let X be a sentence and H be a concept class, in which any concept of the form character (f (x)) for f: X \u2192 R. It may be a finite set X = {x1,..., xn} and H the class of all half-spaces with at least 1 / 100 dots in relation to X; or X = {0, 1} d and H all half-spaces; or X = Rd and H a class of (characters) low polynomials; etcetera.Let S is an unlabeled sample. A S \u2212 query is either a label query in relation to x x, or a comparison query in relation to x1, x2 S. Namely, the allowed queries are \"f (x), (label query) or\" f (x1)."}, {"heading": "1.2.3 General framework", "text": "Next, we will describe how the concept of the inference dimension, as well as Theorem 1.5, extends to settings where the additional questions are not necessarily comparison questions. Consider an interactive model in which the learning algorithm is allowed to use additional queries from a prescribed set of queries Q. Formally, the algorithm can label any point in S. Here, the algorithm is allowed to use additional queries from a set of Q = Q (S) (we emphasize that the allowable queries depend on the input sample). In the setting discussed in the previous section, for example, Q (S) contains all queries under the points in S. Another example used by crowd-sourcing algorithms (Tamuz et al., 2011) includes 3 smart queries of the form \"Is x2 more similar than x3.\""}, {"heading": "1.3 Related work", "text": "A partial list includes: Angluin (1987); Baum (1991); Lang and Baum (1992); Tura \u0301 n (1993); Jackson (1997); Kwek and Pitt (1998); Blum et al. (1998); Bshouty et al. (2004); Feldman (2009a, b); Nowak (2011); Chen et al. (2016). Many of them focus on the case in which the additional queries are queries. We discuss some of these results in Section 1.4.1. In the context of active learning considered in this paper, Balcan and Hanneke consider additional queries of two types: class-related queries and mistake queries, in the first way that the student provides the annotator with a list of examples, and a label and asks them to provide an example in this list with the given label."}, {"heading": "1.4 Discussion and suggested future research", "text": "Next, we offer some potential directions for future research."}, {"heading": "1.4.1 Other types of additional queries", "text": "It is natural to examine other types of additional queries, and if they extend the learning process, then this is a type of query that relates to the way the query occurs in many areas of application. (A popular example of relative queries that are applied in practice is an example of three data points that are applied in practice. (An example of this is the type of query that is applied in practice.) It is an example of deviations from the type of query that are applied in practice. (2007) McFee and Lanckriet (2010); Huang et al al al al al al. (2011); Qian et al. (2013).Membership queries of generative models A natural and well-studied type of queries is allowed to belong to queries."}, {"heading": "1.4.2 Noisy comparisons", "text": "Imagine, for example, that f (x1) = 1.001 and f (x2) = 1.0001. Our algorithms are based on the assumption that the answer to the question \"f (x1) \u2265 f (x2)?\" will always be \"Yes.\" It is plausible to consider a noise model that could give the wrong answer in cases where f (x1) \u2248 f (x2) is suggested by Bradley and Terry (1952)."}, {"heading": "1.4.3 Streaming-based interactive learning with comparison queries", "text": "Another natural and well-studied model is the stream-based model. Here, the algorithm can view the unlabeled examples online one-to-one and decide whether to query the current example. Consider a setting in which the algorithm has limited memory in which it can store past examples. After receiving a new example, it can query its caption and / or compare it with each of the saved examples. In the realisable setting, one possible goal is to minimize the number of queries and the memory size, while maintaining a (partial) hypothesis that is correct in all past examples. Consider, for example, the class of threshold functions above R. In an infinite sequence of independent unlabeled examples x1, x2,..., an unknown distribution is drawn over R. A simple online algorithm with memory M = 2 stores the 2 examples x, y where the character change occurs. When receiving a new example xn, it is checked whether a comparison is made between this one half of the xx and the next one half of the example is then opened (x and one half of the example is inscribed)."}, {"heading": "1.4.4 Exactly learning threshold functions", "text": "Consider the class of threshold functions via the N-dimensional hypercube {0 > 1} q. With Lemma 4.2, the derivative dimension of this class is actually O (N logN). Theorem 4.1 therefore implies that the 2n labels of any threshold function can be detected with a maximum of O (N2 log2N) queries in expectation. This demonstrates the additional power of queries over label queries: In fact, standard arguments show that if only label queries are allowed, then at least 2o (N) queries are necessary in expectation. While only O (N2 log2N) queries are sufficient to detect all labels in 2N log2N, it remains open whether this can be done efficiently in a poly (N) time. A naive implementation of the algorithm from Theorem 4.1 would verify at each step that the labeling capability in {0, 1} N, the labeling of which is not yet known, can be derived from this step."}, {"heading": "2 Preliminaries", "text": "Basic definitions. A hypothesis class is a pair (X, H) in which X is a set, and H is a class of functions h: X: {\u00b1 1}. Each function h: X: {\u00b1 1} is referred to as a hypothesis or concept. In this work, we examine classes H = HF of the formH = {character (f): f \u00b2 F}, in which F = {f: X \u2192 R} is a class of real functions, and character (f) (f (x) = character (f (x))). {\u00b1 1} is equal to + 1 if and only if f (x) \u2265 0. For example, if F is the class of Rd \u2192 R affine functions, then HF is the class of d-dimensional hemispheres. Other examples include neural networks, low-degree polynomials, and more. The reason we \"remember\" the underlying class F is because we use it to compare ques.An example is a pair (x, y)."}, {"heading": "2.1 Active learning", "text": "It is therefore helpful to remember the framework of active learning before extending it by allowing additional queries. A (pool-based) active learning algorithm has an access to the unlabeled sample S. (pool-based) active learning algorithm may be adaptive. (each active learning algorithm is associated with two complexity measures: (i) the sample-complexity n (, \u03b4), is the number of examples required to achieve error at most with confidence at at at at at at at (like in the passive setting), and (ii) the query-complexity (also label-complexity) q (, \u03b4), is the number of queries it makes.In the process of active learning, it is natural to distinguish between points whose label can be derived and points for which there is uncertainty about their label. It is therefore advisable to consider partial hypotheses."}, {"heading": "2.2 Interactive learning with additional queries", "text": "Consider an extension of the active learning environment by allowing the learning algorithm to use additional queries from a prescribed set of queries Q (Q). An additional query is modeled as a Boolean function q: F \u2192 {True, false}. We emphasize that the query may depend on the function f that underlies the learned concept c = character (f) (e.g. comparison queries, see below).In this setting, the algorithm gains access to S, the unlabeled sample underlying S (S), and may: \u2022 query the name of any point in S, \u2022 query an additional q from a prescribed set of queries Q (S). We emphasize that the amount of allowable queries Q (S) depend on the input sample S (S). For example, a comparison query on x1, x2 is the query \"f (x1), x2) \u2264 f."}, {"heading": "3 Inference Dimension", "text": "Let (X, H) be a hypotheses class in which H = HF is F for any class of real functions, and let Q be a series of additional queries. For S X, x-X, f-F, and c = character (f), letS = \u21d2 f x stands for the statement that there is a sequence Q of annotation queries answered by S and / or additional queries by Q (S) that determine the labeling of x if the learned concept is c. Namely, that x-Conf (Q).Definition 3.1 (inference dimension). The inference dimension of (X, H) is the minimum number k, so that for each S X of size k and each c-H there is x-S, so that S\\ {x} = \u21d2 f x. If there is no such k, then the inference dimension of (X, H) is defined as temporary."}, {"heading": "3.1 Upper bound", "text": "Theorem 3.2 (Boosting). Let k recognize the inference dimension of (X, H). Suppose there is an algorithm that, given a realizable sample of size n as input, uses at most q (n) queries and the answers to all queries in Q (S) and to all labels in S (S). Then there is a randomized algorithm that takes all labels in S (4k). We prove Theorem 3.2 in two steps: (i) First, Lemma 3.3 shows that (X, H) has a weak self-confident learner who uses queries in the face of a realizable input sample of size 4k (4k), and gives a partial hypothesis, the 1 / 2. (ii) Then we show that the labels of a given sample of size n are revealed."}, {"heading": "3.2 Lower bound", "text": "Next, we show that if the derivative dimension is large, many queries are necessary to derive all possible names from it. We also assume that each query is t-local, in the sense that it depends on f (x1)..., f (xt) for some x1,.. that there are a lot of allowable queries Q (S) to be all queries containing subsets of S of size. Let (X, H) is a hypotheses category with inference dimension > k, for some k \u2265 3. This means that there is Z'X of size k and a concept c \u00b2 of size H, so that for each z \u00b2 z there is a query C with cz (z) 6 = c (z), but cz (x) = c (x) for all x (z)."}, {"heading": "4 Interactive learning of half spaces with comparison-queries", "text": "In this section, we will limit our attention to the class Hd = {character (f): f: Rd \u2192 R} of semi-spaces in Rd, where, for simplicity's sake, we consider linear functions f (which correspond to homogeneous half-spaces).Our results extend to the inhomogeneous case, since inhomogeneous half-spaces in dimension d can be embedded as homogeneous half-spaces in dimension d + 1. The additional queries allowed are comparison queries. That is, a label query returns the answer to characters (f (x)) and a comparison query returns the answer to f (x1) \u2265 f (x2). In section 4.1, we present our upper limits of query complexity under two natural conditions: low complexity or large margin. In section 4.2, we present lower limits that show that these conditions are actually necessary to achieve a sublinear complexity of the query in the sample."}, {"heading": "4.1 Upper bounds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Bit-complexity", "text": "We formalize the limited bit complexity by assuming that X = [N] d, where [N] = {0,..,.,.,.,.,.,.,. each example can be represented by B = d logN bits. We represent a limit of query complexity that depends efficiently and logN. Variants of arguments we use for other standard methods of quantifying the limited bit complexity.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "4.1.2 Minimal-ratio and margin", "text": "The minimum ratio of X in relation to c is defined as follows: (1 / 2): (1 / 2): (1 / 2): (1 / 2): (1 / 2): (1): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (3): (2): (2): (2): (2): (2): (2): (3)."}, {"heading": "4.2 Lower bounds", "text": "In this section we show that (in the worst case) comparison queries are not advantageous if the bit complexity is large in dimension d \u2265 3 or if the dimension is large, even if the margin is large."}, {"heading": "4.2.1 Dimension d \u2265 3", "text": "We show that (in the worst case) comparison queries do not yield significant savings in query complexity (R3) = 11 functions for learning half spaces, already in R3. This is narrow because, as discussed in the introduction, comparative queries yield exponential savings in R2. 6We are using the standard notation of u + \u03b1A = {u + \u03b1a: a realizable sample of size n, in u-Rd-Rd.Theorem 4.11. Consider the class (R3, H3) of half spaces in R3, any algorithm that shows the labels of any realizable sample of size n must use n-Rd / label queries in the worst cases. We get thatCorollary 4.12. Then any algorithm that learns (R3, H3) with error and confidence at least 5 / 6 must use queries in some realizable distributions."}, {"heading": "4.2.2 Margin", "text": "We show here that, in the worst case, comparative queries do not yield significant savings in terms of the complexity of the query, even if it is guaranteed that the margin is large, say at least 1 / 8.Theorem 4.15. For each n there is a class (X, H), in which X Rn + 1, and H Hn + 1 contains all halves with margin at least 1 / 8, so the following formula applies: Any algorithm that shows the designations of each realizable size sample n must apply a comparison / designation query in the worst case. In the statistical setting, we get these halves with margin at least 1 / 8. For each > 0 there is n and a class (X, H), in which X Rn + 1, and H Hn + 1 contains all halves with margin at least 1 / 8, so that the following applies: any algorithm that learns (X, H) with error and confidence at least 5 / 6 must be used."}, {"heading": "5 Acknowledgements", "text": "This work benefited from various discussions during the Special Program on the Fundamentals of Machine Learning held at the Simons Institute for the Theory of Computing in Berkeley. In particular, the authors would like to thank Sanjoy Dasgupta for stimulating the focus on comparative issues."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We study an extension of active learning in which the learning algorithm may ask the annotator to compare the distances of two examples from the boundary of their label-class. For example, in a recommendation system application (say for restaurants), the annotator may be asked whether she liked or disliked a specific restaurant (a label query); or which one of two restaurants did she like more (a comparison query). We focus on the class of half spaces, and show that under natural assumptions, such as large margin or bounded bit-description of the input examples, it is possible to reveal all the labels of a sample of size n using approximately O(log n) queries. This implies an exponential improvement over classical active learning, where only label queries are allowed. We complement these results by showing that if any of these assumptions is removed then, in the worst case, \u03a9(n) queries are required. Our results follow from a new general framework of active learning with additional queries. We identify a combinatorial dimension, called the inference dimension, that captures the query complexity when each additional query is determined by O(1) examples (such as comparison queries, each of which is determined by the two compared examples). Our results for half spaces follow by bounding the inference dimension in the cases discussed above. \u2217Department of Computer Science and Engineering/Department of Mathematics, University of California, San Diego. dakane@ucsd.edu Supported by NSF Career Award ID 1553288. \u2020Department of Computer Science and Engineering, University of California, San Diego. slovett@cs.ucsd.edu. Research supported by NSF CAREER award 1350481. \u2021Department of Computer Science and Engineering, University of California, San Diego, Simons Institute for the Theory of Computing, Berkeley, and Max Planck Institute for Informatics, Saarbr\u00fccken, Germany. shaymoran1@gmail.com. \u00a7Department of Computer Science and Engineering, University of California, San Diego. jpeng.zhang@gmail.com. Research supported by NSF CAREER award 1350481. ar X iv :1 70 4. 03 56 4v 2 [ cs .L G ] 2 J un 2 01 7", "creator": "LaTeX with hyperref package"}}}