{"id": "1704.03617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning", "abstract": "Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks. We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.", "histories": [["v1", "Wed, 12 Apr 2017 04:38:18 GMT  (40kb)", "http://arxiv.org/abs/1704.03617v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["matthew riemer", "elham khabiri", "richard goodwin"], "accepted": false, "id": "1704.03617"}, "pdf": {"name": "1704.03617.pdf", "metadata": {"source": "CRF", "title": "IMPROVED TEXT ANALYTICS TRANSFER LEARNING", "authors": ["Matthew Riemer", "Elham Khabiri"], "emails": ["rgoodwin}@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.03 617v 1 [cs.C L] 12 Apr 201 7Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetfulness problem impedes the proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task target based on the idea of distillation and a mechanism that directly punishes forgetfulness on the common layer of representation during the knowledge integration phase of the training. We demonstrate our approach to a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks. We show that our technique outperforms networks that are attuned to the target task. Furthermore, we show from empirical evidence as well as examples that it does not forget useful knowledge from the source task, which is forgotten in the standard fine-tuning process. Surprisingly, we find that the first distillation of the knowledge-based gain of a regulated transmitter transmits the transmitter to a target machine."}, {"heading": "1 INTRODUCTION", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "2 RELATED WORK", "text": "In fact, most of us are able to outdo ourselves, \"he said.\" But it's not that we're able to change the world. \"He added,\" It's not that we're able to change the world. \"He added,\" It's not that we're able to change the world. \"He added,\" It's not that we're able to change the world. \"He added,\" But it's not that we're able to change the world. \""}, {"heading": "3 FORGETTING COST REGULARIZATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SEQUENTIAL KNOWLEDGE TRANSFER PROBLEM STATEMENT", "text": "In the problem of sequential knowledge transfer described in this paper, the training is initially carried out exclusively on the source task examples S, including the KS training examples (xSi, ySi), with xSi being the input representation and ySi the output representation. After the training at S is completed, we would now like to use the previous knowledge acquired in the model trained on S to improve the generalization on a new target task with examples T, which include KT training examples (xTi, yTi). Here, we assume that the input representations xSi and xTi are semantically aligned in the same representation space. Thus, if there is useful knowledge in S that applies in any direct or indirect way to the target task that is not present in T, we would expect a good approach to knowledge integration to generalize it better to the target task than it is possible to use the training data alone in T."}, {"heading": "3.2 FORGETTING COST FOR TUNING A TARGET TASK MODEL", "text": "The simplest application of our proposed paradigm of forgetting the costs is in the case of integrating a neural network trained on source task data S that has outputs in the same representation space as the outputs for target task data T. In this case, forgetting amounts to adding a regularization term in the objective function during the integration phase when we train the use of T. This promotes the neural network to be able to restore the soft labels of the initialized model that were found after the training on S before the integration with T. More formally expressed: Loss = L (y, y-ig) + \u03b1fL (yinit, y-ig) (1), where L is a loss function (we use the mean square error in our experiments) and yinit is the soft designation used for the target task input xTi based on the model after the training only on S, the model is easily generated to model the S that is also trained on the task type S."}, {"heading": "3.3 FORGETTING COST FOR KNOWLEDGE TRANSFER FROM A RELATED TASK", "text": "The assumption in Section 3.2 that the output of the source task data S should be in the same representation space as the output of the target task data T is quite large. It excludes the vast majority of knowledge sources that we can potentially use. Therefore, we propose an extension that does not impose this restriction for use in the sequential knowledge transfer of tasks that are not directly semantically aligned. We update our model to include another predicted output independently of y: y-init = finit (Wfixedhshared + bfixed) (2), where y-init is a predicted output that attempts to restore the soft labels of the original model that was trained only on S. finit, is the nonlinearity that is used in the last layer of the source task model. Weight matrix Wfixed and bias bfixed are taken from the last layer of the source task model and are inferred to the soft labels of the original model that was trained only on S. finit, the non-linearity that is used in the last layer of the source task model is invisible. (Weight matrix Wfixed and bias bfixed are taken from the last layer of the source task model and are inferred to the soft labels of the original model trained on S. finit only on S. finit, the nonlinearity is used in the last layer of the source task model and is not updated during integration with the target task data."}, {"heading": "4 RECURRENT NEURAL NETWORK MODEL", "text": "In recent years, recursive neural network models have become a tool of choice for many NLP tasks. In particular, the LSTM variant (Hochreiter & Schmidhuber, 1997) became popular because it mitigates the disappearing gradient problem (Bengio et al., \"1994), which halts the recursive neural networks from learning long-term dependencies on the input sequence. In our experiments, we use the simpler GRU network (Cho et al.,\" \"2014),\" which generally achieves the same accuracy despite a less complex architecture. \"Each time step t is defined by an input xt and a hidden state ht ht ht ht. The mechanics of the GRU are defined with the following equations: zt =\" Whzht \u2212 1) rt =, \"rt,\" \"\", \",\", \",\", \",\", \",\", \",\", \"\", \",\" \",\" \",\", \"\", \",\", \",\", \"\", \"\", \",\" \",\", \",\" \"\", \",\" \"\", \",\" \",\", \",\" \",\" \",\", \",\", \"\", \"\", \",\" \",\", \"\" \",\", \",\" \"\", \",\" \",\", \"\", \"\", \",\" \",\" \"\", \"\" \",\" \"\", \"\", \",\" \",\" \",\" \"\" \",\", \"\" \"\", \",\", \",\", \"\", \",\", \"\" \"\", \"\" \",\", \",\" \"\" \",\", \"\" \"\" \",\" \"\" \",\", \"\" \"\", \",\" \"\" \",\", \"\" \",\", \",\" \",\" \"\" \"\" \",\", \"\", \"\" \",\" \",\", \"\", \",\", \"\", \"\", \"\" \",\" \",\" \"\" \",\", \"\", \",\", \"\" \"\", \"\" \",\" \"\", \",\" \",\", \",\" \",\" \""}, {"heading": "5 SEQUENTIAL KNOWLEDGE TRANSFER EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 EXPERIMENT DETAILS", "text": "rE \"s tis, so rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc"}, {"heading": "5.2 SEQUENTIAL KNOWLEDGE TRANSFER ALGORITHMS", "text": "We are looking at several sequential knowledge transfer algorithms for experimental comparison, each using only the source task data for learning the source task and only the target task data for integration with the target task. In this way, integration is quick and easy, as it does not involve random storage and repetition of examples from the potentially very large source task, as argued in (Li & Hoiem, 2016). Fine-tuning: the representation is simply initialized with the representation found on the source task after training and then trained on the target task as usual. This approach was advanced in (Hinton & Salakhutdinov, 2006), in application to unmonitored source tasks and applied to the implementation of learning tasks in (Bengio et al., 2012) and (Mesnil et al.) The learning rate is learned through a grid search based on the validation task. Progressive Networks: We are also comparing with our implementation of a progressive neural network task (Rusu), where the 2016 is learned."}, {"heading": "5.3 TARGET TASK RESULTS", "text": "Empirically, we evaluate the generalization performance of the cost of sequential knowledge transfer from four different source tasks in Table 1 and Table 2. The source task considered in Table 1 is the distillation of a logical rule model, using the technique outlined in Equation 1. In Table 2, we use the cost of the knowledge transfer in connection with tasks outlined in Equation 3. Our experimental results on the SemEval data confirm our intuition that the cost of forgetting should lead to stronger regulation and better generalization performance. However, our progressive implementation of neural networks is important to note that it effectively only has a hidden layer, because we keep our embedding fixed during model training and the same embedding is distributed among the models used for all tasks. It is possible that several layers of lateral connections are important to achieve good performance. However, this setting was not applicable in our experiments and our knowledge transfer results are based on the significant sequence of the Semmufer."}, {"heading": "5.4 SOURCE TASK PERFORMANCE AFTER TARGET TASK INTEGRATION", "text": "In Table 3 we examine the maintenance of empirical performance in the source task for knowledge transfer algorithms after integration with the target task. Obviously, in these cases it is actually destructive to re-learn the source task model during integration with the target task data. LwF performs significantly better in fine tuning for film reviews, but interestingly not in emoticon heuristics. Also, the effect of the greedy target task initialization strategy appears contradictory. It seems possible that this greedy initialization could improve our proposed forgetfulness paradigm in some cases. However, a rigorous analysis of the target task compromises for this initialization approach goes beyond the scope of this paper. As the source task representation is literally fixed as part of the target task representation in progressive neural networks, it is not clear how to evaluate effective forgetting the source task during task integration."}, {"heading": "5.5 INSPECTION OF LEARNED REPRESENTATIONS", "text": "Now that we have established the empirical benefits of our proposed cost forgetfulness, we will show what this model achieves qualitatively through examples. In Table 4, we will insert a selection of examples that are correctly predicted by transferring the source of knowledge to the cost forgetfulness paradigm rather than fine-tuning integration, the effect being perhaps the easiest to understand for rule-based transfer scenarios and movie reviews. In the rules-based transfer setting, you can literally map insights that are not forgotten by their respective logical rule in the model, as is the case in these examples. In addition, we can see how movie domain-specific terminology such as \"May the force be with\" appears to be forgotten in standard fine-tuning, but not when applying the forgotten cost regulation. Considering that we have shown that a neural Network can distill and improve a representation learned through a logical control machine, the final representation differs from the original logic of the model."}, {"heading": "6 INTEGRATING TRANSFER LEARNING FROM MULTIPLE TASKS WITH ENSEMBLE DISTILLATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 ENSEMBLE METHODOLOGY", "text": "In our experiments, we tried to find a balance between an ensemble model that is strong enough to have an adaptive weighted average decision function and not so strong that it fits our limited training and validation data. Our model, in its architecture of the gating network component, resembles a hierarchical mix of expert models (Jacobs et al., 1991), (Jordan & Jacobs, 1994). We tried our model over all four representations at once and found that it fits. (Riemer et al., 2016) Our experiments have shown that it is more effective to pursue a greedy enabling strategy in which all models are combined at each stage with the most powerful model of the validation phase until only two models are left. (Riemer et al., 2016) Our experiments suggest that many elements of the gating network can be improved with a thrift restriction, but this did not work as well as the gifted strategy for our model A and we combine the A and the following models for a chastic element B and the following two, whereby we create a chain B and ensemble B."}, {"heading": "6.2 ENSEMBLE RESULTS", "text": "In the second phase, the model based on transfer from the binary Movie Review Sentiment Model was combined with each model, and in the third phase, the two remaining models were combined, and the results of our ensemble in Table 5 suggest that it is possible to further improve the performance of a single sequential transfer model by intelligently combining its predictions with models that have different perspectives, because they are modeled using different source tasks for prior knowledge. Impressively, our final distilled model exceeds the results from all previous models on the Benchmark SemEval 2016 by using the same final architecture of a 50 hidden unit GRU model, which is clearly not even competitive when trained simply on the task-specific designated data."}, {"heading": "7 CONCLUSION", "text": "We have shown for the challenging task of Twitter sentiment analysis that it can uncover significant gains in generalization performance, and that it does not seem to have forgotten knowledge that has traditionally been forgotten in fine-tuning the source task. Our strong empirical results still motivate several avenues with high potential for further research into text analysis. Using logical rules to improve neural network models is a promising direction for humans to efficiently contribute to improved model performance. Furthermore, the wide variety of representations learned by multiple classifiers with the same goal, but different source tasks, seems to suggest that much greater gains are possible when integrating multiple sources of knowledge transfer."}, {"heading": "A MAPPING SENTIMENT RULES TO SOFT TARGETS", "text": "The gazetteer-based logical control machine separates sentences and phrases in the text, then applies dictionaries of positive and negative sentiment words and phrases to the corresponding text. For each found positive or negative phrase, it checks whether negation or double negation is applied, and modifies the polarity of sentiment accordingly. The result for each piece of text is a count of positive and negative sentiment occurrences. For this task, we simply count the total number of positive and negative indicators to obtain a positive, negative or neutral total value. Specifically, we have a simple procedure for assigning positive and negative word numbers to soft labels that could be used for distillation. If there are no positive or negative words, the output vector is a hot vector that corresponds to a neutral designation. If there is an unequal number of positive and negative sentiment words, the neutral designation is zero, and the raw number is sent to the positive function to generate a max articulate."}, {"heading": "B SIZE SELECTION FOR THE RULE DISTILLATION TASK", "text": "In Table 6, we describe in detail the performance of distillation of a logical control machine into a GRU-based recurrent neural network by imposing soft labels on unmarked tweets. The fact that we fix our word representations with unattended data for general purposes makes it difficult for the GRU to distill the entire model without a large number of examples. As there were a large number of examples in our distillation experiments, we did not experience a high deviation rate and trained only a single GRU model for each distillation experiment (as opposed to selecting the best validation error of 10 parallel training routines as in our transfer experiments).Our distilled GRU is better based on the test set than the original classifier, probably because this input representation prevents the model from being reworked to the idiosyncrasies of the control experiment.This actually underscores an important point for distillation of the abstract knowledge task we choose as the training model is based on the original distillation during the initial distillation, when the training model is complete based on the knowledge we know during the initial distillation."}], "references": [{"title": "Unipi at semeval-2016 task 4: Convolutional neural networks for sen-timent classification", "author": ["Giuseppe Attardi", "Daniele Sartiano"], "venue": "Proceedings of SemEval,", "citeRegEx": "Attardi and Sartiano.,? \\Q2016\\E", "shortCiteRegEx": "Attardi and Sartiano.", "year": 2016}, {"title": "Do deep nets really need to be deep? In Advances in neural information processing", "author": ["Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "Ba and Caruana.,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana.", "year": 2014}, {"title": "Learning long-term dependencieswith gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "ICML Unsupervised and Transfer Learning,", "citeRegEx": "Bengio,? \\Q2012\\E", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "The Association for Computer Linguistics. ISBN 978-1-941643-95-2", "author": ["Steven Bethard", "Daniel M. Cer", "Marine Carpuat", "David Jurgens", "Preslav Nakov", "Torsten"], "venue": "Zesch (eds.). Proceedings of the 10th International Workshop on Semantic Evaluation,", "citeRegEx": "Bethard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bethard et al\\.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1511.05641,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Neural-symbolic learning systems: foundations and applications", "author": ["Artur S d\u2019Avila Garcez", "Krysia Broda", "Dov M Gabbay"], "venue": null, "citeRegEx": "Garcez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garcez et al\\.", "year": 2012}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang"], "venue": null, "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507,", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Harnessing deep neural networks with logic rules", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing"], "venue": "arXiv preprint arXiv:1603.06318,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Adaptive mixtures of local experts", "author": ["Robert A Jacobs", "Michael I Jordan", "Steven J Nowlan", "Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "Jacobs et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 1991}, {"title": "Ntnusenteval at semeval-2016 task 4: Combining general classifiers for fast twitter sentiment analysis", "author": ["Brage Ekroll Jahren", "Valerij Fredriksen", "Bj\u00f6rn Gamb\u00e4ck", "Lars Bungum"], "venue": "Proceedings of SemEval,", "citeRegEx": "Jahren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jahren et al\\.", "year": 2016}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["Michael I Jordan", "Robert A Jacobs"], "venue": "Neural computation,", "citeRegEx": "Jordan and Jacobs.,? \\Q1994\\E", "shortCiteRegEx": "Jordan and Jacobs.", "year": 1994}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Learning without forgetting", "author": ["Zhizhong Li", "Derek Hoiem"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Li and Hoiem.,? \\Q2016\\E", "shortCiteRegEx": "Li and Hoiem.", "year": 2016}, {"title": "Unifying distillation and privileged", "author": ["David Lopez-Paz", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "Vladimir Vapnik"], "venue": "information. stat,", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Learning and categorization in modular neural networks", "author": ["Jacob MJ Murre"], "venue": null, "citeRegEx": "Murre.,? \\Q1992\\E", "shortCiteRegEx": "Murre.", "year": 1992}, {"title": "Cufe at semeval-2016 task 4: A gated recurrent model for sentiment classification", "author": ["Mahmoud Nabil", "Mohamed Aly", "Amir F Atiya"], "venue": "Proceedings of SemEval,", "citeRegEx": "Nabil et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nabil et al\\.", "year": 2016}, {"title": "Semeval2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanovand", "Fabrizio Sebastiani"], "venue": "In Proc. of the 10th International Workshop on Semantic Evaluation (SemEval),", "citeRegEx": "Nakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A deep learning and knowledge transfer based architecture for social media user characteristic determination", "author": ["Matthew Riemer", "Sophia Krasikov", "Harini Srinivasan"], "venue": "SocialNLP 2015 at NAACL, pp", "citeRegEx": "Riemer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Riemer et al\\.", "year": 2015}, {"title": "Correcting forecasts with multifactor neural attention", "author": ["Matthew Riemer", "Aditya Vempaty", "Flavio Calmon", "Fenno Heath", "Richard Hull", "Elham Khabiri"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Riemer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riemer et al\\.", "year": 2016}, {"title": "Catastrophic forgetting, rehearsal and pseudorehearsal", "author": ["Anthony Robins"], "venue": "Connection Science,", "citeRegEx": "Robins.,? \\Q1995\\E", "shortCiteRegEx": "Robins.", "year": 1995}, {"title": "Consolidation in neural networks and in the sleeping brain", "author": ["Anthony Robins"], "venue": "Connection Science,", "citeRegEx": "Robins.,? \\Q1996\\E", "shortCiteRegEx": "Robins.", "year": 1996}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bengio.", "year": 2014}, {"title": "aspect-based sentiment analysis", "author": ["A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirk"], "venue": "arXiv preprint arXiv:1609.02748,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Improved semantic representations", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q1631\\E", "shortCiteRegEx": "Tai et al\\.", "year": 1631}], "referenceMentions": [{"referenceID": 25, "context": "In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.", "startOffset": 53, "endOffset": 73}, {"referenceID": 32, "context": "While one can argue that forgetting the source task should not matter if only the target task is of interest, our paper adds to the recent empirical evidence across problem domains (Li & Hoiem, 2016),(Rusu et al., 2016) that show additional network stability can lead to empirical benefits over the fine-tuning algorithm.", "startOffset": 200, "endOffset": 219}, {"referenceID": 23, "context": "One popular approach for addressing this problem is rehearsals (Murre, 1992), (Robins, 1995).", "startOffset": 63, "endOffset": 76}, {"referenceID": 29, "context": "One popular approach for addressing this problem is rehearsals (Murre, 1992), (Robins, 1995).", "startOffset": 78, "endOffset": 92}, {"referenceID": 5, "context": "In the transfer setting it can be seen as related to multi-task learning (Caruana, 1997) where two tasks are trained at the same time, rather than sequentially, while sharing a common input encoder to a shared hidden representation.", "startOffset": 73, "endOffset": 88}, {"referenceID": 5, "context": "This technique is very sensible because while fine-tuning is susceptible to catastrophic forgetting, multi-task learning is not (Caruana, 1997).", "startOffset": 128, "endOffset": 143}, {"referenceID": 29, "context": "One compelling technique for addressing this problem is the concept of pseudorehearsals (Robins, 1995), (Robins, 1996), where relearning is performed on an artificially constructed population of pseudoitems instead of the actual old examples.", "startOffset": 88, "endOffset": 102}, {"referenceID": 30, "context": "One compelling technique for addressing this problem is the concept of pseudorehearsals (Robins, 1995), (Robins, 1996), where relearning is performed on an artificially constructed population of pseudoitems instead of the actual old examples.", "startOffset": 104, "endOffset": 118}, {"referenceID": 29, "context": "One compelling technique for addressing this problem is the concept of pseudorehearsals (Robins, 1995), (Robins, 1996), where relearning is performed on an artificially constructed population of pseudoitems instead of the actual old examples. Unfortunately, current automatic techniques in the text analytics domain have not yet mastered producing linguistically plausible data. As such, the pseudorehearsals paradigm is likely to waste computational time that could be spent on learning realistic patterns that may occur during testing. In our work, we extend the Learning without Forgetting (LwF) paradigm of (Li & Hoiem, 2016) to the text analytics domain using Recurrent Neural Networks. In this approach, the target task data is used both for learning the target task and for rehearsing information learned from the source task by leveraging synthetic examples generated for the target task input by the model that only experienced training on the source task data. As argued by Li & Hoiem (2016), this setup strikes an important balance between classification performance, computational efficiency, and simplicity in deployment.", "startOffset": 89, "endOffset": 1002}, {"referenceID": 5, "context": "One reason this is idealistic is because multi-task learning generally only works well when tasks are sampled at different rates or alternatively given different priority in the neural network loss function (Caruana, 1997).", "startOffset": 207, "endOffset": 222}, {"referenceID": 7, "context": "Moreover, we showcase that multiple expert networks trained on the target task with prior knowledge from different source tasks can be effectively combined in an ensemble and then distilled into a single GRU model (Cho et al., 2014), (Chung et al.", "startOffset": 214, "endOffset": 232}, {"referenceID": 8, "context": ", 2014), (Chung et al., 2014).", "startOffset": 9, "endOffset": 29}, {"referenceID": 11, "context": ", 2006) and (Hinton et al., 2015) showed that an ensemble of neural network classifier can be distilled into a single model, knowledge distillation from a teacher network to a student network has become a growing topic of neural network research.", "startOffset": 12, "endOffset": 33}, {"referenceID": 20, "context": "The use of distillation as a means of sharing biases from multiple tasks was explored in (Lopez-Paz et al., 2016), where the teacher network is trained with the output of the other tasks as input.", "startOffset": 89, "endOffset": 113}, {"referenceID": 6, "context": "Additionally, the concept of using distillation for knowledge transfer was also explored in (Chen et al., 2015), where function preserving transformations from smaller to bigger neural network architectures were outlined.", "startOffset": 92, "endOffset": 111}, {"referenceID": 5, "context": "In (Ba & Caruana, 2014) it was shown that a deep teacher neural network can be learned by a shallow student network. This idea was extended in (Romero et al., 2014), where it was demonstrated that a deep and narrow neural network can learn a representation that surpasses its teacher. The use of distillation as a means of sharing biases from multiple tasks was explored in (Lopez-Paz et al., 2016), where the teacher network is trained with the output of the other tasks as input. It is not obvious how to extend a recurrent neural network to best use this kind of capability over a sequence. The idea of distilling from multiple source task teachers into a student network was highlighted in the reinforcement learning setting in (Rusu et al., 2015). Additionally, the concept of using distillation for knowledge transfer was also explored in (Chen et al., 2015), where function preserving transformations from smaller to bigger neural network architectures were outlined. This technique could also provide value in some instances for our approach where wider or deeper neural networks are needed for the task being transferred to than was needed for the original task. Distillation over target task data was first proposed as a means of elevating catastrophic forgetting in sequential knowledge transfer as applied to image classification in (Li & Hoiem, 2016). We extend this approach for its first application to our knowledge for text analytics problems, with a recurrent neural network architecture, and in the setting where the source task and target task have the same output. The chief distinction of our proposed forgetting cost is that source task specific parameters are held fixed during integration with the target task as opposed to the joint training of all parameters used by Li & Hoiem (2016). Our experiments empirically support the intuition that freezing these parameters leads to greater retention of source task performance after target task integration and better generalization to the target task.", "startOffset": 9, "endOffset": 1812}, {"referenceID": 22, "context": "An ensemble over multiple diverse models trained for the same sentiment analysis task was also considered in (Mesnil et al., 2014) for the IMDB binary movie reviews sentiment dataset (Maas et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 21, "context": ", 2014) for the IMDB binary movie reviews sentiment dataset (Maas et al., 2011).", "startOffset": 60, "endOffset": 79}, {"referenceID": 27, "context": "Knowledge transfer from multiple tasks was considered to estimate the age of Twitter users based on the content of their tweets in (Riemer et al., 2015).", "startOffset": 131, "endOffset": 152}, {"referenceID": 32, "context": "Progressive neural networks (Rusu et al., 2016) is a recently proposed method very similar in motivation to our forgetting cost as it is directly trying to solve the catastrophic forgetting problem.", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "These neural-symbolic systems (Garcez et al., 2012) include early examples such as KBANN (Towell et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": "(Hu et al., 2016) very recently also looked at the problem of distilling logical rules into a neural network text analytics classifier.", "startOffset": 0, "endOffset": 17}, {"referenceID": 14, "context": "In (Hu et al., 2016) they consider the individual rules and leverage an iterative convex optimization algorithm alongside the neural network to regularize the subspace of the network.", "startOffset": 3, "endOffset": 20}, {"referenceID": 2, "context": "In particular, the LSTM variant (Hochreiter & Schmidhuber, 1997) has become popular as it alleviates the vanishing gradients problem (Bengio et al., 1994) known to stop recurrent neural networks from learning long term dependencies over the input sequence.", "startOffset": 133, "endOffset": 154}, {"referenceID": 7, "context": "In our experiments we use the simpler GRU network (Cho et al., 2014), (Chung et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 8, "context": ", 2014), (Chung et al., 2014) that generally achieves the same accuracy despite a less complex architecture.", "startOffset": 9, "endOffset": 29}, {"referenceID": 18, "context": "A model that builds on top of GRUs with an external memory storage paradigm (Kumar et al., 2015) currently holds the state of the art on movie review sentiment analysis.", "startOffset": 76, "endOffset": 96}, {"referenceID": 26, "context": "Our GRU model was fed a sequence of fixed 300 dimensional Glove vectors (Pennington et al., 2014), representing words based on analysis of 840 billion words from a common crawl of the internet, as the input xt for all tasks.", "startOffset": 72, "endOffset": 97}, {"referenceID": 11, "context": "We distill off the soft labels as in (Hinton et al., 2015), but set our temperature fixed at 1.", "startOffset": 37, "endOffset": 58}, {"referenceID": 10, "context": "Emoticon Heuristic: Finally, we consider a semi-supervised task based on emoticon prediction motivated by the successful work in (Go et al., 2009), leveraging it in the twitter sentiment domain and its use as a vital component of the SemEval competition winning system (Bethard et al.", "startOffset": 129, "endOffset": 146}, {"referenceID": 4, "context": ", 2009), leveraging it in the twitter sentiment domain and its use as a vital component of the SemEval competition winning system (Bethard et al., 2016).", "startOffset": 130, "endOffset": 152}, {"referenceID": 4, "context": "This is multiple orders of magnitude smaller than the 90 million tweets used in (Bethard et al., 2016) to allow for quick experimentation.", "startOffset": 80, "endOffset": 102}, {"referenceID": 32, "context": "Progressive Networks: We also compare with our implementation of a progressive neural network (Rusu et al., 2016), where the representation learned for the source task is held fixed and integrated with a target task specific model via lateral connections trained using the target task data.", "startOffset": 94, "endOffset": 113}, {"referenceID": 5, "context": "This is achieved by treating the target task data and the output generated by the source task model based on the target task input data as two jointly learned tasks as in (Caruana, 1997).", "startOffset": 171, "endOffset": 186}, {"referenceID": 15, "context": "Our model is quite similar in architecture to the gating network component of a hierarchical mixture of experts model (Jacobs et al., 1991), (Jordan & Jacobs, 1994).", "startOffset": 118, "endOffset": 139}, {"referenceID": 28, "context": "(Riemer et al., 2016) suggests that a many element gating network can be improved with a sparsity constraint, but this did not work as well as the greedy strategy for our model and experiments.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "6% SwissCheese (Bethard et al., 2016) 64.", "startOffset": 15, "endOffset": 37}, {"referenceID": 16, "context": "6% NTNUSentEval (Jahren et al., 2016) 64.", "startOffset": 16, "endOffset": 37}, {"referenceID": 24, "context": "9% CUFE (Nabil et al., 2016) 63.", "startOffset": 8, "endOffset": 28}, {"referenceID": 4, "context": "The prior best model SwissCheese (Bethard et al., 2016) consists of random forests ensemble built utilizing multiple convolutional neural network models and distant supervision.", "startOffset": 33, "endOffset": 55}], "year": 2017, "abstractText": "Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks. We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.", "creator": "LaTeX with hyperref package"}}}