{"id": "1704.03718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Deep Extreme Multi-label Learning", "abstract": "Extreme multi-label learning or classification has been a practical and important problem since the boom of big data. The main challenge lies in the exponential label space which involves 2L possible label sets when the label dimension L is very large e.g. in millions for Wikipedia labels. This paper is motivated to better explore the label space by build- ing and modeling an explicit label graph. In the meanwhile, deep learning has been widely studied and used in various classification problems includ- ing multi-label classification, however it has not been sufficiently studied in this extreme but practi- cal case, where the label space can be as large as in millions. In this paper, we propose a practical deep embedding method for extreme multi-label classifi- cation. Our method harvests the ideas of non-linear embedding and modeling label space with graph priors at the same time. Extensive experiments on public datasets for XML show that our method per- form competitively against state-of-the-art result.", "histories": [["v1", "Wed, 12 Apr 2017 12:09:40 GMT  (367kb,D)", "http://arxiv.org/abs/1704.03718v1", "7 pages, 7 figures"], ["v2", "Mon, 11 Sep 2017 07:46:31 GMT  (262kb,D)", "http://arxiv.org/abs/1704.03718v2", "9 pages, 7 figures"], ["v3", "Thu, 19 Oct 2017 15:59:32 GMT  (287kb,D)", "http://arxiv.org/abs/1704.03718v3", "9 pages, 8 figures"]], "COMMENTS": "7 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wenjie zhang", "liwei wang", "xiangfeng wang", "junchi yan", "hongyuan zha"], "accepted": false, "id": "1704.03718"}, "pdf": {"name": "1704.03718.pdf", "metadata": {"source": "CRF", "title": "Deep Extreme Multi-label Learning", "authors": ["Wenjie Zhang", "Liwei Wang", "Junchi Yan", "Xiangfeng Wang", "Hongyuan Zha"], "emails": ["izhangwenjie@gmail.com", "lwang97@illinois.edu", "jcyan@sei.ecnu.edu.cn", "xfwang@sei.ecnu.edu.cn", "zha@sei.ecnu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Classification methods for XML", "text": "It is not only a matter of time, but also a matter of time before it will come to an outcome. (...) It is a matter of time before it will come to an outcome. (...) It is a matter of time before it will come to an outcome. (...) It is a matter of time before it will come to an outcome. (...) It is a matter of time before it will come to an outcome. (...) It is a matter of time before it will come to an outcome. \"(...) It is a matter of time before it will come to an outcome. (...) It is a matter of time before it will come to an outcome. (...) It is a matter of time until it will come to an outcome.\" (...) It is a matter of time before it will come to an outcome. \"(...) It is a matter of time until it will come to an outcome.\""}, {"heading": "2.2 Traditional multi-label classification", "text": "Traditionally, this problem is addressed with a moderate number of labels [Tsoumakas and Katakis, 2006], which differentiates it from the XML problem, which involves millions or more labels for each sample. Early MLC methods [Boutell et al., 2004] turn the MLC problem into either one or more single-label classification or regression problems. Newer approaches [Cheng et al., 2010; Bi and Kwok, 2011] attempt to directly solve multi-label learning, but if the number of labels grows rapidly, these methods can easily become prohibitive computationally. For example, for tree-based models for traditional MLC [Zhang and Zhang, 2010; Bi and Kwok, 2011], with the large feature dimension and huge sample count, the trees will be huge, resulting in intractability for training. There is also a principle generalization for the one-label-against-all method, which is not effective."}, {"heading": "3 Deep Learning for XML", "text": "Our approach encompasses the training and prediction phase. In the training phase, we map the high-dimensional attribute and the high-dimensional label vectors in a common embedding space via two neural networks. In the prediction phase, a standard k-NN-based classifier is used in the embedded attribute space to determine the final label predictions."}, {"heading": "3.1 Training stage", "text": "In fact, it is such that it is a matter of a way in which people are able to determine for themselves how they want to live. (...) In fact, it is such that people are able to decide what they want. (...) In fact, it is such that people are able to decide what they want. (...) \"It is such that people are able to decide what they want. (...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" ((...) \"(...)\" ((...) \"((...)\" (() ((...) \"() (...)\" () (() (() () () () () () () () () () () () () () () ()) () () () () () () () () () () ()) () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () ()) () () () () ()) () () () () () () () () () ()) () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () (() () () (() () () (() () () () () () () () (() () () (() (() () (() () () (() () () ((() () () (() () () () (("}, {"heading": "3.2 Prediction stage", "text": "The prediction is relatively simple and standard, which is in line with SLEEC [Bhatia et al., 2015]: In the XML setting, similar to [Wang et al., 2016], the positive-negative sample can be defined with a certain label dimension. However, since there are millions of label dimensions, the number of derived positive-negative sample pairs can be intimidating. We perform k-NN searches in its low-dimensional characteristic representation to find its similar samples from the training data set. Then, the average (or simply the sum) of the labels of the k-nearest neighbors is set as the final label prediction."}, {"heading": "4 Experiments", "text": "Experiments are being conducted with publicly available XML benchmark data from the Extreme Classification Repository3, which includes both small-scale datasets [Prabhu and Varma, 2014] and large-scale datasets [Bhatia et al., 2015] compared to state-of-the-art peer methods for both embedding-based and tree-based models. The source code of our method will be published with the publication of this paper."}, {"heading": "4.1 Protocol", "text": "The experiments are implemented under CentOS6.5-bit system, with Intel (R) Xeon (R) CPU E5-2680 v2 @ 2.80GHz \u00d7 6 CPU, GeForce GTX TITAN-X and 32G RAM. Code is written in Python 2.7 and the network is built by MXNet4, which is a flexible and efficient library for deep learning and has been chosen by Amazon as the official deep learning framework for its web services. Datasets The multi-label datasets tested include WikiLSHTC (320K labels), DeliciousLarge Large (Wetzker et al, 200K labels) and Wikipedia (Zubiaga, 2012 labels).It should be noted that some other methods do not scale well on such large datasets."}, {"heading": "4.2 Results and discussion", "text": "In general, our method DXML outperforms almost all other methods except SLEEC.Results on small datasets From Table 2, one can see that our method DXML can be largely classified as top 2 on all four datasets. On MediaMill, DXML outperforms LEML and FastXML by almost 4% on P @ 3 and P @ 5. On EurLEX, DXML outperforms LEML and FastXML by about 10% and 5% respectively. While on Bibtex, it slightly underperforms other methods. The main reason why we suspect that the Bibtex dataset has only 4,880 samples that may be a problem for the formation of our deep neural network is the optimization of large datasets As observed from Table 3, on the Wiki10 dataset, DXML outperforms LEML and LPSRNB by about 10%."}, {"heading": "5 Conclusion", "text": "Regarding the extreme multi-label learning problem (XML), this work begins with modeling the large label space over the label graph. In contrast, existing XML methods either examine the label hierarchy as with many tree-based methods, or perform dimension reduction using the raw label / sample matrix. In addition, a deep neural network is developed to effectively explore the label space. We also study deep neural networks to learn the feature space embedding function induced by embedding the label space. Extensive experimental results confirm the effectiveness of our method."}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Rahul Agrawal", "Archit Gupta", "Yashoteja Prabhu", "Manik Varma"], "venue": "WWW,", "citeRegEx": "Agrawal et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Dismec-distributed sparse machines for extreme multi-label classification", "author": ["Rohit Babbar", "Bernhard Shoelkopf"], "venue": "WSDM,", "citeRegEx": "Babbar and Shoelkopf. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "The landmark selection method for multiple output prediction", "author": ["Krishnakumar Balasubramanian", "Guy Lebanon"], "venue": "ICML,", "citeRegEx": "Balasubramanian and Lebanon. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Fullyconvolutional siamese network for object tracking", "author": ["Luca Bertinetto", "Jack Valmadre", "Joao F. HenriquesAndrea", "Vedaldi", "Philip H.S. Torr"], "venue": "ECCV,", "citeRegEx": "Bertinetto et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Prateek Jain", "Manik Varma"], "venue": "NIPS,", "citeRegEx": "Bhatia et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-label classification on tree-and dag-structured hierarchies", "author": ["Wei Bi", "James T Kwok"], "venue": "ICML,", "citeRegEx": "Bi and Kwok. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient multi-label classification with many labels", "author": ["Wei Bi", "James Tin-Yau Kwok"], "venue": "ICML,", "citeRegEx": "Bi and Kwok. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Pattern recognition", "author": ["Matthew R Boutell", "Jiebo Luo", "Xipeng Shen", "Christopher M Brown. Learning multi-label scene classification"], "venue": "37(9):1757\u20131771,", "citeRegEx": "Boutell et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Featureaware label space dimension reduction for multi-label classification", "author": ["Yao-Nan Chen", "Hsuan-Tien Lin"], "venue": "NIPS,", "citeRegEx": "Chen and Lin. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "arXiv preprint arXiv:1512.01274,", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["Weiwei Cheng", "Eyke H\u00fcllermeier", "Krzysztof J Dembczynski"], "venue": "ICML,", "citeRegEx": "Cheng et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust bloom filters for large multilabel classification tasks", "author": ["Moustapha M Cisse", "Nicolas Usunier", "Thierry Artieres", "Patrick Gallinari"], "venue": "NIPS,", "citeRegEx": "Cisse et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "AISTATS,", "citeRegEx": "Glorot et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale max-margin multi-label classification with priors", "author": ["Bharath Hariharan", "Lihi Zelnik-Manor", "Manik Varma", "Svn Vishwanathan"], "venue": "ICML,", "citeRegEx": "Hariharan et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine learning", "author": ["Bharath Hariharan", "SVN Vishwanathan", "Manik Varma. Efficient max-margin multi-label classification with applications to zero-shot learning"], "venue": "88(1-2):127\u2013155,", "citeRegEx": "Hariharan et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S. Kakade", "J. Langford", "T. Zhang"], "venue": "NIPS", "citeRegEx": "Hsu et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["Ioannis Katakis", "Grigorios Tsoumakas", "Ioannis Vlahavas"], "venue": "ECML PKDD discovery challenge,", "citeRegEx": "Katakis et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient pairwise multilabel classification for large-scale problems in the legal domain", "author": ["E. Menca", "J. Furnkranz"], "venue": "ECML/PKDD", "citeRegEx": "Menca and Furnkranz. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Deepwalk: Online learning of social representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "KDD,", "citeRegEx": "Perozzi et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Fastxml: a fast", "author": ["Yashoteja Prabhu", "Manik Varma"], "venue": "accurate and stable tree-classifier for extreme multi-label learning. In KDD,", "citeRegEx": "Prabhu and Varma. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The challenge problem for automated detection of 101 semantic concepts in multimedia", "author": ["Cees GM Snoek", "Marcel Worring", "Jan C Van Gemert", "Jan-Mark Geusebroek", "Arnold WM Smeulders"], "venue": "ACM-MM,", "citeRegEx": "Snoek et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Alternating minimization algorithms: from blahut-arimoto to expectationmaximization", "author": ["J. Sullivan"], "venue": "A. Vardy, Ed., Codes, Curves, and Signals: Common Threads in Communications", "citeRegEx": "Sullivan. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Neural Computation", "author": ["Farbound Tai", "Hsuan-Tien Lin. Multilabel classification with principal label space transformation"], "venue": "24(9):2508\u20132542,", "citeRegEx": "Tai and Lin. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label classification: An overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining, 3(3),", "citeRegEx": "Tsoumakas and Katakis. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Effective and effcient multilabel classification in domains with large number of labels", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "ECML/PKDD", "citeRegEx": "Tsoumakas et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Max-margin deepwalk: Discriminative learning of network representation", "author": ["Cunchao Tu", "Weicheng Zhang", "Zhiyuan Liu", "Maosong Sun"], "venue": "IJCAI,", "citeRegEx": "Tu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep structure-preserving image-text embeddings", "author": ["Liwei Wang", "Yin Li", "Svetlana Lazebnik"], "venue": "CVPR,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Label partitioning for sublinear ranking", "author": ["Jason Weston", "Ameesh Makadia", "Hector Yee"], "venue": "ICML,", "citeRegEx": "Weston et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Analyzing social bookmarking systems: A del", "author": ["Robert Wetzker", "Carsten Zimmermann", "Christian Bauckhage"], "venue": "icio. us cookbook. In ECAI Mining Social Data Workshop,", "citeRegEx": "Wetzker et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Journal of Machine Learning Research", "author": ["Ting-Fan Wu", "Chih-Jen Lin", "Ruby C Weng. Probability estimates for multi-class classification by pairwise coupling"], "venue": "5(Aug):975\u2013 1005,", "citeRegEx": "Wu et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Network representation learning with rich text information", "author": ["Cheng Yang", "Zhiyuan Liu", "Deli Zhao", "Maosong Sun", "Edward Y. Chang"], "venue": "IJCAI,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In ICML", "author": ["Hsiang-Fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit S Dhillon. Large-scale multi-label learning with missing labels"], "venue": "pages 593\u2013601,", "citeRegEx": "Yu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In AISTATS", "author": ["Yi Zhang", "Jeff G Schneider. Multi-label output codes using canonical correlation analysis"], "venue": "pages 873\u2013882,", "citeRegEx": "Zhang and Schneider. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Multilabel learning by exploiting label dependency", "author": ["Min-Ling Zhang", "Kun Zhang"], "venue": "KDD,", "citeRegEx": "Zhang and Zhang. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhancing navigation on wikipedia with social tags", "author": ["Arkaitz Zubiaga"], "venue": "arXiv preprint arXiv:1202.5469,", "citeRegEx": "Zubiaga. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Compared with traditional multi-label classification methods [Tsoumakas and Katakis, 2006], extreme multi-label classification methods focus on tackling the problem of extremely high input dimensions for both input feature dimension and label dimension.", "startOffset": 61, "endOffset": 90}, {"referenceID": 29, "context": "It should also be emphasized that multi-label learning is distinct from multi-class classification [Wu et al., 2004] whose aim is to predict a single mutually exclusive label.", "startOffset": 99, "endOffset": 116}, {"referenceID": 4, "context": "However, such a hierarchy is often unavailable in many applications [Bhatia et al., 2015].", "startOffset": 68, "endOffset": 89}, {"referenceID": 27, "context": "Tree based methods [Weston et al., 2013; Agrawal et al., 2013; Prabhu and Varma, 2014] have become popular as they enjoy notable accuracy improvement over traditional embedding methods.", "startOffset": 19, "endOffset": 86}, {"referenceID": 0, "context": "Tree based methods [Weston et al., 2013; Agrawal et al., 2013; Prabhu and Varma, 2014] have become popular as they enjoy notable accuracy improvement over traditional embedding methods.", "startOffset": 19, "endOffset": 86}, {"referenceID": 19, "context": "Tree based methods [Weston et al., 2013; Agrawal et al., 2013; Prabhu and Varma, 2014] have become popular as they enjoy notable accuracy improvement over traditional embedding methods.", "startOffset": 19, "endOffset": 86}, {"referenceID": 4, "context": "Notably, the state-of-the-art embedding based method SLEEC (Sparse Local Embeddings for Extreme Multi-label Classification) [Bhatia et al., 2015] achieves significant accuracy gain while still being computationally economical, ar X iv :1 70 4.", "startOffset": 124, "endOffset": 145}, {"referenceID": 27, "context": "Tree based methods for XML The label partitioning by sub-linear ranking (LPSR) method [Weston et al., 2013] is focused on reducing the prediction time by learning a hierarchy over a base classifier.", "startOffset": 86, "endOffset": 107}, {"referenceID": 0, "context": "The multi-label random forest method (MLRF) [Agrawal et al., 2013] seeks to learn an ensemble", "startOffset": 44, "endOffset": 66}, {"referenceID": 1, "context": "It is also worth noting that the recent effort [Babbar and Shoelkopf, 2017] shows that, via intensive system level parallelization, a careful implementation of one-vs-rest mechanism is attainable with competitive accuracy against state-of-the-art FastXML [Prabhu and Varma, 2014] and SLEEC [Bhatia et al.", "startOffset": 47, "endOffset": 75}, {"referenceID": 19, "context": "It is also worth noting that the recent effort [Babbar and Shoelkopf, 2017] shows that, via intensive system level parallelization, a careful implementation of one-vs-rest mechanism is attainable with competitive accuracy against state-of-the-art FastXML [Prabhu and Varma, 2014] and SLEEC [Bhatia et al.", "startOffset": 255, "endOffset": 279}, {"referenceID": 4, "context": "It is also worth noting that the recent effort [Babbar and Shoelkopf, 2017] shows that, via intensive system level parallelization, a careful implementation of one-vs-rest mechanism is attainable with competitive accuracy against state-of-the-art FastXML [Prabhu and Varma, 2014] and SLEEC [Bhatia et al., 2015].", "startOffset": 290, "endOffset": 311}, {"referenceID": 19, "context": "FastXML [Prabhu and Varma, 2014] is proposed to learn a hierarchy not over the label space but over the feature space.", "startOffset": 8, "endOffset": 32}, {"referenceID": 15, "context": "In fact, the main difference of existing embedding models often lies in the choice of compression and decompression techniques for embedding and lifting, including compressed sensing [Hsu et al., 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 183, "endOffset": 201}, {"referenceID": 32, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 22, "endOffset": 49}, {"referenceID": 22, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 2, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 92, "endOffset": 146}, {"referenceID": 6, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 92, "endOffset": 146}, {"referenceID": 11, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al., 2013].", "startOffset": 162, "endOffset": 182}, {"referenceID": 4, "context": "The accuracy for embedding methods achieve significant gain by the recently proposed embedding method SLEEC [Bhatia et al., 2015].", "startOffset": 108, "endOffset": 129}, {"referenceID": 23, "context": "Traditionally this problem is tackled with a moderate number of labels [Tsoumakas and Katakis, 2006].", "startOffset": 71, "endOffset": 100}, {"referenceID": 7, "context": "Early MLC methods [Boutell et al., 2004] transform the MLC problem either into one or more single-label classification or regression problems.", "startOffset": 18, "endOffset": 40}, {"referenceID": 10, "context": "Recent approaches [Cheng et al., 2010; Bi and Kwok, 2011] try to solve the multi-label learning directly.", "startOffset": 18, "endOffset": 57}, {"referenceID": 5, "context": "Recent approaches [Cheng et al., 2010; Bi and Kwok, 2011] try to solve the multi-label learning directly.", "startOffset": 18, "endOffset": 57}, {"referenceID": 33, "context": "For instance, for tree based models for traditional MLC [Zhang and Zhang, 2010; Bi and Kwok, 2011], with the large feature dimension and the huge samples number, the trees will be giant which leads to the intractability for training.", "startOffset": 56, "endOffset": 98}, {"referenceID": 5, "context": "For instance, for tree based models for traditional MLC [Zhang and Zhang, 2010; Bi and Kwok, 2011], with the large feature dimension and the huge samples number, the trees will be giant which leads to the intractability for training.", "startOffset": 56, "endOffset": 98}, {"referenceID": 13, "context": "There is also a principled generalization for the naive one-against-all method [Hariharan et al., 2010] which cost-", "startOffset": 79, "endOffset": 103}, {"referenceID": 26, "context": "Meanwhile, in cross modal retrieval field, deep neural network [Wang et al., 2016] has been designed for learning the shared embedding space between images and texts.", "startOffset": 63, "endOffset": 82}, {"referenceID": 18, "context": "Motivated by the recent work in natural language processing for Network Representation Learning (NRL) [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al.", "startOffset": 102, "endOffset": 160}, {"referenceID": 30, "context": "Motivated by the recent work in natural language processing for Network Representation Learning (NRL) [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al.", "startOffset": 102, "endOffset": 160}, {"referenceID": 25, "context": "Motivated by the recent work in natural language processing for Network Representation Learning (NRL) [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al.", "startOffset": 102, "endOffset": 160}, {"referenceID": 30, "context": ", 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al., 2015; Tu et al., 2016] to define our label graph matrix by:", "startOffset": 87, "endOffset": 123}, {"referenceID": 25, "context": ", 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al., 2015; Tu et al., 2016] to define our label graph matrix by:", "startOffset": 87, "endOffset": 123}, {"referenceID": 30, "context": "1 and the readers are referred to [Yang et al., 2015] for details.", "startOffset": 34, "endOffset": 53}, {"referenceID": 30, "context": "In [Yang et al., 2015], the authors first prove the state-of-theart network representation method DeepWalk [Perozzi et al.", "startOffset": 3, "endOffset": 22}, {"referenceID": 18, "context": ", 2015], the authors first prove the state-of-theart network representation method DeepWalk [Perozzi et al., 2014] is equivalent to perform matrix factorization on a matrix M\u0304 \u2208 R|V |\u00d7|V | where each entry M\u0304ij is logarithm of the average probability that vertex vi randomly walks to vertex vj in t steps.", "startOffset": 92, "endOffset": 114}, {"referenceID": 30, "context": "The rigorous definition is [Yang et al., 2015]:", "startOffset": 27, "endOffset": 46}, {"referenceID": 30, "context": "2 in line with [Yang et al., 2015] since log(M\u0304) has much more non-zero entries than M\u0304 and the complexity of matrix factorization is proportional to the number of non-zero entries.", "startOffset": 15, "endOffset": 34}, {"referenceID": 25, "context": "This tradeoff is further verified in recent work [Tu et al., 2016] for text representation.", "startOffset": 49, "endOffset": 66}, {"referenceID": 21, "context": "the Alternating Minimization Algorithm (AMA) [Sullivan, 1998] given the real-valued matrix M:", "startOffset": 45, "endOffset": 61}, {"referenceID": 12, "context": "the ReLU [Glorot et al., 2015] (speedup convergence) modules in in Fig.", "startOffset": 9, "endOffset": 30}, {"referenceID": 26, "context": "2 There are various siamese (twin) network architectures used in many recent work on deep learning [Wang et al., 2016; Bertinetto et al., 2016].", "startOffset": 99, "endOffset": 143}, {"referenceID": 3, "context": "2 There are various siamese (twin) network architectures used in many recent work on deep learning [Wang et al., 2016; Bertinetto et al., 2016].", "startOffset": 99, "endOffset": 143}, {"referenceID": 26, "context": "However, here our loss is the `2 regression loss rather than the popular ranking loss used in [Wang et al., 2016] as we cannot efficiently derive useful ranking information from the training data2.", "startOffset": 94, "endOffset": 113}, {"referenceID": 18, "context": "Perhaps more importantly, we are inspired by the recent success for unsupervised representation learning for the label space [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] thus we use the network in Fig.", "startOffset": 125, "endOffset": 183}, {"referenceID": 30, "context": "Perhaps more importantly, we are inspired by the recent success for unsupervised representation learning for the label space [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] thus we use the network in Fig.", "startOffset": 125, "endOffset": 183}, {"referenceID": 25, "context": "Perhaps more importantly, we are inspired by the recent success for unsupervised representation learning for the label space [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] thus we use the network in Fig.", "startOffset": 125, "endOffset": 183}, {"referenceID": 4, "context": "The prediction is relatively simple and standard, which is in line with SLEEC [Bhatia et al., 2015]: given a test sample,", "startOffset": 78, "endOffset": 99}, {"referenceID": 26, "context": "In the XML setting, similar to [Wang et al., 2016], the positive/negataive sample can be defined with a certain label dimension.", "startOffset": 31, "endOffset": 50}, {"referenceID": 19, "context": "It includes both small-scale dataset [Prabhu and Varma, 2014] and large-scale dataset [Bhatia et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 4, "context": "It includes both small-scale dataset [Prabhu and Varma, 2014] and large-scale dataset [Bhatia et al., 2015], in comparison with state-of-the-art peer methods for both embedding based and tree based models.", "startOffset": 86, "endOffset": 107}, {"referenceID": 9, "context": "7 and the network is built by MXNet4 [Chen et al., 2015], which is a flexible and efficient library for deep learning and has been chosen by Amazon as the official deep learning framework for its web service.", "startOffset": 37, "endOffset": 56}, {"referenceID": 28, "context": "Datasets The tested multi-label datasets include WikiLSHTC (320K labels), DeliciousLarge [Wetzker et al., 2008] (200K labels) and Wiki10 [Zubiaga, 2012] (30K labels).", "startOffset": 89, "endOffset": 111}, {"referenceID": 34, "context": ", 2008] (200K labels) and Wiki10 [Zubiaga, 2012] (30K labels).", "startOffset": 33, "endOffset": 48}, {"referenceID": 16, "context": "Therefore, we also present comparisons on public relatively small datasets such as BibTex [Katakis et al., 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al.", "startOffset": 90, "endOffset": 136}, {"referenceID": 19, "context": "Therefore, we also present comparisons on public relatively small datasets such as BibTex [Katakis et al., 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al.", "startOffset": 90, "endOffset": 136}, {"referenceID": 20, "context": ", 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al., 2006; Prabhu and Varma, 2014], Delicious [Tsoumakas et al.", "startOffset": 43, "endOffset": 87}, {"referenceID": 19, "context": ", 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al., 2006; Prabhu and Varma, 2014], Delicious [Tsoumakas et al.", "startOffset": 43, "endOffset": 87}, {"referenceID": 24, "context": ", 2006; Prabhu and Varma, 2014], Delicious [Tsoumakas et al., 2008] and EURLex [Menca and Furnkranz, 2008].", "startOffset": 43, "endOffset": 67}, {"referenceID": 17, "context": ", 2008] and EURLex [Menca and Furnkranz, 2008].", "startOffset": 19, "endOffset": 46}, {"referenceID": 4, "context": "Baseline algorithms for comparison Our primary focus is to compare with those state-of-the-art extreme multi-label classification methods, such as embedding based methods SLEEC [Bhatia et al., 2015], LEML [Yu et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 31, "context": ", 2015], LEML [Yu et al., 2014] and tree based like FastXML [Prabhu and Varma, 2014] and LPSR [Weston et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 19, "context": ", 2014] and tree based like FastXML [Prabhu and Varma, 2014] and LPSR [Weston et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 27, "context": ", 2014] and tree based like FastXML [Prabhu and Varma, 2014] and LPSR [Weston et al., 2013].", "startOffset": 70, "endOffset": 91}, {"referenceID": 15, "context": "com/dmlc/mxnet niques such as compressed sensing (CS) [Hsu et al., 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 8, "context": ", 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 6, "context": ", 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 14, "context": ", 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al., 2012] can only be trained on small datasets using commodity computational hardware.", "startOffset": 77, "endOffset": 101}, {"referenceID": 4, "context": "Evaluation metrics The evaluation metric in [Bhatia et al., 2015] is precison@k, Precision at k (P@k)has been widely adopted as the choice of metric for evaluating extreme multilabel algorithms.", "startOffset": 44, "endOffset": 65}, {"referenceID": 21, "context": "3 by the AMA solver [Sullivan, 1998]).", "startOffset": 20, "endOffset": 36}], "year": 2017, "abstractText": "Extreme multi-label learning or classification has been a practical and important problem since the boom of big data. The main challenge lies in the exponential label space which involves 2 possible label sets when the label dimension L is very large e.g. in millions for Wikipedia labels. This paper is motivated to better explore the label space by building and modeling an explicit label graph. In the meanwhile, deep learning has been widely studied and used in various classification problems including multi-label classification, however it has not been sufficiently studied in this extreme but practical case, where the label space can be as large as in millions. In this paper, we propose a practical deep embedding method for extreme multi-label classification. Our method harvests the ideas of non-linear embedding and modeling label space with graph priors at the same time. Extensive experiments on public datasets for XML show that our method perform competitively against state-of-the-art result.", "creator": "LaTeX with hyperref package"}}}