{"id": "1704.03732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Learning from Demonstrations for Real World Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) has achieved several high profile successes in difficult control problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining temporal difference updates with large-margin classification of the demonstrator's actions. We show that DQfD has better initial performance than Deep Q-Networks (DQN) on 40 of 42 Atari games and it receives more average rewards than DQN on 27 of 42 Atari games. We also demonstrate that DQfD learns faster than DQN even when given poor demonstration data.", "histories": [["v1", "Wed, 12 Apr 2017 12:44:37 GMT  (555kb)", "http://arxiv.org/abs/1704.03732v1", null], ["v2", "Tue, 18 Jul 2017 13:19:52 GMT  (753kb,D)", "http://arxiv.org/abs/1704.03732v2", null], ["v3", "Fri, 21 Jul 2017 10:48:28 GMT  (758kb,D)", "http://arxiv.org/abs/1704.03732v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["todd hester", "matej vecerik", "olivier pietquin", "marc lanctot", "tom schaul", "bilal piot", "dan horgan", "john quan", "rew sendonaris", "gabriel dulac-arnold", "ian osband", "john agapiou", "joel z leibo", "audrunas gruslys"], "accepted": false, "id": "1704.03732"}, "pdf": {"name": "1704.03732.pdf", "metadata": {"source": "META", "title": "Learning from Demonstrations for Real World Reinforcement Learning", "authors": ["Todd Hester", "Olivier Pietquin", "Marc Lanctot", "Tom Schaul", "John Agapiou", "Joel Z. Leibo"], "emails": ["TODDHESTER@GOOGLE.COM", "MATEJVECERIK@GOOGLE.COM", "PIETQUIN@GOOGLE.COM", "LANCTOT@GOOGLE.COM", "SCHAUL@GOOGLE.COM", "PIOT@GOOGLE.COM", "SENDOS@GOOGLE.COM", "GABE@SQUIRRELSOUP.NET", "IOSBAND@GOOGLE.COM", "JAGAPIOU@GOOGLE.COM", "JZL@GOOGLE.COM", "AUDRUNAS@GOOGLE.COM"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.03 732v 1 [cs.A I] several high-profile successes in difficult control problems. However, these algorithms typically require an enormous amount of data before they achieve reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but limits the applicability of Deep RL to many real-world tasks where the agent must learn in the real environment. In this paper, we examine an environment in which the agent can access data from the previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), which uses this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining time difference updates with a generous classification of the actions of the demonstrator. We show that DQfD has a better starting performance than DQF-40 games we demonstrate faster than DQN-42 (also in QN) and QN-42."}, {"heading": "1. Introduction", "text": "In recent years, the number of those able to reform has multiplied."}, {"heading": "2. Background", "text": "We adopt the standard Markov Decision Process (MDP) formalism for this work (Sutton & Barto, 1998) Q = Q Updates. An MDP is defined by a tuple < S, A, R, T, \u03b3 > consisting of a number of states S, a set of measures A, a reward function R (s, a), a transitional function T (s, a, s) = P (s, s, a) and a discount factor \u03b3. In each state S, the agent takes a measure A, a reward function R (s, a), a reward function R (s, a) and reaches a new state s \u00b2 determined by the probability distribution P (s, a)."}, {"heading": "3. Deep Q-Learning from Demonstrations", "text": "In many settings of amplification learning, we have access to the data of the system operated by its previous controller Q, but we do not have access to an accurate simulator of the system. Therefore, we want the agent to learn as much as possible from the demonstration data before running on the real system. The goal of the pre-training phase is to learn to imitate the demonstrator with a value function that fulfills the Bellman equation so that it can be updated with TD updates as soon as the agent begins to interact with the environment. During this pre-training phase, the agent samples are applied mini-batches from the demonstration data and network updates by applying three losses: the double Q-learning loss, a supervised large margin loss, and an L2 regulation loss of the L2. The monitored loss is used for the classification of the demonstrator, while the Q-learning learning learning data is updated from the demonstration learning actions and the losses from the three."}, {"heading": "4. Experimental Results", "text": "For all our experiments, we evaluated three different algorithms, each averaged over four studies: \u2022 Full DQfD algorithm \u2022 Double DQN learning without demonstration data \u2022 Supervised imitation of demonstration data without ambient interaction. For DQfD, we first performed informal parameter tuning on four Atari games (Bellemare et al., 2013). \u2022 DQfD was performed with the following parameters: \u2022 Pre-training with 1,000,000 mini-batch updates. \u2022 Expert sample ratio p = 0.1. \u2022 Regarded loss weight \u03bb1 = 1.0. \u2022 L2 regulation weight \u03bb2 = 10 \u2212 5. \u2022 Expert margin l (s, aE, a) = 0.8 for a 6 = aE. \u2022 Greedy exploration with the value of 0.01 used by Double DQN (van Hasselt et al., 2016)."}, {"heading": "4.1. Catch", "text": "In this area, there is a falling ball and the agent must move across the bottom of the screen to catch the ball. The state is represented by a screen of 25x10 pixels, each of which is worth 0 or 1. Only two pixels will be 1, the ball and the agent. At the beginning of each episode, the ball is in the top row in a random column, and the agent is in the bottom row in the middle column. The ball falls in each row. If the ball reaches the bottom row, the episode ends with reward + 1 if the agent is in the same column as the ball, and a reward of -1 otherwise. The agent has 10 actions: move left, move right and eight actions that do nothing. Each time, there is a 10% chance that the agent moves left regardless of the actions he takes."}, {"heading": "4.2. Atari", "text": "We rated DQfD on a much more challenging domain, the Arcade Learning Environment (ALE) (Bellemare et al., 2013). ALE is a set of Atari games that are a standard benchmark for DQN, and contains many games in which people are even better than the best learners. The agent applies a discount factor of 0.99 and all of his actions are repeated for four Atari frames, and the agent stacks four of these frames together."}, {"heading": "4.2.1. MAIN RESULTS", "text": "In the real world, the agent has to perform well from his very first action, so we evaluate the agent in the medium term, rather than just looking at the value of his last policy. Table 2 shows the average rewards achieved by each algorithm in each game over 200 iterations of a million Atari frames each. DQfD outperforms the results in all 42 games. DQfD starts with performance close to the imitation policy and shows the results in Hero that are typical of many of the games that are running. Plots show the results in all 42 games that are included in the appendix."}, {"heading": "4.2.2. DQFD ABLATION STUDIES", "text": "Next, we looked at the impact of each of the five major differences between DQfD and DQN. There are a number of games where DQN learns relatively quickly, and DQfD gets a boost in initial performance and still learns as fast as DQN (e.g. boxing, pong, freeway). We are investigating the impact of the expert sample rate on one of these games, Freeway, in Figure 4. As the sample rate is reduced and the agent sees more self-generated game interactions, he learns faster. Road Runner (Figure 5) is another interesting game in which DQN learns a score exploit that is significantly different from how a human would play the game. DQfD starts with a better initial performance than DQN and continues learning by outperforming the best human performance in demonstrations, but does not match the final performance of DQN."}, {"heading": "5. Related Work", "text": "It is about the question of whether and in what form people are able to move in the world, and about the question of whether they are able to move in the world, or whether they are able to stay in the world, in which they want to move, in the world in which they live, in which they live, in the world in which they live, in the world in which they live, in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "6. Discussion", "text": "The learning framework that we have presented in this paper is one that is very common in the real world, such as controlling data centers, autonomous vehicles (Hester & Stone, 2013), or recommendation systems (Shani et al., 2005).In these problems, there is no accurate simulator, and learning must take place on the real system with real-world consequences. However, there is often data that is operated by a previous controller. We have a new algorithm called DQfD that accelerates learning on the real system, preaching exclusively on demonstration data, with a combination of TD and monitored losses, so that it has a reasonable policy that is in the task."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Keith Anderson, Chris Apps, Ben Coppin, Nando de Freitas, Chris Gamble, Thore Graepel, Georg Ostrovski, Cosmin Paduraru, Jack Rae, Amir Sadik, Jon Scholz, David Silver, Tom Stepleton, Ziyu Wang and many others at DeepMind for their insightful discussions, code contributions and other efforts."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel and Ng,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Abbeel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2007}, {"title": "Apprenticeship learning about multiple intentions", "author": ["M. Babes-Vroman", "V. Marivate", "K. Subramanian", "M. Littman"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Babes.Vroman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Babes.Vroman et al\\.", "year": 2011}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artifificial Intelligence Research (JAIR),", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Reinforcement learning from demonstration through shaping", "author": ["T. Brys", "A. Harutyunyan", "H.B. Suay", "S. Chernova", "M.E. Taylor", "A. Now\u00e9"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Brys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Brys et al\\.", "year": 2015}, {"title": "Policy shaping with human teachers", "author": ["T. Cederborg", "I. Grover", "C.L. Isbell", "A.L. Thomaz"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Cederborg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cederborg et al\\.", "year": 2015}, {"title": "Direct policy iteration from demonstrations", "author": ["J. Chemali", "A. Lezaric"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Chemali and Lezaric,? \\Q2015\\E", "shortCiteRegEx": "Chemali and Lezaric", "year": 2015}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["C. Finn", "S. Levine", "P. Abbeel"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "TEXPLORE: Real-time sampleefficient reinforcement learning for robots", "author": ["Hester", "Todd", "Stone", "Peter"], "venue": "Machine Learning,", "citeRegEx": "Hester et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hester et al\\.", "year": 2013}, {"title": "Generative adversarial imitation learning", "author": ["J. Ho", "S. Ermon"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ho and Ermon,? \\Q2016\\E", "shortCiteRegEx": "Ho and Ermon", "year": 2016}, {"title": "Playing atari games with deep reinforcement learning and human checkpoint replay", "author": ["Hosu", "I.-A", "T. Rebedea"], "venue": "In ECAI Workshop on Evaluating General Purpose AI,", "citeRegEx": "Hosu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hosu et al\\.", "year": 2016}, {"title": "Learning from limited demonstrations", "author": ["B. Kim", "A. Farahmand", "J. Pineau", "D. Precup"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Reinforcement learning with few expert demonstrations", "author": ["Lakshminarayanan", "Aravind S", "Ozair", "Sherjil", "Bengio", "Yoshua"], "venue": "In NIPS Workshop on Deep Learning for Action and Interaction,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning (JMLR),", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Efficient exploration for dialog policy learning with deep BBQ network & replay buffer spiking", "author": ["Lipton", "Zachary C", "Gao", "Jianfeng", "Li", "Lihong", "Xiujun", "Ahmed", "Faisal", "Deng"], "venue": null, "citeRegEx": "Lipton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2016}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S.J. Russell"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Learning from demonstrations: Is it worth estimating a reward function", "author": ["B. Piot", "M. Geist", "O. Pietquin"], "venue": "In European Conference on Machine Learning (ECML),", "citeRegEx": "Piot et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Piot et al\\.", "year": 2013}, {"title": "Boosted bellman residual minimization handling expert demonstrations", "author": ["B. Piot", "M. Geist", "O. Pietquin"], "venue": "In European Conference on Machine Learning (ECML),", "citeRegEx": "Piot et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Piot et al\\.", "year": 2014}, {"title": "Boosted and Reward-regularized Classification for Apprenticeship Learning", "author": ["Piot", "Bilal", "Geist", "Matthieu", "Pietquin", "Olivier"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Piot et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Piot et al\\.", "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning from demonstration", "author": ["Schaal", "Stefan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Schaal and Stefan.,? \\Q1996\\E", "shortCiteRegEx": "Schaal and Stefan.", "year": 1996}, {"title": "An mdpbased recommender system", "author": ["Shani", "Guy", "Heckerman", "David", "Brafman", "Ronen I"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shani et al\\.", "year": 2005}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"], "venue": "search. Nature,", "citeRegEx": "Madeleine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Madeleine et al\\.", "year": 2016}, {"title": "Learning from demonstration for shaping through inverse reinforcement learning", "author": ["Suay", "Halit Bener", "Brys", "Tim", "Taylor", "Matthew E", "Chernova", "Sonia"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Suay et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Suay et al\\.", "year": 2016}, {"title": "Exploration from demonstration for interactive reinforcement learning", "author": ["K. Subramanian", "Jr.", "C.L. Isbell", "A. Thomaz"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Subramanian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Subramanian et al\\.", "year": 2016}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["U. Syed", "R.E. Schapire"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Syed and Schapire,? \\Q2007\\E", "shortCiteRegEx": "Syed and Schapire", "year": 2007}, {"title": "Apprenticeship learning using linear programming", "author": ["U. Syed", "M. Bowling", "R.E. Schapire"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Syed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2008}, {"title": "Integrating reinforcement learning with human demonstrations of varying ability", "author": ["M.E. Taylor", "H.B. Suay", "S. Chernova"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Learning values across many orders of magnitude", "author": ["van Hasselt", "Hado P", "Guez", "Arthur", "Hessel", "Matteo", "Mnih", "Volodymyr", "Silver", "David"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller"], "venue": "In Advances in Neural Information Processing (NIPS),", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": ", 2015), end-to-end policy search for control of robot motors (Levine et al., 2016), model predictive control with embeddings (Watter et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 31, "context": ", 2016), model predictive control with embeddings (Watter et al., 2015), and strategic policies that combined with search led to defeating a top human expert at the game of Go (Silver et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 1, "context": "It still remains difficult to apply these algorithms to real world settings such as data centers, autonomous vehicles (Hester & Stone, 2013), helicopters (Abbeel et al., 2007), or recommendation systems (Shani et al.", "startOffset": 154, "endOffset": 175}, {"referenceID": 21, "context": ", 2007), or recommendation systems (Shani et al., 2005).", "startOffset": 35, "endOffset": 55}, {"referenceID": 3, "context": ", 2016) in average rewards on 27 of 42 Atari games, and out-performs pure imitation learning on 31 of 42 Atari games (Bellemare et al., 2013).", "startOffset": 117, "endOffset": 141}, {"referenceID": 3, "context": "For DQfD, we initially performed informal parameter tuning on four Atari games (Bellemare et al., 2013).", "startOffset": 79, "endOffset": 103}, {"referenceID": 3, "context": "We next evaluated DQfD on a much more challenging domain, the Arcade Learning Environment (ALE) (Bellemare et al., 2013).", "startOffset": 96, "endOffset": 120}, {"referenceID": 12, "context": "(Lakshminarayanan et al., 2016) use a cross entropy loss in their approach, but Figure 6 shows that it results in worse performance than using the large margin loss.", "startOffset": 0, "endOffset": 31}, {"referenceID": 19, "context": "One popular algorithm, DAGGER (Ross et al., 2011), iteratively produces new policies based on polling the expert policy outside its original state space, showing that this leads to no-regret over validation data in the online learning sense.", "startOffset": 30, "endOffset": 49}, {"referenceID": 27, "context": "Another popular paradigm is to setup a zero-sum game where the learner chooses a policy and the adversary chooses a reward function (Syed & Schapire, 2007; Syed et al., 2008; Ho & Ermon, 2016).", "startOffset": 132, "endOffset": 192}, {"referenceID": 7, "context": "Demonstrations have also been used for inverse optimal control in high-dimensional, continuous robotic control problems (Finn et al., 2016).", "startOffset": 120, "endOffset": 139}, {"referenceID": 24, "context": "Recently, demonstration data has been shown to help in difficult exploration problems in RL (Subramanian et al., 2016).", "startOffset": 92, "endOffset": 118}, {"referenceID": 28, "context": "For example, the HAT algorithm transfers knowledge directly from human policies (Taylor et al., 2011).", "startOffset": 80, "endOffset": 101}, {"referenceID": 4, "context": "Follow-ups to this work showed how expert advice or demonstrations can be used to shape rewards in the RL problem (Brys et al., 2015; Suay et al., 2016).", "startOffset": 114, "endOffset": 152}, {"referenceID": 23, "context": "Follow-ups to this work showed how expert advice or demonstrations can be used to shape rewards in the RL problem (Brys et al., 2015; Suay et al., 2016).", "startOffset": 114, "endOffset": 152}, {"referenceID": 5, "context": "A different approach is to shape the policy that is used to sample experience (Cederborg et al., 2015), or to use policy iteration from demonstrations (Kim et al.", "startOffset": 78, "endOffset": 102}, {"referenceID": 11, "context": ", 2015), or to use policy iteration from demonstrations (Kim et al., 2013; Chemali & Lezaric, 2015).", "startOffset": 56, "endOffset": 99}, {"referenceID": 11, "context": ", 2014a) and is also evaluated in (Kim et al., 2013; Chemali & Lezaric, 2015).", "startOffset": 34, "endOffset": 77}, {"referenceID": 14, "context": "(Lipton et al., 2016) show promising results with initializing the DQN agent\u2019s replay buffer with demonstration data on dialog tasks, but they do not pre-train the agent for good initial performance.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "The work that most closely relates to ours is a workshop paper (Lakshminarayanan et al., 2016).", "startOffset": 63, "endOffset": 94}, {"referenceID": 21, "context": "The learning framework that we have presented in this paper is one that is very common in real world problems such as controlling data centers, autonomous vehicles (Hester & Stone, 2013), or recommendation systems (Shani et al., 2005).", "startOffset": 214, "endOffset": 234}], "year": 2017, "abstractText": "Deep reinforcement learning (RL) has achieved several high profile successes in difficult control problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining temporal difference updates with large-margin classification of the demonstrator\u2019s actions. We show that DQfD has better initial performance than Deep Q-Networks (DQN) on 40 of 42 Atari games and it receives more average rewards than DQN on 27 of 42 Atari games. We also demonstrate that DQfD learns faster than DQN even when given poor demonstration data.", "creator": "LaTeX with hyperref package"}}}