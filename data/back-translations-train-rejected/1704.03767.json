{"id": "1704.03767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Parallelized Kendall's Tau Coefficient Computation via SIMD Vectorized Sorting On Many-Integrated-Core Processors", "abstract": "Pairwise association measure is an important operation in data analytics. Kendall's tau coefficient is one widely used correlation coefficient identifying non-linear relationships between ordinal variables. In this paper, we investigated a parallel algorithm accelerating all-pairs Kendall's tau coefficient computation via single instruction multiple data (SIMD) vectorized sorting on Intel Xeon Phis by taking advantage of many processing cores and 512-bit SIMD vector instructions. To facilitate workload balancing and overcome on-chip memory limitation, we proposed a generic framework for symmetric all-pairs computation by building provable bijective functions between job identifier and coordinate space. Performance evaluation demonstrated that our algorithm on one 5110P Phi achieves two orders-of-magnitude speedups over 16-threaded MATLAB and three orders-of-magnitude speedups over sequential R, both running on high-end CPUs. Besides, our algorithm exhibited rather good distributed computing scalability with respect to number of Phis. Source code and datasets are publicly available at", "histories": [["v1", "Wed, 12 Apr 2017 14:29:39 GMT  (1333kb)", "http://arxiv.org/abs/1704.03767v1", "29 pages, 6 figures, 5 tables, submitted to Journal of Parallel and Distributed Computing"]], "COMMENTS": "29 pages, 6 figures, 5 tables, submitted to Journal of Parallel and Distributed Computing", "reviews": [], "SUBJECTS": "cs.DC cs.AI", "authors": ["yongchao liu", "tony pan", "oded green", "srinivas aluru"], "accepted": false, "id": "1704.03767"}, "pdf": {"name": "1704.03767.pdf", "metadata": {"source": "CRF", "title": "Parallelized Kendall\u2019s Tau Coefficient Computation via SIMD Vectorized Sorting On Many-Integrated-Core Processors", "authors": ["Yongchao Liu", "Tony Pan", "Oded Green", "Srinivas Aluru"], "emails": ["yliu@cc.gatech.edu", "tpan7@gatech.edu", "ogreen@gatech.edu", "aluru@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.03 767v 1 [cs.D C] 12 APairwise association measure is an important operation in data analytics. Kendall's tau coefficient is one wide used correlation coefficient identification non-linear relations between ordininal variables. In this paper, we investigated a parallel algorithm that accelerates the calculation of the tau coefficient of all pairs by means of single instruction multiple data (SIMD) vectorized sorting on Intel Xeon Phis using many processing cores and 512-bit SIMD vector instructions. To facilitate workload balancing and overcoming memory constraints on the chip, we proposed a generic framework for symmetrical all-pair computation by establishing verifiable bijective functions between job identifier and coordinate space. Performance evaluation showed that our algorithm achieves two orders of magnitude on a 5110P Phi and the processing speed of all PATLAB and three orders of magnitude."}, {"heading": "1. Introduction", "text": "Identifying interesting pairwise associations between variables is an important operation in data analysis. In bioinformatics and computer-assisted biology, a typical application is seen to be to minimize gene expression relationships using gene expression data that can be realized through a query-based gene expression database search. < < < < < < < < < < < < <"}, {"heading": "2. Intel Many-Integrated-Core (MIC) Architecture", "text": "The first generation is codenamed Knights Corner (KNC) and the second generation is codenamed Knights Landing (KNL) [30]. KNC is a PCI Express (PCIe) connected co-processor that needs to be paired primarily with Intel Xeon CPUs. KNC is actually a shared memory computer [31] with complete cache coherence across the chip and with a specialized Linux operating system with many cores. Each core adopts a fine microarchitecture and has four hardware threads that provide four simultaneous multi-threading processes. In addition to scalar processing, each core is able to process vectorized processing units from a newly designed vector (VPU), the 512-bit SIMD command architecture (ISA)."}, {"heading": "3. Pairwise Correlation Coefficient Kernels", "text": "From their definition, the Kendall coefficient depends only on the order of variable pairs. Therefore, given two ordinals, we can first order all elements in each vector, then replace the original value of each element with its rank in each vector, and finally perform the ordinal coefficient calculation based on the rank that transforms new vectors. This ranking transformation does not affect the resulting coefficient value, but could streamline the calculation, especially for ordinals in complex forms of representation. Moreover, this transformation only needs to be performed once in advance for each vector. Therefore, we assume that all ordinals vectors have already been converted in subsequent discussions. To facilitate discussion, Table 1 shows a list of terms used in our study."}, {"heading": "3.1. Na\u0308\u0131ve Kernel", "text": "The kernel na \u00bc ve lists all possible combinations of connection variables (Ui, vi) (0 \u2264 i < n) and counts the number of concordant pairs nc and the number of discordant pairs nd As mentioned above, two common variable pairs (Ui, vi) and (UJ, vj) (i 6 = j) are considered concordant if Ui > uj and vi > vj or Ui < UJ and vi < Vj, discordant if Ui > uj and vi < VJ or Ui < UJ and vi > vj > vj, and neither concordant if Uj = Uj or vi < UJ and vi < VJ, discordant if the two common variables are concordant if and only if the value (Ui \u2212 uj) is concordant."}, {"heading": "3.2. Generic Sorting-enabled Kernel", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. \"(...)"}, {"heading": "3.3. Vectorized Sorting-enabled Kernel", "text": "The VSE kernel expands the GSE kernel by using 512-bit SIMD vector statements on MIC processors to implement vectorized pair-by-pair merging of sorted subarrays. Unlike the GSE kernel, the VSE kernel then made the following algorithmic changes. < the former packs a high-level variable pair (ui, vi) into a significant 32-bit integer. < the former restricts the VSE kernel to the maximum allowable vector size n to 215 \u2212 1 = 32, 767. The second is that we quickly sort the generic variable pair in step 1 using a significant sorting method and reimplement the discordant pair in step 3 (see algorithm 2)."}, {"heading": "3.3.1. In-register bitonic merge network", "text": "In our algorithm, this network has 16 paths and merges two sorted vectors vMin and vMax (Figure 1 shows the sequence of all elements in one and only one vector (reverse vMax in our case) and this sequence is realized by a permutation: 1 permutation, 1 permutation, 1 permutation, 1 input bitonic sequence of the two vectors, we have to reverse the sequence of all elements in one and only one vector (reverse vMax in our case) and this sequence is realized by a permutation: 1 permutation, i.e. mmv5 permutevar epi32 (\u00b7). After we have completed the sequence inversely, we can complete the sorting of vectors vMin and vMax in log2 (32) = 5 steps. Algorithm 4 shows the pseudocode for our 16-way register merge network."}, {"heading": "3.3.2. Vectorized pairwise merge of sorted subarrays", "text": "Our vectorized pairwise merge of sorted subarrays 16way tover (in the middle, in the middle), is based on the above 16-way-in-register merge network and has introduced a very similar procedure to [33]. Given two sorted subarrays SA and SB, we assume that they are aligned to 64 bytes and their lengths (SA | and | SB | are multipliers of 16, for the convenience of discussion. In this way, the vectorized pairwise merge works as follows. 1. loads the smallest 16 elements of SA and SB on vMin and vMax, respectively, and expands the pointer of each subarray. 2. relies on bitonic merge 16way (\u00b7) to sort vectors vMax and vMax epMax, and then stores the contents of vMin, the smallest 16 elements, on the resulting output array.Algorithm 5 pseudocode of the processing of the 16way subarous levers in both:"}, {"heading": "4. Parallel All-Pairs Computation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. All-Pairs Computation Framework", "text": "We consider the m \u00b7 m job matrix to be a 2-dimensional job job that could be more complex than a 2-dimensional work package at the Cartesian level, and define the upper left corner as the origin, the horizontal x-axis (according to the columns) in the left to right direction, and the vertical y-axis (according to the rows) in the upper to lower direction. For non-symmetric all pairs, the calculation (non-commutative pair calculation), the workload distribution via processing elements (e.g. threads, processes, cores, and so on) is relatively simple, because the coordinates in the 2-dimensional matrix correspond to different jobs. In contrast to non-symmetric all-pairs calculation, it is sufficient that we only calculate the upper triangle (or value triangle) of the symmetric all pairs for the work."}, {"heading": "4.2. Tiled Computing", "text": "Based on the direct bijective mapping, we have adopted the tile calculation with the intention of using q q q = q caches. The logic behind the tile calculation is to cache a small subset of the larger data set and reuse this data block for multiple passes in the cache. This technique divides a matrix into a non-overlapping set of q \u2022 q tiles of equal size. In our case, we partition the job matrix and create a tile matrix of size w \u2022 w tiles, with w = m / q tiles. In this way, all jobs in the upper triangle of the job matrix are still fully covered by the upper triangle of the tile matrix \u2212 by treating a tile as a unit, we can assign a unique identification to each tile in the upper triangle of the tile matrix and then build bijective functions between tile markers and tile coordinates in the tile matrix, similar to the tile matrix."}, {"heading": "4.3. Multithreading", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1. Asynchronous kernel execution", "text": "If m is large, we may not have enough memory to store the resulting m \u00b7 m correlation matrix M\u03c4 fully in memory. To overcome memory constraints, we have introduced a multi-pass core execution model that splits the area of the tile identifier [0, w (w + 1) / 2) into a series of non-intersecting subranges (equal in our case) and completes the calculation one subrange at a time. In this way, we do not need to allocate the entire memory for the matrix M\u03c4. Instead, we only need to allocate a small amount of memory to store the computational results of a subrange, which greatly reduces the memory footprint. If we use the KNC Phi, we need to transfer the recalculated results from the Phi to the host after we have completed each pass of the core execution. If the core execution and data transfer are performed at a time interval, the Phi will be kept synchronized from the device to the host during execution."}, {"heading": "4.3.2. Workload balancing", "text": "In the five-component kernel variant, all jobs have the same amount of computational effort (see algorithm 1), so with a fixed number of threads, we distribute jobs evenly across the threads by assigning one thread each to process a tile. In contrast, with the two variants with sortable cores, different jobs may have different computational effort due to the two sorting rounds used in step 1 and step 3. Typically, a dynamic OpenMP scheduling policy should be used to eliminate workload irregularities, even if it has a relatively higher workload distribution effort than the static scheduling method. However, we have observed that the dynamic scheduling method performs slightly worse in our tests than the static scheduling method. In this respect, we have introduced the static scheduling method in our two sortable variants, assigning one thread each to process a tile."}, {"heading": "4.4. Distributed Computing", "text": "For KNC Phi clusters, we used the MPI offload model, which starts MPI processes just like an ordinary CPU cluster. In this model, one or more phis are assigned to a parental MPI process that uses offload pragmatics / directives to interact with the associated phhis, and communication between the phhis must be explicitly managed by parental processes. In this sense, Phis cannot perceive the existence of remote communication between processes. Our distributed implementations require a one-to-one correspondence between MPI processes and phhis, and have introduced a static workload distribution scheme based on tiled calculation. This static distribution is inspired by our practice of multithreading on individual phhis. In our implementation, we equally distribute tiles for certain p processes to the p processes with the i-th (0 \u2264 i < p) assigned to the process w w (1), the process of which is to calculate Kw (1)."}, {"heading": "5. Performance Evaluation", "text": "We evaluated the performance of our algorithm based on four aspects: (i) comparison between our three variants, (ii) comparison with widely used counterparts: MATLAB (version R2015b) and R (version 3.2.0), (iii) multithreading scalability on a single Phi, and (iv) distributed computational scalability on Phi clusters. In these tests, we used four real data sets for gene expression of the entire human genome (see Table 2) produced by Affymetrix Human Genome U133 Plus 2.0 Array. These data sets are publicly available in the GPL570 data collection of SEEK [1], a query-based computational gene co-expression search engine across large transcriptomic databases. In this study, unless otherwise specified, we calculate performance coefficients between genes, meaning that m is equal to the number of genes and n to the number of samples for each collection."}, {"heading": "5.1. Assessment of Our Three Variants", "text": "All three variants work on CPUs, Phis and their clusters. For the two variants na \ufffd \u0131ve and GSE, they used the same C + + core code for CPUs and MIC-oriented instances. For the VSE variant, its 512-bit SIMD vectorization is only applicable to Phis. In this regard, to support the CPUs, we have further developed an unvectorized merge sorting in step 1 and an unvectorized discordant pair counting algorithm in step 3, based on the integer representation mentioned above. Considering that this non-vectorized version also works on step 1, we have investigated how much our 512-bit SIMD vectorization contributes to a speed improvement by comparing this vectorized version with the non-vectorized version, with the fact that the non-vectorized version works on the same 5110P Phi vectorization, we are comparable to a generic version based on the same table 41, which is only measured by the correlation between the large data sets in the table below."}, {"heading": "5.1.1. On CPU", "text": "We first compared the performance of our three variants on multiple CPU cores. Table 3 shows the performance of each variant on 16 CPU cores, and Figure 3 shows the acceleration of the 16-thread instance of each variant via its single-thread instance. In the 16-thread instance, its 16-thread instance achieves approximately constant acceleration of 13.70 via its single-thread instance. This observation is in line with our expectations, since the runtime of the Na-thread kernel is subject to vector size n, but independent of the actual vector content (see implementation shown in Algorithm 1). In contrast, the speed of the two sortable cores is sensitive to vector content to some degree. This can be explained by the following three factors: Variable pair sorting in step 1, discordant pair counting based on the pair-wise merging of sorted subrays in step 3, and linear time scanning to determine step 69 and step 69 respectively."}, {"heading": "5.1.2. On Xeon Phi", "text": "Subsequently, we evaluated the three variants on a single 5110P Phi (see Table 4). Table 4 shows that the VSE variant outperforms the VSE variant for each dataset, yielding an average acceleration of 1.60 at the minimum acceleration of 1.53 and the maximum acceleration of 1.71. The GSE variant outperforms the VSE variant for each DS3526 dataset, but outperforms the latter for the remaining three datasets. Interestingly, we recalled that the former consistently performs better than the latter on each dataset. As both the VPU variant and the GSE variants use the same pair coefficient kernel code for their respective CPU and MIC-oriented implementations, this discordant performance ranking between the two variants of acceleration is due."}, {"heading": "5.2. Comparison With MATLAB and R", "text": "Secondly, we compared our algorithm with all the pairwise implementations of the \u03c4 coefficient in the widely used models MATLAB (version R2015b) and R (version 3.2.0). MATLAB and R are executed in the aforementioned 16-core computing node, with MATLAB executing 16 threads because it supports multithreading and R sequentially. To be fair, both MATLAB and R only calculate all the pairwise coefficients without performing statistical tests, as our algorithm does. Table 5 shows the runtimes of the 16-thread MATLAB and sequential R instances, as well as their comparison with our three variants executed on the 5110P Phi. The table shows that each variant has excellent speeds, i.e. two orders of magnitude over 16-thread MATLAB and sequential R instances, and their comparison with our three orders of magnitude executed on the 5110P Phi."}, {"heading": "5.3. Parallel Scalability Assessment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1. Multithreading", "text": "Thirdly, we have examined the multi-threading scalability of our variants on the Phi in terms of the number of threads. Figure 5 shows the parallel scalability of the three variants. In this test, we used a balanced thread affinity to ensure that the thread allocation is balanced across the cores and that the threads associated with the same core have consecutive identifiers (i.e. neighbours of each other).This thread affinity configuration is achieved by balancing the KMP AFFINITY environment variable. Figures show that each variant receives a performance improvement as the number of active threads per core increases from 1, over 2 and 3, to 4 (corresponding to 59, 118, 177 and 236 threads, respectively).Since each core is a dual problem and one thread applies proper execution, at least two threads per core are required to saturate the processing power per core, and the intermediate time we switch from two threads per nucleus each to two threads in each nucleus, each of which we have two in each nucleus."}, {"heading": "5.3.2. Distributed computing", "text": "Finally, we measured the parallel scalability of our algorithm by varying the number of phis used in a distributed environment (see Figure 6).This test is performed in a cluster consisting of 16 phis and consisting of the above-mentioned 8 computing nodes. Each variant used 236 threads, as this setting delivers the best performance, as mentioned above. Figure 6 shows that the variant now has near constant acceleration for all data sets on a certain number of phis, while the two sortable variants had slight variations under the same hardware configuration.This can be explained by the fact that the runtime of our variant is na \u00bc ve independent of the vector content, while that of each sortable variant is sensitive to a certain degree to the actual vector content.In addition, for each data set it is observed that each variant has a demonically controlled acceleration in terms of the number of phis. 99, the concrete variant gives 3.93, the variant 1.99, the variant 1.93, the variant, the 3.SE variant, the average variant 1.99, and the variant 1.99."}, {"heading": "6. Conclusion", "text": "In fact, it is the case that most people are able to decide for themselves what they want and what they don't want. In fact, it is the case that most people are able to decide whether they want to or not. In fact, it is the case that most people are able to decide whether they want to or not. In fact, it is the case that most people are able to decide whether they want to or not. In fact, it is the case that people are able to decide whether they want to or not."}, {"heading": "Acknowledgment", "text": "This research is partially supported by the US National Science Foundation under IIS-1416259 and an Intel Parallel Computing Center Award. Conflict of interest: no explanation."}], "references": [{"title": "V", "author": ["Q. Zhu", "A.K. Wong", "A. Krishnan", "M.R. Aure", "A. Tadych", "R. Zhang", "D.C. Corney", "C.S. Greene", "L.A. Bongo"], "venue": "N. Kristensen, et al., Targeted exploration and analysis of large cross-platform human transcriptomic compendia, Nature Methods 12 (3) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "The mutual information: detecting and evaluating dependencies between variables", "author": ["R. Steuer", "J. Kurths", "C.O. Daub", "J. Weise", "J. Selbig"], "venue": "Bioinformatics 18 (suppl 2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "I", "author": ["A.J. Butte"], "venue": "S. Kohane, Unsupervised knowledge discovery in medical databases using relevance networks., in: Proceedings of the AMIA Symposium, American Medical Informatics Association", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Planet: combined sequence and expression comparisons across plant networks derived from seven species", "author": ["M. Mutwil", "S. Klie", "T. Tohge", "F.M. Giorgi", "O. Wilkins", "M.M. Campbell", "A.R. Fernie", "B. Usadel", "Z. Nikoloski", "S. Persson"], "venue": "The Plant Cell 23 (3) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A null model for pearson coexpression networks", "author": ["A. Gobbi", "G. Jurman"], "venue": "PloS One 10 (6) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context", "author": ["A.A. Margolin", "I. Nemenman", "K. Basso", "C. Wiggins", "G. Stolovitzky", "R.D. Favera", "A. Califano"], "venue": "BMC Bioinformatics 7 (Suppl 1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Reverse engineering and analysis of large genome-scale gene networks", "author": ["M. Aluru", "J. Zola", "D. Nettleton", "S. Aluru"], "venue": "Nucleic Acids Research 41 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Aracne-ap: gene network reverse engineering through adaptive partitioning inference of mutual information", "author": ["A. Lachmann", "F.M. Giorgi", "G. Lopez", "A. Califano"], "venue": "Bioinformatics 32 (14) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Thirteen ways to look at the correlation coefficient", "author": ["J. Lee Rodgers", "W.A. Nicewander"], "venue": "The American Statistician 42 (1) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1988}, {"title": "Comparison of co-expression measures: mutual information", "author": ["L. Song", "P. Langfelder", "S. Horvath"], "venue": "correlation, and model based indices, BMC Bioinformatics 13 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Spearmans rank correlation coefficient", "author": ["C. Spearman"], "venue": "Amer J Psychol 15 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1904}, {"title": "Rank correlation methods", "author": ["M.G. Kendall"], "venue": "Griffin", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1948}, {"title": "Efficient test for nonlinear dependence of two continuous variables", "author": ["Y. Wang", "Y. Li", "H. Cao", "M. Xiong", "Y.Y. Shugart", "L. Jin"], "venue": "BMC Bioinformatics 16 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparative analysis of spearman\u2019s rho and kendall\u2019s tau in normal and contaminated normal models", "author": ["W. Xu", "Y. Hou", "Y. Hung", "Y. Zou"], "venue": "Signal Processing 93 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "I", "author": ["G.A. Darbellay"], "venue": "Vajda, et al., Estimation of the information by an adaptive partitioning of the observation space, IEEE Transactions on Information Theory 45 (4) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Estimating mutual information using b-spline functions\u2013an improved similarity measure for analysing gene expression data", "author": ["C.O. Daub", "R. Steuer", "J. Selbig", "S. Kloska"], "venue": "BMC Bioinformatics 5 (1) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Gene regulatory network reconstruction using conditional mutual information", "author": ["K.-C. Liang", "X. Wang"], "venue": "EURASIP Journal on Bioinformatics and Systems Biology 2008 (1) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "N", "author": ["G.J. Sz\u00e9kely", "M.L. Rizzo"], "venue": "K. Bakirov, et al., Measuring and testing dependence by correlation of distances, The Annals of Statistics 35 (6) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "On brownian distance covariance and high dimensional data", "author": ["M.R. Kosorok"], "venue": "The Annals of Applied Statistics 3 (4) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Measuring statistical dependence with hilbert-schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Sch\u00f6lkopf"], "venue": "in: International Conference on Algorithmic Learning Theory, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Detecting novel associations in large data sets", "author": ["D.N. Reshef", "Y.A. Reshef", "H.K. Finucane", "S.R. Grossman", "G. McVean", "P.J. Turnbaugh", "E.S. Lander", "M. Mitzenmacher", "P.C. Sabeti"], "venue": "Science 334 (6062) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature selection via dependence maximization", "author": ["L. Song", "A. Smola", "A. Gretton", "J. Bedo", "K. Borgwardt"], "venue": "Journal of Machine Learning Research 13 (May) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Gsa-lightning: ultra-fast permutation-based gene set analysis", "author": ["B.H.W. Chang", "W. Tian"], "venue": "Bioinformatics 32 (19) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "The kendall rank correlation coefficient", "author": ["H. Abdi"], "venue": "Encyclopedia of Measurement and Statistics. Sage, Thousand Oaks, CA ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimising parallel R correlation matrix calculations on gene expression data using mapreduce", "author": ["S. Wang", "I. Pandis", "D. Johnson", "I. Emam", "F. Guitton", "A. Oehmichen", "Y. Guo"], "venue": "BMC Bioinformatics 15 (1) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A language and environment for statistical computing, R Foundation for", "author": ["R.C. Team", "R et al"], "venue": "Statistical Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM 51 (1) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Parallel pairwise correlation computation on intel xeon phi clusters", "author": ["Y. Liu", "T. Pan", "S. Aluru"], "venue": "in: 28th International Symposium on Computer Architecture and High Performance Computing, IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Knights landing: Second-generation intel xeon phi product", "author": ["A. Sodani", "R. Gramunt", "J. Corbal", "H.-S. Kim", "K. Vinod", "S. Chinthamani", "S. Hutsell", "R. Agarwal", "Y.-C. Liu"], "venue": "IEEE Micro 36 (2) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Intel Xeon Phi coprocessor high-performance programming", "author": ["J. Jeffers", "J. Reinders"], "venue": "Morgan Kaufmann", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "A computer method for calculating kendall\u2019s tau with ungrouped data", "author": ["W.R. Knight"], "venue": "Journal of the American Statistical Association 61 (314) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1966}, {"title": "Aa-sort: A new parallel sorting algorithm for multi-core simd processors", "author": ["H. Inoue", "T. Moriyama", "H. Komatsu", "T. Nakatani"], "venue": "in: Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques, IEEE Computer Society", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Sorting networks and their applications", "author": ["K.E. Batcher"], "venue": "in: Proceedings of the April 30\u2013May 2, 1968, spring joint computer conference, ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1968}, {"title": "Efficient implementation of sorting on multi-core simd cpu architecture", "author": ["J. Chhugani", "A.D. Nguyen", "V.W. Lee", "W. Macy", "M. Hagog", "Y.-K. Chen", "A. Baransi", "S. Kumar", "P. Dubey"], "venue": "Proceedings of the VLDB Endowment 1 (2) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Register level sort algorithm on multicore simd processors", "author": ["T. Xiaochen", "K. Rocki", "R. Suda"], "venue": "in: Proceedings of the 3rd Workshop on Irregular Applications: Architectures and Algorithms, ACM", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Merge path-cacheefficient parallel merge and sort", "author": ["S. Odeh", "O. Green", "Z. Mwassi", "O. Shmueli", "Y. Birk"], "venue": "Tech. rep., Technical report, CCIT Report ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Gpu merge path: a gpu merging algorithm", "author": ["O. Green", "R. McColl", "D.A. Bader"], "venue": "in: Proceedings of the 26th ACM international conference on Supercomputing, ACM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Pairwise element computation with MapReduce", "author": ["T. Kiefer", "P.B. Volk", "W. Lehner"], "venue": "in: Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing, ACM", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Using advanced MPI: Modern features of the message-passing interface", "author": ["W. Gropp", "T. Hoefler", "R. Thakur", "E. Lusk"], "venue": "MIT Press", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Upc: unified parallel c", "author": ["T. El-Ghazawi", "L. Smith"], "venue": "in: Proceedings of the 2006 ACM/IEEE conference on Supercomputing, ACM", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel and scalable shortread alignment on multi-core clusters using upc++, PLoS ONE", "author": ["J. Gonz\u00e1lez-Dom\u0131\u0301nguez", "Y. Liu", "B. Schmidt"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In bioinformatics and computational biology, one typical application is to mine gene co-expression relationship via gene expression data, which can be realized by query-based gene expression database search [1]", "startOffset": 207, "endOffset": 210}, {"referenceID": 1, "context": "or gene co-expression network analysis [2].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "[3] [4] [5]) or non-linear (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[3] [4] [5]) or non-linear (e.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "[3] [4] [5]) or non-linear (e.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "[6] [7] [8]) co-expression measures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] [7] [8]) co-expression measures.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "[6] [7] [8]) co-expression measures.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "A variety of correlation/dependence measures have been proposed in the literature and among them, Pearson\u2019s product-moment correlation coefficient [9] (or Pearson\u2019s r correlation) is the most widely used correlation measure [10].", "startOffset": 147, "endOffset": 150}, {"referenceID": 9, "context": "A variety of correlation/dependence measures have been proposed in the literature and among them, Pearson\u2019s product-moment correlation coefficient [9] (or Pearson\u2019s r correlation) is the most widely used correlation measure [10].", "startOffset": 224, "endOffset": 228}, {"referenceID": 10, "context": "In contrast, Spearman\u2019s rank correlation coefficient [11] (or Spearman\u2019s \u03c1 coefficient) and Kendall\u2019s rank correlation coefficient [12] (or Kendall\u2019s \u03c4 coefficient) are two commonly used measures for non-linear correlations [13].", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "In contrast, Spearman\u2019s rank correlation coefficient [11] (or Spearman\u2019s \u03c1 coefficient) and Kendall\u2019s rank correlation coefficient [12] (or Kendall\u2019s \u03c4 coefficient) are two commonly used measures for non-linear correlations [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "In contrast, Spearman\u2019s rank correlation coefficient [11] (or Spearman\u2019s \u03c1 coefficient) and Kendall\u2019s rank correlation coefficient [12] (or Kendall\u2019s \u03c4 coefficient) are two commonly used measures for non-linear correlations [13].", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "These two rank-based coefficients were shown to play complementary roles in the cases when Pearson\u2019s r is not effective [14].", "startOffset": 120, "endOffset": 124}, {"referenceID": 14, "context": "Among other non-linear measures, mutual information [15] [16] [17], Euclidean distance correlation [18] [19], Hilbert-Schmidt information criterion [20], and maximal information criterion [21] are frequently used as well.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "Among other non-linear measures, mutual information [15] [16] [17], Euclidean distance correlation [18] [19], Hilbert-Schmidt information criterion [20], and maximal information criterion [21] are frequently used as well.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "Among other non-linear measures, mutual information [15] [16] [17], Euclidean distance correlation [18] [19], Hilbert-Schmidt information criterion [20], and maximal information criterion [21] are frequently used as well.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "Among other non-linear measures, mutual information [15] [16] [17], Euclidean distance correlation [18] [19], Hilbert-Schmidt information criterion [20], and maximal information criterion [21] are frequently used as well.", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "Among other non-linear measures, mutual information [15] [16] [17], Euclidean distance correlation [18] [19], Hilbert-Schmidt information criterion [20], and maximal information criterion [21] are frequently used as well.", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "Among other non-linear measures, mutual information [15] [16] [17], Euclidean distance correlation [18] [19], Hilbert-Schmidt information criterion [20], and maximal information criterion [21] are frequently used as well.", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "Among other non-linear measures, mutual information [15] [16] [17], Euclidean distance correlation [18] [19], Hilbert-Schmidt information criterion [20], and maximal information criterion [21] are frequently used as well.", "startOffset": 188, "endOffset": 192}, {"referenceID": 21, "context": "[22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "For this purpose, one approach is permutation test [23].", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "However, a permutation test may need a substantial number of pairwise \u03c4 coefficient computation [24] even for moderately large n, thus resulting in prohibitively long times for sequential execution.", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "[25], which accelerated the sequential \u03c4 coefficient computation in R [26] based on Hadoop MapReduce [27] parallel programming model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[25], which accelerated the sequential \u03c4 coefficient computation in R [26] based on Hadoop MapReduce [27] parallel programming model.", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "[25], which accelerated the sequential \u03c4 coefficient computation in R [26] based on Hadoop MapReduce [27] parallel programming model.", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "In [25], the sequential all-pairs \u03c4 coefficient implementation in R was shown extremely slow on largescale datasets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "This work is a continuation from our previous parallelization of all-pairs Pearson\u2019s r coefficient on Phi clusters [28] and further enriches our LightPCC library (http://lightpcc.", "startOffset": 115, "endOffset": 119}, {"referenceID": 25, "context": "We further compared our algorithm with the all-pairs \u03c4 coefficient implementations in the widely used MATLAB [29] and R [26], revealing that our algorithm on a single 5110P Phi achieves up to 812 speedups over 16-threaded MATLAB and up to 1,166 speedups over sequential R, both of which were benchmarked on high-end CPUs.", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "The first generation is code named as Knights Corner (KNC) and the second generation code named as Knights Landing (KNL) [30].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "KNC is actually a shared-memory computer [31] with full cache coherency over the entire chip and running a specialized Linux operating system over many cores.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "Generic Sorting-enabled Kernel Considering the close relationship between calculating \u03c4 and ordering a list of variables, Knight [32] proposed a merge-sort-like divide-and-conquer approach with O(n logn) time complexity, based on the assumption that no element tie exists within any vector.", "startOffset": 129, "endOffset": 133}, {"referenceID": 31, "context": "[33] employed a SSE vectorized odd-even merge network [34] to merge sorted subarrays in an out-of-core way.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] employed a SSE vectorized odd-even merge network [34] to merge sorted subarrays in an out-of-core way.", "startOffset": 54, "endOffset": 58}, {"referenceID": 33, "context": "[35] adopted", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "similar ideas to [33], but combined a SSE vectorized in-register odd-even merge sort [34] with an in-memory bitonic merge network.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "similar ideas to [33], but combined a SSE vectorized in-register odd-even merge sort [34] with an in-memory bitonic merge network.", "startOffset": 85, "endOffset": 89}, {"referenceID": 34, "context": "[36] absorbed the merge-path idea of [37, 38] and extended the SSE vectorized work of [35] to take advantage of 512-bit SIMD VPUs on KNC Phis and used a vectorized in-register bitonic merge sort, instead of the in-register odd-even merge sort.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] absorbed the merge-path idea of [37, 38] and extended the SSE vectorized work of [35] to take advantage of 512-bit SIMD VPUs on KNC Phis and used a vectorized in-register bitonic merge sort, instead of the in-register odd-even merge sort.", "startOffset": 37, "endOffset": 45}, {"referenceID": 36, "context": "[36] absorbed the merge-path idea of [37, 38] and extended the SSE vectorized work of [35] to take advantage of 512-bit SIMD VPUs on KNC Phis and used a vectorized in-register bitonic merge sort, instead of the in-register odd-even merge sort.", "startOffset": 37, "endOffset": 45}, {"referenceID": 33, "context": "[36] absorbed the merge-path idea of [37, 38] and extended the SSE vectorized work of [35] to take advantage of 512-bit SIMD VPUs on KNC Phis and used a vectorized in-register bitonic merge sort, instead of the in-register odd-even merge sort.", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "In our VSE kernel, we engineered a 512-bit SIMD vectorized in-register bitonic merge network as the core of our pairwise merge procedure for sorted subarrays, which is similar to [36], and further proposed a predict-and-skip mechanism to reduce the number of comparisons during pairwise merge.", "startOffset": 179, "endOffset": 183}, {"referenceID": 31, "context": "Vectorized pairwise merge of sorted subarrays Our vectorized pairwise merge of sorted subarrays relies on the aforementioned 16-way in-register bitonic merge network and adopted a very similar procedure to [33].", "startOffset": 206, "endOffset": 210}, {"referenceID": 37, "context": "In [40], the authors used a very similar job numbering approach to ours (shown in this study), but did not derive a bijective function for symmetric all-pairs computation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "using a shared integer counter to realize dynamic workload distribution via remote memory access operations in MPI [41] and Unified Parallel C (UPC) programming models [42] [43]) and is also particularly useful for parallel computing architectures with hardware", "startOffset": 115, "endOffset": 119}, {"referenceID": 39, "context": "using a shared integer counter to realize dynamic workload distribution via remote memory access operations in MPI [41] and Unified Parallel C (UPC) programming models [42] [43]) and is also particularly useful for parallel computing architectures with hardware", "startOffset": 168, "endOffset": 172}, {"referenceID": 40, "context": "using a shared integer counter to realize dynamic workload distribution via remote memory access operations in MPI [41] and Unified Parallel C (UPC) programming models [42] [43]) and is also particularly useful for parallel computing architectures with hardware", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "Refer to our previous work [28] for more details about the host-side asynchronous execution workflow proposed.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "These datasets are publicly available in the GPL570 data collection of SEEK [1], a query-based computational gene co-expression search engine over large transcriptomic databases.", "startOffset": 76, "endOffset": 79}], "year": 2017, "abstractText": "Pairwise association measure is an important operation in data analytics. Kendall\u2019s tau coefficient is one widely used correlation coefficient identifying non-linear relationships between ordinal variables. In this paper, we investigated a parallel algorithm accelerating all-pairs Kendall\u2019s tau coefficient computation via single instruction multiple data (SIMD) vectorized sorting on Intel Xeon Phis by taking advantage of many processing cores and 512-bit SIMD vector instructions. To facilitate workload balancing and overcome on-chip memory limitation, we proposed a generic framework for symmetric all-pairs computation by building provable bijective functions between job identifier and coordinate space. Performance evaluation demonstrated that our algorithm on one 5110P Phi achieves two orders-of-magnitude speedups over 16-threaded MATLAB and three ordersof-magnitude speedups over sequential R, both running on high-end CPUs. Besides, our algorithm exhibited rather good distributed computing scalability with respect to number of Phis. Source code and datasets are publicly available at http://lightpcc.sourceforge.net.", "creator": "LaTeX with hyperref package"}}}