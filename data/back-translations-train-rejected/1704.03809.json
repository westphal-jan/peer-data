{"id": "1704.03809", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "A Neural Parametric Singing Synthesizer", "abstract": "We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that's competitive in both speed and quality.", "histories": [["v1", "Wed, 12 Apr 2017 15:57:08 GMT  (433kb,D)", "http://arxiv.org/abs/1704.03809v1", "Submitted to Interspeech 2017"], ["v2", "Wed, 7 Jun 2017 10:31:56 GMT  (448kb,D)", "http://arxiv.org/abs/1704.03809v2", null], ["v3", "Thu, 17 Aug 2017 12:20:01 GMT  (380kb,D)", "http://arxiv.org/abs/1704.03809v3", null]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["merlijn blaauw", "jordi bonada"], "accepted": false, "id": "1704.03809"}, "pdf": {"name": "1704.03809.pdf", "metadata": {"source": "CRF", "title": "A NEURAL PARAMETRIC SINGING SYNTHESIZER", "authors": ["Merlijn Blaauw", "Jordi Bonada"], "emails": ["merlijn.blaauw@upf.edu", "jordi.bonada@upf.edu"], "sections": [{"heading": null, "text": "We present a new model for vocal synthesis based on a modified version of the WaveNet architecture. Instead of modelling the raw waveform, we model features generated by a parametric vocoder that separates the influence of pitch and timbre, allowing for convenient pitch adjustment to each target melody, facilitating training with more modest dataset sizes, and significantly reducing training and generation times. Our model makes frame-by-frame predictions based on mixing density outputs instead of categorical outputs to reduce the required number of parameters. Having found that matching with the relatively small datasets used in our experiments is a problem, we propose a method to regulate the model and make the auto-aggressive generation process more robust against prediction errors."}, {"heading": "1 INTRODUCTION", "text": "This means that they transform and link short-wave units selected from an inventory of recordings of a singer. Such systems are state-of-the-art in terms of sound quality and naturalness (Saino et al., 2016), but they are limited in terms of flexibility and difficult to expand or significantly improve. On the other hand, machine learning approaches, such as statistical parametric methods (Saino et al., 2006), are much less rigid and allow things like combining data from multiple speakers, model adjustments using small amounts of training data, joint modelling of timbre and expression, etc. Unfortunately, these systems are not yet able to achieve the sound quality of conventional methods, in particular suffering from frequency and time smoothing."}, {"heading": "2 RELATED WORK", "text": "This year, it has come to the point where it is a purely reactionary project, an attempt, first and foremost, to find a solution that is capable of finding a solution."}, {"heading": "3 PROPOSED SYSTEM", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3.1 MULTI-STREAM ARCHITECTURE", "text": "Most parametric vocoders separate the voice signal into multiple components. In our case, we get three characteristic streams: a harmonic spectral envelope, an aperodicity envelope, and a spoken / unspoken decision (pitch is provided as a control input).These components are largely independent, but their coherence is important (e.g. synthesizing a harmonic component that corresponds to a spoken frame usually leads to artifacts).Instead of modelling all data streams with a single model, we decided to model these components with independent networks, which allows us to use different architectures for each stream and, more importantly, to avoid one current potentially interfering with the other streams. For example, the harmonic component is by far the most important, so we do not want another jointly modelled current that potentially reduces the model capacity of that component. To encourage predictions that stimulate input, we are predicting a network other than we are predicting."}, {"heading": "3.2 CONSTRAINED MIXTURE DENSITY OUTPUT", "text": "The advantage of this approach is that it does not make prior assumptions about the distribution of the data and allows for things like multiple modes, distorted distributions, truncated distributions, etc. Disadvantages of this approach are an increase in model parameters, a loss of the relative arrangement of values, and the need to discredit data that is not inherently discrete or has a high bit depth. As our model predicts an entire frame at once, the problem of increased parameter count gets worse. Instead, we opted for a mixed density output similar (Salimane et al., 2017). This decision was motivated in part because in previous bin-wise versions of our Softmax model (Blaauw & Bonada, 2016) we found that the predicted distributions were generally quite close to Gaussian or distorted Gaussian."}, {"heading": "3.3 ROBUST GENERATION BY REGULARIZATION", "text": "One of the main problems we found in training our model was that the probability of detecting (or validating) errors is often not very indicative of the quality of the final synthesis. The most striking symptom of this was that the synthesizer did not always follow its control input and changed phonemes in an unpredictable way. Our explanation for this is that the training target does not exactly match the generational setting. During training, many samples are predicted in parallel, due to actual past observations. However, the samples are created in sequential order, with any oneconditioning based on past predictions and not on past observations. Therefore, the model may match observations from the dataset, which causes the generation to fail if the predictions differ only slightly from each other. We expect this problem to be much more perceptible in our case because we use relatively small datasets and the data from a parametric data is less structured from nature than other types of raw or coded data."}, {"heading": "3.4 FAST GENERATION ON CPU", "text": "One disadvantage of autoregressive models such as the one we propose is that generation is inherently sequential and therefore cannot take advantage of massively parallel hardware such as modern GPUs. Naive implementations of the generation algorithm tend to be much slower than most other models. We have independently developed a fast generation algorithm that uses caching techniques similar to those in (Ramachandran et al., 2017; Arik et al., 2017). Combining this with our framework model, we can achieve speeds of 20-35 times real-time on the CPU. These runtimes, combined with the low memory and disk requirements, make our system competitive in terms of availability with most existing systems."}, {"heading": "4 EXPERIMENTAL CONDITIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ACOUSTIC AND LINGUISTIC FRONT-END", "text": "We use a fairly standardized acoustic front-end based on the WORLD vocoder (Morise et al., 2016) with a sampling rate of 32 kHz and a hop time of 5 ms. We further reduce the dimensionality of the harmonic and aperiodic components to 60 and 5 coefficients, respectively, by transforming them into generalized coefficients (MGCs) (Tokuda et al., 1994) with a frequency distortion coefficient of \u03b1 = 0.45 and a pole / zero weighting of \u03b3 = 0. To facilitate interpretation, the receiver characteristics are finally converted back into distorted frequency characteristics. In addition, the linguistic characteristics we use are relatively simple compared to most TTS systems, as we do not model prosodies. We use previous, current and next phonemes as uniform coded vectors. We also refer to the normalized position of the current speaker within the current state codecoder as a probability that a phocessor coded in a phoc3 is about the same as the current one."}, {"heading": "4.2 DATASETS", "text": "In the initial evaluation of our system, we use three voices: an English male and female voice (M1, F1) and a Spanish female voice (F2). The recordings consist of short sentences sung in a single pitch and an approximately constant cadence. The sentences have been selected to promote high telephone coverage. 123 sentences are available for the Spanish record, and 524 sentences for the English record (about 16 or 35 minutes, including silence). Note that these records are very small compared to the records normally used to train TTS systems, but this is a realistic limitation given the difficulty and cost of recording a professional singer."}, {"heading": "4.3 CONFIGURATION AND HYPER-PARAMETERS", "text": "We first use a causal folding based on 10 past values, then we use a stack of waves with dilatation factors [1, 2, 4, 1, 2], resulting in a total receptive field of 105 ms. For the harmonic characteristic current, we use 100 channels in the waves, 240 channels in the skip connections. For the aperiodic characteristic current, we use 20 channels in the waves, 20 channels for the skip connections. The spoken / unspoken decision current uses 20 channels in the waves and 4 channels in the skip connections. All networks use a single output stage with tanh nonlinearity. We use mini-batches of 16 sequences, each with a length of 210 images. We use the Adam Optimizer (Kingma & Ba, 2015) with an initial learning rate of 5 x 10 \u2212 4. The model was trained for a total of 1000 epochs and lasts about 8 hours on a single Titan X Pascal GPU."}, {"heading": "5 EVALUATION", "text": "We compare our system (\"NPSS\") with two alternative systems. The first is an HMM-based system (\"HTS\") built using the HTS toolkit (version 2.3) (Zen et al., 2007). Default settings were mostly used, with the exception of a somewhat simplified context dependency (only the two previous and two subsequent phonemes); the second system (\"IS16\") (Bonada et al., 2016) is based on contact synthesis and was the best rated system in the Interspeech 2016 Singing Synthesis Challenge.In Table 1, we show some quantitative results when comparing these systems with our proposed methodology. These ratios were calculated using a validation split of 10%, silence frames and frames with incorrectly matched / inconsistent choices between target and prediction were excluded; the IS16 system is excluded from this table because the low redundancy of the disparity in this system would force some systems to replace it."}, {"heading": "6 CONCLUSIONS", "text": "We introduced a singing synthesizer based on neural networks that can be successfully trained on relatively small amounts of data. Hearing tests showed a remarkable preference for our system compared to a statistical parametric system and a moderate preference compared to a concatenated system. Interestingly, the quantitative indicators do not reflect this and show almost identical results. One explanation for this is that the framed indicators used do not take into account temporal fluctuations that can be significant for singing. Compared to the HMM-based base system, we consider our method to be less overly smoothing, it sounds significantly less static and \"fuzzy,\" reproduces consonants more faithfully and produces smoother transitions in long, persistent vowels that are typical for singing. Compared to the contactive system, we believe that our method produces a similar overall sound quality. However, in certain segments, such as fast singing, subtle errors in the segmentation become apparent."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "We thank NVIDIA Corporation for its support with the donation of the Titan X Pascal GPU used for this research. We also thank Zya for providing the English data sets used. Voctro Labs supplied the Spanish data set and the implementation of the Fast Generation algorithm. This work is partially supported by the Spanish Ministry of Economy and Competitiveness within the framework of the CASAS project (TIN2015-70816-R)."}], "references": [{"title": "A singing synthesizer based on PixelCNN", "author": ["Merlijn Blaauw", "Jordi Bonada"], "venue": "http://www.dtic.upf. edu/\u223cmblaauw/MdM NIPS seminar/,", "citeRegEx": "Blaauw and Bonada.,? \\Q2016\\E", "shortCiteRegEx": "Blaauw and Bonada.", "year": 2016}, {"title": "Synthesis of the singing voice by performance sampling and spectral models", "author": ["Jordi Bonada", "Xavier Serra"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Bonada and Serra.,? \\Q2007\\E", "shortCiteRegEx": "Bonada and Serra.", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1610.10099,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR-15),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "SampleRNN: An unconditional end-to-end neural audio generation model", "author": ["Soroush Mehri", "Kundan Kumar", "Ishaan Gulrajani", "Rithesh Kumar", "Shubham Jain", "Jose Sotelo", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Mehri et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mehri et al\\.", "year": 2017}, {"title": "WORLD: A vocoder-based high-quality speech synthesis system for real-time applications", "author": ["Masanori Morise", "Fumiya Yokomori", "Kenji Ozawa"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Morise et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Morise et al\\.", "year": 2016}, {"title": "Pitch adaptive training for HMM-based singing voice synthesis", "author": ["Keiichiro Oura", "Ayami Mase", "Yoshihiko Nankaku", "Keiichi Tokuda"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Oura et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Oura et al\\.", "year": 2012}, {"title": "Fast generation for convolutional autoregressive models", "author": ["Prajit Ramachandran", "Tom Le Paine", "Pooya Khorrami", "Mohammad Babaeizadeh", "Shiyu Chang", "Yang Zhang", "Mark Hasegawa-Johnson", "Roy Campbell", "Thomas Huang"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Ramachandran et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2017}, {"title": "Generating interpretable images with controllable structure", "author": ["Scott Reed", "A\u00e4ron van den Oord", "Nal Kalchbrenner", "Victor Bapst", "Matt Botvinick", "Nando de Freitas"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Reed et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2017}, {"title": "An HMM-based singing voice synthesis system", "author": ["Keijiro Saino", "Heiga Zen", "Yoshihiko Nankaku", "Akinobu Lee", "Keiichi Tokuda"], "venue": "In Interspeech 2006 - ICSLP, Ninth International Conference on Spoken Language Processing,", "citeRegEx": "Saino et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Saino et al\\.", "year": 2006}, {"title": "PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications", "author": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Salimans et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2017}, {"title": "Integration of speaker and pitch adaptive training for HMM-based singing voice synthesis", "author": ["Kanako Shirota", "Kazuhiro Nakamura", "Kei Hashimoto", "Keiichiro Oura", "Yoshihiko Nankaku", "Keiichi Tokuda"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Shirota et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shirota et al\\.", "year": 2014}, {"title": "Char2Wav: End-to-end speech synthesis", "author": ["Jos\u00e9 Sotelo", "Soroush Mehri", "Kundan Kumar", "Jo\u00e3o Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Sotelo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sotelo et al\\.", "year": 2017}, {"title": "Mel-generalized cepstral analysis - a unified approach to speech spectral estimation", "author": ["Keiichi Tokuda", "Takao Kobayashi", "Takashi Masuko", "Satoshi Imai"], "venue": "In Proceedings of the International Conference on Spoken Language Processing (ICSLP-94),", "citeRegEx": "Tokuda et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Tokuda et al\\.", "year": 1994}, {"title": "Speech parameter generation algorithms for HMM-based speech synthesis", "author": ["Keiichi Tokuda", "Takayoshi Yoshimura", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-00),", "citeRegEx": "Tokuda et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tokuda et al\\.", "year": 2000}, {"title": "Deterministic annealing EM algorithm", "author": ["Naonori Ueda", "Ryohei Nakano"], "venue": "Neural networks,", "citeRegEx": "Ueda and Nakano.,? \\Q1998\\E", "shortCiteRegEx": "Ueda and Nakano.", "year": 1998}, {"title": "WaveNet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew W. Senior", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with PixelCNN decoders", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Yu and Koltun.,? \\Q2016\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2016}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["Heiga Zen", "Hasim Sak"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Zen and Sak.,? \\Q2015\\E", "shortCiteRegEx": "Zen and Sak.", "year": 2015}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["Heiga Zen", "Andrew Senior"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Zen and Senior.,? \\Q2014\\E", "shortCiteRegEx": "Zen and Senior.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "On the other hand, machine learning-based approaches, such as statistical parametric methods (Saino et al., 2006; Oura et al., 2010), are much less rigid and do allow for things such as combining data from multiple speakers, model adaptation using small amounts of training data, joint modeling of timbre and expression, etc.", "startOffset": 93, "endOffset": 132}, {"referenceID": 9, "context": "This type of model was first proposed to model natural images (PixelCNN) (van den Oord et al., 2016b; Reed et al., 2017; Salimans et al., 2017), but was later also applied to modeling raw audio waveform (WaveNet) (van den Oord et al.", "startOffset": 73, "endOffset": 143}, {"referenceID": 11, "context": "This type of model was first proposed to model natural images (PixelCNN) (van den Oord et al., 2016b; Reed et al., 2017; Salimans et al., 2017), but was later also applied to modeling raw audio waveform (WaveNet) (van den Oord et al.", "startOffset": 73, "endOffset": 143}, {"referenceID": 5, "context": "The SampleRNN (Mehri et al., 2017) model proposes an alternative architecture for unconditional raw waveform generation based on multi-scale hierarchical Recurrent Neural Networks (RNNs) rather than dilated Convolutional Neural Networks (CNNs).", "startOffset": 14, "endOffset": 34}, {"referenceID": 13, "context": "Other works (Sotelo et al., 2017; Arik et al., 2017) have extended these two architectures to include attention mechanisms to allow performing end-to-end TTS, i.", "startOffset": 12, "endOffset": 52}, {"referenceID": 15, "context": "This is often partly mitigated by predicting static, delta and delta-delta feature distributions combined with a parameter generation algorithm that maximizes output probability (Tokuda et al., 2000).", "startOffset": 178, "endOffset": 199}, {"referenceID": 10, "context": ", 2016) and statistical parametric methods centered around Hidden Markov Models (HMMs) (Saino et al., 2006; Oura et al., 2010).", "startOffset": 87, "endOffset": 126}, {"referenceID": 7, "context": "While this work focuses on the generation of timbre and does not consider modeling of expression, an important difference between these methods is that statistical models allow joint modeling of timbre and expression from natural singing (Mase et al., 2010; Oura et al., 2012), unlike concatenative methods which typically use disjoint modeling and specialized recordings.", "startOffset": 238, "endOffset": 276}, {"referenceID": 12, "context": "speaker-adaptive training (Shirota et al., 2014).", "startOffset": 26, "endOffset": 48}, {"referenceID": 11, "context": "This approach is similar to (Salimans et al., 2017) where all three RGB channels of a pixel in an image are predicted at once, although in our work we do not incorporate additional linear dependencies between channel means.", "startOffset": 28, "endOffset": 51}, {"referenceID": 2, "context": "While our architecture uses a number of layers closer to PixelCNN than WaveNet, like both these models we use residual and skip connections to facilitate training deeper networks (He et al., 2016).", "startOffset": 179, "endOffset": 196}, {"referenceID": 9, "context": "In our case we do the same thing at the output stack, similar to (Reed et al., 2017).", "startOffset": 65, "endOffset": 84}, {"referenceID": 11, "context": "Instead, we opted to use a mixture density output similar to (Salimans et al., 2017).", "startOffset": 61, "endOffset": 84}, {"referenceID": 9, "context": "(Reed et al., 2017).", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "We have independently developed a fast generation algorithm using caching techniques similar to those used in (Ramachandran et al., 2017; Arik et al., 2017).", "startOffset": 110, "endOffset": 156}, {"referenceID": 6, "context": "We use a fairly standard acoustic front-end based on the WORLD vocoder (Morise et al., 2016) with a 32 kHz sample rate and 5 ms hop time.", "startOffset": 71, "endOffset": 92}, {"referenceID": 14, "context": "We further reduce dimensionality of the harmonic and aperiodic components to 60 and 5 coefficients respectively by transforming them to mel-generalized coefficients (MGCs) (Tokuda et al., 1994) with frequency warping coefficient \u03b1 = 0.", "startOffset": 172, "endOffset": 193}], "year": 2017, "abstractText": "We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that\u2019s competitive in both speed and quality.", "creator": "LaTeX with hyperref package"}}}