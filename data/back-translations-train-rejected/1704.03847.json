{"id": "1704.03847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark", "abstract": "This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.", "histories": [["v1", "Wed, 12 Apr 2017 17:12:57 GMT  (9051kb,D)", "http://arxiv.org/abs/1704.03847v1", "Accepted to ISPRS Annals. The benchmark website is available atthis http URL. The baseline code is available atthis https URL"]], "COMMENTS": "Accepted to ISPRS Annals. The benchmark website is available atthis http URL. The baseline code is available atthis https URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE cs.RO", "authors": ["timo hackel", "nikolay savinov", "lubor ladicky", "jan d wegner", "konrad schindler", "marc pollefeys"], "accepted": false, "id": "1704.03847"}, "pdf": {"name": "1704.03847.pdf", "metadata": {"source": "CRF", "title": "SEMANTIC3D.NET: A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK", "authors": ["Timo Hackel", "Nikolay Savinov", "Lubor Ladicky", "Jan D. Wegner", "Konrad Schindler", "Marc Pollefeys"], "emails": ["konrad.schindler)@geod.baug.ethz.ch", "marc.pollefeys)@inf.ethz.ch"], "sections": [{"heading": null, "text": "This paper presents a new 3D point classification, the benchmark data with over four billion manually labeled dots intended as input for data-hungry (deep) learning methods. We also discuss the first submissions to the benchmark, the deep Convolutionary Neural Networks (CNNs) as workhorses that already have remarkable state-of-the-art performance improvements. The massive amounts of data presented in this paper show that we are aiming to close this data gap in order to unleash the full potential of deep learning methods. Our 3D.net semantic data consists of dense point clouds acquired with static terrestrictive laser scanners. It includes 8 semantic classes and covers a wide range of outdoor scenes: churches, streets, railway tracks, quarterly neighborhoods, football fields, football fields and football fields."}, {"heading": "3. OBJECTIVE", "text": "Faced with a number of points (here: dense scans from a static, terrestrial laser scanner), we determine one individual class name per point. We provide three basic methods intended to represent typical categories of approaches recently used for the task. Our first, na\u00efve baseline classifies only 2D color images without using depth information to link to the vast literature on semantic 2D image segmentation. Modern methods use Deep Convolutional Neural Networks as a workhorse. Encoder decoder architectures, such as SegNet (Badrinarayanan et al., 2015), are able to deduce the labels of an entire image at once. Deep Convolutional Neural Networks as a workhorse."}, {"heading": "3.1 2D Image Baseline", "text": "We convert color values of the scans into separate images (without depth) using cube mapping (Greene, 1986). Ground truth labels are also projected from the dot clouds into the image space, so the 3D dot caption task turns into a pure semantic image segmentation problem of 2D RGB images (Figure 2). We chose the associated hierarchical field method (Ladicky et al., 2013) for semantic segmentation because it proved to be good performance for a variety of tasks (e.g. (Montoya et al., 2014, Ladicky \u0301 et al., 2014) and was available in its original implementation. The method works as follows: four different types of characteristics - text (Malik et al., 2001), SIFT et al., local quantified ternary patterns (Hussain and Triggs, 2012) and self-similarity characteristics of the pixel class, pixel-pixel class - 2007 - are likely."}, {"heading": "3.2 3D Covariance Baseline", "text": "The second baseline was inspired by (Weinmann et al., 2015), which derives the class designation directly from the 3D point cloud by using multi-scale features and discriminatory learning. Again, we had access to the original implementation; this method uses an efficient approach to multi-scale neighborhoods, where the point cloud is submerged into a multi-scale pyramid, so that a constant, small number of neighbors per level captures the multi-scale information; the multi-scale pyramid is generated by voxel grid filtering at an even distance; the amount of feautre extracted at each level is an extension of the one described in (Weinmann et al., 2013); it uses various combinations of eigenvalues and eigenvectors of covariance per point selection with different geometric surface properties; in addition, height characteristics are added based on vertical, cylindrical neighborhoods to emphasize the particular role of gravity cracking (as is common on surface triometry)."}, {"heading": "3.3 3D CNN Baseline", "text": "We design our fundamentals for a uniform task, which is characterized by two fully networked layers."}, {"heading": "4. DATA", "text": "In fact, most of them are able to move around without having to do so."}, {"heading": "4.1 Point Cloud Annotation", "text": "In fact, most of them are able to survive on their own without being able to afford it."}, {"heading": "7. CONCLUSION AND OUTLOOK", "text": "The semantic3D.net benchmark offers a wide range of high-quality terrestrial laser scans with over 4 billion manually annotated points and a standardized evaluation framework. The dataset was recently published and has rarely been submitted so far, but we are optimistic that this will change in the future. Initial submissions already show that CNNs are finally surpassing more conventional approaches, such as our covariance basis for large 3D laser scans. We hope that submissions to this benchmark will provide better comparisons and insights into the strengths and weaknesses of different classification approaches for point cloud processing and hopefully help guide the research effort in the longer term. We hope that the benchmark will meet the needs of the research community and become a central resource for the development of new, efficient and precise methods for classification in 3D space."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work is partially supported by the Swiss NSF Project 163910, the Max Planck CLS Fellowship and the Swiss CTI Project 17136.1 PFES-ES.REFERENCESBadrinarayanan, V., Kendall, A. and Cipolla, R., 2015. Blessed: A deep convolutional encoder-decoder architecture for image segmentation. arXiv preprint arXiv: 1511.00561.Blomley, R., Weinmann, M., Leitloff, J. and Jutzi, B., 2014. Shape distribution features for point cloud analysis-a geometric histogram on multiple scales. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences.Bottou, L., 2010. Large-scale machine learning with stochastic gradient. In: Proceedings of COMPSTAT 2010, Springer, pp. 177-186.Boykov, Y."}], "references": [{"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Shape distribution features for point cloud analysis-a geometric histogram approach on multiple scales. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences", "author": ["R. Blomley", "M. Weinmann", "J. Leitloff", "B. Jutzi"], "venue": null, "citeRegEx": "Blomley et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blomley et al\\.", "year": 2014}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou,? \\Q2010\\E", "shortCiteRegEx": "Bottou", "year": 2010}, {"title": "An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision", "author": ["Y. Boykov", "V. Kolmogorov"], "venue": "Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Boykov and Kolmogorov,? \\Q2004\\E", "shortCiteRegEx": "Boykov and Kolmogorov", "year": 2004}, {"title": "Fast approximate energy minimization via graph cuts. PAMI", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": null, "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Generative and discriminative voxel modeling with convolutional neural networks", "author": ["A. Brock", "T. Lim", "J. Ritchie", "N. Weston"], "venue": null, "citeRegEx": "Brock et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Brock et al\\.", "year": 2017}, {"title": "Benchmarking high density image matching for oblique airborne imagery", "author": ["S. Cavegn", "N. Haala", "S. Nebiker", "M. Rothermel", "P. Tutzauer"], "venue": "In: Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.,", "citeRegEx": "Cavegn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cavegn et al\\.", "year": 2014}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["Chen", "L.-C", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1606.00915", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Urban structure classification using the 3d normal distribution transform for practical robot applications", "author": ["Y. Choe", "I. Shim", "M.J. Chung"], "venue": "Advanced Robotics", "citeRegEx": "Choe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choe et al\\.", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Mean shift: A robust approach toward feature space analysis", "author": ["D. Comaniciu", "P. Meer"], "venue": null, "citeRegEx": "Comaniciu and Meer,? \\Q2002\\E", "shortCiteRegEx": "Comaniciu and Meer", "year": 2002}, {"title": "Unsupervised feature learning for classification of outdoor 3d scans", "author": ["M. De Deuge", "A. Quadros", "C. Hung", "B. Douillard"], "venue": "Australasian Conference on Robitics and Automation,", "citeRegEx": "Deuge et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deuge et al\\.", "year": 2013}, {"title": "Dimensionality based scale selection in 3d lidar point clouds. The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences", "author": ["J. Demantk\u00e9", "C. Mallet", "N. David", "B. Vallet"], "venue": null, "citeRegEx": "Demantk\u00e9 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Demantk\u00e9 et al\\.", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks", "author": ["M. Engelcke", "D. Rao", "D.Z. Wang", "C.H. Tong", "I. Posner"], "venue": null, "citeRegEx": "Engelcke et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Engelcke et al\\.", "year": 2017}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological cybernetics", "citeRegEx": "Fukushima,? \\Q1980\\E", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Environment mapping and other applications of world projections", "author": ["N. Greene"], "venue": "IEEE Computer Graphics and Applications", "citeRegEx": "Greene,? \\Q1986\\E", "shortCiteRegEx": "Greene", "year": 1986}, {"title": "The landscape of dense image matching algorithms", "author": ["N. Haala"], "venue": "Photogrammetric Week", "citeRegEx": "Haala,? \\Q2013\\E", "shortCiteRegEx": "Haala", "year": 2013}, {"title": "Fast semantic segmentation of 3D point clouds with strongly varying point density", "author": ["T. Hackel", "J.D. Wegner", "K. Schindler"], "venue": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences,", "citeRegEx": "Hackel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hackel et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Visual recognition using local quantized patterns", "author": ["S. Hussain", "B. Triggs"], "venue": "European Conference on Computer Vision", "citeRegEx": "Hussain and Triggs,? \\Q2012\\E", "shortCiteRegEx": "Hussain and Triggs", "year": 2012}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["P. Kohli", "L. Ladicky", "P.H.S. Torr"], "venue": "In: Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kohli et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kohli et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Associative hierarchical random fields. PAMI", "author": ["L. Ladicky", "C. Russell", "P. Kohli", "P. Torr"], "venue": null, "citeRegEx": "Ladicky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ladicky et al\\.", "year": 2013}, {"title": "Discriminatively trained dense surface normal estimation", "author": ["L. Ladick\u00fd", "B. Zeisl", "M. Pollefeys"], "venue": "European Conference on Computer Vision,", "citeRegEx": "Ladick\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ladick\u00fd et al\\.", "year": 2014}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Contour and texture analysis for image segmentation", "author": ["J. Malik", "S. Belongie", "T. Leung", "J. Shi"], "venue": null, "citeRegEx": "Malik et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Malik et al\\.", "year": 2001}, {"title": "Voxnet: A 3d convolutional neural network for real-time object recognition", "author": ["D. Maturana", "S. Scherer"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "Maturana and Scherer,? \\Q2015\\E", "shortCiteRegEx": "Maturana and Scherer", "year": 2015}, {"title": "Trees detection from laser point clouds acquired in dense urban areas by a mobile mapping system. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences", "author": ["F. Monnier", "B. Vallet", "B. Soheilian"], "venue": null, "citeRegEx": "Monnier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Monnier et al\\.", "year": 2012}, {"title": "Mind the gap: modeling local and global context in (road) networks. In: German Conference on Pattern Recognition (GCPR)", "author": ["J. Montoya", "J.D. Wegner", "L. Ladick\u00fd", "K. Schindler"], "venue": null, "citeRegEx": "Montoya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montoya et al\\.", "year": 2014}, {"title": "Contextual classification with functional max-margin markov networks", "author": ["D. Munoz", "J.A. Bagnell", "N. Vandapel", "M. Hebert"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "Munoz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Munoz et al\\.", "year": 2009}, {"title": "Octnet: Learning deep 3d representations at high resolutions", "author": ["G. Riegler", "A.O. Ulusoy", "A. Geiger"], "venue": null, "citeRegEx": "Riegler et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Riegler et al\\.", "year": 2017}, {"title": "Learning where to classify in multi-view semantic segmentation", "author": ["H. Riemenschneider", "A. B\u00f3dis-Szomor\u00fa", "J. Weissenberg", "L. Van Gool"], "venue": "European Conference on Computer Vision,", "citeRegEx": "Riemenschneider et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Riemenschneider et al\\.", "year": 2014}, {"title": "ISPRS Test Project on Urban Classification and 3D Building Reconstruction", "author": ["F. Rottensteiner", "G. Sohn", "M. Gerke", "J.D. Wegner"], "venue": "Technical report, ISPRS Working Group III", "citeRegEx": "Rottensteiner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rottensteiner et al\\.", "year": 2013}, {"title": "Paris-rue-madame database: a 3d mobile laser scanner dataset for benchmarking urban detection, segmentation and classification methods", "author": ["A. Serna", "B. Marcotegui", "F. Goulette", "Deschaud", "J.-E"], "venue": "In: 4th International Conference on Pattern Recognition, Applications and Methods ICPRAM", "citeRegEx": "Serna et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Serna et al\\.", "year": 2014}, {"title": "Matching local selfsimilarities across images and videos", "author": ["E. Shechtman", "M. Irani"], "venue": "In: Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Shechtman and Irani,? \\Q2007\\E", "shortCiteRegEx": "Shechtman and Irani", "year": 2007}, {"title": "TextonBoost: Joint appearance, shape and context modeling for multiclass object recognition and segmentation", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "European Conference on Computer Vision", "citeRegEx": "Shotton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shotton et al\\.", "year": 2006}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "European Conference on Computer Vision,", "citeRegEx": "Silberman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Sun rgb-d: A rgbd scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Song et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Song et al\\.", "year": 2015}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W.T. Freeman"], "venue": "IEEE transactions on pattern analysis and machine intelligence", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Sharing features: efficient boosting procedures for multiclass object detection", "author": ["A. Torralba", "K. Murphy", "W. Freeman"], "venue": null, "citeRegEx": "Torralba et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2004}, {"title": "Terramobilita/iqmulus urban point cloud analysis benchmark", "author": ["B. Vallet", "M. Br\u00e9dif", "A. Serna", "B. Marcotegui", "N. Paparoditis"], "venue": "Computers & Graphics", "citeRegEx": "Vallet et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vallet et al\\.", "year": 2015}, {"title": "Feature relevance assessment for the semantic interpretation of 3d point cloud data. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences", "author": ["M. Weinmann", "B. Jutzi", "C. Mallet"], "venue": null, "citeRegEx": "Weinmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weinmann et al\\.", "year": 2013}, {"title": "Distinctive 2d and 3d features for automated large-scale scene analysis in urban areas", "author": ["M. Weinmann", "S. Urban", "S. Hinz", "B. Jutzi", "C. Mallet"], "venue": "Computers & Graphics", "citeRegEx": "Weinmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weinmann et al\\.", "year": 2015}, {"title": "3d shapenets: A deep representation for volumetric shapes", "author": ["Z. Wu", "S. Song", "A. Khosla", "F. Yu", "L. Zhang", "X. Tang", "J. Xiao"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "A novel outdoor scene-understanding framework for unmanned ground vehicles with 3d laser scanners", "author": ["Y. Zhuang", "G. He", "H. Hu", "Z. Wu"], "venue": "Transactions of the Institute of Measurement and Control", "citeRegEx": "Zhuang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhuang et al\\.", "year": 2014}, {"title": "Contextual classification of 3d laser points with conditional random fields in urban environments", "author": ["Y. Zhuang", "Y. Liu", "G. He", "W. Wang"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "Zhuang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhuang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Deep learning has made a spectacular comeback since the seminal paper of (Krizhevsky et al., 2012), which revives earlier work of (Fukushima, 1980, LeCun et al.", "startOffset": 73, "endOffset": 98}, {"referenceID": 15, "context": ", 2015) and Pascal VOC (Everingham et al., 2010) for rgb images, or SUN rgbd (Song et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 42, "context": ", 2010) for rgb images, or SUN rgbd (Song et al., 2015) for rgb-d data.", "startOffset": 36, "endOffset": 55}, {"referenceID": 42, "context": "3 \u00d7 109 px) (Song et al., 2015), which aims at 3D for untrained users, 3D point clouds are harder to interpret.", "startOffset": 12, "endOffset": 31}, {"referenceID": 35, "context": "2Note that, besides laser scanner point clouds, it is also more efficient to classify point clouds generated from SfM pipelines directly instead of going through all individual images to then merge results (Riemenschneider et al., 2014).", "startOffset": 206, "endOffset": 236}, {"referenceID": 36, "context": ", 2014) and the ISPRS Benchmark Test on Urban Object Detection and Reconstruction, which contains several different challenges like semantic segmentation of aerial images and 3D object reconstruction (Rottensteiner et al., 2013).", "startOffset": 200, "endOffset": 228}, {"referenceID": 43, "context": "One of the first successful attempts to object detection in images at very large scale is tinyimages with over 80 million small (32 \u00d7 32 px) images (Torralba et al., 2008).", "startOffset": 148, "endOffset": 171}, {"referenceID": 15, "context": "A milestone and still widely used dataset for semantic image segmentation is the famous Pascal VOC (Everingham et al., 2010) dataset and challenge, which has been used for training and testing many of the well-known, state-of-the-art algorithms today like (Long et al.", "startOffset": 99, "endOffset": 124}, {"referenceID": 23, "context": ", 2015), which made Convolutional Neural Networks popular in computer vision (Krizhevsky et al., 2012).", "startOffset": 77, "endOffset": 102}, {"referenceID": 40, "context": "Popular examples are the NYU Depth Dataset V2 (Silberman et al., 2012) or SUN RGB-D (Song et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 42, "context": ", 2012) or SUN RGB-D (Song et al., 2015) that provide labeled rgb-d", "startOffset": 21, "endOffset": 40}, {"referenceID": 48, "context": ", 2015) resort to artificially generated data from the CAD models of ModelNet (Wu et al., 2015), a rather small, synthetic dataset.", "startOffset": 78, "endOffset": 95}, {"referenceID": 5, "context": ", (Brock et al., 2017)) reach performance of over 97% on ModelNet10, which clearly indicates a model overfit due to limited data.", "startOffset": 2, "endOffset": 22}, {"referenceID": 50, "context": "Those few existing laser scan datasets are mostly acquired with mobile mapping devices or robots like DUT1 (Zhuang et al., 2014), DUT2 (Zhuang et al.", "startOffset": 107, "endOffset": 128}, {"referenceID": 51, "context": ", 2014), DUT2 (Zhuang et al., 2015), or KAIST (Choe et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 8, "context": ", 2015), or KAIST (Choe et al., 2013), which are small (< 10 points) and not publicly available.", "startOffset": 18, "endOffset": 37}, {"referenceID": 33, "context": "Publicly availabe laser scan datasets include the Oakland dataset (Munoz et al., 2009) (< 2 \u00d7 10 points), the Sydney Urban Objects data set (De Deuge et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 37, "context": ", 2013), the Paris-rue-Madame database (Serna et al., 2014) and data from the IQmulus & TerraMobilita Contest (Vallet et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 45, "context": ", 2014) and data from the IQmulus & TerraMobilita Contest (Vallet et al., 2015).", "startOffset": 58, "endOffset": 79}, {"referenceID": 0, "context": "Encoder-decoder architectures, like SegNet (Badrinarayanan et al., 2015), are able to infer the labels of an entire image at once.", "startOffset": 43, "endOffset": 72}, {"referenceID": 7, "context": "Deep architectures can also be combined with Conditional Random Fields (CRF) (Chen et al., 2016).", "startOffset": 77, "endOffset": 96}, {"referenceID": 12, "context": "Typical features encode surface properties based on the covariance tensor of a point\u2019s neighborhood (Demantk\u00e9 et al., 2011) or a randomized set of histograms (Blomley et al.", "startOffset": 100, "endOffset": 123}, {"referenceID": 1, "context": ", 2011) or a randomized set of histograms (Blomley et al., 2014).", "startOffset": 42, "endOffset": 64}, {"referenceID": 14, "context": ", 2017) or sparse voxel grids (Engelcke et al., 2017).", "startOffset": 30, "endOffset": 53}, {"referenceID": 17, "context": "We convert color values of the scans to separate images (without depth) with cube mapping (Greene, 1986).", "startOffset": 90, "endOffset": 104}, {"referenceID": 24, "context": "We chose the associate hierarchical fields method (Ladicky et al., 2013) for semantic segmentation because it has proven to deliver good performance for a variety of tasks (e.", "startOffset": 50, "endOffset": 72}, {"referenceID": 29, "context": "The method works as follows: four different types of features \u2013 texton (Malik et al., 2001), SIFT (Lowe, 2004), local quantized ternary patters (Hussain and Triggs, 2012) and self-similarity features (Shechtman and Irani, 2007) \u2013 are extracted densely per image pixel.", "startOffset": 71, "endOffset": 91}, {"referenceID": 28, "context": ", 2001), SIFT (Lowe, 2004), local quantized ternary patters (Hussain and Triggs, 2012) and self-similarity features (Shechtman and Irani, 2007) \u2013 are extracted densely per image pixel.", "startOffset": 14, "endOffset": 26}, {"referenceID": 21, "context": ", 2001), SIFT (Lowe, 2004), local quantized ternary patters (Hussain and Triggs, 2012) and self-similarity features (Shechtman and Irani, 2007) \u2013 are extracted densely per image pixel.", "startOffset": 60, "endOffset": 86}, {"referenceID": 38, "context": ", 2001), SIFT (Lowe, 2004), local quantized ternary patters (Hussain and Triggs, 2012) and self-similarity features (Shechtman and Irani, 2007) \u2013 are extracted densely per image pixel.", "startOffset": 116, "endOffset": 143}, {"referenceID": 44, "context": "We use multi-class boosting (Torralba et al., 2004) as classifier and the most discriminative weak features are found as explained in (Shotton et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 39, "context": ", 2004) as classifier and the most discriminative weak features are found as explained in (Shotton et al., 2006).", "startOffset": 90, "endOffset": 112}, {"referenceID": 10, "context": "Superpixels are extracted via mean-shift (Comaniciu and Meer, 2002) with 3 sets of coarse-to-fine parameters as described in (Ladicky et al.", "startOffset": 41, "endOffset": 67}, {"referenceID": 24, "context": "Superpixels are extracted via mean-shift (Comaniciu and Meer, 2002) with 3 sets of coarse-to-fine parameters as described in (Ladicky et al., 2013).", "startOffset": 125, "endOffset": 147}, {"referenceID": 22, "context": "Pixel-based and superpixelbased classifiers with additional smoothness priors over pixels and superpixels are combined in a probabilistic fashion in a conditional random field framework as proposed in (Kohli et al., 2008).", "startOffset": 201, "endOffset": 221}, {"referenceID": 4, "context": "The most probable solution of the associative hierarchical optimization problem is found using the move making (Boykov et al., 2001) graph-cut based algorithm (Boykov and Kolmogorov, 2004), with appropriate graph construction for higher-order potentials (Ladicky et al.", "startOffset": 111, "endOffset": 132}, {"referenceID": 3, "context": ", 2001) graph-cut based algorithm (Boykov and Kolmogorov, 2004), with appropriate graph construction for higher-order potentials (Ladicky et al.", "startOffset": 34, "endOffset": 63}, {"referenceID": 24, "context": ", 2001) graph-cut based algorithm (Boykov and Kolmogorov, 2004), with appropriate graph construction for higher-order potentials (Ladicky et al., 2013).", "startOffset": 129, "endOffset": 151}, {"referenceID": 47, "context": "The second baseline was inspired by (Weinmann et al., 2015).", "startOffset": 36, "endOffset": 59}, {"referenceID": 46, "context": "The feautre set extracted at each level is an extension of the one decribed in (Weinmann et al., 2013).", "startOffset": 79, "endOffset": 102}, {"referenceID": 19, "context": "Please refer to (Hackel et al., 2016) for details.", "startOffset": 16, "endOffset": 37}, {"referenceID": 30, "context": "We design our baseline for the point cloud classification task following recent VoxNet (Maturana and Scherer, 2015) and ShapeNet (Wu et al.", "startOffset": 87, "endOffset": 115}, {"referenceID": 48, "context": "We design our baseline for the point cloud classification task following recent VoxNet (Maturana and Scherer, 2015) and ShapeNet (Wu et al., 2015) 3D encoding ideas.", "startOffset": 129, "endOffset": 146}, {"referenceID": 20, "context": "work (He et al., 2016), to have the least amount of parameters per layer and, hence, reduce both the risk of overfitting and the computational cost.", "startOffset": 5, "endOffset": 22}, {"referenceID": 41, "context": "For the 5 separate network paths that act on different resolutions, we use a VGG-like (Simonyan and Zisserman, 2014) architecture:", "startOffset": 86, "endOffset": 116}, {"referenceID": 49, "context": "We use the popular adadelta algorithm (Zeiler, 2012) for optimization, an extension of stochastic gradient decent (Bottou, 2010).", "startOffset": 38, "endOffset": 52}, {"referenceID": 2, "context": "We use the popular adadelta algorithm (Zeiler, 2012) for optimization, an extension of stochastic gradient decent (Bottou, 2010).", "startOffset": 114, "endOffset": 128}, {"referenceID": 9, "context": "Our network is implemented in C++ and Lua and uses the Torch7 framework (Collobert et al., 2011) for deep learning.", "startOffset": 72, "endOffset": 96}, {"referenceID": 15, "context": "We follow Pascal VOC challenge\u2019s (Everingham et al., 2010) choice of the main segmentation evaluation measure and use Intersection over Union (IoU ) averaged over all classes.", "startOffset": 33, "endOffset": 58}], "year": 2017, "abstractText": "This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.", "creator": "LaTeX with hyperref package"}}}