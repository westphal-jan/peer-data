{"id": "1704.03899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward", "abstract": "Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a \"policy network\" and a \"value network\" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.", "histories": [["v1", "Wed, 12 Apr 2017 18:55:03 GMT  (5922kb,D)", "http://arxiv.org/abs/1704.03899v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["zhou ren", "xiaoyu wang", "ning zhang", "xutao lv", "li-jia li"], "accepted": false, "id": "1704.03899"}, "pdf": {"name": "1704.03899.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward", "authors": ["Zhou Ren", "Xiaoyu Wang", "Ning Zhang", "Xutao Lv", "Li-Jia Li"], "emails": ["xutao.lv}@snap.com", "lijiali@cs.stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "It is interesting that we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in a time, in which we feel ourselves in which we feel ourselves in a time, in a time, in a time, in which we feel ourselves in which we feel a time, in which we are in which we feel ourselves in a time, in which we are in a time, in which we are in which we are in a time, in which we feel a time, in which we are in which we are in a time, in which we feel a time, in which we are in which we are in a time"}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Image captioning", "text": "Many approaches to caption have been suggested in the literature. Early approaches addressed this problem using the bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them with language models. Recently, inspired by the successful use of neural networks in machine translation [4], the encoder decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] was brought to caption. Researchers chose such a framework because the \"translation\" of an image into a set of tasks in machine translation was analogous. Approaches following this framework generally coded an image as a single feature vector by revolutionary neural networks [22, 6, 39, 41] and then fed such a framework into recurrent neural networks [14] to generate various image captions."}, {"heading": "2.2. Decision-making", "text": "Decision-making is the core problem in computer games [38], control theory [32], navigation and route planning [49], etc. In these problems, there are agents who interact with the environment, perform a series of actions and aim to fulfil some predefined objectives. Learning skills [45, 21, 40, 31], known as \"a machine learning method that describes how software agents should take action in an environment to maximize a notion of cumulative reward,\" lends itself well to the task of decision-making. Recently, the computer-go program was designed at the professional level by Silver et al. [38] using deep neural networks and tree-hunting at Monte Carlo. Game control at the human level [32] was achieved through deep Q-learning. A visual navigation system [49] was recently proposed based on skill imitations performed by actors and critics."}, {"heading": "3. Deep Reinforcement Learning-based Image", "text": "Caption: In this section, we first define our formulation for a learning-based caption with deep amplification and propose a novel reward function defined by visual-semantic embedding. We then present our training method and our follow-up mechanism."}, {"heading": "3.1. Problem formulation", "text": "We formulate captions as a decision-making process. In decision-making, there is an actor who interacts with the environment and performs a series of actions to optimize a goal. In caption, the goal is to generate a sentence S = {w1, w2,..., wT} against an image I that correctly describes the image content, with wi being a word in sentence S and T. Our model, including the political network p\u03c0 and the value network v\u03b8, can be considered an actor; the environment is the given image I and the previously predicted words {w1,..., wt}; and one action consists in predicting the next word wt + 1."}, {"heading": "3.1.1 State and action space", "text": "A decision-making process consists of a series of actions. After each action a, a state s is observed. In our problem, the state st at a certain point in time consists of the picture I and the words predicted up to t, {w1,..., wt}. The action space is the dictionary Y from which the words are taken, i.e. in the action Y."}, {"heading": "3.1.2 Policy network", "text": "The political network p\u03c0 provides the probability that the agent will take action in each state, p\u03c0 (at | st), with the current state st = {I, w1,..., wt} and the action at = wt + 1. In this essay, we use a Convolutionary Neural Network (CNN) and a Recursive Neural Network (RNN) to construct our political network, which is called CNNp and RNNp. It is similar to the basic model of caption [44] used within the framework of the encoder decoder. As shown in Figure 2, we first use CNNp to encode the visual information of the image. The visual information is then fed into the initial input nodes x0-Rn of RNNp. As the hidden state ht-Rm of RNNp develops over time t, politics is used at any time to take action."}, {"heading": "3.1.3 Value network", "text": "Before introducing our value network v\u03b8, we first define the value function vp of a policy P. vp is defined as predicting the total reward r (to be defined later in Section 3.2) based on the observed state st, provided the decision process follows a policy p, i.e. vp (s) = E [r | st = s, at... T \u0445 p] (5) We approach the value function based on a value network, vTB (s) \u2248 vp (s). It serves as an evaluation of the state st = {I, w1,..., wt}. As shown in Figure 3, our value network consists of a CNN, an RNN and a Multilayer Perceptron (MLP) designated as CNNv, RNNv and MLPv. Our value network absorbs the raw image and sentence inputs. CNNv is used to encode the visual information of I, RNNV is designed to simultaneously investigate the structure of a set of a set of 4 components."}, {"heading": "3.2. Reward defined by visual-semantic embedding", "text": "In our decision framework, it is important to define a concrete and reasonable optimization goal, i.e., the reward for learning the attachment. We propose to use visual-semantic embedding similarities as a reward. Visual-semantic embedding has been successfully applied to image classification [11, 37], retrieval [19, 36, 33], etc. Our embedding model consists of a CNN, an RNN, and a linear mapping layer called CNNe, RNNe, and fe. By learning to assign images and sentences to a semantic embedding space, it provides a measure of the similarity between images and sentences. In the face of a set S, its embedding function is represented by the last hidden state of RNNe, i.e., h \u2032 T (S) and S (S)."}, {"heading": "3.3. Training using deep reinforcement learning", "text": "After this, we learn p\u03c0 and v\u03b8 in two steps. In the first step, we train the political network p\u03c0 = entropy loss, with the loss function as Lp \u2032 = \u2212 log p (w1,..., wT | I; \u03c0) = \u2212 \u2211 Tt = 1 log p\u03c0 (at | st). And we train the value network by minimizing the average square loss. \u2212 r | | 2, where r is the final reward of the generated sentence and si denotes a randomly selected state in the generation process. For a generated sentence, successive states are strongly correlated in that they differ by just one word, but the regression target is shared for each entire labeling process. Therefore, we randomly stamp a single state out of each sentence to prevent overadjustment. In the second step, we train p\u03c0 and vaxy together with deep fixation learning (RL)."}, {"heading": "3.4. Lookahead inference with policy network and", "text": "Value networkOne of the most important contributions of the proposed framework to the existing framework is the inference mechanism = b \u00b7 time. For decision-making problems, the inference is guided by a local guide and a global guide, e.g. AlphaGo [38] uses MCTS to combine the two guides. However, for captions, we propose a novel lookahead inference mechanism that combines the local guidance of the political network and the global guidance of the value network. The learned value network provides a predictive assessment for each decision that can complement the political network and collectively generate capabilities. Beam Search (BS) is the most widely used method for decoding existing captions approaches, where the top B scoring candidates are highly rated at each step. Let us designate the set of B sequences that BS at the time did not call Wdte = {w1, dte,..., dte} where each sequence is generated by then."}, {"heading": "4. Experiments", "text": "In this section, we conduct extensive experiments to evaluate the proposed framework. All reported results are calculated using the Microsoft COCO Capture Evaluation Tool [2], including the BLEU, Meteor, Rouge-L, and CIDER metrics commonly used for fair and thorough performance measurements. First, we discuss the data set and the details of implementation, then we compare the proposed methodology with modern approaches to captioning. Finally, we conduct detailed analysis of our methodology."}, {"heading": "4.1. Dataset and implementation details", "text": "We evaluate our method on the widely used MS COCO dataset [29] for the caption task. For a fair comparison, we take the commonly used columns proposed in [17], using 82,783 images for training, 5,000 images for validation, and 5,000 images for testing. Each image is given at least five captions by different AMT workers. We follow [17] the captions (i.e., building dictionaries, tokenization of captions).Network architecture As shown in Figure 2 and 3, both CNN and an RNN contain. We adopt the same CNN and RNN architectures for them, but train them independently. We use VGG-16 [39] as our CNN architecture and LSTM as our RNN architecture."}, {"heading": "4.2. Comparing with state-of-the-art methods", "text": "In Table 1, we provide a summary of the results of our method and existing methods. In addition, we get a state-of-the-art performance on MS COCO in most evaluation metrics. Note that Semantic ATT [48] uses rich additional data from social media to train its visual attribute predictor, and DCC [13] uses external data to prove its unique transmission capacity, making its results incomparable with other methods that do not use additional training data. Surprisingly, our method performs better even without external training data [48, 13]. Compared to methods other than [48, 13], our approach shows significant improvements in all metrics, except Bleu-1, where our method ranks second. Bleu1 is related to the accuracy of individual words, the performance gap between our method and [46] may be due to different preprocessing for word vocabularies [48, 13]."}, {"heading": "4.3. How much each component contributes?", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "4.4. Value network architecture analysis", "text": "As in Figure 3, we use CNNv and RNNv to extract visual and semantic information from the raw image and sentence input. As the hidden state in the political network is also a representation of each state at all times, a natural question is: \"Can we use the hidden state directly?\" To answer this question, we construct two variants of our value network: the first variant, called the (hidden UN), consists of an MLPV over the hidden state of RNNp's politics; the second variant (hidden In-UN) consists of an MLPv over the concatenation of the hidden state of RNNp's politics and the visual input x0 of RNNp's politics. The results are shown in Table 2. As we see, both variants that use the hidden state of politics do not work well compared to our hidden state of RNNNP and the hidden state of RNNNp."}, {"heading": "4.5. Parameter sensitivity analysis", "text": "There are two important hyperparameters in our method, \u03bb in Equation 10 and the beam size. In this section, we will analyze their sensitivity to answer question 5) above. In Table 3, we will show the assessment of the impact on our method. As in Equation 10, \u03bb is a hyperparameter that combines policies and value networks in a predictive way. As shown in Table 3, the best performance is when it drops from 0.4 to 0 or rises from 0.4 to 1, overall performance decreases monotonously. This confirms the importance of both networks; we should not stress too much on both networks in predictive terms. Also, the best performance is when it drops from 0.4 to 0.0 or when it rises from 0.4 to 1."}, {"heading": "5. Conclusion", "text": "In this paper, we present a novel decision-making framework for captions that delivers state-of-the-art performance compared to previous encoder decoder frameworks. Unlike previous encoder decoder frameworks, our method uses a political network and a value network to generate captions. The political network serves as a local guide and the value network as a global and forward-looking guide. To learn both networks, we use an actor-critical learning approach with novel visual-semantic embedding rewards. We conduct detailed analyses of our framework to understand its merits merits and characteristics. Future work will include improving network architectures and studying the reward design by considering other embedding measures."}], "references": [{"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "ICML,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "C.L. Zitnick"], "venue": "arXiv:1504.00325,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv:1412.3555,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: a large-scale hierachical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["D. Elliott", "F. Keller"], "venue": "EMNLP,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "From from captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["L.A. Hendricks", "S. Venugopalan", "M. Rohrbach", "R. Mooney", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9:1735\u20131780,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Guiding the longshort term memory model for image caption generation", "author": ["X. Jia", "E. Gavves", "B. Fernando", "T. Tuytelaars"], "venue": "ICCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "TACL,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Shifts in selective visual attention: towards the underlying neural circuitry", "author": ["C. Koch", "S. Ullman"], "venue": "Matters of intelligence, pages 115\u2013 141,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1987}, {"title": "Actor-critic algorithms", "author": ["V. Konda", "J. Tsitsiklis"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "ACL,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "The meteor metric for automatic evaluation of machine translation", "author": ["A. Lavie", "M. Denkowski"], "venue": "Machine Translation,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Simple image description generator via a linear phrase-based approach", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "ICLR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "CoNLL,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "WAS,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "L. Bourdev", "R. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "C.L. Zitnick", "P. Dollar"], "venue": "ECCV,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "ICLR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "ACL,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "ICLR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-instance visualsemantic embedding", "author": ["Z. Ren", "H. Jin", "Z. Lin", "C. Fang", "A. Yuille"], "venue": "arXiv:1512.06963,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint image-text representation by gaussian visual-semantic embedding", "author": ["Z. Ren", "H. Jin", "Z. Lin", "C. Fang", "A. Yuille"], "venue": "ACM Multimedia,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "NIPS,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2000}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Diverse beam search: Decoding diverse solutions from neural sequence models", "author": ["A. Vijayakumar", "M. Cogswell", "R.R. Selvaraju", "Q. Sun", "S. Lee", "D. Crandall", "D. Batra"], "venue": "arXiv:1610.02424,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R. Williams"], "venue": "Machine Learning, 8:229\u2013256,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J.L. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daume III", "Y. Aloimonos"], "venue": "EMNLP,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "CVPR,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta"], "venue": "arXiv:1609.05143,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 43, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 29, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 16, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 6, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 45, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 14, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 47, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 42, "context": "Recent state-of-the-art approaches [3, 44, 30, 17, 7, 46, 15, 48, 43] follow an encoder-decoder framework to generate captions for the images.", "startOffset": 35, "endOffset": 69}, {"referenceID": 37, "context": "Reinforcement learning has been widely used in gaming [38], control theory [32], etc.", "startOffset": 54, "endOffset": 58}, {"referenceID": 31, "context": "Reinforcement learning has been widely used in gaming [38], control theory [32], etc.", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "In this paper, we propose to train using an actor-critic model [21] with reward driven by visualsemantic embedding [11, 19, 36, 37].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "In this paper, we propose to train using an actor-critic model [21] with reward driven by visualsemantic embedding [11, 19, 36, 37].", "startOffset": 115, "endOffset": 131}, {"referenceID": 18, "context": "In this paper, we propose to train using an actor-critic model [21] with reward driven by visualsemantic embedding [11, 19, 36, 37].", "startOffset": 115, "endOffset": 131}, {"referenceID": 35, "context": "In this paper, we propose to train using an actor-critic model [21] with reward driven by visualsemantic embedding [11, 19, 36, 37].", "startOffset": 115, "endOffset": 131}, {"referenceID": 36, "context": "In this paper, we propose to train using an actor-critic model [21] with reward driven by visualsemantic embedding [11, 19, 36, 37].", "startOffset": 115, "endOffset": 131}, {"referenceID": 28, "context": "Extensive experiments on the Microsoft COCO dataset [29] show that the proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics, including BLEU [34], Meteor [25], Rouge [28] and CIDEr [42].", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Extensive experiments on the Microsoft COCO dataset [29] show that the proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics, including BLEU [34], Meteor [25], Rouge [28] and CIDEr [42].", "startOffset": 192, "endOffset": 196}, {"referenceID": 24, "context": "Extensive experiments on the Microsoft COCO dataset [29] show that the proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics, including BLEU [34], Meteor [25], Rouge [28] and CIDEr [42].", "startOffset": 205, "endOffset": 209}, {"referenceID": 27, "context": "Extensive experiments on the Microsoft COCO dataset [29] show that the proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics, including BLEU [34], Meteor [25], Rouge [28] and CIDEr [42].", "startOffset": 217, "endOffset": 221}, {"referenceID": 41, "context": "Extensive experiments on the Microsoft COCO dataset [29] show that the proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics, including BLEU [34], Meteor [25], Rouge [28] and CIDEr [42].", "startOffset": 232, "endOffset": 236}, {"referenceID": 9, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 22, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 26, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 46, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 23, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 7, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 25, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 8, "context": "Early approaches tackled this problem using bottom-up paradigm [10, 23, 27, 47, 24, 8, 26, 9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models.", "startOffset": 63, "endOffset": 93}, {"referenceID": 3, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 43, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 29, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 16, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 6, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 45, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 14, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 47, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 42, "context": "Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3, 44, 30, 17, 7, 46, 15, 48, 43] has been brought to image captioning.", "startOffset": 118, "endOffset": 152}, {"referenceID": 21, "context": "Approaches following this framework generally encoded an image as a single feature vector by convolutional neural networks [22, 6, 39, 41], and then fed such vector into recurrent neural networks [14, 5] to generate captions.", "startOffset": 123, "endOffset": 138}, {"referenceID": 5, "context": "Approaches following this framework generally encoded an image as a single feature vector by convolutional neural networks [22, 6, 39, 41], and then fed such vector into recurrent neural networks [14, 5] to generate captions.", "startOffset": 123, "endOffset": 138}, {"referenceID": 38, "context": "Approaches following this framework generally encoded an image as a single feature vector by convolutional neural networks [22, 6, 39, 41], and then fed such vector into recurrent neural networks [14, 5] to generate captions.", "startOffset": 123, "endOffset": 138}, {"referenceID": 40, "context": "Approaches following this framework generally encoded an image as a single feature vector by convolutional neural networks [22, 6, 39, 41], and then fed such vector into recurrent neural networks [14, 5] to generate captions.", "startOffset": 123, "endOffset": 138}, {"referenceID": 13, "context": "Approaches following this framework generally encoded an image as a single feature vector by convolutional neural networks [22, 6, 39, 41], and then fed such vector into recurrent neural networks [14, 5] to generate captions.", "startOffset": 196, "endOffset": 203}, {"referenceID": 4, "context": "Approaches following this framework generally encoded an image as a single feature vector by convolutional neural networks [22, 6, 39, 41], and then fed such vector into recurrent neural networks [14, 5] to generate captions.", "startOffset": 196, "endOffset": 203}, {"referenceID": 16, "context": "Karpathy and Fei-Fei [17], Fang et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "[9] presented methods to enhance their models by detecting objects in images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "To mimic the visual system of humans [20], spatial attention [46] and semantic attention [48] were proposed to direct the model to attend to the meaningful fine details.", "startOffset": 37, "endOffset": 41}, {"referenceID": 45, "context": "To mimic the visual system of humans [20], spatial attention [46] and semantic attention [48] were proposed to direct the model to attend to the meaningful fine details.", "startOffset": 61, "endOffset": 65}, {"referenceID": 47, "context": "To mimic the visual system of humans [20], spatial attention [46] and semantic attention [48] were proposed to direct the model to attend to the meaningful fine details.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "Dense captioning [16] was proposed to handle the localization and captioning tasks simultaneously.", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "[35] proposed a sequence-level training algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Decision-making is the core problem in computer gaming [38], control theory [32], navigation and path planning [49], etc.", "startOffset": 55, "endOffset": 59}, {"referenceID": 31, "context": "Decision-making is the core problem in computer gaming [38], control theory [32], navigation and path planning [49], etc.", "startOffset": 76, "endOffset": 80}, {"referenceID": 48, "context": "Decision-making is the core problem in computer gaming [38], control theory [32], navigation and path planning [49], etc.", "startOffset": 111, "endOffset": 115}, {"referenceID": 44, "context": "Reinforcement learning [45, 21, 40, 31], known as \u201ca machine learning technique concerning how software agent ought to take actions in an environment so as to maximize some notion of cumulative reward\u201d, is well suited for the task of decisionmaking.", "startOffset": 23, "endOffset": 39}, {"referenceID": 20, "context": "Reinforcement learning [45, 21, 40, 31], known as \u201ca machine learning technique concerning how software agent ought to take actions in an environment so as to maximize some notion of cumulative reward\u201d, is well suited for the task of decisionmaking.", "startOffset": 23, "endOffset": 39}, {"referenceID": 39, "context": "Reinforcement learning [45, 21, 40, 31], known as \u201ca machine learning technique concerning how software agent ought to take actions in an environment so as to maximize some notion of cumulative reward\u201d, is well suited for the task of decisionmaking.", "startOffset": 23, "endOffset": 39}, {"referenceID": 30, "context": "Reinforcement learning [45, 21, 40, 31], known as \u201ca machine learning technique concerning how software agent ought to take actions in an environment so as to maximize some notion of cumulative reward\u201d, is well suited for the task of decisionmaking.", "startOffset": 23, "endOffset": 39}, {"referenceID": 37, "context": "[38] using deep neural networks and Monte Carlo Tree Search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Human-level gaming control [32] was achieved through deep Q-learning.", "startOffset": 27, "endOffset": 31}, {"referenceID": 48, "context": "A visual navigation system [49] was proposed recently based on actor-critic reinforcement learning model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 34, "context": "One previous work in text generation [35] has used REINFORCE [45] to train its model by directly optimizing a user-specified evaluation metric.", "startOffset": 37, "endOffset": 41}, {"referenceID": 44, "context": "One previous work in text generation [35] has used REINFORCE [45] to train its model by directly optimizing a user-specified evaluation metric.", "startOffset": 61, "endOffset": 65}, {"referenceID": 34, "context": "Such metricdriven approach [35] is hard to generalize to other metrics.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "In this paper, we propose a training method using actor-critic reinforcement learning [21] driven by visual-semantic embedding [11, 19], which performs well across different evaluation metrics without re-training.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "In this paper, we propose a training method using actor-critic reinforcement learning [21] driven by visual-semantic embedding [11, 19], which performs well across different evaluation metrics without re-training.", "startOffset": 127, "endOffset": 135}, {"referenceID": 18, "context": "In this paper, we propose a training method using actor-critic reinforcement learning [21] driven by visual-semantic embedding [11, 19], which performs well across different evaluation metrics without re-training.", "startOffset": 127, "endOffset": 135}, {"referenceID": 34, "context": "Our approach shows significant performance improvement over [35].", "startOffset": 60, "endOffset": 64}, {"referenceID": 34, "context": "Moreover, we use a decision-making framework to generate captions, while [35] follows the existing encoder-decoder framework.", "startOffset": 73, "endOffset": 77}, {"referenceID": 43, "context": "It is similar to the basic image captioning model [44] used in the encoderdecoder framework.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "Visual-semantic embedding has been successfully applied to image classification [11, 37], retrieval [19, 36, 33], etc.", "startOffset": 80, "endOffset": 88}, {"referenceID": 36, "context": "Visual-semantic embedding has been successfully applied to image classification [11, 37], retrieval [19, 36, 33], etc.", "startOffset": 80, "endOffset": 88}, {"referenceID": 18, "context": "Visual-semantic embedding has been successfully applied to image classification [11, 37], retrieval [19, 36, 33], etc.", "startOffset": 100, "endOffset": 112}, {"referenceID": 35, "context": "Visual-semantic embedding has been successfully applied to image classification [11, 37], retrieval [19, 36, 33], etc.", "startOffset": 100, "endOffset": 112}, {"referenceID": 32, "context": "Visual-semantic embedding has been successfully applied to image classification [11, 37], retrieval [19, 36, 33], etc.", "startOffset": 100, "endOffset": 112}, {"referenceID": 37, "context": "Following [38], we learn p\u03c0 and v\u03b8 in two steps.", "startOffset": 10, "endOffset": 14}, {"referenceID": 44, "context": "Viewing the problem as a partially observable Markov decision process, however, allows us to bring techniques from the RL literature to bear: As shown in [45, 40, 31], a sample approximation to the gradient is:", "startOffset": 154, "endOffset": 166}, {"referenceID": 39, "context": "Viewing the problem as a partially observable Markov decision process, however, allows us to bring techniques from the RL literature to bear: As shown in [45, 40, 31], a sample approximation to the gradient is:", "startOffset": 154, "endOffset": 166}, {"referenceID": 30, "context": "Viewing the problem as a partially observable Markov decision process, however, allows us to bring techniques from the RL literature to bear: As shown in [45, 40, 31], a sample approximation to the gradient is:", "startOffset": 154, "endOffset": 166}, {"referenceID": 48, "context": "The action space of image captioning is in the order of 10 which equals the vocabulary size, while that of visual navigation in [49] is only 4, which indicates four directions to go.", "startOffset": 128, "endOffset": 132}, {"referenceID": 34, "context": "To handle this problem, we follow [35] to apply curriculum learning [1] to train our actor-critic model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "To handle this problem, we follow [35] to apply curriculum learning [1] to train our actor-critic model.", "startOffset": 68, "endOffset": 71}, {"referenceID": 34, "context": "Please refer to [35] for details.", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": ", AlphaGo [38] utilized MCTS to combine both guidances.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "All the reported results are computed using Microsoft COCO caption evaluation tool [2], including the metrics BLEU, Meteor, Rouge-L and CIDEr, which are commonly used together for fair and thorough performance measure.", "startOffset": 83, "endOffset": 86}, {"referenceID": 28, "context": "Dataset We evaluate our method on the widely used MS COCO dataset [29] for the image captioning task.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "For fair comparison, we adopt the commonly used splits proposed in [17], which use 82,783 images for training, 5,000 images for validation, and 5,000 images for testing.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "We follow [17] to preprocess the captions (i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 38, "context": "We use VGG-16 [39] as our CNN architecture and LSTM [14] as our RNN architecture.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "We use VGG-16 [39] as our CNN architecture and LSTM [14] as our RNN architecture.", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": ", ResNet [12], GRU [5], etc.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": ", ResNet [12], GRU [5], etc.", "startOffset": 19, "endOffset": 22}, {"referenceID": 18, "context": "We followed [19] to use VGG-16 [39] as CNNe and GRU [5] as RNNe.", "startOffset": 12, "endOffset": 16}, {"referenceID": 38, "context": "We followed [19] to use VGG-16 [39] as CNNe and GRU [5] as RNNe.", "startOffset": 31, "endOffset": 35}, {"referenceID": 4, "context": "We followed [19] to use VGG-16 [39] as CNNe and GRU [5] as RNNe.", "startOffset": 52, "endOffset": 55}, {"referenceID": 17, "context": "Training details In training, we use Adam [18] algorithm to do model updating.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": "Note that Semantic ATT [48] utilized rich extra data from social media to train their visual attribute predictor, and", "startOffset": 23, "endOffset": 27}, {"referenceID": 43, "context": "Methods Bleu-1 Bleu-2 Bleu-3 Bleu-4 METEOR Rouge-L CIDEr Google NIC [44] 0.", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "m-RNN [30] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "BRNN [17] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "LRCN [7] 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "MSR/CMU [3] \u2212 \u2212 \u2212 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 45, "context": "204 \u2212 \u2212 Spatial ATT [46] 0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "gLSTM [15] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "MIXER [35] \u2212 \u2212 \u2212 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 47, "context": "29 \u2212 \u2212 \u2212 Semantic ATT [48] \u2217 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "243 \u2212 \u2212 DCC [13] \u2217 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "DCC [13] utilized external data to prove its unique transfer capacity.", "startOffset": 4, "endOffset": 8}, {"referenceID": 47, "context": "Surprisingly, even without external training data, our method outperforms [48, 13].", "startOffset": 74, "endOffset": 82}, {"referenceID": 12, "context": "Surprisingly, even without external training data, our method outperforms [48, 13].", "startOffset": 74, "endOffset": 82}, {"referenceID": 47, "context": "Comparing to methods other than [48, 13], our approach shows significant improvements in all the metrics except Bleu-1 in which our method ranks the second.", "startOffset": 32, "endOffset": 40}, {"referenceID": 12, "context": "Comparing to methods other than [48, 13], our approach shows significant improvements in all the metrics except Bleu-1 in which our method ranks the second.", "startOffset": 32, "endOffset": 40}, {"referenceID": 45, "context": "Bleu1 is related to single word accuracy, the performance gap of Bleu-1 between our method and [46] may be due to different preprocessing for word vocabularies.", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": "MIXER [35] is a metric-driven trained method.", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "A model trained with Bleu-4 using [35] is hard to generalize to other metrics.", "startOffset": 34, "endOffset": 38}, {"referenceID": 43, "context": "Especially, considering our policy network shown in Figure 2 is based on a mechanism similar to the very basic image captioning model similar to Google NIC [44], such significant improvement over [44] validates the effectiveness of the proposed decision-making framework that utilizes both policy and value networks.", "startOffset": 156, "endOffset": 160}, {"referenceID": 43, "context": "Especially, considering our policy network shown in Figure 2 is based on a mechanism similar to the very basic image captioning model similar to Google NIC [44], such significant improvement over [44] validates the effectiveness of the proposed decision-making framework that utilizes both policy and value networks.", "startOffset": 196, "endOffset": 200}, {"referenceID": 16, "context": "As discovered in previous work such as [17], the image captioning performance becomes worse as the beam size gets larger.", "startOffset": 39, "endOffset": 43}], "year": 2017, "abstractText": "Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a \u201cpolicy network\u201d and a \u201cvalue network\u201d to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-ofthe-art approaches across different evaluation metrics.", "creator": "LaTeX with hyperref package"}}}