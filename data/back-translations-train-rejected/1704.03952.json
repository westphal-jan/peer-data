{"id": "1704.03952", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Virtual to Real Reinforcement Learning for Autonomous Driving", "abstract": "Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.", "histories": [["v1", "Thu, 13 Apr 2017 00:03:40 GMT  (1543kb,D)", "http://arxiv.org/abs/1704.03952v1", null], ["v2", "Tue, 9 May 2017 08:09:40 GMT  (4942kb,D)", "http://arxiv.org/abs/1704.03952v2", null], ["v3", "Thu, 11 May 2017 16:56:54 GMT  (2131kb,D)", "http://arxiv.org/abs/1704.03952v3", null], ["v4", "Tue, 26 Sep 2017 17:22:04 GMT  (8581kb,D)", "http://arxiv.org/abs/1704.03952v4", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["yurong you", "xinlei pan", "ziyan wang", "cewu lu"], "accepted": false, "id": "1704.03952"}, "pdf": {"name": "1704.03952.pdf", "metadata": {"source": "CRF", "title": "Virtual to Real Reinforcement Learning for Autonomous Driving", "authors": ["Yurong You", "Xinlei Pan", "Ziyan Wang", "Cewu Lu"], "emails": ["yurongyou@sjtu.edu.cn", "xinleipan@berkeley.edu", "zy-wang13@mails.tsinghua.edu.cn", "lu-cw@cs.sjtu.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "The fact is that we are able to hide, at a time when we are able, when we are able to change the world, when we are able to change the world, when we are able to change the world, and when we are able to change the world, and when we are able to change the world."}, {"heading": "2. Related Work", "text": "In fact, it is a purely mental game, which is about finding a solution, which is about finding a solution."}, {"heading": "3. Reinforcement Learning on Realistic", "text": "FramesWe aim to successfully apply a trained agent in virtual environment to driving in the real world. One of the biggest gaps is that what RL has observed are frames rendered by a simulator. These frames differ in appearance from real frames captured by a camera. Therefore, we proposed a realistic translation network to convert virtual frames to realistic frames. Inspired by the work of the Pix2pix network [12], our network comprises two modules, namely virtual frame-to-parsing and parsing-to-realistic network. The first maps virtual frames to scene-to-parsing image. The second translates scenario parsing to realistic frames with similar scene structure as input-virtual frame. These two modules achieve realistic frames and maintain the scene structure of input-virtual frames. The architecture of realistic translation networks is realistically illustrated on Figure 1. Finally, we get a train to reinforce the realistic agent by using a self-driving agent."}, {"heading": "3.1. Realistic Translation Network", "text": "Since there is no direct link between images from the virtual world, such as the images we see in the TORCS environment, and images from the real world, such as the data in [3], it would be cumbersome to directly map images from the virtual world to images from the real world. However, these two types of images express both driving scenes. We can translate them by displaying scenes. Therefore, our proposed realistic translation network tries to establish the connection by first analyzing the virtual image and then translating the representation of scenes into a realistic image."}, {"heading": "3.2. Training Framework", "text": "We present how to train two main modules as follows: Virtual-to-Parsing Network. This network aims to convert the virtual frames rendered by TORCS into scene parsing results. We use the network of [2] to analyze the image into different semantic regions. SegNet's network structure is a deep, fully revolutionary neural network architecture that follows an encoder decoder mode. To use the SegNet to segment images in the virtual world, the network is trained on CityScape driving scene segmentation data set [8]. Then, the trained model is used to analyze virtual scenes in TORCS [31]. Parsing-to-Realistic Network. To produce realistic images, we learn how to translate the scene parsing result into realistic images with an unchanged scene structure. To this end, the architecture is adopted from [12] Scenario set 6 and we train with Aynpathos."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experiments Setting", "text": "In order to demonstrate the effectiveness of our method, we have trained three models: the first is our proposed reinforcement learning model with a realistic translation network; the second is a reinforcement learning model with virtual input as state representation; and the third is a supervised end-to-end learning model, which is trained on real data with action markers. We further evaluate these models using real driving data held with action markers."}, {"heading": "4.2. Dataset", "text": "The virtual image data is collected in the aalborg environment in TORCS [31], and a total of 1673 images covering the entire driving cycle of the aalborg environment are collected. Real driving video data with action signs is collected [6] on a sunny day on the motorway with detailed comments on the steering angle per frame."}, {"heading": "4.3. Training Details", "text": "In fact, most of them will be able to put themselves in a situation where they have to stand in their own way."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Image Segmentation Results", "text": "In order to segment both virtual and real images, we used an image segmentation model trained on the cityscape [8]. Examples of this are shown in Figure 2. Although the original virtual image and the real image look quite different, their results in scene analysis are very similar. Therefore, it makes sense to use scene analysis as an intermediate step to combine virtual and real images."}, {"heading": "5.2. Qualitative Result of Realistic Translation Network", "text": "Figure 3 shows some representative results of our image translation network. The strange columns are virtual images in TORCS, and the even columns are translated images in the real world. The images in the virtual environment appear darker than the translated images, because the real images with which the translation network is trained are taken on a sunny day. Therefore, our model manages to synthesize realistic images with similar looks with the original real images."}, {"heading": "5.3. Reinforcement Training Results", "text": "We first trained the self-driving vehicle with our model and received the reward per iteration curve as shown in Figure 4. We continued to train the self-driving vehicle in the virtual environment using the basic method and received the reward per iteration curve as shown in Figure 4. Both curves show that our model can achieve a similar level of reward through interaction with translated realistic images as the model that is exclusively trained in the virtual environment. Furthermore, we provide the evaluation results of our proposed method (Ours), the basic method (BS) and the supervised learning method (SV). The results include the precision of actions as shown in Table 1. The results show that our proposed method has a better overall performance than the basic method, which trains the amplification training in a virtual environment without seeing real data. The monitored method has the best overall performance, but was trained with supervised, labeled data."}, {"heading": "6. Conclusion", "text": "We have proven that by using synthetic real images as training data in reinforcement learning, the agent generalizes better in a real environment than through pure training with virtual data. The next step would be to develop a better picture-to-picture translation network and a better reinforcement learning framework to exceed the performance of supervised learning. Thanks to the bridge of scene analysis, virtual images can be translated into realistic images that maintain their scene structure.The learned RL model on realistic images can easily be applied to the real environment. We also note that the translation results of a segmentation map are not unique. For example, the segmentation map indicates a car, but it does not indicate what color this car should be. Therefore, one of our future work is to generate various possible manifestations (e.g. color, texture) of the parsing-torealistic network output. In this way, the distortion in RL training would largely reduce the segmentation, so we can provide the first example of self-reinforcement training of a vehicle."}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "Advances in neural information processing systems, 19:1", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D.D. Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang", "X. Zhang", "J. Zhao", "K. Zieba"], "venue": "CoRR, abs/1604.07316", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A.L. Kornhauser", "J. Xiao"], "venue": "CoRR, abs/1505.00256", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "atrous convolution, and fully connected crfs. arXiv:1606.00915", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Transfer from simulation to real world through learning deep inverse dynamics model", "author": ["P. Christiano", "Z. Shah", "I. Mordatch", "J. Schneider", "T. Blackwell", "J. Tobin", "P. Abbeel", "W. Zaremba"], "venue": "arXiv preprint arXiv:1610.03518", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CoRR, abs/1604.01685", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning cpg-based biped locomotion with a policy gradient method: Application to a humanoid robot", "author": ["G. Endo", "J. Morimoto", "T. Matsubara", "J. Nakanishi", "G. Cheng"], "venue": "The International Journal of Robotics Research, 27(2):213\u2013228", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2672\u20132680. Curran Associates, Inc.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["A. Gupta", "C. Devin", "Y. Liu", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1703.02949", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["P. Isola", "J. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CoRR, abs/1611.07004", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Policy gradient reinforcement learning for fast quadrupedal locomotion", "author": ["N. Kohl", "P. Stone"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE International Conference on, volume 3, pages 2619\u20132624. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Evolving large-scale neural networks for vision-based reinforcement learning", "author": ["J. Koutn\u0131\u0301k", "G. Cuccu", "J. Schmidhuber", "F. Gomez"], "venue": "In Proceedings of the 15th annual conference on Genetic and evolutionary computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "CoRR, abs/1512.09300", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "CoRR, abs/1602.01783", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Alvinn", "author": ["D.A. Pomerleau"], "venue": "an autonomous land vehicle in a neural network. Technical report, Carnegie Mellon University, Computer Science Department", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "D. Bagnell"], "venue": "AISTATS, volume 3, pages 3\u20135", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "CoRR, abs/1610.04286", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "cad)$\u02c62$rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "CoRR, abs/1611.04201", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Endto-end deep reinforcement learning for lane keeping assist", "author": ["A.E. Sallab", "M. Abdou", "E. Perot", "S. Yogamani"], "venue": "arXiv preprint arXiv:1612.04340", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML, pages 1889\u20131897", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Safe", "author": ["S. Shalev-Shwartz", "S. Shammah", "A. Shashua"], "venue": "multi-agent, reinforcement learning for autonomous driving. CoRR, abs/1610.03295", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.06907", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "P. Abbeel", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "Workshop on the Algorithmic Foundations of Robotics (WAFR)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["X. Wang", "A. Gupta"], "venue": "ECCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "author": ["J. Wu", "C. Zhang", "T. Xue", "W.T. Freeman", "J.B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, pages 82\u201390", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Torcs", "author": ["B. Wymann", "E. Espi\u00e9", "C. Guionneau", "C. Dimitrakakis", "R. Coulom", "A. Sumner"], "venue": "the open racing car simulator. Software available at http://torcs. sourceforge. net", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Query-efficient imitation learning for end-to-end autonomous driving", "author": ["J. Zhang", "K. Cho"], "venue": "arXiv preprint arXiv:1605.06450", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "The straight-forward idea is end-to-end supervised learning [3, 4], which trains a neural network model mapping visual input directly to action output, and the training data is labeled image-action pairs.", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "The straight-forward idea is end-to-end supervised learning [3, 4], which trains a neural network model mapping visual input directly to action output, and the training data is labeled image-action pairs.", "startOffset": 60, "endOffset": 66}, {"referenceID": 22, "context": "Recently, reinforcement learning has been considered as a promising technique to learn driving policy due to its expertise in action planing [24, 26, 16].", "startOffset": 141, "endOffset": 153}, {"referenceID": 24, "context": "Recently, reinforcement learning has been considered as a promising technique to learn driving policy due to its expertise in action planing [24, 26, 16].", "startOffset": 141, "endOffset": 153}, {"referenceID": 14, "context": "Recently, reinforcement learning has been considered as a promising technique to learn driving policy due to its expertise in action planing [24, 26, 16].", "startOffset": 141, "endOffset": 153}, {"referenceID": 16, "context": "Therefore, most of research is still at the stage of simulations [18, 26, 16], which fails to meet the ultimate expectation of driving in real world, since the appearance of virtual environment is different from real world scene.", "startOffset": 65, "endOffset": 77}, {"referenceID": 24, "context": "Therefore, most of research is still at the stage of simulations [18, 26, 16], which fails to meet the ultimate expectation of driving in real world, since the appearance of virtual environment is different from real world scene.", "startOffset": 65, "endOffset": 77}, {"referenceID": 14, "context": "Therefore, most of research is still at the stage of simulations [18, 26, 16], which fails to meet the ultimate expectation of driving in real world, since the appearance of virtual environment is different from real world scene.", "startOffset": 65, "endOffset": 77}, {"referenceID": 18, "context": "ALVINN [20] provides an early example of using neural network for autonomous driving.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "Powered by deep learning especially convolutional neural network, NVIDIA [3] recently provides an attempt to leverage driving video data for simple lane following task.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "Another work by [4] learns a mapping between input images to a number of key perception indicators, which are closely related to the affordance of a driving state.", "startOffset": 16, "endOffset": 19}, {"referenceID": 31, "context": "On the other hand, imitation learning can also be regarded as supervised learning approach [33], where the agent observes the demonstrations performed by some expert and learns to imitate the action of the expert.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "There is also the covariant shift problem in imitation learning [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 131, "endOffset": 138}, {"referenceID": 7, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 131, "endOffset": 138}, {"referenceID": 0, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 163, "endOffset": 170}, {"referenceID": 24, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 163, "endOffset": 170}, {"referenceID": 12, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 17, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 23, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 14, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 16, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 17, "context": "However, both deep Q-learning method [19] and policy gradient method [16] require the agent to interact with the environment to get reward and feedback.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "However, both deep Q-learning method [19] and policy gradient method [16] require the agent to interact with the environment to get reward and feedback.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Models trained purely on virtual data do not generalize well to real images [7, 28].", "startOffset": 76, "endOffset": 83}, {"referenceID": 26, "context": "Models trained purely on virtual data do not generalize well to real images [7, 28].", "startOffset": 76, "endOffset": 83}, {"referenceID": 20, "context": "Recent progress of transfer and domain adaptation learning in robotics has provide examples of simulation-to-real reinforcement training [22, 11, 27].", "startOffset": 137, "endOffset": 149}, {"referenceID": 9, "context": "Recent progress of transfer and domain adaptation learning in robotics has provide examples of simulation-to-real reinforcement training [22, 11, 27].", "startOffset": 137, "endOffset": 149}, {"referenceID": 25, "context": "Recent progress of transfer and domain adaptation learning in robotics has provide examples of simulation-to-real reinforcement training [22, 11, 27].", "startOffset": 137, "endOffset": 149}, {"referenceID": 20, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 238, "endOffset": 242}, {"referenceID": 21, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 335, "endOffset": 343}, {"referenceID": 25, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 335, "endOffset": 343}, {"referenceID": 20, "context": "The work of [22] proposes to use progressive network to transfer network weights from model trained on virtual data to the real environment and then fine-tune the model in a real setting.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As a more recent work, [23] proposed a new framework for training a reinforcement learning agent with only virtual environment.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "However, as mentioned in the conclusion of their paper [23], the manual engineering work to design suitable training environments is nontrivial, and it is more reasonable to attain better results by combining simulated training with some real data, though it is unclear from their paper how to combine real data with simulated training.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "Recently, the community has made significant progress in generative approaches, mostly based on generative adversarial networks [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "To name a few, the work of [30] explored the use of VAE-GAN [15] in generating 3D voxel models, and the work of [29] proposed a cascade GAN to generate natural image by structure and style.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "To name a few, the work of [30] explored the use of VAE-GAN [15] in generating 3D voxel models, and the work of [29] proposed a cascade GAN to generate natural image by structure and style.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "To name a few, the work of [30] explored the use of VAE-GAN [15] in generating 3D voxel models, and the work of [29] proposed a cascade GAN to generate natural image by structure and style.", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "More recently, the work of [12] developed a general and simple framework for image-to-image translation which can handle various pixel level generative tasks like semantic segmentation, colorization, rendering edge maps, etc.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Many of them are based on deep convolutional neural network or fully convolutional neural network [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "In order to get better quality segmented images, the work of [2] cut down the downsampling layers to avoid resolution reduction, and the work of [32, 5] uses dilated convolution to improve performance.", "startOffset": 61, "endOffset": 64}, {"referenceID": 30, "context": "In order to get better quality segmented images, the work of [2] cut down the downsampling layers to avoid resolution reduction, and the work of [32, 5] uses dilated convolution to improve performance.", "startOffset": 145, "endOffset": 152}, {"referenceID": 4, "context": "In order to get better quality segmented images, the work of [2] cut down the downsampling layers to avoid resolution reduction, and the work of [32, 5] uses dilated convolution to improve performance.", "startOffset": 145, "endOffset": 152}, {"referenceID": 1, "context": "Bi-linear interpolation and deconvolutional methods are also very popular these days such as [2, 17, 5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 15, "context": "Bi-linear interpolation and deconvolutional methods are also very popular these days such as [2, 17, 5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 4, "context": "Bi-linear interpolation and deconvolutional methods are also very popular these days such as [2, 17, 5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 1, "context": "In this paper, we use the SegNet for image segmentation, the structure of the network is reveal in [2], which is composed of two main parts.", "startOffset": 99, "endOffset": 102}, {"referenceID": 10, "context": "Inspired by the work of pix2pix network [12], our network includes two modules, namely virtual-to-parsing and parsing-to-realistic network.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "The approach we adopted is developed by [18], where they use the asynchronous actor-critic reinforcement learning algorithm to train a self-driving vehicle in the car racing simulator TORCS [31].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "The approach we adopted is developed by [18], where they use the asynchronous actor-critic reinforcement learning algorithm to train a self-driving vehicle in the car racing simulator TORCS [31].", "startOffset": 190, "endOffset": 194}, {"referenceID": 2, "context": "As there is no direct connection between virtual world images such as the images we see in the TORCS environment, and the real world images such as the data in [3], a", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "We use the network of [2] to parse the image into different semantic regions.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "To use the SegNet to segment images in virtual world, the network will be trained on CityScape driving scene segmentation dataset [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 29, "context": "Then the trained model is used to parse virtual scenes in TORCS [31].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "To this end, architecture of [12] is adopted.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "The scene segmentations are also obtained using [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "We use a conventional RL solver Asynchronous Advantage Actor-Critic (A3C)[18] to train the self driving vehicle, which has performed well on various machine learning tasks.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "When the algorithm is applied to our experiment, we define a reward function proportional to the agent\u2019s velocity along the center of the track at the agent\u2019s current position [18].", "startOffset": 176, "endOffset": 180}, {"referenceID": 29, "context": "The virtual image data are collected in the aalborg environment in TORCS [31], and a total 1673 images are collected which covers the entire driving cycle of aalborg environment.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "We adopt the image semantic segmentation network design of [2] and their trained segmentation network on the CityScape image segmentation dataset [8] to segment both virtual images rendered by TORCS and real images from [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "We adopt the image semantic segmentation network design of [2] and their trained segmentation network on the CityScape image segmentation dataset [8] to segment both virtual images rendered by TORCS and real images from [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 29, "context": "We used the Aalborg environment in TORCS [31], and collected 1673 images from this environment as well as their segmentations.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "The network structure used in our training is similar to that of [18] where the actor network is a 4-layer convolutional network with ReLU activation functions in-between.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "To make a fair comparison between our method and reinforcement learning with only virtual input, we trained the vehicle in the virtual car racing simulator TORCS [31] with virtual image as input using the same reinforcement learning framework as in our method.", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "We used image segmentation model trained on the cityscape [8] dataset to segment both virtual and real images.", "startOffset": 58, "endOffset": 61}], "year": 2017, "abstractText": "Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.", "creator": "LaTeX with hyperref package"}}}