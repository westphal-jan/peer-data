{"id": "1704.03987", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Mobile Keyboard Input Decoding with Finite-State Transducers", "abstract": "We propose a finite-state transducer (FST) representation for the models used to decode keyboard inputs on mobile devices. Drawing from learnings from the field of speech recognition, we describe a decoding framework that can satisfy the strict memory and latency constraints of keyboard input. We extend this framework to support functionalities typically not present in speech recognition, such as literal decoding, autocorrections, word completions, and next word predictions.", "histories": [["v1", "Thu, 13 Apr 2017 04:00:07 GMT  (268kb,D)", "http://arxiv.org/abs/1704.03987v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tom ouyang", "david rybach", "fran\\c{c}oise beaufays", "michael riley"], "accepted": false, "id": "1704.03987"}, "pdf": {"name": "1704.03987.pdf", "metadata": {"source": "CRF", "title": "MOBILE KEYBOARD INPUT DECODING WITH FINITE-STATE TRANSDUCERS", "authors": ["Tom Ouyang", "David Rybach", "Fran\u00e7oise Beaufays", "Michael Riley"], "emails": ["ouyang@google.com", "rybach@google.com", "fsb@google.com", "riley@google.com"], "sections": [{"heading": null, "text": "We describe the general framework of what we briefly call the \"FST decoder\" and the implementation details, which are new compared to an FST speech decoder. We show that the FST decoder enables new UX functions such as post-corrections. Finally, we outline how this decoder can support advanced functions such as personalization and contextualization. Index terms - keyboard, decoder, FST."}, {"heading": "1. INTRODUCTION", "text": "With the rapidly growing penetration of mobile devices into every aspect of modern life, providing an efficient and enjoyable mobile input experience, has recently become a topic of interest to researchers and technology providers. Voice recognition, for example, has flourished in recent years, mostly driven by the need for convenient input methods [1]. Handwriting recognition has also gained traction, especially in languages with complex scripts such as Chinese and Indian languages [2]. Keyboard input has received relatively little attention from the research community, although it remains a primary input method, as it is often considered the most convenient way to compose text on a mobile device."}, {"heading": "2. KEYBOARD FEATURES AND TERMINOLOGY", "text": "Above all, a mobile keyboard needs to be reliable and fast, which is why all runtime processing happens on the device. Latency limitations are tight: a keystroke is expected to generate visible feedback within about 20 msec. RAM and CPU usage generally need to be kept under tight control to prevent the keyboard from being displaced by simultaneous processes. Memory is also limited: as in embedded speech recognition, the keyboard's speech models should not be higher than 5 to 10 Mb, which usually allows them to model a few hundred thousand words in the keyboard. A soft keyboard's main function is to decode input into words and phrases, as well as a speech recognition system that decodes waveforms."}, {"heading": "3. KEYBOARD TRANSDUCERS", "text": "A type input consists of a time series of touch points, x, which encode the coordinates of the user's keystrokes. Gesture input scans the input path, e.g. every 100 milliseconds, to provide a similar time series. The task of the decoder is to find the word sequence w that best matches the input sequence x."}, {"heading": "3.1. Key Context Dependency and Spatial Model", "text": "The equivalent of speech phonemes in the keyboard world is the set of keys offered in the layout. Accordingly, a spatial model is used to provide a probability distribution across these units. Note that the spatial model does not fully resolve the written language: the letters \"e\" and \"e\" in a French keyboard are typically obtained by long pressing \"e\" and selecting from a small pop-up menu. All three letters have the same spatial score. This confusion is mitigated by context limitations and the assertion of a strong language model. Similarly, just as acoustic context dependence in the language is encoded with a C converter, we implemented spatial context dependence in the keyboard and opted for a bi-key model. Accordingly, the slurs of the converter represent a b: b transitional model, with a b the key being placed the key with a left-hand key between a keyboard (this one)."}, {"heading": "3.2. Lexicon", "text": "The L lexicon converter for the keyboard decoder is a simple key to word mapping, similar to a speech graphics lexicon. Some keys, such as the apostrophe, can be made optional, allowing the user to type \"Ive\" for \"I've.\" Repeated keys can be defined optionally, such as the second \"o\" in \"Google.\" The closure that allows word string conversion is implemented with a space to raise your fingers or tap the space bar between words. If this space is optional, the decoder can correct missing spaces between words or decode multi-word gestures. An example of an FST lexicon is shown in Figure 1."}, {"heading": "3.3. Language Model", "text": "Similar to the language models in embedded speech recognition systems, the keyboard's language models are typically low-order n-grams over a limited vocabulary, such as 64K words [8]. Due to features such as suggestions, completions, and predictions where the language model is more prominent than the spatial model, the language model should be carefully designed. For example, the user gesticulating \"Google\" can expect the keyboard to suggest \"Goggle\" as an alternative, but not \"Gogle\" or \"Gooogle,\" which would be the training corpus's chances, are often loud. Therefore, the keyboard's language models are typically trained on a fixed vocabulary curated by hand to eliminate spelling mistakes, erroneous capital letters, and other unwanted artifacts."}, {"heading": "4. KEYBOARD DECODING WITH FSTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Suggestions and Autocorrections", "text": "In fact, it is not that we notice these errors in our everyday keyboard system (e.g., that they are pushed into the background), it is not that we trade one letter for another (e.g., that it is pushed into the background), it is not that we are able to reactivate (or reactivate) it, and it is not that we are able to reactivate (or reactivate) it."}, {"heading": "4.2. Gesture Typing", "text": "In gesture writing, the interaction model typically assumes that each stroke corresponds to a single word, and spaces between the words are automatically inserted when omitted. This type of input can be particularly challenging, as the finger often slides past many keys while on its way to the actual letter. For example, on a typical QWERTY keyboard layout, the words \"pit,\" \"pot,\" \"\" input \"all work with the same canonical gesture pattern (a straight line from P to T). The same FST decoding frame used for typing, of course, also works for gesture writing, where each input frame is interpreted either as input on the way to a key (e.g., the\" a b \"label in Figure 2A) or aligned to a key (e.g., the\" b \"label)."}, {"heading": "4.3. Literals Decoding", "text": "In order to include the literal decoding of typing in the hypotheses returned by the keyboard decoder, the decoder graph must contain paths for arbitrary keystrokes. The lexicon contains one entry for each key with a corresponding \"literal word\" on the output page. In contrast to conventional words, the lexicon does not allow spaces between these literal word symbols. The input symbols for the literal words in the lexicon are different from conventional key symbols, so that the spatial model can assign an unequal probability only to this \"literal key\" when it is actually clicked. The LM FST contains a subgraph for the literal grammar that assigns weight to the word sequences. It is linked to the unigram state of the model. The grammar also inserts a marker symbol at the end of the literal sequence. This marker forces a blank tip to the next word."}, {"heading": "4.4. Word Completions and Predictions", "text": "To predict word completion, the decoder calculates the most likely extensions for each of the currently hypothesized word prefixes in their respective sentence context. As a hypothesis, the decoder holds a series of states in the decoder graph together with their score (and a traceback pointer).The states in the decoder graph correspond to a tuple of states in the lexicon, the LM-FST, and the composition filters.From a lexicon state, we can calculate the amount of available word (output) labels. In fact, the predictive composition filter also requires this information, and is therefore available as a pre-calculated interval set. Due to the minimization of the lexicon FST and the label sliding, the output labels can already appear on an arc before the last key of a word (for example, see states 2 and 3 in Figure 1)."}, {"heading": "4.5. Post-Corrections", "text": "It can be difficult for the language model to determine whether a word makes sense without seeing what comes after it. For example, there is nothing wrong with the word \"food\" at the beginning of a sentence, but if the user types \"food happiness,\" there is a good chance that it actually means \"happiness\" (especially since on the QWERTY layout the F key is next to the G key). However, traditional smartphone keyboards treat the space bar as a hard commit without the ability to change their mind about words that have already been typed. This is analogous to early speech recognition, when most systems operated with isolated words. By introducing a modern, continuous recognition architecture, the FST decoder enables a streaming interaction model where the keyboard can adjust its interpretation of earlier words based on new insights. There are also new challenges in user interaction by modifying the keyboard's previously typed words."}, {"heading": "5. FST DECODER AND COMPLEX FEATURE NEEDS", "text": "Perhaps one of the most powerful aspects of the FST keyboard input framework is the ease with which dynamic models can be used for personalization and contextualization, which is especially important when decoding on the device, where personalization can compensate for the limited size of the models: you rarely need more than 100,000 words to express yourself in a particular language, but they may need the right 100,000 words."}, {"heading": "5.1. Dynamic Models", "text": "Dynamic models can be used to accumulate N-grams that the user has previously entered, or information such as his contact list or other contextual information. These models are constantly updated, so it would be difficult and costly to perform such updates directly on the decoder graph. Instead, they are incorporated into the decoder graph using an on-the-fly grid rescoring technique, similar to the n-gram biasing approach described in [13]. The vocabulary of the dynamic LM may contain words that are not covered by the main LM and therefore are not part of the lexicon FST. We integrate these OOV words into the decoder graph by transferring a character to a text converter on the LM-FST that is connected to the unigram state. This FST has \"character words\" on the input page and regular words on the output page. The lexicon FST has key sequences to match these sequences (the sequences of characters)."}, {"heading": "6. EVALUATION", "text": "In the data collection study, participants typed and gesticulated English phrases on a QWERTY keyboard. The instructions were sentences from transcribed language interactions. As we wanted to collect natural errors to test the corrective capability of the decoder, participants were asked to type quickly and not worry about correcting errors. The results below show the performance of three decoders on the keyboard. A baseline system without label prediction or post-correction (p.c.), the proposed FST decoder without p.c. and the same FST decoder with p.c. allows. Table 1 shows that the proposed FST decoder is capable of exceeding the baseline in both typing and gesture input, and the relative decrease in water becomes even greater when postcorrection is activated. The example from Table 2 shows this in action."}, {"heading": "7. CONCLUSION", "text": "We showed how the FST speech recognition framework developed over the last decade can be applied to the world of mobile keystrokes, from the basic decoding of tapped key sequences to more advanced features such as the next word prediction. Our experiments to date show that an FST decoder offers strong precision advantages over a more conventional decoder, that it can elegantly scale to the challenges of internationalization, and that it can easily use data on the device to personalize the user's typing experience."}, {"heading": "8. REFERENCES", "text": "[1] Johan Schalkwyk, Doug Beeferman, Franc oise Beaufays, Bill Byrne, Ciprian Chelba, Mike Cohen, Maryam Kamaufvar, and Brian Strope, \"Your word is my command: A case study,\" in Advances in Speech Recognition, pp. 61-90. [2] Daniel Keysers, Thomas Deselaers, Henry A Rowley, Li-Lun Wang, and Victor Carbune, \"Multi-language online handwriting recognition,\" 2016. [3] Nils Nils and Michael Riley, \"Word n-grams for cluster keyboards,\" in Proceedings of the EACL Workshop on Language Modeling for Text Entry Methods. Association for Computational Linguistics, 2003, pp. 51-58. [4] Ahmed Hassan, Sara Noeman, and Hany Hassan, \"Language text correction using ite state finata.\""}], "references": [{"title": "Your word is my command: Google search by voice: A case study", "author": ["Johan Schalkwyk", "Doug Beeferman", "Fran\u00e7oise Beaufays", "Bill Byrne", "Ciprian Chelba", "Mike Cohen", "Maryam Kamvar", "Brian Strope"], "venue": "Advances in Speech Recognition, pp. 61\u201390. Springer, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-language online handwriting recognition", "author": ["Daniel Keysers", "Thomas Deselaers", "Henry A Rowley", "Li-Lun Wang", "Victor Carbune"], "venue": "2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Word n-grams for cluster keyboards", "author": ["Nils Klarlund", "Michael Riley"], "venue": "Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods. Association for Computational Linguistics, 2003, pp. 51\u201358.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Language independent text correction using finite state automata", "author": ["Ahmed Hassan", "Sara Noeman", "Hany Hassan"], "venue": "IJC- NLP. Hyderabad, India, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Shapewriter on the iphone: from the laboratory to the real world", "author": ["Shumin Zhai", "Per Ola Kristensson", "Pengjun Gong", "Michael Greiner", "Shilei Allen Peng", "Liang Mico Liu", "Anthony Dunnigan"], "venue": "CHI\u201909 Extended Abstracts on Human Factors in Computing Systems. ACM, 2009, pp. 2667\u20132670.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech recognition with weighted finite-state transducers", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley"], "venue": "Handbook of Speech Processing, Jacob Benesty, M. Sondhi, and Yiteng Huang, Eds., chapter 28, pp. 559\u2013582. Springer, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Performance and user experience of touchscreen and gesture keyboards ina lab setting and in the wild", "author": ["Shyan Reyal", "Shumin Zhai", "Per Ola Kristensson"], "venue": "CHI\u201915 Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 0}, {"title": "Personalized speech recognition on mobile devices", "author": ["Ian McGraw", "Rohit Prabhavalkar", "Raziel Alvarez", "Montse Gonzalez Arenas", "Kanishka Rao", "David Rybach", "Ouais Alsharif", "Alexander Gruenstein", "Fran\u00e7oise Beaufays", "Carolina Parada"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 5955\u20135959.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Modelling gesture typing movements", "author": ["Philip Quinn", "Zhai Shumin"], "venue": "Human-Computer Interaction. Taylor & Francis Online, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short term memory neural network for keyboard gesture decoding", "author": ["Ouais Alsharif", "Tom Ouyang", "Fran\u00e7oise Beaufays", "Shumin Zhai", "Thomas Breuel", "Johan Schalkwyk"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 2076\u20132080.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A generalized composition algorithm for weighted finite-state transducers", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk"], "venue": "Interspeech, 2009, pp. 1203\u20131206.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Touch behavior with different postures on soft smartphone keyboards", "author": ["Shiri Azenkot", "Shumin Zhai"], "venue": "Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services, New York, NY, USA, 2012, MobileHCI \u201912, pp. 251\u2013260, ACM.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Composition-based on-the-fly rescoring for salient n-gram biasing", "author": ["Keith Hall", "Eunjoon Cho", "Cyril Allauzen", "Fran\u00e7oise Beaufays", "Noah Coccaro", "Kaisuke Nakajima", "Michael Riley", "Brian Roark", "David Rybach", "Linda Zhang"], "venue": "Interspeech, 2015, pp. 1418\u20131422.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Speech recognition for example has flourished in the last few years, mostly fueled by the need for convenient mobile input methods [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "Likewise, handwriting recognition has gained more traction, especially in languages with complex scripts such as Chinese and Indic languages [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "The use of FSTs in the context of keyboard input is not totally new: a report by Klarlund and Riley suggested using FSTs to disambiguate entries on a hardware cluster keyboard, but this was before the invention of smart phones, so the issue of decoding (soft) noisy input sequences was not addressed [3].", "startOffset": 300, "endOffset": 303}, {"referenceID": 3, "context": "FSTs have also been used for similar tasks like spelling correction [4], though not in the context of mobile input.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": ", Shapewriter [5], Swype, Swiftkey, etc.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "General background on FST decoding for speech can be found in [6].", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "While gesture input is easily 10 to 20% faster than tap input [7], relatively few users rely primarily on gesture input.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "This places keyboard somewhere in between our server-based speech recognizer which relies on triphone models and our embedded recognizer which uses monophones [8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "Gesture inputs instead are often modeled with the so-called \u201cminimum-jerk model\u201d that imposes smoothness maximization constraints on the input trajectory [9].", "startOffset": 154, "endOffset": 157}, {"referenceID": 9, "context": "Alternatively, a recurrent neural network model can be used [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "64K words [8].", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "For memory efficiency, we use the on-the-fly composition of (C \u25e6L)\u25e6G using look-ahead composition filters [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "Text entry on a mobile touch-screen device is a very error-prone process, with per-letter error rates around 8-9% [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "In fact, the label look-ahead composition filter requires that information as well and it is therefore available as precomputed interval set [11].", "startOffset": 141, "endOffset": 145}, {"referenceID": 12, "context": "Instead, they are incorporated in the decoding process using an on-the-fly lattice rescoring technique, similar to the n-gram biasing approach described in [13].", "startOffset": 156, "endOffset": 160}], "year": 2017, "abstractText": "We propose a finite-state transducer (FST) representation for the models used to decode keyboard inputs on mobile devices. Drawing from learnings from the field of speech recognition, we describe a decoding framework that can satisfy the strict memory and latency constraints of keyboard input. We extend this framework to support functionalities typically not present in speech recognition, such as literal decoding, autocorrections, word completions, and next word predictions. We describe the general framework of what we call for short the keyboard \u201cFST decoder\u201d as well as the implementation details that are new compared to a speech FST decoder. We demonstrate that the FST decoder enables new UX features such as post-corrections. Finally, we sketch how this decoder can support advanced features such as personalization and contextualization.", "creator": "LaTeX with hyperref package"}}}