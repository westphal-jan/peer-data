{"id": "1704.04133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks", "abstract": "In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.", "histories": [["v1", "Thu, 13 Apr 2017 13:44:33 GMT  (3484kb,D)", "http://arxiv.org/abs/1704.04133v1", null], ["v2", "Thu, 18 May 2017 18:38:06 GMT  (3486kb,D)", "http://arxiv.org/abs/1704.04133v2", "Accepted at Computer Vision and Patter Recognition Workshop (CVPR-W) on Explainable Computer Vision, 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.MM", "authors": ["devinder kumar", "alexander wong", "graham w taylor"], "accepted": false, "id": "1704.04133"}, "pdf": {"name": "1704.04133.pdf", "metadata": {"source": "CRF", "title": "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks", "authors": ["Devinder Kumar", "Alexander Wong", "Graham W. Taylor"], "emails": ["devinder.kumar@uwaterloo.ca", "a28wong@uwaterloo.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1. Introduction", "text": "In recent years, we have seen tremendous successes in the field of artificial intelligence (AI). In particular, many of the recent advances have related to a particular area of machine learning: deep neural networks (DNNs). DNNs have shown that they outperform earlier machine-learning techniques for a variety of tasks, such as fine-grained classification [18, 5], self-driving cars [4], capturing and answering questions via images [9, 1], and even defeating human champions in Go [11]. Although DNNs have shown tremendous effectiveness across a wide range of tasks, they often fail spectacularly, producing inexplicable and incoherent results that make one wonder what caused the DNN to make such decisions. This lack of transparency and interpretative ability of DNNs during the decision-making process is largely due to the complex nature of DNNs, in which individual neural responses, unlike other interpretable decision-making structures, such as decision-making trees, can be traced."}, {"heading": "2. Related Work", "text": "This literature can generally be divided into two groups: firstly, those that focus on understanding the global structure of a trained network, and secondly, those that focus on the process of decision-making, which is explained below: \"Global Understanding-Based Methods.\" Many methods in this field attempt to understand the process of decision-making by measuring its operational characteristics."}, {"heading": "3. Class Enhanced Attentive Response", "text": "(CLEAR) This section explains the procedure for generating the proposed CLass-Enhanced Attentive Response (CLEAR) class, which represents the dominant Attentive Response (CLEAR) class. CLEAR maps \"main objective is to convey the following information: 1) the attentive regions of interest in the image that are responsible for the decision made by DCNN; 2) the attentive levels in these regions of interest, so that we understand their level of influence over the decision made by DCNN; and 3) the dominant class associated with these attentive regions of interest, so that we can better understand why a decision was made. The process for generating CLEAR maps can be summarized as follows (see Figure 2)."}, {"heading": "4. Experiments", "text": "In this section, we demonstrate the effectiveness of CLEAR maps for understanding and interpreting DCNN decision-making. We conducted qualitative and quantitative experiments on three different datasets: the commonly used MNIST and Street View House Numbers (SVHN) benchmarks, and the Stanford Dog dataset [8]. In the following section, we explain the experimental setup."}, {"heading": "4.1. Setup", "text": "To conduct experiments with three different datasets, we trained three different DCNN architectures with all the revolutionary layers. To train these networks, we used the standard traction and test split. We achieved an accuracy of 99.26% and 92.6% for the MNIST and SVHN datasets, respectively. For training on the Stanford Dog dataset, we used a 16-layer VGG network [13], pre-trained on ImageNet. We slightly modified the VGG network by removing the last two fully connected layers and augmenting it at the end with two revolutionary layers. We refined the last two layers using the Stanford Dog dataset [13], pre-trained on ImageNet. As with MNIST and SVHN, the standard traction and test split was applied to this dataset."}, {"heading": "4.2. Qualitative Experiments: Understanding The", "text": "However, the results of the two binary heatmaps represent what information was used in the image for or against the true class in the last layer; the response for the rest of the kernel is formed by overlaying the output response from the kernel, which represents the true class as \"hot\" regions and reaction from the rest of the kernel in the last layer; the response for the rest of the kernel is formed by overlaying the individual output responses to the individual output responses; thus, in the binary heatmaps, the hot regions and green regions are represented for and against the actual class used for decision-making by the network; the binary heatmaps are constructed in a similar way to those [19] and [10]; CLEAR mapping is explained in sections 3 and Fig."}, {"heading": "4.3. Quantitative Experiments", "text": "In order to confirm our observations for the MNIST and SVHN datasets, we performed two different quantitative experiments. In the first experiment, we removed all parts of the image except the regions responsible for activating the core associated with the class of the image (positive nucleus). We designate these regions as strong features associated with the class. In the MNIST dataset, we replace the digit with the background, and in the SVHN dataset, we replace the region with a gray spot. In the second experiment, we do the opposite: we remove the regions responsible for the core associated with the true class of the inputChihuahua Jap. Spaniel Maltese PekineseShih-Tzu B. Spaniel Papillon Toy TerrierR."}, {"heading": "5. Discussion", "text": "In this section, some general points related to the CLEAR maps approach are discussed: 1) It is interesting to note that in Figure 2, the individual response maps from the cores of the last layer are sparse. We observed the same pattern for all the data sets considered. Evidence for classes tends to come from very specific regions. 2) For data sets with a large number of classes, such as the Stanford Dog dataset, we chose and created CLEAR maps with only 10 classes. We did not make an effort to represent the CLEAR maps for all 120 classes, as this would make the interpretation of the decision results extremely difficult. In such cases, it would perhaps be a better approach to represent the ten most activated classes or several different maps with N classes. 3) In the current realization of our approach, we are using devolution reactions with only fully evolutionary networks. We would like to point out that, although in this case, end-to-learning may only be possible with different networks [19], devolution-to-learning is completely possible with different networks."}, {"heading": "6. Conclusion", "text": "This paper introduces a novel approach to better understand and visualize the decision-making process of DNNs in the form of CLass-Enhanced Attentive Response (CLEAR) maps. CLEAR maps allow visualization not only of the areas of interest that predominantly influence the decision-making process, but also of the degree of influence and the dominant influence class in these areas. This multi-faceted view of the decision-making process allows a better understanding of not only where but why certain decisions are made by DCNNs compared to existing heatmap-based approaches. Experiments have been conducted with three different publicly available data sets and demonstrate the effectiveness of CLEAR maps both quantitatively and qualitatively. Furthermore, we have shown that strong areas of interest identified with CLEAR maps play a central role in the correct classification of the class."}, {"heading": "6.3. Stanford Dog", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2. SVHN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. MNIST", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.", "creator": "LaTeX with hyperref package"}}}