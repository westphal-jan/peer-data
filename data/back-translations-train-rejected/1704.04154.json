{"id": "1704.04154", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Learning Joint Multilingual Sentence Representations with Neural Machine Translation", "abstract": "In this paper, we use the framework of neural machine translation to learn joint sentence representations across different languages. Our hope is that a representation which is independent of the language a sentence is written in, is likely to capture the underlying semantics. We search and compare more than 1.4M sentence representations in three different languages and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.", "histories": [["v1", "Thu, 13 Apr 2017 14:40:40 GMT  (130kb,D)", "http://arxiv.org/abs/1704.04154v1", "8 pages, 3 figures"], ["v2", "Tue, 8 Aug 2017 15:09:44 GMT  (134kb,D)", "http://arxiv.org/abs/1704.04154v2", "11 pages, 2 figures, published at ACL workshop RepL4NLP"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["holger schwenk", "matthijs douze"], "accepted": false, "id": "1704.04154"}, "pdf": {"name": "1704.04154.pdf", "metadata": {"source": "CRF", "title": "Learning Joint Multilingual Sentence Representations with Neural Machine Translation", "authors": ["Holger Schwenk", "Ke Tran", "Orhan Firat", "Matthijs Douze"], "emails": ["schwenk@fb.com", "m.k.tran@uva.nl", "matthijs@fb.com", "orhanf@google.com"], "sections": [{"heading": "1 Introduction", "text": "It has been shown that syntactic and semantic relationships can be captured in this (high-dimensional) embedding space, see for example (Mikolov et al., 2013). To process word sequences, these word embedding must be \"combined\" into a representation of the entire sequence. Common approaches include: simple techniques such as word pooling or some kind of pooling, e.g. (Arora et al., 2017), recursive neural networks using recursive languages, e.g. (Socher et al., 2011), recurring neural networks, e.g. Cho et al., 2014), evolutionary neural networks, e.g. we. \"(Collobert and Weston, 2008; Zhang et al.) or hierarchical approaches, e.g."}, {"heading": "2 Architecture", "text": "We propose to use multiple encoders and decoders, one for each source and target language. This means that the notion of multiple input languages can be extended in different modalities (e.g. language and images). One can also imagine adding classification tasks in addition to sequence generation. However, our ultimate goal is to jointly train this generic architecture to perform many tasks simultaneously to obtain a universal multilingual and multimodal representation, ie. This ultimate goal is illustrated in the order of more than fifty words, we focus on fixed-size representations, regardless of the length of the input (and output) sequence. This choice certainly has an impact on the performance of very long sequences, ie. In the order of more than fifty words, we argue that such long sentences are probably not very common in every day-to-day communication. We would also like to stress that the goal of our work is not to improve NMT (for multiple languages), but to learn the NT framework to multilingual sentences."}, {"heading": "2.1 Related work", "text": "The use of multiple encoders and decoders was first investigated in the context of neural MT networks. Dong et al. (2015) used multiple decoder systems, i.e. 1: N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al. (2016b), on the other hand, used multiple encoder models, i.e. M: 1 training. Not surprisingly, this complementarity improves the quality of MT compared to just one input language. Many different configurations were used by (Luong et al., 2015) for sequence-to-sequence models."}, {"heading": "3 Experimental evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data", "text": "We carried out all our experiments with the freely available UN corpus (Ziemski et al., 2016), which contains about 12M sentences in six languages (En, Fr, Eb, Ru, Ar and Zh). We used the version that is 6x parallel, which corresponds to about 8.3M sentences (173M English words) 2. This corpus comes with a predefined development and test set (4000 words each). We used byte pair encoding (BPE), as suggested by Sennrich et al. (2016). The codebook is 20k in size. Unlike other works, we do not lowercase the texts.3"}, {"heading": "3.2 Evaluation", "text": "In fact, it is the case that most people are able to determine for themselves what they want and what they do not want. (...) It is not that people are able to identify themselves. (...) It is not that they can do it. (...) It is as if they would do it. (...) It is as if they would do it. (...) It is as if they would do it. (...) It is as if they would not do it. (...) It is as if they would do it. (...) It is as if they would do it. (...) It is as if they would do it. (...) It is as if they would do it. \"(...) It is as if they would do it.\""}, {"heading": "3.3 Results", "text": "rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "3.4 Large scale out-of domain similarity search", "text": "In this section, we evaluate our sentence representation on data outside the domain and examine the effects of corpora size on the similarity error rate. We do not know of another huge corpus running 6 times in parallel for the same languages as the UN corpus. Therefore, we have chosen the Europarl corpus and limited our study to three languages, namely English, French and Spanish. After excluding duplicates and limiting the sentence length to fifty characters, we have nearly 1.5 million 3 times parallel sentences. Figure 3 shows the similarity error rate depending on the corpus size using the system \"efsra-z\" from Table1. Subsets of the different sizes were randomly evaluated. The average error rate is 3.1% for a subset of 4000 words, i.e. of the same size as the UN Dev set. We then observe a clear logarithmic increase in the similarity error rate in the function of the corpus."}, {"heading": "4 Conclusion", "text": "We have shown that the framework of neural MT with multiple encoders / decoders can be used to learn common fixed sentence representations that have interesting linguistic characteristics. We have examined several training paradigms that partially correspond to paths across the architecture. We have evaluated common sentence embeddings by linguistic similarity search7, which differ significantly in terms of morphology, inflection, word order, etc. We have achieved an average similarity error rate of 2.7% for all 21 language pairs. We have also investigated the evolution of similarity error rate when scaling up to 1.5 million sentences coming from an extra-domain-like corpus. There are many directions of future research into this work. We have used stacked LSTMs for the encoders and decoders. There is a large amount of research on fixed sentence embeddings, and we will investigate which approaches are best suited to learning common multilingual sentence embeddings."}], "references": [{"title": "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "SemEval workshop.", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."], "venue": "ICLR.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "EAMT . pages 261\u2013268.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Multilingual deep learning", "author": ["Sarath Chandar", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha."], "venue": "NIPS DL wshop.", "citeRegEx": "Chandar et al\\.,? 2013", "shortCiteRegEx": "Chandar et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "ICML. pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Huan Wu", "Wei He", "Dianhai Yu", "Haifeng wang."], "venue": "ACL. pages 1723\u20131732.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-way, multilingual neural machine translation with shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Choa", "Yoshua Bengio."], "venue": "NAACL.", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman Vural", "Kyunghyun Cho."], "venue": "EMNLP.", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "DeViSa:E a deep visual-semantic embedding model", "author": ["Andrea Frome", "Grep S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "marc\u2019Aurelio Ranzato", "Thomas Mikolov"], "venue": null, "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "ACL. pages 58\u201368.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Billion-scale similarity search with gpus", "author": ["Jeff Johnson", "Matthijs Douze", "Herv\u00e9 J\u00e9gou."], "venue": "arXiv preprint arXiv:1702.08734 .", "citeRegEx": "Johnson et al\\.,? 2017", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson"], "venue": "https://arxiv.org/ abs/1611.04558.", "citeRegEx": "Johnson,? 2016", "shortCiteRegEx": "Johnson", "year": 2016}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Marcin Juncys-Dowmunt", "Timasz Dwojak", "Hieu Hoang."], "venue": "https://arxiv.org/abs/ 1610.011108.", "citeRegEx": "Juncys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Juncys.Dowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP. pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai."], "venue": "Coling.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "A structured self-attentive sentence embedding", "author": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "venue": "In ICLR", "citeRegEx": "Lin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2017}, {"title": "Learning natural language inference using bidirectional lstm model and inner-attention", "author": ["Yang Liu", "Chenjie Sun", "Lei Lin", "Xiaolong Wang."], "venue": "https://arxiv.org/abs/1605.09090.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous word space representations", "author": ["Tomas Mikolov", "Wen tau Yih", "Geoffrey Zweig."], "venue": "NAACL. pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Zeroresource machine translation by multimodal encoder-decoder network with multimedia pivot", "author": ["Hideki Nakayama", "Noriki Nishida."], "venue": "https://arxiv.org/abs/1611.04503.", "citeRegEx": "Nakayama and Nishida.,? 2016", "shortCiteRegEx": "Nakayama and Nishida.", "year": 2016}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng."], "venue": "ICML.", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Learning distributed representations for multilingual text sequences", "author": ["Hieu Pham", "Minh-Thang Luong", "Christopher D. Manning."], "venue": "1st Workshop on Vector Space Modeling for NLP.", "citeRegEx": "Pham et al\\.,? 2015", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "A correlational encoder decoder architecture for pivot based sequence generation", "author": ["Amrita Saha", "Mitesh M. Kharpa", "Sarath Chandar", "Janarthanan Rajendran", "Kyunghyun Cho."], "venue": "https://arxiv. org/abs/1606.04754.", "citeRegEx": "Saha et al\\.,? 2016", "shortCiteRegEx": "Saha et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "NIPS. pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart."], "venue": "https://arxiv.org/abs/1504.05070.", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Cross-lingual sentiment classification with bilingual document representation learning", "author": ["Xinjie Zhou", "Xiaojun Wan", "Jianguo Xiao."], "venue": "ACL.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "The united nations parallel corpus v1.0", "author": ["M Ziemski", "Marcin Juncys-Dowmunt", "B. Pouliquen"], "venue": null, "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "NAACL. pages 30\u201334.", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "It has been shown that syntactic and semantic relations can be captured in this (high-dimensional) embedding space, see for instance (Mikolov et al., 2013).", "startOffset": 133, "endOffset": 155}, {"referenceID": 1, "context": "(Arora et al., 2017), recursive neural networks, eg.", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "(Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(Cho et al., 2014), convolutional neural networks, eg.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg.", "startOffset": 0, "endOffset": 48}, {"referenceID": 28, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg.", "startOffset": 0, "endOffset": 48}, {"referenceID": 29, "context": "(Zhao et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "A very successful application of this paradigm is neural machine translation (NMT), see for instance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 101, "endOffset": 175}, {"referenceID": 5, "context": "A very successful application of this paradigm is neural machine translation (NMT), see for instance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 101, "endOffset": 175}, {"referenceID": 27, "context": "A very successful application of this paradigm is neural machine translation (NMT), see for instance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 101, "endOffset": 175}, {"referenceID": 2, "context": "A plausible solution is the so-called attention mechanism (Bahdanau et al., 2015).", "startOffset": 58, "endOffset": 81}, {"referenceID": 7, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 32, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 19, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 8, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 18, "context": "A possible solution could be to condition the attention on the inputs only, for instance so-called self-attention (Liu et al., 2016) or inner-attention (Lin et al.", "startOffset": 114, "endOffset": 132}, {"referenceID": 17, "context": ", 2016) or inner-attention (Lin et al., 2017).", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "This was used for instance in (Firat et al., 2016b) in a multilingual NMT system with attention.", "startOffset": 30, "endOffset": 51}, {"referenceID": 19, "context": "Many different configurations were explored by (Luong et al., 2015) for sequence-to-sequence models.", "startOffset": 47, "endOffset": 67}, {"referenceID": 9, "context": "This approach was further refined to enable zeroresource NMT (Firat et al., 2016b).", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.e. 1:N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al.", "startOffset": 0, "endOffset": 122}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.e. 1:N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al. (2016b), on the other hand, used multiple encoders, i.", "startOffset": 0, "endOffset": 147}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.e. 1:N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al. (2016b), on the other hand, used multiple encoders, i.e. M:1 training. It\u2019s not surprising that this complementarity improves MT quality, in comparison to one input language only. Many different configurations were explored by (Luong et al., 2015) for sequence-to-sequence models. Firat et al. (2016a) were the first to use multiple encoders and decoders with a shared attention mechanism.", "startOffset": 0, "endOffset": 441}, {"referenceID": 10, "context": "Several publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016).", "startOffset": 115, "endOffset": 183}, {"referenceID": 22, "context": "Several publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016).", "startOffset": 115, "endOffset": 183}, {"referenceID": 21, "context": "Several publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016).", "startOffset": 115, "endOffset": 183}, {"referenceID": 4, "context": "The usual approach is to optimize a distance or correlation between the two representations or predictive auto-encoders (Chandar et al., 2013).", "startOffset": 120, "endOffset": 142}, {"referenceID": 24, "context": "The same approach was applied to transliteration and captioning (Saha et al., 2016).", "startOffset": 64, "endOffset": 83}, {"referenceID": 1, "context": "Common approaches include: simple techniques like bag-of-words or some type of pooling, eg (Arora et al., 2017), recursive neural networks, eg.", "startOffset": 91, "endOffset": 111}, {"referenceID": 26, "context": "(Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(Cho et al., 2014), convolutional neural networks, eg.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or", "startOffset": 0, "endOffset": 48}, {"referenceID": 28, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or", "startOffset": 0, "endOffset": 48}, {"referenceID": 29, "context": "(Zhao et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015).", "startOffset": 83, "endOffset": 148}, {"referenceID": 30, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015).", "startOffset": 83, "endOffset": 148}, {"referenceID": 23, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015).", "startOffset": 83, "endOffset": 148}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015). Hermann and Blunsom (2014) proposed a compositional vector model to learn document level representations.", "startOffset": 84, "endOffset": 177}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015). Hermann and Blunsom (2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. Pham et al. (2015) directly learn a vector representations for sentences in the absence of compositional property.", "startOffset": 84, "endOffset": 333}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015). Hermann and Blunsom (2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. Pham et al. (2015) directly learn a vector representations for sentences in the absence of compositional property. Zhou et al. (2016) learn bilingual document representation by minimizing Euclidean distance between document representations and their translation.", "startOffset": 84, "endOffset": 448}, {"referenceID": 31, "context": "We have performed all our experiments with the freely available UN corpus (Ziemski et al., 2016).", "startOffset": 74, "endOffset": 96}, {"referenceID": 25, "context": "We used byte-pair encoding (BPE) as proposed by Sennrich et al. (2016). The code book is of size 20k.", "startOffset": 48, "endOffset": 71}, {"referenceID": 14, "context": "After discarding sequences longer than 50 tokens Juncys-Dowmunt et al. (2016) use 30k BPE some desired properties of such embeddings:", "startOffset": 49, "endOffset": 78}, {"referenceID": 16, "context": "We are aware of two approaches which have been used in the literature to evaluate multilingual sentence embeddings: 1) cross-lingual document classification based on the Reuters corpus which was first described in (Klementiev et al., 2012); and 2) cross-lingual evaluation of semantic textual similarity (in short STS).", "startOffset": 214, "endOffset": 239}, {"referenceID": 0, "context": "This task was first introduced in the 2016 edition of SemEval (Agirre et al., 2016).", "startOffset": 62, "endOffset": 83}, {"referenceID": 31, "context": "Such L-way parallel corpora are freely available, for instance Europarl4 (20 languages), UN corpus, 6 languages (Ziemski et al., 2016), or TED, 23 languages, (Cettolo et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 3, "context": ", 2016), or TED, 23 languages, (Cettolo et al., 2012).", "startOffset": 31, "endOffset": 53}, {"referenceID": 12, "context": "This can be very efficiently performed with the FAISS open-source toolkit (Johnson et al., 2017) which offers many", "startOffset": 74, "endOffset": 96}], "year": 2017, "abstractText": "In this paper, we use the framework of neural machine translation to learn joint sentence representations across different languages. Our hope is that a representation which is independent of the language a sentence is written in, is likely to capture the underlying semantics. We search and compare more than 1.4M sentence representations in three different languages and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.", "creator": "LaTeX with hyperref package"}}}