{"id": "1704.04198", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Room for improvement in automatic image description: an error analysis", "abstract": "In recent years we have seen rapid and significant progress in automatic image description but what are the open problems in this area? Most work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved. In this paper, we present a detailed error analysis of the descriptions generated by a state-of-the-art attention-based model. Our analysis operates on two levels: first we check the descriptions for accuracy, and then we categorize the types of errors we observe in the inaccurate descriptions. We find only 20% of the descriptions are free from errors, and surprisingly that 26% are unrelated to the image. Finally, we manually correct the most frequently occurring error types (e.g. gender identification) to estimate the performance reward for addressing these errors, observing gains of 0.2--1 BLEU point per type.", "histories": [["v1", "Thu, 13 Apr 2017 16:21:18 GMT  (3520kb,D)", "http://arxiv.org/abs/1704.04198v1", "Submitted"]], "COMMENTS": "Submitted", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["emiel van miltenburg", "desmond elliott"], "accepted": false, "id": "1704.04198"}, "pdf": {"name": "1704.04198.pdf", "metadata": {"source": "CRF", "title": "Room for improvement in automatic image description: an error analysis", "authors": ["Emiel van Miltenburg", "Desmond Elliott"], "emails": ["emiel.van.miltenburg@vu.nl", "d.elliott@uva.nl"], "sections": [{"heading": null, "text": "In recent years, we have seen rapid and significant advances in automatic image description, but what are the outstanding issues in this area? Most of the work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved. In this paper, we present a detailed error analysis of the descriptions generated by a modern attention-based model. Our analysis works on two levels: firstly, we check the descriptions for accuracy, and secondly, we categorize the types of errors we observe in the inaccurate descriptions. We find that only 20% of the descriptions are error-free, and surprisingly, 26% have nothing to do with the image. Finally, we manually correct the most common error types (e.g. gender identification) to estimate the success premium for correcting these errors, and observe gains of 0.2-1 BLEU points per type."}, {"heading": "1 Introduction", "text": "The task of automatic image description is to describe an image in natural language (Bernardi et al., 2016). Recent advances in this field have been evaluated using text-based similarity metrics such as BLEU (Papineni et al., 2002) or Meteor (Denkowski and Lavie, 2014).These metrics make it easy for researchers to assess the impact of their modelling decisions, but they do not provide any insight into the strengths and weaknesses of a proposed model, especially for gram-based metrics such as BLEU, which measure grammatical flux rather than semantic appropriateness (Reiter and Belz, 2009).In this paper, we present a coarse and fine-grained analysis of the descriptions generated by a state-of-the-art attention-based model (Xu et al., 2015)."}, {"heading": "2 Related work", "text": "Early work on image description was evaluated using text-based similarity measures and a study on the evaluation of human judgment (Bernardi et al., 2016).This type of assessment study involves asking people whether the descriptions accurately describe the image, are grammatically correct, relevant to the image, are human-like, using, among other things, a Likert scale. The main criticisms of studies evaluating human judgment are that they are expensive to perform and difficult to replicate without access to the same subject pool and control samples (e.g. Papineni1All of our codes, data and annotation guidelines will be available after publication.ar Xiv: 170 4.04 198v 1 [cs.C L] 13 Apr 201 7et al. 2002; Hodosh and Hockenmaier 2016).Nevertheless, these studies are the clearest indication of the differences between the models. Our coarse analysis is a binary version of the correction scale of our particular image subject matter (Mitchell), which is detailed in this essay."}, {"heading": "3 Error categories", "text": "We developed a non-exhaustive categorization of errors by reviewing descriptions generated by an attention-based image description model (Xu et al., 2015), we trained the model on the Flickr30K dataset (Young et al., 2014), with 300D word embedding, a 1000D green hidden layer (Cho et al., 2014), and \"CONV5,4\" imaging equipment from the VGG-19 CNN (Simonyan and Zisserman, 2015), and we generated 1,014 descriptions with a beam width of five hypotheses, giving a meteor score of 17.4 on the Flickr30K testset.In total, we identified 20 common types of errors, which we grouped into four main categories: PEP, OBJECT, and THEP, and THEP."}, {"heading": "4 Annotation tasks", "text": "We define two error description tasks: the coarse-grained annotation task is a binary categorization problem, where an annotator determines whether it applies to each description; the fine-grained annotation task is a multi-class categorization problem, given the error types presented in the previous section; each inaccurate description is commented on with one or more error types; we can imagine this task as a means of assessing the semantic processing distance between a generated description and the most accurate alternative; in total, one annotator duplicated all 1,014 generated descriptions into coarse-grained groups: accurate and inaccurate descriptions; the same annotator then performed the fine-grained annotation; and we validated the annotation scheme by duplicating a random selection of 100 descriptions (10% of the data) to determine whether the annotation guidelines provide a reliable basis for the annotation of the error."}, {"heading": "4.1 Results for the coarse-grained task", "text": "In the coarse-grained annotation paper, 812 out of 1014 descriptions (80%) were judged to be inaccurate. We achieved a good match between Cohen's \u03ba = 0.67 annotators, with an accuracy of 91%. The discrepancy between these numbers is explained by the distribution of labels: the INACCURATE category is so dominant that any discrepancy entails a heavy penalty. Of the 100 double-commented descriptions, the first and second annotators rated 86 and 81 descriptions as inaccurate, with 79 descriptions matching."}, {"heading": "4.2 Evaluating the fine-grained annotations", "text": "In the fine-grained annotation task, we doubleannotated the 79 descriptions that both notes agreed contained at least one inaccuracy. Tables 1 and 2 show the number of errors per image, and the distribution of error types across the record. In total, we found 1,265 errors in 812 descriptions, which is an average of 1.56 errors / description.Surprisingly, the most common error category is GENERLY UNLATED (264 times). Errors from the GENERAL and PEOPLE categories are much more common than the other two. Taken together, the SUBJECT category is the least common. Our intuition is that errors in deciphering the subject from the language model affect the entire sentence; the choice of the subject affects the probability of all subsequent words, leading to a general discrepancy."}, {"heading": "5 Correcting the errors", "text": "We have observed the frequency of each type of error and can ask: would there be a positive effect if a model could fix these errors? We have selected the five most common types of error (except for GENERAL UNCONNECTIONS) and corrected each error manually without looking at the reference descriptions. If a description has multiple errors, we only correct the relevant error. We have tried to be conservative in our corrections; for example, for COLOURS OF CLOTHING errors, when the system wrote white shirt instead of plaid / leopard print /... shirt, we left the description untouched instead of inserting the pattern. For the ACTIVITY errors, we have tried to change as little as possible, but editing the activity often also involves changing the object. For example, a sentence that a man holds a sign in a suit has been changed to correct a man in a suit, but neglecting the object."}, {"heading": "6 Conclusion", "text": "Our main contributions are: (1) Providing a taxonomy of errors in automatically generated image descriptions. (2) Quantifying the weaknesses of the model. We assume that any model with a similar architecture will have similar weaknesses. (3) Quantifying the possible improvement of that model when addressing those weaknesses. We focus on the nature of the inaccurate descriptions and look at various errors that they contain. But what about the accurate descriptions? The descriptions that are accurate are also much more general than the human descriptions, which usually include small but prominent details. We propose the following rule: If the majority of human descriptions contain comments on an aspect of the image that is not addressed by a generated description, then that aspect could be improved in terms of clothing."}, {"heading": "Acknowledgments", "text": "EM is supported by the Dutch Organization for Scientific Research (NWO) through the Spinozaprize awarded to Piek Vossen (SPI 30-673, 2014-2019). DE is supported by the NWO Vici Scholarship No. 277-89-002 awarded to Khalil Sima'an."}, {"heading": "A Annotation Guidelines", "text": "This year is the highest in the history of the country."}, {"heading": "A group of people standing in the snow", "text": "In fact, most of them are able to survive on their own if they do not play by the rules."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In recent years we have seen rapid and significant progress in automatic image description but what are the open problems in this area? Most work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved. In this paper, we present a detailed error analysis of the descriptions generated by a state-of-the-art attentionbased model. Our analysis operates on two levels: first we check the descriptions for accuracy, and then we categorize the types of errors we observe in the inaccurate descriptions. We find only 20% of the descriptions are free from errors, and surprisingly that 26% are unrelated to the image. Finally, we manually correct the most frequently occurring error types (e.g. gender identification) to estimate the performance reward for addressing these errors, observing gains of 0.2\u20131 BLEU point per type.", "creator": "LaTeX with hyperref package"}}}