{"id": "1704.04327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Deep API Programmer: Learning to Program with APIs", "abstract": "We present DAPIP, a Programming-By-Example system that learns to program with APIs to perform data transformation tasks. We design a domain-specific language (DSL) that allows for arbitrary concatenations of API outputs and constant strings. The DSL consists of three family of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. We then present a novel neural synthesis algorithm to search for programs in the DSL that are consistent with a given set of examples. The search algorithm uses recently introduced neural architectures to encode input-output examples and to model the program search in the DSL. We show that synthesis algorithm outperforms baseline methods for synthesizing programs on both synthetic and real-world benchmarks.", "histories": [["v1", "Fri, 14 Apr 2017 02:04:06 GMT  (92kb,D)", "http://arxiv.org/abs/1704.04327v1", "8 pages + 4 pages of supplementary material. Submitted to IJCAI 2017"]], "COMMENTS": "8 pages + 4 pages of supplementary material. Submitted to IJCAI 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["surya bhupatiraju", "rishabh singh", "abdel-rahman mohamed", "pushmeet kohli"], "accepted": false, "id": "1704.04327"}, "pdf": {"name": "1704.04327.pdf", "metadata": {"source": "CRF", "title": "Deep API Programmer: Learning to Program with APIs", "authors": ["Surya Bhupatiraju", "Rishabh Singh", "Pushmeet Kohli"], "emails": ["surya@mit.edu", "risin@microsoft.com", "asamir@microsoft.com", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2 Motivating Examples", "text": "An Excel user wanted to convert names into first initials with surnames as shown in Figure 1. Since some input examples had optional middle names, the user had difficulty finding a macro to perform the task. DAPIP learns for this task the following program: Concat (GetFirstChar (v), ConstStr (. \u2032), GetLastWord (v). The learned program uses the GetFirstChar and GetLastWord APIs, which belong to the class of Regex APIs that extract substrings from the input string based on regular expressions. Example 2. An Excel user had a list of addresses and wanted to use the city and state values as shown in Figure 2. This is an example of a very common task that cannot be performed by systems such as FlashFlash Fill. Since the data is available in many different formats, there is no uniform expression that can be used to perform the city and state values as shown in Figure 2. This is an example of a very common system type of Concat."}, {"heading": "3 Overview of Approach", "text": "The training phase of our system is illustrated in Figure 4 and the testing phase is shown in Figure 3. First, we design a DSL that allows for the composition of nested API calls with constant strings. We designed this DSL after studying a large family of real string transformation tasks, so that it is meaningful enough to encode these tasks. During the training phase, we use a program sampler to consistently test a large number of programs from this DSL. For each program, we use a rules-based approach to construct 5 input strings for the program to meet the requirements of the program. We obtain the output strings by executing the program on the input strings. During the training, each sampled program is used along with the corresponding input-output output examples to train the R3NN model, a neural architecture that distributes over the DSL extension examples."}, {"heading": "4 Domain-Specific Language", "text": "The syntax of the domain-specific language for API-based string transformations is shown in Figure 5. The uppermost construct of the language is the Concat function, which returns the concatenation of its argument substrings fi. A substring expression f can be either a constant string s, the input string v, or the result of an API function with f as argument. The Concat operator allows the composition of API calls with constant strings. The DSL expression consists of 3 types of APIs: regex APIs L, Lookup APIs L, and Transformation APIs T.Regex API: The regex APIs search for certain regular expression-based patterns in the input string and return the corresponding string. Some examples of regex APIs: GetFirstNum, GetBetFirstAndSecondCommas, and so on. Our DSL consists of 104 such regex APIs substring kits."}, {"heading": "5 Neural Architecture for Search", "text": "The neural search through the programs in the DSL, based on the input-output examples, is done using the model outlined in [Parisotto et al., 2016]. First, the input-output examples are encoded in a feature vector of fixed length, which aims to capture common patterns between the input and output strings. This example model is then passed to a neural tree-based generative model via program trees called R3NN, in order to generate the desired hidden program."}, {"heading": "5.1 Neural Input-Output Encoder", "text": "Intuitively, the encoder must capture three key pieces of information: parts of the output strings, which are likely to be constant strings; parts of the output strings, which can be computed from input strings; and some features of the sample strings, which help the program generator module identify the set of useful APIs for the given task. To simplify DSL, we assume a fixed universe of possible constant strings, so that we can focus on training the encoder to generate the likely amount of APIs. First, the I / O encoder executes two bidirectional LSTM networks separately on the input and output strings in each sample pair, creating two matrices of size 2 \u00d7 H \u00b7 T, with H being the hidden LSTM dimension and T the maximum length of the I / O string. \u2212 The encoder then pushes the output matrix over the total matrix for each T (the total matrix for each T)."}, {"heading": "5.2 Tree-Structured Generation Model", "text": "The tree generation model incrementally constructs a program tree based on the start symbol of the DSL grammar and the extension of the tree by one derivative each until a tree consisting only of non-terminal nodes is obtained. The R3NN network assigns posterior probabilities to each valid expansion of a sub-tree in order to guide the search algorithm. In other words, in the face of a partial program tree, the R3NN network decides which non-terminal node in the tree should extend and with which expansion rule in the grammar. The R3NN is defined by the following parameters: i) an M-dimensional representation (s).RM for each symbol s in the grammar, ii) an M-dimensional representation (r).RM for each grammar rule r, iii) a deep neural network for each grammar rector rule that is input as a vector x-Q number of grammars."}, {"heading": "6 Evaluation", "text": "We present the results of two large experiments and analyze the model in more detail to assess its validity. We show that our model is capable of synthesizing simple programs when it is equipped with a library of over 100 API functions. We also show that the model is capable of a strong generalization, where it can generalize not only different I / O examples for a particular program, but also for new, invisible programs."}, {"heading": "6.1 Experimental Setup and Training Details", "text": "We use both synthetic benchmarks and real FlashFill benchmarks for evaluation. Synthetic benchmarks are achieved by sampling the programs in the DSL and then using a rules-based approach to generate appropriate input-output examples. For example, if we select a program consisting of GetThirdNum and GetState APIs, the rules-based approach would ensure that the input strings in the example consist of at least three numbers and one state. For each benchmark, we will get five input strings and the corresponding output strings by running the sampling program on the input strings. Several examples of training data are given in Appendix B. We train the R3NN on a DSL consisting of only one family of APIs to evaluate its effectiveness, and the output strings are achieved by executing the input strings on the input strings."}, {"heading": "6.2 Learning API Types", "text": "Each of the three classes of API functions is much easier to interpret, but poses non-trivial challenges to the API model to learn. Lookup API functions contain large dictionaries, and the model needs to learn when to call such APIs given the input output examples. For example, while the difference between names and cities may seem trivial to human practitioners, the model needs to learn to uniquely identify each of these units. Transformation API functions pose an additional challenge; for programs that require this type of API calls, not only does the model need to learn about the encoding of the hidden dictionary, but the output string must not contain any obvious matching substrings in the input string, due to the nature of the API function. As a result, a simple string matching algorithm between inputs and outputs will not work to solve this problem, and the input output encoder needs to learn useful pairs of representations."}, {"heading": "7 80% 67%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 91% 85%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 22% 20%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 64% 63%", "text": "At this length, the DSL can generate programs with API nesting, API composition and concatenation with a constant string; this represents all possible constructs in our DSL. Lookup and Transform APIs In this experiment, we specify the maximum size of the programs in the training and validation specified at size 10, and include only the search and transformation of APIs in the DSL. The results are shown in Table 2. We note that if the DSL is limited to these APIs, the trained models achieve a very high level of accuracy and are able to identify the composition of APIs with very high precision. All APIs: Regex + Transform + Lookup We now present the model evaluation that has been trained on the full DSL. We remember that since we have trained on the full DSL, these models are referred to as FF + + models. The performance of the FF + + models is shown in Table 3."}, {"heading": "7 54% 46%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 75% 64%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 46% 37%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 50% 44%", "text": "The network that can be mapped to the specific nature of the IO encoder, as it is designed to detect patterns in sub-strings between input and output examples. Interestingly, the lookup APIs are harder to learn than the transformation APIs, due to the fact that they encode larger dictionaries than the dictionaries of transform APIs."}, {"heading": "10 32% 50% 72%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50 37% 52% 89%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3 FlashFill using API Compositions", "text": "We now present the results of the best FF and FF + + models based on the FlashFill benchmarks compiled by the authors of FlashFill [Gulwani et al., 2012]. These benchmarks correspond to real string transformation tasks in Excel, where each benchmark consists of 5 input-output string examples."}, {"heading": "FF models", "text": "The basic model performs a uniform search via DSL extensions and tends to use small programs. We also present stochastic sample results for a fair comparison with the performance of the FF models. The uniform search works surprisingly well considering the large space of all possible programs, because the DSL we designed with APIs can solve many of the benchmarks with a single call, e.g. GetFirstWord, and the uniform search sampler tends to use shorter programs. The performance of the FF model on FlashFill benchmarks We now evaluate the trained models whose accuracy is reported on synthetic data in Table 1. Note: Unlike in Table1, each model is evaluated on the same data set, so the results are comparable across rows."}, {"heading": "7 Related Work", "text": "We describe the related work from the domains of VSA-based programming using sample systems and neural program induction and synthesis systems. Programming By Example for String Manipulations There have been many recent works on the design of version-space algebra-based PBE systems for performing data transformations and extractions. FlashFill [Gulwani, 2011; Gulwani et al., 2012] is a PBE system that performs regular expression transformations using examples. Faced with an input output sample string, FlashFill first searches for all possible ways to decompose the output string and present the set of these subprograms concisely using a DAG data structure. This VSA-based approach was then expanded to also build PBE systems for number transformations [Singh and Gulwani, 2012b], Table connects [Singh and Gulwani, Gulwana, 2012SS, Gulwana, 2012SS, and Gul2SS, 2012SS]."}, {"heading": "8 Future Work", "text": "There are a number of ways in which we can expand the results and techniques presented in this paper to both improve the current numbers and allow us to scale larger programs."}, {"heading": "8.1 Function embeddings", "text": "We rely on the R3NN and the input-output encoder to implicitly encode the semantics of each function, and we have demonstrated through a series of experiments that the tree model is capable of doing so. This is impressive in itself, but to further improve performance, we should expand the model to support explicit, continuous representations of each function, which can be achieved in several ways - the simplest being to encode each function as a randomly initialized vector and allow the model to take care of API functions that may be relevant to the input-output examples. We can freeze the embedding or choose to undo errors through both the attention mechanism and the embedding, and learn these representations together."}, {"heading": "8.2 Divide and conquer", "text": "Function embedding allows us to better solve existing problems by giving the model more information about what decisions to make when creating the tree. However, this does not solve the problem of scalability. However, even with functional embedding, as inputs and outputs increase in size and complexity, we do not have a scalable method to draw conclusions about which programs to synthesize. However, rather than looking at the problem as a whole, we can break the problem down into smaller pieces and try to solve each piece and concatenate the answers to each other. This \"parts-and-victory\" approach allows us to treat larger problems as conglomerations of a number of smaller problems. This procedure requires two general mechanisms: one module must predict how to split the output string into smaller, significant chunks, and the second module consumes each piece of input-output, synthesizes the correct program, and each part is eventually linked together."}, {"heading": "8.3 Extending the DSL", "text": "An interesting addition to DSL is the addition of API function calls with multiple arguments. This could produce more general API functions, such as GetNthObj (n, o), and replace functions such as GetFirstWord, GetSecondNumber. In addition, we can also add Concat functions with multiple arguments; this idea fits well with the \"divide-and-conquer\" approach and can be used to scale the model to synthesize larger programs."}, {"heading": "8.4 Batching Trees", "text": "While the \"divide-and-win\" approach is an algorithmic improvement to speed up the process of training the model, we can also use the model to integrate faster batch proocols. Using a tree-based generative model allows us to stack operations that take place at the same depth in the tree, as each operation is independent of all its siblings. In addition, we can stack multiple trees together to increase performance."}, {"heading": "9 Conclusion", "text": "In this paper, we introduced DAPIP, a system that attempts to automatically learn a synthesis algorithm at DSL. Specifically, we developed a DSL consisting of APIs as the first class construct that allows the system to perform more extensive tasks with small programs. We used the recently introduced neural architecture R3NN to automatically learn a synthesis algorithm for our DSL. Preliminary results indicate that the system is capable of learning programs up to size 10 efficiently and with an accuracy of about 45% on real benchmarks. We believe that this direction of using neural architectures to automatically develop synthesis algorithms for PBE systems can lead to great advances in program synthesis techniques and make them more widely applicable to many new areas."}, {"heading": "B Samples of Training Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Samples of Solved FlashFill Benchmarks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D Samples of Unsolved FlashFill Benchmarks", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Syntax-guided synthesis", "author": ["Alur et al", "2013] Rajeev Alur", "Rastislav Bodik", "Garvit Juniwal", "Milo MK Martin", "Mukund Raghothaman", "Sanjit A Seshia", "Rishabh Singh", "Armando Solar-Lezama", "Emina Torlak", "Abhishek Udupa"], "venue": "In Formal Methods in Computer-Aided Design (FMCAD),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Flashrelate: extracting relational data from semi-structured spreadsheets using examples", "author": ["Daniel W. Barowy", "Sumit Gulwani", "Ted Hart", "Benjamin G. Zorn"], "venue": "PLDI, pages 218\u2013228,", "citeRegEx": "Barowy et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "volume 479", "author": ["Tamraparni Dasu", "Theodore Johnson. Exploratory data mining", "data cleaning"], "venue": "John Wiley & Sons,", "citeRegEx": "Dasu and Johnson. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "Gaunt et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber. Framewise phoneme classification with bidirectional lstm", "other neural network architectures"], "venue": "18(5):602\u2013610,", "citeRegEx": "Graves and Schmidhuber. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 1st International Joint Conference on Artificial Intelligence", "author": ["Cordell Green. Application of theorem proving to problem solving"], "venue": "IJCAI\u201969, pages 219\u2013239,", "citeRegEx": "Green. 1969", "shortCiteRegEx": null, "year": 1969}, {"title": "ACM", "author": ["Sumit Gulwani", "William R. Harris", "Rishabh Singh. Spreadsheet data manipulation using examples. Commun"], "venue": "55(8):97\u2013105,", "citeRegEx": "Gulwani et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In POPL", "author": ["Sumit Gulwani. Automating string processing in spreadsheets using input-output examples"], "venue": "pages 317\u2013330,", "citeRegEx": "Gulwani. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Armand Joulin", "Tomas Mikolov. Inferring algorithmic patterns with stackaugmented recurrent nets"], "venue": "pages 190\u2013198,", "citeRegEx": "Joulin and Mikolov. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser and Sutskever. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wrangler: interactive visual specification of data transformation scripts", "author": ["Kandel et al", "2011] Sean Kandel", "Andreas Paepcke", "Joseph M. Hellerstein", "Jeffrey Heer"], "venue": "In Proceedings of the International Conference on Human Factors in Computing Systems,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural random-access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.06392,", "citeRegEx": "Kurach et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine Learning", "author": ["Tessa A. Lau", "Steven A. Wolfman", "Pedro M. Domingos", "Daniel S. Weld. Programming by demonstration using version space algebra"], "venue": "53(1-2):111\u2013156,", "citeRegEx": "Lau et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Flashextract: a framework for data extraction by examples", "author": ["Vu Le", "Sumit Gulwani"], "venue": "PLDI, page 55,", "citeRegEx": "Le and Gulwani. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Neuro-symbolic program synthesis", "author": ["Emilio Parisotto", "Abdel-rahman Mohamed", "Rishabh Singh", "Lihong Li", "Dengyong Zhou", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1611.01855,", "citeRegEx": "Parisotto et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Flashmeta: a framework for inductive program synthesis", "author": ["Oleksandr Polozov", "Sumit Gulwani"], "venue": "OOPSLA, pages 107\u2013126,", "citeRegEx": "Polozov and Gulwani. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "PVLDB", "author": ["Rishabh Singh", "Sumit Gulwani. Learning semantic string transformations from examples"], "venue": "5(8):740\u2013751,", "citeRegEx": "Singh and Gulwani. 2012a", "shortCiteRegEx": null, "year": 2012}, {"title": "In CAV", "author": ["Rishabh Singh", "Sumit Gulwani. Synthesizing number transformations from inputoutput examples"], "venue": "pages 634\u2013651,", "citeRegEx": "Singh and Gulwani. 2012b", "shortCiteRegEx": null, "year": 2012}, {"title": "Blinkfill: Semi-supervised programming by example for syntactic string transformations", "author": ["Rishabh Singh"], "venue": "PVLDB, 9(10):816\u2013827,", "citeRegEx": "Singh. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "The ability to discover a program consistent with a given user intent (specification) is considered as one of the central problems in artificial intelligence [Green, 1969].", "startOffset": 158, "endOffset": 171}, {"referenceID": 7, "context": "Moreover, the state-of-the-art synthesis techniques [Gulwani et al., 2012] require a great deal of domain expertise with manually designed heuristics and rules to develop an efficient search procedure.", "startOffset": 52, "endOffset": 74}, {"referenceID": 2, "context": "Some studies have reported that this data wrangling process can sometimes take up to 80% of the total data analysis time [Dasu and Johnson, 2003; Kandel et al., 2011].", "startOffset": 121, "endOffset": 166}, {"referenceID": 8, "context": "Recently, ProgrammingBy-Example (PBE) techniques such as FlashFill [Gulwani, 2011; Gulwani et al., 2012] and BlinkFill [Singh, 2016] were developed to help users perform data transformation tasks using examples instead of having to write complex programs.", "startOffset": 67, "endOffset": 104}, {"referenceID": 7, "context": "Recently, ProgrammingBy-Example (PBE) techniques such as FlashFill [Gulwani, 2011; Gulwani et al., 2012] and BlinkFill [Singh, 2016] were developed to help users perform data transformation tasks using examples instead of having to write complex programs.", "startOffset": 67, "endOffset": 104}, {"referenceID": 21, "context": ", 2012] and BlinkFill [Singh, 2016] were developed to help users perform data transformation tasks using examples instead of having to write complex programs.", "startOffset": 22, "endOffset": 35}, {"referenceID": 18, "context": "These techniques encode the space of programs using a domain-specific language (DSL), and then develop algorithms based on version-space algebra (VSA) [Polozov and Gulwani, 2015; Lau et al., 2003] to efficiently search the space of programs.", "startOffset": 151, "endOffset": 196}, {"referenceID": 15, "context": "These techniques encode the space of programs using a domain-specific language (DSL), and then develop algorithms based on version-space algebra (VSA) [Polozov and Gulwani, 2015; Lau et al., 2003] to efficiently search the space of programs.", "startOffset": 151, "endOffset": 196}, {"referenceID": 17, "context": "The second shortcoming is handled by learning the synthesis algorithm in DAPIP automatically from data using two recently introduced neural modules [Parisotto et al., 2016].", "startOffset": 148, "endOffset": 172}, {"referenceID": 9, "context": "The first module called the cross-correlational encoder computes a fixed-dimension vector representation of the inputoutput examples by using tensor representations obtained by running two bi-directional LSTMs [Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005] on the input and output strings and computing their cross correlation.", "startOffset": 210, "endOffset": 274}, {"referenceID": 4, "context": "The first module called the cross-correlational encoder computes a fixed-dimension vector representation of the inputoutput examples by using tensor representations obtained by running two bi-directional LSTMs [Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005] on the input and output strings and computing their cross correlation.", "startOffset": 210, "endOffset": 274}, {"referenceID": 17, "context": "The neural search over the programs in the DSL conditioned on the input-output examples is performed using the model outlined in [Parisotto et al., 2016].", "startOffset": 129, "endOffset": 153}, {"referenceID": 13, "context": "We use the Adam optimizer [Kingma and Ba, 2014], with an initial learning rate of 0.", "startOffset": 26, "endOffset": 47}, {"referenceID": 7, "context": "3 FlashFill using API Compositions We now present the results of the best FF and FF++ models on the FlashFill benchmarks obtained from the authors of FlashFill [Gulwani et al., 2012].", "startOffset": 160, "endOffset": 182}, {"referenceID": 17, "context": "It surpasses the performance of Neural FlashFill [Parisotto et al., 2016], which achieves an accuracy of 23% with 100 samples and 34% with 1000 samples.", "startOffset": 49, "endOffset": 73}, {"referenceID": 8, "context": "FlashFill [Gulwani, 2011; Gulwani et al., 2012] is a PBE system that performs regular expression based string transformations using examples.", "startOffset": 10, "endOffset": 47}, {"referenceID": 7, "context": "FlashFill [Gulwani, 2011; Gulwani et al., 2012] is a PBE system that performs regular expression based string transformations using examples.", "startOffset": 10, "endOffset": 47}, {"referenceID": 20, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al.", "startOffset": 100, "endOffset": 126}, {"referenceID": 19, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al.", "startOffset": 140, "endOffset": 166}, {"referenceID": 16, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al.", "startOffset": 184, "endOffset": 206}, {"referenceID": 1, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al., 2015].", "startOffset": 227, "endOffset": 248}, {"referenceID": 11, "context": "The goal in neural program induction is to teach neural networks the functional behavior of a program by augmenting the neural networks with additional computational modules such as Neural GPU [Kaiser and Sutskever, 2015], Neural Turing Machine [Graves et al.", "startOffset": 193, "endOffset": 221}, {"referenceID": 5, "context": "The goal in neural program induction is to teach neural networks the functional behavior of a program by augmenting the neural networks with additional computational modules such as Neural GPU [Kaiser and Sutskever, 2015], Neural Turing Machine [Graves et al., 2014], and stacks-augmented RNNs [Joulin and Mikolov, 2015].", "startOffset": 245, "endOffset": 266}, {"referenceID": 10, "context": ", 2014], and stacks-augmented RNNs [Joulin and Mikolov, 2015].", "startOffset": 35, "endOffset": 61}, {"referenceID": 3, "context": "More recent work, such as Terpret [Gaunt et al., 2016] and Neural-RAM [Kurach et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 14, "context": ", 2016] and Neural-RAM [Kurach et al., 2015] seek to mitigate the interpretability issue but they need to be trained for each individual benchmark problem, which is prohibitively expensive.", "startOffset": 23, "endOffset": 44}, {"referenceID": 17, "context": "A recent approach was proposed to use the R3NN-based neural architectures to synthesize programs in a DSL similar to that of FlashFill [Parisotto et al., 2016].", "startOffset": 135, "endOffset": 159}], "year": 2017, "abstractText": "We present DAPIP, a Programming-By-Example system that learns to program with APIs to perform data transformation tasks. We design a domainspecific language (DSL) that allows for arbitrary concatenations of API outputs and constant strings. The DSL consists of three family of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. We then present a novel neural synthesis algorithm to search for programs in the DSL that are consistent with a given set of examples. The search algorithm uses recently introduced neural architectures to encode input-output examples and to model the program search in the DSL. We show that synthesis algorithm outperforms baseline methods for synthesizing programs on both synthetic and real-world benchmarks.", "creator": "LaTeX with hyperref package"}}}