{"id": "1704.04336", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "An entity-driven recursive neural network model for chinese discourse coherence modeling", "abstract": "Chinese discourse coherence modeling remains a challenge taskin Natural Language Processing field.Existing approaches mostlyfocus on the need for feature engineering, whichadoptthe sophisticated features to capture the logic or syntactic or semantic relationships acrosssentences within a text.In this paper, we present an entity-drivenrecursive deep modelfor the Chinese discourse coherence evaluation based on current English discourse coherenceneural network model. Specifically, to overcome the shortage of identifying the entity(nouns) overlap across sentences in the currentmodel, Our combined modelsuccessfully investigatesthe entities information into the recursive neural network freamework.Evaluation results on both sentence ordering and machine translation coherence rating task show the effectiveness of the proposed model, which significantly outperforms the existing strong baseline.", "histories": [["v1", "Fri, 14 Apr 2017 02:41:13 GMT  (321kb)", "http://arxiv.org/abs/1704.04336v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fan xu", "shujing du", "maoxi li", "mingwen wang"], "accepted": false, "id": "1704.04336"}, "pdf": {"name": "1704.04336.pdf", "metadata": {"source": "CRF", "title": "AN ENTITY-DRIVEN RECURSIVE NEURAL NETWORK MODEL FOR CHINESE DISCOURSE COHERENCE MODELING", "authors": ["Fan Xu", "Shujing Du", "Maoxi Li", "Mingwen Wang"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijaia.2017.8201 1Modeling the coherence of Chinese discourse remains a challenge for the field of natural language processing. Existing approaches mainly focus on the need for feature engineering that adopts the complex features to capture the logical or syntactic or semantic relationships within a text. In this paper, we present a holistic recursive depth model for assessing the coherence of Chinese discourse based on the current English discourse model. In particular, to address the lack of overlaps of unity (nouns) between sentences in the current model, our combined model successfully examines the information of units in the recursive neural network space. Evaluation results on both the statute order and the coherence of machine translations show the effectiveness of the proposed model, which significantly exceeds the existing strong base model."}, {"heading": "1. INTRODUCTION", "text": "It is one of the most important problems in the field of science (NLP) because it is applied in many areas of science. [8] In general, a coherent discourse has many similar components (lexical overlaps or corrections)."}, {"heading": "2. RELATED WORK", "text": "In this section we describe the related work on modelling the discourse coherence of traditional or neural network modes."}, {"heading": "2.1. TRADITIONAL COHERENCE MODEL", "text": "The task of DCM was first introduced by Foltz et al. [19], who formulated discourse coherence as a function of the semantic relationship between two adjacent sentences within a text and used a vector-based representation of lexical meaning to calculate semantic relation. Since then, many supervised approaches to DCM, such as the entity model [11] [13] [15], discourse relationship-based model [16], syntactic pattern-based model [17], co-reference-based resolution-based model [20], content-based model of the Hidden Markov Model (HMM) [22], and cohesion-driven model [23] have been proposed in the literature. To be more specific, Barzilay and Lapata [11] refer to an entity-based model for capturing units of discourse within a text."}, {"heading": "2.2. NEURAL COHERENCE MODEL", "text": "Recently, Li et al. [18] presented a neural depth model for English discourse coherence modeling, showing the effectiveness of both relapsing and recursive neural networks (RNN) for the English situation. However, as mentioned in Section 1, their model did not take into account the distribution or overlap of entities (nouns) across sentences within a text. In fact, the overlap between two adjacent sentences indicates logical or semantic coherence for parattexts, so we are successfully integrating this information into their model."}, {"heading": "3. ENTITY-DRIVEN RNN COHERENCE MODEL", "text": "In this section we describe our holistic Chinese discourse model of RNN coherence."}, {"heading": "3.1. FRAMEWORK", "text": "Figure 1 shows the entity-driven recursive deep model for Chinese discourse coherence modeling. Our deep model is based on Li et al. [18] s English discourse coherence framework. In comparison, their model does not intensify the effectiveness of entities in every sentence of a text. Therefore, we successfully integrate the entities into the current recursive neural network model."}, {"heading": "3.2. SENTENCE REPRESENTATION", "text": "For word-level representation, each word can be represented in a sentence by using a vector representation (or word embedding) and can capture the semantic meanings by toolkit, e.g. word2vec 1 or glove 2. Specifically, the word of a sentence can be represented by a specific vector embedding ew = {ew 1, ew 2,..., ew K}, with K embedding the dimension of the word. 1http: / / code.google.com / p / word2vec / 2http: / / nlp.stanford.edu / projects / glove / For sentence-level representation, as shown in Figure 1, the vector representation for the entire sentence is recursively calculated as a representation for each parent node based on its immediate children until reaching the root of the tree. Specifically, the vector representation for each parent node is calculated on the basis of its immediate children, recursively until reaching the tree root."}, {"heading": "3.3. ENTITY-DRIVEN SENTENCE CONVOLUTION", "text": "The framework treats a sentence window like a clique C (sliding window of L records) and associates each clique with a tag yc, which takes the value 1 if it is coherent, and 0 otherwise. As shown in Figure 1, each clique C takes a (L * K) * 1vector hc as input, concatenating the embedding of all of its contained records. The hidden layer takes hc as input and performs the folding using a nonlinear Tanh function. Thus, the concatenating output vector for hidden layers, defined as qc, can be rewritten as follows: qc = f (Wsen * (hc * hentity) + bsen) (2), where Wsen is a H * (L * K) dimensional matrix and Bsen is a H * 1 dimensional bias vector; H refers to the number of neurons in the hidden layer."}, {"heading": "3.3.1. ENTITY-DRIVEN MECHANISM", "text": "First, we perform a vector sum operation for the word embedding of each noun to generate a hentity formulated as follows: Hentity = ewNNN1, ewNN2, ewNNk (3). Then we perform an elementary multiplication operation between hc and hentity.The value of the output layer can be formulated as follows: P (yc = 1) = sigmod (U T qc + b) (4), where U is a H * 1 vector and b denotes bias; yc with a value of 1 means the text is coherent, and 0 others.Therefore, the total coherence value for a given document is the probability that all cliques within the document are coherent given by: Sd = KE = dC-Zyp) 1 ((5). Finally, we can determine whether a text is coherent according to the value of its coherence point."}, {"heading": "3.4. TRAINING AND OPTIMIZATION", "text": "The cost function of the model is derived from: 201 () 2C trainsetQ J HM M \u03b8. The cost function of the model is derived from: 201 () 2C trainsetQ J HM M."}, {"heading": "4. EXPERIMENTS", "text": "In this section, we show the effectiveness of our discourse coherence model through both sentence order and machine translation coherence assessment tasks. The former aims to distinguish an original text from a permutated arrangement of its sentences, while the latter aims to distinguish a human or reference translation from automatically generated translations."}, {"heading": "4.1. DATASET", "text": "Sentence Ordering Dataset: We select documents for Chinese Treebank 6.0 from the Linguistic Data Consortium (LDC) with catalog number LDC2007T36 and ISBN1-58563-450-6. We select the 100 documents from chtb _ 2946 to chtb _ 3045 as our training data set and the 100 documents from chtb _ 3046 to chtb _ 3145 as our test data set. The sentences in each source file are permutated a maximum of 20 times. The total number of test texts is 1027. The average number of sentences is 10.33 and 13.56 for training data sets and test data sets respectively. In the evaluation, we consider the original texts more coherent (positive instances) than the permutated (negative instances). Machine translation data sets: Similarly, we consider documents for NIST Open Machine Translation 2008 Evaluation (MT08) selected Reference and System Translations from Linguistic Data Consortium (negative instances)."}, {"heading": "4.2. EXPERIMENTAL SETTINGS", "text": "Initialization: Similar to Li et al. [18], the parameter Wsen, Wrecursive and h0 is initialized by random dragging from the uniform distribution. The number of hidden layer H is set to 100. Learning rate in the optimization process is set to 0.01 and the batch size to 20. Word embedding {e} for Chinese is trained differently with word2vec or Glove. The dimension for Word embedding is 50 or 100. Window size L is 3 or 5. Evaluation Metric: We report on the performance of the system with accuracy, that is the ratio of the number of selected original text / translation document divided by the total number of texts / translation documents. Baseline System 1: Entity-graph-based model [14], which was implemented as a simple but effective implementation of the entity-based system with accuracy. We implement their method in this paper, which is based on publicly available Code 3: Entity-Grapty-2 Baseline-System: Entih-2, but successful as a simple one."}, {"heading": "4.3. EXPERIMENT RESULTS", "text": "In this section, we report on the results of the experiment to model the coherence of Chinese discourse, both in terms of punctuation and in terms of assessing the coherence of machine translation."}, {"heading": "4.3.1. RESULTS ON SENTENCE ORDERING", "text": "Table 1 shows the performance of our enterprise-oriented depth model using different window sizes, different dimensions and with different word embedding. As shown in Table 1, performance increases with increasing window size. In fact, the larger the dimension, the more representative it becomes. (2) Window sizePerformance decreases with increasing window size, and the best performance comes with window size at 3. It is largely caused by the local entity distribution characteristic of Barzilay and Lapata [11]. [12], Guinaudeau and Strube [14]. As the increase in the number of window sizes occurs, the interaction of the entities decreases accordingly. 3http: / / github.com / karins / CoherenceFramework. 4http: / nlp.stanford.edu / software / lex-parser.shtml. This subordinate Chinese model shows that the subordinate Chinese model is largely ineffective under current circumstances."}, {"heading": "4.3.2. RESULTS ON MACHINE TRANSLATION COHERENCE RATING", "text": "Table 3 below lists the performance of our model and base model. In fact, the evaluation of discourse coherence for the task of machine translation is more common than the evaluation of the task. As the results in Table 3 show, our model significantly exceeds the current model. Furthermore, our model significantly exceeds the traditional model of entity distribution. It is largely caused by the fact that the entity distribution is not obvious in the text generated by the machine, but the information about entities (nouns) can still be integrated into the current recursive neural network model."}, {"heading": "5. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we present an action-driven recursive depth model for modelling the coherence of Chinese discourses. We successfully integrate the units across each sentence into the current recursive neural framework. Evaluation results for both sentence order and machine translation coherence show the effectiveness of the proposed model. Our future work is to integrate the co-reference mechanism into the current combined recursive neural network model along with other coherence assessment tasks."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank the anonymous reviewers for their comments on this paper. This research was supported by the National Natural Science Foundation of China under funding numbers 61402208, No.61462045, No.61462044, No.61662030, the Natural Science Foundation and Education Department of Jiangxi Province under funding numbers 20151BAB207027 and GJJ150351, and the State Language Commission research project under funding numbers YB125-99."}], "references": [{"title": "Cohesion and Statistical Machine Translation", "author": ["J. Heidi"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Generation Using Utility-Trained Coherence Models", "author": ["Radu Soricut", "Daniel"], "venue": "In Proceedings of COLING-ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Lee,(2004),\u201cCatching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization", "author": ["Regina Barzilay", "Lillian"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Hierarchical Neural Autoencoder for Paragraphs and Documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan"], "venue": "In Proceedings of ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation", "author": ["Zi-Heng Lin", "Hwee Tou Ng", "Min-Yen"], "venue": "In Proceedings of ACL,pages", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Bottom-Up Approach to Sentence Ordering for Multi-Document Summarization\u201d,In", "author": ["Danushka Bollegala", "Naoaki Okazaki", "Mitsuru"], "venue": "Proceedings of ICCL-ACL,pages", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Briscoe,(2012),\u201cModeling coherence in ESOL learner texts", "author": ["Helen Yannakoudakis", "Ted"], "venue": "In Proceedings of ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Entity-Based Features to Model Coherence in Student Essays", "author": ["Jill Burstein", "Joel Tetreault", "Slava"], "venue": "In Proeedings of NAACL-HLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Multiple Aspects of Coherence in Student Essays", "author": ["Derrick Higgins", "Jill Burstin", "Daniel Marcu", "Claudia"], "venue": "In Proceedings of NAACL-HLT,pages185-192", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Framework for Modeling the Local Coherence of Discourse\u201d,Computational", "author": ["Barbara J. Grosz", "Scott Weinstein", "Aravind K"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "Local Coherence: An Entity-Based Approach", "author": ["Regina Barzilay", "Mirella"], "venue": "In Proceedings of ACL,pages", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Local Coherence: An Entity-Based Approach\u201d,Computational Linguistics, 34(1):1-34", "author": ["Regina Barzilay", "Mirella"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Automatic Evaluation of Text Coherence: Models and Representations", "author": ["Mirella Lapata", "Regina"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Graph-based local coherence modeling", "author": ["Camille Guinaudeau", "Michael Strube"], "venue": "In Proceedings of ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Hirst,(2012),\u201cExtending the Entity-based Coherence Model with Multiple Ranks", "author": ["Vanessa Wei Feng", "Graeme"], "venue": "In Proceedings of EACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Evaluating Text Coherence Using Discourse Relations", "author": ["Zi-Heng Lin", "Hwee Tou Ng", "Min-Yen"], "venue": "In Proceedings of ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Nenkova,(2012),\u201cA coherence model based on syntactic patterns", "author": ["Annie Louis", "Ani"], "venue": "InProceedings of EMNLP-CNLL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Hovy,(2014),\u201cA Model of Coherence Based on Distributed Sentence Representation", "author": ["Jiwei Li", "Eduard"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2048}, {"title": "Landauer,(1998),\u201cThe measurement of textual coherence with latent semantic analysis\u201d,Discourse", "author": ["Peter W. Foltz", "Walter Kintsch", "Thomas K"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Metric for Evaluating Discourse Coherence based on Coreference Resolution", "author": ["Ryu Iida", "Takenobu"], "venue": "In Proceedings of COLING, Poster,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Unified Local and Global Model for Discourse Coherence", "author": ["Micha Elsner", "Joseph Austerweil", "Eugene"], "venue": "In Proceedings of NAACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Discourse Coherence Modeling", "author": ["Fan Xu", "Qiaoming Zhu", "Guodong Zhou", "Mingwen"], "venue": "Journal of Chinese Information Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "It is considered one of the key problems in Natural Language Processing (NLP) due to its wide usage in many NLP applications, such as statistical machine translation, discourse generation, text automation summarization, student essay scoring [7][8][9] .", "startOffset": 242, "endOffset": 245}, {"referenceID": 7, "context": "It is considered one of the key problems in Natural Language Processing (NLP) due to its wide usage in many NLP applications, such as statistical machine translation, discourse generation, text automation summarization, student essay scoring [7][8][9] .", "startOffset": 245, "endOffset": 248}, {"referenceID": 8, "context": "It is considered one of the key problems in Natural Language Processing (NLP) due to its wide usage in many NLP applications, such as statistical machine translation, discourse generation, text automation summarization, student essay scoring [7][8][9] .", "startOffset": 248, "endOffset": 251}, {"referenceID": 14, "context": "Thereafter, many extension works were presented such as Feng and Hirst [15] \u2019s multiple ranking model, Lin et al.", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "[16] \u2019s discourse relationbased approach, Louis and Nenkova\u2019s syntactic patterns-based model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.", "startOffset": 191, "endOffset": 195}, {"referenceID": 2, "context": "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.", "startOffset": 252, "endOffset": 255}, {"referenceID": 20, "context": "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.", "startOffset": 255, "endOffset": 259}, {"referenceID": 10, "context": "To be more specific, Barzilay and Lapata [11][12] presented an entity-based model to capture the distribution of discourse entities between two adjacent sentences within a text.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "To be more specific, Barzilay and Lapata [11][12] presented an entity-based model to capture the distribution of discourse entities between two adjacent sentences within a text.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "[3] and Elsner et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[22] showed that an Hidden Markov Model (HMM)-based content model can be used to capture the topic\u2019s transfer from the first sentence to the end sentence of a text, where topics were formulated as hidden states and sentences were treated as observations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] \u2019s English discourse coherence framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] , the parameter Wsen, Wrecursive and h0are initialized by randomly drawing from the uniform distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "It is mostly caused by the local entity distribution characteristic demenstrated by Barzilay and Lapata [11][12] ,Guinaudeau and Strube [14] .", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "It is mostly caused by the local entity distribution characteristic demenstrated by Barzilay and Lapata [11][12] ,Guinaudeau and Strube [14] .", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "It is mostly caused by the local entity distribution characteristic demenstrated by Barzilay and Lapata [11][12] ,Guinaudeau and Strube [14] .", "startOffset": 136, "endOffset": 140}], "year": 2017, "abstractText": "Chinese discourse coherence modeling remains a challenge taskin Natural Language Processing field.Existing approaches mostlyfocus on the need for feature engineering, whichadoptthe sophisticated features to capture the logic or syntactic or semantic relationships acrosssentences within a text.In this paper, we present an entity-drivenrecursive deep modelfor the Chinese discourse coherence evaluation based on current English discourse coherenceneural network model. Specifically, to overcome the shortage of identifying the entity(nouns) overlap across sentences in the currentmodel, Our combined modelsuccessfully investigatesthe entities information into the recursive neural network freamework.Evaluation results on both sentence ordering and machine translation coherence rating task show the effectiveness of the proposed model, which significantly outperforms the existing strong baseline.", "creator": "PScript5.dll Version 5.2.2"}}}