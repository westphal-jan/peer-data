{"id": "1704.04341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Environment-Independent Task Specifications via GLTL", "abstract": "We propose a new task-specification language for Markov decision processes that is designed to be an improvement over reward functions by being environment independent. The language is a variant of Linear Temporal Logic (LTL) that is extended to probabilistic specifications in a way that permits approximations to be learned in finite time. We provide several small environments that demonstrate the advantages of our geometric LTL (GLTL) language and illustrate how it can be used to specify standard reinforcement-learning tasks straightforwardly.", "histories": [["v1", "Fri, 14 Apr 2017 03:41:59 GMT  (108kb,D)", "http://arxiv.org/abs/1704.04341v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael l littman", "ufuk topcu", "jie fu", "charles isbell", "min wen", "james macglashan"], "accepted": false, "id": "1704.04341"}, "pdf": {"name": "1704.04341.pdf", "metadata": {"source": "CRF", "title": "Environment-Independent Task Specifications via GLTL", "authors": ["Michael L. Littman", "Ufuk Topcu", "Jie Fu", "Charles Isbell", "Min Wen", "James MacGlashan"], "emails": [], "sections": [{"heading": null, "text": "We propose a new task specification language for Markov decision-making processes that aims to improve on reward functions by being environment-independent. Language is a variant of Linear Temporal Logic (LTL) that is extended to probabilistic specifications so that approximations can be learned in finite time. We provide several small environments that demonstrate the benefits of our geometric LTL (GLTL) language and illustrate how it can be used to easily specify standard amplification tasks."}, {"heading": "1 Introduction", "text": "The thesis of this paper is that (1) rewards are an excellent way to control the behavior of agents, but (2) rewards are difficult to use to specify behaviors in an environmentally independent manner, so (3) we need intermediate representations between behavioral specifications and reward functions. The intermediate representation we propose is a novel variant of linear time logic that is modified to be likely to better support affirmation and learning tasks. In the past, linear time logic has been used to specify reward functions that depend on temporal sequences (Bacchus et al., 1996); here, we expand the role to provide robust and consistent semantics that allows to specify desired behaviors in an environmentally independent manner. In short, our approach involves specifying tasks via temporal operators that have a constant probability that this step will occur at each step of the decision-making process."}, {"heading": "1.1 Specifying behavior via reward functions", "text": "An MDP consists of a finite state space, action space, transition function, and reward function as a reward function. Faced with an environment, an agent should behave in a way that maximizes the cumulative discounted expected reward. \u2212 However, the problems of learning and planning in such environments have been extensively studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009).An agent of enhanced learning (RL) must learn to maximize cumulative reward, starting with an incomplete model of MDP itself.For \"programming\" reinforcement-learning agents, the state of the art is to define a reward function and then for the learning agent to interact with the environment to maximize its reward. Reward-based specifications have proven to be extremely valuable for optimal planning in complex, uncertain environments (Russell & Norvig, 1994)."}, {"heading": "1.2 Specifying behavior via LTL", "text": "An alternative to specifying tasks via reward functions is the use of a formal specification such as linear temporal logic or LTL (Manna & Pnueli, 1992; Baier & Katoen, 2008).Linear temporal logic formulas are built from a series of atomic suggestions; the logical connectives: negation (\u00ac), disjunction (\u20ac), conjunction (\u20ac) and material implication (\u2192); and the temporal modal operators: further (\u00a9), finally () and up to (U).A wide class of properties, including safety (\u00ac b), goal guarantee (\u2666 g), progress (\u2666 g), reaction (b \u2192 g), stability (\u0445 g), where b and g are atomic propositions, can be expressed as LTL formulas. More complicated specifications can be derived from the composition of such simple formulas these."}, {"heading": "2 Learning To Satisfy LTL", "text": "While demonstrable guarantees of efficiency and optimism have been at the center of literature on learning (Fiechter, 1994; Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Li et al., 2011), correctness in complex, high-level task specifications - during learning itself or in the behavior that results from the learning phase - has attracted limited attention (Abbeel & Ng, 2005)."}, {"heading": "2.1 Geometric linear temporal logic", "text": "The idea of the GLTL is to limit the period of validity of time operators to limited windows - similar to the limited semantics of the LTL (Manna & Pnueli, 1992). To this end, GLTL introduces operators in the form of \u2666 b with the atomic clause b, which holds as \"b finally within k time steps in which k is a random variable following a geometric distribution with the parameter \u00b5.\" Similar semantics \u00b5 is a limitation of the window of validity for other time operators. This kind of geometric decade fits very well with MDPs for a few reasons. It can be seen as a generalization of reward dislocation that already exists in many MDP models."}, {"heading": "3 Related Work", "text": "De Alfaro et al. (2003, 2004) extend the calculation tree logic (CTL) with discounting and develop fixed-point-based algorithms to verify such properties for probabilistic systems and games. Almagor et al. (2014) explicitly refine the \"ultimate\" operator of LTL into a discounting agent, so that the longer it takes to complete the task, the lower the satisfaction value. They also show that discounted LTL is more meaningful than discounted CTL. They use both discounted and discounted LTL up to the expression of traditional and discounted versions. However, model testing and synthesis algorithms have the LTL for probable time systems and games Kong.Lahska provocative root domains and their discounted version."}, {"heading": "4 Generating Specification MDPs", "text": "Similar to the GLTL formulas are also the GLTL formulas built from a row of atomic propositions (MDP). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2. (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2. (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). ("}, {"heading": "5 Example Domain", "text": "Consider the following formula: (\u00ac blueU\u00b5red) \u0442 (\u2666 \u00b5 (red \u0445 \u2666 \u00b5green).It indicates the task to achieve a red state without encountering a blue state, and, once a red state is reached, to switch to a green state. Figure 5 illustrates a grid world environment in which this task can be performed. It consists of differently colored grid cells. The agent can move to one of the four object cells at its current position with an action in the north, south, east, or west. However, selecting an action for one direction has a 0.02% probability of moving in one of the three other directions. This stochastic motion causes the agent to maintain its distance from dangerous grid cells, which could lead to a failure of the task whenever possible. The solid line in the figure outlines the path of the optimal policy to follow this specification in the grid. As can be seen, the agent moves in one of the other three directions."}, {"heading": "6 Conclusion", "text": "Unlike the standard MDP reward functions, we have provided an environment-independent specification for tasks. We have shown that this specification language can capture standard tasks used in the MDP community, and that it can be automatically integrated into an MDP environment to create a fixed MDP to be solved. Maximizing the reward in this resulting MDP maximizes the likelihood of meeting the task specification."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2005}, {"title": "Discounting in ltl", "author": ["Almagor", "Shaull", "Boker", "Udi", "Kupferman", "Orna"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Almagor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Almagor et al\\.", "year": 2014}, {"title": "Using local trajectory optimizers to speed up global optimization in dynamic programming", "author": ["Atkeson", "Christopher G"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Atkeson and G.,? \\Q1994\\E", "shortCiteRegEx": "Atkeson and G.", "year": 1994}, {"title": "Rewarding behaviors", "author": ["Bacchus", "Fahiem", "Boutilier", "Craig", "Grove", "Adam"], "venue": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence,", "citeRegEx": "Bacchus et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 1996}, {"title": "Principles of Model Checking", "author": ["Baier", "Christel", "Katoen", "Joost-Pieter"], "venue": null, "citeRegEx": "Baier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Baier et al\\.", "year": 2008}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["Barto", "Andrew G", "Sutton", "Richard S", "Anderson", "Charles W"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["Boutilier", "Craig", "Dean", "Thomas", "Hanks", "Steve"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Formal verification of probabilistic systems", "author": ["De Alfaro", "Luca"], "venue": "PhD thesis,", "citeRegEx": "Alfaro and Luca.,? \\Q1998\\E", "shortCiteRegEx": "Alfaro and Luca.", "year": 1998}, {"title": "Discounting the future in systems theory", "author": ["De Alfaro", "Luca", "Henzinger", "Thomas A", "Majumdar", "Rupak"], "venue": "In Automata, Languages and Programming,", "citeRegEx": "Alfaro et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Alfaro et al\\.", "year": 2003}, {"title": "Model checking discounted temporal properties", "author": ["De Alfaro", "Luca", "Faella", "Marco", "Henzinger", "Thomas A", "Majumdar", "Rupak", "Stoelinga", "Mari\u00eblle"], "venue": null, "citeRegEx": "Alfaro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Alfaro et al\\.", "year": 2004}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Ltl control in uncertain environments with probabilistic satisfaction", "author": ["Ding", "Xu Chu", "Smith", "Stephen L", "Belta", "Calin", "Rus", "Daniela"], "venue": "guarantees. CoRR,", "citeRegEx": "Ding et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2011}, {"title": "Efficient reinforcement learning", "author": ["Fiechter", "Claude-Nicolas"], "venue": "Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory,", "citeRegEx": "Fiechter and Claude.Nicolas.,? \\Q1994\\E", "shortCiteRegEx": "Fiechter and Claude.Nicolas.", "year": 1994}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael", "Singh", "Satinder"], "venue": "In Proceedings of the 15th International Conference on Machine Learning, pp", "citeRegEx": "Kearns et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1998}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael J", "Singh", "Satinder P"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Temporal-logic-based reactive mission and motion planning", "author": ["H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas"], "venue": "IEEE Tans. on Robotics,", "citeRegEx": "Kress.Gazit et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kress.Gazit et al\\.", "year": 2009}, {"title": "Correct, reactive robot control from abstraction and temporal logic specifications", "author": ["H. Kress-Gazit", "T. Wongpiromsarn", "U. Topcu"], "venue": "IEEE RAM,", "citeRegEx": "Kress.Gazit et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kress.Gazit et al\\.", "year": 2011}, {"title": "Prism: Probabilistic symbolic model checker", "author": ["Kwiatkowska", "Marta", "Norman", "Gethin", "Parker", "David"], "venue": "In Computer Performance Evaluation: Modelling Techniques and Tools,", "citeRegEx": "Kwiatkowska et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kwiatkowska et al\\.", "year": 2002}, {"title": "Control of Markov decision processes from PCTL specifications", "author": ["M. Lahijanian", "S.B. Andersson", "C. Belta"], "venue": "In Proc. of the American Control Conference,", "citeRegEx": "Lahijanian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lahijanian et al\\.", "year": 2011}, {"title": "Knows what it knows: A framework for self-aware learning", "author": ["Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J", "Strehl", "Alexander L"], "venue": "Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Synthesis of reactive switching protocols from temporal logic specifications", "author": ["Liu", "Jun", "Ozay", "Necmiye", "Topcu", "Ufuk", "Murray", "Richard M"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "The Temporal Logic of Reactive & Concurrent Sys", "author": ["Manna", "Zohar", "Pnueli", "Amir"], "venue": null, "citeRegEx": "Manna et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Manna et al\\.", "year": 1992}, {"title": "Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued spaces", "author": ["Moore", "Andrew W"], "venue": "In Proc. Eighth International Machine Learning Workshop,", "citeRegEx": "Moore and W.,? \\Q1991\\E", "shortCiteRegEx": "Moore and W.", "year": 1991}, {"title": "Markov Decision Processes\u2014 Discrete Stochastic Dynamic Programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q1994\\E", "shortCiteRegEx": "Puterman and L.", "year": 1994}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Russell", "Stuart J", "Norvig", "Peter"], "venue": null, "citeRegEx": "Russell et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Russell et al\\.", "year": 1994}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["Strehl", "Alexander L", "Li", "Lihong", "Littman", "Michael L"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "Christopher J.C. H"], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}, {"title": "Robust control of uncertain markov decision processes with temporal logic specifications", "author": ["Wolff", "Eric M", "Topcu", "Ufuk", "Murray", "Richard M"], "venue": "In Proc. of the IEEE Conference on Decision and Control,", "citeRegEx": "Wolff et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wolff et al\\.", "year": 2012}, {"title": "Receding horizon temporal logic planning", "author": ["T. Wongpiromsarn", "U. Topcu", "R.M. Murray"], "venue": "IEEE T. on Automatic Control,", "citeRegEx": "Wongpiromsarn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wongpiromsarn et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Linear temporal logic has been used in the past to specify reward functions that depend on temporal sequences (Bacchus et al., 1996); here, we expand the role to provide a robust and consistent semantics that allows desired behaviors to be specified in an environment-independent way.", "startOffset": 110, "endOffset": 132}, {"referenceID": 6, "context": "The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009).", "startOffset": 126, "endOffset": 186}, {"referenceID": 25, "context": "The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009).", "startOffset": 126, "endOffset": 186}, {"referenceID": 19, "context": "While provable guarantees of efficiency and optimality have been at the core of the literature on learning (Fiechter, 1994; Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Li et al., 2011), correctness with respect to complicated, high-level task specifications\u2014during the learning itself or in the behavior resulting from the learning phase\u2014 has attracted limited attention (Abbeel & Ng, 2005).", "startOffset": 107, "endOffset": 191}, {"referenceID": 1, "context": "Almagor et al. (2014) explicitly refine the \u201ceventually\u201d operator of LTL to a discounting operator such that the longer it takes to fulfill the task the smaller the value of satisfaction.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 15, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 20, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 27, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 11, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 18, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 16, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 17, "context": "While temporal logic had initially focused on reasoning about temporal and logical relations, its dialects with probabilistic modalities have been used increasingly for robotics applications (Baier & Katoen, 2008; De Alfaro, 1998; Kwiatkowska et al., 2002).", "startOffset": 191, "endOffset": 256}], "year": 2017, "abstractText": "We propose a new task-specification language for Markov decision processes that is designed to be an improvement over reward functions by being environment independent. The language is a variant of Linear Temporal Logic (LTL) that is extended to probabilistic specifications in a way that permits approximations to be learned in finite time. We provide several small environments that demonstrate the advantages of our geometric LTL (GLTL) language and illustrate how it can be used to specify standard reinforcementlearning tasks straightforwardly.", "creator": "LaTeX with hyperref package"}}}