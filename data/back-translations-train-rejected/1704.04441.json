{"id": "1704.04441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?", "abstract": "This paper investigates the robustness of NLP against perturbed word forms. While neural approaches can achieve (almost) human-like accuracy for certain tasks and conditions, they often are sensitive to small changes in the input such as non-canonical input (e.g., typos). Yet both stability and robustness are desired properties in applications involving user-generated content, and the more as humans easily cope with such noisy or adversary conditions. In this paper, we study the impact of noisy input. We consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions for training and testing. Moreover, we empirically evaluate the robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different NLP tasks (morphological tagging, machine translation).", "histories": [["v1", "Fri, 14 Apr 2017 14:43:44 GMT  (260kb,D)", "http://arxiv.org/abs/1704.04441v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["georg heigold", "g\\\"unter neumann", "josef van genabith"], "accepted": false, "id": "1704.04441"}, "pdf": {"name": "1704.04441.pdf", "metadata": {"source": "CRF", "title": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?", "authors": ["Georg Heigold", "G\u00fcnter Neumann", "Josef van Genabith"], "emails": ["georg.heigold@dfki.de", "neumann@dfki.de", "genabith@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Related Work", "text": "Much of the work on regularization techniques to learn robust representations and models exists. Examples are \"2-regularization, dropout (Hinton et al., 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noise. Compared to other fields of application such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and language (Lippmann et al., 1987; Tu \ufffd ske et al., 2014; Cui et al., 2015; Doulaty et al., 2016, work on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank et al.) and especially word noise (Yitong et al., 2017), do not have a long and extensive history in NLP."}, {"heading": "3 Noise Types", "text": "In this work, we are experimenting with three different types of sound: character swaps, character rotations, and word swaps were not total approximations of typos. Word swaps are motivated by psycholinguistic studies (Rawlinson, 1976). This choice of sound types allows us to automatically generate intoxicating text with different types and density distributions from existing correctly edited \"clean\" corpora. Using synthetic data is clearly suboptimal, but we are using synthetic data because of its ease of availability and because it gives us better control over the experimental characters. Character swaps These types of disturbances are randomly swapped between two adjacent characters in a word. Words are processed from left to right, but we are using synthetic data because of their ease of availability and because it gives us better control over the experimental characters. Character swaps of this kind of disturbances are likely to be swapped for two adjacent characters on the left of one word."}, {"heading": "4 Modeling", "text": "This section briefly summarizes the modeling approaches used in this paper. First, we address the choice of unit. As illustrated in Table 1 using an example from Corpus3, a word-based unit does not appear to be an appropriate unit in the presence of perturbations. Any change in the word form implies a different, independent word index. Worse, most perturbed word forms do not represent valid words and do not contain word-specific information, suggesting that we use BPE units (Sennrich et al., 2015) and characters like the basic units. BPE units are based on char-3http: / / dependencies.org / acter co-occurbter word units."}, {"heading": "5 Experiments", "text": "In this section we evaluate empirically the robustness to disturbed word forms (Section 3) for the two most common NLP tasks, morphological marking and machine translation."}, {"heading": "5.1 Morphological Tagging", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "I uesd my card to pchasure a mael on the mneu and the ttaol on my repciet was $", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "89.5 but wehn I went on line to chcek my", "text": "This year, it has come to the point where it is only a matter of time before a result is achieved."}, {"heading": "5.2 Machine Translation", "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they are moving."}, {"heading": "6 Conclusion", "text": "In fact, we will be able to assert ourselves, we will be able to change the world, and we will be able to change the world. \""}], "references": [{"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "CoRR abs/1608.04631.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1603.06147. http://arxiv.org/abs/1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Data augmentation for deep neural network acoustic modeling", "author": ["Xiaodong Cui", "Vaibhava Goel", "Brian Kingsbury."], "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing 23(9):1469\u20131477.", "citeRegEx": "Cui et al\\.,? 2015", "shortCiteRegEx": "Cui et al\\.", "year": 2015}, {"title": "Course in general linguistics", "author": ["Ferdinand de Saussure"], "venue": null, "citeRegEx": "Saussure.,? \\Q1916\\E", "shortCiteRegEx": "Saussure.", "year": 1916}, {"title": "Twitter part-of-speech tagging", "author": ["Leon Derczynski", "Alan Ritter", "Sam Clark", "Kalina Bontcheva"], "venue": null, "citeRegEx": "Derczynski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Derczynski et al\\.", "year": 2013}, {"title": "Automatic optimization of data perturbation distributions for multi-style training in speech recognition", "author": ["Mortaza Doulaty", "Richard Rose", "Olivier Siohan."], "venue": "Proceedings of the IEEE 2016 Workshop on Spoken Language Technology (SLT2016).", "citeRegEx": "Doulaty et al\\.,? 2016", "shortCiteRegEx": "Doulaty et al\\.", "year": 2016}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "CoRR abs/1512.00103.", "citeRegEx": "Gillick et al\\.,? 2015", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "author": ["Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathan Shlens", "Christian Szegedy."], "venue": "CoRR abs/1412.6572.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "An extensive empirical evaluation of character-based morphological tagging for 14 languages", "author": ["Georg Heigold", "G\u00fcnter Neumann", "Josef van Genabith."], "venue": "EACL.", "citeRegEx": "Heigold et al\\.,? 2017", "shortCiteRegEx": "Heigold et al\\.", "year": 2017}, {"title": "Character-based neural machine translation", "author": ["Georg Heigold", "Josef van Genabith."], "venue": "Technical report, DFKI GmbH.", "citeRegEx": "Heigold and Genabith.,? 2016", "shortCiteRegEx": "Heigold and Genabith.", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "CoRR abs/1207.0580. http://arxiv.org/abs/1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning. Morgan", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning robust representations of text", "author": ["Yitong Li", "Trevor Cohn", "Timothy Baldwin."], "venue": "CoRR abs/1609.06082. http://arxiv.org/abs/1609.06082.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the 2015", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-style training for robust isolated-word speech recognition", "author": ["Richard Lippmann", "Edward Martin", "D. Paul."], "venue": "ICASSP. volume 12, pages 705\u2013708.", "citeRegEx": "Lippmann et al\\.,? 1987", "shortCiteRegEx": "Lippmann et al\\.", "year": 1987}, {"title": "Robust morphological tagging with word representations", "author": ["Thomas M\u00fcller", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "M\u00fcller and Sch\u00fctze.,? 2015", "shortCiteRegEx": "M\u00fcller and Sch\u00fctze.", "year": 2015}, {"title": "What to do about nonstandard (or non-canonical) language in NLP", "author": ["Barbara Plank."], "venue": "CoRR abs/1608.07836. http://arxiv.org/abs/1608.07836.", "citeRegEx": "Plank.,? 2016", "shortCiteRegEx": "Plank.", "year": 2016}, {"title": "The significance of letter position in word recognition", "author": ["G.E. Rawlinson."], "venue": "Ph.D. thesis, Psychology Department, University of Nottingham, Nottingham UK. Unpublished Ph.D. Thesis.", "citeRegEx": "Rawlinson.,? 1976", "shortCiteRegEx": "Rawlinson.", "year": 1976}, {"title": "Raeding wrods with jubmled lettres there is a cost", "author": ["K. Rayner", "S.J. White", "R.L. Johnson", "S.P. Liversedge."], "venue": "Psychological Science 17(3):192\u2013 193.", "citeRegEx": "Rayner et al\\.,? 2006", "shortCiteRegEx": "Rayner et al\\.", "year": 2006}, {"title": "The manifold tangent classifier", "author": ["Salah Rifai", "Yann Dauphin", "Pascal Vincent", "Yoshua Bengio", "Xavier Muller."], "venue": "NIPS\u20192011. Student paper award.", "citeRegEx": "Rifai et al\\.,? 2011", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Recurrent neural networks are universal approximators", "author": ["Anton Maximilian Sch\u00e4fer", "Hans Georg Zimmermann."], "venue": "Proceedings of the 16th International Conference on Artificial Neural Networks Volume Part I. Springer-Verlag, Berlin, Heidelberg,", "citeRegEx": "Sch\u00e4fer and Zimmermann.,? 2006", "shortCiteRegEx": "Sch\u00e4fer and Zimmermann.", "year": 2006}, {"title": "How grammatical is characterlevel neural machine translation? Assessing MT quality with contrastive translation pairs", "author": ["Rico Sennrich."], "venue": "CoRR abs/1612.04629.", "citeRegEx": "Sennrich.,? 2016", "shortCiteRegEx": "Sennrich.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "CoRR abs/1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Data augmentation, feature combination, and multilingual neural networks to improve asr and kws performance for low-resource languages", "author": ["Zolt\u00e1n T\u00fcske", "Pavel Golik", "David Nolden", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "INTERSPEECH. pages", "citeRegEx": "T\u00fcske et al\\.,? 2014", "shortCiteRegEx": "T\u00fcske et al\\.", "year": 2014}, {"title": "Data Noising as Smoothing in Neural Network Language Models", "author": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "ArXiv:1703.02573v1.", "citeRegEx": "Xie et al\\.,? 2017", "shortCiteRegEx": "Xie et al\\.", "year": 2017}, {"title": "Robust training under linguistic adversity", "author": ["Li Yitong", "Trevor Cohn", "Timothy Baldwin."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017). Valencia, Spain, pages 21\u201327.", "citeRegEx": "Yitong et al\\.,? 2017", "shortCiteRegEx": "Yitong et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Different research groups have shown that NMT can generate natural and fluent translations (Bentivogli et al., 2016), achieving human-like performance in certain settings (Wu et al.", "startOffset": 91, "endOffset": 116}, {"referenceID": 22, "context": "It should be noted that neural networks with sufficient capacity, in particular recurrent neural networks, are universal function approximators (Sch\u00e4fer and Zimmermann, 2006).", "startOffset": 144, "endOffset": 174}, {"referenceID": 2, "context": "In particular, it can be expected that training on noisy data will make NLP more robust, as it was successfully demonstrated for other application domains including vision (Cui et al., 2015) and speech recognition (Doulaty et al.", "startOffset": 172, "endOffset": 190}, {"referenceID": 5, "context": ", 2015) and speech recognition (Doulaty et al., 2016).", "startOffset": 31, "endOffset": 53}, {"referenceID": 11, "context": "Examples include `2-regularization, dropout (Hinton et al., 2012), Jacobian-based sensitivity penalty (Rifai et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 21, "context": ", 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising.", "startOffset": 44, "endOffset": 81}, {"referenceID": 14, "context": ", 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising.", "startOffset": 44, "endOffset": 81}, {"referenceID": 13, "context": "Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al.", "startOffset": 53, "endOffset": 98}, {"referenceID": 8, "context": "Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al.", "startOffset": 53, "endOffset": 98}, {"referenceID": 16, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 25, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 2, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 5, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 7, "context": ", 2016), working on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al.", "startOffset": 31, "endOffset": 90}, {"referenceID": 4, "context": ", 2016), working on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al.", "startOffset": 31, "endOffset": 90}, {"referenceID": 18, "context": ", 2016), working on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al.", "startOffset": 31, "endOffset": 90}, {"referenceID": 27, "context": ", 2013; Plank, 2016) and in particular data noising (Yitong et al., 2017), do not have a long and extensive history in NLP.", "startOffset": 52, "endOffset": 73}, {"referenceID": 6, "context": "typos) have been proposed both on the byte-level (Gillick et al., 2015) and the word-level (Xie et al.", "startOffset": 49, "endOffset": 71}, {"referenceID": 26, "context": ", 2015) and the word-level (Xie et al., 2017).", "startOffset": 27, "endOffset": 45}, {"referenceID": 27, "context": "Syntactic and semantic noise for semantic analysis was studied in (Yitong et al., 2017).", "startOffset": 66, "endOffset": 87}, {"referenceID": 19, "context": "From a human perception perspective, word scrambling may be of interest (Rawlinson, 1976; Rayner et al., 2006).", "startOffset": 72, "endOffset": 110}, {"referenceID": 20, "context": "From a human perception perspective, word scrambling may be of interest (Rawlinson, 1976; Rayner et al., 2006).", "startOffset": 72, "endOffset": 110}, {"referenceID": 24, "context": "Examples for sub-word units include BPE based units (Sennrich et al., 2015), characters (Ling et al.", "startOffset": 52, "endOffset": 75}, {"referenceID": 15, "context": ", 2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 1, "context": ", 2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 9, "context": ", 2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 6, "context": ", 2017) or even bytes (Gillick et al., 2015).", "startOffset": 22, "endOffset": 44}, {"referenceID": 23, "context": "A comparison of BPE and characters for machine translation regarding grammaticality can be found in (Sennrich, 2016).", "startOffset": 100, "endOffset": 116}, {"referenceID": 19, "context": "Word scrambling is motivated from psycholinguistic studies (Rawlinson, 1976).", "startOffset": 59, "endOffset": 76}, {"referenceID": 24, "context": "Here, we use BPE units (Sennrich et al., 2015) and characters as the basic units.", "startOffset": 23, "endOffset": 46}, {"referenceID": 26, "context": ", noise modeling reduces to word-level label dropout (and rarely word-level label flips) (Xie et al., 2017).", "startOffset": 89, "endOffset": 107}, {"referenceID": 22, "context": "Deep neural networks are universal function approximators (Sch\u00e4fer and Zimmermann, 2006).", "startOffset": 58, "endOffset": 88}, {"referenceID": 12, "context": "We compare the neural networks with a conditional random field (Lafferty et al., 2001).", "startOffset": 63, "endOffset": 86}, {"referenceID": 9, "context": "We used the model configurations and setups from (Heigold et al., 2017) for the morphological tagging experiments in this paper.", "startOffset": 49, "endOffset": 71}, {"referenceID": 9, "context": "For this, we compare a char-LSTMBLSTM, a char-CNNHighway-BLSTM (same as char-LSTM-BLSTM but uses a convolutional neural network to compute the word vectors) (Heigold et al., 2017), and a conditional random field (M\u00fcller and Sch\u00fctze, 2015) including word-based features and prefix/suffix features up to length 10 for rare words (we used MarMoT6 for the experiments).", "startOffset": 157, "endOffset": 179}, {"referenceID": 17, "context": ", 2017), and a conditional random field (M\u00fcller and Sch\u00fctze, 2015) including word-based features and prefix/suffix features up to length 10 for rare words (we used MarMoT6 for the experiments).", "startOffset": 40, "endOffset": 66}], "year": 2017, "abstractText": "This paper investigates the robustness of NLP against perturbed word forms. While neural approaches can achieve (almost) human-like accuracy for certain tasks and conditions, they often are sensitive to small changes in the input such as non-canonical input (e.g., typos). Yet both stability and robustness are desired properties in applications involving user-generated content, and the more as humans easily cope with such noisy or adversary conditions. In this paper, we study the impact of noisy input. We consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions for training and testing. Moreover, we empirically evaluate the robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different NLP tasks (morphological tagging, machine translation).", "creator": "TeX"}}}