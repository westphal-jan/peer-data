{"id": "1704.04470", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Lean From Thy Neighbor: Stochastic & Adversarial Bandits in a Network", "abstract": "An individual's decisions are often guided by those of his or her peers, i.e., neighbors in a social network. Presumably, being privy to the experiences of others aids in learning and decision making, but how much advantage does an individual gain by observing her neighbors? Such problems make appearances in sociology and economics and, in this paper, we present a novel model to capture such decision-making processes and appeal to the classical multi-armed bandit framework to analyze it. Each individual, in addition to her own actions, can observe the actions and rewards obtained by her neighbors, and can use all of this information in order to minimize her own regret. We provide algorithms for this setting, both for stochastic and adversarial bandits, and show that their regret smoothly interpolates between the regret in the classical bandit setting and that of the full-information setting as a function of the neighbors' exploration. In the stochastic setting the additional information must simply be incorporated into the usual estimation of the rewards, while in the adversarial setting this is attained by constructing a new unbiased estimator for the rewards and appropriately bounding the amount of additional information provided by the neighbors. These algorithms are optimal up to log factors; despite the fact that the agents act independently and selfishly, this implies that it is an approximate Nash equilibria for all agents to use our algorithms. Further, we show via empirical simulations that our algorithms, often significantly, outperform existing algorithms that one could apply to this setting.", "histories": [["v1", "Fri, 14 Apr 2017 16:24:58 GMT  (905kb,D)", "http://arxiv.org/abs/1704.04470v1", "This article was first circulated in January 2015 and presented at ISMP 2015 under the title \"Bandit in a Network\" (this https URL&amp;mmnno=264&amp;ppnno=85856)"]], "COMMENTS": "This article was first circulated in January 2015 and presented at ISMP 2015 under the title \"Bandit in a Network\" (this https URL&amp;mmnno=264&amp;ppnno=85856)", "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["l elisa celis", "farnood salehi"], "accepted": false, "id": "1704.04470"}, "pdf": {"name": "1704.04470.pdf", "metadata": {"source": "CRF", "title": "Lean From Thy Neighbor: Stochastic & Adversarial Bandits in a Network", "authors": ["L. Elisa Celis", "Farnood Salehi"], "emails": ["elisa.celis@epfl.ch", "farnood.salehi@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "This year, it is time for us to set out to find the solution we are seeking."}, {"heading": "1.1 Summary of Our Results", "text": "In fact, most of them will be able to move to another world, in which they will be able to move to another world, in which they will be able to move, in which they will be able to move."}, {"heading": "2 Related Work", "text": "Spread across a wide range of issues that have come to the fore, the question is how to get a grip on people's needs, and how to get a grip on them."}, {"heading": "3 Technical Overview for the Stochastic Setting", "text": "To describe our algorithm, let us first verify the UCB algorithm first introduced by Auer et. al. [ACBF02] and since then widely extended and studied (see e.g. [Bub10, MMS11, GC11]). UCB is an asymptotically optimal algorithm for stochastic bandits with many well-studied variants (see e.g. [BCB12] for an overview. Indeed, the idea behind the algorithm uses the principle of optimism in the face of uncertainty; the algorithm maintains an optimistic upper limit for the mean reward of each arm and selects an arm with a maximum upper limit. As is standard, we assume that the probability distribution meets Hoeffding's predicament. Then, arm j has an upper limit Uj (t) = \u00b5 j (t) + upright (t) in time t."}, {"heading": "4 Technical Overview for the Adversarial Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Preliminaries", "text": "The multiplicative weight update method has been discovered in many areas in the last century (see [AHK12] for an overview). It is a simple but surprisingly effective method for conservatively updating beliefs about the benefits of a given arm and is extremely effective for opposing bandits and is asymptotically optimal for logging factors (see, for example, [ACBFS02, FKM05, ACBGM13]). Such complete information setting algorithms (where all rewards are observed at each time step) maintain a vector of weights wj for each arm j and update it at each time step (multiplicative): wj (t + 1) = wj (t) e\u03b4gj (t), where 0 \u2264 gj (t) \u2264 1 is the reward observed at each time, and vice versa the updating parameter wj (t). The probability of choosing the arm j at each time should be used is \u2264 (\u2264 j) \u2264 (\u2264 j) \u2264 (\u2264 j) \u2264 (\u2264 (see) \u2264 (t)."}, {"heading": "4.2 Formal Statement of Results", "text": "We call our algorithms in a way that makes it easier for us to interpret them than for us to take note of them all. (t) We remember that the probability that an individual person has a complete regret may be greater than the probability that his neighbor plays a role in the algorithm or in the main result. (t) We call the number of nodes in a network designated by N much greater, but the remaining network does not play a role in the algorithm or main result.Theorem 4.1 In the face of a person with b neighbors playing arbitrarily, the number of nodes in a network used by N is much greater, but the remaining network does not play a role in the algorithm. (T + T) lnK (3), where a person with b neighbors playing arbitrarily regrets when the EXPN algorithms are used. (t) Before discussing the evidence, we write the results in a way that makes it easier (t)."}, {"heading": "4.4 Proof of Theorem 4.1", "text": "While the above version of the algorithm provides a natural interpretation of the parameters to achieve the stronger regret bound in Theorem 4.1, we take a slightly different approach. Instead of decoupling the parameters of E (1 + 2), we instead use an adaptive analysis based on the amount of information obtained from the neighborhood. [5] In particular, we allow the first part of the evidence (from Equation 4.1) to lead to Equation (12). [5] Importantly, we use the same unbiased estimate in the eithermal version of the algorithms. [6] The first part of the evidence (from Equation 4.1) to Equation (12) adopts parallels to the traditional analysis of multiplicative weight algorithms; for completion, we perform the steps without going into the details (see [BCB12] for exposure."}, {"heading": "4.6 A Centralized Solution for the Network", "text": "Our model and algorithm are formulated for one person, because we can draw the most general conclusions from it - and limit the regret of the individual as a function of the behavior of the neighbors. However, a surprising feature is that it can also be transformed into a centralized solution. In general, this requires the assumption that there is an external coordinator who can select an individual of the highest degree to direct and direct the rest in the shortest way between them and v?, and who is designed to copy the probability distribution used by vu in the previous time.Theorem 4.6. Using the above centralized algorithm, the regret of all individuals is at mostR = O (\u0445 + \u221a nodes (1 + bmax) T lnK), (15) where bmax follows the degree v? and the dimensions of the network at the least delay."}, {"heading": "5 Empirical Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Adversarial Setting", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves in the world, and that we are able to assert ourselves in the world, that we are able to stay in the world, \"he said."}, {"heading": "5.2 Stochastic Setting", "text": "The setup for the empirical results in this section parallel that of Section 5.1. Recall that we do not have any assumption in our algorithm about our neighbors or how they play. We simply observe their actions and rewards. We leave \u03b1 = 2.5 in the UCBN algorithm; performance could be improved by optimizing \u03b1. We observe first that more neighbors lead to less regret (Figure 4 (f)). We then consider the regret of the UCBN on different network topologies on 10 corners: the complete network, a random 5-regular network and a star network (Figure 4 (c)). Similar to the previous experiments, for networks where all corners have the same number of neighbors (all except the star network), all agents achieve the same regret and therefore we report the average regret. However, this is not the case when the number of neighbors differ; the star is the extreme example and we represent the minimum (for the middle node)."}, {"heading": "6 Conclusion & Future Work", "text": "In this paper, we consider a model of social learning that places the problem within the bandit framework. This model allows us to analyze the problem in both the stochastic and the hostile bandit framework, and we provide algorithms for both cases. The regret of our algorithms interpolates between the regret of the traditional bandit framework (for example, if a person does not have neighbors) and the regret of the complete information framework (for example, if the number of neighbors goes infinitely), and is optimal for logging factors. We show, both theoretically and empirically, that we exceed the most advanced bandit algorithms that could be applied to this environment, and illustrate how our approach could also lead to centralized algorithms of interest.In terms of improvements to the social learning model, a loosening of the assumption (3) would be ideal. As we have shown (see Section 4.5), their complete elimination leads to strictly weaker regret limits, which would allow the adoption of the individual dignity to extend."}, {"heading": "A Adversarial Bandits", "text": "In this case, the regret of an agent using the EXPN algorithm is, in fact, R (O) (O) (O) (O) (as opposed to adaptive) parameters (O) (1 \u2212 K) and 1 \u2212 K (1 \u2212 K). In this case, the regret of an agent using the EXPN algorithm is, R (O) (O). (O). (O). (O). (O). (O). (O). (O). (O). (O). (K). (K). (O). (K). (O). (O). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K. (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (1 \u2212 K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). K). (K). (K). (K)."}, {"heading": "B Stochastic Bandits", "text": "In this section, we first formally state the results for the stochatic attitude and prove it. Let us first remember our result: Theorem B.1 (Theorem 3.1). Let us consider an agent with neighbors who plays arbitrarily. Let n \"i\" (t) be the number of qualities selected by one of his neighbors after time. (Then, the UCBN's regret for any agent (2) is the difference between \"i\" and \"i.\""}], "references": [{"title": "Online learning with feedback graphs: Beyond bandits", "author": ["N. Alon", "N. Cesa-Bianchi", "O. Dekel", "T. Koren"], "venue": "Conference on Learning Theory (COLT)", "citeRegEx": "ACBDK15", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, 47:235\u2013256", "citeRegEx": "ACBF02", "shortCiteRegEx": null, "year": 2002}, {"title": "The non-stochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing, 32:47\u201377", "citeRegEx": "ACBFS02", "shortCiteRegEx": null, "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing", "citeRegEx": "ACBFS03", "shortCiteRegEx": null, "year": 2003}, {"title": "Journal of Computer and System Sciences", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Claudio Gentile. Adaptive", "self-confident on-line learning algorithms"], "venue": "64(1):48\u201375,", "citeRegEx": "ACBG02", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour"], "venue": "Arxiv", "citeRegEx": "ACBG14", "shortCiteRegEx": null, "year": 2014}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "Proceedings of the 26th Conference on Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "ACBGM13", "shortCiteRegEx": null, "year": 2013}, {"title": "In COLT", "author": ["Shipra Agrawal", "Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem"], "venue": "pages 39\u20131,", "citeRegEx": "AG12", "shortCiteRegEx": null, "year": 2012}, {"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing, 8:121\u2013164", "citeRegEx": "AHK12", "shortCiteRegEx": null, "year": 2012}, {"title": "Budgeted prediction with expert advice", "author": ["Kareem Amin", "Satyen Kale", "Gerald Tesauro", "Deepak Turaga"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "AKTT15", "shortCiteRegEx": null, "year": 2015}, {"title": "Decoupling exploration and exploitation in multi-armed bandits", "author": ["O. Avener", "S. Mannor", "O. Shamir"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML)", "citeRegEx": "AMS12", "shortCiteRegEx": null, "year": 2012}, {"title": "Corruption driven by imitative behavior", "author": ["E. Accinelli", "J. S\u00e1nchez-Carrera"], "venue": "Econ. Letters, 117:84\u201387", "citeRegEx": "ASC12", "shortCiteRegEx": null, "year": 2012}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "volume 5. Foundations and Trends in Machine Learning", "citeRegEx": "BCB12", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-armed bandits in the presence of side observations in networks", "author": ["S. Buccapatnam", "A. Eryilmaz", "N.B. Shroff"], "venue": "Proceedings of the 2014 SIGMETRICS conference", "citeRegEx": "BES13", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic bandits with side observations on networks", "author": ["S. Buccapatnam", "A. Eryilmaz", "N.B. Shroff"], "venue": "Proceedings of the 52nd IEEE Conference on Decision and Control", "citeRegEx": "BES14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from neighbours", "author": ["V. Bala", "S. Goyal"], "venue": "Review of Econ. Studies, 65:595\u2013621", "citeRegEx": "BG98", "shortCiteRegEx": null, "year": 1998}, {"title": "Conformism and diversity under social learning", "author": ["V. Bala", "S. Goyal"], "venue": "Econ. Theory, 17:101\u2013120", "citeRegEx": "BG01", "shortCiteRegEx": null, "year": 2001}, {"title": "Bandits games and clustering foundations", "author": ["S. Bubeck"], "venue": "PhD thesis, Universite Lille", "citeRegEx": "Bub10", "shortCiteRegEx": null, "year": 2010}, {"title": "A gang of bandits", "author": ["N. Cesa-Bianchi", "C. Gentile", "G. Zappella"], "venue": "Proceedings of the 26th Conference on Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "CBGZ13", "shortCiteRegEx": null, "year": 2013}, {"title": "Leveraging side observations in stochastic bandits", "author": ["S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat"], "venue": "Proceedings of Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "CKLB12", "shortCiteRegEx": null, "year": 2012}, {"title": "Rules of thumb for social learning", "author": ["G. Ellison", "D. Fudenberg"], "venue": "J. of Political Economy, 101", "citeRegEx": "EF93", "shortCiteRegEx": null, "year": 1993}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "Proceedings of the 16th ACM/SIAM symposium on Discrete algorithms (SODA)", "citeRegEx": "FKM05", "shortCiteRegEx": null, "year": 2005}, {"title": "The kl-ucb algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "Proceedings of the Conference on Learning Theory (COLT)", "citeRegEx": "GC11", "shortCiteRegEx": null, "year": 2011}, {"title": "Naive learning in social networks and the wisdom of crowds", "author": ["B. Golub", "M.O. Jackson"], "venue": "American Econ. Jourmal: MicroEcon., 2", "citeRegEx": "GJ10", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian learning in social networks", "author": ["D. Gale", "S. Kariv"], "venue": "Games and Econ. Behavior, 45", "citeRegEx": "GK03", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning in networks", "author": ["S. Goyal"], "venue": "G. Demange and M. Wooders, editors, Group formation in Econ.: networks, clubs, and coalitions, chapter 4, pages 122\u2013167. Cambridge University Press", "citeRegEx": "Goy05", "shortCiteRegEx": null, "year": 2005}, {"title": "Personal Influence", "author": ["E. Katz", "P. Lazersfeld"], "venue": "The Free Press", "citeRegEx": "KL55", "shortCiteRegEx": null, "year": 1955}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "Proceedings of the 27th Conference on Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "KNVM14", "shortCiteRegEx": null, "year": 2014}, {"title": "The People\u2019s Choice", "author": ["P. Lazarsfeld", "B. Berelson", "H. Gaudet"], "venue": "Columbia University Press", "citeRegEx": "LBG48", "shortCiteRegEx": null, "year": 1948}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6", "citeRegEx": "LR85", "shortCiteRegEx": null, "year": 1985}, {"title": "A finite-time analysis of multiarmed bandits problems with kullback-leibler divergences", "author": ["O.A. Maillard", "R. Munos", "G. Stoltz"], "venue": "Proceedings of the Conference on Learning Theory (COLT)", "citeRegEx": "MMS11", "shortCiteRegEx": null, "year": 2011}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), pages 684\u2013692", "citeRegEx": "MS11", "shortCiteRegEx": null, "year": 2011}, {"title": "Essays on Social Learning and Imitation", "author": ["B. Sanditov"], "venue": "PhD thesis, Universitaire Pers Maastricht", "citeRegEx": "San06", "shortCiteRegEx": null, "year": 2006}, {"title": "In 30th International Conference on Machine Learning (ICML 2013)", "author": ["Balazs Szorenyi", "R\u00f3bert Busa-Fekete", "Istv\u00e1n Heged\u00fcs", "R\u00f3bert Orm\u00e1ndi", "M\u00e1rk Jelasity", "Bal\u00e1zs K\u00e9gl. Gossip-based distributed stochastic bandit algorithms"], "venue": "volume 28, pages 19\u201327. Acm Press,", "citeRegEx": "SBFH13", "shortCiteRegEx": null, "year": 2013}, {"title": "Piecewise-stationary bandit problems with side observations", "author": ["J.Y. Yu", "S. Mannor"], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML)", "citeRegEx": "YM09", "shortCiteRegEx": null, "year": 2009}, {"title": "Individual and social learning in bio-technology adoption: The case of gm corn in the u.s", "author": ["D. Yoo"], "venue": "In Agricultural & Applied Econ. Association\u2019s Annual Meeting (AAEA),", "citeRegEx": "Yoo.,? \\Q2012\\E", "shortCiteRegEx": "Yoo.", "year": 2012}, {"title": "Expertise networks in online communities: Structures and algorithms", "author": ["J. Zhang", "M.S. Ackerman", "L. Adamic"], "venue": "Proceedings of the World Wide Web Conference (WWW)", "citeRegEx": "ZAA07", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": ", [AKTT15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": ", [ACBDK15]) an action-network model has been studied: Here, the actions form a network and the individual observes the rewards of the neighbors of the action she selects (as opposed to the rewards of the actions that her neighbors select).", "startOffset": 2, "endOffset": 11}, {"referenceID": 25, "context": "In the study of non-strategic learning on networks, individuals are connected via a network, and each individual has a finite set of actions with probabilistic rewards whose distributions depend on the state of the world (see [Goy05], Chapter 2 for a survey).", "startOffset": 226, "endOffset": 233}, {"referenceID": 34, "context": "Such models have been studied both for stochastic ([YM09]) and adversarial ([AMS12, AKTT15]) bandits.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "[ACBF02] and since widely extended and studied (see, e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": ", [BCB12] for an overview).", "startOffset": 2, "endOffset": 9}, {"referenceID": 12, "context": ", [BCB12] for a template), and can be found along with further discussion in Appendix B.", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": "1 Preliminaries The multiplicative weight update method has been discovered many times in many fields over the past century (see [AHK12] for an overview).", "startOffset": 129, "endOffset": 136}, {"referenceID": 2, "context": "This algorithm, also known as EXP3 [ACBFS02], achieves regret O( \u221a TK lnK), and is optimal up to log factors for the right choice of parameters (see [BCB12] for an exposition).", "startOffset": 35, "endOffset": 44}, {"referenceID": 12, "context": "This algorithm, also known as EXP3 [ACBFS02], achieves regret O( \u221a TK lnK), and is optimal up to log factors for the right choice of parameters (see [BCB12] for an exposition).", "startOffset": 149, "endOffset": 156}, {"referenceID": 1, "context": ", [ACBF02]), and is given in Appendix A.", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": "The first part of proof (from Equation (7) to Equation (12)) parallels the traditional analysis for analyzing multiplicative weight update algorithms; for completeness we present the steps without going into the details (see [BCB12] for an exposition).", "startOffset": 225, "endOffset": 232}, {"referenceID": 4, "context": "5 of [ACBG02], and take expectation of the both sides of Equation (14).", "startOffset": 5, "endOffset": 13}, {"referenceID": 0, "context": "We consider EXP3G ([ACBDK15]), which is the state-of-the-art solution for such problems, and performed best amongst arm-network algorithms in our empirical simulations.", "startOffset": 19, "endOffset": 28}, {"referenceID": 5, "context": "The proof follows from Theorem 5 of [ACBG14].", "startOffset": 36, "endOffset": 44}, {"referenceID": 9, "context": "1 using the fact that the number of neighbors is b\u2212 1 on a complete network, and that a centralized solution has average regret \u03a9( \u221a( 1 + Kb ) T ) as shown in [AKTT15].", "startOffset": 159, "endOffset": 167}, {"referenceID": 0, "context": "We compare our algorithm against the bandit algorithms developed for various settings with sideinformation, namely EXP3G ([ACBDK15]), EXP.", "startOffset": 122, "endOffset": 131}, {"referenceID": 27, "context": "IX ([KNVM14]) and BEXP ([AKTT15]).", "startOffset": 4, "endOffset": 12}, {"referenceID": 9, "context": "IX ([KNVM14]) and BEXP ([AKTT15]).", "startOffset": 24, "endOffset": 32}, {"referenceID": 18, "context": "Finally, we compare our algorithm to the one proposed in [CBGZ13] (GOB.", "startOffset": 57, "endOffset": 65}], "year": 2017, "abstractText": "An individual\u2019s decisions are often guided by those of his or her peers, i.e., neighbors in a social network. Presumably, being privy to the experiences of others aids in learning and decision making, but how much advantage does an individual gain by observing her neighbors? Such problems make appearances in sociology and economics and, in this paper, we present a novel model to capture such decision-making processes and appeal to the classical multi-armed bandit framework to analyze it. Each individual, in addition to her own actions, can observe the actions and rewards obtained by her neighbors, and can use all of this information in order to minimize her own regret. We provide algorithms for this setting, both for stochastic and adversarial bandits, and show that their regret smoothly interpolates between the regret in the classical bandit setting and that of the full-information setting as a function of the neighbors\u2019 exploration. In the stochastic setting the additional information must simply be incorporated into the usual estimation of the rewards, while in the adversarial setting this is attained by constructing a new unbiased estimator for the rewards and appropriately bounding the amount of additional information provided by the neighbors. These algorithms are optimal up to log factors; despite the fact that the agents act independently and selfishly, this implies that it is an approximate Nash equilibria for all agents to use our algorithms. Further, we show via empirical simulations that our algorithms, often significantly, outperform existing algorithms that one could apply to this setting.", "creator": "LaTeX with hyperref package"}}}