{"id": "1704.04517", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "ShapeWorld - A new test methodology for multimodal language understanding", "abstract": "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter's specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require true generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating various visual question answering models on four different tasks, and show how our framework gives us detailed insights into their capabilities and limitations. By open-sourcing our framework, we hope to stimulate progress in the field of multimodal language understanding.", "histories": [["v1", "Fri, 14 Apr 2017 19:01:51 GMT  (264kb,D)", "http://arxiv.org/abs/1704.04517v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["alexander kuhnle", "ann copestake"], "accepted": false, "id": "1704.04517"}, "pdf": {"name": "1704.04517.pdf", "metadata": {"source": "CRF", "title": "SHAPEWORLD: A new test methodology for multimodal language understanding", "authors": ["Alexander Kuhnle", "Ann Copestake"], "emails": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they feel able to survive on their own. Most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, and most of them are able to survive on their own."}, {"heading": "2 Related work", "text": "In fact, it is so. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. It is. It is. (...) It is. It is. (...) It is. It is. It is. (...). It is. It is. It is. It is. It is. (...). It is. It is. It is. (...). It is. It is. It is. It is. (...). It is. It is. It is. It is. (...). It is. It is. It is."}, {"heading": "3 The SHAPEWORLD framework", "text": "The SHAPEWORLD Framework2 is based on micro-worlds - small and self-contained artificial scenarios - that guide the process of generating data. SHAPEWORLD micro-worlds consist only of colored shapes. This area of the closed world allows comprehensive coverage of the space of possible micro-worlds and associated captions. The vocabulary used focuses on closed-class words - the open-class vocabulary is currently far less than 100 words. In the following, we explain the details of the data generation process within the SHAPEWORLD Framework. A schematic representation of the process is shown in Figure 2."}, {"heading": "3.1 Image caption agreement task", "text": "The SHAPEWORLD code is written in Python 3 and is available on GitHub (https: / / github.com / AlexKuhnle / ShapeWorld). However, the generated data is returned as NumPy arrays, so it is possible to integrate them into Python-based deep learning projects that use common frameworks such as TensorFlow, Theano, etc. In our experiments, we use TensorFlow and we provide the models in this paper as part of the package. For the internal DMRS-based caption generation that uses Python package Pydmrs (Copestake et al., 2016), as well as a reduced version of the English resource Grammar (Flickinger, 2000) and Packard's Constraint Engine (image: leslingaguw./) is included in this paper."}, {"heading": "3.2 World and image generation", "text": "At the core of each micro-world instance is an abstract world model. Internal representation of a micro-world is simply a list of entities (datasets that contain their primary attributes such as position, shape, color, which are considered high-level semantic aspects reflected in captions. In addition, an entity has secondary attributes and methods that control, for example, visual appearance details, visual noise infusion, or collision-free placement of entities. Importantly, all of these possibilities of noise infusion can also be controlled, which is particularly useful as noise is often considered important for successful training of deep models. The generator module automatically generates a world model by sampling all of these attributes from a range of available values. Both values and other aspects of the generating process can be specified and adjusted accordingly for each dataset. Internal abstract representation is then used as the basis for extracting a concrete micro-world form consisting of image and caption."}, {"heading": "3.3 Caption generation", "text": "We offer an implementation of the SHAPEWORLD Captioner interface using a gram-based approach. Specifically, the dependence on Minimal Recursion Semantics (DMRS) (Copestake et al., 2016) is an abstract semantic representation designed for use with high-precision grammars as distributed by the DELPH-IN consortium. Semantic representation, like DMRS, is particularly suited to the SHAPEWORLD framework because it essentially mirrors the internal world model and therefore acts as a (partial) language-specific annotation. Here, noun the nodes correspond to the entities, adjective nodes append attributes and verb phrases to the relationships between entities. The semantics of words like \"square\" or \"red\" are interpreted as a subversion of matching entities."}, {"heading": "3.4 Training and testing on SHAPEWORLD datasets", "text": "Because SHAPEWORLD datasets are actually data generation processes, training and evaluation work differently from classical datasets. Where you normally have a fixed set of instances, models are trained and tested on a fixed set of higher generator configuration constraints. In particular, the evaluation constraints are different from training constraints and therefore require true generalization capabilities. For example, it is possible to disable and never generate a certain shape color combination, a certain number of objects, a spatial location, or a label type during training, so concepts need to be recombined at test dates. Thus, it is possible that a system achieves optimum performance during training, but fails completely during evaluation. Another important feature of SHAPEWORLD datasets, especially for future enhancements, is their compositionality. Instead of having to redefine a dataset from scratch each time, we can specialize atomic datasets downwards and then combine them in a hierarchical way that combines different aspects of the factual nature of the data set."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "In this paper, we look at four sets of data, each designed to examine one aspect of the ability to understand language in a multimodal setup. Figure 4 provides more information about these sets of data. Note that since we first capture a microworld model and then a caption, we cannot always easily control the generation process to capture every possible caption in a completely uniform way, especially when we focus on more specific captions that may not apply to a microworld and therefore require re-capture."}, {"heading": "4.2 Network architecture", "text": "We evaluate several multi-modal, deep neural network architectures recently proposed for VQA (Goyal et al., 2016; Agrawal et al., 2016; Jabri et al., 2016; Antol et al., 2015; Ren et al., 2015). Figure 5 shows the general architecture underlying all of these models. Implementations in TensorFlow, adapted to the ICA task, are included as part of GitHub repository4. Each model is trained end-to-end at the task, including the CNN module and word embedding, as opposed to using pre-formed, universal versions. We train for 5000 iterations 5 with a stack size of 128, using Adam optimization (Kingma and Ba, 2014) with the learning rate 0.001.4https: / github.com / AlexKuhnle / ShapeWorld5We tracked the validation performance based on the GRI hierarchy and the learning from the GRI hierarchy."}, {"heading": "4.3 Results", "text": "Figure 6 reports on the train6 / validation / test performance of four models. In addition to general accuracy, it includes a detailed analysis of the models \"ability to handle certain instance types. Accuracy for these dataset partitions6Note that training accuracy is an interesting measurement in itself, since no exact same instance is ever seen twice. It is achieved by restricting the dataset generator, a valuation set of only one instance type.Due to space limitations, we do not report detailed figures for the other models. Essentially, they all show the same (or worse) behavior as the LSTM-only model, except CNN + GRU: Mult, the CNN + LSTM: Multi. A number of conclusions from these results: \u2022 The consistently low performance (best: 60%) indicates that all models learn essentially no spatial relationships, consistent with the results of Johnson et al. ONONONONONONONONONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONIONION"}, {"heading": "4.4 Future work", "text": "The basic SHAPEWORLD framework can be worked out in many ways. We plan to add new datasets dedicated to other aspects of language, as well as include options to improve the language generation module, with the aim of providing more diverse and natural image descriptions. For example, we expect to include a subsequent step that applies paraphrase rules after captions are created - Copestake et al. (2016) describing how this can be done at the level of DMRS charts."}, {"heading": "5 Conclusion", "text": "With SHAPEWORLD, we have introduced a new evaluation method and a new framework for multimodal deep learning models, focusing on formal semantic generalization capabilities, in which artificial data is automatically generated according to predefined specifications. This controlled data generation enables previously invisible instance configurations to be introduced during evaluation, requiring the system to recombine learned concepts in a new way, i.e. true generalization. We have evaluated various VQA models based on four sets of image match data sets, where the system must decide whether a statement applies to an image. We have shown how the SHAPEWORLD framework can be used in detail to examine what these models learn in terms of multimodal language understanding. By showing specific multimodal scenarios in which current multimodal systems fail (e.g. spatial relationships), and by providing a multimodal, comparable, and detailed evaluation bed for advancement."}], "references": [{"title": "Analyzing the behavior of visual question answering models", "author": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, EMNLP 2016, pages", "citeRegEx": "Agrawal et al\\.,? 2016", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics. NAACL", "citeRegEx": "Andreas et al\\.,? 2016a", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2016.", "citeRegEx": "Andreas et al\\.,? 2016b", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE International Conference on Computer Vision. ICCV 2015.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP). Austin, Texas, USA.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling."], "venue": "Journal of Artificial Intelligence Research 47(1):253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Transactions on Neural Networks 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Recursive neural networks can learn logical semantics", "author": ["Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. Association for Compu-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "OpenAI Gym", "author": ["Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Brockman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brockman et al\\.", "year": 2016}, {"title": "Resources for building applications with Dependency Minimal Recursion Semantics", "author": ["Ann Copestake", "Guy Emerson", "Michael W. Goodman", "Matic Horvat", "Alexander Kuhnle", "Ewa Muszy\u0144ska."], "venue": "Proceedings of the 10th International", "citeRegEx": "Copestake et al\\.,? 2016", "shortCiteRegEx": "Copestake et al\\.", "year": 2016}, {"title": "Minimal Recursion Semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A. Sag."], "venue": "Research on Language and Computation 3(4):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "On building a more efficient grammar by exploiting types", "author": ["Dan Flickinger."], "venue": "Natural Language Engineering 6(1):15\u201328.", "citeRegEx": "Flickinger.,? 2000", "shortCiteRegEx": "Flickinger.", "year": 2000}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."], "venue": "Proceedings of the 2016 Conference on Empirical", "citeRegEx": "Fukui et al\\.,? 2016", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "LSTM recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber."], "venue": "Transactions on Neural Networks 12(6):1333\u20131340.", "citeRegEx": "Gers and Schmidhuber.,? 2001", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh."], "venue": "CoRR abs/1612.00837.", "citeRegEx": "Goyal et al\\.,? 2016", "shortCiteRegEx": "Goyal et al\\.", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV). Santiago,", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Focused evaluation for image description with binary forcedchoice tasks", "author": ["Micah Hodosh", "Julia Hockenmaier."], "venue": "Proceedings of the 5th Workshop on Vision and Language. Berlin, Germany.", "citeRegEx": "Hodosh and Hockenmaier.,? 2016", "shortCiteRegEx": "Hodosh and Hockenmaier.", "year": 2016}, {"title": "Revisiting Visual Question Answering Baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten"], "venue": null, "citeRegEx": "Jabri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross Girshick."], "venue": "Computer Vision and Pattern Recognition", "citeRegEx": "Johnson et al\\.,? 2017", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "The Malmo platform for artificial intelligence experimentation", "author": ["Matthew Johnson", "Katja Hofmann", "Tim Hutton", "David Bignell."], "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16). AAAI Press, Palo Alto,", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov."], "venue": "Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 190\u2013198.", "citeRegEx": "Joulin and Mikolov.,? 2015", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2015, pages 3128\u20133137.", "citeRegEx": "Karpathy and Li.,? 2015", "shortCiteRegEx": "Karpathy and Li.", "year": 2015}, {"title": "Virtual embodiment: A scalable long-term strategy for artificial intelligence research", "author": ["Douwe Kiela", "Luana Bulat", "Anita L. Vero", "Stephen Clark."], "venue": "CoRR abs/1610.07432.", "citeRegEx": "Kiela et al\\.,? 2016", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "A roadmap towards machine intelligence", "author": ["Tomas Mikolov", "Armand Joulin", "Marco Baroni."], "venue": "CoRR abs/1511.08130.", "citeRegEx": "Mikolov et al\\.,? 2015", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "English as a formal language", "author": ["Richard Montague."], "venue": "Bruno Visentini, editor, Linguaggi nella societa e nella tecnica, Edizioni di Communita, pages 188\u2013221.", "citeRegEx": "Montague.,? 1970", "shortCiteRegEx": "Montague.", "year": 1970}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associ-", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2015.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "The meaning of \u2019most\u2019: Semantics, numerosity and psychology", "author": ["Paul Pietroski", "Jeffrey Lidz", "Tim Hunter", "Justin Halberda."], "venue": "Mind and Language 24(5):554\u2013585.", "citeRegEx": "Pietroski et al\\.,? 2009", "shortCiteRegEx": "Pietroski et al\\.", "year": 2009}, {"title": "Discovering objects and their relations from entangled scene representations", "author": ["David Raposo", "Adam Santoro", "David Barrett", "Razvan Pascanu", "Timothy Lillicrap", "Peter W. Battaglia."], "venue": "International Conference on Learning Representations 2017 (ICLR", "citeRegEx": "Raposo et al\\.,? 2017", "shortCiteRegEx": "Raposo et al\\.", "year": 2017}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["Mengye Ren", "Ryan Kiros", "Richard S. Zemel."], "venue": "CoRR abs/1505.02074.", "citeRegEx": "Ren et al\\.,? 2015", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Look, some green circles!\u201d: Learning to quantify from images", "author": ["Ionut Sorodoc", "Angeliki Lazaridou", "Gemma Boleda", "Aur\u00e9lie Herbelot", "Sandro Pezzelle", "Raffaella Bernardi."], "venue": "Proceedings of the 5th Workshop on Vision and Language. Berlin,", "citeRegEx": "Sorodoc et al\\.,? 2016", "shortCiteRegEx": "Sorodoc et al\\.", "year": 2016}, {"title": "RNN Approaches to Text Normalization: A Challenge", "author": ["Richard Sproat", "Navdeep Jaitly."], "venue": "CoRR abs/1611.00068.", "citeRegEx": "Sproat and Jaitly.,? 2016", "shortCiteRegEx": "Sproat and Jaitly.", "year": 2016}, {"title": "MazeBase: A sandbox for learning from games", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Gabriel Synnaeve", "Soumith Chintala", "Rob Fergus."], "venue": "CoRR abs/1511.07401.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian J. Goodfellow", "Rob Fergus."], "venue": "CoRR abs/1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems. MIT Press, Montreal, Canada, NIPS\u201915, pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "CoRR abs/1410.4615.", "citeRegEx": "Zaremba and Sutskever.,? 2014", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals."], "venue": "International Conference on Learning Representations 2017 (ICLR 2017) .", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Yin and Yang: Balancing and answering binary visual questions", "author": ["Peng Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C. Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2013, pages 3009\u20133016.", "citeRegEx": "Zitnick and Parikh.,? 2013", "shortCiteRegEx": "Zitnick and Parikh.", "year": 2013}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C. Lawrence Zitnick", "Ramakrishna Vedantam", "Devi Parikh."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 38(4):627\u2013638.", "citeRegEx": "Zitnick et al\\.,? 2016", "shortCiteRegEx": "Zitnick et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (VQA) (Antol et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 3, "context": "Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (VQA) (Antol et al., 2015) can now be tackled with great success.", "startOffset": 108, "endOffset": 128}, {"referenceID": 36, "context": "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 29, "context": "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 40, "context": "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 15, "context": ", 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al., 2015).", "startOffset": 128, "endOffset": 145}, {"referenceID": 14, "context": "(Goyal et al., 2016; Agrawal et al., 2016; Zhang et al., 2016).", "startOffset": 0, "endOffset": 62}, {"referenceID": 0, "context": "(Goyal et al., 2016; Agrawal et al., 2016; Zhang et al., 2016).", "startOffset": 0, "endOffset": 62}, {"referenceID": 41, "context": "(Goyal et al., 2016; Agrawal et al., 2016; Zhang et al., 2016).", "startOffset": 0, "endOffset": 62}, {"referenceID": 16, "context": "For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001).", "startOffset": 97, "endOffset": 159}, {"referenceID": 13, "context": "For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001).", "startOffset": 97, "endOffset": 159}, {"referenceID": 38, "context": "We think of the SHAPEWORLD tasks as unit-testing multimodal systems for specific linguistic generalization capabilities, in a similar way to the bAbI tasks (Weston et al., 2015) for text-only understanding.", "startOffset": 156, "endOffset": 177}, {"referenceID": 23, "context": "gued to be a promising driver for artificial intelligence research (Kiela et al., 2016).", "startOffset": 67, "endOffset": 87}, {"referenceID": 5, "context": "Various platforms following this paradigm have been released, mostly aimed at reinforcement learning: the Arcade Learning Environment / Atari 2600 games (Bellemare et al., 2013), OpenAI Gym (Brockman", "startOffset": 153, "endOffset": 177}, {"referenceID": 20, "context": ", 2016), Project Malmo (Johnson et al., 2016), to name a few of the most popular.", "startOffset": 23, "endOffset": 45}, {"referenceID": 38, "context": "The bAbI tasks (Weston et al., 2015) are generated by internally simulating a short scene and extracting a few simple sentences from it.", "startOffset": 15, "endOffset": 36}, {"referenceID": 35, "context": "The MazeBase game environment (Sukhbaatar et al., 2015) uses language as a mean to represent the game world.", "startOffset": 30, "endOffset": 55}, {"referenceID": 27, "context": "A similar approach is taken by Narasimhan et al. (2015), but here the simulation is more complex, comprising a text-based role-playing game.", "startOffset": 31, "endOffset": 56}, {"referenceID": 26, "context": "The long-term research proposal of Mikolov et al. (2015) also simulates a world where", "startOffset": 35, "endOffset": 57}, {"referenceID": 21, "context": "Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al.", "startOffset": 79, "endOffset": 105}, {"referenceID": 37, "context": "Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al., 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others.", "startOffset": 130, "endOffset": 152}, {"referenceID": 39, "context": ", 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others.", "startOffset": 48, "endOffset": 77}, {"referenceID": 33, "context": "(2015) and Sorodoc et al. (2016) are more similar to our work in focusing on specific linguistic aspects.", "startOffset": 11, "endOffset": 33}, {"referenceID": 32, "context": "1 However, there have been experiments in which parts of the data are artificial and/or generated automatically, for instance, automatic question generation from annotation (Ren et al., 2015) or systematic modification of captions (Hodosh and Hockenmaier, 2016).", "startOffset": 173, "endOffset": 191}, {"referenceID": 17, "context": ", 2015) or systematic modification of captions (Hodosh and Hockenmaier, 2016).", "startOffset": 47, "endOffset": 77}, {"referenceID": 43, "context": "Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013) and to balance existing VQA datasets (Zhang et al.", "startOffset": 60, "endOffset": 108}, {"referenceID": 42, "context": "Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013) and to balance existing VQA datasets (Zhang et al.", "startOffset": 60, "endOffset": 108}, {"referenceID": 41, "context": ", 2016; Zitnick and Parikh, 2013) and to balance existing VQA datasets (Zhang et al., 2016).", "startOffset": 71, "endOffset": 91}, {"referenceID": 19, "context": "Most similar to the SHAPEWORLD framework is the CLEVR dataset (Johnson et al., 2017).", "startOffset": 62, "endOffset": 84}, {"referenceID": 27, "context": "In fact, our generation system closely resembles classical work in formal semantics, where a statement corresponds to a logical expression which can be evaluated against an abstract world model (Montague, 1970).", "startOffset": 194, "endOffset": 210}, {"referenceID": 10, "context": "We utilize semantic representations based on Minimal Recursion Semantics (Copestake et al., 2005) and broad-coverage, grammarbased realization driven by the English Resource Grammar (Flickinger, 2000) to make the internal world model compatible with language.", "startOffset": 73, "endOffset": 97}, {"referenceID": 11, "context": ", 2005) and broad-coverage, grammarbased realization driven by the English Resource Grammar (Flickinger, 2000) to make the internal world model compatible with language.", "startOffset": 92, "endOffset": 110}, {"referenceID": 14, "context": "It nevertheless presents the intended problems clearly, without any uncontrolled noise, biases or hidden correlations, which can obfuscate results when using real-world images and text (Goyal et al., 2016; Agrawal et al., 2016).", "startOffset": 185, "endOffset": 227}, {"referenceID": 0, "context": "It nevertheless presents the intended problems clearly, without any uncontrolled noise, biases or hidden correlations, which can obfuscate results when using real-world images and text (Goyal et al., 2016; Agrawal et al., 2016).", "startOffset": 185, "endOffset": 227}, {"referenceID": 9, "context": "For the internal DMRS-based caption generation, the Python package pydmrs (Copestake et al., 2016), as well as a reduced version of the English Resource Grammar (Flickinger, 2000) and of Packard\u2019s Answer Constraint Engine (http://sweaglesw.", "startOffset": 74, "endOffset": 98}, {"referenceID": 11, "context": ", 2016), as well as a reduced version of the English Resource Grammar (Flickinger, 2000) and of Packard\u2019s Answer Constraint Engine (http://sweaglesw.", "startOffset": 70, "endOffset": 88}, {"referenceID": 25, "context": ", yes/no questions), it neither requires the evaluated model to generate answers nor to rephrase the problem to fit it into a classification task of some sort \u2013 for instance, over the 1000/3000 most common answers, as is common practice recently (Lu et al., 2016; Fukui et al., 2016).", "startOffset": 246, "endOffset": 283}, {"referenceID": 12, "context": ", yes/no questions), it neither requires the evaluated model to generate answers nor to rephrase the problem to fit it into a classification task of some sort \u2013 for instance, over the 1000/3000 most common answers, as is common practice recently (Lu et al., 2016; Fukui et al., 2016).", "startOffset": 246, "endOffset": 283}, {"referenceID": 12, "context": ", 2016; Fukui et al., 2016). ICA most closely corresponds to the work of Jabri et al. (2016), who present VQA as a binary classification of image-question-answer triples.", "startOffset": 8, "endOffset": 93}, {"referenceID": 30, "context": ", quantifiers such as most (Pietroski et al., 2009).", "startOffset": 27, "endOffset": 51}, {"referenceID": 9, "context": "(Copestake et al., 2016) is an abstract semantic graph representation designed for use with highprecision grammars, such as those distributed by the DELPH-IN consortium.", "startOffset": 0, "endOffset": 24}, {"referenceID": 11, "context": "Although we currently use the English Resource Grammar (Flickinger, 2000), other DELPH-IN grammars use a compatible approach, so SHAPEWORLD can easily be ported to other languages.", "startOffset": 55, "endOffset": 73}, {"referenceID": 24, "context": "We train for 5000 iterations5 with a batch size of 128, using Adam optimization (Kingma and Ba, 2014) with learning rate 0.", "startOffset": 80, "endOffset": 101}, {"referenceID": 25, "context": "Finally, hierarchical co-attention (Lu et al., 2016) combines visual information on word-, phrase- and sentence-level with the language input, which is processed by a CNN.", "startOffset": 35, "endOffset": 52}, {"referenceID": 12, "context": "In the near future, we plan to also adapt the technique of multimodal compact bilinear pooling (Fukui et al., 2016), neural module networks (Andreas et al.", "startOffset": 95, "endOffset": 115}, {"referenceID": 31, "context": ", 2016a,b) and potentially also relation networks (Raposo et al., 2017) to the ICA task, and upload implementations to the GitHub repository.", "startOffset": 50, "endOffset": 71}, {"referenceID": 19, "context": "60%) indicates that all models essentially fail to learn spatial relations, in line with the findings of Johnson et al. (2017).7", "startOffset": 105, "endOffset": 127}, {"referenceID": 18, "context": "\u2022 Unsurprisingly, LSTM-only, CNN-only and also CNN+BoW:Mult are not able to learn actual multimodal understanding, in contrast to their good performance on real-world data (Jabri et al., 2016).", "startOffset": 172, "endOffset": 192}, {"referenceID": 9, "context": "For instance, we expect to integrate a subsequent step applying paraphrase rules after caption generation \u2013 Copestake et al. (2016) describe how this can be implemented on the level of DMRS graphs.", "startOffset": 108, "endOffset": 132}], "year": 2017, "abstractText": "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter\u2019s specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require true generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating various visual question answering models on four different tasks, and show how our framework gives us detailed insights into their capabilities and limitations. By opensourcing our framework, we hope to stimulate progress in the field of multimodal language understanding.", "creator": "LaTeX with hyperref package"}}}