{"id": "1704.04520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Neural Machine Translation Model with a Large Vocabulary Selected by Branching Entropy", "abstract": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because the training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In this paper, we propose to select phrases that contain out-of-vocabulary words using the statistical approach of branching entropy. This allows the proposed NMT system to be applied to a translation task of any language pair without any language-specific knowledge about technical term identification. The selected phrases are then replaced with tokens during training and post-translated by the phrase translation table of SMT. Evaluation on Japanese-to-Chinese and Chinese-to-Japanese patent sentence translation proved the effectiveness of phrases selected with branching entropy, where the proposed NMT system achieves a substantial improvement over a baseline NMT system without our proposed technique.", "histories": [["v1", "Fri, 14 Apr 2017 19:35:14 GMT  (1458kb,D)", "https://arxiv.org/abs/1704.04520v1", "EMNLP 2017, under review"], ["v2", "Sun, 14 May 2017 11:41:39 GMT  (1914kb,D)", "http://arxiv.org/abs/1704.04520v2", "EMNLP 2017 and MTSummit 2017, under review"], ["v3", "Tue, 6 Jun 2017 05:58:48 GMT  (1914kb,D)", "http://arxiv.org/abs/1704.04520v3", "EMNLP 2017 and MTSummit 2017, under review. arXiv admin note: text overlap witharXiv:1704.04521"], ["v4", "Thu, 20 Jul 2017 01:59:10 GMT  (5100kb,D)", "http://arxiv.org/abs/1704.04520v4", "ACL 2017 rejected"], ["v5", "Wed, 26 Jul 2017 08:26:54 GMT  (4759kb,D)", "http://arxiv.org/abs/1704.04520v5", "MT summit 2017 poster"], ["v6", "Wed, 6 Sep 2017 04:06:07 GMT  (4759kb,D)", "http://arxiv.org/abs/1704.04520v6", "MT summit 2017 poster"]], "COMMENTS": "EMNLP 2017, under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zi long", "ryuichiro kimura", "takehito utsuro", "tomoharu mitsuhashi", "mikio yamamoto"], "accepted": false, "id": "1704.04520"}, "pdf": {"name": "1704.04520.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation Model with a Large Vocabulary Selected by Branching Entropy", "authors": ["Zi Long", "Ryuichiro Kimura", "Takehito Utsuro", "Tomoharu Mitsuhashi", "Mikio Yamamoto"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they"}, {"heading": "2 Neural Machine Translation", "text": "NMT uses a single neural network that is jointly trained to maximize translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014). Given a source rate x = (x1,.., xN) and target rate y = (y1,.., yM), an NMT model uses a neural network to calculate and maximize the protocol probability of the target rate by aslog p (y | x) = 1 protocol p (yz | y < z, x) for 1 \u2264 z \u2264 M. In this paper, we use an NMT model similar to that of Bahdanau et al (2015), which consists of a hidden sentence, the 2014, the 2014, the 2014, and the 2014 short-term sentences; 1, the hidden sentences; 1, the hidden sentences; 5; 5)."}, {"heading": "3 Phrase Pair Selection using Branching Entropy", "text": "Branching entropy has been applied to the process of text segmentation (e.g. (Jin and TanakaIshii, 2006) and key text extraction (e.g. (Chen et al., 2010)). In this thesis, we use branching left-right entropy to detect the boundaries of phrases, and therefore automatically select phrase pairs.1 It is known that NMT models tend to have the problem of translation. Tu et al. (2016) proposed a cover-based NMT that takes into account the problem of translation."}, {"heading": "3.1 Branching Entropy", "text": "The left branching entropy and the right branching entropy of a phrase w are each defined as asHl (w) = \u2212 \u2211 v-Vwlpl (v) log2 pl (v) Hr (w) = \u2212 \u2211 v-Vwrpr (v) log2 pr (v), where w is the phrase of interest (e.g. \"/\" in the Japanese sentence in Figure 1, which means \"bridge interface\"), Vwl is a series of words to the left of w (e.g. \"\" in Figure 1, which is a Japanese particle) and Vwr is a set of words to the right of w (e.g. \"388\" in Figure 1), the probabilities pl (v) and pr (v) are each calculated as aspl (v) = fv, w fwpr (v) = fw, v fww is the frequency of phrase w, and f is a high frequency of w (v)."}, {"heading": "3.2 Selecting Phrase Pairs", "text": "In the case of a parallel sentence pair < Ss, St >, all n-gram phrases of the source sentence Ss and the target sentence St are extracted and aligned based on the phrase translation table and the word alignment of SMT according to the approaches described in Long et al. (2016). Next, the phrase translation pair < ts, tt > is selected from < Ss, St >, which meets and extracts all of the following conditions: (1) Either ts or tt contains at least one vocabulary outside the vocabulary. (2) Neither ts nor tt contains predetermined stopwords. (3) Entropies Hl (ts), Hl (tt), Hr (ts) and Hr (tt) are larger than a lower limit, while the left / right branched entropy of the substrings of ts and tt are lower or equal to the lower limit."}, {"heading": "4 NMT with a Large Phrase Vocabulary", "text": "In this thesis, the NMT model is trained on a bilingual corpus in which pairs of sentences are replaced by tokens, and the NMT system is then used as a decoder to translate the source sentences and replace the tokens with phrases that are translated using SMT."}, {"heading": "4.1 NMT Training after Replacing Phrase Pairs with Tokens", "text": "Figure 2 illustrates the method of building the parallel patent set model, in which phrase pairs are replaced by phrase pairs < T s1, T t1 >, < T s2, T t2 >, etc. Step 1 of Figure 2 selects source-target phrase pairs containing at least one vocabulary word using the branching entropy approach described in Section 3.2. As shown in Step 2 of Figure 2, in each of the parallel patent set pairs occurrences of phrase pairs < ts1, tt1 >, < ts2, tt2 >,.,. < tsk, ttk > are then replaced by token pairs < T s1, T t1 >, < T s2 > sentence. (., < T s2, T. \""}, {"heading": "4.2 NMT Decoding and SMT Phrase Translation", "text": "Figure 3 illustrates the procedure for creating target translations by decoding the input source set using the method proposed in this paper. In the first step of Figure 3, when we receive an input source set, we first extract its translation by decoding the SMT translation model. In the second step of Figure 3, we automatically extract the phrase pairs by branching the entropy according to the procedure of Section 3.2, treating the input sentence and its SMT translation as a pair of parallel sentences. Phrase pairs containing at least one foreign word are extracted and replaced by phrase pairs < T si, T ti > (i = 1, 2,...). As a result, we have an input set in which the tokens \"T si\" (i = 1, 2,...) represent the positions of the phrases and a list of SMT phrase translations, \"as in the next step to the Japanese sentence, as in the fourth step of the Mt."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Patent Documents", "text": "Japanese-Chinese parallel patent documents were collected from the Japanese patent documents published by the Japan Patent Office (JPO) in 2004-2012. From the documents collected, we automatically extracted 312,492 patent families between 2005 and 2010, and the method of Utiyama and Isahara (2007) was applied to the text of the extracted patent families to align the Japanese and Chinese sentences. Japanese sentences were divided into a sequence of morphemes with the Japanese morphological analyzer MeCab7 within 2014. 6 In this context, we used a Japanese-Chinese translation dictionary with approximately 170,000 Chinese entries. 7 http: / / mecab.sourceforge.net / the morpheme lexicon IPAdic, 8 and the Chinese sentences were published in a sequence of words with the Chinese morphological analyzer Stanford Word Segment."}, {"heading": "5.2 Training and Test Sets", "text": "Among the selected parallel sentence pairs, we randomly extracted 1,000 sentence pairs for the test set and 1,000 sentence pairs for the validation set; the remaining sentence pairs were used for the training set. Table 1 shows statistics of the data sets. In accordance with the procedure of Section 3.2, we collected 426,551 occurrences of Japanese-Chinese sentence pairs from the Japanese-Chinese sentence pairs of the training set, with 8 http: / / sourceforge.jp / projects / ipadic / 9 It is expected that the proposed NMT model will improve the baseline NMT without the proposed technique for translating longer sentences containing more than 40 morphemes / words. Because the approach of replacing sentences with tokens shortens the input types, shortens the input types of input types that are likely to contribute to solving the weakness of the NMT model when long sentences are translated."}, {"heading": "5.3 Training Details", "text": "To train the SMT model, including word alignment and phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for phrase-based SMT models. We trained the SMT model on the training set and optimized it with the validation set. To train the NMT model, our training procedures and hyperparameter selections were similar to Bahdanaus et al. (2015).The encoder consists of forward and backward-facing, deep neural LSTM networks, each consisting of three layers of 512 cells in each layer.The decoder is a three-layer, deep LSTM with 512 cells in each layer.Both the source vocabulary and the target vocabulary are limited to the most commonly used 40K morphemes / words in the training set, with 512 cells in each layer."}, {"heading": "5.4 Evaluation Results", "text": "This year, the number of people registered in the EU who are able to remain in the EU has multiplied, many times higher than in the EU."}, {"heading": "6 Conclusion", "text": "Compared to the method of Long et al. (2016), the contribution of the proposed NMT model is that it can be used for selecting technical terms on any language pair without language-specific knowledge. We observed that the proposed NMT model performs much better in all language pairs than the NMT base system: Japanese-Chinese / Chinese-Japanese and Japanese-English / English-Japanese. One of our important future tasks is to compare the translation performance of the proposed NMT model with that based on subword units (e.g. Sennrich et al. (2016)). Another future task is to improve the performance of the present study by including the non-compositional phrases in the vocabulary whose translation cannot be achieved by translating their components (e.g. Sennrich et al. (2016)))."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Proc. 3rd ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features", "author": ["Y. Chen", "Y. Huang", "S. Kong", "L. Lee"], "venue": "Proc. 2010 IEEE SLT Workshop, pages 265\u2013270.", "citeRegEx": "Chen et al\\.,? 2010", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merri\u00ebnboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proc. EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Character-based neural machine translation", "author": ["M.R. Costa-Juss\u00e0", "J.A.R. Fonollosa"], "venue": "Proc. 54th ACL, pages 357\u2013361.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa", "year": 2016}, {"title": "Toward the evaluation of machine translation using patent information", "author": ["A. Fujii", "M. Utiyama", "M. Yamamoto", "T. Utsuro"], "venue": "Proc. 8th AMTA, pages 97\u2013106.", "citeRegEx": "Fujii et al\\.,? 2008", "shortCiteRegEx": "Fujii et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "Y. Bengio", "R. Memisevic"], "venue": "Proc. 28th NIPS, pages 1\u201310.", "citeRegEx": "Jean et al\\.,? 2014", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Unsupervised segmentation of Chinese text by use of branching entropy", "author": ["Z. Jin", "K. Tanaka-Ishii"], "venue": "Proc. COLING/ACL 2006, pages 428\u2013435.", "citeRegEx": "Jin and Tanaka.Ishii,? 2006", "shortCiteRegEx": "Jin and Tanaka.Ishii", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "Proc. EMNLP, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proc. 45th ACL, Companion Volume, pages 177\u2013180.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Towards zero unknown word in neural machine translation", "author": ["X. Li", "J. Zhang", "C. Zong"], "venue": "Proc. 25th IJCAI, pages 2852\u20132858.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Translation of patent sentences with a large vocabulary of technical terms using neural machine translation", "author": ["Z. Long", "T. Utsuro", "T. Mitsuhashi", "M. Yamamoto"], "venue": "Proc. 3rd WAT, pages 47\u201357.", "citeRegEx": "Long et al\\.,? 2016", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Neural machine translation model with a large vocabulary selected by branching entropy", "author": ["Z. Long", "T. Utsuro", "T. Mitsuhashi", "M. Yamamoto"], "venue": "https://arxiv.org/ abs/1704.04520v4. Online; accessed 24-July-2017.", "citeRegEx": "Long et al\\.,? 2017", "shortCiteRegEx": "Long et al\\.", "year": 2017}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["M. Luong", "C.D. Manning"], "venue": "Proc. 54th ACL, pages 1054\u20131063.", "citeRegEx": "Luong and Manning,? 2016", "shortCiteRegEx": "Luong and Manning", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M. Luong", "H. Pham", "C.D. Manning"], "venue": "Proc. EMNLP, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["M. Luong", "I. Sutskever", "O. Vinyals", "Q.V. Le", "W. Zaremba"], "venue": "Proc. 53rd ACL, pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Overview of the 2nd workshop on Asian translation", "author": ["T. Nakazawa", "H. Mino", "I. Goto", "G. Neubig", "S. Kurohashi", "E. Sumita"], "venue": "Proc. 2nd WAT, pages 1\u201328.", "citeRegEx": "Nakazawa et al\\.,? 2015", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J."], "venue": "Proc. 40th ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "Proc. 54th ACL, pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural machine translation", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Proc. 27th NIPS, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A conditional random field word segmenter for Sighan bakeoff 2005", "author": ["H. Tseng", "P. Chang", "G. Andrew", "D. Jurafsky", "C. Manning"], "venue": "Proc. 4th SIGHAN Workshop on Chinese Language Processing, pages 168\u2013171.", "citeRegEx": "Tseng et al\\.,? 2005", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Modeling coverage for neural machine translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "Proc. ACL 2016, pages 76\u201385.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "A Japanese-English patent parallel corpus", "author": ["M. Utiyama", "H. Isahara"], "venue": "Proc. MT Summit XI, pages 475\u2013482.", "citeRegEx": "Utiyama and Isahara,? 2007", "shortCiteRegEx": "Utiyama and Isahara", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 2, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 6, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 8, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 19, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy.", "startOffset": 0, "endOffset": 157}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed replacing outof-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data.", "startOffset": 0, "endOffset": 431}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed replacing outof-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and out-of-vocabulary words as sequences of subword units.", "startOffset": 0, "endOffset": 587}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed replacing outof-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and out-of-vocabulary words as sequences of subword units. Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings.", "startOffset": 0, "endOffset": 727}, {"referenceID": 3, "context": "Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings.", "startOffset": 119, "endOffset": 152}, {"referenceID": 3, "context": "Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings. However, these previous approaches have limitations when translating patent sentences. This is because their methods only focus on addressing the problem of out-of-vocabulary words even though the words are parts of technical terms. It is obvious that a technical term should be considered as one word that comprises components that always have different meanings and translations when they are used alone. An example is shown in Figure 1, where the Japanese word \u201c \u201d(bridge) should be translated to Chinese word \u201c \u201d when included in technical term \u201cbridge interface\u201d; however, it is always translated as \u201c \u201d. To address this problem, Long et al. (2016) proposed extracting compound nouns as technical terms and replacing them with tokens.", "startOffset": 119, "endOffset": 867}, {"referenceID": 3, "context": "Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings. However, these previous approaches have limitations when translating patent sentences. This is because their methods only focus on addressing the problem of out-of-vocabulary words even though the words are parts of technical terms. It is obvious that a technical term should be considered as one word that comprises components that always have different meanings and translations when they are used alone. An example is shown in Figure 1, where the Japanese word \u201c \u201d(bridge) should be translated to Chinese word \u201c \u201d when included in technical term \u201cbridge interface\u201d; however, it is always translated as \u201c \u201d. To address this problem, Long et al. (2016) proposed extracting compound nouns as technical terms and replacing them with tokens. These compound nouns then are post-translated with the phrase translation table of the statistical machine translation (SMT) system. However, in their work on Japanese-to-Chinese patent translation, Japanese compound nouns are identified using several heuristic rules that use specific linguistic knowledge based on part-of-speech tags of morphological analysis of Japanese language, and thus, the NMT system has limited application to the translation task of other language pairs. In this paper, based on the approach of training an NMT model on a bilingual corpus wherein technical term pairs are replaced with tokens as in Long et al. (2016), we aim to select phrase pairs using the statistical approach of branching entropy; this allows the proposed technique to be applied to the translation task on any language pair without needing specific language knowledge to formulate the rules for technical term identification.", "startOffset": 119, "endOffset": 1598}, {"referenceID": 0, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 2, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 8, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 14, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 19, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 5, "context": "(2015), which consists of an encoder of a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and another LSTM as decoder.", "startOffset": 86, "endOffset": 120}, {"referenceID": 0, "context": "In this paper, we use an NMT model similar to that used by Bahdanau et al. (2015), which consists of an encoder of a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and another LSTM as decoder.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": "In this paper, we use an NMT model similar to that used by Bahdanau et al. (2015), which consists of an encoder of a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and another LSTM as decoder. In the model of Bahdanau et al. (2015), the encoder consists of forward and backward LSTMs.", "startOffset": 59, "endOffset": 264}, {"referenceID": 1, "context": ", (Chen et al., 2010)).", "startOffset": 2, "endOffset": 21}, {"referenceID": 21, "context": "Tu et al. (2016) proposed coverage-based NMT which considers the problem of the under-translation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "Given a parallel sentence pair \u3008Ss, St\u3009, all n-grams phrases of source sentence Ss and target sentence St are extracted and aligned using phrase translation table and word alignment of SMT according to the approaches described in Long et al. (2016). Next, phrase translation pair \u3008ts, tt\u3009 obtained from \u3008Ss, St\u3009 that satisfies all the following conditions is selected as a phrase pair and is extracted:", "startOffset": 230, "endOffset": 249}, {"referenceID": 14, "context": "2 One of the major focus of this paper is the comparison between the proposed method and Luong et al. (2015b). Since Luong et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 14, "context": "2 One of the major focus of this paper is the comparison between the proposed method and Luong et al. (2015b). Since Luong et al. (2015b) proposed to pre-process and post-translate only out-of-vocabulary words, we focus only on", "startOffset": 89, "endOffset": 138}, {"referenceID": 22, "context": "From the collected documents, we extracted 312,492 patent families, and the method of Utiyama and Isahara (2007) was applied6 to the text of the extracted patent families to align the Japanese and Chinese sentences.", "startOffset": 86, "endOffset": 113}, {"referenceID": 9, "context": "Baseline SMT (Koehn et al., 2007) 52.", "startOffset": 13, "endOffset": 33}, {"referenceID": 15, "context": "9 (Luong et al., 2015b) NMT with phrase translation by SMT (phrase 57.", "startOffset": 2, "endOffset": 23}, {"referenceID": 20, "context": "the morpheme lexicon IPAdic,8 and the Chinese sentences were segmented into a sequence of words using the Chinese morphological analyzer Stanford Word Segment (Tseng et al., 2005) trained using the Chinese Penn Treebank.", "startOffset": 159, "endOffset": 179}, {"referenceID": 4, "context": "9 Japanese-English patent documents are provided in the NTCIR-7 workshop (Fujii et al., 2008), which are collected from the 10 years of unexamined Japanese patent applications published by the Japanese Patent Office (JPO) and the 10 years patent grant data published by the U.", "startOffset": 73, "endOffset": 93}, {"referenceID": 4, "context": "9 Japanese-English patent documents are provided in the NTCIR-7 workshop (Fujii et al., 2008), which are collected from the 10 years of unexamined Japanese patent applications published by the Japanese Patent Office (JPO) and the 10 years patent grant data published by the U.S. Patent & Trademark Office (USPTO) in 1993-2000. The numbers of documents are approximately 3,500,000 for Japanese and 1,300,000 for English. From these document sets, patent families are automatically extracted and the fields of \u201cBackground of the Invention\u201d and \u201cDetailed Description of the Preferred Embodiments\u201d are selected. Then, the method of Utiyama and Isahara (2007) is applied to the text of those fields, and Japanese and English sentences are aligned.", "startOffset": 74, "endOffset": 655}, {"referenceID": 15, "context": "5 (Luong et al., 2015b) NMT with phrase translation by SMT (phrase 14.", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for phrase-based SMT models.", "startOffset": 112, "endOffset": 132}, {"referenceID": 0, "context": "For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Bahdanau et al. (2015). The encoder consists of forward and backward deep LSTM neural networks each consisting of three layers, with 512 cells in each layer.", "startOffset": 110, "endOffset": 133}, {"referenceID": 0, "context": "For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Bahdanau et al. (2015). The encoder consists of forward and backward deep LSTM neural networks each consisting of three layers, with 512 cells in each layer. The decoder is a three-layer deep LSTM with 512 cells in each layer. Both the source vocabulary and the target vocabulary are limited to the 40K most-frequently used morphemes / words in the training set. The size of the word embedding was set to 512. We ensured that all sentences in a minibatch were roughly the same length. Further training details are given below: (1) We set the size of a minibatch to 128. (2) All of the LSTM\u2019s parameter were initialized with a uniform distribution ranging between -0.06 and 0.06. (3) We used the stochastic gradient descent, beginning at a fixed learning rate of 1. We trained our model for a total of 10 epochs, and we began to halve the learning rate every epoch after the first seven epochs. (4) Similar to Sutskever et al. (2014), we rescaled the normalized gradient to ensure that its norm does not exceed 5.", "startOffset": 110, "endOffset": 1043}, {"referenceID": 17, "context": "In this work, we calculated automatic evaluation scores for the translation results using a popular metrics called BLEU (Papineni et al., 2002).", "startOffset": 120, "endOffset": 143}, {"referenceID": 9, "context": "As shown in Table 2, we report the evaluation scores, using the translations by Moses (Koehn et al., 2007) as the baseline SMT and the scores using the translations produced by the baseline NMT system without our proposed approach as the baseline NMT.", "startOffset": 86, "endOffset": 106}, {"referenceID": 9, "context": "Baseline SMT (Koehn et al., 2007) 3.", "startOffset": 13, "endOffset": 33}, {"referenceID": 15, "context": "2 (Luong et al., 2015b) NMT with phrase translation by SMT (phrase 4.", "startOffset": 2, "endOffset": 23}, {"referenceID": 12, "context": "Furthermore, we quantitatively compared our study with the work of Luong et al. (2015b). Table 2 compares the NMT model with the PosUnk model, which is the best model proposed by Luong et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 12, "context": "Furthermore, we quantitatively compared our study with the work of Luong et al. (2015b). Table 2 compares the NMT model with the PosUnk model, which is the best model proposed by Luong et al. (2015b). The proposed NMT model achieves performance gains of 0.", "startOffset": 67, "endOffset": 200}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al. (2017), when translating Japanese into Chinese, the BLEU of the NMT system of Long et al.", "startOffset": 44, "endOffset": 98}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al. (2017), when translating Japanese into Chinese, the BLEU of the NMT system of Long et al. (2016) in which all the selected compound nouns are replaced with tokens is 58.", "startOffset": 44, "endOffset": 188}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al. (2017), when translating Japanese into Chinese, the BLEU of the NMT system of Long et al. (2016) in which all the selected compound nouns are replaced with tokens is 58.6, the BLEU of the NMT system in which only compound nouns that contain out-of-vocabulary words are selected and replaced with tokens is 57.4, while the BLEU of the proposed NMT system of this paper is 57.7. Out of all the selected compound nouns of Long et al. (2016), around 22% contain out-of-vocabulary words, of which around 36% share substrings with the phrases selected by branching entropy.", "startOffset": 44, "endOffset": 529}, {"referenceID": 16, "context": "In this study, we also conducted two types of human evaluations according to the work of Nakazawa et al. (2015): pairwise evaluation and JPO adequacy evaluation.", "startOffset": 89, "endOffset": 112}, {"referenceID": 16, "context": "In contrast to the study conducted by Nakazawa et al. (2015), we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement.", "startOffset": 38, "endOffset": 61}, {"referenceID": 15, "context": "model (Luong et al., 2015b) in Table 5, the number of the untranslated words of the NMT model with PosUnk model is almost the same as that of the baseline NMT, which is much more than that of the proposed NMT model.", "startOffset": 6, "endOffset": 27}, {"referenceID": 11, "context": "Compared with the method of Long et al. (2016), the contribution of the proposed NMT model is that it can be used on any language pair without language-specific knowledge for technical terms selection.", "startOffset": 28, "endOffset": 47}, {"referenceID": 11, "context": "Compared with the method of Long et al. (2016), the contribution of the proposed NMT model is that it can be used on any language pair without language-specific knowledge for technical terms selection. We observed that the proposed NMT model performed much better than the baseline NMT system in all of the language pairs: Japanese-to-Chinese/Chineseto-Japanese and Japanese-to-English/English-to-Japanese. One of our important future tasks is to compare the translation performance of the proposed NMT model with that based on subword units (e.g. Sennrich et al. (2016)).", "startOffset": 28, "endOffset": 571}], "year": 2017, "abstractText": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because the training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In this paper, we propose to select phrases that contain out-of-vocabulary words using the statistical approach of branching entropy. This allows the proposed NMT system to be applied to a translation task of any language pair without any language-specific knowledge about technical term identification. The selected phrases are then replaced with tokens during training and post-translated by the phrase translation table of SMT. Evaluation on Japanese-to-Chinese, Chinese-to-Japanese, Japanese-to-English and English-to-Japanese patent sentence translation proved the effectiveness of phrases selected with branching entropy, where the proposed NMT model achieves a substantial improvement over a baseline NMT model without our proposed technique. Moreover, the number of translation errors of under-translation by the baseline NMT model without our proposed technique reduces to around half by the proposed NMT model.", "creator": "LaTeX with hyperref package"}}}