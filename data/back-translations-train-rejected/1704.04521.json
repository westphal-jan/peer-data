{"id": "1704.04521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation", "abstract": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In NMTs, words that are out of vocabulary are represented by a single unknown token. In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms. We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT. We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens. Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.", "histories": [["v1", "Fri, 14 Apr 2017 19:36:54 GMT  (2383kb,D)", "http://arxiv.org/abs/1704.04521v1", "WAT 2016. arXiv admin note: substantial text overlap witharXiv:1704.04520"]], "COMMENTS": "WAT 2016. arXiv admin note: substantial text overlap witharXiv:1704.04520", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zi long", "takehito utsuro", "tomoharu mitsuhashi", "mikio yamamoto"], "accepted": false, "id": "1704.04521"}, "pdf": {"name": "1704.04521.pdf", "metadata": {"source": "CRF", "title": "Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation", "authors": ["Zi Long", "Takehito Utsuro", "Tomoharu Mitsuhashi", "Mikio Yamamoto"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is in such a way that most of them will be able to move into another world, in which they are able, in which they are able to integrate themselves, and in which they are able, in which they are able, in which they are able, to change the world, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live."}, {"heading": "2 Japanese-Chinese Patent Documents", "text": "Japan-China parallel patent documents were collected from the Japanese patent documents published by the Japan Patent Office (JPO) in 2004-2012 and the Chinese patent documents published by the Office of Intellectual Property of the People's Republic of China (SIPO) in 2005-2010. From the documents collected, we extracted 312,492 patent families, and the method of Utiyama and Isahara (2007) was applied to the text of the extracted patent families to align the Japanese and Chinese sentences. Japanese sentences were segmented with the Japanese morphology analyst MeCab2 with the morphology lexicon IPAdic, 3 and Chinese sentences with the Chinese morphology analyst Stanford Word Segment (Tseng et al., 2005) into a sequence of morphemes segmented with the Japanese morphology analyst MeCab2 with the morphology lexicon IPAdic, 3 and Chinese sentences with the Chinese morphology analyst Stanford Word Segment (Tseng et, 2005)."}, {"heading": "3 Neural Machine Translation (NMT)", "text": "NMT uses a single neural network that has been jointly trained to maximize translation performance (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a). Given a source set x = (x1,.., xN) and target set y = (y1,.., yM), an NMT system uses a neural network to calculate and maximize the protocol probability of the target set, since the original word sentencelog p (y | x) = 1 protocol p (yl | y < l, x) for the proposed mechanism. (1) In this paper, we use an NMT model similar to the one used by the network to transmit two deep sentences each (2014)."}, {"heading": "4 NMT with a Large Technical Term Vocabulary", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 NMT Training after Replacing Technical Term Pairs with Tokens", "text": "< B > B > B > B > B > B > B > B > B > B > B > B > B > B > B > B > B > B, \"B,\" B, \"B,\" B, \"B,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, D, \"D,\" D, \", D,\" D, D, \"D, D, D,\" D, D, D, D, \"D, D, D,\" D, D, D, D, D, \"D, D, D, D, D,\" D, D, D, D, D, \"D, D, D, D,\" D, D, D, D, D, D, D, D, \"D, D, D, D, D, D, D, D, D, D, D, D, D, D\" D, D, D, D, D, D, D, D, D, D, D, D, D, D \"D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D\" D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D \"D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D"}, {"heading": "4.2 NMT Decoding and SMT Technical Term Translation", "text": "Figure 3 illustrates the procedure for creating Chinese translations by decoding the Japanese sentence using the method proposed in this thesis. In the first step of Figure 3, when we enter a Japanese sentence, we first automatically extracted the technical terms and replaced them with the technical terms \"TTi\" (i = 1, 2,...). Consequently, we have an input sentence in which the technical terms \"TTi\" (i = 1, 2,..) represent the positions of the technical terms and a list of the extracted Japanese technical terms. Next, as shown in step 2-N of Figure 3, the Japanese starting sentence is translated with technical terms using the procedure described in section 4.1, while the extracted Japanese technical terms are translated with an SMT phrase table in step 2-S of Figure 3.8. Finally, in step 3, we replace the technical term \"TTi\" (= 1, 2, the Japanese phrase is translated with an SMT-8."}, {"heading": "4.3 NMT Rescoring of 1,000-best SMT Translations", "text": "As shown in step 1 of Figure 4, similar to the approach of the NMT rescoring set out in Sutskever et al. (2014), we first obtain the 1,000 best translation lists of the given Japanese sentence using the SMT system. In the second step, we then replace the technical terms in the translation sentences with the technical terms \"TTi\" described in section 4.1 (i = 1, 2, 3,.), which must be identical with the tokens of their Japanese origin terms in the Japanese sentence entered. The technique for matching Japanese technical terms with their Chinese translations corresponds to the technique described in section 4.1. In step 3 of Figure 4, the 1,000 best translations in which technical terms are presented as tokens are rectified using the NMT model, which is trained according to the procedure described in section 4.1. In view of a Japanese sentence SJ and its 1,000 best Chinese translations, the 1,000 best translations S nC (n = 1, 2, point J > S < finally, the pair of the < S < S is translated with the highest S < S."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Training and Test Sets", "text": "We evaluated the effectiveness of the proposed NMT system in translating the Japanese-Chinese parallel patent sentences described in Section 2. From the 2.8M parallel sentence pairs of the test sentence, we randomly extracted 1,000 sentence pairs and 1,000 sentence pairs for the development sentence; the remaining sentence pairs were used for the training sentence. In accordance with the procedure in Section 4.1, we collected 6.5 million occurrences of technical terminology pairs from the total 6.5 million occurrences of technical terminology pairs, i.e. 1.3 million types of technical terminology pairs with 800K unique Japanese terms and 1.0M unique Chinese terms. Using the phrase translation table, the remaining 6.5 million occurrences of technical terminology pairs were replaced by technical terminology compositional translation approaches, in which the Chinese translation is generated compositively from the components of Japanese technical terms."}, {"heading": "5.2 Training Details", "text": "For training of the SMT model, including word alignment and phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT model. For training of the NMT model, our training procedures and hyperparameter selections were similar to those of Sutskever et al. (2014). We used a deep neural LSTM network consisting of three layers of 512 cells in each layer and a 512-dimensional word embedding. Similarly to Sutskever et al. (2014), we reversed the words in the source sentences and made sure that all sentences in a minibatch are roughly the same length. Further training details are given below: \u2022 All parameters of the LSTM were initialized with a uniform distribution between -0.06 and 0.06. \u2022 We set the size of a minibatch SMxT to 128. \u2022 We used the stocking chastisement starting with a learning scenario of 0.5."}, {"heading": "5.3 Evaluation Results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed an NMT method that can translate patent sentences with a large vocabulary of technical terms. We trained an NMT system based on a bilingual corpus, replacing technical terms with technical terms, which allows most source sentences to be translated outside of technical terms. Similar to Sutskever and al. (2014), we used it as a decoder to translate source sentences with technical terms and replace the tokens with technical terms translated with SMT. We also used it to replace the 1000 best SMT translations based on the average of SMT scores and NMT translation with technical terms. For the translation of Japanese patent judgments, we observed that our proposed NMT system does better than the phrase SMT system and the corresponding NMMT system without our proposed approach."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2015] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proc. 3rd ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merri\u00ebnboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proc. EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Character-based neural machine translation", "author": ["Costa-juss\u00e0", "Fonollosa2016] M.R. Costa-juss\u00e0", "J.A.R. Fonollosa"], "venue": "In Proc. 54th ACL,", "citeRegEx": "Costa.juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2016}, {"title": "Collecting bilingual technical terms from Japanese-Chinese patent families by SVM", "author": ["Dong et al.2015] L. Dong", "Z. Long", "T. Utsuro", "T. Mitsuhashi", "M. Yamamoto"], "venue": "In Proc. PACLING,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Automatic evaluation of translation quality for distant language pairs", "author": ["Isozaki et al.2010] H. Isozaki", "T. Hirao", "K. Duh", "K. Sudoh", "H. Tsukada"], "venue": "In Proc. EMNLP,", "citeRegEx": "Isozaki et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Isozaki et al\\.", "year": 2010}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2014] S. Jean", "K. Cho", "Y. Bengio", "R. Memisevic"], "venue": "In Proc. 28th NIPS,", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continous translation models", "author": ["Kalchbrenner", "Blunsom2013] N. Kalchbrenner", "P. Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "In Proc. 45th ACL, Companion Volume,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Towards zero unknown word in neural machine translation", "author": ["X. Li", "J. Zhang", "C. Zong"], "venue": "In Proc. 25th IJCAI,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Luong", "Manning2016] M. Luong", "C.D. Manning"], "venue": "In Proc. 54th ACL,", "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015a] M. Luong", "H. Pham", "C.D. Manning"], "venue": "In Proc. EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015b] M. Luong", "I. Sutskever", "O. Vinyals", "Q.V. Le", "W. Zaremba"], "venue": "In Proc. 53rd ACL,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Overview of the 2nd workshop on asian translation", "author": ["Nakazawa et al.2015] T. Nakazawa", "H. Mino", "I. Goto", "G. Neubig", "S. Kurohashi", "E. Sumita"], "venue": "In Proc. 2nd WAT,", "citeRegEx": "Nakazawa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Papineni et al.2002] K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In Proc. 40th ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["Sennrich et al.2016] R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proc. 54th ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural machine translation", "author": ["O. Vinyals", "Q.V. Le"], "venue": "In Proc. 28th NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A conditional random field word segmenter for Sighan bakeoff", "author": ["Tseng et al.2005] H. Tseng", "P. Chang", "G. Andrew", "D. Jurafsky", "C. Manning"], "venue": "In Proc. 4th SIGHAN Workshop on Chinese Language Processing,", "citeRegEx": "Tseng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "A Japanese-English patent parallel corpus", "author": ["Utiyama", "Isahara2007] M. Utiyama", "H. Isahara"], "venue": "In Proc. MT Summit XI,", "citeRegEx": "Utiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Utiyama et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 16, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 1, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 0, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 6, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system.", "startOffset": 8, "endOffset": 1076}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy.", "startOffset": 8, "endOffset": 1205}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data.", "startOffset": 8, "endOffset": 1456}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and unknown words as sequences of subword units.", "startOffset": 8, "endOffset": 1614}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and unknown words as sequences of subword units. Luong and Manning (2016) provided a character-level ar X iv :1 70 4.", "startOffset": 8, "endOffset": 1744}, {"referenceID": 16, "context": "We use an NMT model similar to that used by Sutskever et al. (2014), which uses a deep long short-term memories (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the input sentence and a separate deep LSTM to output the translation.", "startOffset": 44, "endOffset": 68}, {"referenceID": 16, "context": "We use an NMT model similar to that used by Sutskever et al. (2014), which uses a deep long short-term memories (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the input sentence and a separate deep LSTM to output the translation. We train the NMT model on a bilingual corpus in which the technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Similar to Sutskever et al. (2014), we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using statistical machine translation (SMT).", "startOffset": 44, "endOffset": 466}, {"referenceID": 17, "context": "The Japanese sentences were segmented into a sequence of morphemes using the Japanese morphological analyzer MeCab2 with the morpheme lexicon IPAdic,3 and the Chinese sentences were segmented into a sequence of words using the Chinese morphological analyzer Stanford Word Segment (Tseng et al., 2005) trained using the Chinese Penn Treebank.", "startOffset": 280, "endOffset": 300}, {"referenceID": 16, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 89, "endOffset": 207}, {"referenceID": 1, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 89, "endOffset": 207}, {"referenceID": 0, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 89, "endOffset": 207}, {"referenceID": 12, "context": "In this paper, we use an NMT model similar to that used by Sutskever et al. (2014). It uses two separate deep LSTMs to encode the input sequence and output the translation.", "startOffset": 59, "endOffset": 83}, {"referenceID": 12, "context": "In this paper, we use an NMT model similar to that used by Sutskever et al. (2014). It uses two separate deep LSTMs to encode the input sequence and output the translation. The encoder, which is implemented as a recurrent neural network, reads the source sentence one word at a time and then encodes it into a large vector that represents the entire source sentence. The decoder, another recurrent neural network, generates a translation on the basis of the encoded vector one word at a time. One important difference between our NMT model and the one used by Sutskever et al. (2014) is that we added an attention mechanism.", "startOffset": 59, "endOffset": 584}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015) proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences.", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015) proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences. Luong et al. (2015a) proposed an attention mechanism for different scoring functions in order to compare the source and target hidden states as well as different strategies for placing the attention.", "startOffset": 10, "endOffset": 163}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015) proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences. Luong et al. (2015a) proposed an attention mechanism for different scoring functions in order to compare the source and target hidden states as well as different strategies for placing the attention. In this paper, we utilize the attention mechanism proposed by Bahdanau et al. (2015), wherein each output target word is predicted on the basis of not only a recurrent hidden state and the previously predicted word but also a context vector computed as the weighted sum of the hidden states.", "startOffset": 10, "endOffset": 427}, {"referenceID": 3, "context": "According to the approach proposed by Dong et al. (2015), we identify Japanese-Chinese technical term pairs using an SMT phrase translation table.", "startOffset": 38, "endOffset": 57}, {"referenceID": 16, "context": "7 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 119, "endOffset": 237}, {"referenceID": 1, "context": "7 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 119, "endOffset": 237}, {"referenceID": 0, "context": "7 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 119, "endOffset": 237}, {"referenceID": 16, "context": "As shown in the step 1 of Figure 4, similar to the approach of NMT rescoring provided in Sutskever et al.(2014), we first obtain 1,000-best translation list of the given Japanese sentence using the SMT system.", "startOffset": 89, "endOffset": 112}, {"referenceID": 8, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT models.", "startOffset": 112, "endOffset": 132}, {"referenceID": 8, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT models. For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Sutskever et al. (2014). We used a deep LSTM neural network comprising three layers, with 512 cells in each layer, and a 512-dimensional word embedding.", "startOffset": 113, "endOffset": 309}, {"referenceID": 8, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT models. For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Sutskever et al. (2014). We used a deep LSTM neural network comprising three layers, with 512 cells in each layer, and a 512-dimensional word embedding. Similar to Sutskever et al. (2014), we reversed the words in the source sentences and ensure that all sentences in a minibatch are roughly the same length.", "startOffset": 113, "endOffset": 473}, {"referenceID": 16, "context": "\u2022 Similar to Sutskever et al. (2014), we rescaled the normalized gradient to ensure that its norm does not exceed 5.", "startOffset": 13, "endOffset": 37}, {"referenceID": 8, "context": "Baseline SMT (Koehn et al., 2007) 52.", "startOffset": 13, "endOffset": 33}, {"referenceID": 8, "context": "Baseline SMT (Koehn et al., 2007) 3.", "startOffset": 13, "endOffset": 33}, {"referenceID": 14, "context": "We calculated automatic evaluation scores for the translation results using two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al.", "startOffset": 102, "endOffset": 125}, {"referenceID": 5, "context": ", 2002) and RIBES (Isozaki et al., 2010).", "startOffset": 18, "endOffset": 40}, {"referenceID": 8, "context": "As shown in Table 1, we report the evaluation scores, on the basis of the translations by Moses (Koehn et al., 2007), as the baseline SMT11 and the scores based on translations produced by the equivalent NMT system without our proposed approach as the baseline NMT.", "startOffset": 96, "endOffset": 116}, {"referenceID": 13, "context": "In this study, we also conducted two types of human evaluation according to the work of Nakazawa et al. (2015): pairwise evaluation and JPO adequacy evaluation.", "startOffset": 88, "endOffset": 111}, {"referenceID": 13, "context": "(Nakazawa et al., 2015), we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement.", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": "Similar to Sutskever et al. (2014), we used it as a decoder to translate the source sentences with technical term tokens and replace the tokens with technical terms translated using SMT.", "startOffset": 11, "endOffset": 35}, {"referenceID": 0, "context": "One of our important future works is to evaluate our proposed method in the NMT system proposed by Bahdanau et al. (2015), which introduced a bidirectional recurrent neural network as encoder and is the state-of-the-art of pure NMT system recently.", "startOffset": 99, "endOffset": 122}, {"referenceID": 0, "context": "Our proposed NMT system is expected to improve the translation performance of patent sentences by applying approach of Bahdanau et al. (2015). Another important future work is to quantitatively compare our study with the work of Luong et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 0, "context": "Our proposed NMT system is expected to improve the translation performance of patent sentences by applying approach of Bahdanau et al. (2015). Another important future work is to quantitatively compare our study with the work of Luong et al. (2015b). In the work of Luong et al.", "startOffset": 119, "endOffset": 250}, {"referenceID": 0, "context": "Our proposed NMT system is expected to improve the translation performance of patent sentences by applying approach of Bahdanau et al. (2015). Another important future work is to quantitatively compare our study with the work of Luong et al. (2015b). In the work of Luong et al. (2015b), they replace low frequency single words and translate them in a post-processing Step using a dictionary, while we propose to replace the whole technical terms and post-translate them with phrase translation table of SMT system.", "startOffset": 119, "endOffset": 287}], "year": 2017, "abstractText": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In NMTs, words that are out of vocabulary are represented by a single unknown token. In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms. We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT. We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens. Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.", "creator": "LaTeX with hyperref package"}}}