{"id": "1704.04522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Hierarchic Kernel Recursive Least-Squares", "abstract": "We present a new hierarchic kernel based modeling technique for modeling evenly distributed multidimensional datasets that does not rely on input space sparsification. The presented method reorganizes the typical single-layer kernel based model in a hierarchical structure, such that the weights of a kernel model over each dimension are modeled over the adjacent dimension. We show that the imposition of the hierarchical structure in the kernel based model leads to significant computational speedup and improved modeling accuracy (over an order of magnitude in many cases). For instance the presented method is about five times faster and more accurate than Sparsified Kernel Recursive Least- Squares in modeling of a two-dimensional real-world data set.", "histories": [["v1", "Fri, 14 Apr 2017 19:43:47 GMT  (2720kb,D)", "http://arxiv.org/abs/1704.04522v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hossein mohamadipanah", "girish chowdhary"], "accepted": false, "id": "1704.04522"}, "pdf": {"name": "1704.04522.pdf", "metadata": {"source": "CRF", "title": "Hierarchic Kernel Recursive Least-Squares", "authors": ["Hossein Mohamadipanah", "Girish Chowdhary"], "emails": ["hmohamadipan@wisc.edu", "girishc@illinois.edu"], "sections": [{"heading": null, "text": "Hierarchic Kernel Recursive Least-SquaresHossein Mohamadipanah University of Wisconsin Madison, WI 53792 E-Mail: hmohamadipan @ wisc.eduGirish Chowdhary University of Illinois at Urbana ChampaignUrbana, IL 61801 E-Mail: girishc @ illinois.eduWe present a new hierarchical core-based modeling technology for modeling uniformly distributed multidimensional datasets that does not rely on input space savings. The presented method reorganizes the typical single-layer core-based model into a hierarchical structure so that the weights of a core model are modeled over each dimension across the adjacent dimension. We show that the imposition of the hierarchical structure in the core-based model results in significant computational acceleration and improved modeling accuracy (in many cases by an order of magnitude)."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Hierarchic Kernel Recursive Least-Squares", "text": "We start in Section 2.1 with an overview of the KRLS method. Afterwards, our main contribution is presented in Section 2.2 and its computational efficiency is discussed in Section 2.3."}, {"heading": "2.1 Preliminaries", "text": "Consider a set of recorded input / output pairs (z1, y1), (z2, y2),.., (zs, ys) where the input is zs-X, for a certain space X and ys-R. By definition, a kernel k (z, z) takes two arguments z and z \"and maps them to real values (X \u00b7 X \u2192 R). Therefore, this paper assumes that the grammar matrix [K] i = k (zi, z j) is a symmetrical positive semidefined core. Thus, the grammar matrix [K] i = k (z, j) is a symmetrical positive semidefined core."}, {"heading": "2.2 Hierarchic Kernel Recursive Least-Squares", "text": "This subsection explains the details of the presented algorithm when modeling a two- or three-dimensional problem in sections 2.2.1 and 2.2.2 respectively. Finally, the algorithm for high-dimensional problems is generalized in 2.2.3."}, {"heading": "2.2.1 2D Hierarchic Kernel Recursive Least-Squares", "text": "Suppose that the intention is to model a function with a two-dimensional input denoted by x and d1 and a one-dimensional deviation; the output of this function is a function of the inputs x and d1, f (x, d1), and the goal of the modeling is to find this function (Figure 2); in the first step, the modeling is recorded on all inputs x and d1, f (x, d1) and the corresponding weight \u03c11; then the modeling is performed on all inputs of x on the next sample of d1; this process continues until the last sample of d1, illustrated in Figure 3. After recording from 1 to 2 m1, we can merge these vectors and obtain a matrix P as (2). P = [[[1] m0 x] m0 [2] m0 x."}, {"heading": "2.2.2 3D Hierarchic Kernel Recursive Least-Squares", "text": "Consider a function with a three-dimensional input referred to as x, d1, and d2, and a dimensional output referred to as y. the output is a function of the inputs x, d1, and d2, and the object of the modeling is to find the function f (x, d1, d2). The modeling is performed on the first sample of d1 and the corresponding weight class 2.1. The modeling is recorded up to the last sample of d1 and the corresponding weight class m1.1, with the dimension m0 \u00d7 1. Then the modeling is performed on all samples of x at the next sample of d1 and the corresponding weight class 2.1. This model is continued until the last sample of d1 and the recording of the weight class m1,1 is repeated. This process is repeated for all samples of d2, illustrated in Figure 5, and the corresponding weight classes are recorded."}, {"heading": "2.2.3 General Hierarchic Kernel Recursive LeastSquares", "text": "In this part, we expand on the idea presented for modeling higher dimensional datasets. The goal of modeling is to find the function f (x, d1, d2,.), in which the inputs represented by x, d1, d2 to dn.Figure 9 are the main structure of a general form of the Hierarchical Kernel Recursive Least Squares (H-KRLS) method. Consider a function with n + 1 dimensional input, represented by x, d2,.., and with mk (k = 0.1,.., n) samples in each dimension. The key idea is to perform training in several hierarchical steps. The first step is called Initial Modeling, in which the modeling is recorded via the first dimension (x) and the corresponding weights."}, {"heading": "2.3 Computational Efficiency of the H-KRLS Method", "text": "Although the approach in Section 2.2 may seem complicated at first glance, it is actually much more efficient than the KRLS method, because the H-KRLS algorithm divides the training procedure into several steps, as shown in Figure 9, and uses smaller kernel matrices instead of using a large kernel matrix. Overall, the calculation costs of the H-KRLS algorithm for an n + 1 dimensional dataset are O (mnmn \u2212 1.. m1 (m0) 2 + mnmn \u2212 1. mnmH (mn) 2 + \u00b7 mn (mn) 2 + (mn) 2 + (mn) 2) 2 + (mn) 2), which is significantly less than the cost of the KRLS method, O (((m0m1. mn)."}, {"heading": "3 Numerical Experiments", "text": "The H-KRLS method is illustrated by two synthetic and two real data sets in sections 3.1 to 3.4, followed by a discussion of the cross-correlation between space and time in Section 3.5, and finally, in Section 3.6, the performance of the H-KRLS method is compared with the literature. It should be noted that all algorithms on an Intel (R) core (T M) i7 \u2212 4700MQCPU @ 2.40GHz with 8GB of RAM and the Gaussian kernel have been implemented, with k (z, z \u2032) = exp (\u2212 (z \u2212 z \u2032) T (z \u2212 z \u2032) / (2\u03c32)) being used to execute all of the algorithms contained therein."}, {"heading": "3.1 Synthetic 2D Data Modeling", "text": "The two-dimensional nonlinear function \u0432 (x, d) is given by the following numbers: \u0432 (x, d) = sin (x) cos (d 2), (23) where x and d are arranged in such a way that they have 145 and 150 uniformly divided numbers between [0,1,4\u03c0] and [0,1,8\u03c0], respectively, while the trigonometric functions are in radians. Consequently, the presence of 145 and 150 data points in each of the two directions results in the total number of data points 21750. For training and validation, this data set is randomly divided with 80% of the data used for training and 20% for validation in each dimension. For training, there are 116 points in the x direction and 120 points in the d direction and a total of 13,920 data points."}, {"heading": "3.2 Synthetic 3D Data Modeling", "text": "In order to demonstrate the ability of the H-KRLS algorithm to model higher-dimensional datasets, the presented algorithm is implemented in this subsection to a synthetic three-dimensional function. A three-dimensional function is defined as follows: \u2022 (x, d1, d2) = cos (x) sin (d1 2) sin (d2 3), (24), where x, d1 and d2 are arranged to represent 145, 150 and 100 uniformly distributed numbers in the range between [0,1,4\u03c0], [0,1,8\u03c0] and [0,1,12\u03c0] sin (24), respectively, while the trigonometric functions are in radians. Consequently, the total number of data points is equal to 2,175,000, (145 \u00d7 150 \u00d7 100). To train and validate this dataset, it is randomly divided with 80% for education and 20% for validation."}, {"heading": "3.3 Temperature Modeling on Intel Lab Dataset", "text": "In this subsection, the presented algorithm is illustrated using a realistic two-dimensional spatio-temporal environment [13] in which 54 sensors have been arranged in the Intel laboratory and the temperature is recorded. It is assumed that the sensor indices represent the position of the sensors in space x and every 30 seconds represent the time step over time, i.e. time d is arranged from 301 seconds to 400 seconds (with a second interval) and 52 sensors are used (sensor numbers 5 and 15 are not used to reduce outliers). Thus, having 100 and 52 data points in each direction results in a total number of data points of 5200. To reduce the outliers, the data set is filtered through a 2D Gaussian filter of variance 5 and size 6 x 6. To train and validate this data set, this data set is randomly divided by 80% for training and 20% for validation over time."}, {"heading": "3.4 Plant Growth Modeling from Sequence of Images", "text": "In this subsection, the H-KRLS algorithm is used in modeling the growth of plants in polyculture, which was a practical motivation in developing this algorithm. On polycultural farms, different types of plants are planted side by side, and due to their interactions, their growth rate is unknown and must be modeled. To perform the modeling for such a high spatial and temporal scale, data is first collected from a camera located in the polycultural field, which consists of different types of plants in parallel lanes. The camera is designed to capture every single day at the same time (RGB). Images taken on day 50 are shown in Figure 12 (top left). Fifty days are used for modeling in this experiment. In order to identify and distinguish plants from grasses, the expectation maximization (EM) and the segmentation method based on color must be implemented."}, {"heading": "3.5 Space-Time Cross-Correlation", "text": "This subsection emphasizes the importance of cross-correlations between dimensions. In the presented algorithm, all possible correlations between data points from different dimensions (e.g. space and time) are taken into account and no assumed predefined function is used in the modeling of these correlations. In contrast, cross-correlations between space and time have been modeled in the literature by providing different space-time covariance functions [21], such as: C (u, h) = 1 (a \u2032 2u2 + 1) exp (\u2212 b \u2032 2h2a \u2032 2u2 + 1), (25), in which one \u2032 and b \u2032 scaling parameters of time and space respectively are scaled. We considered C as the input to KRLS, called the NONSTILL-KRLS method, inspired by the NUD-GP."}, {"heading": "3.6 Summary of Comparison with Existing Methods", "text": "The main comparison of H-KRLS with leading existing kernel-based modeling methods in the literature is presented in this paragraph. Table 2 presents the comparison in terms of computation time and maximum validation error of the presented algorithm with the investigated kernel adaptive filter algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4] and NORMA [5] (the codes used here are modified versions of it in [23]). As detailed in Table 2, the H-KRLS method resulted in less computing time and also in a lower maximum validation error compared to the other methods in the literature. For example, it is about five times faster and more accurate than S-KRLS in modeling Intel Lab datasets. The coefficients used in executing the algorithms are presented in Table 3 in Table 3."}, {"heading": "4 Conclusion", "text": "We presented a hierarchical kernel method for modeling evenly distributed multidimensional datasets, which was compared with a number of leading kernel least squares algorithms and showed that it performs better in terms of both modeling accuracy and computational requirements. Although the proposed batch method was developed for the specific type of datasets that are multidimensional and require periodic sampling across all dimensions, the proposed method offers a different perspective that may lead to new techniques for scaling kernel-based models. Finally, future work suggests systematic tuning of kernel parameters and investigating error propagation in modeling high-dimensional datasets."}, {"heading": "Acknowledgements", "text": "We thank Chinmay Soman, Kevin Wolz and Evan DeLucia for providing data from the site Woody Perennial Polyculture at the University of Illinois Urbana-Champagne and Hassan Kingravi with Pindrop securities for his comments in this work."}, {"heading": "A Proof", "text": "The proof is provided by induction: Leave n = 1, m1 (m0) 2 < (m0m1) 2 as m0 = m1 =... = mn = m and mi \u2265 2 as i = 1.2,..., n as m3 < m4 and the sentence applies to n = 1. Assume that the sentence applies to n = k: A = b < 0 leave n = k + 1. Therefore, the statement A = b < 0 can be used as: mk + 1mkmk + 1) 2 + 1mkmk mk \u2212 1 (m1) 2 +... + mk + 1 (mk + 1) 2 + (mk + 1) 2 \u2212 mk mk mk mk + 1 (mk mk + 1) 1 mk mk mk mk (1) 1 mk mk) 1 (mk) 1 mk mk) 1 (mk) 1 \u2212 mk mk) 1 (mk) 1 \u2212 mk (mk) 1 mk) 1 (mk) 1 \u2212 mk) 1 (mk) 1 \u2212 mk (mk) 1 \u2212 mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk) 2 \u2212 mk mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk) 2 \u2212 mk mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk (mk) 2 \u2212 mk) 1 \u2212 mk"}, {"heading": "B Coefficients", "text": "The coefficients used in the execution of the algorithms are shown in table 3."}], "references": [{"title": "Conceptual data modeling for spatiotemporal applications", "author": ["N. Tryfona", "C. Jensen"], "venue": "GeoInformatica,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Imagebased spatio-temporal modeling and view interpolation of dynamic events", "author": ["S. Vedula", "S. Baker", "T. Kanade"], "venue": "ACM Trans. Graph.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A sliding-window kernel rls algorithm and its application to nonlinear channel identification", "author": ["S. Van Vaerenbergh", "J. Via", "I. Santamaria"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Sliding window recursive quadratic optimiza-  tion with variable regularization", "author": ["J. Hoagg", "A. Ali", "M. Mossberg", "D. Bernstein"], "venue": "In American Control Conference (ACC),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Sliding-windowed weighted recursive least-squares method for parameter estimation", "author": ["Choi", "B.-Y", "Z. Bien"], "venue": "Electronics Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Fixed-budget kernel recursive leastsquares", "author": ["S. Van Vaerenbergh", "I. Santamaria", "W. Liu", "J. Principe"], "venue": "In Acoustics Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Kernel recursive least-squares tracker for time-varying regression", "author": ["S. Van Vaerenbergh", "M. Lazaro-Gredilla", "I. Santamaria"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Quantized kernel least mean square algorithm", "author": ["B. Chen", "S. Zhao", "P. Zhu", "J. Principe"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The kernel recursive least-squares algorithm", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Recurrent online kernel recursive least square algorithm for nonlinear modeling", "author": ["H. Fan", "Q. Song", "Z. Xu"], "venue": "In IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Nonstationary gaussian process regression using point estimates of local smoothness", "author": ["C. Plagemann", "K. Kersting", "W. Burgard"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Learning non-stationary space-time models for environmen-  tal monitoring", "author": ["S. Garg", "A. Singh", "F. Ramos"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "The kriged kalman filter", "author": ["K. Mardia", "C. Goodall", "E. Redfern", "F. Alonso"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Big data deep learning: Challenges and perspectives", "author": ["X. wen Chen", "X. Lin"], "venue": "Access, IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deep Gaussian processes", "author": ["A. Damianou", "N. Lawrence"], "venue": "In Proceedings of the Sixteenth International Workshop on Artificial Intelligence and Statistics (AISTATS-13),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Image segmentation with em algorithm", "author": ["R. Lu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Classes of nonseparable, spatio-temporal stationary covariance functions", "author": ["N. Cressie", "Huang", "H.-C"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "A comparative study of kernel adaptive filtering algorithms", "author": ["S. Van Vaerenbergh", "I. Santamaria"], "venue": "In Digital Signal Processing and Signal Processing Education Meeting (DSP/SPE),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Kernel methods for nonlinear identification, equalization and separation of signals", "author": ["S. Van Vaerenbergh"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Modeling such phenomena has been the focus of much attention during the past decades [1, 2].", "startOffset": 85, "endOffset": 91}, {"referenceID": 1, "context": "Modeling such phenomena has been the focus of much attention during the past decades [1, 2].", "startOffset": 85, "endOffset": 91}, {"referenceID": 2, "context": "Of the many techniques studied, Kernel methods [3], have emerged as a leading tool for data-driven modeling of nonlinear spatiotemporally varying phenomena.", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 4, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 5, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 6, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 7, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 8, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 9, "context": "Kernel based estimators with recursive least squares (RLS) (or its sparsified version) learning algorithms represent the state-of-the-art in data-driven spatiotemporal modeling [4\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 10, "context": "The primary reason for this is that as the size of the dataset increases, the number of kernels that need to be utilized begins to increase, which consequently leads to large kernel matrix size and computational inefficiency of the algorithm [11].", "startOffset": 242, "endOffset": 246}, {"referenceID": 3, "context": "For instance, in [4], authors present a SlidingWindow Kernel Recursive Least Squares (SW-KRLS) method that only considers predefined last observed samples.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "The Naive Online regularized Risk Minimization (NORMA) [5] algorithm is developed based on the idea of stochastic gradient descent within a feature space.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Naturally, the main drawback of SlidingWindow based approaches is that they can forget long-term patterns as they discard old observed samples [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "For example, in [8], the Fixed-Budget KRLS (FB-KRLS) algorithm is presented, in which the sample that plays the least significant role, the least error upon being omitted, is discarded.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "A Bayesian approach is also presented in [9] that utilizes confidence intervals for handling non-stationary scenarios with a predefined dictionary size.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "In [10], a Quantized Kernel Least Mean Square (QKLMS) algorithm is presented that is developed based on a vector quantization method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Moreover, Sparsified KRLS (S-KRLS) is presented in [11] which adds an input to the dictionary by comparing its approximate linear dependency (ALD) to the observed inputs, assuming a predefined threshold.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "In [12] a recurrent kernel recursive least square algorithm for online learning is presented in which a compact dictionary is chosen by a sparsification method based on the Hessian matrix of the loss function that continuously examines the importance of the new training sample to utilize in dictionary update of the dictionary according to the importance measure, using a predefined fixed budget dictionary.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "A number of authors have also explored non-stationary kernel design and local-region based hyper-parameter optimization to accommodate spatiotemporal variations [14,15].", "startOffset": 161, "endOffset": 168}, {"referenceID": 13, "context": "A number of authors have also explored non-stationary kernel design and local-region based hyper-parameter optimization to accommodate spatiotemporal variations [14,15].", "startOffset": 161, "endOffset": 168}, {"referenceID": 13, "context": "As our results show, the presented method can far outperform a variant of the NOSTILL-GP algorithm [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "The Kriged Kalman Filter (KKF) [16] models evolution of weights of a kernel model with a linear model over time.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "It should be noted that works in the area of Deep Neural Network [17] and recently Deep GP [18] literature have explored hierarchic structures in the bases.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "It should be noted that works in the area of Deep Neural Network [17] and recently Deep GP [18] literature have explored hierarchic structures in the bases.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "The Kernel Recursive Least-Squares (KRLS) algorithm has quadratic computational cost O(m2), where m denotes the number of samples of the input vector [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 2, "context": "To identify and distinguish plants from grasses, the Expectation Maximization (EM) [3] segmentation method, based on color, is implemented on the images and the result of that is shown in Figure 12 (Left Bottom) (the EM code used here is a modification of that can be found in [20]).", "startOffset": 83, "endOffset": 86}, {"referenceID": 17, "context": "To identify and distinguish plants from grasses, the Expectation Maximization (EM) [3] segmentation method, based on color, is implemented on the images and the result of that is shown in Figure 12 (Left Bottom) (the EM code used here is a modification of that can be found in [20]).", "startOffset": 277, "endOffset": 281}, {"referenceID": 18, "context": "In contrast, cross-correlations between space and time have been modeled in the literature by providing different space-time covariance functions [21], such as:", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "We considered C as the input to KRLS, called the NONSTILL-KRLS method herein, inspired from NONSTILL-GP [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 185, "endOffset": 189}, {"referenceID": 9, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 204, "endOffset": 208}, {"referenceID": 7, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 218, "endOffset": 221}, {"referenceID": 10, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 230, "endOffset": 234}, {"referenceID": 3, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 244, "endOffset": 247}, {"referenceID": 4, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 259, "endOffset": 262}, {"referenceID": 20, "context": "Table 2 presents the comparison in terms of computational training time and maximum validation error of the presented algorithm with the studied kernel adaptive filtering algorithms in [22], namely QKLMS [10], FB-KRLS [8], S-KRLS [11], SW-KRLS [4], and NORMA [5] (the codes used here are modified versions of that found in [23]).", "startOffset": 323, "endOffset": 327}], "year": 2017, "abstractText": "We present a new hierarchic kernel based modeling technique for modeling evenly distributed multidimensional datasets that does not rely on input space sparsification. The presented method reorganizes the typical single-layer kernel based model in a hierarchical structure, such that the weights of a kernel model over each dimension are modeled over the adjacent dimension. We show that the imposition of the hierarchical structure in the kernel based model leads to significant computational speedup and improved modeling accuracy (over an order of magnitude in many cases). For instance the presented method is about five times faster and more accurate than Sparsified Kernel Recursive LeastSquares in modeling of a two-dimensional real-world data set.", "creator": "LaTeX with hyperref package"}}}