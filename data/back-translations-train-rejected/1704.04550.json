{"id": "1704.04550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Distributional model on a diet: One-shot word learning from text only", "abstract": "We test whether distributional models can do one-shot learning of definitional properties from text only. Using Bayesian models, we find that first learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative.", "histories": [["v1", "Fri, 14 Apr 2017 22:29:27 GMT  (202kb)", "http://arxiv.org/abs/1704.04550v1", "Keywords: Distributional semantics; Lexical semantics; Bayesian models"], ["v2", "Thu, 7 Sep 2017 14:25:46 GMT  (46kb)", "http://arxiv.org/abs/1704.04550v2", "Accepted at IJCNLP 2017 as a conference paper"], ["v3", "Thu, 12 Oct 2017 01:14:58 GMT  (49kb)", "http://arxiv.org/abs/1704.04550v3", "Accepted at IJCNLP 2017 as a conference paper"], ["v4", "Fri, 13 Oct 2017 14:14:25 GMT  (49kb)", "http://arxiv.org/abs/1704.04550v4", "The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017)"]], "COMMENTS": "Keywords: Distributional semantics; Lexical semantics; Bayesian models", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["su wang", "stephen roller", "katrin erk"], "accepted": false, "id": "1704.04550"}, "pdf": {"name": "1704.04550.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["shrekwang@utexas.edu,", "roller@cs.utexas.edu,", "katrin.erk@mail.utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.04 550v 1 [cs.C L] 14 Apr 201 7We test whether distribution models can learn definition properties only from text at a glance. Using Bayesian models, we find that the first learning of a parent structure in the known data, regularities in textual contexts and properties helps learning at a glance, and that individual context elements can be highly informative."}, {"heading": "1 Introduction", "text": "When people come across an unknown word in the text, they can often deduce roughly what it means, as in this example from Lazaridou et al. (2014): We found a nice, hairy wampimuk sleeping behind the tree. People who hear this sentence typically guess that a wampimuk is an animal, or even that it is a mammal. Distribution models that describe the meaning of a word in relation to its observed contexts (Turney and Pantel, 2010) were typically proposed as a model of how people learn word meanings (Landauer and Dumais, 1997). However, distribution models typically require hundreds of instances of a word to derive a high-quality representation from it, while humans can often deduce a passable meaning from just one sentence (as in the above example). This phenomenon is known as fast mapping (Carey and Bartlett, 1978).While there is preliminary evidence that Lazaridop can be distributable (al)."}, {"heading": "2 Background", "text": "Fast mapping and textual context. Fast mapping (Carey and Bartlett, 1978) is the human ability to construct provisional word meaning representations after one or few exposures. An important reason why people can take quick mapping approaches is that they acquire general knowledge that limits learning (Smith et al., 2002; Colunga et Smith, 2005; Kemp et al., 2007; Xu and Tenenbaum, 2007; Maas and Kemp, 2009). In this paper, we ask which forms of general knowledge will be useful for text-based word learning (Smith et al., 2005) we consider fast mapping for grounded word learning, mapping image data for distributed representations, which is in a way the mirror image of our task. Lazaridou et al al al al al (2016) were the first to explore fast mapping for text-based word learning, using an extension to word2vec with textual and visual features."}, {"heading": "3 Models", "text": "In this section, we will develop a series of models to test our hypothesis that the acquisition of general knowledge is helpful for text learning, especially knowledge of similarities between context elements (H1) and co-occurrences between properties (H2). The numerical model does not implement any of these hypotheses, while the bimodal theme model implements both. To test the hypotheses separately, we will use two cluster approaches via Bernoulli Mixtures, which we will use as an extension of the Countbased Model and the bimodal theme model."}, {"heading": "3.1 The Count-based Model", "text": "The task is to determine properties that are applicable to an unknown concept that is applicable to an unknown concept. Each concept is associated with a vector cInd (where \"Ind\" stands for \"independent Bernoulli probabilities\"), where the i-th entry of cInd is the probability that an instance of the concept c would have property. These probabilities are independent of Bernoulli probabilities. AlligatorInd would have an entry of 0.95 for dangerous. An instance c {0, 1} | Q of aconcept c \u00b2 C is a vector of zeros and entries drawn from cInd, where an entry of 1 at position i means that this instance learns property qi.The model in two steps."}, {"heading": "3.2 The Bimodal Topic Model", "text": "We use an extension of LDA (Lead et al., 2003) to implement our hypotheses on the usefulness of an overarching structure (Lead et al., 2003). In particular, we build on Andrew et al. (2009) by using a bi-modal theme model in which a single theme simultaneously creates both a context object and a property. We build on Dinu and Lapata (2010) by having a \"pseudo document\" for each concept c to represent its observed events. In our case, this pseudo document contains pairs of a context object w-V and a property q-Q, meaning that w was observed to occur with an instance of c that q.The generative story is as follows: For each known concept c, we draw a multinomic model that extends across topics."}, {"heading": "3.3 Bernoulli Mixtures", "text": "To evaluate the two hypotheses individually, we use the accumulation of Bernoulli mixture models of both contexts, each of which is a vector of Bernoulli or the properties of Bernoulli mixture models, each of which is a vector of m Bernoulli probabilities: p (x) x-x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "4 Data and Experimental Setup", "text": "So we have the QMR dataset (Herbelot and Vecchi, 2015b), which is ideally suited. QMR has 532 concrete terms, each associated with a set of quantified properties. We choose the majority rating if it exists, and otherwise the minimum rating. To address this, we need to focus primarily on the one-sided learning attitude, which we call for less than 5 concepts, which allows us to judge with 503 concepts and 220 properties. It is a problem of the two original McRae et al. (2005) data and QMR, which are not listed, even if they are not listed."}, {"heading": "5 Results and Discussion", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6 Conclusion", "text": "To test whether distribution models, such as humans, can learn word meanings from a single textual occurrence, we have developed several probabilistic distribution models to learn only from the textual context; the models were developed to test the hypothesis that general knowledge about the coexistence of distribution contexts (H1) or the coexistence of properties (H2) supports word learning; we find evidence that both types of general knowledge are helpful, especially in combination (in Bi-TM), or when used on very clean property data (in the Animal Dataset); we also found that some context elements alone can be highly informative; and in a preliminary investigation of computer science measures, we find that average similarities between seen role fillers (AvgCos) achieve success in predicting which context elements lead to successful learning (H2); an obvious next step will be other types of known concepts (in particular, Xxu and Kemp)."}], "references": [{"title": "Integrating experiential and distributional data to learn semantic representations", "author": ["Mark Andrews", "Gabriella Vigliocco", "David Vinson."], "venue": "Psychological Review 116(3):463\u2013498.", "citeRegEx": "Andrews et al\\.,? 2009", "shortCiteRegEx": "Andrews et al\\.", "year": 2009}, {"title": "Latent Dirichlet Allocation", "author": ["David Blei", "Andrew Ng", "Michael Jordan."], "venue": "Journal of Machine Learning Research 3(4-5):993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Acquiring a single new word", "author": ["Susan Carey", "Elsa Bartlett."], "venue": "Papers and Reports on Child Language Development 15:17\u201329.", "citeRegEx": "Carey and Bartlett.,? 1978", "shortCiteRegEx": "Carey and Bartlett.", "year": 1978}, {"title": "From the lexicon to expectations about kinds: A role for associative learning", "author": ["Eliana Colunga", "Linda B. Smith."], "venue": "Psychological Review 112(2):347\u2013 382.", "citeRegEx": "Colunga and Smith.,? 2005", "shortCiteRegEx": "Colunga and Smith.", "year": 2005}, {"title": "Analyzing the factors underlying the structure and computation of the meaning of chipmunk, cherry, chisel, cheese, and cello (andmany other such concrete nouns)", "author": ["George S. Cree", "Ken McRae."], "venue": "Journal of Experimental Psychology: General 132:163\u2013201.", "citeRegEx": "Cree and McRae.,? 2003", "shortCiteRegEx": "Cree and McRae.", "year": 2003}, {"title": "Measuring distributional similarity in context", "author": ["Georgiana Dinu", "Mirella Lapata."], "venue": "Proceedings of EMNLP. Cambridge, MA.", "citeRegEx": "Dinu and Lapata.,? 2010", "shortCiteRegEx": "Dinu and Lapata.", "year": 2010}, {"title": "A flexible, corpus-driven model of regular and inverse selectional preferences", "author": ["Katrin Erk", "Sebastian Pad\u00f3", "Ulrike Pad\u00f3."], "venue": "Computational Linguistics 36(4).", "citeRegEx": "Erk et al\\.,? 2010", "shortCiteRegEx": "Erk et al\\.", "year": 2010}, {"title": "From distributional semantics to feature norms: Grounding semantic models in human perceptual data", "author": ["Luana F\u0103g\u0103r\u0103\u015fan", "Eva Maria Vecchi", "Stephen Clark."], "venue": "Proceedings of IWCS. London, Great Britain.", "citeRegEx": "F\u0103g\u0103r\u0103\u015fan et al\\.,? 2015", "shortCiteRegEx": "F\u0103g\u0103r\u0103\u015fan et al\\.", "year": 2015}, {"title": "Background to FrameNet", "author": ["C.J. Fillmore", "C.R. Johnson", "M. Petruck."], "venue": "International Journal of Lexicography 16:235\u2013250.", "citeRegEx": "Fillmore et al\\.,? 2003", "shortCiteRegEx": "Fillmore et al\\.", "year": 2003}, {"title": "Probabilistic semantics and pragmatics: Uncertainty in language and thought", "author": ["Noah D. Goodman", "Daniel Lassiter."], "venue": "Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, Wiley-Blackwell.", "citeRegEx": "Goodman and Lassiter.,? 2014", "shortCiteRegEx": "Goodman and Lassiter.", "year": 2014}, {"title": "What is in a text, what isn\u2019t and what this has to do with lexical semantics", "author": ["Aur\u00e9lie Herbelot."], "venue": "Proceedings of IWCS .", "citeRegEx": "Herbelot.,? 2013", "shortCiteRegEx": "Herbelot.", "year": 2013}, {"title": "Building a shared world:mapping distributional to modeltheoretic semantic spaces", "author": ["Aur\u00e9lie Herbelot", "Eva Vecchi."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Herbelot and Vecchi.,? 2015a", "shortCiteRegEx": "Herbelot and Vecchi.", "year": 2015}, {"title": "Many speakers, many worlds", "author": ["Aur\u00e9lie Herbelot", "Eva Maria Vecchi."], "venue": "Linguistic Issues in Language Technology 12(4):1\u201320.", "citeRegEx": "Herbelot and Vecchi.,? 2015b", "shortCiteRegEx": "Herbelot and Vecchi.", "year": 2015}, {"title": "Perceptual inference through global lexical similarity", "author": ["Brendan T Johns", "Michael N Jones."], "venue": "Topics in Cognitive Science 4(1):103\u2013120.", "citeRegEx": "Johns and Jones.,? 2012", "shortCiteRegEx": "Johns and Jones.", "year": 2012}, {"title": "Bernoulli mixture models for binary images", "author": ["Alfons Juan", "Enrique Vidal."], "venue": "Proceedings of ICPR.", "citeRegEx": "Juan and Vidal.,? 2004", "shortCiteRegEx": "Juan and Vidal.", "year": 2004}, {"title": "Learning overhypotheses with hierarchical Bayesian models", "author": ["Charles Kemp", "Amy Perfors", "Joshua B. Tenenbaum."], "venue": "Developmental Science 10(3):307\u2013321.", "citeRegEx": "Kemp et al\\.,? 2007", "shortCiteRegEx": "Kemp et al\\.", "year": 2007}, {"title": "VerbNet: A broadcoverage, comprehensive verb lexicon", "author": ["Karin Kipper-Schuler."], "venue": "Ph.D. thesis, Computer and Information Science Dept., University of Pennsylvania, Philadelphia, PA.", "citeRegEx": "Kipper.Schuler.,? 2005", "shortCiteRegEx": "Kipper.Schuler.", "year": 2005}, {"title": "Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments", "author": ["Kazuaki Kishida."], "venue": "NII Technical Reports 2005(14):1\u201319.", "citeRegEx": "Kishida.,? 2005", "shortCiteRegEx": "Kishida.", "year": 2005}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas Landauer", "Susan Dumais."], "venue": "Psychological Review pages 211\u2013240.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "author": ["Angeliki Lazaridou", "Elia Bruni", "Marco Baroni."], "venue": "Proceedings of ACL.", "citeRegEx": "Lazaridou et al\\.,? 2014", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Multimodal word meaning induction from minimal exposure to natural text", "author": ["Angeliki Lazaridou", "MarcoMarelli", "Marco Baroni"], "venue": "Cognitive Science", "citeRegEx": "Lazaridou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2016}, {"title": "One-shot learning with Bayesian networks", "author": ["Andrew L. Maas", "Charles Kemp."], "venue": "Proceedings of the 31st Annual Conference of the Cognitive Science Society. Amsterdam, The Netherlands.", "citeRegEx": "Maas and Kemp.,? 2009", "shortCiteRegEx": "Maas and Kemp.", "year": 2009}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "author": ["Ken McRae", "George S. Cree", "Mark S. Seidenberg", "Chris McNorgan."], "venue": "Behavior Research Methods 37(4):547\u2013559.", "citeRegEx": "McRae et al\\.,? 2005", "shortCiteRegEx": "McRae et al\\.", "year": 2005}, {"title": "Latent variable models of selectional preference", "author": ["Diarmuid \u00d3 S\u00e9aghdha."], "venue": "Proceedings of ACL.", "citeRegEx": "S\u00e9aghdha.,? 2010", "shortCiteRegEx": "S\u00e9aghdha.", "year": 2010}, {"title": "Probabilistic distributional semantics with latent variable models", "author": ["Diarmuid \u00d3 S\u00e9aghdha", "Anna Korhonen."], "venue": "Computational Linguistics 40(3):587\u2013631.", "citeRegEx": "S\u00e9aghdha and Korhonen.,? 2014", "shortCiteRegEx": "S\u00e9aghdha and Korhonen.", "year": 2014}, {"title": "A Latent Dirichlet Allocation method for selectional preferences", "author": ["Alan Ritter", "Mausam", "Oren Etzioni."], "venue": "Proceedings of ACL.", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "A multimodal lda model integrating textual, cognitive and visual modalities", "author": ["Stephen Roller", "Sabine Schulte im Walde."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Roller and Walde.,? 2013", "shortCiteRegEx": "Roller and Walde.", "year": 2013}, {"title": "How well do distributional models capture different types of semantic knowledge? In Proceedings of ACL", "author": ["Dana Rubinstein", "Effi Levi", "Roy Schwartz", "Ari Rappoport."], "venue": "volume 2, pages 726\u2013730.", "citeRegEx": "Rubinstein et al\\.,? 2015", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2015}, {"title": "Object name learning provides on-the-job training for attention", "author": ["Linda B. Smith", "Susan S. Jones", "Barbara Landau", "Lisa Gershkoff-Stowe", "Larissa Samuelson."], "venue": "Psychological Science 13(1):13\u201319.", "citeRegEx": "Smith et al\\.,? 2002", "shortCiteRegEx": "Smith et al\\.", "year": 2002}, {"title": "Probabilistic topic models", "author": ["Mark Steyvers", "Tom Griffiths."], "venue": "T. Landauer, D.S. McNamara, S. Dennis, and W. Kintsch, eds., Handbook of Latent Semantic Analysis .", "citeRegEx": "Steyvers and Griffiths.,? 2007", "shortCiteRegEx": "Steyvers and Griffiths.", "year": 2007}, {"title": "The British National Corpus, version 3 (BNC XML Edition)", "author": ["The BNC Consortium."], "venue": "Oxford University Computing Services, URL: http://www.natcorp.ox.ac.uk/.", "citeRegEx": "Consortium.,? 2007", "shortCiteRegEx": "Consortium.", "year": 2007}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Representing the meanings of object and action words: The featural and unitary semantic space hypothesis", "author": ["Gabriella Vigliocco", "David Vinson", "William Lewis", "Merrill Garrett"], "venue": "Cognitive Psychology", "citeRegEx": "Vigliocco et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Vigliocco et al\\.", "year": 2004}, {"title": "Word learning as Bayesian inference", "author": ["Fei Xu", "Joshua B. Tenenbaum."], "venue": "Psychological Review 114(2):245\u2013272.", "citeRegEx": "Xu and Tenenbaum.,? 2007", "shortCiteRegEx": "Xu and Tenenbaum.", "year": 2007}], "referenceMentions": [{"referenceID": 19, "context": "When humans encounter an unknown word in text, they can often infer approximately what it means, as in this example from Lazaridou et al. (2014):", "startOffset": 121, "endOffset": 145}, {"referenceID": 31, "context": "Distributional models, which describe the meaning of a word in terms of its observed contexts (Turney and Pantel, 2010), have been suggested as a model for how humans learn word meanings (Landauer and Dumais, 1997).", "startOffset": 94, "endOffset": 119}, {"referenceID": 18, "context": "Distributional models, which describe the meaning of a word in terms of its observed contexts (Turney and Pantel, 2010), have been suggested as a model for how humans learn word meanings (Landauer and Dumais, 1997).", "startOffset": 187, "endOffset": 214}, {"referenceID": 2, "context": "ping (Carey and Bartlett, 1978).", "startOffset": 5, "endOffset": 31}, {"referenceID": 20, "context": "While there is preliminary evidence that fast mapping can be modeled distributionally (Lazaridou et al., 2016), it is unclear what enables it.", "startOffset": 86, "endOffset": 110}, {"referenceID": 15, "context": "The literature emphasizes the importance of learning general knowledge or overarching structure across all concepts (Kemp et al., 2007), for example knowledge about which properties are most important to object naming (Smith et al.", "startOffset": 116, "endOffset": 135}, {"referenceID": 28, "context": ", 2007), for example knowledge about which properties are most important to object naming (Smith et al., 2002; Colunga and Smith, 2005), or a taxonomy of concepts (Xu and Tenenbaum, 2007).", "startOffset": 90, "endOffset": 135}, {"referenceID": 3, "context": ", 2007), for example knowledge about which properties are most important to object naming (Smith et al., 2002; Colunga and Smith, 2005), or a taxonomy of concepts (Xu and Tenenbaum, 2007).", "startOffset": 90, "endOffset": 135}, {"referenceID": 33, "context": ", 2002; Colunga and Smith, 2005), or a taxonomy of concepts (Xu and Tenenbaum, 2007).", "startOffset": 60, "endOffset": 84}, {"referenceID": 16, "context": "Distributional representations of syntactic context are directly interpretable as selectional constraints, which in manually created resources are typically characterized through highlevel taxonomy classes (Kipper-Schuler, 2005; Fillmore et al., 2003).", "startOffset": 206, "endOffset": 251}, {"referenceID": 8, "context": "Distributional representations of syntactic context are directly interpretable as selectional constraints, which in manually created resources are typically characterized through highlevel taxonomy classes (Kipper-Schuler, 2005; Fillmore et al., 2003).", "startOffset": 206, "endOffset": 251}, {"referenceID": 6, "context": "Also, it has been shown that selectional constraints can be learned distributionally (Erk et al., 2010; \u00d3 S\u00e9aghdha and Korhonen, 2014; Ritter et al., 2010).", "startOffset": 85, "endOffset": 155}, {"referenceID": 25, "context": "Also, it has been shown that selectional constraints can be learned distributionally (Erk et al., 2010; \u00d3 S\u00e9aghdha and Korhonen, 2014; Ritter et al., 2010).", "startOffset": 85, "endOffset": 155}, {"referenceID": 2, "context": "Fast mapping (Carey and Bartlett, 1978) is the human ability to construct provisional word meaning representations after one or few exposures.", "startOffset": 13, "endOffset": 39}, {"referenceID": 28, "context": "An important reason for why humans can do fast mapping is that they acquire general knowledge that constrains learning (Smith et al., 2002; Colunga and Smith, 2005; Kemp et al., 2007; Xu and Tenenbaum, 2007; Maas and Kemp, 2009).", "startOffset": 119, "endOffset": 228}, {"referenceID": 3, "context": "An important reason for why humans can do fast mapping is that they acquire general knowledge that constrains learning (Smith et al., 2002; Colunga and Smith, 2005; Kemp et al., 2007; Xu and Tenenbaum, 2007; Maas and Kemp, 2009).", "startOffset": 119, "endOffset": 228}, {"referenceID": 15, "context": "An important reason for why humans can do fast mapping is that they acquire general knowledge that constrains learning (Smith et al., 2002; Colunga and Smith, 2005; Kemp et al., 2007; Xu and Tenenbaum, 2007; Maas and Kemp, 2009).", "startOffset": 119, "endOffset": 228}, {"referenceID": 33, "context": "An important reason for why humans can do fast mapping is that they acquire general knowledge that constrains learning (Smith et al., 2002; Colunga and Smith, 2005; Kemp et al., 2007; Xu and Tenenbaum, 2007; Maas and Kemp, 2009).", "startOffset": 119, "endOffset": 228}, {"referenceID": 21, "context": "An important reason for why humans can do fast mapping is that they acquire general knowledge that constrains learning (Smith et al., 2002; Colunga and Smith, 2005; Kemp et al., 2007; Xu and Tenenbaum, 2007; Maas and Kemp, 2009).", "startOffset": 119, "endOffset": 228}, {"referenceID": 22, "context": "Feature norm datasets are available from McRae et al. (2005) and Vigliocco et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 22, "context": "Feature norm datasets are available from McRae et al. (2005) and Vigliocco et al. (2004). There are several recent approaches", "startOffset": 41, "endOffset": 89}, {"referenceID": 13, "context": "that learn to map distributional representations to feature norms (Johns and Jones, 2012; Rubinstein et al., 2015; F\u0103g\u0103r\u0103\u015fan et al., 2015; Herbelot and Vecchi, 2015a).", "startOffset": 66, "endOffset": 166}, {"referenceID": 27, "context": "that learn to map distributional representations to feature norms (Johns and Jones, 2012; Rubinstein et al., 2015; F\u0103g\u0103r\u0103\u015fan et al., 2015; Herbelot and Vecchi, 2015a).", "startOffset": 66, "endOffset": 166}, {"referenceID": 7, "context": "that learn to map distributional representations to feature norms (Johns and Jones, 2012; Rubinstein et al., 2015; F\u0103g\u0103r\u0103\u015fan et al., 2015; Herbelot and Vecchi, 2015a).", "startOffset": 66, "endOffset": 166}, {"referenceID": 11, "context": "that learn to map distributional representations to feature norms (Johns and Jones, 2012; Rubinstein et al., 2015; F\u0103g\u0103r\u0103\u015fan et al., 2015; Herbelot and Vecchi, 2015a).", "startOffset": 66, "endOffset": 166}, {"referenceID": 12, "context": "In the current paper we use the QuantifiedMcRae (QMR) dataset (Herbelot and Vecchi, 2015b), which extends the McRae et al.", "startOffset": 62, "endOffset": 90}, {"referenceID": 10, "context": "(2005) feature norms by ratings on the proportion of category members that have a property, and the Animal dataset (Herbelot, 2013), which is smaller but has the same shape.", "startOffset": 115, "endOffset": 131}, {"referenceID": 10, "context": "In the current paper we use the QuantifiedMcRae (QMR) dataset (Herbelot and Vecchi, 2015b), which extends the McRae et al. (2005) feature norms by ratings on the proportion of category members that have a property, and the Animal dataset (Herbelot, 2013), which is smaller but has the same shape.", "startOffset": 63, "endOffset": 130}, {"referenceID": 1, "context": "(2009) and Roller and Schulte im Walde (2013), who use multi-modal extensions of Latent Dirichlet Allocation (LDA) models (Blei et al., 2003) to represent co-occurrences of textual context and definitional features.", "startOffset": 122, "endOffset": 141}, {"referenceID": 3, "context": "The Bayesian models in lexical semantics that are most related to our approach are Dinu and Lapata (2010), who represent word meanings as distributions over latent topics that approximate senses, and Andrews et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 0, "context": "The Bayesian models in lexical semantics that are most related to our approach are Dinu and Lapata (2010), who represent word meanings as distributions over latent topics that approximate senses, and Andrews et al. (2009) and Roller and Schulte im Walde (2013), who use multi-modal extensions of Latent Dirichlet Allocation (LDA) models (Blei et al.", "startOffset": 200, "endOffset": 222}, {"referenceID": 0, "context": "The Bayesian models in lexical semantics that are most related to our approach are Dinu and Lapata (2010), who represent word meanings as distributions over latent topics that approximate senses, and Andrews et al. (2009) and Roller and Schulte im Walde (2013), who use multi-modal extensions of Latent Dirichlet Allocation (LDA) models (Blei et al.", "startOffset": 200, "endOffset": 261}, {"referenceID": 0, "context": "The Bayesian models in lexical semantics that are most related to our approach are Dinu and Lapata (2010), who represent word meanings as distributions over latent topics that approximate senses, and Andrews et al. (2009) and Roller and Schulte im Walde (2013), who use multi-modal extensions of Latent Dirichlet Allocation (LDA) models (Blei et al., 2003) to represent co-occurrences of textual context and definitional features. \u00d3 S\u00e9aghdha (2010) and Ritter et al.", "startOffset": 200, "endOffset": 449}, {"referenceID": 0, "context": "The Bayesian models in lexical semantics that are most related to our approach are Dinu and Lapata (2010), who represent word meanings as distributions over latent topics that approximate senses, and Andrews et al. (2009) and Roller and Schulte im Walde (2013), who use multi-modal extensions of Latent Dirichlet Allocation (LDA) models (Blei et al., 2003) to represent co-occurrences of textual context and definitional features. \u00d3 S\u00e9aghdha (2010) and Ritter et al. (2010) use Bayesian approaches to model selectional preferences.", "startOffset": 200, "endOffset": 474}, {"referenceID": 1, "context": "We use an extension of LDA (Blei et al., 2003) to implement our hypotheses on the usefulness of overarching structure, both commonalities in selectional constraints across predicates, and cooccurrence of properties across concepts.", "startOffset": 27, "endOffset": 46}, {"referenceID": 0, "context": "In particular, we build on Andrews et al. (2009) in using a bimodal topic model, in which a single topic simultaneously generates both a context item and a property.", "startOffset": 27, "endOffset": 49}, {"referenceID": 0, "context": "In particular, we build on Andrews et al. (2009) in using a bimodal topic model, in which a single topic simultaneously generates both a context item and a property. We further build on Dinu and Lapata (2010) in having a \u201cpseudo-document\u201d for each concept c to represent its observed occurrences.", "startOffset": 27, "endOffset": 209}, {"referenceID": 14, "context": "A Bernoulli Mixture model (Juan and Vidal, 2004) assumes that a population ofm-dimensional binary vectors x has been generated by a set of mixture components K , each of which is a vector ofm Bernoulli probabilities:", "startOffset": 26, "endOffset": 48}, {"referenceID": 12, "context": "So the QMR dataset (Herbelot and Vecchi, 2015b) is ideally suited.", "startOffset": 19, "endOffset": 47}, {"referenceID": 21, "context": "It is a problem of both the original McRae et al. (2005) data and QMR that if a property is not named by participants, it is not listed, even if it applies.", "startOffset": 37, "endOffset": 57}, {"referenceID": 10, "context": "So we additionally use the Animal dataset of Herbelot (2013), where every property has a rating for every concept.", "startOffset": 45, "endOffset": 61}, {"referenceID": 29, "context": "For training the bi-TM, we use collapsed Gibbs sampling (Steyvers and Griffiths, 2007) with 500 iterations for burn-in.", "startOffset": 56, "endOffset": 86}, {"referenceID": 10, "context": "Following Herbelot and Vecchi (2015a) we omit these properties.", "startOffset": 10, "endOffset": 38}, {"referenceID": 10, "context": "Following Herbelot and Vecchi (2015a) we omit these properties. 4 https://spacy.io ing setting that adds one occurrence at a time. While this is possible in principle, the computational cost is prohibitive for the bi-TM.) We compare to the Partial Least Squares (PLS) model of Herbelot and Vecchi (2015a) to see whether our models perform at state of the art levels.", "startOffset": 10, "endOffset": 305}, {"referenceID": 10, "context": "Following Herbelot and Vecchi (2015a) we omit these properties. 4 https://spacy.io ing setting that adds one occurrence at a time. While this is possible in principle, the computational cost is prohibitive for the bi-TM.) We compare to the Partial Least Squares (PLS) model of Herbelot and Vecchi (2015a) to see whether our models perform at state of the art levels. We also compare to a baseline that always predicts the probability of a property to be its relative frequency in the set C of known concepts (Baseline). We can directly use the property probabilities in QMR and the Animal data as concept representations cInd for the Count Independent model. For the Count Multinomial model, we never explicitly compute cMult. To sample from it, we first sample an instance c \u2208 {0, 1}|Q| from the independent Bernoulli vector of c, cInd. From the properties that apply to c, we sample one (with equal probabilities) as the observed property. All priors for the count-based models (Beta priors or Dirichlet priors, respectively) are set to 1. For the bi-TM, a pseudo-document for a known concept c is generated as follows. Given an occurrence of known concept c with context item w in the BNC, we sample a property q from c (in the same way as for the Count Multinomial model), and add \u3008w, q\u3009 to the pseudodocument for c. For training the bi-TM, we use collapsed Gibbs sampling (Steyvers and Griffiths, 2007) with 500 iterations for burn-in. The Dirichlet priors are uniformly set to 0.1 following Roller and Schulte im Walde (2013). We use 50 topics throughout.", "startOffset": 10, "endOffset": 1532}, {"referenceID": 10, "context": "For the PLS benchmark, we use 50 components with otherwise default settings, following Herbelot and Vecchi (2015a). Evaluation.", "startOffset": 87, "endOffset": 115}, {"referenceID": 17, "context": "When we evaluate with Generalized Average Precision (GAP) (Kishida, 2005), which does take gold property weights into account, the baseline performance is on average more than 10 points below the other models.", "startOffset": 58, "endOffset": 73}, {"referenceID": 10, "context": "els perform on par with the PLS benchmark of Herbelot and Vecchi (2015a) on QMR, and on the Animal dataset they outperform the benchmark.", "startOffset": 45, "endOffset": 73}, {"referenceID": 21, "context": "McRae et al. (2005) classify properties based on the brain region taxonomy of Cree and McRae (2003).", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "(2005) classify properties based on the brain region taxonomy of Cree and McRae (2003). This enables us to test what types of properties are learned most", "startOffset": 65, "endOffset": 87}, {"referenceID": 33, "context": "One obvious next step will be to test other types of general knowledge, in particular a taxonomy of known concepts (Xu and Tenenbaum, 2007), or overhypotheses on model hyperparameters (Kemp et al.", "startOffset": 115, "endOffset": 139}, {"referenceID": 15, "context": "One obvious next step will be to test other types of general knowledge, in particular a taxonomy of known concepts (Xu and Tenenbaum, 2007), or overhypotheses on model hyperparameters (Kemp et al., 2007).", "startOffset": 184, "endOffset": 203}, {"referenceID": 9, "context": "with an overall probabilistic model of semantics (Goodman and Lassiter, 2014).", "startOffset": 49, "endOffset": 77}], "year": 2017, "abstractText": "We test whether distributional models can do one-shot learning of definitional properties from text only. Using Bayesian models, we find that first learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative.", "creator": "LaTeX with hyperref package"}}}