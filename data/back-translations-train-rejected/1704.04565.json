{"id": "1704.04565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "Neural Paraphrase Identification of Questions with Noisy Pretraining", "abstract": "We present a solution to the problem of paraphrase identification of questions. We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model (Parikh et al., 2016) results in accurate performance on this task, while being far simpler than many competing neural architectures. Furthermore, when the model is pretrained on a noisy dataset of automatically collected question paraphrases, it obtains the best reported performance on the dataset.", "histories": [["v1", "Sat, 15 Apr 2017 02:09:31 GMT  (35kb,D)", "https://arxiv.org/abs/1704.04565v1", null], ["v2", "Sun, 20 Aug 2017 02:41:42 GMT  (35kb,D)", "http://arxiv.org/abs/1704.04565v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gaurav singh tomar", "thyago duque", "oscar t\\\"ackstr\\\"om", "jakob uszkoreit", "dipanjan das"], "accepted": false, "id": "1704.04565"}, "pdf": {"name": "1704.04565.pdf", "metadata": {"source": "CRF", "title": "Neural Paraphrase Identification of Questions with Noisy Pretraining", "authors": ["Gaurav Singh Tomar", "Thyago Duque", "Oscar T\u00e4ckstr\u00f6m", "Jakob Uszkoreit", "Dipanjan Das"], "emails": ["dipanjand}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Question Paraphrase Identification is a widely used NLP application. For example, in question and answer forums that are ubiquitous on the Web, there are an enormous number of duplicate questions. Identifying these duplicates and consolidating their answers automatically increases the efficiency of such QA forums. In addition, identifying questions with the same semantic content could help answer question systems on the Web scale, which are increasingly focused on retrieving focused answers to users \"queries. Here, we are focusing on a current dataset published on the QA website Quora.com that contains over 400K commented question pairs that represent binary paraphrase labels.1 We believe this dataset represents a great opportunity for the NLP research community and practitioners due to its size and quality; it can lead to systems that accurately identify duplicated questions and thus improve the quality of question and data formats."}, {"heading": "2 Related Work", "text": "In this context, we focus on an instance that consists in finding questions of identical importance. Most relevant for this work is the task of Wang et al. (2016), which presents the best results on the Quora dataset prior to this work (dos Santos et al., 2015), but it contains two orders of magnitude fewer annotations, limiting the quality of each model. Most relevant for this work is that of Wang et al. (2017), which presents the best results on the Quora dataset prior to this work. Wang et al. \"s bilateral multiperspective matching model (BIMPM) uses a character-based LSTM (Hochreiter and Schmidhuber, 1997) at its input education level, a layer of bi-LSTMs for the calculation of context information."}, {"heading": "3 Approach", "text": "Our starting point is the decomposable attention model (Parikh et al., 2016, DECATT from now on), which despite its simplicity and efficiency has proven remarkably good for the associated task of natural language reasoning (Bowman et al., 2015). We extend this model to include character embedding in n-Grammys and noisy pre-training for the task of question paraphrase identification."}, {"heading": "3.1 Problem Formulation", "text": "Let a = (a1,.., a'a) and b = (b1,..., b'b) be two input texts, each consisting of \"a\" and \"b.\" We assume that each ai, bj \"Rd is encoded in a vector of dimension d. Then a context window of size c is applied, so that the input into the model (a,\" b \") consists of partially overlapping phrases a\" i \"= [ai\" c., \"ai.,\" ai \"c,\" b \"j\" = [bj \"c.,\" bj., \"bj\" + c \"[R2c + 1\" d. The model is estimated in the form of designated pairs {a \"n,\" b \"n,\" y \"(n), Nn = 1, where y (n) {0, 1} is a binary label indicating whether a is a paraphrase of b or not."}, {"heading": "3.2 The Decomposable Attention Model", "text": "The DECATT model divides the prediction into three steps: Attend, Comare and Aggregates. Due to the lack of space, we offer only a brief outline and refer to Parikh et al. (2016) for more details on each of these steps. Firstly, the elements of a network (b) and b) are aligned using a variant of neural attention (Bahdanau et al., 2015) to break down the problem into the comparison of aligned phrases. (1) > F (b) > F (b) > j) > Function F is a forward-facing network. The aligned phrases are calculated as follows: \u03b2i: \"b) b (eij) b) b (eik) b (eik) b (b) b) b) b) b) j:\" an exp (b) b) j: \"an exp.\""}, {"heading": "3.4 Noisy Pretraining", "text": "While coding character diagrams helps with the effective distribution of parameters, data poverty remains a problem. Pretraining embedding with a task-agnostic goal on large corpora (Pennington et al., 2014) is a common remedy for this problem. However, such pretraining is limited in the following ways: First, it applies only to the initial presentation, leaving subsequent parts of the model to random initialization. Second, embedding may be suboptimal if the embedding is not pre-trained in the same area as the final task (e.g. questions). Finally, since the goal of pretraining differs from that of the final task (e.g. paraphrase identification), embedding may be suboptimal. As an alternative to task-agnostic pretraining of embedding on a very large body, we suggest preparing all parameters of the model on a moderately large body."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Implementation Details", "text": "In fact, most of us will be able to move to another world, in which they will be able to integrate, and in which they will be able to integrate, \"he said in an interview with the\" New York Times. \"\" I believe that we will be able to change the world, and that we will be able to change the world. \"He added,\" I do not believe that we will be able to change the world we are in. \""}, {"heading": "4.2 Results", "text": "We observe that the simple FFNN baselines work better than the more complex Siamese and multi-perspective CNN or LSTM models, all the more so when the character-based embeddings are used. Our basic decomposable attention model DECATTword is better than most models that have used all the embeddings of GloVe. An interesting observation is that the DECATTchar model without pre-formed embeddings is better than DECATTglove, which uses task-agnostic embeddings of GloVe. Furthermore, when the embeddings of characters are pre-trained, in a task-specific way in DECATTparalex \u2212 char model, we observe a significant boost in performance."}, {"heading": "5 Conclusion and Future Work", "text": "We have made a focused contribution to the identification of question phrases in the recently published Quora corpus. Firstly, we have shown that replacing the word embedding of the decomposable attention model by Parikh et al. (2016) with character embedding in n-Grammys leads to significantly better accuracy in this task. Secondly, we have shown that pre-training the complete model on automatically labeled loud but task-specific data leads to further improvements. Our methods perform better than several complex neural architectures and reach the state of the art. Although they are conceptually simple, we believe these are two important insights that could be more widely applicable in the field of understanding natural languages."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv 1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Discriminative learning over constrained latent representations", "author": ["Ming-Wei Chang", "Dan Goldwasser", "Dan Roth", "Vivek Srikumar."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Chang et al\\.,? 2010", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Enhancing and combining sequential and tree LSTM for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang."], "venue": "arXiv 1609.06038 .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ACL.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Paraphrase identification as probabilistic quasi-synchronous recognition", "author": ["Dipanjan Das", "Noah A. Smith."], "venue": "Proceedings of ACL-IJCNLP.", "citeRegEx": "Das and Smith.,? 2009", "shortCiteRegEx": "Das and Smith.", "year": 2009}, {"title": "Learning hybrid representations to retrieve semantically equivalent questions", "author": ["Cicero dos Santos", "Luciano Barbosa", "Dasha Bogdanova", "Bianca Zadrozny."], "venue": "Proceedings of ACL.", "citeRegEx": "Santos et al\\.,? 2015", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of ACL.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Multiperspective sentence similarity modeling with convolutional neural networks", "author": ["Hua He", "Kevin Gimpel", "Jimmy Lin."], "venue": "Proceedings of EMNLP.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."], "venue": "Proceedings of CIKM.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Yoon Kim", "Carl Denton", "Loung Hoang", "Alexander M. Rush."], "venue": "Proceedings of ICLR.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proceedings of AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Semi-supervised question retrieval with gated convolutions", "author": ["Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi Jaakkola", "Kateryna Tymoshenko", "Alessandro Moschitti", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proceedings of NAACL", "citeRegEx": "Lei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of ACL.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Parikh et al\\.,? 2016", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of ACL.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Deep multitask learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of ACL.", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "Bilateral multi-perspective matching for natural language sentences", "author": ["Zhiguo Wang", "Wael Hamza", "Radu Florian."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Sentence similarity learning by lexical decomposition and composition", "author": ["Zhiguo Wang", "Haitao Mi", "Abraham Ittycheriah."], "venue": "Proceedings of COLING.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wieting et al\\.,? 2016", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model (Parikh et al., 2016) results in accurate performance on this task, while being far simpler than many competing neural architectures.", "startOffset": 147, "endOffset": 168}, {"referenceID": 18, "context": "We examine a simple model family, the decomposable attention model of Parikh et al. (2016), that has shown promise in modeling natural", "startOffset": 70, "endOffset": 91}, {"referenceID": 4, "context": "language inference and has inspired recent work on similar tasks (Chen et al., 2016; Kim et al., 2017).", "startOffset": 65, "endOffset": 102}, {"referenceID": 13, "context": "language inference and has inspired recent work on similar tasks (Chen et al., 2016; Kim et al., 2017).", "startOffset": 65, "endOffset": 102}, {"referenceID": 9, "context": "Second, to significantly improve our model performance, we pretrain all our model parameters on the noisy, automatically collected question-paraphrase corpus Paralex (Fader et al., 2013), followed by fine-tuning the parameters on the Quora dataset.", "startOffset": 166, "endOffset": 186}, {"referenceID": 11, "context": "uses a character-based LSTM (Hochreiter and Schmidhuber, 1997) at its input representation layer, a layer of bi-LSTMs for computing context information, four different types of multi-perspective matching layers, an additional bi-LSTM aggregation layer, followed by a ar X iv :1 70 4.", "startOffset": 28, "endOffset": 62}, {"referenceID": 3, "context": "Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al.", "startOffset": 78, "endOffset": 246}, {"referenceID": 3, "context": "Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al., 2015), but it contains two orders of magnitude less annotations, thus limiting the quality of any model. Most relevant to this work is that of Wang et al. (2017), who present the best results on the Quora dataset prior to this work.", "startOffset": 78, "endOffset": 484}, {"referenceID": 12, "context": "While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016).", "startOffset": 95, "endOffset": 162}, {"referenceID": 24, "context": "While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016).", "startOffset": 95, "endOffset": 162}, {"referenceID": 1, "context": "While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016).", "startOffset": 95, "endOffset": 162}, {"referenceID": 20, "context": "More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015).", "startOffset": 95, "endOffset": 200}, {"referenceID": 17, "context": "More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015).", "startOffset": 95, "endOffset": 200}, {"referenceID": 14, "context": "More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015).", "startOffset": 95, "endOffset": 200}, {"referenceID": 6, "context": "More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015).", "startOffset": 95, "endOffset": 200}, {"referenceID": 16, "context": "More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015).", "startOffset": 95, "endOffset": 200}, {"referenceID": 2, "context": ", 2016, DECATT henceforth), which despite its simplicity and efficiency has been shown to work remarkably well for the related task of natural language inference (Bowman et al., 2015).", "startOffset": 162, "endOffset": 183}, {"referenceID": 18, "context": "Due to lack of space, we only provide a brief outline below and refer to Parikh et al. (2016) for further", "startOffset": 73, "endOffset": 94}, {"referenceID": 0, "context": "First, the elements of \u0101 and b\u0304 are aligned using a variant of neural attention (Bahdanau et al., 2015) to decompose the problem into the comparison of aligned phrases.", "startOffset": 80, "endOffset": 103}, {"referenceID": 5, "context": "In this optional step, we modify the input representations using \u201cself-attention\u201d to encode compositional relationships between words within each sentence, as proposed by (Cheng et al., 2016).", "startOffset": 171, "endOffset": 191}, {"referenceID": 19, "context": "Pretraining embeddings with a task-agnostic objective on large-scale corpora (Pennington et al., 2014) is a common remedy to this problem.", "startOffset": 77, "endOffset": 102}, {"referenceID": 22, "context": "We use the same data and split as Wang et al. (2017), with 10,000 question pairs each for development and", "startOffset": 34, "endOffset": 53}, {"referenceID": 9, "context": "When pretraining the full model parameters, we use the Paralex corpus (Fader et al., 2013), which consists of 36 million noisy paraphrase pairs including duplicate reversed paraphrases.", "startOffset": 70, "endOffset": 90}, {"referenceID": 22, "context": "The first six rows are taken from (Wang et al., 2017).", "startOffset": 34, "endOffset": 53}, {"referenceID": 19, "context": "Second, we compare purely supervised variants of decomposable attention model, namely a word-based model without any pretrained embeddings (DECATTword), a word-based model with GloVe (Pennington et al., 2014) embeddings (DECATTglove), a character ngram model (DECATTchar) without pretrained embeddings and DECATTparalex\u2212char whose character n-gram embeddings are pretrained with Paralex while all other parameters are learned from scratch on Quora.", "startOffset": 183, "endOffset": 208}, {"referenceID": 22, "context": "Other than our baselines, we compare with Wang et al. (2017) in Table 2.", "startOffset": 42, "endOffset": 61}, {"referenceID": 18, "context": "First, we showed that replacing the word embeddings of the decomposable attention model of Parikh et al. (2016) with character n-gram embeddings results in significantly better accuracy on this task.", "startOffset": 91, "endOffset": 112}], "year": 2017, "abstractText": "We present a solution to the problem of paraphrase identification of questions. We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model (Parikh et al., 2016) results in accurate performance on this task, while being far simpler than many competing neural architectures. Furthermore, when the model is pretrained on a noisy dataset of automatically collected question paraphrases, it obtains the best reported performance on the dataset.", "creator": "LaTeX with hyperref package"}}}