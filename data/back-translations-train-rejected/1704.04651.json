{"id": "1704.04651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "The Reactor: A Sample-Efficient Actor-Critic Architecture", "abstract": "In this work we present a new reinforcement learning agent, called Reactor (for Retrace-actor), based on an off-policy multi-step return actor-critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy {\\pi} (the actor), an action-value Q-function (the critic) evaluating the current policy {\\pi}, and an estimated behavioral policy {\\hat \\mu} which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel {\\beta}-leave-one-out policy gradient estimate (which uses both the off-policy corrected return and the estimated Q-function). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance.", "histories": [["v1", "Sat, 15 Apr 2017 15:38:23 GMT  (727kb,D)", "http://arxiv.org/abs/1704.04651v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["audrunas gruslys", "mohammad gheshlaghi azar", "marc g bellemare", "remi munos"], "accepted": false, "id": "1704.04651"}, "pdf": {"name": "1704.04651.pdf", "metadata": {"source": "META", "title": "The Reactor: A Sample-Efficient Actor-Critic Architecture", "authors": ["Audrunas Gruslys", "Mohammad Gheshlaghi Azar", "Marc G. Bellemare", "Remi Munos"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that it is a matter of a way in which people blame themselves and others, while blaming themselves and others. (...) It is not as if they blame the blame on themselves. (...) It is as if they blame themselves. (...) It is as if they put themselves in the shoes. (...) It is as if they put themselves in the shoes. (...) \"It is as if they see themselves in a position to hate themselves and blame others. (...) It is as if they put themselves in the shoes. (...) It is as if they put themselves in the shoes. (...) It is as if they put themselves in the shoes. (...) It is as if they hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate, hate.\""}, {"heading": "2. The actor-critic algorithm", "text": "In this section we will first describe the general algorithm we used for the critic (Retrace) and the actor (\u03b2-leave-oneout). In the next section we will describe our specific implementation. First, we will define a notation. We will consider a Markov decision-making process with the state space X and the finite action space A. A (stochastic) policy is a representation of X on the distributions of actions. We will consider a \u03b3-discounted criterion for the endless horizon and define for each policy the Q value of any pair of conditions (x, a) asQ\u03c0 (x, a) def = E [\u2211 t \u2265 0 \u03b3tr (xt, at) | x0 = x, a0 = a, \u03c0] whereby ({{xt} t \u2265 0) is a trajectory generated by the choice of a property in x and the subsequent one, i.e., in the case of a \u0432\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "2.1. The critic: Retrace(\u03bb)", "text": "For the critic, we use the Retrace (\u03bb) algorithm introduced in (Munos et al., 2016), which is a general non-political RL algorithm that uses multi-stage returns. Suppose that a path {x0, a0, r0, x1, a1, r1,..., xt, at, rt,..., was created according to a behavior policy \u00b5, i.e., at \u0445 \u00b5 (\u00b7 | xt). Now, we want to evaluate the value of another target policy \u03c0, i.e. we want to estimate Q\u03c0. The Retrace algorithm updates our current estimate Q from Q\u03c0 towards \u0445 Q (xt, at) def = \u0445 s."}, {"heading": "2.2. The actor: \u03b2-LOO policy gradient algorithm", "text": "The reactor architecture represents both a political approach derived from an approach (possibly deviating from behaviour) and Q values. We use a political gradient algorithm to train the actor \u03c0 who recommends our current estimate Q (x, a) of Q\u03c0 (x, a). Let us leave V \u03c0 (x0) the value function at an initial state x0, the political gradient theorem (Sutton et al., 2000) stating that this approach (x0) = approach of the approach of the approach of the approach of the approach of the approach of the approach of the approach of the approach (x) = approach of the approach of the approach of the approach of the approach of the approach (where the approach of the approach of the approach of the approach corresponds to the approach of the approach of the approach of the method of the method of the approach),"}, {"heading": "3. Reactor implementation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Off-policy past actions handled by Retrace", "text": "Since the policy of the agent changes over time, we must take into account that past actions {as \u0445 \u03c0s} s \u2264 t stored in memory were generated according to a policy that differs from the policy \u03c0t we want to evaluate at the present time. As described in Section 2.1, the retrace algorithm uses two strategies, behavioral and target policies. In reactor, the behavior policy \u00b5 corresponds to actions {as} s \u2264 t that were generated by the agent in the past and stored in the memory buffer. Target policy corresponds to the current policy of the agent we want to evaluate."}, {"heading": "3.2. Behaviour probability estimates \u00b5\u0302", "text": "The behavior of the agent is defined by scanning actions from the current policy area and stores observed states, actions, redeployments, and selected action probabilities (xt, at, rt, \u00b5t) in the repetition memory. Retrace uses an extra-political correction that requires knowledge of the probability under which actions were generated (i.e. the trace cutting coefficient in (2) depends on \u00b5 (as | xs). We can either use the behavior probabilities \u00b5 (as | xs) of the selected actions stored in memory or we can construct and estimate a \"p\" (as | xs) based on past samples by predicting actions P (as | xs). The repetition memory contains a mixture of trajectories \"p\" p \"p p p p p p p p p p p p p p p p p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\""}, {"heading": "3.3. Network architecture", "text": "The reactor architecture uses a recursive neural network architecture that takes an observation problem as input and produces three results: current action value estimates Q (xt, a), current policy \u03c0 (a | xt), and estimated behavior policy \u00b5 (a | xt), for all actions a (Figure 1). Action values use a dueling architecture (Wang et al., 2015) that internally divides Q value into government value and benefit forecasts, which in turn is connected to a recurrent LSTM network (Hochreiter & Schmidhuber, 1997).Politicians and behavioral heads use the last softmax layer mixed with a fixed uniform distribution of the choice of a random action, where this mixing ratio is a hyperparameter. Each political network \u00b5 and each policy have their own separate LSTM, as this works much better than sharing the same LSTM with action head Q.All three LSTMs in turn are connected to a common linear layer that is connected to a common evolutionary network."}, {"heading": "3.4. Target networks", "text": "We used a target network QT (fixed copy of the current network), similar to the DQN (Mnih et al., 2015). Our implementation of the retrace algorithm uses the target network QT. Contiguous sequences {xu,.., xu + \u03c4} of length \u03c4 are evenly randomly sampled from our memory buffer. We use the target network QT to update our current Q estimate Q (xt, at) in any state xt (t, u + \u03c4]) towards the new target: rt + \u03b3E\u03c0 [QT (xt + 1, \u00b7) + u + \u03c4 \u0445 s = t + 1 \u03b3s \u2212 t (ct + 1... cs)."}, {"heading": "3.5. Learning Q, \u03c0, \u00b5\u0302, from a replayed sequence", "text": "In each learning step, a sequence of lengths is (uniformly) sampled from memory; the target QT values are evaluated along the sequence and the current policy \u03c0 and behavioral policy \u00b5 probabilities are evaluated by the current network. Learning Q-values: Retrace algorithm is used to trace all current Q values along the sequence back to the new target values (using an L2 loss) defined by (7). If \u00b5 values are used, they are evaluated from the moving network to obtain the most up-to-date behavioral estimates. Learning \u03c0: The current policy is updated by using the previously discussed \u03b2-LOO policy gradient. To prevent deterrent strategies from converging too quickly, we add a scaled entropy reward to the policy gradient. To make the entropy reward scale more consistent across games with different reward amplitudes and frequencies, we have a plot gap evaluated over each action step."}, {"heading": "3.6. Parallelism", "text": "To improve CPU / GPU utilization, we can decouple action from learning, which is an important aspect of our architecture: An acting thread receives observations, transmits actions to the environment, and stores transition information in memory, while a LernThread picks up sequences of experiences from memory and learns from them (Figure 4). Compared to the standard DQN framework (Mnih et al., 2015), Reactor is able to improve CPU / GPU utilization through acting learning, and compared to the A3C framework (Mnih et al., 2016), we can take advantage of the computing advantages of batch learning."}, {"heading": "4. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Comparison of different policy gradients within Reactor architecture", "text": "We trained different versions of Reactor on 57 Atari games for 200 million frames with 3 randomly selected seeds per experiment. We used the following method to compare the performance of the algorithm in all games as a function of training time: (1). For each game, for each algorithm pair and for each time step, we evaluated the probability that algorithm 1 has more score than algorithm 2 by comparing all seeds in pairs, resulting in a tensor of size P [game x alg1 x alg2 x time]. (2) For each algorithm pair and for each time step, we calculated probabilities in all games that produced Q [alg1 x alg2 x time]. This quantity has an interpretable meaning for the probability that a random instance of algorithm 1 would execute a random instance of algorithm 2 at a time when a randomly selected game is not present. (3) For each algorithm and each time, we evaluated the average probability by averaging each other algorithm over each other probability."}, {"heading": "4.2. Estimating versus memorizing behavioural probabilities", "text": "The results are shown in Figures 5 and 6. A notable exception is the TISLR algorithm, where estimated behavioural probabilities for most training steps and most parameter configurations performed better than estimated values, especially for the time period of the target network update Tupdate = 10000. A notable exception is the TISLR algorithm, where estimated behavioural probabilities performed better for most training steps and most parameter configurations. However, the performance difference was not as large as in the case of TSILR (Figure 5), where 200 million steps of learning behavioural probabilities equals about 1 / 3 the probability of achieving better results than values for both beta-LOO algorithms."}, {"heading": "4.3. Comparing different policy gradients", "text": "As can be seen from Figure 6, leave-one-out with \u03b2 = 1 performed slightly better than leave-one-out with \u03b2 = min (1 / \u00b5, 5) for both memorized and estimated behavioral probability values. Both algorithms performed better than TISLR for memorized behavioral probabilities and behaved similarly when probabilities were estimated."}, {"heading": "4.4. Comparing different architectures", "text": "In fact, it is such that we are able to maneuver ourselves into a situation in which we see ourselves in a position in which we see ourselves in which we are able to change the world, in which we are able to live, in which we are able to live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are"}, {"heading": "5. Conclusion", "text": "In this work, we introduced a new non-political agent based on the rerace-actor-critic architecture and demonstrated that he can perform similar services to the current state of the art. We also demonstrated that, under certain circumstances, estimated behavioural probabilities can exceed memorised behavioural probabilities when used for non-political corrections. Since the framework is completely outside politics, it can be used to test various exploration ideas."}, {"heading": "6. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Toward minimax off-policy value estimation", "author": ["Li", "Lihong", "Munos", "R\u00e9mi", "Szepesv\u00e1ri", "Csaba"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["Munos", "R\u00e9mi", "Stepleton", "Tom", "Harutyunyan", "Anna", "Bellemare", "Marc"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Understanding and improving convolutional neural networks via concatenated rectified linear units", "author": ["Shang", "Wenling", "Sohn", "Kihyuk", "Almeida", "Diogo", "Lee", "Honglak"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "Mcallester", "David", "Singh", "Satinder", "Mansour", "Yishay"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "Schaul", "Tom", "Hessel", "Matteo", "van Hasselt", "Hado", "Lanctot", "Marc", "de Freitas", "Nando"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Wang et al\\.", "year": 1995}, {"title": "Sample efficient actor-critic with experience replay", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Model free deep reinforcement learning has achieved remarkable success lately in many domains ranging from achieving human and super-human level control in video games (Mnih et al., 2016), (Mnih et al.", "startOffset": 168, "endOffset": 187}, {"referenceID": 6, "context": ", 2015) to continuous motor control tasks (Lillicrap et al., 2015).", "startOffset": 42, "endOffset": 66}, {"referenceID": 7, "context": ", 2015) and A3C (Mnih et al., 2016), have several disadvantages: DQN suffers from potentially slow learning caused by single-step temporal difference updates, while A3C has a relatively low data-efficiency (since it does not use a memory buffer) and requires being trained on several copies of the same environment simultaneously.", "startOffset": 16, "endOffset": 35}, {"referenceID": 8, "context": "The critic implements the Retrace (Munos et al., 2016) algorithm, while the actor is trained by a new policy gradient algorithm, called \u03b2-leave-one-out, which makes use of both the off-policy Retrace-corrected return and the estimated Q-function.", "startOffset": 34, "endOffset": 54}, {"referenceID": 7, "context": "Related works: Like A3C (Mnih et al., 2016), Reactor is an actor-critic multi-step returns algorithm.", "startOffset": 24, "endOffset": 43}, {"referenceID": 6, "context": "DDPG (Lillicrap et al., 2015) uses an actor-critic architecture but in a continuous control setting making use of the deterministic policy gradient algorithm and does not consider multi-step returns.", "startOffset": 5, "endOffset": 29}, {"referenceID": 2, "context": "UNREAL agent (Jaderberg et al., 2016) improved final performance and data efficiency of A3C framework by introducing replay memory and unsupervised auxiliary tasks.", "startOffset": 13, "endOffset": 37}, {"referenceID": 14, "context": "The closest work to ours is the ACER algorithm (Wang et al., 2017) which is also an actor-critic based on the Retrace algorithm which makes use of memory replay.", "startOffset": 47, "endOffset": 66}, {"referenceID": 8, "context": "For the critic we use the Retrace(\u03bb) algorithm introduced in (Munos et al., 2016).", "startOffset": 61, "endOffset": 81}, {"referenceID": 8, "context": "the Q estimates, see (Munos et al., 2016).", "startOffset": 21, "endOffset": 41}, {"referenceID": 11, "context": "Let V (x0) be the value function at some initial state x0, the policy gradient theorem (Sutton et al., 2000) says that \u2207V (x0) = E [\u2211 t \u03b3 t \u2211 aQ (xt, a)\u2207\u03c0(a|xt) ] , where \u2207 refers to the gradient w.", "startOffset": 87, "endOffset": 108}, {"referenceID": 14, "context": "This truncated 1/\u03bc coefficient shares similarities with the truncated IS gradient estimate introduced in (Wang et al., 2017) (which we call TISLR for truncated-ISLR):", "startOffset": 105, "endOffset": 124}, {"referenceID": 8, "context": "Instead, in our implementation we use the off-policy corrected return computed by the Retrace algorithm, which produces a (biased) estimate ofQ(A) but whose bias vanishes asymptotically, see (Munos et al., 2016).", "startOffset": 191, "endOffset": 211}, {"referenceID": 5, "context": "Note that some theoretical findings (Li et al., 2015) support that using a regression estimate may indeed improve importance sampling.", "startOffset": 36, "endOffset": 53}, {"referenceID": 4, "context": "All three LSTMs in turn are connected to a shared linear layer which is connected to a shared convolutional neural network (Krizhevsky et al., 2012).", "startOffset": 123, "endOffset": 148}, {"referenceID": 10, "context": "Concatenated rectified linear units are used as nonlinearities in the network (Shang et al., 2016).", "startOffset": 78, "endOffset": 98}, {"referenceID": 10, "context": "Firstly, as was shown by (Shang et al., 2016), convolutional neural networks with rectified linear units tend to lead to pairs of opposite features.", "startOffset": 25, "endOffset": 45}, {"referenceID": 7, "context": ", 2015) Reactor is able to improve CPU/GPU utilization by learning while acting, and comparing to A3C framework (Mnih et al., 2016) we can exploit computational benefits of batched learning.", "startOffset": 112, "endOffset": 131}, {"referenceID": 14, "context": "We reported the final performance by taking the best training curve value from each learning curve in order to make evaluation comparable to the results reported by (Wang et al., 2017).", "startOffset": 165, "endOffset": 184}, {"referenceID": 0, "context": "Table 2 contains a comparison of our algorithm with several other synchronous state-of-art algorithms across 57 Atari games for a fixed random seed across all games (Bellemare et al., 2013).", "startOffset": 165, "endOffset": 189}, {"referenceID": 9, "context": ", 2016), DQN with prioritized experience replay (Schaul et al., 2015), duelling architecture and prioritized duelling architecture (Wang et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 14, "context": "Table 3 contains a comparison of our algorithm with ACER algorithm (Wang et al., 2017).", "startOffset": 67, "endOffset": 86}, {"referenceID": 14, "context": "The best value for ACER was obtained from Figure 1 in (Wang et al., 2017) by reading off the highest learning curve below 200 million steps.", "startOffset": 54, "endOffset": 73}, {"referenceID": 14, "context": "On the other hand our evaluation was somewhat more pessimistic than the one used by (Wang et al., 2017), because we averaged rewards over windows of 1 million steps, while in the case of ACER the rewards were averaged over windows of 200 thousand frames.", "startOffset": 84, "endOffset": 103}], "year": 2017, "abstractText": "In this work we present a new reinforcement learning agent, called Reactor (for Retraceactor), based on an off-policy multi-step return actor-critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy \u03c0 (the actor), an action-value Q-function (the critic) evaluating the current policy \u03c0, and an estimated behavioural policy \u03bc\u0302 which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel \u03b2-leave-oneout policy gradient estimate (which uses both the off-policy corrected return and the estimated Qfunction). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}