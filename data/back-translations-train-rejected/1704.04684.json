{"id": "1704.04684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "Generic LSH Families for the Angular Distance Based on Johnson-Lindenstrauss Projections and Feature Hashing LSH", "abstract": "In this paper we propose the creation of generic LSH families for the angular distance based on Johnson-Lindenstrauss projections. We show that feature hashing is a valid J-L projection and propose two new LSH families based on feature hashing. These new LSH families are tested on both synthetic and real datasets with very good results and a considerable performance improvement over other LSH families. While the theoretical analysis is done for the angular distance, these families can also be used in practice for the euclidean distance with excellent results [2]. Our tests using real datasets show that the proposed LSH functions work well for the euclidean distance.", "histories": [["v1", "Sat, 15 Apr 2017 19:32:51 GMT  (876kb,D)", "http://arxiv.org/abs/1704.04684v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.IR", "authors": ["luis argerich", "natalia golmar"], "accepted": false, "id": "1704.04684"}, "pdf": {"name": "1704.04684.pdf", "metadata": {"source": "CRF", "title": "Generic LSH Families for the Angular Distance Based on Johnson-Lindenstrauss Projections and Feature Hashing LSH", "authors": ["Luis Argerich", "Natalia Golmar"], "emails": ["largerich@fi.uba.ar", "ngolmar@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTIONLocality Sensitive Hashing (LSH) is a modern solution to the Approximate Nearest Neighbors Problem (ANN) for large datasets. The basic idea of LSH is to hash similar elements with the same bucket. After we hack a query point, we can restore the points in the bucket as candidates and calculate the distance to the query only for these points. This is a significant improvement over the raw force approach of comparing the query point with all other points in the dataset. In this work, we are working with LSH for the angle distance above the unit sphere. Angle distance is often used in areas such as information query and word embedding. [13] [14] [15] In practice, the LSH schemas created for the angle distance show that SH schemas for the angle distance can also be used for clicking function with eudic families, with very similar results from these two SH being used for very significant results."}, {"heading": "II. PRELIMINARIES AND NOTATION", "text": "LSH was introduced in 1998 by Indyk and Motwani [7] [6]. Their basic idea of LSH is to hack similar points on the same bucket, which allows an approximate query time of O (1) when we retrieve close neighbors. If we have a query point, we hack the point and then go to the bucket pointed to by the LSH function, and compare the query to the points in the bucket. A very good LSH function minimizes the number of points to compare and maximizes the number of real close neighbors in the bucket. We define a Minhash h h h h h h h (x) \u2192 [0.. m) as a result of applying a hash function to a d-dimensional point x and getting a bucket number to impress the point between 0 and m \u2212 1. For a hash function to be considered a good minhash function, we need the following conditions: 1) The hash function must be simple to allow the hash function to imposition a hash point, so that the hash points should be simple to allow the hash function to be a hash point = 2)."}, {"heading": "III. AMPLIFICACION OF LSH FAMILIES", "text": "For any LSH family H (d1, d2, p1, p2) where p1 is the probability of retrieving points that are close to a query point, we want to keep p1 as high as possible. 1 \u2212 p1 is the occurrence of false negatives, which means that some points that are closer than d1 to our query point cannot be retrieved. On the other hand, p2 is the probability of retrieving points that are further than desired to our query point; p2 is the probability number Xiv: 170 4.04 684v 1 [cs.D S] 1 5A prof false positives that we want to have as small as possible. Generally, false negatives affect the precision of the LSH scheme, while false positives affect its performance, as the number of distances that need to be compared can be high."}, {"heading": "IV. PREVIOUS WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Hyperplanes LSH", "text": "Our first LSH family for angular distance is based on random hyperplanes minhashes and was proposed by Charikar in 2002. [4] The construction is very simple: each minhash uses a random hyperplane vi of the same dimensionality as the points to be hashed, the minhash is then defined as the sign of the dot product between the point and the random vector. hi (x) = sign < vi, x >. Since the minhash can only define 2 values per minhash, we can use d \u2032 -bit minhash to create a d \u2032 -minhash. Since each hyperplane can be seen as a division of the unit hypersphere into two halves, we can see that the probability of a collision is related to the distance 1 between the points. Specifically, the probability of a collision is 1 \u2212 Higase, where \u03b1 is the angle between the vectors.1 From now on, we use the distance as a pre-expansion."}, {"heading": "C. Cross Polytope LSH", "text": "The cross-polytopic LSH method was introduced by Teresawa and Tanaka in 2007 [10]. Each minhash uses a random rotation from dimensions d to dimensions d \"and then the closest vertex of the d\" -dimensional cross polyp is chosen as the value of the minhash. In dimensions d \"each polytopic has 2d vertex, e.g. in < 2 the polytopic is a square determined by the vertex (0.1); (1.0); (-1.0) and (0, -1). In fact, the cross-polytopic method is a variant of the voronoi hashing, if we assume a Gaussian matrix as a pseudo-rotation and consider the maximum absolute value plus the character of the element, then voronoi hashing is the same as the cross-polytopic method. For example, if we use 5 Gaussian vectors as pseudo-rotation and consider the maximum absolute value plus the character of the element, then Voronoi hashing is the same as the cross-polytopic method."}, {"heading": "D. Fast Cross Polytope LSH", "text": "Kennedy [8] proposes a faster version of the Cross-Polytopic Method, which uses a Fast Johnson Lindenstrauss transformation from the original d dimensions to a reduced space with m dimensions and then random elevated rotation from m to d dimensions. Our experiments show that this method is actually slower in practice for small dimension vectors than direct random rotation and the other methods studied. As the dimensionality of the vectors is greater, this method can be more efficient, but then a direct dimension reduction of the vectors can be applied using feature hashing."}, {"heading": "E. Even Faster Cross Polytope LSH", "text": "Instead of an FJL transformation function, hash can be used, making the method of [8] much faster. This means that the FH is used to project from the d-to-m dimension and then a random rotation from the m-to-d dimension is used. Experiments show that the results offer very similar precision, but are significantly faster, but not as fast as the other methods studied in this paper. In general, the two-step approach from d to m and from m to d \"to achieve a minhash is not a speed improvement over a direct minhash from d to d.\""}, {"heading": "V. JOHNSON-LINDENSTRAUSS PROJECTIONS FOR LSH", "text": "We have mentioned that any function h (x) where P [h) = h (y)] = f (| x \u2212 y | |) with f monotonous as Minhash can be used. In particular, a family of functions that transforms the points from one dimensional space to another, the conservation of the norms of vectors can be used, and therefore any projection derived from the Johnson-Lindenstrauss dim can be used as Minhash. This can be proven for economical vectors regardless of the number of vectors used via restricted isometry. (RIP) As an example, the number of vectors is not sparse as the Johnson-Lindenstrauss dim is applied, and then for a large number of points that are obtained in pairs between vectors."}, {"heading": "VI. FEATURE HASHING FOR LSH", "text": "Feature hashing [9], also known as The Hashing Trick, is a very simple method to reduce the dimensionality from the original d dimensions to d \u2032. A nice advantage is that if a feature of our data is categorical, it can be hashed into multiple dimensions without having to know the total number of different values for the feature. To mitigate the effects of collisions, Weinberger [11] suggested the use of a second hash function that returns the character to be used (\u00b1 1). With the addition of the second function, the effect of collisions is softened and multiple features can be hashed into the same target area with minimal interaction [11]."}, {"heading": "A. Feature Hashing is a Johnson Lindenstrauss Transform", "text": "Feature hashing as a random projection is used in [16] to transform sparse vectors (= = 0 = expected distances) (= \u00b1 1 = = 2 expected distances, [11] also shows that FH preserves the distances between vectors. We begin to show that feature hashing can be represented as a matrix. Each feature (column) of our original vectors is mapped by the first hash function from the d dimensions to the d dimensions, and the second hash function determines the character. This is the same as multiplying the original (1xd) vector by a dxd matrix in which each row contains exactly one \u00b1 1 element. If we use k hash functions instead of just one then each line contains up to k \u00b1 1 elements and it could contain up to \u00b1 k weighted elements due to collisions. So, for example, if we use the data point V = (0, 0, 3, 0.5, 0, 0, 1, 0, 1, we can use the following dimensions (we can use F2, F1, 0), and 1, we can expect them to be: 1, 1, and 1 = hash."}, {"heading": "B. Method 1: Feature Hashing LSH", "text": "After demonstrating that feature hashing is a form of JohnsonLindenstrauss projection and finding that any JohnsonLindenstrauss projection can be used as a minhash, we propose two LSH families based on feature hashing. The first is a direct application of feature hashing. Each minhash uses a random dxd \u2032 matrix M with k \u00b1 1 elements in each line. Minhash is then calculated as follows: h (x) = argmax i = 0.. d \u00b2 \u2212 1 < x, Mi > Where Mi is the ith column of M."}, {"heading": "C. Method 2: Directional Feature Hashing LSH", "text": "In Method 2, we again use a dxd \u2032 -matrix M with k \u00b1 1 elements, then we look at the signs of each element in d \u2032 and form a d \u2032 -bit minhash as a result. This is the same as hyperplane LSH, but use only k \u00b1 1 elements in each random hyperplane. If the method is applicable, we have a direct performance improvement compared to the LSH family we are examining. h (x) = Sign (< x, Mi >) Where Mi is the ith column of M"}, {"heading": "VII. RESULTS", "text": "This year, as never before, it will be able to retaliate, to retaliate."}, {"heading": "VIII. ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Feature Hashing LSH", "text": "Feature hashing is a very flexible LSH family. It can be applied in matrix form or using hash functions, the latter being very convenient for data with categorical columns or texts where the other methods cannot be used without first performing a data transformation. In matrix form, we have only k-unequal values in each row that form their time complexity O (d-k). Since each member of the original vector can only be used once and only one addition or subtraction is performed, we can claim that the method is optimal in terms of velocity. The number of hash functions or the number of unequal values in each row of the associated matrix can be adjusted to reduce the number of false-positive elements regardless of gain. Adding more \u00b1 1 elements reduces the rate of false positives at the expense of increasing the calculation for each minhash. We note (Figure 4) that for vectors in 128 dimensions we do not need to use a complete dimension."}, {"heading": "B. Directional Feature Hashing LSH", "text": "Directional feature hashing is very similar to Hyperplanes LSH, but faster because there are only k \u00b1 1 elements in each projection. At k = d, the method corresponds to Hyperplanes Hashing. The number of bits can be adjusted so that the method works as expected regardless of the amplification used. This LSH family shows that Hyperplanes LSH can be made faster if only the random projections are made more economical."}, {"heading": "IX. CONCLUSION", "text": "Generally, any random function that depends on the distance between the vectors is suitable as a Minhash function. The number of different Minhash functions that can be generated corresponds to the size of the LSH family, and a high number is desirable. We have shown how each Minhash can be extended to work within the parameters of false positives and false negatives that we expect by amplification. We have shown that any form of random Johnson-Lindenstrauss projection can be used to create an LSH family for square distance, because the projections maintain the norms of the vectors. We have shown how feature hashing is a form of JohnsonLindenstrauss projection. Then, two new LSH families based on feature hashing were proposed, one with a low rate of false negatives and the other with a higher rate of false negatives."}], "references": [{"title": "Database-friendly random projections: Johnson- Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Practical and Optimal LSH for Angular Distance", "author": ["A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"], "venue": "Nips pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Beyond Locality- Sensitive Hashing", "author": ["A. Andoni", "P. Indyk", "H.L. Nguyen", "I. Razenshteyn"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing - STOC", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Product Quantization for Nearest Neighbor Search Herve", "author": ["M. Douze", "C. Schmid", "H. J\u00e9gou"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Similarity Search in High Dimensions via Hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases 99(1),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality", "author": ["P. Indyk", "R. Motwd"], "venue": "Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Fast Cross-Polytope Locality-Sensitive", "author": ["C. Kennedy", "R. Ward"], "venue": "Hashing pp", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Spherical LSH for Approximate Nearest Neighbor", "author": ["K. Terasawa", "Y. Tanaka"], "venue": "Search on Unit Hypersphere p", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Feature Hashing for Large Scale Multitask Learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Attenberg", "J. Langford", "A. Smola"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning (Icml), (pp. 1113\u20131120)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Optimal data-dependent hashing for approximate near neighbors", "author": ["Alexandr Andoni", "Ilya Razenshteyn"], "venue": "In STOC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Product quantization for nearest neighbor search", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Largescale speaker identification", "author": ["Ludwig Schmidt", "Matthew Sharifi", "Ignacio Lopez Moreno"], "venue": "In ICASSP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Streaming similarity search over one billion tweets using parallel locality- sensitive hashing", "author": ["Narayanan Sundaram", "Aizana Turmukhametova", "Nadathur Satish", "Todd Mostak", "Piotr Indyk", "Samuel Madden", "Pradeep Dubey"], "venue": "In VLDB,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "While the theoretical analysis is done for the angular distance, these families can also be used in practice for the euclidean distance with excellent results [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "[13][14][15] In the practice the LSH schemes created for the angular distance can also be used for the euclidean distance with very good results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13][14][15] In the practice the LSH schemes created for the angular distance can also be used for the euclidean distance with very good results.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "[13][14][15] In the practice the LSH schemes created for the angular distance can also be used for the euclidean distance with very good results.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "LSH was introduced by Indyk and Motwani in 1998 [7][6].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "LSH was introduced by Indyk and Motwani in 1998 [7][6].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "Our first LSH family for the angular distance is based on random hyperplanes minhashes and was proposed by Charikar in 2002 [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "1from now on we\u2019ll use distance to refer indistinctly to the angular distance or the euclidean distance in the unit hypersphere It was also shown in [4] that a random vector formed by just \u00b11 elements is enough as a random hyperplane.", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "This is related to a Johnson Lindenstrauss projection presentd by Achlioptas[1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Voronoi LSH [3] uses T random Gaussian vectors of the same dimensionality as the data points.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "The Cross Polytope LSH method was introduced by Teresawa and Tanaka in 2007 [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "To speed up the computation of the rotation a pseudorotation using Hadamard matrices has been proposed [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Kennedy[8] proposes a faster version of the Cross Polytope method using a Fast Johnson Lindenstrauss transform from the original d dimensions to a reduced space with m dimensions and then the random lifted rotation from m to d\u2032 dimensions.", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "Instead of a FJL transform feature hashing can be used making the method from [8] a lot faster.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Voronoi LSH is indeed a direct application of a JohnsonLindenstrauss projection using a Gaussian Matrix [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "Hyperplanes LSH is another application of a JL projection using a matrix with \u00b11 random elements [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "To mitigate the effect of collisions Weinberger [11] proposed the use of a second hash function that will return the sign to be used (\u00b11).", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "With the addition of the second function the effect of collisions is mitigated and several features can be hashed into the same target space with minimal interplay[11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "Feature hashing as a random projection is used in [16] to transform sparse vectors preserving pairwise distances, [11] also shows that FH preserves the distances between vectors.", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "For the real dataset we used the SIFT 1 million dataset [5] that is widely used for Near Neighbors experiments.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "In terms of optimality Spherical hashing [3] is optimal in terms of precision but the minhashes are not practical to compute while Feature Hashing as presented here is optimal in terms of speed and the corresponding minhashes can be used in the practice because they offer good precision after amplification.", "startOffset": 41, "endOffset": 44}], "year": 2017, "abstractText": "In this paper we propose the creation of generic LSH families for the angular distance based on Johnson-Lindenstrauss projections. We show that feature hashing is a valid J-L projection and propose two new LSH families based on feature hashing. These new LSH families are tested on both synthetic and real datasets with very good results and a considerable performance improvement over other LSH families. While the theoretical analysis is done for the angular distance, these families can also be used in practice for the euclidean distance with excellent results [2]. Our tests using real datasets show that the proposed LSH functions work well for the euclidean distance.", "creator": "LaTeX with hyperref package"}}}