{"id": "1704.04712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2017", "title": "Learn-Memorize-Recall-Reduce A Robotic Cloud Computing Paradigm", "abstract": "The rise of robotic applications has led to the generation of a huge volume of unstructured data, whereas the current cloud infrastructure was designed to process limited amounts of structured data. To address this problem, we propose a learn-memorize-recall-reduce paradigm for robotic cloud computing. The learning stage converts incoming unstructured data into structured data; the memorization stage provides effective storage for the massive amount of data; the recall stage provides efficient means to retrieve the raw data; while the reduction stage provides means to make sense of this massive amount of unstructured data with limited computing resources.", "histories": [["v1", "Sun, 16 Apr 2017 02:55:07 GMT  (697kb)", "http://arxiv.org/abs/1704.04712v1", "6 pages, 7 figures"], ["v2", "Tue, 18 Apr 2017 00:20:51 GMT  (697kb)", "http://arxiv.org/abs/1704.04712v2", "6 pages, 7 figures"]], "COMMENTS": "6 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.RO", "authors": ["shaoshan liu", "bolin ding", "jie tang", "dawei sun", "zhe zhang", "grace tsai", "jean-luc gaudiot"], "accepted": false, "id": "1704.04712"}, "pdf": {"name": "1704.04712.pdf", "metadata": {"source": "CRF", "title": "Learn-Memorize-Recall-Reduce: A Robotic Cloud Computing Paradigm", "authors": ["Shaoshan Liu", "Bolin Ding", "Jie Tang", "Dawei Sun", "Zhe Zhang", "Grace Tsai"], "emails": [], "sections": [{"heading": null, "text": "Keywords Cloud Architecture; Distributed Computing; Storage; Robotics"}, {"heading": "1. INTRODUCTION", "text": "Robots are mobile devices, and the rise of robotic applications has placed enormous pressure on our existing cloud infrastructure. For example, a mobile phone sends at least 30MB of structured data every day to the clouds of service providers, application operators, etc. (Structured data means data that machines can easily understand, store, and retrieve.) By contrast, even a very simple robot can easily generate over 1GB of unstructured multimedia data per day. An extreme form of robot, driverless cars, can generate up to 2GB of unstructured data per second [1] (unstructured data means that data cannot be easily understood, stored, and retrieved by machines), so we face the pressing challenge of designing and implementing a cloud architecture to process this massive robotic unstructured data."}, {"heading": "2. ROBOTIC CLOUD ARCHITECTURE", "text": "The architecture consists of the following components: \u2022 Robotic Client Devices: These devices capture multimedia feeds and send them to the cloud along with their metadata. \u2022 Streaming Server: This server processes incoming multimedia feeds and streams live multimedia feeds on demand to users. \u2022 Object recognition: This is a deep learning evaluation engine for automatically extracting semantic information from incoming videos. \u2022 Key Value Store: This key value store organizes the video feeds along with the learned / extracted semantic information. \u2022 Query Engine: This query engine supports retrieval of video feeds. You can search with any combination of time, location and extracted labels. \u2022 Business Analytics Engine: This engine generates high-level statistics on all multimedia data. For example, you may be interested in which objects are most common to manage the storage layer, as well as managing fast storage media."}, {"heading": "2.1 Learning", "text": "As multimedia data flows into the cloud system, the first task is to learn from the raw data and extract semantic information from the multimedia data. For video streams, we can extract object labels from frames and associate these labels with the video stream. For audio streams, we can extract phrases from the spoken language. Then, this semantic information can be used as keys, and the raw data streams can be used as values, and together, they are stored in the key-value store. As shown in Figure 2, we need a learning machine that extracts object labels from a video stream. This machine needs to be accurate in terms of the detection rate, and this machine needs to be fast so that we can capture as many objects in the video as possible. Therefore, in this implementation, we are using a faster r-cnn [3] network that runs on Caffe [4] and at the same time reaches high values without detection."}, {"heading": "2.2 Memorization", "text": "After we have extracted the semantic information from the raw multimedia data, the extracted labels are stored together with the raw data in the key value store for easy retrieval (Figure 3). We implemented the key value store with MongoDB [5]. In this case, the metadata including < sessionID, timestamp, duration, location, < list of labels > >, and the value is a file path of the raw data in the memory layer. This presents two challenges: First, in real scenarios, users vary in their choice of persistent storage for their data. Some prefer Amazon S3, some their own provision of HDFS, others Ceph, etc. One way to work around this problem is to create a set of APIs for each persistent storage, but this would be impossible as the number of persistent storage options grows. A second and probably better way to manage this is to have a unified system."}, {"heading": "2.3 Recall", "text": "The final stage is Recall, where users can use any combination of metadata such as time, location, or recognized labels to precisely retrieve the target multimedia data. In our implementation, we use MongoDB to perform smart searches using metadata as the key to first retrieve a file path from the memory layer. Ideally, the performance is at its best when the requested data is stored in the local machine's memory. However, it is impossible to store all the data in the local machine's memory. We need a mechanism to create a cache-like structure so that we have different levels of memory at different speeds, including local memory."}, {"heading": "3. OPTIMIZATIONS", "text": "As mentioned in Section 2, the write throughput of the memory layer is critical to the performance of the memory phase and the read latency of the memory layer is critical to the performance of the recall phase. In this section, we will delve into the optimizations we implement in the memory layer to improve both the write throughput and read latency."}, {"heading": "3.1 Write Optimization", "text": "If there is not enough space for the block at the top level, the system checks for free space in the next layer. This is repeated for each layer until free space is found. For example, if both the memory layer and the SSD layer are full, free space can be found in the hard disk layer. Then, the scavenger moves one block in the SSD layer to the hard disk layer and then moves a block in the memory layer to the SSD layer. Finally, the block can be written back to the storage layer. This approach is inefficient when the animal memory is full, as the scavenger must move blocks over all layers with each new block before the new block can be written. This greatly reduces the write throughput, as it takes a long time to write each block in the memory layer. To address this problem, we implement a direct read wizard so that it can write a block before that block can be written."}, {"heading": "3.2 Read Optimization", "text": "To optimize read performance, we strive to keep as much data as possible in the front-end servers, thereby avoiding the latency of retrieving blocks of data from remote back-end storage servers. We have collected text-based query data from over 1,000 users over six months. We have found that without any optimization, we have achieved a hit rate of about 50%, which means that 50% of queries satisfy the front-end servers without hitting the remote back-end storage servers, while the other 50% hit the remote servers, leading to high latency. An effective approach to improving this situation is prefetching [2]. A simple improvement when front-end servers are less busy, we get data from the most requested tables to the front-end servers. This simple improvement immediately increases the hit rate to 70%. For the next step, we break the day into six periods, each of which is four hours long."}, {"heading": "4. PERFORMANCE AND SCALABILITY", "text": "In this section, we delve into the details of the performance and scalability of the implementation of the Learn-Memorizerecall Robotic Cloud Computing Paradigm. The machine configuration used in this implementation consists of an Intel Core-i7 CPU at 3 GHz and a Titan X GPU with 12 GB graphics memory and 32 GB system memory."}, {"heading": "4.1 Object Recognition Performance", "text": "First, we look at the performance of the object detection machine. We emphasize the machine by supplying it with video streams and measuring its performance. As shown in Figure 4, when loaded on a single server, it takes an average of 0.16 seconds to process an image. Furthermore, this workload is tied to the GPU, as it consumes 95% of GPU resources, but only 2-3% of CPU resources and 3% of system memory. In our real-world use case, the in-home service robots move at a speed of about 30 centimeters per second. Therefore, in most cases, we only need to extract labels from an image every two seconds without missing an object in the scene, which means that we could support 10 incoming video streams simultaneously with each server."}, {"heading": "4.2 Query Engine Performance", "text": "We now examine the performance of the query engine. We emphasize the machine by starting 100 clients that repeatedly send queries to the query engine, and we measure the response time. As shown in Figure 5, when loaded on a single server, it takes an average of 4 ms to process a query. This workload is tied to the CPU, as it consumes 98% of the CPU resources, none of the GPU resources, and 2% of the system memory, confirming that we could easily process over 100 users at the same time with one server."}, {"heading": "4.3 Storage Performance", "text": "Then we examine the performance of the storage engine. First, we compare the throughput of a copy operation with Alluxio, with the Native File System, and with Remote HDFS. This parameter is critical because it determines how fast we can write a video feed to the storage. If the throughput is too low, the storage layer could become the bottleneck of the entire multimedia data pipeline. As shown in Figure 6, we could easily achieve throughput of more than 650 MB / s with the Alluxio in-memory engine, whereas with the Native File System we could only reach 120 MB / s. With Remote HDFS, performance is worst, only with throughput of less than 20 MB / s. This result indicates that in order for the storage engine to avoid becoming a storage bottleneck, it is important to maintain most of the write operations upright in order to hit the in-memory storage layer. \"Then, we evaluate the video call-up latency by reducing the amount of memory we use by 500 seconds to Alluxio in the memory."}, {"heading": "4.4 Deployment and Scalability", "text": "After understanding the performance of each component, we take a look at the provision of data as well as the scalability of this architecture. First, Table 1 summarizes the resource usage of the three components under stress. It is interesting that each component emphasizes a type of system resource: the faster R-CNN detection engine emphasizes the GPU, MongoDB emphasizes the CPU, while Alluxio emphasizes system memory. This suggests that it is best to locate these three services on the same servers. This approach offers the following advantages: \u2022 Cost-efficiency: as we co-locate the services, we reduce the cost. \u2022 Performance: the memory layer, Alluxio, is colored with the same servers as the query engine, which ensures high write throughput and low latency."}, {"heading": "5. NEXT STEP: DATA REDUCTION", "text": "As mentioned in the introduction, the amount of data collected by robotic devices can easily grow exponentially over time, enabling our robotic cloud to provide users with data analysis and decision support services, but also posing performance challenges in terms of storage and response time to all three levels, memorization, and performance. Although the architecture presented in this paper is optimized for robotic workloads, but has a fixed and limited computing resource budget, users ultimately have to discard more and more data and tolerate longer response times."}, {"heading": "5.1 Data Reduction Techniques", "text": "Data reduction techniques, such as random sampling, can be a viable solution to address the above problem on robotic clouds, especially when small errors are tolerated. For example, when data collected by robots is used to estimate traffic volume / speed or to learn trajectory patterns, small errors in results are usually inevitable due to noise in the initial data collection and learning phases. To achieve this, we use approximate query processing (AQP) techniques that have been studied for decades mainly in relation to relational databases. AQP aims to provide approximate responses to a subclass of SQL queries with interactive response times and estimated error limits."}, {"heading": "5.2 Usages in Robotic Clouds", "text": "Below are our preliminary suggestions. \u2022 Sampling before learning. We can reduce the amount of raw data collected by users by placing samplers in client devices or the streaming server. \u2022 Sampling before saving the raw data is selected only with a certain probability determined by some lightweight calculations based on the metadata of the tuple, such as time and location. \u2022 Sampling before saving information is extracted from the raw data tuples, and we can only precalculate and store a subsample in the key value memory. For example, a set < sessionID, timestamp, location, label > > can be placed in memory with a probability determined by the \"list of labels.\""}, {"heading": "6. CONCLUSIONS", "text": "Unlike existing mobile devices, robots generate vast amounts of unstructured data that machines cannot easily understand, store, and retrieve. To address this problem, we have proposed a paradigm for robotic clouds in this paper: the learning phase extracts semantic information from incoming unstructured data and converts it into structured data; the storage phase provides effective storage for the vast amount of data; the retrieval phase provides efficient means to retrieve the raw data; and the reduction phase provides means to make meaningful use of an enormous amount of unstructured data with limited computing resources."}, {"heading": "7. REFERENCES", "text": "This year, it has reached the stage where it will be able to take the lead, in the same way as it has done in the past."}], "references": [{"title": "Implementing a Cloud Platform for Autonomous Driving", "author": ["S. Liu", "J. Tang", "C. Wang", "Q. Wang", "J-L. Gaudiot"], "venue": "arXiv preprint arXiv:1704.02696,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Using predictive prefetching to improve world wide web latency", "author": ["V.N. Padmanabhan", "J.C. Mogul"], "venue": "Computer Communications Review,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "arXiv preprint arXiv:1506.01497,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "MongoDB: the definitive guide", "author": ["K. Chodorow"], "venue": "O\u2019Reilly Media Inc,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Reliable, Memory Speed Storage for Cluster Computing Frameworks", "author": ["H. Li", "A. Ghodsi", "M. Zaharia", "S. Shenker", "I. Stoica"], "venue": "In Proc. SoCC,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "HDFS architecture guide", "author": ["D. Borthakur"], "venue": "HADOOP APACHE PROJECT", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Ceph: A Scalable, High-Performance Distributed File System", "author": ["S. Weil", "S. Brandt", "E. Miller", "D. Long", "C. Maltzahn"], "venue": "Proceedings of the 7th Conference on Operating Systems Design and Implementation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Sample + Seek: Approximating Aggregates with Distribution Precision Guarantee", "author": ["B. Ding", "S. Huang", "S. Chaudhuri", "K. Chakrabarti", "C. Wang"], "venue": "In Proc. SIGMOD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Quickr: Lazily Approximating Complex Ad-Hoc Queries in Big Data Clusters", "author": ["S. Kandula", "A. Shanbhag", "A. Vitorovic", "M. Olma", "R. Grandl", "S. Chaudhuri", "B. Ding"], "venue": "In Proc. SIGMOD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Approximate Query Processing: No Silver Bullet", "author": ["S. Chaudhuri", "B. Ding", "S. Kandula"], "venue": "In Proc. SIGMOD,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "BlinkDB: Queries with Bounded Errors and Bounded Response", "author": ["Agarwal", "B. Mozafari", "A. Panda", "H. Milner", "S. Madden", "I. Stoica"], "venue": "Times on Very Large Data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "An extreme form of robot, driverless cars, can generate as much as 2 GB of unstructured data per second [1] (unstructured data means data that cannot be easily understood, stored, and retrieved by machines).", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Therefore, in this implementation, we utilize faster r-cnn [3] network running on Caffe [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "Therefore, in this implementation, we utilize faster r-cnn [3] network running on Caffe [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "We implemented the key-value store using MongoDB [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 5, "context": "To this end, Alluxio enables effective data management across different storage systems through its use of transparent naming and mounting API [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "An effective approach to improve this situation is to use prefetching [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 11, "context": ", BlinkDB [13], QuickR [11], and Sample+Seek [10].", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": ", BlinkDB [13], QuickR [11], and Sample+Seek [10].", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": ", BlinkDB [13], QuickR [11], and Sample+Seek [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "Interested readers can refer to [12] for a brief survey of the progresses.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "A number of sampler can be borrowed from AQP systems [12] to provide estimates of answers and error bounds for such queries.", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "The rise of robotic applications has led to the generation of a huge volume of unstructured data, whereas the current cloud infrastructure was designed to process limited amounts of structured data. To address this problem, we propose a learn-memorize-recall-reduce paradigm for robotic cloud computing. The learning stage converts incoming unstructured data into structured data; the memorization stage provides effective storage for the massive amount of data; the recall stage provides efficient means to retrieve the raw data; while the reduction stage provides means to make sense of this massive amount of unstructured data with limited computing resources.", "creator": "Word"}}}