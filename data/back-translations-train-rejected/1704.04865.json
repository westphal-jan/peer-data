{"id": "1704.04865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking", "abstract": "Traditional generative adversarial networks (GAN) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution. A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs, and alleviate the gradient vanishing, instability, and mode collapse issues that are common in the GAN training. In this work, we aim at improving on the WGAN by first generalizing its discriminator loss to a margin-based one, which leads to a better discriminator, and in turn a better generator, and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages. We call this method Gang of GANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN. We have also proposed a new way of measuring GAN quality which is based on image completion tasks. We have evaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10, and 50K-SSFF, and have seen both visual and quantitative improvement over baseline WGAN.", "histories": [["v1", "Mon, 17 Apr 2017 04:42:56 GMT  (8407kb,D)", "http://arxiv.org/abs/1704.04865v1", "16 pages. 11 figures"]], "COMMENTS": "16 pages. 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["felix juefei-xu", "vishnu naresh boddeti", "marios savvides"], "accepted": false, "id": "1704.04865"}, "pdf": {"name": "1704.04865.pdf", "metadata": {"source": "CRF", "title": "Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking", "authors": ["Felix Juefei-Xu", "Vishnu Naresh Boddeti", "Marios Savvides"], "emails": ["felixu@cmu.edu", "vishnu@msu.edu", "msavvid@ri.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2. Related Work", "text": "In fact, most of us are able to survive on our own by putting ourselves in the role of outsider."}, {"heading": "3. Proposed Method: Gang of GANs", "text": "In this section we will review the original GAN [10] and its revolutionary variant DCGAN [28], analyze how the GAN model can be further improved with WGAN [3], and present our Gang of GANs (GoGAN) method."}, {"heading": "3.1. GAN and DCGAN", "text": "The GAN [10] framework forms two networks, a generator G (z): z \u2192 x and a discriminator D (x): x \u2192 [0, 1]. G maps a random vector z scanned from a previous pz (z) distribution onto the image space. D maps an input image with a high probability. G's purpose is to generate realistic images, while D plays the opposite role in distinguishing between the image generated from G and the image sampled from the data of the data distribution. D trains the networks by optimizing the following miniloss function: minG max D V (G, D) = Ex-pdata (x) [log (D (x)] + Ez-pz (z) [log (1 \u2212 D (G (z))]], where x is the sample from the pdata distribution; z is randomly generated and lies in a latent space. There are many ways to structure G (z)."}, {"heading": "3.2. Wasserstein GAN and Improvement over GAN", "text": "In the original version of JS divergence, there will always be a constant 2, which allows an almost optimal minimization of JS divergence between Pr and Pg, but due to the aforementioned reasons, the JS divergence will result in two optimal loss functions, which will minimize JS divergence between Pr and Pg almost optimally, but due to the aforementioned reasons, the JS divergence between Pr and Pg will always be a constant 2-fold differentiation of JS divergence."}, {"heading": "3.3. Gang of GANs (GoGAN)", "text": "In this section, we will discuss our proposed GoGAN method, which is a progressive training paradigm for improving the GAN by allowing GANs in later stages to contribute to a new ranking loss function that will further improve GAN performance. Also, we will generalize at each GoGAN level based on the WGAN discrimination loss and arrive at a marginalized discrimination loss, and we will call up the network margin GAN (MGAN), which represents the entire GoGAN flowchart in Figure 2, and we will involve the components. Based on the previous discussion, we have seen that WGAN has several advantages over the traditional GAN. Recall that Dwi (x) and Dwi (z) are the discrimination results for the real image x and the generated image Gig i (z) in Stage (i + 1) GoGAN. To further improve it, we have proposed a margin-based GAN discrimination loss."}, {"heading": "4. Theoretical Analysis", "text": "In the WGAN, the following loss function, which includes the weight update of the discriminator and the generator, is used for the optimal distribution of 1 million euros. (<) This loss function is essentially the gap between the real data distribution and the generated data distribution, and of course the discriminator tries to increase the gap. (The realization of this loss function for a group is as follows: Gap = 1 million euros (1). (1) This loss function is essentially the gap between the real data distribution and the generated data distribution, and of course the discriminator tries to increase the gap. (1) The realization of this loss function for a group is as follows: Gap = 1 million euros (1). (1) This gap reduces the gap between the real data distribution Pr and the generated data distribution. (1) We will reduce the gap between the real data distribution Pr and the generated data distribution by at least half."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Evaluating GANs via Image Completion Tasks", "text": "There is no universal metric for quantitatively evaluating GAN performance, and we often rely on visual examination, largely due to the lack of an objective function: how are the generated images compared since there are no corresponding basic truth images for the generated ones? These are the questions that need to be answered. During the WGAN training, we saw a successful gap indicator that correlates well with the image quality. However, it is highly dependent on the respective WGAN model on which it is based, and it will be difficult to evaluate fairly the image quality generated in various WGAN models. We need a measurement that is independent and does not depend on the models. Perhaps, the Inception score [29] is by far the best solution we have. The score is based on pre-formed conception models. Images are passed by the model, and those that contain meaningful objects have a conditional label distribution with a low label pie."}, {"heading": "5.2. Details on the Image Completion Tasks", "text": "As discussed above, we propose to use image completion tasks as a quality benchmark for various GAN models. In short, the quality of the GAN models can be measured quantitatively by image completion fidelity, in terms of PSNR and SSIM. The motivation is that image completion tasks require both discriminator D and generator G to work well to achieve high-quality image completion results, as we will see. To address the lack of data challenge as well as image completion tasks, we need to use both the G and D networks from the GoGAN (and its benchmark WGAN), pre-trained with uncorrupted data. After training, G \u2212 olted is able to embed the images from the percorrupt data into some non-linear manifolds of z. An image that does not consist of pdata (e.g. images with missing pixels) should not lie on the learned manifolds."}, {"heading": "5.3. Methods to be Evaluated and Dataset", "text": "The WGAN baseline uses the Waterstone discriminator loss [3]. The MGAN uses margin-based discriminator loss function discussed in Section 3.3. It is precisely the Stage1 GoGAN dataset, which represents a baseline for subsequent GoGAN levels. Level-2 GoGAN contains margin-based rank losses discussed in Section 3.3. These 3 methods are evaluated using three large-area visual datasets. CelebA dataset [20] is a large-area dataset with facial attributes with more than 200K celebrity images. The images in this dataset cover large pose variations and background disorder. The dataset includes 10,177 number of subjects and 202,599 number of facial images. We process and align the facial images using the dLib images provided by the OpenFace [1]. The LSUN dataset 37 [is intended for large-area aircraft data classes, 364 aircraft aircraft, 342 aircraft, 364 aircraft, 364 aircraft, 342 aircraft, 364."}, {"heading": "5.4. Training Details of GoGAN", "text": "We use the DCGAN [28] architecture for both the generator and discriminator in all phases of the training. Both the generator and discriminator are learned with optimizers (RMSprop [31]) that are not based on impulses, as recommended in [3], with a learning rate of 5e-5. To learn the model in level 2, we initialize it with the model we learned in level 1. In the second stage, the model is updated with the ranking loss, while the model was held in level one. Finally, no data augmentation was used for any of our experiments. Different GoGAN phases are trained with the same number of total eras for a fair comparison. We will make our implementation publicly available so that readers can refer to it for more detailed hyperparameters, timing, etc."}, {"heading": "5.5. Results and Discussion", "text": "Figure 4 shows this effect of our proposed approach, which reduces the gap between the discriminator values between the actual distribution and the generated distribution from level 1 to level 2. To quantify the effectiveness of our approach, we look at the task of image completion, i.e. the lack of data imputation by the generated model. This task is evaluated using three different visual datasets by varying the amount of missing data. We look at five different obscurity levels that cover the central square region (9%, 25%, 49%, 64% and 81%) of the image. The task of image completion is evaluated by measuring the accuracy between the generated images and the underlying images using two metrics: PSNR and SSIM. The results are consolidated in Tables 2, 3 and 4 for the 3 datasets. GoGAN consistently performs better than the results of the GAN-1, which we demonstrate using the results of the GAN model."}, {"heading": "5.6. Ablation Studies", "text": "In this section we offer additional experiments and degradation studies of the proposed GoGAN method and show its improvement over its predecessors. For this type of experiments we collect a single dataset of 50K frontal images of 50K individuals, which we obtain from several frontal images."}, {"heading": "6. Conclusions", "text": "To improve the WGAN, we first generalize its discriminator loss to a margin-based loss that results in a better discriminator and thus a better generator, and then implement a progressive training paradigm that includes multiple GANs to contribute to the maximum margin ranking loss, so that the GAN will improve in the later stages in the early stages. We have theoretically shown that the proposed GoGAN can reduce the gap between the actual data distribution and the data distribution generated in an optimally trained WGAN by at least half. We have also proposed a new method of measuring GAN quality based on image completion tasks. We evaluated our method using four visual data sets: CelebA, LSUN Bedroom, CIFAR-10 and 50K-SSFF, and have seen both visual and quantitative improvements over the baseline WGAN."}], "references": [{"title": "Openface: A general-purpose face recognition library with mobile applications", "author": ["B. Amos", "L. Bartosz", "M. Satyanarayanan"], "venue": "Technical report, CMU-CS-16-118, CMU School of Computer Science", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards principled methods for training generative adversarial networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "ICLR (under review)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "Neural photo editing with introspective adversarial networks", "author": ["A. Brock", "T. Lim", "J. Ritchie", "N. Weston"], "venue": "arXiv preprint arXiv:1609.07093", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1606.03657", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pages 1486\u20131494", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "arXiv preprint arXiv:1605.09782", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarially learned inference", "author": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"], "venue": "arXiv preprint arXiv:1606.00704", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative multiadversarial networks", "author": ["I. Durugkar", "I. Gemp", "S. Mahadevan"], "venue": "arXiv preprint arXiv:1611.01673", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pages 2672\u20132680", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-pie", "author": ["R. Gross", "I. Matthews", "J. Cohn", "T. Kanade", "S. Baker"], "venue": "Image and Vision Computing, 28(5):807\u2013813", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Boundary-seeking generative adversarial networks", "author": ["R.D. Hjelm", "A.P. Jacob", "T. Che", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1702.08431", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Stacked generative adversarial networks", "author": ["X. Huang", "Y. Li", "O. Poursaeed", "J. Hopcroft", "S. Belongie"], "venue": "arXiv preprint arXiv:1612.04357", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating images with recurrent adversarial networks", "author": ["D.J. Im", "C.D. Kim", "H. Jiang", "R. Memisevic"], "venue": "arXiv preprint arXiv:1602.05110", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Imageto-image translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "arXiv preprint arXiv:1611.07004", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "CIFAR-10 Database", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Cunningham", "A. Acosta", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang"], "venue": "Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y. Lau", "Z. Wang"], "venue": "arXiv preprint arXiv:1611.04076", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["A. Nguyen", "J. Yosinski", "Y. Bengio", "A. Dosovitskiy", "J. Clune"], "venue": "arXiv preprint arXiv:1612.00005", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["S. Nowozin", "B. Cseke", "R. Tomioka"], "venue": "arXiv preprint arXiv:1606.00709", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["A. Odena", "C. Olah", "J. Shlens"], "venue": "arXiv preprint arXiv:1610.09585", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Distinguishing identical twins by face recognition", "author": ["P.J. Phillips", "P.J. Flynn", "K.W. Bowyer", "R.W.V. Bruegge", "P.J. Grother", "G.W. Quinn", "M. Pruitt"], "venue": "Automatic Face & Gesture Recognition and Workshops ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Overview of the face recognition grand challenge", "author": ["P.J. Phillips", "P.J. Flynn", "T. Scruggs", "K.W. Bowyer", "J. Chang", "K. Hoffman", "J. Marques", "J. Min", "W. Worek"], "venue": "Computer vision and pattern recognition, 2005. CVPR 2005. IEEE computer society conference on, volume 1, pages 947\u2013954. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised representation learning with deep convolutional generative adver-  sarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "Advances in Neural Information Processing Systems, pages 2226\u20132234", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised crossdomain image generation", "author": ["Y. Taigman", "A. Polyak", "L. Wolf"], "venue": "arXiv preprint arXiv:1611.02200", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.05328,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Optimal transport: old and new", "author": ["C. Villani"], "venue": "volume 338. Springer Science & Business Media", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Lr-gan - layered recursive generative adversarial networks for image generation", "author": ["J. Yang", "A. Kannan", "B. Batra", "D. Parikh"], "venue": "ICLR (under review)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "Semantic image inpainting with perceptual and contextual losses", "author": ["R. Yeh", "C. Chen", "T.Y. Lim", "M. Hasegawa-Johnson", "M.N. Do"], "venue": "arXiv preprint arXiv:1607.07539", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "arXiv preprint arXiv:1506.03365", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Energy-based generative adversarial network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "arXiv preprint arXiv:1609.03126", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Variational autoencoders (VAEs) [17] formalize the generative problem in the framework of probabilistic graphical models where we are to maximize a lower bound on the log likelihood of the training data.", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Autoregressive models such as PixelRNN [32] and PixelCNN [33] get rid of the latent variables and instead directly model the conditional distribution of every individual pixel given previous pixels starting from top-left corner.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "Autoregressive models such as PixelRNN [32] and PixelCNN [33] get rid of the latent variables and instead directly model the conditional distribution of every individual pixel given previous pixels starting from top-left corner.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "Generative adversarial networks (GANs) [10] simultaneously train a generator network for generating realistic images, and a discriminator network for distinguishing between the generated images and the samples from the training data (true distribution).", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Imagine if we have a weak discriminator which does a poor job telling generated images from the true images, it takes only a little effort for the generator to win the two-player minimax game as described in the original work of GAN [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 27, "context": "Deep convolutional generative adversarial networks (DCGAN) [28] are proposed to replace the multilayer perceptron in the original GAN [10] for more stable training, by utilizing strided convolutions in place of pooling layers, and fractional-strided convolutions in place of image upsampling.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "Deep convolutional generative adversarial networks (DCGAN) [28] are proposed to replace the multilayer perceptron in the original GAN [10] for more stable training, by utilizing strided convolutions in place of pooling layers, and fractional-strided convolutions in place of image upsampling.", "startOffset": 134, "endOffset": 138}, {"referenceID": 21, "context": "Conditional GAN [22] is proposed as a variant of GAN by extending it to a conditional model, where both the generator and discriminator are conditioned on some extra", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Another variant of GAN is called auxiliary classifier GAN (AC-GAN) [25], where every generated sample has a corresponding class label in addition to the noise.", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "Bidirectional GAN (BiGAN) [7] is proposed to bridge the gap that conventional GAN does not learn the inverse mapping which projects the data back into the latent space, which can be very useful for unsupervised feature learning.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "There is a concurrent work proposed in [8] that has the identical model.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "A sequential variant of the GAN is the Laplacian generative adversarial networks (LAPGAN) [6] model which generates images in a coarse-to-fine manner by generating and upsampling in multiple steps.", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "It is worth mentioning the sequential variant of the VAE is the deep recurrent attentive writer (DRAW) [11] model that generates images by accumulating updates into a canvas using a recurrent network.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Built upon the idea of sequential generation of images, the recurrent adversarial networks [15] has been proposed to let the recurrent network to learn the optimal generation procedure by itself, as opposed to imposing a coarse-to-fine structure on the procedure.", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "Introspective adversarial network (IAN) [4] is proposed to hybridize the VAE and the GAN.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "The generative multi-adversarial networks (GMAN) [9] extends the GANs to multiple discriminators.", "startOffset": 49, "endOffset": 52}, {"referenceID": 34, "context": "Layered recursive generative adversarial networks (LR-GAN) [35] generates images in a recursive fashion.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "Authors of [24] have shown that the generative-adversarial approach in GAN is a special case of an existing more general variational divergence estimation approach, and that any f divergence can be used for training generative neural samplers.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "InfoGAN [5] method is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation.", "startOffset": 8, "endOffset": 11}, {"referenceID": 37, "context": "To strive for a more stable GAN training, the energy-based generative adversarial networks (EBGAN) [38] is proposed which views the discriminator as an energy function that assigns low energy to the regions near the data manifold and higher energy to other regions.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "The boundary-seeking GAN (BGAN) [13] aims at generating samples that lie on the decision boundary of a current discriminator in training at each update.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "Least squares GAN [21] adopts a least squares loss function for the discriminator, which is equivalent to a multi-class GAN with the `2 loss function.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "The stacked GAN (SGAN) [14] consists of a top-down stack of GANs, each trained to generate plausible lower-level representations, conditioned on higher-level representations.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "Perhaps the most seminal GAN-related work since the inception of the original GAN [10] idea is the Wasserstein GAN (WGAN) [3].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "Perhaps the most seminal GAN-related work since the inception of the original GAN [10] idea is the Wasserstein GAN (WGAN) [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Efforts have been made to fully understand the training dynamics of generative adversarial networks through theoretical analysis [2], which leads to the creation of the WGAN.", "startOffset": 129, "endOffset": 132}, {"referenceID": 29, "context": "Other applications include cross-domain image generation [30] through a domain transfer network (DTN) which employs a compound of loss functions including a multiclass GAN loss, an f -constancy component, and a regularization component that encourages the generator to map samples from the target domain to themselves.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "The imageto-image translation approach [16] is based on conditional GAN, and learns a conditional generative model for generating a corresponding output image at a different domain, conditioned on an input image.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "The image superresolution GAN (SRGAN) [19] combines both the image content loss and the adversarial loss for recovering highresolution counterpart of the low-resolution input image.", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "The plug and play generative networks (PPGN) [23] is able to produce high quality images at higher resolution for all 1000 ImageNet categories.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "In this section we will review the original GAN [10] and its convolutional variant DCGAN [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "In this section we will review the original GAN [10] and its convolutional variant DCGAN [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "We will then analyze how to further improve the GAN model with WGAN [3], and introduce our Gang of GANs (GoGAN) method.", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "The GAN [10] framework trains two networks, a generator G\u03b8(z) : z \u2192 x, and a discriminator D\u03c9(x) : x \u2192 [0, 1].", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "The GAN [10] framework trains two networks, a generator G\u03b8(z) : z \u2192 x, and a discriminator D\u03c9(x) : x \u2192 [0, 1].", "startOffset": 103, "endOffset": 109}, {"referenceID": 27, "context": "The DCGAN [28] uses fractionally-strided convolutions to upsample images instead of fully-connected neurons as shown in Figure 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "[10] have proposed the following two loss functions for the generator: Ez\u223cPz(z)[log(1\u2212D(G(z)))] andEz\u223cPz(z)[\u2212 logD(G(z))].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The latter one is referred to as the \u2212 logD trick [10, 2, 3].", "startOffset": 50, "endOffset": 60}, {"referenceID": 1, "context": "The latter one is referred to as the \u2212 logD trick [10, 2, 3].", "startOffset": 50, "endOffset": 60}, {"referenceID": 2, "context": "The latter one is referred to as the \u2212 logD trick [10, 2, 3].", "startOffset": 50, "endOffset": 60}, {"referenceID": 1, "context": "WGAN [2, 3] avoids the gradient vanishing and mode collapse issues in the original GAN and many of its variants by adopting a new distance metric: the Wasserstein-1 distance, or the earth-mover distance as follows:", "startOffset": 5, "endOffset": 11}, {"referenceID": 2, "context": "WGAN [2, 3] avoids the gradient vanishing and mode collapse issues in the original GAN and many of its variants by adopting a new distance metric: the Wasserstein-1 distance, or the earth-mover distance as follows:", "startOffset": 5, "endOffset": 11}, {"referenceID": 33, "context": "Thanks to the Kantorovich-Rubinstein duality [34], the Wasserstein distance becomes: W (Pr,Pg) = sup\u2016f\u2016L\u22641Ex\u223cPr [f(x)] \u2212 Ex\u223cPg [f(x)], where the supremum is over all the 1Lipschitz functions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "In WGAN [3], the following loss function involving the weights updating of the discriminator and the generator is", "startOffset": 8, "endOffset": 11}, {"referenceID": 28, "context": "Perhaps, the Inception score [29] is by far the best solution we have.", "startOffset": 29, "endOffset": 33}, {"referenceID": 35, "context": "We ask the GANs to carry out image completion tasks [36], and the GAN performance is measured by the fidelity (PSNR, SSIM) of the completed image against its ground-truth.", "startOffset": 52, "endOffset": 56}, {"referenceID": 35, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The WGAN baseline uses the Wasserstein discriminator loss [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "The CelebA dataset [20] is a large-scale face attributes dataset with more than 200K celebrity images.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "We pre-process and align the face images using dLib as provided by the OpenFace [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 36, "context": "The LSUN Bedroom dataset [37] is meant for large-scale scene understanding.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "The CIFAR-10 [18] is an image classification dataset containing a total of 60K 32 \u00d7 32 color images, which are across the following 10 classes: airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships, and trucks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "We use the DCGAN [28] architecture for both the generator and the discriminator at all stages of the training.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "Both the generator and the discriminator are learned using optimizers (RMSprop [31]) that are not based on momentum as recommended in [3] with a learning rate of 5e-5.", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Both the generator and the discriminator are learned using optimizers (RMSprop [31]) that are not based on momentum as recommended in [3] with a learning rate of 5e-5.", "startOffset": 134, "endOffset": 137}, {"referenceID": 26, "context": "0 dataset [27], the MPIE dataset [12], the ND-Twin dataset [26], and mugshot dataset from Pinellas County Sheriff\u2019s Office (PCSO).", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "0 dataset [27], the MPIE dataset [12], the ND-Twin dataset [26], and mugshot dataset from Pinellas County Sheriff\u2019s Office (PCSO).", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "0 dataset [27], the MPIE dataset [12], the ND-Twin dataset [26], and mugshot dataset from Pinellas County Sheriff\u2019s Office (PCSO).", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "celebrity face dataset such as CelebA [20], our collected 50K-SSFF dataset is dedicated for one-shot learning in the GAN context due to its single-sample nature.", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "Traditional generative adversarial networks (GAN) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution. A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs, and alleviate the gradient vanishing, instability, and mode collapse issues that are common in the GAN training. In this work, we aim at improving on the WGAN by first generalizing its discriminator loss to a margin-based one, which leads to a better discriminator, and in turn a better generator, and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages. We call this method Gang of GANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN. We have also proposed a new way of measuring GAN quality which is based on image completion tasks. We have evaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10, and 50K-SSFF, and have seen both visual and quantitative improvement over baseline WGAN.", "creator": "LaTeX with hyperref package"}}}