{"id": "1704.04866", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Effective Warm Start for the Online Actor-Critic Reinforcement Learning based mHealth Intervention", "abstract": "Online reinforcement learning (RL) is increasingly popular for the personalized mobile health (mHealth) intervention. It is able to personalize the type and dose of interventions according to user's ongoing statuses and changing needs. However, at the beginning of online learning, there are usually too few samples to support the RL updating, which leads to poor performances. A delay in good performance of the online learning algorithms can be especially detrimental in the mHealth, where users tend to quickly disengage with apps. To address this problem, we propose a new online RL methodology that focuses on an effective warm start. The main idea is to make full use of the data accumulated and the decision rule achieved in a former study. As a result, we can greatly enrich the data size at the beginning of online learning in our method. Such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process. Besides, we use the decision rules achieved previously to initialize the parameter in our online RL model for new users. It provides a good initialization for the proposed online RL algorithm. Experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method.", "histories": [["v1", "Mon, 17 Apr 2017 04:43:05 GMT  (21kb)", "https://arxiv.org/abs/1704.04866v1", null], ["v2", "Mon, 1 May 2017 04:51:43 GMT  (21kb)", "http://arxiv.org/abs/1704.04866v2", null], ["v3", "Sun, 21 May 2017 21:00:12 GMT  (13kb)", "http://arxiv.org/abs/1704.04866v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["feiyun zhu", "peng liao"], "accepted": false, "id": "1704.04866"}, "pdf": {"name": "1704.04866.pdf", "metadata": {"source": "CRF", "title": "Effective Warm Start for the Online Actor-Critic Reinforcement Learning based mHealth Intervention", "authors": ["Feiyun Zhu", "Peng Liao"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is such that most people who are able to survive themselves are able to survive themselves, to survive themselves, namely in a way in which they survive themselves, in a way in which they survive themselves, in another way in which they survive themselves, in which they survive themselves, in another way in which they survive themselves, in which they survive themselves, in another way in which they survive themselves, in another way in which they survive themselves, in which they survive themselves, in another way in which they survive themselves, in which they live themselves, in another way in which they live themselves, in which they survive themselves, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they are able to survive."}, {"heading": "II. MARKOV DECISION PROCESS (MDP) AND ACTOR-CRITIC REINFORCEMENT LEARNING", "text": "MDP: The dynamic system (i.e. the environment) with which RL interacts is generally modelled as the Markov decision-making process (MDP). [8] An MDP is a tuple {S, A, P, R, \u03b3} [9], [10], [11] where S is the (finite) state space and A is the (finite) space of action. [9] The state transition probability P: S \u00b7 A \u00b7 S 7 \u2192 [0, 1], from state s to next state s \u00b2, when action comes, is given by P (s, a, s \u2032). Let St, At and Rt + 1 be the random variables representing the state, action and immediate reward respectively. The expected immediate reward R (s, a) = E (Rt + 1 | St = s, At = a), it is assumed that the decision on the state and the spaces of action will be limited [2]."}, {"heading": "III. OUR METHOD", "text": "The Actor Criticist RL still has a small number of examples to set the Actor Criticism in motion. This problem concerns only the Actor Criticism. (i.e. t) The Actor Criticism is large. (i.e. t) The Actor Criticism is updated with so few samples. (i.e. t) A popular and generally accepted method is to accumulate a small number of tuples via micro-randomized studies. (i.e.) The Actor Criticism of Actor Criticism is provided by the application of random policies with the likelihood of intervention. (i.e.) The Actor Criticism of Actor Criticism is very high. (i.e.) The Actor Criticism of Actor Criticism is very high. (i.e.) The Actor Criticism of Actor Criticism is very high."}, {"heading": "IV. EXPERIMENTS", "text": "To verify performance, we compare our method (i.e. NWS-RL) with the conventional RL method with the random warm start (RWS-RL) on the HeartSteps application [3]. HeartSteps is a 42-day mHealth intervention that encourages users to increase the steps they take each day with positive interventions, such as a walk after sedentary behavior. The 7 actions are binary, including {0, 1}, where a = 1 means providing active treatments, such as sending an intervention to the user's smart device, while a = 0 means no treatment [2]."}, {"heading": "A. Simulated Experiments", "text": "In the experiments, we generate T-tuples from each user, i.e., DT = {(S0, A0, R0), (S1, A1, R1), \u00b7 \u00b7 \u00b7, (ST, AT, RT)}, where the observation St is a column vector with p-elements. Initial states and actions are generated by S0 \u00b2 Normalp {0, \u03a3} and A0 = 0, where V = 1 00 Ip \u2212 3 and V = 1 0.3 \u2212 0.3 \u2212 0.3 \u2212 0.3 1. For t \u2265 1, we have the state generation model and the immediate reward model as a follow-up St, 1 = \u03b21St \u2212 1.1 + 1, St, 2 = \u03b22St \u2212 1.2 + \u03b23At \u2212 1 +, (6) St, 3 = \u03b24St \u2212 1.3 + \u03b25St \u2212 1.3At \u2212 1.3At \u2212 St, and St, J = 7 times \u2212 1.2 + \u03b23At \u2212 t, (Normal, 3 = \u03b24), Normal + 14 + \u03b23 (Normal), + 13.3."}, {"heading": "B. Experiment Settings", "text": "The expectation of a long-term average reward (ElrAR) E = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 3 = 3 = 3 = 4 = 4 = 4 = 4 = 4 = 5 = 5 = 5 = 5 = 5 The average reward is calculated by applying the rewards of the last 4, 000 elements in a path of 5, 000 tuples over a long period of time; a larger ElrAR corresponds to a better performance. The average reward is calculated by applying the rewards of the last 4, 000 elements in a path of 5, 000 tuples in which the policy begins."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank the editor and reviewers for their valuable suggestions. In addition, this work is supported by R01 AA023187, P50 DA039838, U54EB020404, R01 HL125440."}], "references": [{"title": "An actor-critic contextual bandit algorithm for personalized interventions using mobile devices", "author": ["H. Lei", "A. Tewari", "S. Murphy"], "venue": "NIPS 2014 Workshop: Personalization: Methods and Applications, pp. 1 \u2013 9, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A batch, off-policy, actor-critic algorithm for optimizing the average reward", "author": ["S.A. Murphy", "Y. Deng", "E.B. Laber", "H.R. Maei", "R.S. Sutton", "K. Witkiewitz"], "venue": "CoRR, vol. abs/1607.05047, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Randomised trials for the fitbit generation", "author": ["W. Dempsey", "P. Liao", "P. Klasnja", "I. Nahum-Shani", "S.A. Murphy"], "venue": "Significance, vol. 12, pp. 20 \u2013 23, Dec 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Constructing just-in-time adaptive interventions", "author": ["P. Liao", "A. Tewari", "S. Murphy"], "venue": "Phd Section Proposal, pp. 1\u201349, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Cohesion-based online actor-critic reinforcement learning for mhealth intervention", "author": ["F. Zhu", "P. Liao", "X. Zhu", "Y. Yao", "J. Huang"], "venue": "arXiv:1703.10039, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention", "author": ["H. Lei"], "venue": "PhD thesis, University of Michigan,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "A smartphone application to support recovery from alcoholism: a randomized clinical trial", "author": ["D. Gustafson", "F. McTavish", "M. Chih", "A. Atwood", "R. Johnson", "M.B. ...", "D. Shah"], "venue": "JAMA Psychiatry, vol. 71, no. 5, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Algorithmic survey of parametric value function approximation", "author": ["M. Geist", "O. Pietquin"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, no. 6, pp. 845\u2013867, 2013.  10", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of actor-critic reinforcement learning: Standard and natural policy gradients", "author": ["I. Grondman", "L. Busoniu", "G.A.D. Lopes", "R. Babuska"], "venue": "IEEE Trans. Systems, Man, and Cybernetics, vol. 42, no. 6, pp. 1291\u20131307, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "J. of Machine Learning Research (JLMR), vol. 4, pp. 1107\u2013 1149, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "10,000+ times accelerated robust subset selection (ARSS)", "author": ["F. Zhu", "B. Fan", "X. Zhu", "Y. Wang", "S. Xiang", "C. Pan"], "venue": "Proc. Assoc. Adv. Artif. Intell. (AAAI), pp. 3217\u20133224, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust hyperspectral unmixing with correntropy-based metric", "author": ["Y. Wang", "C. Pan", "S. Xiang", "F. Zhu"], "venue": "IEEE Transactions on Image Processing, vol. 24, no. 11, pp. 4027\u20134040, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured sparse method for hyperspectral unmixing", "author": ["F. Zhu", "Y. Wang", "S. Xiang", "B. Fan", "C. Pan"], "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 88, pp. 101\u2013118, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A label propagation method using spatial-spectral consistency for hyperspectral image classification", "author": ["H. Li", "Y. Wang", "S. Xiang", "J. Duan", "F. Zhu", "C. Pan"], "venue": "International Journal of Remote Sensing, vol. 37, no. 1, pp. 191\u2013211, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].", "startOffset": 205, "endOffset": 208}, {"referenceID": 1, "context": ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].", "startOffset": 210, "endOffset": 213}, {"referenceID": 2, "context": ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].", "startOffset": 215, "endOffset": 218}, {"referenceID": 3, "context": ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].", "startOffset": 220, "endOffset": 223}, {"referenceID": 1, "context": ") to deliver effective treatments that target behavior regularization [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": ", Just in time adaptive intervention) is especially interesting and practical due to the appealing properties [1]: (1) JITAIs could make adaptive and efficacious interventions according to user\u2019s ongoing statuses and changing needs; (2) JITAIs allow for the real-time delivery of interventions, which is very portable, affordable and flexible [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": ", Just in time adaptive intervention) is especially interesting and practical due to the appealing properties [1]: (1) JITAIs could make adaptive and efficacious interventions according to user\u2019s ongoing statuses and changing needs; (2) JITAIs allow for the real-time delivery of interventions, which is very portable, affordable and flexible [5].", "startOffset": 343, "endOffset": 346}, {"referenceID": 3, "context": ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "Normally, JITAIs is formed as an online sequential decision making (SDM) problem that is aimed to construct the optimal decision rules to decide when, where and how to deliver effective treatments [4], [2], [5].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "Normally, JITAIs is formed as an online sequential decision making (SDM) problem that is aimed to construct the optimal decision rules to decide when, where and how to deliver effective treatments [4], [2], [5].", "startOffset": 202, "endOffset": 205}, {"referenceID": 4, "context": "Normally, JITAIs is formed as an online sequential decision making (SDM) problem that is aimed to construct the optimal decision rules to decide when, where and how to deliver effective treatments [4], [2], [5].", "startOffset": 207, "endOffset": 210}, {"referenceID": 0, "context": "In 2014, Lei [1] made a first attempt to formulate the mHealth intervention as an online actor-critic contextual bandit problem.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "However, this method ignores the important delayed effects of the SDM\u2014the current action may affect not only the immediate reward but also the next states and, through that, all subsequent rewards [8].", "startOffset": 197, "endOffset": 200}, {"referenceID": 4, "context": "It requires much more data to acquire good and stable decision rules [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "A simple and widely used method is to collect a fixed length of trajectory (T0 = 10, say) via the micro-randomized trials [4], accumulating a few of samples, then starting the online updating.", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "the environment) that RL interacts with is generally modeled as a Markov Decision Process (MDP) [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "An MDP is a tuple {S,A, P, R, \u03b3} [9], [10], [11], where S is (finite) state space and A is (finite) action space.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "An MDP is a tuple {S,A, P, R, \u03b3} [9], [10], [11], where S is (finite) state space and A is (finite) action space.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "An MDP is a tuple {S,A, P, R, \u03b3} [9], [10], [11], where S is (finite) state space and A is (finite) action space.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "The state transition probability P : S\u00d7A\u00d7S 7\u2192 [0, 1], from state s to the next state s when taking action a, is given by P (s, a, s).", "startOffset": 46, "endOffset": 52}, {"referenceID": 1, "context": "The expected immediate reward R (s, a) = E (Rt+1 | St = s, At = a) is assumed to be bounded over the state and action spaces [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 8, "context": "The parameterized functions are generally employed to approximate the value and policy functions since [9] the system usually have too many states and actions to achieve an accurate estimation of value and policy.", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "Due to the great properties of quick convergences [10], the actor-critic RL algorithms are widely accepted to esimate the parameterized value Q w (s, a) = wx (s, a) \u2248 Q and stochastic policy \u03c0\u03b8 (\u00b7 | s) \u2248 \u03c0 \u2217 (\u00b7 | s), where x (s, a) is a feature function for the Q-value that merges the information in state s and action a.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": ", policy improvement) for \u03b8 to search a better policy based on the newly estimated Q-value [10], [4].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": ", policy improvement) for \u03b8 to search a better policy based on the newly estimated Q-value [10], [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "By using the data in D, the Least-Squares Temporal Difference for Q-value (LSTDQ) [11], [8] is used for the critic updating to estimate \u0175t at time point t: \u0175t = [ \u03b6cI+ 1 t t \u2211", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "By using the data in D, the Least-Squares Temporal Difference for Q-value (LSTDQ) [11], [8] is used for the critic updating to estimate \u0175t at time point t: \u0175t = [ \u03b6cI+ 1 t t \u2211", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": ", a widely accepted criterion [10], we have the objective function for the actor updating (i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "When the discount factor \u03b3 = 0, the RL algorithm in (4), (5) is equivalent to the state-of-the-art contextual bandit method in the mHealth [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "A popular and widely accepted method is to accumulate a small number of tuples via the micro-randomized trials [4] (called RWS).", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": ", NWS-RL) with the conventional RL method with the random warm start (RWS-RL) on the HeartSteps application [3].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": ", sending an intervention to the user\u2019s smart device, while a = 0 means no treatment [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "where \u2212\u03b213Ot,3 is the treatment fatigue [4], [2]; {\u03bet,i} p i=1 \u223c Normal (0, \u03c3 s) at the t th point is the noise in the state transition (6) and \u033at \u223c Normal (0, \u03c3 2 r) is the noise in the immediate reward model (7).", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "where \u2212\u03b213Ot,3 is the treatment fatigue [4], [2]; {\u03bet,i} p i=1 \u223c Normal (0, \u03c3 s) at the t th point is the noise in the state transition (6) and \u033at \u223c Normal (0, \u03c3 2 r) is the noise in the immediate reward model (7).", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "The value of \u03b3 specifies different RL methods: (a) \u03b3 = 0 means the contextual bandit [1], (b) 0 < \u03b3 < 1 indicates the discounted reward RL.", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "Online reinforcement learning (RL) is increasingly popular for the personalized mobile health (mHealth) intervention. It is able to personalize the type and dose of interventions according to user\u2019s ongoing statuses and changing needs. However, at the beginning of online learning, there are usually too few samples to support the RL updating, which leads to poor performances. A delay in good performance of the online learning algorithms can be especially detrimental in the mHealth, where users tend to quickly disengage with the mHealth app. To address this problem, we propose a new online RL methodology that focuses on an effective warm start. The main idea is to make full use of the data accumulated and the decision rule achieved in a former study. As a result, we can greatly enrich the data size at the beginning of online learning in our method. Such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process. Besides, we use the decision rules achieved in a previous study to initialize the parameter in our online RL model for new users. It provides a good initialization for the proposed online RL algorithm. Experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method.", "creator": "LaTeX with hyperref package"}}}