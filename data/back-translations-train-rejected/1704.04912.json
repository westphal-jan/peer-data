{"id": "1704.04912", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Pseudorehearsal in actor-critic agents", "abstract": "Catastrophic forgetting has a serious impact in reinforcement learning, as the data distribution is generally sparse and non-stationary over time. The purpose of this study is to investigate whether pseudorehearsal can increase performance of an actor-critic agent with neural-network based policy selection and function approximation in a pole balancing task and compare different pseudorehearsal approaches. We expect that pseudorehearsal assists learning even in such very simple problems, given proper initialization of the rehearsal parameters.", "histories": [["v1", "Mon, 17 Apr 2017 09:27:52 GMT  (6kb)", "http://arxiv.org/abs/1704.04912v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marochko vladimir", "leonard johard", "manuel mazzara"], "accepted": false, "id": "1704.04912"}, "pdf": {"name": "1704.04912.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["v.marochko@innopolis.ru,", "l.johard@innopolis.ru,", "m.mazzara@innopolis.ru"], "sections": [{"heading": null, "text": "In fact, the agent has reached a state that is significantly better than the previous one in terms of the given task or successfully completed task, then the reward is positive. If the agent has reached a state in which the task is clearly worse or not completed, then the reward is negative. If the agent has reached a state in which the task is completed, then the reward is negative. Reinforcement learning algorithms can be divided into two different classes to choose long-term optimal measures. It is based on generating an environmental model and evaluating the states in this model to predict the future reward."}, {"heading": "II. THEORETICAL ISSUES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Actor-critic algorithm", "text": "Actor-critic approaches achieve high performance because they require a minimal calculation to select actions - if policies are explicitly stored, no calculations are required for action selection. Actor-critic can also learn an explicitly stochastic policy that is very useful for continuous learning problems that are common for enhanced learning. Actor-critic algorithms use the actor to select actions based on the current state. Actor is typically a political gradient function, which means that political gradient actors can take the form of a neural network and construct the distribution of probabilities of action, even if they are selected at each step. Political gradient methods have good convergence characteristics, so that they can achieve their optimum quickly, but they do not store information about the environment. This means that their performance is limited. Critics learn about and appreciate the policies currently pursued by the actor, probabilities. It offers a critique that takes the form of both an actor-learning and a fault-D."}, {"heading": "B. Catastrophic forgetting", "text": "Catastrophic forgetting is a common problem in neural networks. It occurs when a neural network that has learned to perform some tasks properly fulfills changed conditions or learns the new task. This neural network learns the new task without any problems, but the old information could be almost completely erased. This may not be a serious problem with monitored learning tasks such as digit recognition, where networks have been trained and never retrained, but reinforcement learning, where learning in networks repeats itself on a regular basis, for example, every episode or even every step - catastrophic forgetting can seriously damage performance. [3] When learning online in continuous space, things get worse. In this case, networks rarely get real feedback and most of their learning processes are based on assumptions. Even a small noise after a thousand steps can cause the network to forget everything it has learned."}, {"heading": "C. Pseudorehearsal", "text": "One of the methods or solution of catastrophic forgetting is pseudorearsal. It uses a two-step process: the first step is to construct the set of pseudo-patters and the second is to train the network on pseudo-patters combined in lots of real patterns. The common way of constructing pseudo-patters is to create a set of pseudo-vectors, feed them through the network and store the results at each level. This approach saves memory compared to trial-based approaches because no real examples of catastrophic forgetting need to be kept. This approach does not involve changes in the network structure as some other approaches do. In practice, the generated approximations of the real data are sufficiently precise to reduce forgetting. Even extremely crude generative models have proven to be highly effective. In the original work in this area by [5], pure noise fed into the network is almost completely catastrophic interferences are eliminated."}, {"heading": "III. EXPERIMENTAL DESIGN", "text": "In fact, most of them are able to go to another world, in which they are able to go to another world, in which they are able to go to another world, in which they are able to go to where they are."}, {"heading": "IV. CONCLUSION", "text": "This work will show us whether the actor-critic algorithm is susceptible to catastrophic forgetting and how good it is to solve this problem in the case of two interacting neural networks pseudo-earsal. We will find the best possible parameters of pseudo-earsal approaches and try to find a dependency between the pseudo-earsal parameters and the performance of the agent and explain this dependence. If the pseudo-earsal algorithm succeeds in successfully eliminating the catastrophic forgetting of continuous actor-critic algorithms, many amplification learning tasks will be easier to solve and agents will react quickly to the immediate changes in the environment."}], "references": [{"title": "Pseudorehearsal in value function approximation", "author": ["V. Marochko", "L. Johard", "M. Mazzara"], "venue": "11th KES International Conference, KES- AMSTA 2017 Vilamoura, Algarve, Portugal, June 2017 Proceedings, 2017.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Policy gradient vs. value function approximation: A reinforcement learning shootout", "author": ["J. Beitelspacher", "J. Fager", "G. Henriques", "A. McGovern"], "venue": "School of Computer Science, University of Oklahoma, Tech. Rep, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Catastrophic Forgetting in Reinforcement-Learning Environments", "author": ["A. Cahill"], "venue": "PhD thesis, University of Otago,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Catastophic forgetting in neural networks", "author": ["O.-M. Moe-Helgesen", "H. Stranden"], "venue": "Dept. Comput. & Information Sci., Norwegian Univ. Science & Technology (NTNU), Trondheim, Norway, Tech. Rep, vol. 1, p. 22, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Catastrophic forgetting, rehearsal and pseudorehearsal", "author": ["A. Robins"], "venue": "Connection Science, vol. 7, no. 2, pp. 123\u2013146, 1995.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Connectionist models of recognition memory: constraints imposed by learning and forgetting functions", "author": ["R. Ratcliff"], "venue": "Psychological review, vol. 97, no. 2, p. 285, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Reinforcement learning in continuous time and space: Interference and not ill conditioning is the main problem when using distributed function approximators", "author": ["B. Baddeley"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 38, no. 4, pp. 950\u2013956, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "[1] and now want to test it on more interesting and complex actor-critic algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] After each action selection critic determines whether things have gone better or worse than expected.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] During the online learning in continuous space things become even worse.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4]", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In the original work in this area by [5], pure noise fed to the network was able to almost completely eliminate catastrophic interference.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [5], supervised learning [6] and reinforcement learning [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "Pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [5], supervised learning [6] and reinforcement learning [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "Pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [5], supervised learning [6] and reinforcement learning [7].", "startOffset": 214, "endOffset": 217}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Catastrophic forgetting has a serious impact in reinforcement learning, as the data distribution is generally sparse and non-stationary over time. The purpose of this study is to investigate whether pseudorehearsal can increase performance of an actor-critic agent with neural-network based policy selection and function approximation in a pole balancing task and compare different pseudorehearsal approaches. We expect that pseudorehearsal assists learning even in such very simple problems, given proper initialization of the rehearsal parameters.", "creator": "LaTeX with hyperref package"}}}