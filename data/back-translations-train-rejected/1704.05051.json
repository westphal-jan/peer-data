{"id": "1704.05051", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2017", "title": "Google's Cloud Vision API Is Not Robust To Noise", "abstract": "Google has recently introduced the Cloud Vision API for image analysis. According to the demonstration website, the API \"quickly classifies images into thousands of categories, detects individual objects and faces within images, and finds and reads printed words contained within images.\" It can be also used to \"detect different types of inappropriate content from adult to violent content.\"", "histories": [["v1", "Sun, 16 Apr 2017 09:47:46 GMT  (3872kb,D)", "http://arxiv.org/abs/1704.05051v1", null], ["v2", "Thu, 20 Jul 2017 05:31:16 GMT  (2724kb,D)", "http://arxiv.org/abs/1704.05051v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hossein hosseini", "baicen xiao", "radha poovendran"], "accepted": false, "id": "1704.05051"}, "pdf": {"name": "1704.05051.pdf", "metadata": {"source": "CRF", "title": "Google\u2019s Cloud Vision API Is Not Robust To Noise", "authors": ["Hossein Hosseini", "Baicen Xiao", "Radha Poovendran"], "emails": ["rp3}@uw.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "III. THE PROPOSED ATTACK ON CLOUD VISION API", "text": "In this section, we describe the attack on the Cloud Vision API. The aim of the attack is to alter a particular image in such a way that the API has completely different outputs from that of the original image, while a human observer perceives its original contents. We conduct the experiments with the various image types, including the text contained in the images. The attack procedure is as follows: We test the API with the text. The API identifies the images, recognizes the faces within the image, and reads the text contained in the images."}, {"heading": "IV. DISCUSSION", "text": "Our experimental results suggest that Google's Cloud Vision API is susceptible to random input errors. Specifically, we have shown that by adding noise, an adversary can deceive the API to output completely irrelevant captions, or not recognize faces, or not identify text within the image. [18] Such a vulnerability undermines the API's applicability in real-world applications. For example, a search engine based on the API may falsely suggest a noise image to users looking for other content. [18] In addition, an adversary can easily bypass a filtering system by adding noise to an image with inappropriate content.The loud images used in our attack can be viewed as a form of contradictory examples. An adverse example is defined as modified input that causes the classifier to output a different captionage while a human observer would recognize its original content."}, {"heading": "V. COUNTERMEASURES", "text": "It has been shown that the robustness of ML algorithms can be improved through the use of regularization or data enlargement during training. In [23], the authors proposed a contradictory training that creates an iterative supply of contradictory examples and incorporates these into the training data. However, approaches based on robust optimization cannot be practicable because the model retrains. For image recognition algorithms, a more practicable approach is the pre-processing of the inputs. Natural images have special properties, such as a high correlation between adjacent pixels, low energy in high frequencies. [12] Noisy inputs are generally not in the same space as natural images."}, {"heading": "VI. CONCLUSION", "text": "In this article, we showed that Google's Cloud Vision API can easily be deceived by an adversary without compromising the system or having any knowledge of the specific details of the algorithms used. In essence, we found that by adding noise, we can always force the API to print false labels or not recognize any face or text within the image. We also showed that when testing the recovered images, the API generates largely the same outputs as the original images, suggesting that the robustness of the system can be slightly improved by applying a noise filter to the inputs without having to update the image recognition algorithms."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pp. 1097\u20131105, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeplysupervised nets", "author": ["C.-Y. Lee", "S. Xie", "P.W. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "AISTATS, vol. 2, p. 5, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar"], "venue": "Proceedings of the 4th ACM workshop on Security and artificial intelligence, pp. 43\u201358, ACM, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372\u2013387, IEEE, 2016. 4", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Concrete problems in ai safety", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Hidden voice commands", "author": ["N. Carlini", "P. Mishra", "T. Vaidya", "Y. Zhang", "M. Sherr", "C. Shields", "D. Wagner", "W. Zhou"], "venue": "25th USENIX Security Symposium (USENIX Security 16), Austin, TX, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. Sharif", "S. Bhagavatula", "L. Bauer", "M.K. Reiter"], "venue": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 1528\u20131540, ACM, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deceiving google\u2019s perspective api built for detecting toxic comments", "author": ["H. Hosseini", "S. Kannan", "B. Zhang", "R. Poovendran"], "venue": "arXiv preprint arXiv:1702.08138, 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Deceiving google\u2019s cloud video intelligence api built for summarizing videos", "author": ["H. Hosseini", "B. Xiao", "R. Poovendran"], "venue": "arXiv preprint arXiv:1703.09793, 2017.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Handbook of image and video processing", "author": ["A.C. Bovik"], "venue": "Academic press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Real-time impulse noise suppression from images using an efficient weighted-average filtering", "author": ["H. Hosseini", "F. Hessar", "F. Marvasti"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 8, pp. 1050\u20131054, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Salt-and-pepper noise removal by median-type noise detectors and detail-preserving regularization", "author": ["R.H. Chan", "C.-W. Ho", "M. Nikolova"], "venue": "IEEE Transactions on image processing, vol. 14, no. 10, pp. 1479\u20131485, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Reliable and fast structureoriented video noise estimation", "author": ["A. Amer", "A. Mitiche", "E. Dubois"], "venue": "Image Processing. 2002. Proceedings. 2002 International Conference on, vol. 1, pp. I\u2013I, IEEE, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013 255, IEEE, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Blocking transferability of adversarial examples in black-box learning systems", "author": ["H. Hosseini", "Y. Chen", "S. Kannan", "B. Zhang", "R. Poovendran"], "venue": "arXiv preprint arXiv:1703.04318, 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards evaluating the robustness of neural networks", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv:1608.04644, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Understanding adversarial training: Increasing local stability of neural nets through robust optimization", "author": ["U. Shaham", "Y. Yamada", "S. Negahban"], "venue": "arXiv preprint arXiv:1511.05432, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572, 2014. 5", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, Machine Learning (ML) techniques have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance [1]\u2013 [3].", "startOffset": 232, "endOffset": 235}, {"referenceID": 2, "context": "In recent years, Machine Learning (ML) techniques have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance [1]\u2013 [3].", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": "Recent research however have pointed out their vulnerability in adversarial environments [5]\u2013[7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Recent research however have pointed out their vulnerability in adversarial environments [5]\u2013[7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "These noise types often occur during image acquisition and transmission [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "Impulse Noise, also known as Salt-and-Pepper Noise, is commonly modeled by [13]:", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "We use the weighted-average filtering method, proposed in [13], for restoring the images corrupted by impulse noise.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "Gaussian noise can be reduced by filtering the input with a low-pass kernel [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "For images of size d1\u00d7d2\u00d73, the PSNR is computed as follows [14]:", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "are usually considered to be between 20 and 40 dB, where higher is better [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "We perform the experiments on different image types, including natural images from the ImageNet dataset [16], images containing faces from the Faces94 dataset [17], and images with text.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "The noisy images used in our attack can be viewed as a form of adversarial examples [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "attacks on learning systems [19], [20], we have no information about the training data or even the set of output labels of the model.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "attacks on learning systems [19], [20], we have no information about the training data or even the set of output labels of the model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Moreover, unlike the current methods for generating adversarial examples [21], we perturb the input completely randomly, which results in a more serious attack vector in real-world applications.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "It has been shown that the robustness of ML algorithms can be improved by using regularization or data augmentation during training [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "In [23], the authors proposed adversarial training, which iteratively creates a supply of adversarial examples and includes them into the training data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "properties, such as high correlation among adjacent pixels, sparsity in transform domain or having low energy in high frequencies [12].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "6: The restored images, generated by applying the weighted-average filter [13] on the noisy images of figures 3", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Restored images are generated by applying the weighted-average filter [13] for impulse noise and a lowpass filter for Gaussian noise.", "startOffset": 70, "endOffset": 74}], "year": 2017, "abstractText": "Google has recently introduced the Cloud Vision API for image analysis. According to the demonstration website, the API \u201cquickly classifies images into thousands of categories, detects individual objects and faces within images, and finds and reads printed words contained within images.\u201d It can be also used to \u201cdetect different types of inappropriate content from adult to violent content.\u201d In this paper, we evaluate the robustness of the Google\u2019s Cloud Vision API to input perturbation. In particular, we show that by adding sufficient noise to the image, the API generates completely different outputs for the noisy image, while a human observer would perceive its original content. We show that the attack is consistently successful, by performing extensive experiments on different image types, including natural images, images containing faces and images with texts. Our findings indicate the vulnerability of the API in adversarial environments. For example, an adversary can bypass an image filtering system by adding noise to an image with inappropriate content.", "creator": "LaTeX with hyperref package"}}}