{"id": "1704.05091", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings", "abstract": "This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-of-embeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S\\&amp;P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.", "histories": [["v1", "Mon, 17 Apr 2017 18:48:00 GMT  (22kb)", "http://arxiv.org/abs/1704.05091v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["pedro saleiro", "eduarda mendes rodrigues", "carlos soares", "eug\\'enio oliveira"], "accepted": false, "id": "1704.05091"}, "pdf": {"name": "1704.05091.pdf", "metadata": {"source": "CRF", "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings", "authors": ["Pedro Saleiro", "Eduarda Mendes Rodrigues", "Carlos Soares", "Eug\u00e9nio Oliveira"], "emails": ["pssc@fe.up.pt", "eduarda@fe.up.pt", "csoares@fe.up.pt", "eco@fe.up.pt"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.05 091v 1 [cs.C L] 17 Apr 201 7This paper presents the approach developed at the Faculty of Engineering of the University of Porto to participating in SemEval 2017, Task 5: Fine-grained Mood Analysis for Financial Microblogs and Messages. The task was to predict a true continuous variable from -1.0 to + 1.0, representing the polarity and intensity of sentiment regarding companies / stocks mentioned in short texts. We modeled the task as a problem of regression analysis and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical features with improved financial cul-sac. We used an external collection of tweets and news headlines mentioning companies / stocks from the S & P 500 to create financial word embeddings that are capable of capturing domain-specific syntactical and mansec similarities."}, {"heading": "1 Introduction", "text": "Sentiment Analysis on financial texts has received increased attention in recent years (Nardo et al., 2016). Nevertheless, there are some challenges that remain to be overcome (Smailovi\u0107 et al., 2014). Financial texts such as microblogs or newswire tend to contain highly technical and specific vocabulary or jargon, which requires the development of specific lexical and machine learning approaches. Most research results in Sentiment Analysis in the financial sector focus on the analysis of subjective texts labeled with explicit sentimentality. However, it is also common to implicitly express financial feelings. Economic news often points to events that point to positive or negative effects, as in the news headline \"Company X will cut 1000 jobs.\" Economic indicators, such as unemployment and future government modifiers, such as decline or increase, can also provide clues to the implicit sensitivity (Musat and Trausan-Matu, 2010)."}, {"heading": "2 Task Description", "text": "Task 5 of SemEval 2017 (Cortis et al., 2017) consisted of a fine-grained sentiment analysis of financial short texts and was divided into two subtasks based on the nature of the text. Task 5.1 - Microblogs - consisted of stock tweets and tweets focused on stock market events and valuations of investors and traders. Companies / shares were identified by share symbols, the so-called cash tags, e.g. \"$AMZN\" for Amazon.com, Inc. Task 5.2 - news headlines - consisted of sentences taken from Yahoo Finance and other financial news sources on the Internet. In this case, companies / shares were identified using their canonical name and previously commented by task organizers. The goal of both subtasks was the following: Prediction of sentiment polarity and intensity for each of the mentioned companies / shares in a short text instance (Microblog message or message). Tutorials 1, Tutorials 1, the Sentiment is positive (a very continuous range of 1.0) Tutorials 1, Tutorials 1, Tutorials 1, Tutorials 1, Tutorials 1, Tutorials 1, Tutorials 1, Tutorials 1."}, {"heading": "3 Financial Word Embeddings", "text": "Mikolov et al. (2013a) created word2vec, a mathematically efficient method to learn the distributed representation of words, in which each word is represented by a distribution of weights (embedding) over a specified number of dimensions. In addition, Mikolov et al. (2013b) showed that this representation is capable of encoding syntactical and semantic similarities in the embedding space. In view of the wt word in a vocabulary, the goal of the Skip model defined by Mikolov et al. (2013b) is to learn the target word representation (embedding), which maximizes the prediction of its surrounding words in a context window. Given the wt word in a vocabulary, the goal is to maximize the average literal probability: 1TT-t = 1-c \u2264 j \u2264 j j \u2264 c, j 6 = 0log P (wt + j | wt), where the total context size of the T is the words in the context."}, {"heading": "4 Approach", "text": "In this section, we describe the details of implementing the proposed approach."}, {"heading": "4.1 Pre-Processing", "text": "A series of pre-processing operations are applied to each microblog message and message set in the training / test sets of subtasks 5.1 and 5.2, as well as in the external collections for embedding training words: \u2022 Character encoding and stopwords: Each message and headline is in UTF-8. Standard English stopword removal is also used. \u2022 Company / share and cash veiling: both cash tags and canonical company names have been replaced by strings. \u2022 Dollar or euro characters followed by numbers have been replaced by the string cash amount. \u2022 Mapping of numbers and characters: numbers have been mapped to strings with containers (0-10, 10-20, 20-50, 50-100, > 100). \u2022 Minor and plus signs have been changed to minus and plus, \"B\" and \"M\" to billions and millions, respectively."}, {"heading": "4.2 Features", "text": "We combined three distinct groups of traits: pouches, lexical traits, and pouch beds. \u2022 Pouches: We used standard words as traits. We tried unique ones, bigrams, and tri-grams with unique ones that proved to be more cosinal in both tasks. \u2022 Sentiment lexicon traits: We integrated knowledge from manually curated sentimental lexicon for generic sensation analysis, as well as financial-related lexicon. The Laughran-Mcdonald financial lexicon (Bodnaruk et al., 2015) has several types of word classes: positive, negative, constraining, cunning, uncertain, and modal. \"For each word class, we create a binary trait to match a word in a microblog / heading, and a polarity trait (positively - negatively normalized by the text span).\" As a universal sense of superiority, we use the polarity A for each task, as well as the polarity A (finality)."}, {"heading": "5 Experimental Setup", "text": "To avoid overadjustment, we created a validation set from the original training data sets provided by the organizers, using an 80% -20% split and sampling the validation set using the same distribution as the original training set. We sorted the examples in the training set by target variable values, leaving all 5 examples to be evaluated on the basis of cosmic similarity (Cortis et al., 2017) and Mean Average Error (MAE). The former places more emphasis on differences in the polarity of the predicted feeling, while the latter is aimed at how well the system predicts the intensity of the feelings. We decided to model both subtasks as individual regression problems. Three different regressors were applied: Random Forests (RF), Support Vector Machines (SVM) and MultiLayer Perceptron (MLP). Parameter adjustment was done using 10-fold cross-validation on the training sets."}, {"heading": "6 Results and Analysis", "text": "In this section, we present the experimental results achieved in both tasks. We offer a comparison of different learning algorithms using all characteristics, as well as a comparison of different subsets of characteristics to understand the information contained in them and how they complement each other."}, {"heading": "6.1 Task 5.1 - Microblogs", "text": "The results in the test set are worse than in the validation set with the exception of the MLP. The official value achieved in subtask 5.1 was 0.6948 using the Random Forest (RF), the regressor that achieves a higher cosine similarity and a lower MAE in both training and validation. We compared the results with different subsets of characteristics using the best regressor, RF, as shown in Table 3. Interestingly, bag similarity (BoW) and bag similarity (BoE) complement each other and achieve a better cosmic similarity than the system using all characteristics. Financial word embedding (BoE) collects relevant information with respect to the target variables. As a single group of characteristics, it achieves a cosmic similarity (BoW) and bag similarity (BoE) of 0.6118 and MAE of 0.2322. It is also able to increase the overall performance of BoW with increases of more than one cosmic group of cosinals, with only 0.06 in cosinality."}, {"heading": "6.2 Task 5.2 - News Headlines", "text": "The results obtained in news headlines are very different from those obtained in the previous sub-task, proving that predicting the polarity and intensity of mood in news headlines is a completely different problem than in microblogs. Table 4 shows that MLP achieves the best results in the test set, while SVR achieves the best performance in the validation set. The best regressor of sub-task 5.1, RF is surpassed by both SVR and MLP. The official result of sub-task 5.2 was a cosinal similarity of 0.68 when using MLP. Table 5 shows the results of the various feature groups of sub-task 5.2 for MLP regressor. The most obvious observation is that word embedding is not effective in this scenario. On the other hand, lexical-based features exhibit significantly better performance in news headlines than in microblogs. Nevertheless, the best results are achieved with all characteristics."}, {"heading": "6.3 Analysis", "text": "Financial word embedding was able to summarize valuable information in sub-task 5.1 - Mi-croblogs, but not so much in the case of sub-task 5.2 - news headlines. We suspect that because we had access to a much smaller dataset to train financial word embedding for news headlines, this resulted in a reduced ability to capture semantic similarities in the financial sphere. Other similar work in Sentiment Analysis usually uses a much larger dataset to train word embedding (Deriu et al., 2016). On the other hand, lexical features performed poorly in microblog texts but seem to be very useful in news headlines. The fact that microblogs have poorly grammatically constructed texts, slang and informal language shows that financial word embedding created using well-written and formal financial reports results better in news headlines than in microblogs."}, {"heading": "7 Conclusions", "text": "Previous work has shown that sentiment is often implicit in this area. In order to obtain domain-specific syntactic and semantic relationships between words, we have developed finance-specific continuous word representations. We combined traditional cul-de-sac and lexical traits with cul-de-sac embedding to train a regressor of both mood polarity and intensity. Results show that different combinations of traits performed differently in each subtask. Future work will consist of collecting larger external datasets for embedding financial words in both microblogs and news headlines."}], "references": [{"title": "Using 10-k text to gauge financial constraints", "author": ["Andriy Bodnaruk", "Tim Loughran", "Bill McDonald."], "venue": "Journal of Financial and Quantitative Analysis 50(04).", "citeRegEx": "Bodnaruk et al\\.,? 2015", "shortCiteRegEx": "Bodnaruk et al\\.", "year": 2015}, {"title": "Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news", "author": ["Keith Cortis", "Andre Freitas", "Tobias Daudert", "Manuela Huerlimann", "Manel Zarrouk", "Brian Davis"], "venue": "Proceedings of SemEval ", "citeRegEx": "Cortis et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cortis et al\\.", "year": 2017}, {"title": "Swisscheese at semeval-2016 task 4: Sentiment classification using an ensemble of convolutional neural networks with distant supervision", "author": ["J. Deriu", "M. Gonzenbach", "F. Uzdilli", "A. Lucchi", "V. De Luca", "M. Jaggi."], "venue": "Proceedings of SemEval .", "citeRegEx": "Deriu et al\\.,? 2016", "shortCiteRegEx": "Deriu et al\\.", "year": 2016}, {"title": "Part-ofspeech tagging for twitter: Annotation, features, and experiments", "author": ["K. Gimpel", "N. Schneider", "B. O\u2019Connor", "D. Das", "D. Mills", "J. Eisenstein", "M. Heilman", "D. Yogatama", "J. Flanigan", "Noah A Smith"], "venue": "In ACL HLT: short papers-Volume", "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Sentiment analysis and opinion mining", "author": ["Bing Liu."], "venue": "Synthesis lectures on human language technologies 5(1).", "citeRegEx": "Liu.,? 2012", "shortCiteRegEx": "Liu.", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Hlt-naacl. volume 13.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The impact of valence shifters on mining implicit economic opinions", "author": ["Claudiu Musat", "Stefan Trausan-Matu."], "venue": "International Conference on Artificial Intelligence: Methodology, Systems, and Applications. Springer.", "citeRegEx": "Musat and Trausan.Matu.,? 2010", "shortCiteRegEx": "Musat and Trausan.Matu.", "year": 2010}, {"title": "Walking down wall street with a tablet: A survey of stock market predictions using the web", "author": ["Michela Nardo", "Marco Petracco-Giudici", "Mins Naltsidis."], "venue": "Journal of Economic Surveys 30(2).", "citeRegEx": "Nardo et al\\.,? 2016", "shortCiteRegEx": "Nardo et al\\.", "year": 2016}, {"title": "Stream-based active learning for sentiment analysis in the financial domain", "author": ["Jasmina Smailovi\u0107", "Miha Gr\u010dar", "Nada Lavra\u010d", "Martin \u017dnidar\u0161i\u010d."], "venue": "Information Sciences 285.", "citeRegEx": "Smailovi\u0107 et al\\.,? 2014", "shortCiteRegEx": "Smailovi\u0107 et al\\.", "year": 2014}, {"title": "Fine-grained analysis of explicit and implicit sentiment in financial news articles", "author": ["Marjan Van de Kauter", "Diane Breesch", "V\u00e9ronique Hoste."], "venue": "Expert Systems with applications 42(11).", "citeRegEx": "Kauter et al\\.,? 2015", "shortCiteRegEx": "Kauter et al\\.", "year": 2015}, {"title": "Recognizing contextual polarity in phraselevel sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "EMNLP.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "Sentiment Analysis on financial texts has received increased attention in recent years (Nardo et al., 2016).", "startOffset": 87, "endOffset": 107}, {"referenceID": 10, "context": "Neverthless, there are some challenges yet to overcome (Smailovi\u0107 et al., 2014).", "startOffset": 55, "endOffset": 79}, {"referenceID": 8, "context": "Economic indicators, such as unemployment and future state modifiers such as drop or increase can also provide clues on the implicit sentiment (Musat and Trausan-Matu, 2010).", "startOffset": 143, "endOffset": 173}, {"referenceID": 4, "context": "Contrary to explicit expressions (subjective utterances), factual text types often contain objective statements that convey a desirable or undesirable fact (Liu, 2012).", "startOffset": 156, "endOffset": 167}, {"referenceID": 1, "context": "The task 5 of SemEval 2017 (Cortis et al., 2017) consisted of fine-grained sentiment analysis of financial short texts and it was divided in two subtasks based on the type of text.", "startOffset": 27, "endOffset": 48}, {"referenceID": 1, "context": "com/saleiro/Financial-Sentiment-Analysis missions were evaluated using the cosine similarity (Cortis et al., 2017).", "startOffset": 93, "endOffset": 114}, {"referenceID": 5, "context": "We performed simple algebraic operations to capture semantic relations between words, as described in Mikolov et al. (2013c). For instance, the skip-gram model trained on tweets shows that vector (\u201cbearish\u201d) - vector(\u201closs\u201d) + vector(\u201cgain\u201d) results in vector (\u201cbullish\u201d) as most similar word representation.", "startOffset": 102, "endOffset": 125}, {"referenceID": 3, "context": "\u2022 Tokenization, punctuation, lowercasing: tokenization was performed using Twokenizer (Gimpel et al., 2011), the remaining punctuation was removed and all characters were converted to lowercase.", "startOffset": 86, "endOffset": 107}, {"referenceID": 0, "context": "The Laughran-Mcdonald financial sentiment dictionary (Bodnaruk et al., 2015) has several types of word classes: positive, negative, constraining, litigious, uncertain and modal.", "startOffset": 53, "endOffset": 76}, {"referenceID": 12, "context": "As a general-purpose sentiment lexicon we use MPQA (Wilson et al., 2005) and created binary features for positive, negative and neutral words, as well as, the polarity score feature.", "startOffset": 51, "endOffset": 72}, {"referenceID": 1, "context": "Results are evaluated using Cosine similarity (Cortis et al., 2017) and Mean Average Error (MAE).", "startOffset": 46, "endOffset": 67}, {"referenceID": 2, "context": "Other related works in Sentiment Analysis usually take advantage of a much larger dataset for training word embeddings (Deriu et al., 2016).", "startOffset": 119, "endOffset": 139}], "year": 2017, "abstractText": "This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-ofembeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 Microblogs and 0.68 in sub-task 5.2 News Headlines.", "creator": "LaTeX with hyperref package"}}}