{"id": "1704.05155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Stein Variational Autoencoder", "abstract": "A new method for learning variational autoencoders is developed, based on an application of Stein's operator. The framework represents the encoder as a deep nonlinear function through which samples from a simple distribution are fed. One need not make parametric assumptions about the form of the encoder distribution, and performance is further enhanced by integrating the proposed encoder with importance sampling. Example results are demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.", "histories": [["v1", "Tue, 18 Apr 2017 00:08:34 GMT  (7065kb,D)", "http://arxiv.org/abs/1704.05155v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yunchen pu", "zhe gan", "ricardo henao", "chunyuan li", "shaobo han", "lawrence carin"], "accepted": false, "id": "1704.05155"}, "pdf": {"name": "1704.05155.pdf", "metadata": {"source": "META", "title": "Stein Variational Autoencoder", "authors": ["Yunchen Pu", "Zhe Gan", "Ricardo Henao", "Chunyuan Li", "Shaobo Han", "Lawrence Carin"], "emails": ["<yunchen.pu@duke.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to take the lead, in the manner in which it is a country, in which it is not a country, in which it is not a country, but a country, in which it is a country, a country, a country, a country and a country, a country, a country and a country, a country and a country, a country and a country, a country and a country, a country and a country, a country and a country, a country and a country, a country and a country, a country and a country, a country and a country, a country, a country, a country, a country, a country and a country, a country, a country and a country, a country and a country, a country and a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country and a country, a country, a country and a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country and a country, a country, a country, a country, a country and a country, a country, a country, a country and a country, a country, a country and a country, a country, a country, a country, a country, a country and a country, a country, a country, a country, a country, a country and a country, a country, a country, a country, a country, a country, a country and a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "2. Stein Variational Autoencoder (Stein VAE)", "text": "(Consider the data D = {xn} Nn = 1, where xn is modeled as xn | zn \u0445 p (x | zn; \u03b8). The distribution p (x | zn; \u03b8) can be regarded as a probable decoder of latent codes zn, and \u03b8 represents the decoder parameters. The set of codes associated with all xn \u00b2 D becomes Z = {zn} Nn = 1. In Bayesian statistics, set a previous value to {\u03b8, Z}, here representing p (\u03b8, Z) = p (\u03b8). We want the pos algorithm 1 stone of Variable Autoencoder. Required: Input data D and number of samples M 1: Initialize samples {ctu0j} Mj = 1 of p (\u03b8) and vice versa: 2: for t = 1 to maximum iterations we do 3: Sample {update} Mj = 1 of q0 (4)."}, {"heading": "2.1. Stein Variational Gradient Descent (SVGD)", "text": "Suppose that we have samples: Mj = 1 from the distribution q (\u03b8), and samples {zjn} Mj = 1 from the distribution q (Z). The distribution q (\u03b8) q (Z) is to some extent removed from the actual subordinate p (\u03b8, Z | D). It is desirable that in a KL sense qT (Z) q (Z) is closer to p (\u03b8, Z | D) than to q (Z) q (Z) than to q (Z). The following theorem is useful to define how best to update: Mj = 1.Theorem 1 Assume 1 and Z are random variables (RVs) derived from the distribution q (Z) and q (Z)."}, {"heading": "2.2. Stein Recognition Model", "text": "In iterating t of the above learning procedure, we implement a set of latent-variable (code) samples (code) {z (t) jn} Mj = 1 for each xn-D sample in the analysis. For large N, this training can be computationally expensive. Furthermore, the need to develop (learn) samples (zj) Mj = 1 for each new sample x * is undesirable. Therefore, we develop a recognition model that efficiently calculates the samples of codes for a data sample of interest. The recognition model draws samples above zjn = f\u03b7 (xn) (zj) Mj = 1 for each new sample x *). The distribution q0 \u2212 n is selected so that it can be easily sampled, e.g. isotropic Gaussian. After each iteration of algorithm 1, we refute the recognition model f\u03b7 (x) for imitating the stone sample dynamics."}, {"heading": "3. Stein Variational Importance Weighted Autoencoder (Stein VIWAE)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Multi-sample importance-weighted KL divergence", "text": "Consider a known common distribution p (x, z) where x is observed and z is latent q (e.g., according to the code discussed above).The marginal log probability protocol p (x) = log p (x, z) dz is typically intractable. Frequently, attempts are made to bind log p (x) by approximating q (z | x) to the trailing p (z | x): L (x) = Eq (z, z) dz (z) / q (x)] \u2264 Log p (x).This lower limit plays a central role in the UAE (Kingma & Welling, 2014), and q (z | x) is analogous to the encoder discussed above. Recently, we have shown Burda et al al al al al al. (2016); Mnih & Rezende (2016) that the multi-sample (k samples) is weighted Lk (x)."}, {"heading": "3.2. Importance-weighted learning procedure", "text": "Theorem 3: Let us RVs (K) = Distribution q = Distribution q = Distribution q (K) and KLkq, p (K) represent the multi-sample weighted meaning KL divergence in (9). Let T (K) = Distribution + q (K) and qT (K) represent the distribution of individual samples (T). We have the sample (KLkq), p (K), p (K), p (Q), p (D), p (D), p (A) and p (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). K (K). (K). K. (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K. (K). (K). (K). (K). (K. (K). (K). (K). (K. (K). (K. (K). (K). (K. (K). (K). (K. (K). (K. (K). (K. (K). (K). (K). (K (K). (K). (K. (K). (K. (K). (K). (K. (K). (K). (K). (K. (K. (K). (K. (K). (K). (K).). (K.). (K.).). (K (K (K (K.).).). (K (K (K.).).). (K."}, {"heading": "4. Semi-supervised Learning with Stein VAE and Stein VIWAE", "text": "We extend our model to semi-supervised learning. Let us consider the data as pairs Dl = q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = 1, where the designation Y = 1,., C = and the decoder as (xn, yn) Q = Q = Q (x, y | zn; p = P (x, zn; p) p (y, zn; p) p (p) p (p) p (p) p (p) p (p) p (p) p)), where the parameters of the decoder for labels are represented. The set of codes associated with all designated data is represented Zl = {zn} Nln = 1. We would like to approximate the posterior distribution over the entire dataset p (p)."}, {"heading": "5. Experiments", "text": "For all experiments, we use a radial base core (RBF) as in Liu & Wang (2016), i.e. k (x, x) = exp (\u2212 1h, x \u2212 x \u00b2 22), with the bandwidth h being the mean of the paired distances between current samples. q0 (\u03b8) and q0 (\u044b) are set to isotropic Gaussian distributions. We divide the samples across all data points, i.e. for n = 1,.., N (this is not necessary, but it saves calculation). Samples of \u03b8 and z and the parameters of the recognition model are optimized via Adam (Kingma & Ba, 2015) with the learning rate 0.0002 and with the gradient approximations in (3), (4), (12) and (13). We do not perform dataset-specific or other tunings unless we set the setpoints (64) and determine the setpoints."}, {"heading": "5.1. Expressive power of Stein recognition model", "text": "We are first going to analyze the explicit power of the non-Gaussian posterior approach on the basis of the Stone Recognition Model (UAE). (UAE) We are first going to analyze the expressiveness of the non-Gaussian posterior approach on the basis of the Stone Recognition Model (UAE). (UAE) We have the data of the UAE (UAE) and only generate samples of z and updated encoder parameters. (VAE) Any comparison with the basic probability of VAE and stone VAE.Gaussian Mixture Model We are synthesizing the data of (i) drawing cn. (\u00b51, I) + 12N (I), I), where we are the accuracy of z and updated encoder parameters during training, for both standards VAE and stone VAE.Gaussian Mixture Model We are synthesizing the data of (\u00b51, I) + 12\u00b5N, we are [zerial + \u00b5I] of 12\u00b5I, 12\u00b5I, 12\u00b5I = 5, 12\u00b5I, 12\u00b5I = 5, 12\u00b5I)."}, {"heading": "5.2. Density estimation", "text": "We look at five benchmark datasets: MNIST and four text corporas: 20 SBO (20News), 7 SBO (20News), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 7 SR (2016), 8 SR (2016), 8 SR (2016), 8 SR (2016)."}, {"heading": "5.3. Semi-supervised Classification", "text": "In fact, it is such that it is a very rare disease for which one can feel oneself responsible. (...) It is not so that one sees oneself in a position to know such a disease. (...) It is not so that one sees oneself in a position to know it. (...) It is not so that one sees oneself in a position to know it. (...) It is also not so that one is in a position to know it. (...) It is such that one cannot perceive it. (...) It is such that one cannot perceive it. (...) It is such that one cannot perceive it. (...) It is such that one cannot perceive it. (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (.). (.). (.). (. (.). (.). (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (. (.).). (.). (. (.). (.). (. (.). (.). (.). (.). (. (.). (.). (.).). (.). (. (. (.). (. (.).). (It is. (.). (.). (. (.).). (. (.).).). (It is. (It is. (. (.). (. (. (.). (.). (.). (.). ().).). (). ("}, {"heading": "6. Conclusion", "text": "We have used the Stone Operator to develop a new method for designing the encoder in a varying autoencoder and for learning an approximate distribution (samples) for the parameters of the decoder. Distributions for the codes and for the decoder parameters are represented non-parametrically in the form of samples, derived by minimizing KL removal and using an RKHS. Rapid inference manifests itself by learning an identification model that imitates the manifestation of the derived code samples. The method is further generalized and improved by performing importance tests."}], "references": [{"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "A kernel test of goodness of fit", "author": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"], "venue": "In ICML,", "citeRegEx": "Chwialkowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 2016}, {"title": "Scalable deep poisson factor analysis for topic modeling", "author": ["Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Variational gaussian copula inference", "author": ["S. Han", "X. Liao", "D.B. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["D.P. Kingma", "T. Salimans", "R. Jozefowicz", "Chen", "X.i", "I. Sutskever", "M. Welling"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Laulyi"], "venue": "In NIPS,", "citeRegEx": "Larochelle and Laulyi,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Laulyi", "year": 2012}, {"title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "author": ["Q. Liu", "D. Wang"], "venue": "In NIPS,", "citeRegEx": "Liu and Wang,? \\Q2016\\E", "shortCiteRegEx": "Liu and Wang", "year": 2016}, {"title": "A kernelized stein discrepancy for goodness-of-fit tests", "author": ["Q. Liu", "J.D. Lee", "M. Jordan"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "Blunsomi", "Phil"], "venue": "In ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In ICML,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Variational inference for monte carlo objectives", "author": ["A. Mnih", "D.J. Rezende"], "venue": "In ICML,", "citeRegEx": "Mnih and Rezende,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende", "year": 2016}, {"title": "Pixel recurrent neural network", "author": ["A. Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Variational autoencoder for deep learning of images, labels and captions", "author": ["Y. Pu", "Z. Gan", "R. Henao", "X. Yuan", "C. Li", "A. Stevens", "L. Carin"], "venue": null, "citeRegEx": "Pu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pu et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "In ICLR,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M.Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Hierarchical variational models", "author": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": "In ICML,", "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2016}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In NIPS,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["D.J. Rezende", "S. Mohamed"], "venue": "In ICML,", "citeRegEx": "Rezende and Mohamed,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR workshop,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Learning to samples: With application to amoritized mle for generalized adversarial learning", "author": ["D. Wang", "Q. Liu"], "venue": "In arXiv:1611.01722v2,", "citeRegEx": "Wang and Liu,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu", "year": 2016}, {"title": "Betanegative binomial process and Poisson factor analysis", "author": ["M. Zhou", "L. Hannah", "D. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 30, "context": "The autoencoder (Vincent et al., 2010) is a widely employed unsupervised framework to learn (typically) lowdimensional features from complex data.", "startOffset": 16, "endOffset": 38}, {"referenceID": 20, "context": "The VAE may also be scaled to handle massive amounts of data, such as ImageNet (Pu et al., 2016).", "startOffset": 79, "endOffset": 96}, {"referenceID": 10, "context": "Additionally, when given labels on a subset of data, a classifier may be associated with the latent features, allowing for semi-supervised learning (Kingma et al., 2014; Pu Duke University. Correspondence to: Yunchen Pu <yunchen.pu@duke.edu>. et al., 2016).", "startOffset": 148, "endOffset": 256}, {"referenceID": 20, "context": "Further, the latent features (codes) may be associated with state-of-the-art natural-language-processing models, such as the Long Short-Term Memory (LSTM) network, for semi-supervised learning of text captions from an image (Pu et al., 2016).", "startOffset": 224, "endOffset": 241}, {"referenceID": 25, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 10, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 23, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 20, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 9, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 25, "context": "This requirement has motivated design of encoders in which a neural network maps input data to the parameters of a distribution in the exponential family (Kingma & Welling, 2014; Rezende et al., 2014), which serves as the latent-features distribution.", "startOffset": 154, "endOffset": 200}, {"referenceID": 25, "context": "For example, Gaussian distributions have been widely utilized (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2016; Burda et al., 2016).", "startOffset": 62, "endOffset": 152}, {"referenceID": 23, "context": "For example, Gaussian distributions have been widely utilized (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2016; Burda et al., 2016).", "startOffset": 62, "endOffset": 152}, {"referenceID": 0, "context": "For example, Gaussian distributions have been widely utilized (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2016; Burda et al., 2016).", "startOffset": 62, "endOffset": 152}, {"referenceID": 0, "context": "Researchers have recently considered the idea of viewing the encoder as a proposal distribution for the latent features, thus using importance weighting when sampling from this distribution (Burda et al., 2016).", "startOffset": 190, "endOffset": 210}, {"referenceID": 1, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016).", "startOffset": 112, "endOffset": 157}, {"referenceID": 14, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016).", "startOffset": 112, "endOffset": 157}, {"referenceID": 1, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016). Motivated by these ideas, we present a new approach for learning the distribution of latent features within a VAE framework. We recognize that the need for an explicit form for the encoder distribution is only a consequence of the fact that learning is performed based on the variational lower bound. For inference (e.g., at test time), we do not need an explicit form for the distribution of latent features, we only require fast sampling from the encoder. Consequently, in the proposed approach, we no longer explicitly use the variational lower bound to train the encoder. Rather, we seek an encoder that minimizes the Kullback-Leibler (KL) distance between the distribution of codes and the true (posterior) distribution on the latent features. To minimize this KL distance, we generalize use of Stein\u2019s identity, and employ ideas associated with a reproducing kernel Hilbert space (RKHS). Analogous work was introduced by Liu & Wang (2016); Wang & Liu (2016); however, they considered sampling from a general unnormalized distribution.", "startOffset": 113, "endOffset": 1104}, {"referenceID": 1, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016). Motivated by these ideas, we present a new approach for learning the distribution of latent features within a VAE framework. We recognize that the need for an explicit form for the encoder distribution is only a consequence of the fact that learning is performed based on the variational lower bound. For inference (e.g., at test time), we do not need an explicit form for the distribution of latent features, we only require fast sampling from the encoder. Consequently, in the proposed approach, we no longer explicitly use the variational lower bound to train the encoder. Rather, we seek an encoder that minimizes the Kullback-Leibler (KL) distance between the distribution of codes and the true (posterior) distribution on the latent features. To minimize this KL distance, we generalize use of Stein\u2019s identity, and employ ideas associated with a reproducing kernel Hilbert space (RKHS). Analogous work was introduced by Liu & Wang (2016); Wang & Liu (2016); however, they considered sampling from a general unnormalized distribution.", "startOffset": 113, "endOffset": 1123}, {"referenceID": 0, "context": "Recently, Burda et al. (2016); Mnih & Rezende (2016) showed that the multi-sample (k samples) importance-weighted estimator", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "Recently, Burda et al. (2016); Mnih & Rezende (2016) showed that the multi-sample (k samples) importance-weighted estimator", "startOffset": 10, "endOffset": 53}, {"referenceID": 0, "context": "Inspired by Burda et al. (2016), the following theorem (proved in Appendix A) shows that increasing the number of samples k is guaranteed to reduce the KL divergence and provide a better approximation of target distribution.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "However, Burda et al. (2016) has showed that the estimator based on log of importance weighted average will not lead to high variance.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "Motivated by assigning the same weight to every data point (Pu et al., 2016), we set \u03b6 = NX/(C\u03c1) in the experiments, where NX is the dimension of xn, C is the number of categories for the corresponding label and \u03c1 is the proportion of labeled data in the mini-batch.", "startOffset": 59, "endOffset": 76}, {"referenceID": 29, "context": "We do not perform any dataset-specific tuning or regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets.", "startOffset": 83, "endOffset": 108}, {"referenceID": 32, "context": "Poisson Factor Analysis Given a discrete vector xn \u2208 Z+, Poisson factor analysis (Zhou et al., 2012) assumes xn is a weighted combination of V latent factors xn \u223c Pois(\u03b8zn), where \u03b8 \u2208 RP\u00d7V + is the factor loadings matrix and zn \u2208 R+ is the vector of factor scores.", "startOffset": 81, "endOffset": 100}, {"referenceID": 2, "context": "\u03b8 is first learned by Markov chain Monte Carlo (MCMC) (Gan et al., 2015).", "startOffset": 54, "endOffset": 72}, {"referenceID": 25, "context": "Method NLL DGLM (Rezende et al., 2014) 89.", "startOffset": 16, "endOffset": 38}, {"referenceID": 0, "context": "10 VAE + IWAE (Burda et al., 2016)\u2020 86.", "startOffset": 14, "endOffset": 34}, {"referenceID": 0, "context": "76 IWAE + IWAE (Burda et al., 2016)\u2021 84.", "startOffset": 15, "endOffset": 35}, {"referenceID": 4, "context": "analysis (Han et al., 2016).", "startOffset": 9, "endOffset": 27}, {"referenceID": 0, "context": "We further estimate the marginal log-likelihood/perplexity values via the stochastic variational lower bound, as the mean of 5K-sample importance weighting estimate (Burda et al., 2016).", "startOffset": 165, "endOffset": 185}, {"referenceID": 22, "context": "\u00a7(Larochelle & Laulyi, 2012); \u2020(Ranganath et al., 2015); \u2021(Miao et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 16, "context": ", 2015); \u2021(Miao et al., 2016).", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "2 or Stein VIWAE in Section 3; the second term denotes the testing log-likelihood/perplexity is estimated by the ELBO in (16) or the stochastic variational lower bound, S-ELBO (Burda et al., 2016).", "startOffset": 176, "endOffset": 196}, {"referenceID": 22, "context": "For the text corpora, we build a three-layer deep Poisson network (Ranganath et al., 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 22, "context": "For the text corpora, we build a three-layer deep Poisson network (Ranganath et al., 2015). The sizes of hidden units are 200, 200 and 50 for the first, second and third layer, respectively (see Ranganath et al. (2015) for detailed architectures).", "startOffset": 67, "endOffset": 219}, {"referenceID": 3, "context": "Gregor et al. (2015) and Oord et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Gregor et al. (2015) and Oord et al. (2016), which exploit spatial structure, achieved log-likelihoods of around -80 nats.", "startOffset": 0, "endOffset": 44}, {"referenceID": 27, "context": "We consider semi-supervised classification on MNIST and ImageNet (Russakovsky et al., 2014) data.", "startOffset": 65, "endOffset": 91}, {"referenceID": 10, "context": "\u00a7(Kingma et al., 2014); \u2020our implementation.", "startOffset": 1, "endOffset": 22}, {"referenceID": 15, "context": "We use the leaky rectified activation (Maas et al., 2013).", "startOffset": 38, "endOffset": 57}, {"referenceID": 24, "context": "State-of-the-art results (Rasmus et al., 2015) are achieved by the Ladder network, which can be employed with our Stein-based approach, however, we will consider this extension as future work.", "startOffset": 25, "endOffset": 46}, {"referenceID": 9, "context": "We consider two types of decoders for images p(xn|zn,\u03b8) and encoder f\u03b7(x, \u03be): (i) FNN: Following Kingma et al. (2014), we use a 50-dimensional latent variables zn and two hidden layers, each with 600 hidden units, for both encoder and decoder; softplus log(1 + e) is employed as the nonlinear activation function.", "startOffset": 97, "endOffset": 118}, {"referenceID": 9, "context": "We consider two types of decoders for images p(xn|zn,\u03b8) and encoder f\u03b7(x, \u03be): (i) FNN: Following Kingma et al. (2014), we use a 50-dimensional latent variables zn and two hidden layers, each with 600 hidden units, for both encoder and decoder; softplus log(1 + e) is employed as the nonlinear activation function. (ii) All convolutional nets (CNN): Inspired by Springenberg et al. (2015), we replace the two hidden layers with 32 and 64 kernels of size 5 \u00d7 5 and a stride of 2.", "startOffset": 97, "endOffset": 388}, {"referenceID": 20, "context": "\u2020(Pu et al., 2016) VAE Stein VAE Stein VIWAE DGDN\u2020 1 % 35.", "startOffset": 1, "endOffset": 18}, {"referenceID": 28, "context": "We employ all convolutional net (Springenberg et al., 2015) for both the encoder and decoder, which replaces deterministic pooling (e.", "startOffset": 32, "endOffset": 59}, {"referenceID": 21, "context": "Such a model has been shown to be effective for training higher resolution and deeper generative models (Radford et al., 2016).", "startOffset": 104, "endOffset": 126}, {"referenceID": 5, "context": "The decoder for labels is employed as global average pooling with softmax, which has been utilized in state-of-the-art image classification models (He et al., 2016).", "startOffset": 147, "endOffset": 164}, {"referenceID": 15, "context": "We use the leaky rectified activation (Maas et al., 2013).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Residual connections (He et al., 2016) are incorporated to encourage gradient flow.", "startOffset": 21, "endOffset": 38}, {"referenceID": 11, "context": "A 224\u00d7 224 crop is randomly sampled from the images or its horizontal flip with the mean subtracted (Krizhevsky et al., 2012).", "startOffset": 100, "endOffset": 125}, {"referenceID": 20, "context": "When the proportion of labeled examples is too small (< 10%), DGDN outperforms all the VAE-based models, which is not surprising provided that our models are deeper, thus have considerably more parameters than DGDN (Pu et al., 2016).", "startOffset": 215, "endOffset": 232}, {"referenceID": 5, "context": "The decoder for labels is employed as global average pooling with softmax, which has been utilized in state-of-the-art image classification models (He et al., 2016). Batch normalization (Ioffe & Szegedy, 2015) is used to stabilize learning by normalizing the activations throughout the network and preventing relatively small parameter changes from being amplified into larger but suboptimal activation changes in other layers. We use the leaky rectified activation (Maas et al., 2013). Residual connections (He et al., 2016) are incorporated to encourage gradient flow. The model architecture is detailed in Appendix E. Following Krizhevsky et al. (2012), images are resized to 256\u00d7 256.", "startOffset": 148, "endOffset": 656}], "year": 2017, "abstractText": "A new method for learning variational autoencoders is developed, based on an application of Stein\u2019s operator. The framework represents the encoder as a deep nonlinear function through which samples from a simple distribution are fed. One need not make parametric assumptions about the form of the encoder distribution, and performance is further enhanced by integrating the proposed encoder with importance sampling. Example results are demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.", "creator": "LaTeX with hyperref package"}}}