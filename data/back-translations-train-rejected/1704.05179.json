{"id": "1704.05179", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "abstract": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.", "histories": [["v1", "Tue, 18 Apr 2017 02:42:17 GMT  (154kb,D)", "https://arxiv.org/abs/1704.05179v1", null], ["v2", "Thu, 4 May 2017 14:07:21 GMT  (154kb,D)", "http://arxiv.org/abs/1704.05179v2", null], ["v3", "Sun, 11 Jun 2017 11:51:06 GMT  (154kb,D)", "http://arxiv.org/abs/1704.05179v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matthew dunn", "levent sagun", "mike higgins", "v ugur guney", "volkan cirik", "kyunghyun cho"], "accepted": false, "id": "1704.05179"}, "pdf": {"name": "1704.05179.pdf", "metadata": {"source": "CRF", "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "authors": ["Matt Dunn", "Levent Sagun", "Mike Higgins", "Volkan Cirik"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "One of the driving forces behind the recent success of deep learning in challenging tasks, such as object recognition (Krizhevsky et al., 2012), speech recognition (Xiong et al., 2016) and machine translation (Bahdanau et al., 2014), is the increasing availability of large-scale annotations, which have also led to the creation of a large-scale annotation database. (2015) Bordes et al. (2015) Answer is a large-scale dataset of question and answer pairs from Freebase, and Hermann et al. (2015) Two datasets have been published, each consisting of closed question and answer pairs automatically generated by news articles. (2015) It was published by Hill et al. (2015), Rajpurkar et al. (2016) and Onishi et al."}, {"heading": "2 SearchQA", "text": "The question we asked ourselves is whether we used the entire set of question-and-answer pairs from the popular TV show Jeopardy to retrieve a set of relevant website snippets; the relevance in this case was determined by an unknown but unproductive algorithm underlying Google's search engine, so it is much closer to a realistic scenario of question-and-answer pairs."}, {"heading": "3 Related Work", "text": "An example of such a data set is SimpleQA by (Bordes et al., 2015). SimpleQA consists of 100k question pairs and uses Freebase as a knowledge base. The great limitation of this data set is that all questions are in the form of (subject, relationship,). Closed-World Question-Answering Although we use open-world snippets, the final SearchQA is a closed world in which questions are answered, all in the form of (subject, relationship,)."}, {"heading": "4 Experiments and Results", "text": "As part of our publication of SearchQA, we provide a set of basic services that other researchers can use to compare their future approaches. Unlike most previous datasets, SearchQA adds a loud, real-world context to each question-and-answer pair retrieved from the world's largest search engine. This implies that human performance is not necessarily the upper limit, but we do provide it as a guide."}, {"heading": "4.1 Human Evaluation", "text": "We have designed a web interface that displays a query and retrieves snippets, allowing a user to select an answer by clicking on words on the screen. Users are given up to 40 minutes to answer as many questions as possible. We randomly selected pairs of question-answer context pairs from the test set.We recruited thirteen volunteers from the master program of the Center for Data Science at NYU. They were consistently randomly divided into two groups: the first group was confronted with questions that answered only one word (unigram), and the other group with questions that had either one word or several words (n-gram) answers. On average, each participant answers 47.23 questions with the standard deviation of 30.42. We report on the average and standard deviation of the answers given by the volunteers in Table 1. We note the significant gap between the accuracy of the first and second group, indicating that the difficulty of answering questions increases with increasing length of the answer."}, {"heading": "4.2 Machine Baselines", "text": "TF-IDF Max An interesting feature of the proposed SearchQA is that the context of each question-answer pair was retrieved from Google with the question as a query, implying that the information about the question itself can be implicitly embedded in the snippets. Therefore, we are testing a naive strategy (TF-IDF Max) of selecting the word with the highest TF-IDF score in the context as an answer. Note: This can only be used for questions with an unigrammed answer. Attention Sum Reader Attention sum reader (ASR, Kadlec et al., 2016) is a variant of a pointer network (Vinyals et al., 2015) specifically designed to solve a cloze-style question."}, {"heading": "5 Conclusion", "text": "We have developed a new question and answer research dataset called SearchQA, created using a commercial search engine, which closely mirrors the entire pipeline of a (hypothetical) general question and answer system consisting of information gathering and response synthesis. We conducted both human and machine evaluation. Using the latest technology, ASR, we demonstrate that there is a significant gap between man and machine, which suggests the potential of SearchQA as a benchmark task for question and answer research. We publish SearchQA publicly, including our own implementation of ASR and n-gram ASR in PyTorch.3"}, {"heading": "Acknowledgments", "text": "KC is grateful for the support of Google, NVIDIA, eBay and Facebook. MD conducted this work as part of DS-GA 1010: Independent Study in Data Science at the Center for Data Science at New York University."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Conference on Empirical Methods in", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547 .", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems. pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "arXiv preprint arXiv:1608.05457 .", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "NewsQA: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems. pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Achieving human parity in conversational speech recognition", "author": ["Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig."], "venue": "arXiv preprint arXiv:1610.05256 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "One of the driving forces behind the recent success of deep learning in challenging tasks, such as object recognition (Krizhevsky et al., 2012), speech recognition (Xiong et al.", "startOffset": 118, "endOffset": 143}, {"referenceID": 14, "context": ", 2012), speech recognition (Xiong et al., 2016) and machine translation (Bahdanau et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 0, "context": ", 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data.", "startOffset": 32, "endOffset": 55}, {"referenceID": 0, "context": ", 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data. This observation has also led to the interest in building a large-scale annotated dataset for question-answering. In 2015, Bordes et al. (2015) released a large-scale dataset of 100k open-world question-answer pairs constructed from Freebase, and Hermann et al.", "startOffset": 33, "endOffset": 269}, {"referenceID": 0, "context": ", 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data. This observation has also led to the interest in building a large-scale annotated dataset for question-answering. In 2015, Bordes et al. (2015) released a large-scale dataset of 100k open-world question-answer pairs constructed from Freebase, and Hermann et al. (2015) released two datasets,", "startOffset": 33, "endOffset": 394}, {"referenceID": 4, "context": "The latter was followed by Hill et al. (2015), Rajpurkar et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 4, "context": "The latter was followed by Hill et al. (2015), Rajpurkar et al. (2016) and Onishi et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 4, "context": "The latter was followed by Hill et al. (2015), Rajpurkar et al. (2016) and Onishi et al. (2016), each of which has released a set of large-scale closed-", "startOffset": 27, "endOffset": 96}, {"referenceID": 5, "context": "We evaluate this new dataset, to which we refer as SearchQA, with a variant of recently proposed attention sum reader (Kadlec et al., 2016) and with human volunteers.", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "example of such a dataset is SimpleQA by (Bordes et al., 2015).", "startOffset": 41, "endOffset": 62}, {"referenceID": 4, "context": "One family of such datasets includes Children\u2019s Book dataset (Hill et al., 2015), CNN and DailyMail (Hermann et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 3, "context": ", 2015), CNN and DailyMail (Hermann et al., 2015).", "startOffset": 27, "endOffset": 49}, {"referenceID": 10, "context": "This family includes SQuAD (Rajpurkar et al., 2016) and NEWSQA (Trischler et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 12, "context": ", 2016) and NEWSQA (Trischler et al., 2016).", "startOffset": 19, "endOffset": 43}, {"referenceID": 8, "context": "MS MARCO (Nguyen et al., 2016)\u2013the most recently released dataset to our knowledge\u2013 is perhaps most similar to the proposed SearchQA.", "startOffset": 9, "endOffset": 30}, {"referenceID": 8, "context": "MS MARCO (Nguyen et al., 2016)\u2013the most recently released dataset to our knowledge\u2013 is perhaps most similar to the proposed SearchQA. Nguyen et al. (2016) selected a subset of actual user-generated queries to Microsoft Bing that correspond to questions.", "startOffset": 10, "endOffset": 155}, {"referenceID": 13, "context": ", 2016) is a variant of a pointer network (Vinyals et al., 2015) that was specifically constructed to solve a cloze-style questionanswering task.", "startOffset": 42, "endOffset": 64}, {"referenceID": 6, "context": "We use Adam (Kingma and Ba, 2014) and dropout (Srivastava et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 11, "context": "We use Adam (Kingma and Ba, 2014) and dropout (Srivastava et al., 2014) for training.", "startOffset": 46, "endOffset": 71}], "year": 2017, "abstractText": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet\u2019s URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.", "creator": "LaTeX with hyperref package"}}}