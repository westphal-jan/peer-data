{"id": "1704.05228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees", "abstract": "Prominent applications of sentiment analysis are countless, including areas such as marketing, customer service and communication. The conventional bag-of-words approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we thus develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label (sub-)clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetoric structure, we propose a tensor-based, tree-structured deep neural network (named RST-LSTM) in order to process the complete discourse tree. The underlying attention mechanism infers the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation (node reordering and artificial leaf insertion) that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Ultimately, this work advances the status quo in natural language processing by developing algorithms that incorporate semantic information.", "histories": [["v1", "Tue, 18 Apr 2017 08:24:20 GMT  (395kb,D)", "http://arxiv.org/abs/1704.05228v1", null], ["v2", "Mon, 9 Oct 2017 08:03:06 GMT  (663kb,D)", "http://arxiv.org/abs/1704.05228v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mathias kraus", "stefan feuerriegel"], "accepted": false, "id": "1704.05228"}, "pdf": {"name": "1704.05228.pdf", "metadata": {"source": "CRF", "title": "Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees", "authors": ["Mathias Krausa", "Stefan Feuerriegel"], "emails": ["mathias.kraus@is.uni-freiburg.de;", "mathias.kraus@is.uni-freiburg.de", "stefan.feuerriegel@is.uni-freiburg.de"], "sections": [{"heading": null, "text": "As a remedy, we therefore develop a discourse-conscious method based on the discourse structure of documents. To learn from the resulting rhetorical structure, we propose a tensor-based, tree-structured deep neural network (called RST-LSTM) to label the complete discourse tree. The underlying attention mechanism violates the prominent passages of narrative materials. In addition, we propose two algorithms for the rhetorical structure: a tensor-based, tree-structured deep-neuronal network structure (called RST-LSTM) to process the entire discourse tree."}, {"heading": "1. Introduction", "text": "In this context, the derivation of subjective information from the net proves to be inescapable. [8] In this context, the derivation of information from the net proves to be inescapable. [9] In this context, the derivation of information from the net proves to be inescapable."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Rhetorical structure theory", "text": "Rhetorical structural theory formalizes the discourse in narrative materials by assembling sub-clauses, sentences, and paragraphs into a hierarchy [13]. The premise is that a document is divided into elementary units of discourse, which form the smallest, indivisible segments. These EDUs are then connected by one of 18 different types of relation that represent edges in the discourse tree; see Table 1 for a list [19]. Each relationship is further designated by a hierarchy type, i.e. either a nucleus (N) or satellite (S), with a nucleus representing a more substantial unit of information, while a satellite indicates a supporting or background unit of information. We note that this type of hierarchy is not necessarily exclusive or is because both children can be designated as nucleus (N) or satellites at the same time. Figure 2 provides an example of a treatment of discourse. [56] Previous research has suggested various methods for automating the discourse trees of documents."}, {"heading": "2.2. Sentiment analysis with RST", "text": "The underlying stigmatisation can be seen almost exclusively on the basis of prefabricated stigmatisation effects."}, {"heading": "2.3. Representation learning for sequential and tree data", "text": "S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"s.\" s. \"s\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" S \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s\" s. \"s\" s. \"s.\" s \"s\" s. \"s\" s \"s\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S. \"s.\" S \"s.\" S \"s\" s. \"S\" s \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S"}, {"heading": "3. Discourse-based sentiment analysis with deep learning", "text": "This section introduces our discourse-based methodology, which derives mood values from text materials. Figure 3 illustrates the underlying framework and divides the method into steps for discourse parsing, calculating low polarity characteristics, data expansion, and prediction. The prediction phase implements either one of the basic lines (e.g. LSTM or Tree-LSTM) or our proposed RST-LSTM."}, {"heading": "3.1. Discourse parsing", "text": "We create discourse trees for our data sets by using the DPLP parser [21]. For simplicity, we introduce the following notation: We designate the relationship type of the node i as \u03c1i element (elaboration, reasoning,...). The complete list of relationship types is in Table 1. It should also be noted that \u03c0i indicates a path from the root to a specific node i in the tree, which is given by a continuous list of node (N) or satellite (S) traverses. We then designate the relationship type i as \u03c1\u03c0i. For example, \u03c1SN refers to the relationship type of the node SN, which is the nucleus 11in the first satellite branch. In this respect, the symbol R represents the root node of a discourse tree and thus provides the relationship type of the root. In addition, we designate the list of relationship types of all nodes in a specific RST tree as the RST to this type."}, {"heading": "3.2. Polarity features", "text": "We follow common procedures in sentiment analysis and use a predefined dictionary that identifies terms as positive or negative [1,2]. This approach has several advantages, as it is independent of the area and works reliably even with a few training observations. Furthermore, it is easy to replace the underlying dictionary with one that not only measures polarity or negativity, but also deals with other linguistic terms such as subjectivity, certainty or the domain-specific tone. Our experimental results are based on the SentiWordNet 3.0 dictionary [30], which provides sentiment labels for 117,659 words. Based on the sentiment labels at the word level, we then return to calculating the sentiment score \u03c3i for each EDU i via\u03c3i = 1 | [w | w | w | i}."}, {"heading": "3.3. Tree-LSTM baseline", "text": "It is not the only forgotten genera for every child. This enables each parent to form a hidden gate cell for children. (D) It is not the only one. Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-Gate-"}, {"heading": "3.4. RST-LSTM", "text": "(W) s (W) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S (S) s (S) s (S (S) s) s (S (S) s) s (S (S) s) s (S (S) s) s (S (S) s) s (S (S) s) s (S (S) s) s (S (S (S) s) s (S (S (S) s) s (S (S (S) s) s (S (S (S) s) s (S (S (S (S (S) s) s) s (S (S (S (S (S (S (S (S (S) s) s) s) s (S (S (S (S (S (S (S (S) s) s) s) s (S (S (S (S (S (S (S (S (S (S (S (S) s) s) s) s) s (S (S (S (S (S (S (S (S (S (S (s) s) s) s) s) s) s) s) s (S (S (S (S (S (S (S (S (S (S (S (S (S (S (s) s) s) s) s) s"}, {"heading": "3.5. Training data augmentation", "text": "Deep neural networks typically have a complex structure with thousands of weights that need to be trained, making them prone to overadjustment. A workable remedy is to artificially increase the number of training samples in order to better adjust the parameters. Such approaches are common in computer vision, where different crops are extracted from the same image and later considered a training instance. We therefore propose similar techniques for tree structures that increase our training set. These algorithms take a tree as input and then slightly change its structure in each training period (a complete training cycle on the training set). The first variant, known as node reordering, replaces subtrees, while the second, the artificial insertion of a leaf, randomly swaps a leaf for a knot with two new children."}, {"heading": "3.5.1. Node reordering", "text": "This means that the text passages within the nodes must maintain their original order, otherwise the content could change its meaning or grammatical structure. Therefore, our approach randomly selects an inner node n and moves it to the position of its sibling m in the tree. Sibling m is then moved down the tree and becomes a child of n. Subsequently, the previous position of n is occupied by one of its previous children. Consequently, the order of l, r, and m from left to right remains unchanged. The corresponding algorithm for an inner node n is shown in Figure 7. Overall, this data augmentation approach attempts to change the structure slightly, thereby generating potentially different representations of the same tree. The extent of the reordering depends on the level n, because a reordering of a node at a higher level usually has a greater effect on the overall tree structure than a reordering at a lower level."}, {"heading": "3.5.2. Artificial leaf insertion", "text": "The insertion of leaves into a subtree is shown in Figure 8. This approach randomly selects a leaf n from the tree and adds two newly created child nodes l and r, which then represent the leaves, while the n becomes an inner node. We calculate \u03c3l and \u03c3r by multiplying \u03c3n by random weights \u03c9 [0, 1] and (1 \u2212 \u03c9), i.e. \u03c3l = \u03c3n, (35) \u03c3r = (1 \u2212 \u03c9) \u03c3n. (36) Therefore, these update rules try to leave the overall information unchanged, but distribute the values of n to two separate children with a certain ratio \u043e. Finally, we randomly select the conversion type \u0432n and the hierarchy type \u0441n. (20)"}, {"heading": "4. Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "Building on previous work, we use two common sets of data with movie reviews and corresponding user ratings. The first set of data consists of 2000 Rotten Tomatoes movie reviews [31], for which we perform 10-fold cross-validation and then calculate predictive performance on average across splits. The second set consists of 50,000 reviews from the Internet Movie Database (IMDb), which are evenly divided into 25,000 reviews for training purposes and 25,000 reviews for testing purposes [32]. It includes a maximum of 30 reviews for a movie, as ratings for the same movie tend to have correlated ratings. In addition, the training and test sets contain a disjoint set of films to avoid correlation based on film terms. Within the training and test sets, 12,500 reviews are designated as positive and 12,500 reviews as negative to provide a balanced pattern. All ratings are processed as follows: We perform tokenization, convert characters to lower case letters, and enjoy the latter by lower case letters. \""}, {"heading": "4.2. Descriptive statistics", "text": "Overall, the Rotten Tomatoes corpus contains 15,462 positive, 20,153 negative and 842,537 neutral terms in the sense of the SentiWordNet dictionary. Consequently, 1.76% of the words are considered positive, while 2.29% convey a negative connotation. IMDb reviews include 213,723 positive, 255,335 negative and 9,031,977 neutral words, resulting in 2.25% positive and 2.69% negative words. The resulting discourse trees have the following characteristics. In the case of reviews from Rotten Tomatoes, they average 51.09 EDUs, while this number drops to 19.79 EDUs for IMDb reviews. The largest discourse tree contains 154 levels. Table 3 reports on the relationship types and frequencies in the corpus.22"}, {"heading": "4.3. Bag-of-words baseline", "text": "As a second baseline, we also scale term frequencies using the term frequency approach (tf-idf), which weights characteristic terms more strongly [11]. Both feature spaces are then inserted into a random forest, since this traditional classifier for machine learning can detect highly nonlinear relationships, but still performs satisfactorily outside the box. These benchmarks allow us to distinguish the mood conveyed by words from that transmitted by the discourse structure. 23"}, {"heading": "4.4. Model evaluation", "text": "To fine-tune the model parameters (see Table 4), we proceed as follows: In the case of the random forest baseline, we identify the optimal parameters using a grid search along with a 10x cross validation applied to the training set. In contrast, we reduce the computing requirements of deep learning architectures by using 20% of the training data in each period as a validation set. After each period, we remix the observations and expand our training set by constructing additional samples based on our data augmentation technique."}, {"heading": "5. Results", "text": "In this section we evaluate the performance of our RST-LSTM and compare it with previous baselines. The evaluation provides evidence that the inclusion of semantic structures in the task of mood analysis improves predictive power."}, {"heading": "5.1. Dataset 1: movie reviews from Rotten Tomatoes", "text": "Table 5 describes the prediction results for the Rotten Tomatoes film review dataset. The Nave benchmark with tf-idf features yields a balanced accuracy of 0.746 and an F1 score of 0.763. However, the tffeatures and LSTM approach achieves a similar performance. Here, we see no clear indication that one of the baselines is consistently better than another. However, when comparing the pruned RST trees, we find that a higher cutting depth also increases predictive performance. For example, the best results come from an RST tree pruned on level 4, achieving a balanced accuracy of 0.758 and an F1 score of 0.753. However, this value is still below the best scores of the actual benchmarks. Furthermore, the inclusion of the F1-R type reduces predictive power compared to the approach with only sentiment scores of 0.007."}, {"heading": "5.2. Dataset 2: IMDb movie reviews", "text": "Table 6 reports the predictive results for the larger of the two datasets based on 50,000 IMDb movie reviews. Random Forest with tf-idf achieves a performance superior to the previous task and yields an accuracy of 0.825 and an F1 score of 0.823. In terms of deep learning, the LSTM baseline improves accuracy by 0.006 to 0.831, but slightly decreases the F1 score by 0.002. Accuracy is further increased by the use of pruned discourse trees. Again, the most powerful trait group is achieved by felling the tree at level 4, resulting in an accuracy of 0.839 (an increase of 0.014 over the reference values) and an F1 score of 0.837 (an improvement of 0.006). We also observe mixed results in terms of performance when incorporating the transformation type between LR.Again, tree-structured LSTMs exceed overall accuracy."}, {"heading": "5.3. Comparison", "text": "In addition, we compare our RST-LSTM with the relativity-specific attention mechanism in [17], which, on the contrary, summarizes the representations in each recursive call and, therefore, cannot distinguish between core and satellite. Moreover, its approach uses a recursive neural network, which is known to suffer from disappearing or exploding gradients [18]. In response to such inadequacies, we decided to resort to a long-term short-term memory. We proceed as follows in order to evaluate the attention mechanism itself specifically and leave all other parameters unchanged (i.e. identical to the 27previous experiments). In other words, we feed the networks by using characteristics at EDU level derived from the previous dictionary-based sentiment scores. The performance measurements indicate that the resulting predictive accuracy is worse than the RST-LSTM. For the data set of Red Tomatoes, its approach thus represents an accuracy of 0.0000000.000.000.000.000.000.000.000.000.000.000.000.0.000.000.000.000.000.000.000.000.000.000.000.0.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000"}, {"heading": "5.4. Discussion", "text": "We are now investigating the trained weights of our tensor-based attention mechanisms within the RST-LSTM. This facilitates insights into how the neural network processes the discourse and derives the mood from the semantic structure of textual materials. Figure 9 compares the normalized weights of the tensors U (u) m across different relationship types m. The values result from the use of a child sum RST-LSTM without data augmentation. Overall, the tensor weights between the two data sets are strongly correlated, with the corresponding correlation coefficient standing at 0.640, statistically significant at the level of 99%. However, we observe large differences in the relative meaning between the relationship types. For example, relationship types such as background and textual organization are only a marginal meaning that is in line with initial expectations."}, {"heading": "6. Conclusion", "text": "While these models usually achieve high predictive power when applied to short texts, the complexity of linguistic discourse impedes performance in relation to longer documents. As a remedy, our paper proposes an innovative, discourse-conscious approach: we first analyze the semantic structure based on rhetorical structural theory, thereby tailoring the document to a discourse tree that encodes its action. We apply tailored, in-depth neural networks with an additional attention mechanism that allows us to learn the full discourse tree directly. Each of the architectures implies more than 10,000 parameters, making the models highly non-linear. Our results show that our RST-LSTM essentially exceeds the basics."}], "references": [{"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval 2 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Techniques and applications for sentiment analysis", "author": ["R. Feldman"], "venue": "Communications of the ACM 56 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining comparative opinions from 32  customer reviews for competitive intelligence", "author": ["K. Xu", "S.S. Liao", "J. Li", "Y. Song"], "venue": "Decision Support Systems 50 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "An empirical analysis of the antecedents of electronic commerce service continuance", "author": ["A. Bhattacherjee"], "venue": "Decision Support Systems 32 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "in: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL \u201905)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining online reviews for predicting sales performance: A case study in the movie domain", "author": ["X. Yu", "Y. Liu", "X. Huang", "A. An"], "venue": "IEEE Transactions on Knowledge and Data Engineering 24 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Does chatter really matter? Dynamics of usergenerated content and stock performance", "author": ["S. Tirunillai", "G.J. Tellis"], "venue": "Marketing Science 31 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "News-based trading strategies", "author": ["S. Feuerriegel", "H. Prendinger"], "venue": "Decision Support Systems 90 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Negation scope detection in sentiment analysis: Decision support for news-driven trading", "author": ["N. Pr\u00f6llochs", "S. Feuerriegel", "D. Neumann"], "venue": "Decision Support Systems 88 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Whose and what chatter matters? The effect of tweets on movie sales", "author": ["H. Rui", "Y. Liu", "A. Whinston"], "venue": "Decision Support Systems 55 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Foundations Of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "MIT Press, Cambridge MA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Advances in natural language processing", "author": ["J. Hirschberg", "C.D. Manning"], "venue": "Science 349 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["W.C. Mann", "S.A. Thompson"], "venue": "Text-Interdisciplinary Journal for the Study of Discourse 8 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1988}, {"title": "F", "author": ["A. Hogenboom", "F. Frasincar"], "venue": "de Jong, U. Kaymak, Using rhetorical structure in sentiment analysis, Communications of the ACM 58 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving sentiment analysis with document-level semantic relationships from rhetoric discourse structures", "author": ["J. M\u00e4rkle-Hu\u00df", "S. Feuerriegel", "H. Prendinger"], "venue": "50th Hawaii International Conference on System Sciences ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "F", "author": ["A. Hogenboom", "F. Frasincar"], "venue": "de Jong, U. Kaymak, Polarity classification using structure-based vector representations of text, Decision Support Systems 74 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural discourse structure for text categorization", "author": ["Y. Ji", "N. Smith"], "venue": "arXiv preprint arXiv:1702.01829 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks 5 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "F", "author": ["B. Heerschop", "F. Goossen", "A. Hogenboom", "F. Frasincar", "U. Kaymak"], "venue": "de Jong, Polarity analysis of texts using discourse structure, in: Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM \u201911)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Hilda: A discourse parser using support vector machine classification", "author": ["H. Hernault", "H. Prendinger", "D.A. DuVerle", "M. Ishizuka", "T. Paek"], "venue": "Dialogue and Discourse 1 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Representation learning for text-level discourse parsing", "author": ["Y. Ji", "J. Eisenstein"], "venue": "in: Proceedings of the 52nd Annual Meeting Annual Meeting on Association for Computational Linguistics (ACL \u201914)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Fine-grained sentiment analysis with structural features", "author": ["C. Zirn", "M. Niepert", "H. Stuckenschmidt", "M. Strube"], "venue": "in: Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP \u201911)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Extracting sentiment as a function of discourse structure and topicality", "author": ["M. Taboada", "K. Voll", "J. Brooke"], "venue": "Simon Fraser Univeristy School of Computing Science Technical Report ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Rhetorical structure theory for polarity estimation: An experimental study", "author": ["J.M. Chenlo", "A. Hogenboom", "D.E. Losada"], "venue": "Data & Knowledge Engineering 94 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Better document-level sentiment analysis from RST discourse parsing", "author": ["P. Bhatia", "Y. Ji", "J. Eisenstein"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \u201915)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning: Adaptive Computation And Machine Learning Series", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press, Cambridge MA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning representations by back-propagating errors", "author": ["D. Williams", "G.E. Hinton"], "venue": "Nature 323 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1986}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL \u201915)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "F", "author": ["S. Baccianella", "A. Esuli"], "venue": "Sebastiani, SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining, in: Proceedings of the International Conference on Language Resources and Evaluation (LREC \u201910)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "in: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL \u201904)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "in: Proceedings of the 49th Annual Meeting on Association for Computational Linguistics (ACL \u201911)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program 14 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1980}], "referenceMentions": [{"referenceID": 0, "context": "In this context, sentiment analysis facilitates the extraction of subjective information from user-generated content, or narrative materials in general, by quantifying the positivity or negativity of natural language [1, 2].", "startOffset": 217, "endOffset": 223}, {"referenceID": 1, "context": "In this context, sentiment analysis facilitates the extraction of subjective information from user-generated content, or narrative materials in general, by quantifying the positivity or negativity of natural language [1, 2].", "startOffset": 217, "endOffset": 223}, {"referenceID": 2, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 139, "endOffset": 148}, {"referenceID": 5, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 139, "endOffset": 148}, {"referenceID": 6, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 139, "endOffset": 148}, {"referenceID": 7, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 178, "endOffset": 184}, {"referenceID": 8, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 178, "endOffset": 184}, {"referenceID": 5, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 206, "endOffset": 213}, {"referenceID": 9, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 206, "endOffset": 213}, {"referenceID": 0, "context": "Sentiment analysis traditionally utilizes bag-of-words approaches, which merely count the frequency of words (and tuples thereof) to obtain a mathematical representation of documents in matrix form [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 1, "context": "Sentiment analysis traditionally utilizes bag-of-words approaches, which merely count the frequency of words (and tuples thereof) to obtain a mathematical representation of documents in matrix form [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 10, "context": "A remedy to this problem may be found in the form of n-grams, which construct tuples of n contiguous words from a text and thus incorporate the contextual information of words into the representation of text [11].", "startOffset": 208, "endOffset": 212}, {"referenceID": 0, "context": "Here research commonly utilizes n-grams of size two (bi-grams) and three (tri-grams), which are often enriched with additional features such as part-of-speech [1].", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "A recent review in Science points out that novel techniques are required to leverage semantics [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "RST structures documents hierarchically [13] by splitting the content into (sub-)clauses called elementary discourse units (EDUs).", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "For instance, one can reweigh the importance of passages based on their relation type [14] or depth [15] in the discourse tree.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "For instance, one can reweigh the importance of passages based on their relation type [14] or depth [15] in the discourse tree.", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "2 or 4 levels [15].", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Other approaches train machine learning classifiers based on the relation types as input features [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "The RST-LSTM also differs from the attention mechanism in [17], which can only exploit the relation type and not the hierarchy.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Furthermore, former approaches are based on traditional recursive neural networks, which are limited by the fact that they can persist information for only a few iterations [18].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "Rhetorical structure theory Rhetorical structure theory formalizes the discourse in narrative materials by organizing sub-clauses, sentences and paragraphs into a hierarchy [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "These EDUs are then connected by one of 18 different relation types, which represent edges in the discourse tree; see Table 1 for a list [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Table 1: Overview of the different relation types that connect elementary discourse units [19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Common implementations for documents consisting of multiple paragraphs are represented by the high-level discourse analyzer HILDA [20] and the DPLP parser [21], of which the DPLP parser currently achieves the better F1-score in identifying relation types .", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "Common implementations for documents consisting of multiple paragraphs are represented by the high-level discourse analyzer HILDA [20] and the DPLP parser [21], of which the DPLP parser currently achieves the better F1-score in identifying relation types .", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 18, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 13, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 15, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 22, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "A recent approach calculates ex ante vector representations for the EDUs based on word embeddings [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Among the most common methods are simple weighting rules that aggregate the sentiment scores of EDUs based on the tree structure [19, 14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 13, "context": "Among the most common methods are simple weighting rules that aggregate the sentiment scores of EDUs based on the tree structure [19, 14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 22, "context": "the root node) of the discourse tree and scale the relative importance based on (hand-crafted) weights [23, 19, 14].", "startOffset": 103, "endOffset": 115}, {"referenceID": 18, "context": "the root node) of the discourse tree and scale the relative importance based on (hand-crafted) weights [23, 19, 14].", "startOffset": 103, "endOffset": 115}, {"referenceID": 13, "context": "the root node) of the discourse tree and scale the relative importance based on (hand-crafted) weights [23, 19, 14].", "startOffset": 103, "endOffset": 115}, {"referenceID": 23, "context": "The underlying weights can also be optimized using logistic regression [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "In contrast, other works specifically focus on the hierarchy labels at leaf level, arguing that this choice facilitates a more fine-grained evaluation [14], while neglecting the discourse tree from above.", "startOffset": 151, "endOffset": 155}, {"referenceID": 13, "context": "To fill the gap between top-level and leaf-level analysis, recent research also applies a recursive weighting scheme that utilizes a scaling factor to diminish the influence of increasing depth [14, 15].", "startOffset": 194, "endOffset": 202}, {"referenceID": 14, "context": "To fill the gap between top-level and leaf-level analysis, recent research also applies a recursive weighting scheme that utilizes a scaling factor to diminish the influence of increasing depth [14, 15].", "startOffset": 194, "endOffset": 202}, {"referenceID": 14, "context": "2 or 4 levels [15].", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 56, "endOffset": 68}, {"referenceID": 23, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 56, "endOffset": 68}, {"referenceID": 13, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 56, "endOffset": 68}, {"referenceID": 21, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "A potential remedy is to traverse the RST tree with a recursive neural network [25]; however, this approach only incorporates the edges and lacks information regarding the relation type.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "attention mechanism [17].", "startOffset": 20, "endOffset": 24}, {"referenceID": 25, "context": "Representation learning for sequential and tree data Recent advances in deep neural networks have rendered it possible to learn representations of unstructured data such as sequences or text [26].", "startOffset": 191, "endOffset": 195}, {"referenceID": 26, "context": "This can, for instance, be achieved by recurrent neural networks, which entail an internal architecture in the form of a directed cycle, thereby creating an internal state with which to learn dependent structures [27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 17, "context": "However, in practice, information only persists for a few iterations [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "These enhance recurrent neural networks by capturing long dependencies among input signals [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "This tree-structured LSTM network traverses trees bottomup in order to generate representations of the underlying structure [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "[2 5 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d D P L P H ie r a r c h ic a l w e ig h t in g r u le 7 3 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[2 5 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d D P L P H ie r a r c h ic a l w e ig h t in g r u le 7 3 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 4 ] B L O G S 0 6 B lo g p o s t r a n k in g W e ig h t s o p t im iz e d D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 3 7 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[2 4 ] B L O G S 0 6 B lo g p o s t r a n k in g W e ig h t s o p t im iz e d D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 3 7 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1 9 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d S P A D E P o s it io n -b a s e d w e ig h t in g r u le 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[1 9 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d S P A D E P o s it io n -b a s e d w e ig h t in g r u le 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1 4 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d H IL D A P o s it io n -b a s e d w e ig h t in g 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[1 4 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d H IL D A P o s it io n -b a s e d w e ig h t in g 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1 6 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g W e ig h t s o p t im iz e d b y S V M D ic t io n a r y -b a s e d S P A D E T o p a n d le a fs p li t w e ig h t in g 3 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[1 6 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g W e ig h t s o p t im iz e d b y S V M D ic t io n a r y -b a s e d S P A D E T o p a n d le a fs p li t w e ig h t in g 3 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "J i a n d S m it h [1 7 ] Y e lp r e v ie w s , U s e r r a t in g o r T r a n s fo r m a t io n o f R S T t r e e H id d e n s t a t e s o f D P L P A v e r a g e o f a ll E D U s 7 7 7", "startOffset": 19, "endOffset": 25}, {"referenceID": 6, "context": "J i a n d S m it h [1 7 ] Y e lp r e v ie w s , U s e r r a t in g o r T r a n s fo r m a t io n o f R S T t r e e H id d e n s t a t e s o f D P L P A v e r a g e o f a ll E D U s 7 7 7", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "[1 5 ] F in a n c ia l d is c lo s u r e s D ir e c t io n a l s t o c k W e ig h t s o p t im iz e d b y g r id -s e a r c h D ic t io n a r y -b a s e d H IL D A H ie r a r c h ic a l w e ig h t in g r u le 7 3 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[1 5 ] F in a n c ia l d is c lo s u r e s D ir e c t io n a l s t o c k W e ig h t s o p t im iz e d b y g r id -s e a r c h D ic t io n a r y -b a s e d H IL D A H ie r a r c h ic a l w e ig h t in g r u le 7 3 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 3 ] O p in io n s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t s D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 7 U p t o le v e l 1 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[2 3 ] O p in io n s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t s D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 7 U p t o le v e l 1 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 2 ] M D S A U s e r r a t in g D ic t io n a r y -b a s e d M a r k o v lo g ic n e t w o r k C o n t r a s t iv e a n d 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 2 ] M D S A U s e r r a t in g D ic t io n a r y -b a s e d M a r k o v lo g ic n e t w o r k C o n t r a s t iv e a n d 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 20, "context": "Discourse parsing We generate discourse trees for our datasets by utilizing the DPLP parser [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Polarity features We follow common procedures in sentiment analysis and utilize a pre-defined dictionary that labels terms as positive or negative [1, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 1, "context": "Polarity features We follow common procedures in sentiment analysis and utilize a pre-defined dictionary that labels terms as positive or negative [1, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 29, "context": "0 dictionary [30], which provides sentiment labels for 117,659 words.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "Tree-LSTM baseline We draw upon the Tree-LSTM as a baseline, since it is widely regarded as the status quo for tree learning [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "Figure 4) and adapts the ideas of both a memory cell and gates from traditional LSTMs, but extends these concepts to tree structures [29].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Our experiments later compare the performance of two different architectures of Tree-LSTM models, namely, the child-sum and N -ary Tree-LSTM [29].", "startOffset": 141, "endOffset": 145}, {"referenceID": 25, "context": "with the negative log-likelihood of the true class label y as the cost function [26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "We compute \u03c3l and \u03c3r by multiplying \u03c3n by random weights \u03c9 \u2208 [0, 1] and (1\u2212 \u03c9), i.", "startOffset": 61, "endOffset": 67}, {"referenceID": 30, "context": "The first consists of 2000 movie reviews from Rotten Tomatoes [31], for which we perform 10-fold cross-validation and then average the predictive performance across splits.", "startOffset": 62, "endOffset": 66}, {"referenceID": 31, "context": "The second dataset comprises 50,000 reviews from the Internet Movie Database (IMDb), which are split evenly into 25,000 reviews for training and 25,000 for testing [32].", "startOffset": 164, "endOffset": 168}, {"referenceID": 32, "context": "\u201cenjoyed\u201d and \u201cenjoying\u201d are both reduced to \u201cenjoy\u201d [33].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "As a second baseline, we also scale the term frequencies using the term frequency-inverse document frequency approach (tf-idf), which puts stronger weights on characteristic terms [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 16, "context": "Comparison We additionally compare our RST-LSTM to the relation-specific attention mechanism in [17], which, in contrast, sums the representations in each recursive call and hence cannot distinguish between nucleus and satellite.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "In addition, their approach utilizes a recursive neural network, which is known to suffer from vanishing or exploding gradients [18].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "Prominent applications of sentiment analysis are countless, including areas such as marketing, customer service and communication. The conventional bag-ofwords approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we thus develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label (sub-)clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetoric structure, we propose a tensor-based, tree-structured deep neural network (named RST-LSTM) in order to process the complete discourse tree. The underlying attention mechanism infers the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation (node reordering and artificial leaf insertion) that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Ultimately, this work advances the status quo in natural language processing by developing algorithms that incorporate semantic information.", "creator": "LaTeX with hyperref package"}}}