{"id": "1704.05325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Anomaly detection and motif discovery in symbolic representations of time series", "abstract": "The advent of the Big Data hype and the consistent recollection of event logs and real-time data from sensors, monitoring software and machine configuration has generated a huge amount of time-varying data in about every sector of the industry. Rule-based processing of such data has ceased to be relevant in many scenarios where anomaly detection and pattern mining have to be entirely accomplished by the machine. Since the early 2000s, the de-facto standard for representing time series has been the Symbolic Aggregate approXimation (SAX).In this document, we present a few algorithms using this representation for anomaly detection and motif discovery, also known as pattern mining, in such data. We propose a benchmark of anomaly detection algorithms using data from Cloud monitoring software.", "histories": [["v1", "Tue, 18 Apr 2017 13:19:50 GMT  (966kb,D)", "http://arxiv.org/abs/1704.05325v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["fabio guigou", "pierre collet", "pierre parrend"], "accepted": false, "id": "1704.05325"}, "pdf": {"name": "1704.05325.pdf", "metadata": {"source": "CRF", "title": "Anomaly detection and motif discovery in symbolic representations of time series", "authors": ["Fabio Guigou", "Pierre Collet", "Pierre Parrend"], "emails": [], "sections": [{"heading": null, "text": "Anomaly detection and motif detection in symbolic representations of the time seriesFabio Guigou, Pierre Collet, Pierre ParrendTECHNICAL REPORT No69427 / 2Wednesday, 19 April 20174PFactory: E-Laboratory on the Factory of the FutureA RK un itw in -c s. o rg / a rk: / 69 42 7 / 02ar Xiv: 170 4.05 325v 1 [cs.A I] 1 8 A pr 2 01 7Anomaly detection and motif detection in symbolic representations of the time seriesFabio Guigou Pierre Collet Pierre ParrendWednesday, 19 April 2017AbstractThe advent of the big data hype and the consistent acquisition of event logs and real-time data from sensors, the monitoring of software and machine configuration has generated an enormous amount of time-varying data in about every sector of the industry. Rule-based processing of data has led to this pattern being relevant in many scenarios and in many of the early mountain scenarios."}, {"heading": "1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9", "text": "1.1 Problem 91.2 Change and anomaly in time series 91.2.1 Symbolic representations.........................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2 The SAX representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Anomaly detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13", "text": "3.1 Hot SAX 133.2 Sequitur 153.3 Chaos Game Representation 17"}, {"heading": "4 Motif discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19", "text": "4.1 Minimum Description Length 194.2 Grammar Inference 194.3 MK Algorithm 204.4 Motif Tracking Algorithm 204.5 Approximate Mining Motifs 20"}, {"heading": "5 Experimental evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25", "text": "..................................................................................................................................."}, {"heading": "6 Conclusion and perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Scientific validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33", "text": "3List of Figures5.1 CPU load anomaly stuck at 100%..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Problem statement", "text": "We look at the problem of detecting anomalies and determining baseline behavior in time series captured by network surveillance software. Since the introduction of such software, error detection is largely done by querying a number of metrics stored for expert analysis and applying simple decision rules based on the last data points of each series independently, such as signaling an error when all points remain above a certain threshold within a short analysis window. Although this approach has worked well enough to keep data centers running, it still fails to capture many important events, such as deviations from a normal baseline or early signs of failure. Although algorithms exist to perform such tasks, they require significant hardware upgrades to monitor. We are exploring alternative methods to fill the gap between primitive expert systems and costly anomaly and pattern mining algorithms."}, {"heading": "1.2 Change and anomaly detection in time series", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.2.1 Symbolic representations", "text": "To solve this problem, many alternative representations for time series have been proposed: discrete Fourier transformation, wavelet transformation [1], singular value decomposition... SAX was proposed in 2003 [2] as a simple, compact, text-based representation that reduces dimensionality and allows the use of string processing algorithms to analyze time series [3]. Applications have been made for time series indexing [4], visualization [5] and various mining tasks [6, 7, 8]. Applications have even been tried on objects that are remotely associated with time series, such as motion detection [9]."}, {"heading": "1.2.2 On raw data", "text": "Other statistical approaches have been applied to raw time series data. Changing point detection, i.e. detecting when a model no longer fits the data and a new one needs to be derived, has been implemented in many ways: using Bayesian models [10] or sequential testing [11, 12] for online detection. These methods have proven to be computationally efficient and able to reliably detect structural changes in the data stream, but do not offer the wide range of applications associated with symbolic representation, especially string representation. In this report, we focus on the possibilities that this paradigm offers."}, {"heading": "1.3 Context and goals", "text": "In the ever-growing area of cloud computing and cloud networking, service providers face the challenge of operating a large number of physical and virtual devices while maintaining a contracted level of service known as a SLA (Service Level Agreement), which requires strict monitoring of the entire infrastructure. Monitoring software typically surveys devices that use ICMP and SNMP, collects data such as response time, CPU load, memory usage, and compares these values to thresholds. If the alarm threshold is exceeded, a kind of warning is sent to administrators. 9CHAPTER 1. INTRODUCTION 10 While such software \"does its job,\" it is often inaccurate, gives many false positives or fails too late if the device has already failed or a customer has already opened a case for performance deterioration. These deficiencies indicate the need for more advanced techniques, such as detecting behavioral abnormalities that can improve or replace the current threshold."}, {"heading": "2 The SAX representation", "text": "SAX is a time series representation designed to greatly reduce data dimensionality and redundancy through subsampling and quantization = 43. Typical settings use a subsampling factor n from 8 to 10 and only \u03b1 = 3 or 4 bins for quantization. It is generally accepted that the effects of this parameter are small. The basic algorithm works across the entire time series by applying a z-normalization (setting the mean to 0 and the default deviation to 1), replacing any non-overlapping window of n points with its average - a process known as Piecewise Aggregate Approximation (PAA) - and a Gaussian quantization of this PAA into a binder.It is worth noting that converting a time series into a string is not a familiar operation, applying the substamping and quantizing process is the basis of any compression algorithm, an audit law for JPEG..."}, {"heading": "3 Anomaly detection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Hot SAX", "text": "The question that arises is whether the search is a way in which an anomaly with the highest distance to another sequence, i.e. a quadratic number of distance calculations, is not suitable for more than a few hundred points. Note: The closest neighborhood of a subsequence is only searched among other subsequences that do not have a common point. Naive implementation is shown in Algorithm 3: Algorithm 3 brute force method BRUTE FORUTE-FORT-FORT-FORT-SQs with other subsequences is shown in algorithms."}, {"heading": "3.2 Sequitur", "text": "The sequitur [15] algorithms proposed by Nevill-Manning in 1997 [16] are a kind of dictatorial condensation, using the concepts of symbol, rule and diagram to build a compact representation of their input data. In Sequitur, a symbol is either an input token (e.g. a single letter) or a token representing a rule; a rule is a symbol representing a digram; and a digram is a pair of symbols. Thus, in a string, \"abc,\" \"b\" and \"c\" are symbols and \"bc\" are digrams that have the form, \"meaning that a string\" Ac \"can be read as\" abc. \""}, {"heading": "3.3 Chaos game representation", "text": "Representation of the Chaos Game [20, 21] is a way to create a bitmap = i.e. a string generated by an alphabet of four symbols (A, C, T, G) from a DNA fragment. \u2212 It recursively splits the two-dimensional space into pixels representing the number of occurrences of certain strings, adding a character on each level (e.g. level one) (e.g. level one, generating a 4-pixel image with the total number of A, C, G and T bases. On level 2, the image is 4-pixel wide, with pixels corresponding to the length 2 strings such as AA, AC, AT,..., TG, TT. The proposed algorithm uses the SAX representation of a time series with an alphabet size of 4 to create such bitmaps [22], and then compare the bitmap of a recognition window (after the currently analyzed point) and a delay window (before the point)."}, {"heading": "4 Motif discovery", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Minimal Description Length", "text": "Although not directly using the SAX representation, the Minimal Description Length algorithm [23] transforms the time series into a symbolic representation using a multi-scale Gaussian Blur called scale-space image; at each level, segments limited by the zero traverses of the first derivative of the smoothed time series are extracted. These segments are described compactly by two coordinates: their length and the difference between their first and last point. This description is then quantified by a k-mean clustering step. The time series on each scale can be edited as a string. The Minimal Description Length (MDL) framework is an approach in which a subject is selected when it increases the compression ratio of the time series, i.e. it allows a shorter description. Given the complexity of the exact compression it would require (which would require the extraction and evaluation of all possible subject groups)."}, {"heading": "4.2 Grammar inference", "text": "As shown in the previous section, the grammar inference [24] can be used not only to detect anomalies but, more naturally, to extract motifs. In this approach, the time series is SAX-coded with a sliding window and converted into its sequence representation, just as in the detection of anomalies. The rules created 19 CHAPTER 4. MOTIF-DISCOVER 20 during compression are then mapped back to time series segments. In contrast to detecting anomalies, the points with the highest rule density are selected, i.e. the longest branches of the Sequitur grammar tree. A post-processing step is required to refine the results by eliminating self-matches (motif that adjusts within a very small offset); selecting the longest patterns that are more likely to convey useful information; removing \"obvious\" patterns, e.g. monotonically increasing or merging, and if necessary; and overlapping."}, {"heading": "4.3 MK algorithm", "text": "The MK (Mueen-Keogh) [25] algorithm is closely related to HOTSAX because it uses heuristics to truncate an otherwise exact search for the most similar (and not the most different) sequence pair within a time series. However, this algorithm is the only one in our collection that relies on the raw time series data rather than a symbolic representation, and we chose to include it in this report because of the similarity it exhibits with HOTSAX. Instead of SAX pre-processing to reduce the number of distance evaluations, the algorithm uses reference points: sequences randomly selected in the time series, and orders the other subsequences by their distance to the reference. Heuristics are based on the assumption that sequences that are close to each other in this projection, as shown in Algorithm 10.Note, that the algorithm can use multiple reference points, using only one subsequence to calculate the actual sequence, and using only one of them to recalculate the actual sequence."}, {"heading": "4.4 Motif Tracking algorithm", "text": "The Motif Tracking Algorithm [26], shown in Algorithm 11, is an attempt to bring the class of Artificial Immune Systems (AIS), which work almost exclusively on strings, into the realm of time series processing. SAX representation is used to produce antibodies / antigens. It uses a subsequence length corresponding to the word size, so each subsequence is represented by a single symbol. Trackers are used as memory cells; they contain the string representation of a subsequence and a match number. Unlike the usual AIS algorithm, this is deterministic. Since this algorithm recognizes exact motives (without mutation between two arbitrary events) when they occur at least twice, it logically corresponds to the grammar induction algorithm described above."}, {"heading": "4.5 Mining approximate motifs", "text": "The \"Mining approximate motifs\" algorithm procedure MKOTOTIF = input:... \"in algorithm 12,\" in algorithm 12, \"in algorithm 12,\" in algorithm = 1, \"in algorithm 12,\" in algorithm = 1, \"in algorithm = 1,\" in algorithm = 1, \"in algorithm = 1,\" in algorithm = 1, \"in algorithm = 1,\" in algorithm = 1, \"in algorithm = 1,\" in algorithm = 1, \"in algorithm = 1,\" in algorithm = 1, \"in algorithm = 1,\" in algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"algorithm = 1,\" in algorithm = 1, \"algorithm = 1,\" algorithm = 1, \"algorithm = 1,\" in algorithm = 1, \"algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"algorithm = 1,\" in algorithm = 1, \"algorithm = 1,\" in algorithm = 1, \"algorithm = 1,\" algorithm = 1, \"algorithm = 1,\" algorithm = 1, \"in algorithm = 1,\" algorithm = 1, \"algorithm = 1,\" algorithm = 1, \"algorithm = 1,\" in algorithm = 1, \"algorithm = 1,\" algorithm = 1, \"algorithm = 1,\" algorithm = 1, \"algorithm = 1,"}, {"heading": "5 Experimental evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Input data", "text": "To compare these algorithms, we use a collection of 14 time series spanning two months, representing CPU usage, memory usage, number of processes, and active TCP sessions of 3 production servers and firewalls. Measurements are measured at a resolution of one point per minute. These series were selected because they contain both identifiable motives and anomalies, and in particular they feature various error modes, some of which override the basic assumptions of certain algorithms and are therefore not detected. Anomalies such as those in Figure 5.1 are easy to detect even for conventional commercial monitoring software. However, more subtle changes (Figure 5.1), which can be symptomatic of a critical component crash, are more difficult to detect and are usually overlooked; instead, the crash could be detected later if it causes other symptoms and has an impact on other parts of the system. Detection of such anomalies is the key to combine faster network correction, lower impact, and easier to predict by combining it with other components."}, {"heading": "5.2 Anomaly detection benchmark", "text": "Since the very concept of an anomaly is difficult to quantify (i.e. classifying data into \"normal\" or \"abnormal\" time series may be somewhat arbitrary outside of the obvious transitions), we will perform a qualitative evaluation of these algorithms, taking into account execution speed (i.e. CPU load), error modes (i.e. types of anomalies that cannot be detected), detection delay (i.e. the number of data points needed to detect an anomaly), and sensitivity to periodic inputs (i.e. precision losses due to the signal that is periodic)."}, {"heading": "5.2.1 Note on figure reading", "text": "The blue graph is the raw time series, the green is the anomaly value and the red vertical lines indicate the discovery of an anomaly (anomaly value exceeding a tunable threshold, here five standard deviations). The anomaly 25CHAPTER 5th EXPERIENCE 26CHAPTER 5th EXPERIENCE 27CHAPTER does not necessarily start at the same point as the series: it is delayed by buffering in the case of Chaos Game, which means that the alarm is triggered only after this buffering time."}, {"heading": "5.2.2 Performance figures", "text": "The results show that sequence and chaos game provide approximately the same running time, between 6 and 7 seconds per month of data. Running time, as predicted, is linear with the size of the series. Hot SAX, on the other hand, indicates a much larger and square running time; we have run it only once per month of data and therefore have no estimate of the variability of our result of almost 11 minutes. At a time series of two months, the algorithm ran over one hour without producing a result. As it definitively excludes it as a viable candidate for IT monitoring, we did not attempt to have it completed."}, {"heading": "5.2.3 On SAX parameters", "text": "As noted in almost all Keogh et.al. papers, the parameters used for SAX encoding (cardinality and word size) have little influence on the behavior of any algorithm. At most, they can change the sensitivity and runtime, such as weighing the accuracy for faster execution or moving along the precision callback curve. However, the window size generally needs to be adjusted to the scale of any feature in the series, i.e. the size of the anomaly we are trying to detect, or the time period of the signal. This is one of the biggest deficiencies we have observed in all SAX-based algorithms: for mostly weekly periodic signals, the best settings would delay any analysis until at least 7 days after detection of a data point, due to the discrepancy between the scale of observed phenomena and the detection time required. In most cases, SAX is used to examine phenomena on the scale of one second with most of these points per observing."}, {"heading": "5.2.4 Hot SAX", "text": "As described above, Hot SAX is the only algorithm that does not generate an \"anomaly score\" or any kind of distance measurement. It only returns the single anomaly point in a row. While this point always corresponded to the real anomaly (or one of the anomalies) in all of our tests, the algorithm remains extremely slow and its result is of little use in real-time settings. In fact, our tests even had difficulty completing it, taking over 10 minutes for a single month of data (while the other algorithms only took half a dozen seconds). Since Hot SAX always displays the mathematically defined worst anomaly in its design, it has no error mode by nature. In fact, it could be used in IT monitoring, with some limitations: the data must be subsampled. The search window must be limited.Hot SAX must only be applied to some critical metrics."}, {"heading": "5.2.5 Sequitur", "text": "Although Sequitur is fast and able to detect most anomalies and is immune to cyclic phenomena, it relies on the low compressibility of the anomalies to detect them. Therefore, it does not detect any anomaly that leads to a simplified pattern, as shown in Figure 5.1. An interesting feature is that the runtime of the algorithm depends more on complexity (i.e. the number of rules generated by Sequitur) than on the number of data points. We found that it is the best algorithm in terms of performance and detection speed, requiring little to no look ahead (unlike Chaos Game) and insensitive to cyclical variations (compare correct detection in Figure 5.2.5 with errors in Figure 5.2.6). However, the failure mode we observed is not compatible with a real production environment in which it can detect a serious failure or attack on a factory. If such a sequence is to be used with another algorithm, it must therefore be dangerous."}, {"heading": "5.2.6 Chaos Game", "text": "The Chaos Game algorithm is the most precise anomaly detector with acceptable runtime. We did not find any error mode in our data set. Runtime is strictly proportional to the length of the time series and therefore predictable. However, we found that the forward view required to correctly detect the anomaly (at least twice as long as the feature window) is unacceptably long, and a short feature window makes the algorithm very sensitive to cyclic patterns. See Figure 5.2.6 for an example of a false positive (correctly handled by Sequitur in Figure 5.2.5). We could not find a correct tuning that would be able to ignore these low frequency variations without extending the window (and thus the time between an occurring anomaly and its detection) to at least one day."}, {"heading": "6 Conclusion and perspectives", "text": "After implementing and comparing various anomaly detection algorithms based on SAX representation, we found that none fit perfectly with real-time detection, partly due to the representation itself, which takes time to consider a new point (due to strong downsampling and sliding window analysis), but fine-tuning the parameters for good detection led us to conclude that the feature window will inevitably be too large for practical analysis: for typical weekly patterns found in network monitoring, a good feature window takes about a day, which in turn results in huge SAX words, with a single symbol representing an hour of data. We conclude that algorithms based on SAX are largely unsuitable for the kind of real-time analysis we want to perform. Given these results, we have not performed a benchmark for pattern mining algorithms, as they are bound to be subject to limitations."}, {"heading": "7 Scientific validation", "text": "This paper has been reviewed by Ismaila Diouf, McF, Universit\u00e9 Cheikh Antar Diop de Dakar (S\u00e9n\u00e9gal) Dominique Pastor, Professor des Universit\u00e9s, T\u00e9l\u00e9com Bretagne (France) 33 and approved for publication as a paper of the Complex System-Digital Campus."}], "references": [{"title": "Hot sax: Efficiently finding the most unusual time series subsequence", "author": ["Eamonn Keogh", "Jessica Lin", "Ada Fu"], "venue": "Data mining, fifth IEEE international conference on", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Time series anomaly discovery with grammar-based compression.", "author": ["Pavel Senin"], "venue": "EDBT", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Identifying hierarchical strcture in sequences: A linear-time algorithm", "author": ["Craig G. Nevill-Manning", "Ian H. Witten"], "venue": "J. Artif. Intell. Res.(JAIR)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Using compression algorithms to support the comprehension of program traces", "author": ["Neil Walkinshaw", "Sheeva Afshan", "Phil McMinn"], "venue": "Proceedings of the Eighth International Workshop on Dynamic Analysis. ACM", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Supporting efficient query processing on compressed XML files", "author": ["Yongjing Lin"], "venue": "Proceedings of the 2005 ACM symposium on Applied computing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Enhanced sequitur for finding structure in data", "author": ["Erin Earl", "Richard E Ladner"], "venue": "Data Compression Conference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Assumption-Free Anomaly Detection in Time Series.", "author": ["Li Wei"], "venue": "In: SSDBM. Vol", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Time-series Bitmaps: a Practical Visualization Tool for Working with Large Time Series Databases.", "author": ["Nitin Kumar"], "venue": "In: SDM. SIAM", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Mining characteristic multi-scale motifs in sensor-based time series", "author": ["Ugo Vespier", "Siegfried Nijssen", "Arno Knobbe"], "venue": "Proceedings of the 22nd ACM international conference on Conference on information & knowledge management", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Approximate variable-length time series motif discovery using grammar inference", "author": ["Yuan Li", "Jessica Lin"], "venue": "Proceedings of the Tenth International Workshop on Multimedia Data Mining. ACM", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Exact Discovery of Time Series Motifs.", "author": ["Abdullah Mueen"], "venue": "In: SDM", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "The motif tracking algorithm", "author": ["William Wilson", "Phil Birkin", "Uwe Aickelin"], "venue": "In: International Journal of Automation and Computing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Mining approximate motifs in time series", "author": ["Pedro G Ferreira"], "venue": "Discovery Science. Springer", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}], "referenceMentions": [], "year": 2017, "abstractText": "The advent of the Big Data hype and the consistent recollection of event logs and real-time data from sensors, monitoring software and machine configuration has generated a huge amount of time-varying data in about every sector of the industry. Rule-based processing of such data has ceased to be relevant in many scenarios where anomaly detection and pattern mining have to be entirely accomplished by the machine. Since the early 2000s, the de-facto standard for representing time series has been the Symbolic Aggregate approXimation (SAX). In this document, we present a few algorithms using this representation for anomaly detection and motif discovery, also known as pattern mining, in such data. We propose a benchmark of anomaly detection algorithms using data from Cloud monitoring software.", "creator": "LaTeX with hyperref package"}}}