{"id": "1704.05396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "A Study of Deep Learning Robustness Against Computation Failures", "abstract": "For many types of integrated circuits, accepting larger failure rates in computations can be used to improve energy efficiency. We study the performance of faulty implementations of certain deep neural networks based on pessimistic and optimistic models of the effect of hardware faults. After identifying the impact of hyperparameters such as the number of layers on robustness, we study the ability of the network to compensate for computational failures through an increase of the network size. We show that some networks can achieve equivalent performance under faulty implementations, and quantify the required increase in computational complexity.", "histories": [["v1", "Tue, 18 Apr 2017 15:33:10 GMT  (439kb,D)", "http://arxiv.org/abs/1704.05396v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jean-charles vialatte", "fran\\c{c}ois leduc-primeau"], "accepted": false, "id": "1704.05396"}, "pdf": {"name": "1704.05396.pdf", "metadata": {"source": "CRF", "title": "A Study of Deep Learning Robustness Against Computation Failures", "authors": ["Jean-Charles Vialatte"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is in such a way that one must be able to go to another world, in which one can go to another world, in which one must go to another world, in which one must go to another world, in which one must go to another world, in which one must go to another world, in which one must feel oneself to another world, in which one must go to another world, in which one must go to another world, in which one must go to another world, in which one must go to another world, in which one must go back to another world, in which one feels oneself to be transported to another world, in which one lives a world, in which one lives, in which one lives, in which one lives in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives."}, {"heading": "II. BACKGROUND AND NAMING CONVENTIONS", "text": "A neural network is a neural representation of a function f, which is a composition of layer functions fi. Each layer function normally consists of a linear operation followed by a nonlinear operation. A dense layer is such that its inputs and outputs are vectors. Its linear part can be written as a matrix multiplication between an input x and a weight matrix W: x 7 \u2192 Wx. The number of neurons of a dense layer refers to the number of rows of W. An n-dimensional revolutionary layer is such that its inputs and outputs are tensors of rank n. Its linear part can be used as an n-dimensional link between an input x and a weight sensor W of rank n: x 7 \u2192 (\u2211 p Wpq; n xp)."}, {"heading": "III. NEURAL NETWORK MODELS", "text": "In order to simplify the selection of the model, we put some mild constraints on the hyperparameter space, since we are more interested in the general robustness of the models than in finding models with the best accuracy. As described below, we limit most layers to the same number of neurons and the same activation function. We also try to use only one optimization algorithm, and consider only one type of weight initialization. The first type of model that we consider an MLP network consists of layers each containing N neurons, which we call MLP-L-N."}, {"heading": "IV. DEVIATION MODELS", "text": "We are at a stage where nothing is at stake any more, where it is all about saving the world, \"he told the German Press Agency in an interview.\" We have not yet understood why it is like that, \"he said.\" But it is not there yet. \""}, {"heading": "V. RESULTS", "text": "The classification performance of the MLP and CNN models was evaluated using Monte Carlo simulations, which examine deviations according to the deviation models described in Section IV. Since we want to evaluate a large number of neural network models, each MC simulation is performed by evaluating only 10 deviation realizations."}, {"heading": "A. Effect of some hyperparameters on robustness", "text": "Robustness refers to the ability to maintain good classification accuracy in the presence of deviations. A model has better robustness if it achieves a lower classification error for a given p. We investigated the impact of several hyperparameters on the robustness of the conclusion. We evaluated performance using the Clipped-reLU and Tanh activation functions and found that the robustness is similar in both cases. In CNN models, we also evaluated the impact of the choice of the pooling function. In this case, we found that the use of pooling instead of max pooling resulted in a slight improvement in robustness. Finally, we looked at the impact of the number of layers L. The effect of L on the classification error is shown in Figure 1 for an MLP-L-N network and in Figure 2 for a CNN-L-CP-F pool network."}, {"heading": "B. Fault tolerance", "text": "In fact, most people who are able to survive themselves have to survive themselves, \"he told the German Press Agency.\" I don't think they're able to survive me, \"he said.\" I don't think they're going to survive me. \"He added,\" I don't think they're going to be able to survive me. \"He added,\" I don't think they're going to be able to survive me. \"He added,\" I don't think they're going to be able to survive me. \"He added,\" I don't think I'm going to be able to survive me. \""}, {"heading": "VI. CONCLUSION", "text": "In this paper, we examined the impact of faulty calculations on the performance of MLP and CNN using a pessimistic and an optimistic deviation model. We have shown that, using standard training methods, it is possible in many cases to find models that compensate for computational errors when the probability of deviation is in the order of 10 \u2212 3 and at the expense of increasing the number of parameters. We have also examined the efficiency of the error tolerance achieved with standard model training. Our results show that the error tolerance efficiency decreases when the performance target is increased. It seems reasonable to expect that networks designed for fault tolerance could achieve an efficiency that depends only on the amount of deviations that occur during the inference process, and not on the performance target. Therefore, these results provide a basis for future work that seeks to identify systematic methods for developing robust deeper neural networks, and emphasize that one purpose of the network design should be to decouple the failure target."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei"], "venue": "CoRR, vol. abs/1512.02595, 2015. [Online]. Available: http://arxiv.org/abs/1512.02595", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Near-threshold computing: Reclaiming Moore\u2019s law through energy efficient integrated circuits", "author": ["R. Dreslinski", "M. Wieckowski", "D. Blaauw", "D. Sylvester", "T. Mudge"], "venue": "Proc. of the IEEE, vol. 98, no. 2, pp. 253\u2013266, Feb. 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Ultra-low power VLSI circuit design demystified and explained: A tutorial", "author": ["M. Alioto"], "venue": "Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 59, no. 1, pp. 3\u201329, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Modeling and energy optimization of LDPC decoder circuits with timing violations", "author": ["F. Leduc-Primeau", "F.R. Kschischang", "W.J. Gross"], "venue": "CoRR, vol. abs/1503.03880, 2015. [Online]. Available: http://arxiv.org/abs/1503.03880", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1mb model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": "CoRR, vol. abs/1602.07360, 2016. [Online]. Available: http://arxiv.org/abs/1602.07360", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "CoRR, vol. abs/1602.02830, 2016. [Online]. Available: http://arxiv.org/abs/1602.02830", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "XNOR-Net: ImageNet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "CoRR, vol. abs/1603.05279, 2016. [Online]. Available: http://arxiv.org/abs/1603.05279", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "VLSI implementation of deep neural network using integral stochastic computing", "author": ["A. Ardakani", "F. Leduc-Primeau", "N. Onizawa", "T. Hanyu", "W.J. Gross"], "venue": "IEEE Trans. on VLSI Systems, 2017, to appear.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Variation-tolerant architectures for convolutional neural networks in the near threshold voltage regime", "author": ["Y. Lin", "S. Zhang", "N.R. Shanbhag"], "venue": "2016 IEEE International Workshop on Signal Processing Systems (SiPS), Oct 2016, pp. 17\u201322.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 315\u2013323.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A.Y. Hannun"], "venue": "CoRR, vol. abs/1412.5567, 2014. [Online]. Available: http://arxiv.org/abs/1412.5567", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": "1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1929}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "CoRR, vol. abs/1212.5701, 2012. [Online]. Available: http://arxiv.org/abs/1212.5701", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks achieve excellent performance at various artificial intelligence tasks, such as speech recognition [1] and computer vision [2].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Deep neural networks achieve excellent performance at various artificial intelligence tasks, such as speech recognition [1] and computer vision [2].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "However, whereas in the past the energy consumption of integrated circuits was decreasing steadily with each new integrated circuit technology, the energy improvements that can be expected from further shrinking of CMOS circuits is small [3].", "startOffset": 238, "endOffset": 241}, {"referenceID": 3, "context": "A possible approach to continue improving the energy efficiency of CMOS circuits is to operate them in the near-threshold regime, which unfortunately drastically increases the amount of delay variations in the circuit, which can lead to functional failures [4].", "startOffset": 257, "endOffset": 260}, {"referenceID": 4, "context": "For the case of low-density parity-check codes, it was indeed shown that a decoder can save energy by operating unreliably while preserving equivalent performance [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "For example, [6] proposes an approach to decrease the number of parameters in a deep convolutional neural network (CNN) while preserving prediction accuracy, and [7, 8] propose approaches to replace floating-point operations with much simpler binary operations.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "For example, [6] proposes an approach to decrease the number of parameters in a deep convolutional neural network (CNN) while preserving prediction accuracy, and [7, 8] propose approaches to replace floating-point operations with much simpler binary operations.", "startOffset": 162, "endOffset": 168}, {"referenceID": 7, "context": "For example, [6] proposes an approach to decrease the number of parameters in a deep convolutional neural network (CNN) while preserving prediction accuracy, and [7, 8] propose approaches to replace floating-point operations with much simpler binary operations.", "startOffset": 162, "endOffset": 168}, {"referenceID": 8, "context": "It was shown previously for an implementation of a multilayer perceptron (MLP) based on stochastic computing that it is possible to compensate for a reduced precision by increasing the size of the network [9].", "startOffset": 205, "endOffset": 208}, {"referenceID": 9, "context": "[10] using a different approach that consists in adding compensation mechanisms in hardware while keeping the same network size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "An MLP [11] is composed only of dense layers.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "A CNN [12] is mainly composed of convolutional layers.", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "The activation function used in all the layers is chosen as the rectified linear unit (reLU) [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "We note that such an activation function has been used before in a different context, in order to avoid the problem of diverging gradients in the training of recurrent networks [14].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "The second model type is a CNN network composed of L convolutional C \u00d7 C layers with P \u00d7 P pooling of type pool, ultimately followed by a dense layer of 200 neurons [12].", "startOffset": 165, "endOffset": 169}, {"referenceID": 14, "context": "For simplicity, we opted to train the networks on the task of digit classification using the MNIST dataset [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "Initialization was done as suggested in [16].", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "To prevent overfitting during training, a dropout [17] of 25% and 50% of the neurons have been applied on convolutional and dense layers, respectively, except on the first layer.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "We used categorical crossentropy as the loss function and the \u201cadadelta\u201d optimizer [18].", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "For many types of integrated circuits, accepting larger failure rates in computations can be used to improve energy efficiency. We study the performance of faulty implementations of certain deep neural networks based on pessimistic and optimistic models of the effect of hardware faults. After identifying the impact of hyperparameters such as the number of layers on robustness, we study the ability of the network to compensate for computational failures through an increase of the network size. We show that some networks can achieve equivalent performance under faulty implementations, and quantify the required increase in computational complexity.", "creator": "LaTeX with hyperref package"}}}