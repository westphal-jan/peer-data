{"id": "1704.05495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "histories": [["v1", "Tue, 18 Apr 2017 18:46:12 GMT  (400kb,D)", "http://arxiv.org/abs/1704.05495v1", "8 pages, 3 figures, NIPS 2016 Deep Reinforcement Learning Workshop"]], "COMMENTS": "8 pages, 3 figures, NIPS 2016 Deep Reinforcement Learning Workshop", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["jean harb", "doina precup"], "accepted": false, "id": "1704.05495"}, "pdf": {"name": "1704.05495.pdf", "metadata": {"source": "CRF", "title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "authors": ["Jean Harb"], "emails": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "We are interested in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we are investigating the use of two methods that are known to mitigate these problems: recurring networks that provide a form of memory that summarizes past experiences, and permission histories that allow information to be disseminated over multiple time steps. Authorization histories have been empirically proven to facilitate faster learning (Sutton & Barto (2017), in preparation), but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015))."}, {"heading": "2 Background", "text": "A Markov Decision Process (MDP) consists of a tuple < S, A, r, P, \u03b3 >, where S is the entirety of the states, A is the entirety of the measures, R: S \u00b7 A 7 \u2192 R is the reward function, P (s) | s, a) is the transition function (where the next state distribution is given due to the current state and the current measures), and A is the entirety of the measures, r: S \u00b7 A 7 \u2192 R is the reward function, P (s) is a framework for solving unknown MDPs, which means finding a good (or optimal) course of action, also referred to as politics. RL works by obtaining transitions from the environment and using them to calculate a policy that maximizes the expected return given by E."}, {"heading": "2.1 Eligibility Traces", "text": "Credentials are a basic reinforcement learning mechanism that enables a trade-off between TD and MC. MC methods suffer from high variance, as many trajectories can be taken from any given state and stochasticity is often present in the MDP. TD suffers from high bias, because it updates values based on its own estimates. The use of credentials allows to design algorithms that cover the middle ground between MC and TD. The central term for this is n-step returns, which provide a way to calculate the target by estimating the value for the state that n steps will occur in the future (compared to the current state): R (n) t = n \u2212 1 Credentials of experience i + i + \u03b3 nV (st + n). If n is 1, the results are the TD target, and taking n-steps result in the MC target."}, {"heading": "2.1.1 Q(\u03bb)", "text": "Q (\u03bb) is a variant of Q-Learning that uses permission tracking to calculate the TD error. As already mentioned, traditionally the reverse view of tracks is used. Some versions of Q (\u03bb) exist, but the most commonly used is Watkins \"Q (\u03bb). Since Q-Learning takes place outside of politics, the sequence of actions used in the past to calculate the track may differ from the actions that the current policy could take. In this case, one should not take the path past the point at which the actions differ. To handle such a case, Watkins\" Q (\u03bb) sets the track to 0 if the action that the current policy would choose differs from the one used in the past."}, {"heading": "2.2 Deep Q-Networks", "text": "Mnih et al. (2015) introduced Deep Q-Networks (DQN), one of the first successful reinforcement learning algorithms to use deep learning for functional approximation in a way that is generic enough and applicable to a variety of environments. Applied to a number of Atari games, they used a Convolutionary Neural Network (CNN) that uses the last four frames of the game as input and outputs Q values for any action. Equation 6 shows the DQN cost function, where we optimize \u03b8 parameters."}, {"heading": "2.2.1 Deep Recurrent Q-Networks", "text": "As introduced in Hausknecht & Stone (2015), Deep Recurrent Q-Networks (DRQN) are a modification of the DQN, in which individual frames are routed through a CNN that generates a feature vector, which is then fed to an RNN that eventually issues Q values. This architecture gives the agent a memory that allows him to learn long-term time effects and deal with partial observation, which is the case in many environments.The authors showed that arbitrarily blanking out frames was difficult for DQN, but DRQN learned to deal with them.To train DRQN, they proposed two variants of experience playback: The first was to test entire trajectories and run the RNN from end to end. However, this is very computationally intensive, as some trajectories can be over 10,000 steps long."}, {"heading": "2.3 Optimizers", "text": "Stochastic Gradient Drop (SGD) is generally the algorithm used to optimize neural networks, but some information is lost in the process as past gradients may signal that a weight needs to change drastically or that it oscillates, requiring a reduction in the learning rate. Adaptive SGD algorithms are designed to use this information.RMSprop (Tieleman & Hinton (2012)) uses a geometric mean of gradients squared and divides the current gradient by its square root. To perform RMSprop, we first calculate the mean as g = \u03b2g + (1 \u2212 \u03b2), and then update the parameters."}, {"heading": "3 Experimental Setup", "text": "As explained, anticipating permission histories can be useful, but is mathematically demanding in terms of memory and time. It is necessary to store all transitions and apply the neural network to any state in the trajectory. By using DRQN, experience reproduction is already part of the algorithm, eliminating the memory requirements of the tracks. Afterwards, the states must be traversed by training on partial stretches of data with all state values as output, eliminating computing costs. Finally, the only thing left is to calculate the weighted sum of targets, which is very cheap. In this section, we will analyze the use of permission histories in DRQN training and try both RMSprop and Adam as optimizers. We tested the algorithms only on fully observable games to compare learning capacities without the unfair advantage of memory, as would be the case in partially observable environments."}, {"heading": "3.1 Architecture", "text": "We tested the algorithms on two rectified linear units (LSTM)."}, {"heading": "4 Experimental Results", "text": "We describe experiments in two Atari games: Pong and Tennis. We chose Pong because it enables fast experimentation, and Tennis because it is one of the games that has proven to be difficult in all published results on Atari."}, {"heading": "4.1 Pong", "text": "First, we tested an RNN model with both \u03bb = 0 and \u03bb = 0.8, trained with RMSprop. Figure 2 shows that the model without a track (\u03bb = 0) learns at the same speed as DQN, while the model with tracks (\u03bb = 0.8) learns much faster and more stable without showing any periods of depressed performance. This is probably due to the fact that in a single update, the permission histories regress by many steps, rather than the actions that are performed after the ball has already been left behind. In Pong, the tracks clearly help to send this signal back faster before the ball either passes or passes the opponent. Afterwards, we tested the same models, but with Adam as optimizer instead of RMSprop. All models learn much faster with this setting."}, {"heading": "4.2 Tennis", "text": "The second Atari 2600 game we tested was Tennis. A match consists of only one set won by the player who first won 6 \"games\" (as in regular tennis), and the score ranges from 24 to -24, as the difference between the number of balls won by the two players is given. As in Pong, we first tried to achieve an RMSprop and the standard learning rate of 0.00025, both with and without credentials. Figure 3 shows that both RNN models have learned to achieve optimal results after 50 epochs, unlike DQN, which is never able to exceed the threshold of 0, with large variations ranging from -24 to 0."}, {"heading": "5 Discussion and Conclusion", "text": "In this article, we analyzed the effects of using permission tracking and various optimization features. We showed that permission tracking can improve and stabilize learning, and Adam can greatly accelerate learning. As can be seen from the Pong results, the model that uses permission tracking has not gained much power from using Adam. One possible cause is the frozen network. While it has a stabilizing effect in the DQN by preventing policies from drastically changing from a single update, it also prevents newly learned values from being redistributed. Double DQN seems to partially circumvent this problem, allowing the politics of the next state to change while the values remain frozen. In future experiments, we must consider eliminating or increasing the update frequency of the frozen network."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht and Stone.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Playing fps games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot"], "venue": "arXiv preprint arXiv:1609.05521,", "citeRegEx": "Lample and Chaplot.,? \\Q2016\\E", "shortCiteRegEx": "Lample and Chaplot.", "year": 2016}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["Sergey Levine", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine and Abbeel.,? \\Q2014\\E", "shortCiteRegEx": "Levine and Abbeel.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["R\u00e9mi Munos", "Tom Stepleton", "Anna Harutyunyan", "Marc G Bellemare"], "venue": "arXiv preprint arXiv:1606.02647,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Reinforcement learning: An introduction, In Preparation", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q2017\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 2017}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "True online td (lambda)", "author": ["Harm van Seijen", "Rich Sutton"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Seijen and Sutton.,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton.", "year": 2014}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)).", "startOffset": 78, "endOffset": 118}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)).", "startOffset": 78, "endOffset": 156}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)).", "startOffset": 78, "endOffset": 649}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)).", "startOffset": 78, "endOffset": 745}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)).", "startOffset": 78, "endOffset": 772}, {"referenceID": 1, "context": "DQN (Graves (2013)) introduced a variant of RMSprop where the gradient is instead divided by the standard deviation of the running average.", "startOffset": 5, "endOffset": 19}, {"referenceID": 1, "context": "DQN (Graves (2013)) introduced a variant of RMSprop where the gradient is instead divided by the standard deviation of the running average. First we calculate the running averages m = \u03b2m+(1\u2212\u03b2)\u2207\u03b8 and g = \u03b2g+(1\u2212\u03b2)\u2207\u03b8, and then update the parameters using \u03b8 \u2190 \u03b8+\u03b1 \u2207\u03b8 \u221a g\u2212m2+ . In the rest of the paper, when mentioning RMSprop, we\u2019ll be referring to this version. Finally, Kingma & Ba (2014) introduced Adam, which is essentially RMSprop coupled with Nesterov momentum, along with the running averages being corrected for bias.", "startOffset": 5, "endOffset": 388}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis.", "startOffset": 91, "endOffset": 115}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84.", "startOffset": 91, "endOffset": 212}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84. These are then fed to a CNN with the first layer being 32 8x8 filters and a stride of 4, followed by 64 4x4 filters with a stride of 2, then by 64 3x3 filters with a stride of 1. The output of the CNN is then flattened before being fed to a single dense layer of 512 output neurons, which is finally fed to an LSTM (Hochreiter & Schmidhuber (1997)) with 512 cells.", "startOffset": 91, "endOffset": 623}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84. These are then fed to a CNN with the first layer being 32 8x8 filters and a stride of 4, followed by 64 4x4 filters with a stride of 2, then by 64 3x3 filters with a stride of 1. The output of the CNN is then flattened before being fed to a single dense layer of 512 output neurons, which is finally fed to an LSTM (Hochreiter & Schmidhuber (1997)) with 512 cells. We then have a last linear layer that takes the output of the recurrent layer to output the Q-values. All layers before the LSTM are activated using rectified linear units (ReLU). As mentioned in subsection 2.2.1, we also altered experience replay to sample sub-trajectories. We use backprop through time (BPTT) to train the RNN, but only train on a sub-trajectory of experience. In runtime, the RNN will have had a large sequence of inputs in its hidden state, which can be problematic if always trained with an empty hidden state. Like in Lample & Singh Chaplot (2016), we therefore sample a slightly longer length of trajectory and use the first m states to fill the hidden state.", "startOffset": 91, "endOffset": 1211}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al.", "startOffset": 123, "endOffset": 142}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al.", "startOffset": 123, "endOffset": 163}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al.", "startOffset": 123, "endOffset": 183}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al. (2016), Schaul et al.", "startOffset": 123, "endOffset": 203}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al. (2016), Schaul et al. (2015)), which tend to get stuck in the policy of delaying.", "startOffset": 123, "endOffset": 225}], "year": 2017, "abstractText": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "creator": "LaTeX with hyperref package"}}}