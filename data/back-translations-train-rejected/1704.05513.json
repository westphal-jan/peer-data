{"id": "1704.05513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "25 Tweets to Know You: A New Model to Predict Personality with Social Media", "abstract": "Predicting personality is essential for social applications supporting human-centered activities, yet prior modeling methods with users written text require too much input data to be realistically used in the context of social media. In this work, we aim to drastically reduce the data requirement for personality modeling and develop a model that is applicable to most users on Twitter. Our model integrates Word Embedding features with Gaussian Processes regression. Based on the evaluation of over 1.3K users on Twitter, we find that our model achieves comparable or better accuracy than state of the art techniques with 8 times fewer data.", "histories": [["v1", "Tue, 18 Apr 2017 20:16:31 GMT  (151kb)", "http://arxiv.org/abs/1704.05513v1", "Accepted as a short paper at ICWSM 2017. Please cite the ICWSM version and not the ArXiv version"]], "COMMENTS": "Accepted as a short paper at ICWSM 2017. Please cite the ICWSM version and not the ArXiv version", "reviews": [], "SUBJECTS": "cs.SI cs.AI cs.CL cs.HC", "authors": ["pierre-hadrien arnoux", "anbang xu", "neil boyette", "jalal mahmud", "rama akkiraju", "vibha sinha"], "accepted": false, "id": "1704.05513"}, "pdf": {"name": "1704.05513.pdf", "metadata": {"source": "CRF", "title": "25 Tweets to Know You: A New Model to Predict Personality with Social Media", "authors": ["Pierre-Hadrien Arnoux", "Anbang Xu", "Neil Boyette", "Jalal Mahmud", "Rama Akkiraju", "Vibha Sinha"], "emails": ["vibha.sinha}@us.ibm.com"], "sections": [{"heading": "Introduction", "text": "There is a growing trend in social applications to consider users \"personalities in order to provide a more adaptable and personalized user experience (Hu, et al. 2016; Liu, et al. 2016). The user's self-written text is often used to predict the personality measured by the Big 5 personality dimensions, openness, conscientiousness, extraversion, agreement, neuroticism (OCEAN) (McCrae and John 1992; Schwartz, et al. 2013; Yarkoni 2010). Given that hundreds of millions of users participate in social media and share self-written content, social media offers an enormous opportunity for personality modeling. However, previous modeling methods require too much input data to be realistically used in the context of social media. In fact, previous work modeling accuracy based on an average of 200 Facebook posts (Schwartz, et al. 2013) or even 100,000 words (Yarkoni 2010) report that users have 22 posts on Twitter (Burger et al)."}, {"heading": "Word Embedding with Gaussian Processes", "text": "Our method combines Word Embedding with Gaussian processes. We extract the words from users \"tweets and calculate the average representation of their Word Embedding in a single vector. The Gaussian process model then takes these vectors as input for training and testing."}, {"heading": "The Word Embedding features", "text": "In this study, we introduce the functionality of Word Embedding to the field of personality modeling. Word Embedding is a technique that presents words as a dense, low-dimensional and real vector, based on syntactic and semantic relationships between words. Commonly learned from large amounts of unstructured text data, this representation helps learning algorithms achieve better results in processing natural language by bringing similar words closer together (Mikolov and Dean 2013). It was also shown that learning methods in dealing with short text are improved (Kenter and de Rijke 2015). Although there are many available word embedding models, we chose the Twitter 200-dimensional GloVe model (Pennington, et al. 2014) because it was trained on 2B tweets."}, {"heading": "The Gaussian Processes model", "text": "This work also introduces a new non-linear model: Gaussian Processes (GP) (Rasmussen 2006). Used in natural language processing and data mining, GP is very well suited for regression, as it allows explicit quantification of noise and modulation of the usefulness of features by adapting a core function to the data. Recent work has shown that GP, in combination with Word Embedding, can be very efficient in classifying short texts (Ma, et al. 2015) or in non-linear modeling of text properties (Yoshikawa, et al. 2015)."}, {"heading": "Experiment design", "text": "We obtain the basic truth data by interviewing over 1.3 K participants, and we compare the performance of our method with previous methods under different conditions."}, {"heading": "Ground Truth Collection", "text": "After prior work (Schwartz, et al. 2013), a survey is conducted to collect the participants \"self-reported personality ratings as well as their tweets. To reach a participant, we create a web application that we promote via Twitter. Through this app, participants voluntarily agree to share their tweets and respond to a personality survey. The survey uses the 50-point\" International Personality Item Pool \"form (Goldberg, et al. 2006) to evaluate the participants\" Big 5 personality traits and takes about 15 minutes to complete. Once the survey is complete, participants are presented with their personality ratings. In this setting, participants are motivated to complete the survey to display their personality ratings. We recruit 1323 participants with at least 200 non-re-tweets (\u00b5 = 1020, d = 541). This allows us to design a sample experiment with tweets from 10 to 200 and set the same user for all experiments."}, {"heading": "Methods for Comparison", "text": "We compare our method with two state-of-the-art methods: Linguistic Inquiry and Word Count (LIWC) with Ridge Regression (RR). The original method proposed by (Yarkoni 2010) uses LIWC (Pennebaker, et al. 2015) to extract features and RR as learning algorithm. 3-gram with Ridge Regression. We implement the previous state-of-the-art method (Schwartz, et al. 2013) by using 3-gram and RR. Word Embedding with Gaussian Process (GP). Our method integrates GloVe features with Gaussian Process Regression as learning algorithm. To adjust our parameters and evaluate our performance, we use a 3-sentence split: Training, Validation and Testing. The data is split between testing and training using a 10-way cross-validation and training."}, {"heading": "Comparison Settings", "text": "The performance of these three methods is compared in three different settings: Full Setting. In this setting, the methods are trained and tested over the entire corpus of texts. Sampling Setting. To simulate users with different numbers of tweets, we perform a downsampling on the tweets of the test users and vary the number of tweets used. Real-life Setting. The last setting aims to further investigate the performance of the methods in real-world applications by training the models on a large number of users with a large number of tweets and testing them on a small group of real users with a small number of tweets. To perform this analysis, we also collect a small group of 55 users without restricting the number of tweets after the previous procedure. Users had an average of 28 tweets without re-tweeting (\u00b5 = 28, d = 11), which is in line with the previous number of 22 publicly available tweets. Results"}, {"heading": "Full Setting", "text": "In addition, we are testing 3 other combinations of features and models: GloVe with RR, LIWC with GP and 3-gram with GP. Table 1 shows that our method has a new state-of-the-art performance with an average correlation of 0.33 over the Big 5 characteristics, 33% better than the previous best method. In agreement with (Schwartz, et al. 2013), we find that 3-gram characteristics with RR achieve better results than LIWC characteristics. The additional combination tests show that GloVe characteristics and GP contribute equally to the performance of the method. In fact, the combination of LIWC GP as well as the combination of GloVe RR leads to an average correlation of 0.26. We also find that GP does not perform well in combination with Bag-Of-Words characteristics, such as features such as 3-gram suggested by (Yoshikawa et al. 2015)."}, {"heading": "Sampling Setting", "text": "For this analysis, we train our models with all available tweets, and then select 20 random subsets of tweets for each user in the test set and test the model on them. We vary the number of tweets in these subsets, and report on the correlation of the models averaged across the Big 5 characteristics, depending on the number of tweets. Figure 1. shows the predictive accuracy in terms of the number of test tweets. First, we confirm that for a large number of tweets, the performance of the 3 methods converges with the results of the previous correlation analysis. We also see that the 3 gram method exceeds the LIWC method for a large number of tweets, but the opposite for fewer than 75 tweets. This highlights the need to use not sparse features for small texts. Most importantly, Figure 1., our method outperforms the other methods in all tweets."}, {"heading": "Real-life Setting", "text": "For this real-world application example, we report the results of an ANOVA test on the absolute error averaged over the Big 5 characteristics to determine the significance of the cross-method difference. Figure 2. shows the absolute error averaged over the Big 5 characteristics for the set of 55 users. The average absolute error of our method is 25% smaller than the state of the art and 11% smaller than the original method. These results are significant (F (2.51) = 6.82, p < 0.01). Match. Match. Match. Match. Extra new red. Openn.00.050.10.15LIWC RR 3-Grm RR GloVe with GPA bsol ute Erro rFigure 2. The mean absolute error is on average above the Big 5 characteristics. We also report a pair-wise t-test on the same data between our method and the next-best method. The two sets correlate at 0.01 and &p."}, {"heading": "Discussion", "text": "Our first finding is that our proposed method outperforms the most advanced methods of predicting personality in all three experiments. Both Word embedding features and Gaussian processes contribute to these performance improvements. As GloVe features are learned through a large corpus of external tweets, its vector representation brings external knowledge to our problem. Also, because of its dense representation of words, our method processes better short texts and invisible data. In addition, the Word embedding features fit well with GP. In fact, GP relies on the covariance of features due to its internal kernel representation. When applied to features such as N-Grams, the kernel calculation cannot detect similarities between documents if similar words do not appear in the same documents. However, when used in combination with GloVe features, and to a lesser extent on LIWC features, the computing power of the kernel degrees is able to rely on embedded co-occurrences of words on document level."}], "references": [{"title": "Discriminating gender on Twitter", "author": ["Burger", "J. D"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Burger and D,? \\Q2011\\E", "shortCiteRegEx": "Burger and D", "year": 2011}, {"title": "Predicting personality from twitter", "author": ["J Golbeck"], "venue": "PASSAT and 2011 SocialCom,", "citeRegEx": "Golbeck,? \\Q2011\\E", "shortCiteRegEx": "Golbeck", "year": 2011}, {"title": "The international personality item pool and the future of public-domain personality measures", "author": ["Goldberg", "L. R"], "venue": "Journal of Research in personality 40(1):84-96", "citeRegEx": "Goldberg and R,? \\Q2006\\E", "shortCiteRegEx": "Goldberg and R", "year": 2006}, {"title": "What the Language You Tweet Says About Your Occupation", "author": ["T Hu"], "venue": "Tenth International AAAI Conference on Web and Social Media", "citeRegEx": "Hu,? \\Q2016\\E", "shortCiteRegEx": "Hu", "year": 2016}, {"title": "Short text similarity with word embeddings", "author": ["T. Kenter", "M. de Rijke"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "Kenter and Rijke,? \\Q2015\\E", "shortCiteRegEx": "Kenter and Rijke", "year": 2015}, {"title": "Analyzing Personality through Social Media Profile Picture Choice", "author": ["L Liu"], "venue": "Tenth International AAAI Conference on Web and Social Media", "citeRegEx": "Liu,? \\Q2016\\E", "shortCiteRegEx": "Liu", "year": 2016}, {"title": "Distributional Representations of Words for Short Text Classification", "author": ["C Ma"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Ma,? \\Q2015\\E", "shortCiteRegEx": "Ma", "year": 2015}, {"title": "An introduction to the fivefactor model and its applications", "author": ["R.R. McCrae", "O.P. John"], "venue": "Journal of personality 60(2):175-215.", "citeRegEx": "McCrae and John,? 1992", "shortCiteRegEx": "McCrae and John", "year": 1992}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "Advances in neural information processing systems.", "citeRegEx": "Mikolov and Dean,? 2013", "shortCiteRegEx": "Mikolov and Dean", "year": 2013}, {"title": "The development and psychometric properties of LIWC2015", "author": ["Pennebaker", "J. W"], "venue": "UT Faculty/Researcher Works", "citeRegEx": "Pennebaker and W,? \\Q2015\\E", "shortCiteRegEx": "Pennebaker and W", "year": 2015}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP. 1532-4.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "Rasmussen,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen", "year": 2006}, {"title": "Personality, gender, and age in the language of social media: The open-vocabulary approach", "author": ["Schwartz", "H. A"], "venue": "PloS one 8(9):e73791", "citeRegEx": "Schwartz and A,? \\Q2013\\E", "shortCiteRegEx": "Schwartz and A", "year": 2013}, {"title": "Personality in 100,000 words: A large-scale analysis of personality and word use among bloggers", "author": ["T. Yarkoni"], "venue": "Journal of research in personality 44(3):363-373.", "citeRegEx": "Yarkoni,? 2010", "shortCiteRegEx": "Yarkoni", "year": 2010}, {"title": "Non-Linear Regression for Bag-of-Words Data via Gaussian Process Latent Variable Set Model", "author": ["Y. Yoshikawa", "T. Iwata", "H. Sawada"], "venue": "AAAI. 3129-3135.", "citeRegEx": "Yoshikawa et al\\.,? 2015", "shortCiteRegEx": "Yoshikawa et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Users\u2019 self-authored text is often used to predict personality measured by the Big-5 personality dimensions, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism (OCEAN) (McCrae and John 1992; Schwartz, et al. 2013; Yarkoni 2010).", "startOffset": 187, "endOffset": 246}, {"referenceID": 13, "context": "Users\u2019 self-authored text is often used to predict personality measured by the Big-5 personality dimensions, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism (OCEAN) (McCrae and John 1992; Schwartz, et al. 2013; Yarkoni 2010).", "startOffset": 187, "endOffset": 246}, {"referenceID": 13, "context": "2013) or even 100 000 words (Yarkoni 2010).", "startOffset": 28, "endOffset": 42}, {"referenceID": 8, "context": "Usually learned from large amounts of unstructured text data, this representation helps learning algorithms achieve better results on natural language processing tasks by bringing similar words closer together (Mikolov and Dean 2013).", "startOffset": 210, "endOffset": 233}, {"referenceID": 11, "context": "This work also introduces a new non-linear model: Gaussian Processes (GP) (Rasmussen 2006).", "startOffset": 74, "endOffset": 90}, {"referenceID": 13, "context": "The original method proposed by (Yarkoni 2010) uses LIWC (Pennebaker, et al.", "startOffset": 32, "endOffset": 46}], "year": 2017, "abstractText": "Predicting personality is essential for social applications supporting human-centered activities, yet prior modeling methods with users\u2019 written text require too much input data to be realistically used in the context of social media. In this work, we aim to drastically reduce the data requirement for personality modeling and develop a model that is applicable to most users on Twitter. Our model integrates Word Embedding features with Gaussian Processes regression. Based on the evaluation of over 1.3K users on Twitter, we find that our model achieves comparable or better accuracy than state-of-the-art techniques with 8 times fewer data.", "creator": "Word"}}}