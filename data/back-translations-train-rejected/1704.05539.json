{"id": "1704.05539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Beating Atari with Natural Language Guided Reinforcement Learning", "abstract": "We introduce the first deep reinforcement learning agent that learns to beat Atari games with the aid of natural language instructions. The agent uses a multimodal embedding between environment observations and natural language to self-monitor progress through a list of English instructions, granting itself reward for completing instructions in addition to increasing the game score. Our agent significantly outperforms Deep Q-Networks (DQNs), Asynchronous Advantage Actor-Critic (A3C) agents, and the best agents posted to OpenAI Gym on what is often considered the hardest Atari 2600 environment: Montezuma's Revenge.", "histories": [["v1", "Tue, 18 Apr 2017 21:31:29 GMT  (603kb,D)", "http://arxiv.org/abs/1704.05539v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["russell kaplan", "christopher sauer", "alexander sosa"], "accepted": false, "id": "1704.05539"}, "pdf": {"name": "1704.05539.pdf", "metadata": {"source": "CRF", "title": "Beating Atari with Natural Language Guided Reinforcement Learning", "authors": ["Russell Kaplan", "Christopher Sauer", "Alexander Sosa"], "emails": ["aasosa}@cs.stanford.edu"], "sections": [{"heading": null, "text": "Videos of trained MONTEZUMA'S REVENGE Agents: Our Best Current Model. Score 3500. Best Model Current on OpenAI Gym. Score 2500. Standard A3C Agent Fails to Learn. Score 0.Figure 1: Left: An agent exploring the first room of MONTEZUMA's REVENGE. Right: An example from the list of natural voice instructions that could be given to the agent. The agent treats himself to an additional reward after completing the current instruction. \"Completion\" is learned by training a generalized multimodal embedding between game images and text.ar Xiv: 170 4.05 539v 1 [cs.A I] 1 8A pr"}, {"heading": "1 Introduction", "text": "Humans do not usually learn to interact with the world in a vacuum, without interacting with others, nor do we live in the stateless, exemplary world of supervised learning. Instead, we live in a wonderfully complex and state-supporting world, where past actions influence current outcomes. In our learning, we benefit from the guidance of others by arbitrarily receiving high-level instruction in natural language - and learning to fill the gaps between those instructions - while navigating through a world of different sources of reward, both intrinsic and extrinsic. Building truly intelligent artificial agents requires that they be able to act in state-bearing worlds like ours. They must also be able to learn from and follow human instructions. Moreover, these instructions must be specified at each high level, which is convenient - not just the low level, but fully specified instructions given in current programs. Inspired by the dream, we must instruct the highest level of natural agents to teach them."}, {"heading": "2 Background", "text": "DeepMind shocked the deep and reinforced learning communities in 2013 with the introduction of deep Q-learning. For the first time, reinforcement learners learned from high-dimensional visual input using Convolutionary Neural Networks. [10] Using screen pixels as input and the score as reward, their agents achieved superhuman performance on about half of the Atari 2600 console games, with the resulting models performing impressively well; however, in games with extended times for rewards such as MONTEZUMA's REVENGE, learning fails completely, and the models require extensive exploration time to find good strategies, even for simpler games such as Breakout. [11, 7] Since then, reinforcement learning methods have been increasingly successful in a variety of tasks, particularly as a bridge to learning non-differentiable government policies. The range of applications extends far beyond the original paper, from early attention-generating learning mechanisms to significant improvements [8]."}, {"heading": "2.1 Approaches to Reinforcement Learning", "text": "Reinforcement Learning is a comprehensive conceptual framework that summarizes what it means to learn to interact in a state-insecure and unknown world. In Reinforcement Learning, an agent observes a state of his environment at each step and chooses an action. Subsequently, the agent observes the updated state - which is influenced by both the agent's actions and external factors - and the agent can receive a reward. This cycle repeats until a condition for termination is met. A successful Reinforcement Learning Agent learns from his experience in his environment to improve his acquisition of time-discounted rewards in future runs. For all Reinforcement learners, there is an idea of the value of a state or action. The goal is to maximize an exponentially time-discounted sum of future rewards. That is, there is a tiring factor 0 < \u2264 1, and the value of a state or action - depending on the formulation - is up to the sum of future rewards."}, {"heading": "2.1.1 Deep Q-Learning: The Action-Value Formulation", "text": "The original Atari work of DeepMind used an approach they called Deep Q-Learning, based on the older idea of Q-Learning [10]. Q-Learning Agent learns a Q function that takes the current state as input and applies a discounted appreciation for each possible action for the rest of the game. At test time, the Q-Learning Agent simply selects the action with the highest estimated value for that state. At training time, he balances the exploitation of what he considers the best action with the exploration of other actions. Q function is parameterized as a revolutionary neural network and is trained to match the observed value of actions in a particular state with a stored history of observed values by default backpropagation and gradient descent. Of course, the value of a particular action changes as the agent learns to play better in future time steps, so that this training is a moving target when the agent learns."}, {"heading": "2.1.2 Policy Iteration and Asynchronous Advantage Actor-Critic: The Action-Distribution Formulation", "text": "While Deep Q-Networks are known from the original DeepMind Atari paper, policy-based approaches, notably Asynchronous Advantage Actor-Critic (A3C), now dominate most leaderboards on OpenAI Gym, a source of standardized learning environments to reinforce [4].Rather than trying to evaluate the value of action, political networks move to direct learning policy, a function that maps a state with a distribution of measures. Political networks maximize the expected, discounted reward for adherence to this policy, which can be shown to be equated with a drop in favor proportional to the discounted reward R, the protocol of probability assigned to the action. Unlike Deep Q-Networks, learning from Thompson samples or other techniques that effectively utilize the network's degree of uncertainty about the best next action can benefit."}, {"heading": "3 Approach and Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview of Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 Baseline Models", "text": "As a newcomer to reinforcement learning, our first step was to train a robust base model on Atari Breakout using a standard Deep Q network based on an implementation in TensorFlow [5, 1]. However, after 30 hours of training, the agent failed to converge on one that could beat the game. As we could not reproduce the results of DeepMind with DQN without weeks of training, we instead trained a stronger baseline using a state-of-the-art model for many of the gains of reinforcement learning, including Breakout: Asynchronous Advantage Actor-Critic (A3C). We based the model on an implementation of Tensorpack [14] and relied on OpenAI's gym framework for training reinforcement learning agents [4]. After training overnight, A3C managed to achieve a perfect breakout within 30 million training frameworks."}, {"heading": "3.3 Viability of Sub-Task Rewards", "text": "Equipped with this promising baseline, and caught up on our understanding of the state of the art, we have decided to provide a reward for the usefulness of the additional instruction to reinforce the learner by injecting the agent with an additional reward signal when he is in a state we have designated for his learning. Instead of obtaining this additional reward on the basis of a natural language course, we initially simply rewarded the agent for arriving at a state of play in which the position of the ball was directly above the paddle. To achieve this, we have written a template matching library for the breakthrough to return the positions of the ball and paddle within the environment, which has proven to be non-trivial as the paddle in the breakout environment shrinks as the game progresses and the ball overlaps in certain scenarios and the ball changes colors based on Y position to name a few complications."}, {"heading": "3.4 Experiments in Multimodal Embeddings", "text": "The goal of our initial experiments on Breakout was to use the simpler environment to refine and fully understand the sub-components of our model of natural language before assembling them to tackle MONTEZUMA'S REVENGE. After experimenting and testing the reinforcement (A3C) and the reward of the sub-components, the multimodal embedding of the descriptions and frames of natural language into a single embedding space where we could determine whether the description was applied to the frame, the last component of our model that we had not yet executed was embedding in a single embedding space. We first generated a data set of several thousand frames by running the A3C model that we had just trained for breaking out and using our template matching code to give the relative descriptions of entities within the frame. Examples are some of the possible framework descriptions that contain the right to paddle descriptions."}, {"heading": "3.5 MONTEZUMA\u2019S REVENGE", "text": "Contrary to Breakout, the REVENGE environment of MONTEZUMA has a very sparse reward signal. Look at Figure 1 to get an idea: Even after following the seven listed instructions, the agent still has no reward. Only after reaching the key will a reward be received, and on the way to the key results, the skull will be touched. Unsurprisingly, given these challenges, both DQN and A3C have difficulty learning anything. We ran the A3C algorithm on the REVENGE of MONTEZUMA and found that after 72 hours of gameplay and more than 200,000,000 images watched, the agent achieved a consistent score of 0 in the environment. The actions that the trained agent would choose tend to lead to near-immediate death. With this much less impressive base model, we repeated the experiment of matched rewards we conducted in the breakout environment."}, {"heading": "3.6 Dataset Generation", "text": "After demonstrating that subtask rewards are very promising for a reinforcement learning agent learning to play MONTEZUMA's REVENGE, we were next tasked with creating a dataset to learn mappings between natural language state descriptions and raw pixels from the environment. However, such a dataset did not exist, and so we created our own mappings of the game state to natural language descriptions that satisfied them. To do this, we played through the game several times and saved game state frames for each run. Using the template matching code, we were able to create lists of selected commands that were fulfilled by a specific set of consecutive frames. After several playthroughs, we collected 15,000 training frames and kept a further 3,000 frames - a complete separate playthrough - ready for validation."}, {"heading": "3.7 Learning Frame-Command Mappings with Bimodal Network", "text": "As with breakout, the first training phase consisted in creating a multimodal embedding between frame pairs that represented the player's motion and command instructions in Appendix A. The embedding was trained so that when commands were satisfied through the frames, there was a positive point product between the frame and the command embedding. If the commands were not fulfilled, the point product was trained for negativity. We used an LSTM with verbatim embedding to extract command embedding and an undulating neural network running over pairs of frames stacked on the channel dimension for the frame embedding."}, {"heading": "3.7.1 Evidence of Generalization", "text": "Considering the complexity of the bimodal embedding model and its access to a training dataset that contains all the rooms for the first level, we wanted to show that the bimodal has actually learned to generalize the meaning of the commands, not only that the agent is in exactly the same position as in the training data to classify a command as complete. Of course, by combating invisible play-through as validation - which contains many frames different from those in the training set - we were already testing generalization within a room. However, we hoped that the command frame embedding in invisible rooms could be generalized. To test this, we retrained our bimodal embedding by removing access to all training data for the second room, but still including frames from the second room in the validation so that the game elements that are visible in the second room (e.g. ladders)."}, {"heading": "3.8 Run-Time Learning From Natural Language Reward Using Multimodal Embedding", "text": "When training on multimodal embedding is complete, we move on to the learning phase of embedding. For each A3C employee, we load the embedding weights and a list of commands that the agent must execute sequentially. We calculate whether a command has been executed using our pre-trained bi-modal embedding by passing the current observed state and the current command to execute through the network, and mark the command as completed if the score product between the resulting frame and the command embedding is positive. If we determine that a command is completed, we give our reinforcement learning agent an additional reward for successfully completing the task and proceed to the next command, inserting the embedding as additional features in the learning agents, as described in Figure 4."}, {"heading": "3.9 Final Results", "text": "Table 2 shows a comparison between different learning algorithms for reinforcement and our after 10 million training frameworks. Table 3 compares our results with the ranking on OpenAI's gymnasium. Our agent is the clear and convincing winner of both comparisons. It is important to note that these tables do not represent a comparison between apples. The \"environment\" of our agent for MONTEZUMA \"S REVENGE\" differs from that of all others: First, the type of monitoring that our agent has received from the language has shown that he can generalize new states - the agent is able to use language to support navigation through frames that he has never seen before, and that have not been included in the training program for multimodal embedding, so that it is not just a matter of uses."}, {"heading": "4 Conclusion", "text": "We present a novel framework for training reinforcement learners that allows the agent to learn from the lessons in natural language. It is a promising start for collaboration between reinforcement learners and their human trainers: the agent achieves impressive scores in relatively few frames where traditional agents fail. We think this approach to reinforcement learning will be even more fruitful in the real world, for example in robotics, because extremely rich, labeled data sets already exist for images in the real world, allowing for a much more complex multimodal embedding between image and language than what is possible with our synthetic data set. In addition, many tasks in robotics also suffer from the sparse reward problem to which our approach is specifically geared. We think it would be quite useful to have an intelligent robot that can be trained by any human, not just an expert programmer, to quickly learn these new tasks."}, {"heading": "Appendix A: Full List of Commands Usable for MONTEZUMA\u2019S REVENGE", "text": "Climb down the ladder Climb up the ladder Get the key Get the sword Get the torch Get the coin Jump on the rope"}, {"heading": "Go to the left side of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the right side of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the top of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the left room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the right room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the top room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go between the lasers", "text": "Final architectures for MONTEZUMA'S REVENGE: Multimodal Embedding Network: Framehead: Conv [5x5, 32 Filters] ReLU MaxPool [2x2] Conv [5x5, 32 Filters] ReLU MaxPool [2x2] Conv [4x4, 64 Filters] ReLU MaxPool [2x2] Conv [3x3, 64 Filters] FullyConnected [Output Dimension 10] PReLU FullyConnected [Output Dimension 10] Set Header: Text vectors of size 12 - > Hidden State 10RL Guidelines and Value Network: Conv [5x5, 32 Filters] ReLU MaxPool [2x2, 32 Filters] ReLU MaxPool [2x2] Connection [4x4, 64 Filters] Connev [3x3, 64] Connected [5x5, 32 Filters Fulfilled Instructions]"}, {"heading": "Go to the right side of the room Climb down the ladder", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room", "text": "Go to the left side of the room Climb up the ladder and get the key Climb down the ladder"}, {"heading": "Go to the bottom of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the right side of the room Climb up the ladder Jump to the rope", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room Climb up the ladder", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the top of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the right side of the room Use the key", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the right room", "text": "# Room 2"}, {"heading": "Go to the center of the room Climb down the ladder", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom room", "text": "# Room 3"}, {"heading": "Go to the left side of the room Get the sword", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the right side of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the right room", "text": "# Room 4"}, {"heading": "Go between the lasers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room Get the key", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go between the lasers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room Climb down the ladder", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom room", "text": "# Room 5"}, {"heading": "Go to the left side of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the left room", "text": "# Room 6"}, {"heading": "Go between the lasers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go between the lasers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the left side of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the left room", "text": "# Room 7"}, {"heading": "Go to the center of the room Climb up the ladder", "text": "Go to top room # 8 (Torch Room) 709 Use the key"}, {"heading": "Go to the center of the room Jump to the rope", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the top of the room", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room Get the torch Jump to the rope", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the center of the room Jump to the rope", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Go to the bottom of the room", "text": "Go to the lower room"}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["M.G. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "Advances in Neural Information Processing Systems, 30,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["M.G. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "CoRR, abs/1606.01868,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "CoRR, abs/1611.05397,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "CoRR, abs/1604.06057,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": "CoRR, abs/1606.01541,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "CoRR, abs/1602.01783,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M.A. Riedmiller"], "venue": "CoRR, abs/1312.5602,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533, 02", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Feudal networks for hierarchical reinforcement", "author": ["A.S. Vezhnevets", "S. Osindero", "T. Schaul", "N. Heess", "M. Jaderberg", "D. Silver", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Tensorpack", "author": ["Y. Wu"], "venue": "https://github.com/ppwwyyxx/tensorpack,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2017}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Our agent significantly outperforms Deep Q-Networks (DQNs), Asynchronous Advantage ActorCritic (A3C) agents, and the best agents posted to OpenAI Gym [4] on what is often considered the hardest Atari 2600 environment [2]: MONTEZUMA\u2019S REVENGE.", "startOffset": 217, "endOffset": 220}, {"referenceID": 7, "context": "For the first time, reinforcement learning agents were learning from highdimensional, visual input using convolutional neural networks [10].", "startOffset": 135, "endOffset": 139}, {"referenceID": 8, "context": "[11]", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11, 7].", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "[11, 7].", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "The range of applications extends far beyond the original paper, from early attention mechanisms to productive dialogue generation [15, 8].", "startOffset": 131, "endOffset": 138}, {"referenceID": 5, "context": "The range of applications extends far beyond the original paper, from early attention mechanisms to productive dialogue generation [15, 8].", "startOffset": 131, "endOffset": 138}, {"referenceID": 6, "context": "In particular, there has been a shift away from the original deep Q-formulation toward the Asynchronous Advantage Actor-Critic because of improved learning speed and stability [9].", "startOffset": 176, "endOffset": 179}, {"referenceID": 9, "context": ", memory [12, 13, 6, 3].", "startOffset": 9, "endOffset": 23}, {"referenceID": 3, "context": ", memory [12, 13, 6, 3].", "startOffset": 9, "endOffset": 23}, {"referenceID": 2, "context": ", memory [12, 13, 6, 3].", "startOffset": 9, "endOffset": 23}, {"referenceID": 7, "context": "DeepMind\u2019s original Atari paper used an approach that they termed deep Q-learning, based on the older idea of Q-learning [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "A popular variant, A3C, learns from many games in parallel, increases stability by subtracting an estimated state value V (s) from the reward multiplier R in updates, and converges faster than other available options [9, 13].", "startOffset": 217, "endOffset": 224}, {"referenceID": 9, "context": "A popular variant, A3C, learns from many games in parallel, increases stability by subtracting an estimated state value V (s) from the reward multiplier R in updates, and converges faster than other available options [9, 13].", "startOffset": 217, "endOffset": 224}, {"referenceID": 0, "context": "For this we used a standard Deep Q-Network based on an implementation in TensorFlow [5, 1].", "startOffset": 84, "endOffset": 90}, {"referenceID": 10, "context": "We based the model on an implementation from Tensorpack [14] and relied on OpenAI\u2019s Gym framework for training reinforcement learning agents [4].", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "In [2], which uses Intrinsic Motivation as a source of bonus reward, the authors report that their best run achieved a score of 6600 after 100 million training frames.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "We introduce the first deep reinforcement learning agent that learns to beat Atari games with the aid of natural language instructions. The agent uses a multimodal embedding between environment observations and natural language to selfmonitor progress through a list of English instructions, granting itself reward for completing instructions in addition to increasing the game score. Our agent significantly outperforms Deep Q-Networks (DQNs), Asynchronous Advantage ActorCritic (A3C) agents, and the best agents posted to OpenAI Gym [4] on what is often considered the hardest Atari 2600 environment [2]: MONTEZUMA\u2019S REVENGE. Videos of Trained MONTEZUMA\u2019S REVENGE Agents: Our Best Current Model. Score 3500. Best Model Currently on OpenAI Gym. Score 2500. Standard A3C Agent Fails to Learn. Score 0. Figure 1: Left: An agent exploring the first room of MONTEZUMA\u2019S REVENGE. Right: An example of the list of natural language instructions one might give the agent. The agent grants itself an additional reward after completing the current instruction. \u201cCompletion\u201d is learned by training a generalized multimodal embedding between game images and text. \u2217The authors contributed equally. 1 ar X iv :1 70 4. 05 53 9v 1 [ cs .A I] 1 8 A pr 2 01 7", "creator": "LaTeX with hyperref package"}}}