{"id": "1704.05550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Extractive Summarization: Limits, Compression, Generalized Model and Heuristics", "abstract": "Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data.", "histories": [["v1", "Tue, 18 Apr 2017 22:21:22 GMT  (141kb,D)", "http://arxiv.org/abs/1704.05550v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["rakesh verma", "daniel lee"], "accepted": false, "id": "1704.05550"}, "pdf": {"name": "1704.05550.pdf", "metadata": {"source": "CRF", "title": "Extractive Summarization: Limits, Compression, Generalized Model and Heuristics", "authors": ["Rakesh Verma", "Daniel Lee"], "emails": ["rmverma@cs.uh.edu,", "dljr0122@cs.uh.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, we have reached the point where it is only half as much as it was half the time we were looking for."}, {"heading": "2 Related Work", "text": "Most of the summary literature focuses on single documents and multiple documents summary algorithms and frameworks rather than the limits of the performance of summary systems. As of [8], competitive summary systems are usually extractive, selecting representative sentences to link them and push them into more sentences within the framework. Summary literature is huge, so we refer the reader to the recent survey [11], which is fairly comprehensive for summary research up to 2015. Here, we give a sample of the literature and focus on recent research and / or reviews."}, {"heading": "3 Limits on Extractive Summarization", "text": "In all cases, the ROUGE ratings contain the best schemes, as shown in [14], which are usually Rouge-2 (Bigram) and Rouge-3 (Trigram) with stem and stop word animation. We also include the results without stop word animation. The only change was that the original parameters limited the size of the generated summary; we removed this option."}, {"heading": "3.1 Single-document Summarization", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "3.2 Multi-document Extractive Summarization", "text": "For multi-document summaries, there are at least two different scenarios in which the limits of extractive summaries are examined. In the second, we compare each document as a summary in terms of model summaries and then average the results for documents that belong to the same topic.For multi-document summaries, experiments were conducted on data from DUC records for 2004 and 2005. Data were grouped into document clusters. Each cluster held documents that were about a single topic. For the 2004 contest (DUC 2004) we focused on the English document clusters. There were a total of 50 document clusters and each document cluster had an average of 10 documents. DUC 2005 also had 50 document clusters, but there was a minimum of 25 documents for each set.Please note that since the results for R-3 and R-4 were quite low (being 0.23) these results. Super-Now we're not overlapping the documents."}, {"heading": "4 A General Model for Summarization", "text": "The starting point is a document that builds up a summary of the individual sentences in the abstract world of units of thought specializing in syntactical summaries of individual sentences, and then writes a summary of the document on the basis of those sentences. Therefore, we formulate a model for semantic summaries in the abstract world of units of thought, 3 which can be specialized in syntactical summaries by using units of thought. We suspect that a document is a collection of units of thought, some of which are more important than others, with a mapping of sentences to units of thought. The natural mapping is that the implication or inclusion could be partially implied, but not necessarily a complete implication."}, {"heading": "4.1 Algorithms for Single-document Summarization", "text": "Most of them are able to surpass themselves by going in search of a solution that serves their purpose. Most of them are able to find a solution that meets the needs of the market. Most of them are able to surpass themselves. Most of them are able to surpass themselves, most of them are able to surpass themselves, but most of them are not able to put themselves in a position, most of them are able to put themselves in a position, to put themselves in a position, to put themselves in a position to put themselves in a position, to put themselves in a position to put themselves in a position."}, {"heading": "4.2 Results", "text": "Our results include runtime experiments of DocSumm's algorithms. In addition, we compare the performance metrics of DocSumm to DUC 2001 and DUC 2002 datasets.Run times The runtime dataset is created by sampling sentences from the book of Genesis. However, we have created documents of increasing length that measure the length in verses. Verse counting ranges from 4 to 320. However, for documents larger than 20 sentences, the top-down dynamic algorithm runs out of memory. So there are no results on the top-down algorithm. Table 5 shows slight increases in time as the document size increases. Both tfidf and bottom-up there is a significant increase in runtime. Summarization We compare the heuristics for individual documents synmarization to DUC 2001 and DUC 2002 datasets. For the 305 unique documents of the DUC 2001 asmaries, we compare the summarization algorithms."}, {"heading": "5 Conclusions and Future Work", "text": "We have identified limitations to the recall of automatic extractive summaries of DUC records in ROUGE evaluations. Our limitations show that the current state-of-the-art systems evaluated on DUC data reach about 54% of this limit (Rouge 1 recall) for summarizing individual documents, and the best systems for summarizing multiple documents only about one-third of that limit. This is encouraging news, but much work remains to be done on the summary. We have also examined compressibility, a generalized model, and new and existing heuristics for summarizing individual documents. To our knowledge, compressibility as we have defined and studied it is a new concept, and we plan to explore it further in future work. We believe that compressibility could prove a useful measure to examine the performance of automatic summary systems and perhaps also for authorization if, for example, authors are found to be consistently compressible."}, {"heading": "Acknowledgments", "text": "We thank the critics of CICLING 2017 for their constructive comments."}, {"heading": "A Appendix - Proof of Theorem 1", "text": "The reduction of the set cover problem for NP hardness. Given a universe U and a family of S subsets of U, a cover is a subfamily C of S whose union is U. In the set cover problem, the input is a pair (U, S) and a number k, the question is whether there is a cover of the size at most k. We reduce the set cover to a summary as follows. For each member u of U, we select a thought unit t from T and a sentence c that expresses t. For each sentence S in the family, we construct a sentence s consisting of clauses that correspond to the members of S (I have a Boolean value). We merge all the sentences into one document. The capacity constraint C = k and represents the number of sentences we can select for the summary. It is easy to see that a cover corresponds to a summary that maximizes usefulness and satisfies capacity constraints, and vice versa."}, {"heading": "B Appendix - DocSumm Tool", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Appendix - Document 250, AP900625-0153, from DUC 2002", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Fast and robust compressive summarization with dual decomposition and multi-task learning", "author": ["M.B. Almeida", "A.F. Martins"], "venue": "ACL (1). pp. 196\u2013206", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining syntax and semantics for automatic extractive singledocument summarization", "author": ["A. Barrera", "R. Verma"], "venue": "CICLING. vol. LNCS 7182, pp. 366\u2013377", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Jointly learning to extract and compress", "author": ["T. Berg-Kirkpatrick", "D. Gillick", "D. Klein"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. pp. 481\u2013490. Association for Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Concept-based summarization using integer linear programming: From concept pruning to multiple optimal solutions", "author": ["F. Boudin", "H. Mougard", "B. Favre"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 1914\u20131918", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Extractive vs", "author": ["G. Carenini", "J.C.K. Cheung"], "venue": "nlg-based abstractive summarization of evaluative text: The effect of corpus controversiality. In: Proceedings of the Fifth International Natural Language Generation Conference. pp. 33\u201341. Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised sentence enhancement for automatic summarization", "author": ["J.C.K. Cheung", "G. Penn"], "venue": "EMNLP. pp. 775\u2013786", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["S. Chopra", "M. Auli", "A.M. Rush", "S. Harvard"], "venue": "Proceedings of NAACL-HLT16 pp. 93\u201398", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Overview of the tac 2008 update summarization task", "author": ["H.T. Dang", "K. Owczarzak"], "venue": "Proceedings of text analysis conference. pp. 1\u201316", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research pp. 457\u2013479", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "A formal model for information selection in multisentence text extraction", "author": ["E. Filatova", "V. Hatzivassiloglou"], "venue": "Proceedings of the 20th international conference on Computational Linguistics. p. 397. Association for Computational Linguistics", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Recent automatic text summarization techniques: a survey", "author": ["M. Gambhir", "V. Gupta"], "venue": "Artif. Intell. Rev. 47(1), 1\u201366", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions", "author": ["K. Ganesan", "C. Zhai", "J. Han"], "venue": "Proceedings of the 23rd international conference on computational linguistics. pp. 340\u2013348. Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A scalable global model for summarization", "author": ["D. Gillick", "B. Favre"], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. pp. 10\u201318. Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE", "author": ["Y. Graham"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 128\u2013137", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Single-document summarization as a tree knapsack problem", "author": ["T. Hirao", "Y. Yoshida", "M. Nishino", "N. Yasuda", "M. Nagata"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1515\u20131520", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Approximation algorithms for NP-hard problems", "author": ["D.S. Hochbaum"], "venue": "PWS Publishing Co.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "A knowledge induced graph-theoretical model for extract and abstract single document summarization", "author": ["N. Kumar", "K. Srinathan", "V. Varma"], "venue": "Computational Linguistics and Intelligent Text Processing, pp. 408\u2013423. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving multi-documents summarization by sentence compression based on expanded constituent parse trees", "author": ["C. Li", "Y. Liu", "F. Liu", "L. Zhao", "F. Weng"], "venue": "EMNLP. pp. 691\u2013701. Citeseer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic Evaluation of Summaries Using n-gram Co-occurrence Statistics", "author": ["C. Lin", "E. Hovy"], "venue": "HTL-NAACL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards abstractive speech summarization: Exploring unsupervised and supervised approaches for spoken utterance compression", "author": ["F. Liu", "Y. Liu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on 21(7), 1469\u20131480", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Advances in Automatic Summarization", "author": ["I. Mani", "M. Maybury"], "venue": "MIT Press, Cambridge, Massachusetts", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Summarization with a joint model for sentence extraction and compression", "author": ["A.F. Martins", "N.A. Smith"], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. pp. 1\u20139. Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["R. McDonald"], "venue": "Proc. of the 29th ECIR. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Extractive summarization under strict length constraints", "author": ["Y. Mehdad", "A. Stent", "K. Thadani", "D. Radev", "Y. Billawala", "K. Buchner"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Ranking systems evaluation for keywords and keyphrases detection", "author": ["M. Meseure"], "venue": "Tech. rep., Department of Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Textrank: Bringing order into text", "author": ["R. Mihalcea", "P. Tarau"], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Automatic Text Summarization of Newswire: Lessons Learned from the document understanding conference", "author": ["A. Nenkova"], "venue": "AAAI. pp. 1436\u20131441", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Topical coherence for graph-based extractive summarization", "author": ["D. Parveen", "H. Ramsl", "M. Strube"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 1949\u20131954", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated pyramid scoring of summaries using distributional semantics", "author": ["R.J. Passonneau", "E. Chen", "W. Guo", "D. Perin"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 2: Short Papers. pp. 143\u2013147", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 379\u2013389", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Text summarization model based on maximum coverage problem and its variant", "author": ["H. Takamura", "M. Okumura"], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. pp. 781\u2013789. Association for Computational Linguistics", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Summarization evaluation using transformed basic elements", "author": ["S. Tratz", "E.H. Hovy"], "venue": "Proceedings of the First Text Analysis Conference, TAC 2008, Gaithersburg, Maryland, USA, November 17-19, 2008", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Event-centric summary generation", "author": ["L. Vanderwende", "M. Banko", "A. Menezes"], "venue": "Working notes of DUC pp. 127\u2013132", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Extractive summarization using supervised and semi-supervised learning", "author": ["K. Wong", "M. Wu", "W. Li"], "venue": "COLING 2008, 22nd International Conference on Computational Linguistics, Proceedings of the Conference, 18-22 August 2008, Manchester, UK. pp. 985\u2013992", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Extractive summarization by maximizing semantic volume", "author": ["D. Yogatama", "F. Liu", "N.A. Smith"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 1961\u20131966", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Those that did outperform the baseline could not do so in a statistically significant way [27].", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "Summarization can be extractive or abstractive [21]: in extractive summarization sentences are chosen from the article(s) given as input, whereas in abstractive summarization sentences may be generated or a new representation of the article(s) may be output.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "(a) Specifically, we show that when the documents themselves from the DUC 2001-2002 datasets are compared using ROUGE [19] to abstractive summaries, the average Rouge-1 (unigram) recall is around 90%.", "startOffset": 118, "endOffset": 122}, {"referenceID": 7, "context": "As pointed out by [8], competitive summarization systems are typically extractive, selecting representative sentences, concatenating them and often compressing them to squeeze in more sentences within the constraint.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "The summarization literature is vast, so we refer the reader to the recent survey [11], which is fairly comprehensive for summarization research until 2015.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical structure with separate importance, coherence and topic coverage functions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical structure with separate importance, coherence and topic coverage functions.", "startOffset": 187, "endOffset": 191}, {"referenceID": 27, "context": "In [28], the authors present results for single-document summarization on a subset of PLOS Medicine articles and DUC 2002 dataset without mentioning the number of articles used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 72, "endOffset": 75}, {"referenceID": 32, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 8, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 25, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 16, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 23, "context": "Several systems were compared against a newly-devised supervised method on a dataset from Yahoo in [24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 12, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 2, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 0, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 17, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 3, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 34, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 33, "context": "Supervised and semi-supervised learning based extractive summarization was studied in [34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 11, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 5, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 19, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 29, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 6, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 9, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 63, "endOffset": 73}, {"referenceID": 22, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 63, "endOffset": 73}, {"referenceID": 30, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 63, "endOffset": 73}, {"referenceID": 14, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 131, "endOffset": 138}, {"referenceID": 28, "context": "There is also the Pyramid approach [29] and BE [32], for example.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "There is also the Pyramid approach [29] and BE [32], for example.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "Our choice of ROUGE is based on its popularity, ease of use, and correlation with human assessment [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "Our choice of ROUGE configurations includes the one that was found to be best according to the paper [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "In all instances the ROUGE evaluations include the best schemes as shown by [14], which are usually Rouge-2 (bigram) and Rouge-3 (trigram) with stemming and stopword elimination.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "Table 2 shows that, for the DUC 20022 dataset, when the document themselves are considered as summaries and evaluated against a set of 100-word human abstractive summaries, the average Rouge-1 (unigram) [19] score is approximately 91 %.", "startOffset": 203, "endOffset": 207}, {"referenceID": 1, "context": ", see [2,17].", "startOffset": 6, "endOffset": 12}, {"referenceID": 16, "context": ", see [2,17].", "startOffset": 6, "endOffset": 12}, {"referenceID": 9, "context": "Further, if \u201cthought units\u201d are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31].", "startOffset": 178, "endOffset": 188}, {"referenceID": 22, "context": "Further, if \u201cthought units\u201d are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31].", "startOffset": 178, "endOffset": 188}, {"referenceID": 30, "context": "Further, if \u201cthought units\u201d are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31].", "startOffset": 178, "endOffset": 188}, {"referenceID": 24, "context": "This ranking system was compared to other popular keyword identification algorithms and was found to be quite competitive in results [25].", "startOffset": 133, "endOffset": 137}, {"referenceID": 22, "context": "In addition to this optimal algorithm, DocSumm also implements a version of the algorithm presented in [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 30, "context": "A couple of greedy algorithms and a dynamic programming algorithm of DocSumm appeared in [31], the rest are new.", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Comparison On the 533 unique articles in the DUC 2002 dataset, we now compare our greedy and dynamic solutions against the following classes of systems: (i) two top of the line single-document summarizers, SynSem [2], and the best extractive summarizer from [17], which we call KKV, (ii) top five (out of 13) systems, S28, S19, S29, S21, S23, from DUC 2002 competition, (iii) TextRank, (iv) MEAD, (v) McDonald Algorithm and (vi) the DUC 2002 Baseline summaries consisting of the first 100 words of news articles.", "startOffset": 213, "endOffset": 216}, {"referenceID": 16, "context": "Comparison On the 533 unique articles in the DUC 2002 dataset, we now compare our greedy and dynamic solutions against the following classes of systems: (i) two top of the line single-document summarizers, SynSem [2], and the best extractive summarizer from [17], which we call KKV, (ii) top five (out of 13) systems, S28, S19, S29, S21, S23, from DUC 2002 competition, (iii) TextRank, (iv) MEAD, (v) McDonald Algorithm and (vi) the DUC 2002 Baseline summaries consisting of the first 100 words of news articles.", "startOffset": 258, "endOffset": 262}, {"referenceID": 1, "context": "Note that the results for SynSem are from [2], who also used only the 533 unique articles in the DUC 2002 dataset.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Unfortunately, the authors did not report the Rouge bigram (ROUGE-2) and Rouge LCS (ROUGE-L) F1 scores in [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "KKV\u2019s results are from [17], who did not remove the 33 duplicate articles in the DUC 2002 dataset, which is why we flagged those entries in Table 6 with *.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "When we consider Rouge bigram (ROUGE-2) F1-scores Dynamic and Greedy outperform the rest of the field (surprisingly even [17]).", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Our limits show that the current stateof-the-art systems evaluated on DUC data [2,17] are achieving about 54% of this limit (Rouge-1 recall) for single-document summarization and the best systems for multi-document summarization are achieving only about a third of their", "startOffset": 79, "endOffset": 85}, {"referenceID": 16, "context": "Our limits show that the current stateof-the-art systems evaluated on DUC data [2,17] are achieving about 54% of this limit (Rouge-1 recall) for single-document summarization and the best systems for multi-document summarization are achieving only about a third of their", "startOffset": 79, "endOffset": 85}], "year": 2017, "abstractText": "Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data.", "creator": "LaTeX with hyperref package"}}}