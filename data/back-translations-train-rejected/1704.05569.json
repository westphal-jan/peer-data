{"id": "1704.05569", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Using Contexts and Constraints for Improved Geotagging of Human Trafficking Webpages", "abstract": "Extracting geographical tags from webpages is a well-motivated application in many domains. In illicit domains with unusual language models, like human trafficking, extracting geotags with both high precision and recall is a challenging problem. In this paper, we describe a geotag extraction framework in which context, constraints and the openly available Geonames knowledge base work in tandem in an Integer Linear Programming (ILP) model to achieve good performance. In preliminary empirical investigations, the framework improves precision by 28.57% and F-measure by 36.9% on a difficult human trafficking geotagging task compared to a machine learning-based baseline. The method is already being integrated into an existing knowledge base construction system widely used by US law enforcement agencies to combat human trafficking.", "histories": [["v1", "Wed, 19 Apr 2017 00:52:02 GMT  (929kb,D)", "http://arxiv.org/abs/1704.05569v1", "6 pages, GeoRich 2017 workshop at ACM SIGMOD conference"]], "COMMENTS": "6 pages, GeoRich 2017 workshop at ACM SIGMOD conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["rahul kapoor", "mayank kejriwal", "pedro szekely"], "accepted": false, "id": "1704.05569"}, "pdf": {"name": "1704.05569.pdf", "metadata": {"source": "META", "title": "Using Contexts and Constraints for Improved Geotagging of Human Trafficking Webpages", "authors": ["Rahul Kapoor", "Marina Del Rey", "Mayank Kejriwal", "Pedro Szekely"], "emails": ["rahulkap@isi.edu", "kejriwal@isi.edu", "pszekely@isi.edu", "permissions@acm.org."], "sections": [{"heading": "KEYWORDS", "text": "Integer Linear Programming; Information Extraction; Named Entity Recognition; Human Tracking; Feature Agnostic; Distribution Semantics"}, {"heading": "ACM Reference format:", "text": "Rahul Kapoor, Mayank Kejriwal, and Pedro Szekely. 2017. Using Contexts and Constraints for Improved Geotagging of Human Tra cking Webpages. In Proceedings of GeoRich '17, Chicago, IL, USA, 14 May 2017, 6 pages. DOI: h p: / / dx.doi.org / 10.1145 / 3080546.3080547"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that most people are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves."}, {"heading": "2 RELATEDWORK", "text": "Extracting geolocation information, such as cities and countries, from unstructured data is an important problem that tends to fall within the field of information extraction (IE). IE is an old field of research for which a wide range of techniques has been proposed; however, for an accessible collection of Web-IE approaches, we refer the reader to [1]. The objectives of this work are similar to other geolocalization systems for \"di cult\" datasets such as Twi, which is a good example [5]. However, we note that illicit domain challenges differ from those of social media, an important example being the obfuscation of information [3], [15]. Individual components used to build the framework are well established in the research community. For example, word embedding methods used in the contextual classes within our framework have made remarkable progress in NLP (and IE in particular) performance."}, {"heading": "3 FRAMEWORK", "text": "The general approach is illustrated in Figure 1. Entering the system consists of a corpus of web pages serialized as raw HTML by a domain discovery crawling system, and the internal output consists of a series of high-precision geolocation extractions for each web page. We describe the individual steps in the following approach. In this paper, we assume that the web pages originate from the human domain, a cult domain whose challenges have been previously described."}, {"heading": "3.1 Preprocessing", "text": "Since the relevant geotags on the website typically come in natural language elements such as title, description and text (as opposed to structured text), the first step is to extract the text from the website. the pre-processing step is not trivial and involves the automatic removal of foreign elements such as HTML tags and irrelevant characters. Like other extractors, there is often a precision retrieval tradeo, i.e. aggressive removal of irrelevant information can also lead to the removal of relevant information. For this reason, we used an open-source text extractor called Angeles Readability Text Extractor2, and optimized it to achieve high text retrieval and precision separately."}, {"heading": "3.2 Dictionary-based Candidate Extraction", "text": "Due to the explosion of structured data on the web, there are open-source dictionaries, also called semantic lexicographies [13], for identifying candidate geotags from text. Geotags are an almost comprehensive standard dictionary of geonames [16]. Since geonames contains many geotags that are not of interest in our area, such as uninhabited places, we limit our lexicon to a subset that includes states3, countries, and also cities with populations of more than 15,000. One reason for using the trie is that there are places in the lexicon that contain several contiguous tokens (e.g., \"loose angles\"); the trie implements an exact string matching algorithm on the lists of tokens that are issued by pre-processing; one reason for using the trie is that there are places in the lexicon that contain several contiguous tokens (e.g., \"loose angles\") that are relevant."}, {"heading": "3.3 Context Based Classi cation", "text": "To improve the accuracy of the candidates, we suggest using the candidate's context in the text. Intuitively, even the local context is a very revealing indication to people whether, for example, \"Charlo e\" is a name or a city. In the current example, the words that presuppose Charlo e (\"My name is\") allow us to label Charlo e as a name with near certainty. Context-based classisi cation identifies candidates as positive or negative with a supervised machine learning process trained on true positives and negatives. In previous work, CRFs, with manually generated functions, have been used for this purpose."}, {"heading": "3.4 Context-rich ILP Framework", "text": "In fact, most of them are in a position to put themselves in another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "United States(country), Texas(state) in Unites States(country), North Carolina(state) in United States (country), England(state) in United", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Kingdom(country), Pampanga (state) in Philippines(country).", "text": "The new non-composite TSTs introduced by the composite TSTs are California - State, Texas - State, North Carolina - State, England -"}, {"heading": "State, Pampanga - State, United States - Country, United Kingdom -", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Country, Philippines - Country.", "text": "In order to fully specify the ILP model, we need to formulate the objective function and modelling limitations for each country. (It is a question of whether the ILP model in each country is actually a candidate for candidate selection). (It is a question of how far it can be optimized in each country as a candidate for candidate selection. (It is a question of how far this option is suitable for preliminary experiments in this area.) We use two factors for the non-composite TST variables (token source and context truthfulness), and two factors for the compound TST variables (population and zero weight). (It is a question of the different factors that go into determining the weight.) We use two factors for the non-composite TST variables (token source and context truthfulness), and two factors for the compound TST variables (population and zero weight).Token Source."}, {"heading": "3.5 Extraction Selection", "text": "Since variables in our model are binary, each variable can only be set to 1 or 0. As before, we refer to the variables set to 1 as selected variables. In the case of non-composite TST variables, the candidate underlying a selected variable will be marked as the correct extraction of its semantic type. Due to domain semantics, we allow a maximum of one TST per semantic type to be selected. 5In the solution, the composite TST of such a \"selected\" city-country or city-state pair would be 1. In the case of composite TST variables, a selected variable would indicate the underlying relationship as correct. For example, if Los Angeles (city) - in - California (state) is set to 1, the selected candidate city is Los Angeles, the selected candidate state is California, and the relationship is that we refer to Los Angeles in California."}, {"heading": "4 PRELIMINARY EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "The datasets for the experiments are scanned from websites in the Human Tracking Domain that are searched under the DARPA MEMEX program.Since we use the monitored contextual class described in our previous thesis [6], a training dataset is required in conjunction with the Geonames Lexicon (described in Sections 3.2 and 3.3) [16].We train the contextual class using an example of 75 websites with manually annotated geotags.We look at two baselines. We compare (using metrics described below) the selected candidate city per page with canonical geotags (e.g. \"Los Angeles, California, United States,\" which has a Geonames Identi code).We compare (using metrics described below) the selected candidate city with the correct city."}, {"heading": "4.2 Results", "text": "Unsurprisingly, the random baseline performs worst, and the use of contextual classes results in a marked improvement in comparison. The additive effects of ILP on performance are promising and lead to improvements in both accuracy and callback. Error analysis. Further research of the false results found that the ILP marks the wrong candidate as correct when a large population city is present in the footer (or other areas) of the website. We believe that this problem can possibly be mitigated by text extraction (by marking a segment of the 6h p: / / www.darpa.mil / program / memex 7Because the ILP is not monitored, these geotags are not commented on canonically; i.e. a geotag is simply marked as correct or incorrect city."}, {"heading": "4.3 Discussion", "text": "Early results show that ILP provides a simple, unsupervised way to improve geotags output using upstream machine learning models. Although these results are preliminary and experiments with more data sets and domains are needed to fully validate them, we believe that the main advantage of such a model is a systematic coding of constraints that apply universally in the field. In practice, the constraints are able to successfully deal with noisy candidates and candidate-class cations by taking advantage of exclusivity and feasibility limitations. Interestingly, the ILP model also allows us to encode other geographic information, such as the urban population, which is clearly important in the real world when identifying geolocations in the face of loud and uncertain information. Other information that is also readily available from geonames and other knowledge bases, can also be similarly incorporated."}, {"heading": "5 FUTUREWORK", "text": "We will incorporate further limitations into the ILP-based model to further improve performance, as well as further experiments (using both more records and more illegal domains) to validate the early results in this paper. Active investigations are already underway to scale the system to many millions of scratched human websites. e views and conclusions contained in this paper are supported by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under contract number FA8750-14-C0240. e views and conclusions are those of the authors and should not necessarily be interpreted to represent the official guidelines or endorsements of DARPA, AFRL or the US government, either explicitly or implicitly."}], "references": [{"title": "A survey of web information extraction systems", "author": ["C.-H. Chang", "M. Kayed", "M.R. Girgis", "K.F. Shaalan"], "venue": "IEEE transactions on knowledge and data engineering, 18(10):1411\u20131428", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "A uni\u0080ed architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Leveraging publicly available data to discern pa\u008aerns of human-tra\u0081cking activity", "author": ["A. Dubrawski", "K. Miller", "M. Barnes", "B. Boecking", "E. Kennedy"], "venue": "Journal of Human Tra\u0081cking, 1(1):65\u201385", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Text-based twi\u008aer user geolocation prediction", "author": ["B. Han", "P. Cook", "T. Baldwin"], "venue": "Journal of Arti\u0080cial Intelligence Research, 49:451\u2013500", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Information extraction in illicit domains", "author": ["M. Kejriwal", "P. Szekely"], "venue": "arXiv preprint arXiv:1703.03097", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Wrapper induction for information extraction", "author": ["N. Kushmerick"], "venue": "PhD thesis, University of Washington", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Toponym resolution in text: Annotation", "author": ["J.L. Leidner"], "venue": "evaluation and applications of spatial grounding of place names. Universal-Publishers", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Deepdive: Web-scale knowledge-base construction using statistical learning and inference", "author": ["F. Niu", "C. Zhang", "C. R\u00e9", "J.W. Shavlik"], "venue": "VLDS, 12:25\u201328", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Gurobi optimizer reference manual", "author": ["G. Optimization"], "venue": "URL: h\u0088p://www. gurobi. com,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Hybrid geo-information processing: Crowdsourced supervision of geo-spatial machine learning tasks", "author": ["F. Ostermann"], "venue": "Proceedings of the 18th AGILE International Conference on Geographic Information Science, Lisbon, Portugal, pages 9\u201312", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning dictionaries for information extraction by multi-level bootstrapping", "author": ["E. Rilo", "R. Jones"], "venue": "In AAAI/IAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Noun-phrase co-occurrence statistics for semiautomatic semantic lexicon construction", "author": ["B. Roark", "E. Charniak"], "venue": "Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 1110\u20131116. Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Text-driven toponym resolution using indirect supervision", "author": ["M. Speriosu", "J. Baldridge"], "venue": "ACL (1), pages 1466\u20131476", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["P. Szekely", "C.A. Knoblock", "J. Slepicka", "A. Philpot", "A. Singh", "C. Yin", "D. Kapoor", "P. Natarajan", "D. Marcu", "K. Knight"], "venue": "Building and using a knowledge graph to combat human tra\u0081cking. In International Semantic Web Conference, pages 205\u2013221. Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Geonames", "author": ["M. Wick", "C. Boutreux"], "venue": "GeoNames Geographical Database", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Although precise numbers for human tra\u0081cking Web advertising activity are not known, they are very high, possibly in the tens of millions of (not necessarily unique) advertisements posted on the Web [3].", "startOffset": 199, "endOffset": 202}, {"referenceID": 7, "context": "Recent advances in information extraction and knowledge base construction technology, especially using techniques like deep neural networks and word embeddings [9], [2], gives investigators (such as law enforcement and intelligence agencies) the valuable opportunity to turn the Web against illicit players.", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Recent advances in information extraction and knowledge base construction technology, especially using techniques like deep neural networks and word embeddings [9], [2], gives investigators (such as law enforcement and intelligence agencies) the valuable opportunity to turn the Web against illicit players.", "startOffset": 165, "endOffset": 168}, {"referenceID": 4, "context": "some speci\u0080c challenges [6], [15].", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "some speci\u0080c challenges [6], [15].", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "Such tags are o\u0089en present in free text \u0080elds like description or the page body, and not within structured HTML tags (hence, cannot be extracted by wrapper-based extractors [7]).", "startOffset": 173, "endOffset": 176}, {"referenceID": 4, "context": "Using a lexicon to directly extract geolocations is problematic for this reason; richer clues like context (such as the words surrounding an extraction) are necessary for disambiguation [6], [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "Using a lexicon to directly extract geolocations is problematic for this reason; richer clues like context (such as the words surrounding an extraction) are necessary for disambiguation [6], [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 14, "context": "as geolocation candidates [16].", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "To determine the probability of the candidate being a geolocation, we use a recent machine learning-based approach that uses context features [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "IE is an old research area for which a wide range of techniques have been proposed; for an accessible survey of Web IE approaches, we refer the reader to [1].", "startOffset": 154, "endOffset": 157}, {"referenceID": 3, "context": "\u008ce goals of this work are similar to other geolocation prediction system for \u2018di\u0081cult\u2019 datasets like Twi\u008aer, a good example being [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "However, we note that illicit domain challenges are di\u0082erent from those of social media, an important example being information obfuscation [3], [6], [15].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "However, we note that illicit domain challenges are di\u0082erent from those of social media, an important example being information obfuscation [3], [6], [15].", "startOffset": 145, "endOffset": 148}, {"referenceID": 13, "context": "However, we note that illicit domain challenges are di\u0082erent from those of social media, an important example being information obfuscation [3], [6], [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 1, "context": "For example, word embedding methods, used in the contextual classi\u0080er in our framework, have achieved notable advances in NLP (and especially IE) performance [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "Lexicon-based IE has also received much coverage in the literature, an in\u0083uential recent work being [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "Given its importance, geolocation extraction has received a lot of focused a\u008aention in the literature, an important related work being the recent text and context-based approach by Speriosu and Baldridge [14].", "startOffset": 204, "endOffset": 208}, {"referenceID": 6, "context": "Some of the techniques in this work, such as usage of text and populations, derive from extant techniques on toponym resolution [8], [14].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "Some of the techniques in this work, such as usage of text and populations, derive from extant techniques on toponym resolution [8], [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "A good description may be found in the book by Leidner [8].", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "We note, however, that except for a recent paper that we published [6], no work has tackled the challenges of high-performance geolocation extraction in domains Figure 1: A work\u0083ow-level illustration of the geotagging", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "available dictionaries, also called semantic lexicons [13], for identifying candidate geotags from text.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "For geotags, a standard nearcomprehensive lexicon is Geonames [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "such feature functions are o\u0089en problematic for irregular, obfuscated domains like human tra\u0081cking [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "high supervision problems by \u0080rst e\u0081ciently deriving low-dimensional word embeddings from an extracted text corpora as word feature vectors (WFVs) [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "Since we use the supervised contextual classi\u0080er described in our previous work [6], in conjunction with the Geonames lexicon (described in Sections 3.", "startOffset": 80, "endOffset": 83}, {"referenceID": 14, "context": "3) [16], a training dataset is required.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "We used the licensed version of Gurobi Optimizer [10] for modeling and solving ILP.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "A promising alternative, on which there is limited work, is to acquire more labels by crowd-sourcing to facilitate be\u008aer training of the context-based classi\u0080er [11].", "startOffset": 161, "endOffset": 165}], "year": 2017, "abstractText": "Extracting geographical tags from webpages is a well-motiva-ted application in many domains. In illicit domains with unusual language models, like human tra\u0081cking, extracting geotags with both high precision and recall is a challenging problem. In this paper, we describe a geotag extraction framework in which context, constraints and the openly available Geonames knowledge base work in tandem in an Integer Linear Programming (ILP) model to achieve good performance. In preliminary empirical investigations, the framework improves precision by 28.57% and F-measure by 36.9% on a di\u0081cult human tra\u0081cking geotagging task compared to a machine learning-based baseline. \u008ce method is already being integrated into an existing knowledge base construction system widely used by US law enforcement agencies to combat human tra\u0081cking.", "creator": "LaTeX with hyperref package"}}}