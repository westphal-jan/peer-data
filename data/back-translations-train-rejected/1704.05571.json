{"id": "1704.05571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain", "abstract": "Word embeddings have made enormous inroads in recent years in a wide variety of text mining applications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two financial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classifier using the labeled context vectors, and use the trained classifier to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires little feature crafting effort and performs well across roles.", "histories": [["v1", "Wed, 19 Apr 2017 00:55:23 GMT  (787kb)", "http://arxiv.org/abs/1704.05571v1", "DSMM 2017 workshop at ACM SIGMOD conference"]], "COMMENTS": "DSMM 2017 workshop at ACM SIGMOD conference", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mayank kejriwal"], "accepted": false, "id": "1704.05571"}, "pdf": {"name": "1704.05571.pdf", "metadata": {"source": "META", "title": "Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain", "authors": ["Mayank Kejriwal"], "emails": ["kejriwal@isi.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar Xiv: 170 4.05 571v 1 [cs.C L] 19 Apr 201 7Word embeddings have made tremendous progress in a variety of text mining applications in recent years. In this paper, we examine a Word embeddedness architecture to predict the relevance of a role between two nancial units in the context of natural language sentences. In this advanced abstract, we propose a pooled approach that uses a collection of sentences to train word embeddedness using the word2vec architecture. We use word embeddedness to obtain context vectors to which one or more labels are assigned based on manual annotations."}, {"heading": "KEYWORDS", "text": "Role Relevance; Distribution Semantics; Triples Ranking; Word2Vec"}, {"heading": "ACM Reference format:", "text": "Mayank Kejriwal. 2017. Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain. In Proceedings of DSMM '17, Chicago, IL, USA, May 14, 2017, 3 pages. DOI: h p: / / dx.doi.org / 10.1145 / 3077240.3077249"}, {"heading": "METHODS AND RESULTS", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "DSMM\u201917, May 14, 2017, Chicago, IL, USA Mayank Kejriwal", "text": "This year it is more than ever before in the history of the city."}, {"heading": "Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain DSMM\u201917, May 14, 2017, Chicago, IL, USA", "text": "In Proceedings of the Workshop on Data Science for Macro-Modeling (DSMM @ SIGMOD), 2017. [3] C. Zhai and J. La erty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual ACM SIGIR conference on Research and development in information retrieval, pages 334-342. ACM, 2001.This figure \"approach.png\" is available in \"png\" format from: http: / / arxiv.org / ps / 1704.05571v1This figure \"cities-tsne.png\" is available in \"http: / / arxiv.org / ps / 17.05571.pg\" This figure \"tsnexcities-format\" 1.pis available in \"http: / / arxiv.org / nxiv.org / ps / 17.051.pg\" This figure \"tsnexcities-format\" is available in \"this figure: http: / / / arxiv.org / nxivnxiv.org / 17.071.pg\" This figure \"is available in\" tspis: This figure: This figure is 7.07v.7v / this figure: This figure: This figure is available in \"tspis 7v / this figure: This figure: 7v / 7v / this figure is available in\" 7v / this figure: This figure: This figure: This figure is 7v / 7v / this figure: This figure: This figure is 7v: This figure in \"tspis 7v: This figure:"}], "references": [{"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Financial entity identi\u0080cation and information integration (FEIII) challenge 2017:  Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain DSMM\u201917, May 14, 2017, Chicago, IL, USA \u008ce report of the organizing commi\u008aee", "author": ["L. Raschid", "D. Burdick", "M. Flood", "J. Grant", "J. Langsam", "I. Soboro", "E. Zotkina"], "venue": "In Proceedings of the Workshop on Data Science for Macro-Modeling (DSMM@SIGMOD),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "La\u0082erty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334\u2013342", "author": ["J.C. Zhai"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "(fh , r , ft ) that consists of a head \u0080nancial entity fh , a tail \u0080nancial entity ft , and a role r , typically from a closed domain-speci\u0080c universe R of roles, role relevance prediction is de\u0080ned as the problem of assigning a relevance score in [0, 1] to the triple in a given context c .", "startOffset": 248, "endOffset": 254}, {"referenceID": 2, "context": "3077249 Role relevance is not unlike an ad-hoc Information Retrieval (IR) problem in that the relevance function is domain-speci\u0080c and adhoc [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "[1], and with only 30 dimensions in the latent space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "62% NDCG [2].", "startOffset": 9, "endOffset": 12}], "year": 2017, "abstractText": "Word embeddings have made enormous inroads in recent years in awide variety of text mining applications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two \u0080nancial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classi\u0080er using the labeled context vectors, and use the trained classi\u0080er to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires li\u008ale feature cra\u0089ing e\u0082ort and performswell across roles.", "creator": "LaTeX with hyperref package"}}}