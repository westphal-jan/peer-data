{"id": "1704.05588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Learning to Fly by Crashing", "abstract": "How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see:", "histories": [["v1", "Wed, 19 Apr 2017 02:20:20 GMT  (2824kb,D)", "http://arxiv.org/abs/1704.05588v1", null], ["v2", "Thu, 27 Apr 2017 00:13:19 GMT  (2824kb,D)", "http://arxiv.org/abs/1704.05588v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["dhiraj gandhi", "lerrel pinto", "abhinav gupta"], "accepted": false, "id": "1704.05588"}, "pdf": {"name": "1704.05588.pdf", "metadata": {"source": "CRF", "title": "Learning to Fly by Crashing", "authors": ["Dhiraj Gandhi", "Lerrel Pinto"], "emails": ["gabhinav}@andrew.cmu.edu"], "sections": [{"heading": null, "text": "In fact, it is that we are able to maneuver ourselves into a situation where we have to put ourselves at the center, \"he said.\" It's not that we are able to maneuver ourselves into such a situation, \"he said.\" But it's not that we are able to maneuver ourselves into such a situation. \""}, {"heading": "II. RELATED WORK", "text": "This work, which combines self-supervised learning with flying a drone indoors, touches on the wide areas of robot learning and drone control. We briefly describe this work and its links to our method."}, {"heading": "A. Drone control", "text": "Several methods [1], [3], [9] - [13] use range or visual sensors to derive environmental maps while estimating their position on the map. However, these SLAM-based methods are computationally expensive due to explicit 3D reconstruction, which greatly limits the ability to navigate in real time on a low-cost platform. Another method that comes closer to our method in real time is the depth estimation method. Onboard range sensors can be used to fly autonomously, avoiding obstacles [14], [15] but this is not practical for publicly available drones, which often have a low battery life and low carrying capacity. Stereo vision-based assessment methods can also be used [16], [17] with light and cheap cameras."}, {"heading": "B. Deep learning for robots", "text": "Self-monitoring methods [6], [7], [19], [20] show how large-scale data collections can be used in the real world to learn tasks such as grasping and pushing objects in a desktop environment. Our work extends this idea of self-monitoring to flying a drone in an indoor environment. Generalization methods [21] have shown impressive results, but are too data-intensive (sequence of millions of examples) for our task of drone flying. A central component of deep learning is the large amount of data required to train these generalizable models [22], [23]. Self-monitoring learning comes into play here, enabling the collection of large amounts of data with minimal human monitoring."}, {"heading": "III. APPROACH", "text": "We will now describe details of our data-driven flight approach with discussions on methods of data collection and learning. Further we will describe our hardware setup and implementation for reproducibility."}, {"heading": "A. Hardware Specifications:", "text": "An important goal of our method is to demonstrate the effectiveness of cost-effective systems for the complex task of indoor flying. To this end, we use the Parrot Ar-Drone 2.0, which due to its inaccuracies is often operated as an outdoor hobby drone. As part of data acquisition, we do not install additional sensors / cameras in the airspace. The most important components required for the drone are its built-in camera, which transmits 720p images at 30 Hz, its built-in accelerometer and a safety fuselage to collide with objects without damaging the rotors."}, {"heading": "B. Data Collection:", "text": "In fact, most of us are able to move to another world in which we are in the position in which we find ourselves."}, {"heading": "C. Learning Methodology", "text": "We will now describe our learning methodology in the light of the binary classification data sets we have collected. We will first describe our learning architecture and follow it by describing the test time.1) Network Architecture: Given the recent success of deep networks in learning from visual input factors, we will use it to learn control for flying. We will use the AlexNet architecture [22]. We will use ImageNet pre-trained weights as the starting point for our network [23]. This network architecture is shown in Figure 4. Note that the weights for the last fully connected layer are initialized by a Gaussian distribution. We will learn a simple classification network that predicts from an input screen whether the drone should move forward in a straight line or not. Therefore, the last layer is a binary softmax and the loss of negative log-likelihood.2) Test Time Execution: Our model essentially learns when it is going in a particular direction, is good or not. But how can we use this model to fly autonomously and with multiple obstacles?"}, {"heading": "IV. EXPERIMENTAL EVALUATION", "text": "We now describe the evaluation process of our method for interiors and compare it to highly depth-driven baselines."}, {"heading": "A. Baselines", "text": "To evaluate our model, we compare the performance with a policy of a straight line, a policy of depth prediction based on guidelines and a policy of human control. 1) Policy of a straight line: A weak baseline for indoor navigation is a straight line with an open loop. 2) Policy of depth prediction: Recent advances in depth estimation by monocular cameras [28] have shown impressive results. These depth estimation models are often trained about 220k depth data indoors. A strong baseline is to use this depth prediction network to create detailed maps that give a monocular image and draw a conclusion about the direction of motion based on this depth map.3) Human policy: One of the strongest baseline is to use a human operator to let the drone fly. In this case, we ask participants to control the drone so that they determine the direction of movement of the drone they have seen."}, {"heading": "B. Testing Environments", "text": "To demonstrate the universality of our method, we test it on 6 complex interiors: \"Glass Door,\" \"NSH 4th Floor,\" \"NSH Entrance,\" \"Wean Hall,\" \"Hallway\" and \"Corridor with Chairs.\" Floor plans for these environments can be seen in Figure 5. These environments have unique challenges that include most of the challenges in general indoor navigation. For each of these environments, our method is run along with all baselines 5 times with different launch orientations and positions. This is done to ensure that the comparisons are robust to initiate introduction.1) Glass Door: This environment is corridor with a corner that has transparent doors. The challenge for the drone is to take a curve at that corner without hitting the transparent door."}, {"heading": "C. Results", "text": "In order to assess the performance of the various baselines, we used average distance and average flight time without collisions as a measurement for the assessment of battery windows. This measurement also terminates flight paths when they take small loops (rotate on site). Quantitative results are presented in Table I. We also qualitatively show in Figure 6 the comparison of trajectories generated by our method with the baselines based on depth prediction. In every environment / environment we test, we see that our method performs much better than the depth baseline. The best straight baseline gives an estimate of how difficult the environments are. Human-controlled baselines are higher than our method for most environments. However, in some environments, such as \"corridor with chairs,\" the presence of cluttered objects makes it difficult for participants to navigate through narrow spaces, which allows our method to exceed human level control in this environment."}, {"heading": "V. CONCLUSION", "text": "We propose a data-driven approach to learning to fly by crashing more than 11,500 times in a variety of different training environments. These crash trajectories generate the largest (to our knowledge) UAV crash dataset and demonstrate the importance of negative data in learning. A standard, deep network architecture is trained on these indoor crash data, with the task of binary classification. By learning how NOT to fly, we show that even simple strategies easily surpass depth prediction methods based on a variety of test environments and are comparable to human control in some environments. This work shows: (a) it is possible to collect self-monitored data for large-scale navigation; (b) such data is critical to learn how to navigate."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by ONR MURI N000141612007, NSF IIS-1320083 and Google Focused Award. Partially supported by Sloan Research Fellowship."}], "references": [{"title": "Autonomous flight in unstructured and unknown indoor environments", "author": ["A.G. Bachrach"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning deep control policies for autonomous aerial vehicles with mpc-guided policy search", "author": ["T. Zhang", "G. Kahn", "S. Levine", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE, 2016, pp. 528\u2013535.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Autonomous mav flight in indoor environments using single image perspective cues", "author": ["C. Bills", "J. Chen", "A. Saxena"], "venue": "Robotics and automation (ICRA), 2011 IEEE international conference on. IEEE, 2011, pp. 5776\u20135783.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning monocular reactive uav control in cluttered natural environments", "author": ["S. Ross", "N. Melik-Barkhudarov", "K.S. Shankar", "A. Wendel", "D. Dey", "J.A. Bagnell", "M. Hebert"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2013, pp. 1765\u20131772.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "ICRA, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["P. Agrawal", "A. Nair", "P. Abbeel", "J. Malik", "S. Levine"], "venue": "arXiv preprint arXiv:1606.07419, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "cad)$\u02c62$rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "CoRR, vol. abs/1611.04201, 2016. [Online]. Available: http://arxiv.org/abs/1611.04201", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Lsd-slam: Large-scale direct monocular slam", "author": ["J. Engel", "T. Sch\u00f6ps", "D. Cremers"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 834\u2013849.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Rslam: A system for large-scale mapping in constant-time using stereo", "author": ["C. Mei", "G. Sibley", "M. Cummins", "P. Newman", "I. Reid"], "venue": "International journal of computer vision, vol. 94, no. 2, pp. 198\u2013214, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft kinect sensor and its effect", "author": ["Z. Zhang"], "venue": "IEEE multimedia, vol. 19, no. 2, pp. 4\u201310, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Rgb-d mapping: Using kinect-style depth cameras for dense 3d modeling of indoor environments", "author": ["P. Henry", "M. Krainin", "E. Herbst", "X. Ren", "D. Fox"], "venue": "The International Journal of Robotics Research, vol. 31, no. 5, pp. 647\u2013663, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Stereo vision based indoor/outdoor navigation for flying robots", "author": ["K. Schmid", "T. Tomic", "F. Ruess", "H. Hirschm\u00fcller", "M. Suppa"], "venue": "Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on. IEEE, 2013, pp. 3955\u20133962.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Quadrotor using minimal sensing for autonomous indoor flight", "author": ["J.F. Roberts", "T. Stirling", "J.-C. Zufferey", "D. Floreano"], "venue": "European Micro Air Vehicle Conference and Flight Competition (EMAV2007), no. LIS-CONF-2007-006, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "State estimation for aggressive flight in gps-denied environments using onboard sensing", "author": ["A. Bry", "A. Bachrach", "N. Roy"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on. IEEE, 2012, pp. 1\u20138.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Stereo vision and laser odometry for autonomous helicopters in gps-denied indoor environments", "author": ["M. Achtelik", "A. Bachrach", "R. He", "S. Prentice", "N. Roy"], "venue": "SPIE Defense, security, and sensing. International Society for Optics and Photonics, 2009, pp. 733 219\u2013733 219.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Vision-based autonomous mapping and exploration using a quadrotor mav", "author": ["F. Fraundorfer", "L. Heng", "D. Honegger", "G.H. Lee", "L. Meier", "P. Tanskanen", "M. Pollefeys"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE, 2012, pp. 4557\u20134564.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Aerial Informatics and Robotics platform", "author": ["S. Shah", "D. Dey", "C. Lovett", "A. Kapoor"], "venue": "Microsoft Research, Tech. Rep. MSR-TR-2017-9, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning handeye coordination for robotic grasping with deep learning and largescale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "ISER, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to push by grasping: Using multiple tasks for effective learning", "author": ["L. Pinto", "A. Gupta"], "venue": "CoRR, vol. abs/1609.09025, 2016. [Online]. Available: http://arxiv.org/abs/1609.09025", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580\u2013587.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Training region-based object detectors with online hard example mining", "author": ["A. Shrivastava", "A. Gupta", "R.B. Girshick"], "venue": "CoRR, vol. abs/1604.03540, 2016. [Online]. Available: http://arxiv.org/abs/1604. 03540", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Fastslam: A factored solution to the simultaneous localization and mapping problem", "author": ["M. Montemerlo", "S. Thrun", "D. Koller", "B. Wegbreit"], "venue": "2002.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "Simultaneous localization and mapping with sparse extended information filters", "author": ["S. Thrun", "Y. Liu", "D. Koller", "A.Y. Ng", "Z. Ghahramani", "H. Durrant- Whyte"], "venue": "The International Journal of Robotics Research, vol. 23, no. 7-8, pp. 693\u2013716, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Parallel tracking and mapping for small AR workspaces", "author": ["G. Klein", "D. Murray"], "venue": "Proc. Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR\u201907), Nara, Japan, November 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Advances in neural information processing systems, 2014, pp. 2366\u20132374.  NSH 4th Floor Wean Hall  Glass Door D  ep th O  ur D  ep th O  ur D  ep th O  ur NSH Entrance D  ep th O  ur Hallway D  ep th O  ur Hallway with Chairs D  ep th O ur Fig. 6. Here we show the comparisons of the trajectories of our method vs the strong baseline of depth based prediction on our testing environments. The arrows denote the action taken by corresponding method.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Most early research focused on a two-step approach: the first step being perception where either SLAM [1] or sensors [2] are used to estimate the underlying map and/or 3D [3]; the second step is to use the predicted depth or map to issue motor commands", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "Most early research focused on a two-step approach: the first step being perception where either SLAM [1] or sensors [2] are used to estimate the underlying map and/or 3D [3]; the second step is to use the predicted depth or map to issue motor commands", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Most early research focused on a two-step approach: the first step being perception where either SLAM [1] or sensors [2] are used to estimate the underlying map and/or 3D [3]; the second step is to use the predicted depth or map to issue motor commands", "startOffset": 171, "endOffset": 174}, {"referenceID": 3, "context": "possibility is to use imitation learning [4].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "Recently, there has been growing interest in using selfsupervised learning for variety of tasks like navigation [5], grasping [6] and pushing/poking [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "Recently, there has been growing interest in using selfsupervised learning for variety of tasks like navigation [5], grasping [6] and pushing/poking [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "Recently, there has been growing interest in using selfsupervised learning for variety of tasks like navigation [5], grasping [6] and pushing/poking [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "Can we use selfsupervised learning to remove the labeling bottleneck of imitation learning? But how do we collect data for selfsupervised learning? In contemporary work, Sadeghi and Levine [8] use Reinforcement Learning (RL) in simulation to train the navigation policy of the drone.", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "We note that the testing scenarios in [8] consist mostly of empty corridors where perspective cues are sufficient", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "for navigation [3].", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "One can use onboard range sensors to fly autonomously while avoiding obstacles [14], [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "One can use onboard range sensors to fly autonomously while avoiding obstacles [14], [15].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "One can also use stereo vision based estimation [16], [17] with light and cheap cameras.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "One can also use stereo vision based estimation [16], [17] with light and cheap cameras.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "Monocular camera based methods [3] use vanishing points as a guidance for drone flying, but still rely on range sensors for collision avoidance.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Researchers [4] have used imitation learning strategies to transfer human demonstrations to autonomous navigation.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "drone data [8], [18].", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "drone data [8], [18].", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "Deep reinforcement learning methods [21] have shown impressive results, however they are too data intensive (order of million examples) for our task of drone flying.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "A key component of deep learning, is the high amount of data required to train these generalizable models [22], [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "A key component of deep learning, is the high amount of data required to train these generalizable models [22], [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "This sampling of hard negatives has been shown to improve performance [6], [24].", "startOffset": 70, "endOffset": 73}, {"referenceID": 23, "context": "This sampling of hard negatives has been shown to improve performance [6], [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "1) Collecting collision data: Most methods for learning with drones [3], [4] often have very few examples of colliding with objects.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "1) Collecting collision data: Most methods for learning with drones [3], [4] often have very few examples of colliding with objects.", "startOffset": 73, "endOffset": 76}, {"referenceID": 24, "context": "Another way to collect this data is by commanding the drone to autonomously navigate the environment by SLAM based approaches [25], [26] and collect failure cases.", "startOffset": 126, "endOffset": 130}, {"referenceID": 25, "context": "Another way to collect this data is by commanding the drone to autonomously navigate the environment by SLAM based approaches [25], [26] and collect failure cases.", "startOffset": 132, "endOffset": 136}, {"referenceID": 26, "context": "For this purpose, we use PTAM [27] module that localizes the robot and helps it backtrack.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "We use the AlexNet architecture [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "We use ImageNet-pretrained weights as initialization for our network [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "The convolutional weights of our network (in grey) are pretrained from ImageNet classification [22], while the fully connected weights (in orange) is initialized randomly and learnt entirely from the collision data.", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "2) Depth prediction based policy: Recent advances in depth estimation from monocular cameras [28] have shown impressive results.", "startOffset": 93, "endOffset": 97}], "year": 2017, "abstractText": "How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see: https://youtu.be/HbHqC8HimoI", "creator": "LaTeX with hyperref package"}}}