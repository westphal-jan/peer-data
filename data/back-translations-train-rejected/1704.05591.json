{"id": "1704.05591", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "OCRAPOSE II: An OCR-based indoor positioning system using mobile phone images", "abstract": "In this paper, we propose an OCR (optical character recognition)-based localization system called OCRAPOSE II, which is applicable in a number of indoor scenarios including office buildings, parkings, airports, grocery stores, etc. In these scenarios, characters (i.e. texts or numbers) can be used as suitable distinctive landmarks for localization. The proposed system takes advantage of OCR to read these characters in the query still images and provides a rough location estimate using a floor plan. Then, it finds depth and angle-of-view of the query using the information provided by the OCR engine in order to refine the location estimate. We derive novel formulas for the query angle-of-view and depth estimation using image line segments and the OCR box information. We demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios. It is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition.", "histories": [["v1", "Wed, 19 Apr 2017 02:43:23 GMT  (8582kb,D)", "http://arxiv.org/abs/1704.05591v1", "14 pages, 22 Figures"]], "COMMENTS": "14 pages, 22 Figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed sadeghi", "shahrokh valaee", "shahram shirani"], "accepted": false, "id": "1704.05591"}, "pdf": {"name": "1704.05591.pdf", "metadata": {"source": "CRF", "title": "OCRAPOSE II: An OCR-based indoor positioning system using mobile phone images", "authors": ["Hamed Sadeghi", "Shahrokh Valaee", "Shahram Shirani"], "emails": ["hsadeghi@ece.utoronto.ca.", "valaee@ece.utoronto.ca."], "sections": [{"heading": null, "text": "Index Terms - Indoor Localization, Depth Assessment, Viewpoint Assessment, OCR, Vanishing Point."}, {"heading": "1 INTRODUCTION", "text": "The most common method of indoor localization is based on merging Wi-Fi RSS fingerprints with data from inertial sensors [1]. These methods require an appropriate number of Wi-Fi access points, which must be visible at each location [2]. Under these conditions, they have localization errors of about 2 meters, where training points have a granularity of 1 meter [3]. In scenarios where insufficient Wi-Fi access points are available, access to Wi-Fi RSS reader hardware is blocked (e.g. iPhones), or where greater localization accuracy (e.g. below the meter) is required, image-based methods can be used as an effective solution. Although image-based localization has long been used in the fields of robotics [4], [5] and augmented reality [6], [7] it has only been used in the last decade for mobile phones in scenarios where spatial localization is not possible."}, {"heading": "H. Sadeghi and S. Valaee are with the Department of Electrical and Computer Engineering, University of Toronto, ON, Canada, M5S 2E4. E-mail: hsadeghi,valaee@ece.utoronto.ca.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S. Shirani is with the Department of Electrical and Computer Engineering,", "text": "It is not that in a number of indoor spaces such as office buildings, parking lots, airports, grocery stores, etc., where the distinctive landmarks are text and / or numbers (characters in general), the above methods fail to provide good detection performance for a significant percentage of the query results. In scenarios with large image databases, stereo feature matching is mathematically expensive and is not performed for the best matches in the image evaluation methods. Instead, bags of feature-based methods are used to find the best matching landmarks / images (s). Here, we use stereo feature matching to illustrate the existence of similar features and the lack of distinguishing features as the main reasons for failure in the best detection (s). We use stereo feature matching matching methods to illustrate the existence of the best matching landmarks / images. We use stereo feature matching to illustrate the existence of similar features, to illustrate the existence of stereo feature matching features."}, {"heading": "2 RELATED WORK", "text": "Even if the pictures are not pictures of people, but of people who are in another country, but of people who are in another country, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another, in another"}, {"heading": "3 THE PROPOSED SYSTEM (OCRAPOSE II)", "text": "Fig. 2 shows the structure of the proposed system. As you can see, the proposed system recognizes the horizontal vanishing point in order to estimate the AOV of the user in relation to the position of the center of the characters. In addition, it recognizes and recognizes the characters in the query image, which together with the floor plan provides the necessary information for a rough localization. Next, using the estimated AOV and the width of the OCR box in the image and in the real world, the depth of the query is estimated. Finally, a fine estimate of the user's location is calculated based on the estimated depth and AOV. Subsequently, we explain the role of the system blocks in detail."}, {"heading": "3.1 Characters detection", "text": "In fact, it is as if most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is as if most of us are able to abide by the rules. (...) In fact, it is as if they cannot abide by the rules. (...) It is as if they do not have to abide by the rules of the market. (...) It is as if they must abide by the rules of the market. (...) It is as if they do not have to abide by the rules of the market. \"(...) It is as if they must abide by the rules of the market.\" (...) It is as if they must abide by the rules of the market. (...) It is as if they must abide by the rules of the market. (...) It is as if they must abide by the rules of the market. (...)"}, {"heading": "3.2 OCR", "text": "The OCR block recognizes the existing characters in the detected regions. We perform some pre-processing before entering the extended MSER regions into the OCR block. This pre-processing includes global binarization and removal of relatively small / large regions. Next, we enter the remaining regions into the OCR engine. To perform OCR, we use the OCR function MATLAB OCR, which works with the engine Google Tesseract. The engine is based on the approach of a revolutionary neural network for character recognition. Tesseract is considered one of the most accurate OCR engines [28]."}, {"heading": "3.3 angle-of-view (AOV) estimation", "text": "We recognize the line segments in the query image and use them to estimate the horizontal vanishing point (VP). Next, we derive a new formula for AOV estimation using the detected vanishing point Fig.5Fig.2. Block diagram of the proposed localization system (OCRAPOSE II) Fig. 3rd edition of OCR"}, {"heading": "3.3.1 Vanishing point detection", "text": "The question of the way in which the images are presented in the way in which they are presented, how the images are presented in the way in which they are presented, how they are presented in the way in which they are presented, how they are presented in the way in which they are presented, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the manner, how they are presented in the manner, how they are presented in the way, how they are presented in the manner, how they are presented in the manner, how they are presented in the way, how they are presented in the way, how they are presented in the manner, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they are presented in the way, how they"}, {"heading": "3.3.2 Estimating AOV from the horizontal vanishing point", "text": "In this section, we propose a novel formula for estimating the AOV in the practical scenario presented in Fig. 5. We are using only the OCR field surrounding the figures shown in Fig. 6. The approximate formula derived for estimating the AOV formula is in practice independent of the dimensions of the OCR field. It is only the depth calculation formula that requires the actual width of the field. What we need to do is to measure only the width (W) of the OCR formula. Since this value is the same with a high probability for the text / number plates in a single building, we need to perform the measurement only once. If the character boxes are of different sizes in an environment, it is possible to tabulate the dimensions and use them once."}, {"heading": "3.4 Depth estimation", "text": "We propose a new practical depth assessment formula that uses the actual and image width of the OCR box."}, {"heading": "3.4.1 Depth estimation using width of OCR box", "text": "It is obvious that when the user is further away from the center of the image, the width of the OCR field in the image of the query becomes smaller. Here, we formulate this relation.9 Thesis 3. When the user measures the viewing angle and the actual and image width of the OCR field, the estimated characters are W and W, respectively, the depth of the user can be calculated asd = 12 cos W w (1 + \u221a 1 + w2 tan2 \u03b8), where the query AOV.Proof 4. Consider the OCR field in Figure 6. To estimate the AOV, lower and upper horizontal row segments of this field are examined. Here, we calculate the box width (w) and show that it is a function of depth, AOV and W. Similar to the derivation of the viewing angle, we only use the corner coordinates to derive the depth formula. Actually, it is not necessary to recognize the corners of the actual quantum phase."}, {"heading": "3.5 Location estimation", "text": "Once depth and AOV are calculated, the X and Z coordinates of the user location for the scenario shown in Fig. 5 are obtained as"}, {"heading": "X = d sin \u03b8", "text": "Z = d cos \u03b8 (30) The y-coordinate is of no interest for 2D localization in indoor applications, while it is indispensable for other applications such as augmented reality (AR)."}, {"heading": "4 NON-ZERO TILT ANGLE EFFECT ON AOV AND DEPTH ESTIMATION ERROR", "text": "In the derivation of xhor and w formulas, we assume that the angle of inclination (\u03c6) is zero. To examine the effects of the zero angle in practice, we derive the formulas for the angle of inclination from 0 to 1.5 percent if the actual error magnitude (s1 + s2 s3 + s4) (31) 10andw = {x2 \u2212 x1, \u03c6 > 0 x3 \u2212 x4, \u03c6 = 4d2W cos2 = s6 + s7 | (32), where the actual error magnitude (s1 = 8d 3 cos2 + 2dH2 sin2 \u2212 2dW 2 cos2), the absolute error magnitude (s2 = 4d 2H sin 2p + 4d2W cos2), is the actual error depth (32)."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "The localization performance of the proposed system is compared to the state-of-the-art works of Liang's [10] and Toriis [21]. We compare the methods in terms of detection error rate and location estimation. In Liang's method, the detection error is defined as the percentage of incorrectly captured images as the best match. In our location detection problem, the incorrect best match is a database image that does not contain the same characters as the query. In Torii's method, the detection error rate is meaningless as there is no image evaluation phase and the best image pair is selected from the database images. In our method, i.e. OCRAPOSE II, the incorrect detection of one or more characters is incorrect. In conventional buildings, the incorrect detection of a single character can lead to an incorrect position right next to the query, and the best image pair is selected from the database images."}, {"heading": "5.1 Scenario I - University building", "text": "In this case, it is as if it were a purely reactionary project."}, {"heading": "5.2 Scenario II - Parking", "text": "The actual width (W) of the word is 95.4 cm. Fig. 20 shows sample images taken in this scenario. The images are taken from three angles of 0, + 45 and \u2212 45 \u043c. The depths are part of the interval of [1, 40] m. Table 3 shows a summary of the results of the location estimation error. As can be seen, the OCRAPOSE II performs better in terms of location accuracy with a large gap. It is due to the presence of a larger OCR box in this scenario compared to previous ones. It results in lower relative errors in the estimation of the box width. Fig. 21 compares the results of the localization error with the results of the OCRAPOSE II. As can be seen, OCRAPOSE II exhibits smaller average localization errors. In addition, Fig. 22 shows the effect of database locations reducing the size of the box. Fig."}, {"heading": "6 CONCLUSION", "text": "In this paper, we discuss a number of indoor scenarios that challenge existing localization methods, and propose identifying characters using OCR as suitable site-specific landmarks; a novel system that uses OCR for rough localization; two new formulas for perspective and depth estimation that are used to refine location estimation; and our experiments show that the proposed OCR-based system performs better than modern localization methods in terms of location recognition rate and average localization error; and that the performance of benchmarks deteriorates when defined database locations become sparse, while the performance of the proposed system is independent of database sparseness and remains constant."}], "references": [{"title": "Tracking mobile users in wireless networks via semi-supervised colocalization", "author": ["J. Pan", "S. Pan", "J. Yin", "L. Ni", "Q. Yang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 34, no. 3, pp. 587\u2013600, March 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressive sensing based positioning using rss of wlan access points", "author": ["C. Feng", "W.S.A. Au", "S. Valaee", "Z. Tan"], "venue": "INFOCOM, 2010 Proceedings IEEE. IEEE, 2010, pp. 1\u20139.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Received-signal-strength-based indoor positioning using compressive sensing", "author": ["\u2014\u2014"], "venue": "Mobile Computing, IEEE Transactions on, vol. 11, no. 12, pp. 1983\u20131993, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1983}, {"title": "Monoslam: Realtime single camera slam", "author": ["A. Davison", "I. Reid", "N. Molton", "O. Stasse"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 6, pp. 1052\u20131067, June 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Autonomous visual self-localization in completely unknown environment", "author": ["P. Sadeghi-Tehran", "S. Behera", "P. Angelov", "J. Andreu"], "venue": "Evolving and Adaptive Intelligent Systems (EAIS), 2012 IEEE Conference on, May 2012, pp. 90\u201395.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Location-based augmented reality on mobile phones", "author": ["R. Paucher", "M. Turk"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on. IEEE, 2010, pp. 9\u201316.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Going out: robust model-based tracking for outdoor augmented reality", "author": ["G. Reitmayr", "T. Drummond"], "venue": "Mixed and Augmented Reality, 2006. ISMAR 2006. IEEE/ACM International Symposium on, Oct 2006, pp. 109\u2013118.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Sextant: Towards ubiquitous indoor localization service by photo-taking of the environment", "author": ["R. Gao", "Y. Tian", "F. Ye", "G. Luo", "K. Bian", "Y. Wang", "T. Wang", "X. Li"], "venue": "Mobile Computing, IEEE Transactions on, vol. PP, no. 99, pp. 1\u20131, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Fiducial marker indoor localization with artificial neural network", "author": ["G. Kim", "E. Petriu"], "venue": "Advanced Intelligent Mechatronics (AIM), 2010 IEEE/ASME International Conference on, July 2010, pp. 961\u2013966.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Image based localization in indoor environments", "author": ["J.Z. Liang", "N. Corso", "E. Turner", "A. Zakhor"], "venue": "Computing for Geospatial Research and Application (COM. Geo), 2013 Fourth International Conference on. IEEE, 2013, pp. 70\u201375.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A weighted knn epipolar geometry-based approach for vision-based indoor localization using smartphone cameras", "author": ["H. Sadeghi", "S. Valaee", "S. Shirani"], "venue": "Sensor Array and Multichannel Signal Processing Workshop (SAM), 2014 IEEE 8th. IEEE, 2014, pp. 37\u201340.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fiducial marker indoor localization with artificial neural network", "author": ["G. Kim", "E. Petriu"], "venue": "Advanced Intelligent Mechatronics (AIM), 2010 IEEE/ASME International Conference on, July 2010, pp. 961\u2013966.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised logo-based indoor localization using smartphone cameras", "author": ["H. Sadeghi", "S. Valaee", "S. Shirani"], "venue": "IEEE PIMRC, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust wide-baseline stereo from maximally stable extremal regions", "author": ["J. Matas", "O. Chum", "M. Urban", "T. Pajdla"], "venue": "Image and vision computing, vol. 22, no. 10, pp. 761\u2013767, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Wide baseline stereo matching", "author": ["P. Pritchett", "A. Zisserman"], "venue": "Computer Vision, 1998. Sixth International Conference on, Jan 1998, pp. 754\u2013760.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Asift: A new framework for fully affine invariant image comparison", "author": ["J.-M. Morel", "G. Yu"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 2, pp. 438\u2013469, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Asift: an algorithm for fully affine invariant comparison", "author": ["G. Yu", "J.-M. Morel"], "venue": "Image Processing On Line, vol. 2011, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning visibility of landmarks for vision-based localization", "author": ["P. Alcantarilla", "S.M. Oh", "G. Mariottini", "L. Bergasa", "F. Dellaert"], "venue": "Robotics and Automation (ICRA), 2010 IEEE International Conference on, May 2010, pp. 4881\u20134888.  14", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Recognition of multiple characters in a scene image using arrangement of local features", "author": ["M. Iwamura", "T. Kobayashi", "K. Kise"], "venue": "Document Analysis and Recognition (ICDAR), 2011 International Conference on, Sept 2011, pp. 1409\u20131413.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Ocrapose: An indoor positioning system using smartphone/tablet cameras and ocr-aided stereo feature matching", "author": ["H. Sadeghi", "S. Valaee", "S. Shirani"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, April 2015, pp. 1473\u20131477.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual localization by linear combination of image descriptors", "author": ["A. Torii", "J. Sivic", "T. Pajdla"], "venue": "Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, Nov 2011, pp. 102\u2013109.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Contextdependent logo matching and recognition", "author": ["H. Sahbi", "L. Ballan", "G. Serra", "A. Del Bimbo"], "venue": "Image Processing, IEEE Transactions on, vol. 22, no. 3, pp. 1018\u20131031, March 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Logo recognition and localization in real-world images by using visual patterns", "author": ["W.-T. Chu", "T.-C. Lin"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, March 2012, pp. 973\u2013976.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "On-body multi-input indoor localization for dynamic emergency scenarios: fusion of magnetic tracking and optical character recognition with mixed-reality display", "author": ["J. Orlosky", "T. Toyama", "D. Sonntag", "A. Sarkany", "A. Lorincz"], "venue": "Pervasive Computing and Communications Workshops (PERCOM Workshops), 2014 IEEE International Conference on. IEEE, 2014, pp. 320\u2013325.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "All about vlad", "author": ["R. Arandjelovic", "A. Zisserman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, June 2013, pp. 1578\u20131585.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Scene text detection via connected component clustering and nontext filtering", "author": ["H.I. Koo", "D.H. Kim"], "venue": "Image Processing, IEEE Transactions on, vol. 22, no. 6, pp. 2296\u20132305, June 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust text detection in natural images with edgeenhanced maximally stable extremal regions", "author": ["H. Chen", "S.S. Tsai", "G. Schroth", "D.M. Chen", "R. Grzeszczuk", "B. Girod"], "venue": "Image Processing (ICIP), 2011 18th IEEE International Conference on. IEEE, 2011, pp. 2609\u20132612.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "An overview of the tesseract ocr engine.", "author": ["R. Smith"], "venue": "in ICDAR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Camera parameters estimation in soccer scenes on the basis of points at infinity", "author": ["V.B. Kashany", "H. Pourreza"], "venue": "IET computer vision, vol. 6, no. 2, pp. 133\u2013139, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Single view pose estimation of mobile devices in urban environments", "author": ["A. Hallquist", "A. Zakhor"], "venue": "Applications of Computer Vision (WACV), 2013 IEEE Workshop on. IEEE, 2013, pp. 347\u2013354.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Lsd: A fast line segment detector with a false detection control", "author": ["R.G. Von Gioi", "J. Jakubowicz", "J.-M. Morel", "G. Randall"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 4, pp. 722\u2013732, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Communications of the ACM, vol. 24, no. 6, pp. 381\u2013395, 1981.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1981}, {"title": "Minimal solution for uncalibrated absolute pose problem with a known vanishing point", "author": ["B. Micusik", "H. Wildenauer"], "venue": "3D Vision - 3DV 2013, 2013 International Conference on, June 2013, pp. 143\u2013150.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org/, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3304\u20133311.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "THE most prevalent method for indoor localization is based on fusion of Wi-Fi RSS fingerprints with inertial sensors data [1] .", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "These methods require a fair number of Wi-Fi access points to be visible at each location [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "Under these conditions, they demonstrate localization errors about 2 meters, where training points have granularity of 1 meter [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 218, "endOffset": 221}, {"referenceID": 8, "context": "The proposed methods can be categorized into two classes [9], image retrieval-based (fingerprinting-based) [10], [11] and landmark-based (e.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "The proposed methods can be categorized into two classes [9], image retrieval-based (fingerprinting-based) [10], [11] and landmark-based (e.", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "The proposed methods can be categorized into two classes [9], image retrieval-based (fingerprinting-based) [10], [11] and landmark-based (e.", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "logo-based) [12], [13].", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "logo-based) [12], [13].", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "Furthermore, most of the methods proposed in the literature, utilize feature extraction and matching for localization [10], [13], while feature extraction and corresponding database creation for a large environment is highly time consuming.", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "Furthermore, most of the methods proposed in the literature, utilize feature extraction and matching for localization [10], [13], while feature extraction and corresponding database creation for a large environment is highly time consuming.", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "set of objects) present in two images cannot be acceptably matched if the difference in AOV is large [14], [15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "set of objects) present in two images cannot be acceptably matched if the difference in AOV is large [14], [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "Methods such as ASIFT (affine SIFT) [16], which are", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "5 times of that of SIFT [17].", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "The reason of failure in detecting the correct existing characters using the landmark-based methods is that the characters (numbers) are not as textured as commonly used landmarks such as commercial logos or fiducial markers [12].", "startOffset": 225, "endOffset": 229}, {"referenceID": 17, "context": "Hence, point feature-based recognition approaches [18] fail to extract enough distinctive features required to distinguish different numbers from each other.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "Other image retrieval techniques such as bag of features or localization-specific ones [10] might also fail to retrieve the correct matching characters.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "We evaluate the recognition performance of the Liang\u2019s method [10] as an example in our experiments.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Use of point feature-based techniques proposed originally for character recognition such as [19] will result in an OCR-based localization method, which resides in the same category as our method.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "In [20], we demonstrated how OCR can improve a landmark-based localization system in terms of location recognition and localization accuracy.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "These methods perform very well in applications with abundant distinctive landmarks such as shopping malls [10] or scenarios with unrepeated scenery such as outdoors (using Google street view database) [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "These methods perform very well in applications with abundant distinctive landmarks such as shopping malls [10] or scenarios with unrepeated scenery such as outdoors (using Google street view database) [21].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "Moreover, a large database of images is required to be collected and Geo-tagged (posetagged [11]) in the training phase.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Furthermore, in the test phase, the query image is compared with the entire (or part of) database to find the best match [10] or best pair [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 20, "context": "Furthermore, in the test phase, the query image is compared with the entire (or part of) database to find the best match [10] or best pair [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 9, "context": "Landmark-based methods are applicable in scenarios, where highly textured and distinctive landmarks are present [10], [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Landmark-based methods are applicable in scenarios, where highly textured and distinctive landmarks are present [10], [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "These methods use logo/landmark detection techniques [22], [23] to detect existing landmarks/logos in the query image.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "These methods use logo/landmark detection techniques [22], [23] to detect existing landmarks/logos in the query image.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "For instance, in shopping malls, where textured commercial logos are ubiquitous, these methods can provide great localization accuracy [10].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "Among a few works proposed in the literature that utilize OCR for localization, [24] can be mentioned for its explicit use of OCR as a rough localizer.", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "A good indoor localization system that we use as one of our benchmarks is proposed in [10] that uses CBIR (contentbased image retrieval) to perform rough localization.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "The second benchmark is the method proposed in [21], which proposes a feature-based method for fine localization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "For image similarity computation, the bag of features representation is used in [21].", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "In this paper, we use the modified VLAD\u2019s representation [25], which has demonstrated supreme performance.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "In [20], we proposed an OCR-aided localization system called OCRAPOSE.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "Text detection in natural scene images is an extremely difficult problem [26], [27] and might need parameters fine tuning in the scenario under investigation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "Text detection in natural scene images is an extremely difficult problem [26], [27] and might need parameters fine tuning in the scenario under investigation.", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Such regions are good candidates to be detected as maximally stable extremal regions (MSER) [14] as also suggested by [27].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "Such regions are good candidates to be detected as maximally stable extremal regions (MSER) [14] as also suggested by [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "In works such as [27], this Geometric filtering is carried out after processing the MSER regions.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "MSER region detector is known to be sensitive to image blur [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "Hence, we enhance the MSER regions using [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "[27] uses Canny edge detector to enhance the outline of detected extremal regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Tesseract is believed to be one of the most accurate OCR engines [28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "Vanishing points present in the image contain information about query camera rotation with respect to the seen objects [29].", "startOffset": 119, "endOffset": 123}, {"referenceID": 29, "context": "Hence, we detect it by solving a novel robust optimization problem partly inspired by [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "In [30], line segments are detected using an edge-based method proposed in [31].", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [30], line segments are detected using an edge-based method proposed in [31].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "In order to do this, we propose a novel robust optimization problem inspired by [30] using RANSAC [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "In order to do this, we propose a novel robust optimization problem inspired by [30] using RANSAC [32].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "As stated in [33], if the vertical vanishing point (i.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "As mentioned in [10], in indoor images, most of the visible objects are located on a single wall.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "The localization performance of the proposed system is compared with the state-of-the-art works of Liang\u2019s [10] and Torii\u2019s [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "The localization performance of the proposed system is compared with the state-of-the-art works of Liang\u2019s [10] and Torii\u2019s [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 33, "context": "Feature extraction and processing needed for the benchmarks is performed using the codes available at [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 24, "context": "In Torii\u2019s method, we use the modified VLAD decriptors [25] for image description to obtain high recognition performance.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "The modified VLAD descriptor [35] is an improved version of the descriptor suggested in [35] that has demonstrated supreme performance compared to the tf-idf methods such as the one used in [21].", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "The modified VLAD descriptor [35] is an improved version of the descriptor suggested in [35] that has demonstrated supreme performance compared to the tf-idf methods such as the one used in [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "The modified VLAD descriptor [35] is an improved version of the descriptor suggested in [35] that has demonstrated supreme performance compared to the tf-idf methods such as the one used in [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "Depths belong to the interval of [1, 40] m.", "startOffset": 33, "endOffset": 40}], "year": 2017, "abstractText": "In this paper, we propose an OCR (optical character recognition)-based localization system called OCRAPOSE II, which is applicable in a number of indoor scenarios including office buildings, parkings, airports, grocery stores, etc. In these scenarios, characters (i.e. texts or numbers) can be used as suitable distinctive landmarks for localization. The proposed system takes advantage of OCR to read these characters in the query still images and provides a rough location estimate using a floor plan. Then, it finds depth and angle-of-view of the query using the information provided by the OCR engine in order to refine the location estimate. We derive novel formulas for the query angle-of-view and depth estimation using image line segments and the OCR box information. We demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios. It is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition.", "creator": "LaTeX with hyperref package"}}}