{"id": "1704.05596", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Stochastic Gradient Twin Support Vector Machine for Large Scale Problems", "abstract": "For classification problems, twin support vector machine (TSVM) with nonparallel hyperplanes has been shown to be more powerful than support vector machine (SVM). However, it is time consuming and insufficient memory to deal with large scale problems due to calculating the inverse of matrices. In this paper, we propose an efficient stochastic gradient twin support vector machine (SGTSVM) based on stochastic gradient descent algorithm (SGD). As far as now, it is the first time that SGD is applied to TSVM though there have been some variants where SGD was applied to SVM (SGSVM). Compared with SGSVM, our SGTSVM is more stable, and its convergence is also proved. In addition, its simple nonlinear version is also presented. Experimental results on several benchmark and large scale datasets have shown that the performance of our SGTSVM is comparable to the current classifiers with a very fast learning speed.", "histories": [["v1", "Wed, 19 Apr 2017 03:08:38 GMT  (73kb)", "http://arxiv.org/abs/1704.05596v1", "26 pages, 46 figures except 3 oversized figures"]], "COMMENTS": "26 pages, 46 figures except 3 oversized figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["zhen wang", "yuan-hai shao", "lan bai", "li-ming liu", "nai-yang deng"], "accepted": false, "id": "1704.05596"}, "pdf": {"name": "1704.05596.pdf", "metadata": {"source": "CRF", "title": "Stochastic Gradient Twin Support Vector Machine for Large Scale Problems", "authors": ["Zhen Wang", "Yuan-Hai Shao", "Lan Bai", "Li-Ming Liu", "Nai-Yang Deng"], "emails": ["wangzhen@imu.edu.cn.", "shaoyuanhai21@163.com.", "bailanhaomei@163.com.", "llm5609@163.com"], "sections": [{"heading": null, "text": "However, it is time-consuming and insufficient to deal with major problems caused by the calculation of the reversal of matrices. In this paper, however, we propose an efficient method for the SGSM two different types of vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}], "references": [{"title": "Support vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, pp. 273\u2013297, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "The new interpretation of support vector machines on statistical learning theory", "author": ["C. Zhang", "Y. Tian", "N. Deng"], "venue": "Science China, vol. 53, no. 1, pp. 151\u2013164, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Support vector machine for regression and applications to financial forecasting", "author": ["H. Ince", "T. Trafalis"], "venue": "International Joint Conference on Neural Networks, Italy, 2002, pp. 6348\u20136354.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Twin support vector machines for pattern classification", "author": ["Jayadeva", "R. Khemchandani", "S. Chandra"], "venue": "IEEE Trans.PatternAnal. Machine Intell, vol. 29, no. 5, pp. 905\u2013910, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Improvements on twin support vector machines", "author": ["Y. Shao", "C. Zhang", "X. Wang", "N. Deng"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 6, pp. 962 \u2013 968, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "TPMSVM: A novel twin parametric-margin support vector machine for pattern recognition", "author": ["X. Peng"], "venue": "Pattern Recognition, vol. 44, no. 10-11, pp. 2678\u20132692, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient weighted lagrangian twin support vector machine for imbalanced data classification", "author": ["Y. Shao", "W. Chen", "J. Zhang", "Z. Wang", "N. Deng"], "venue": "Pattern Recognition, vol. 47, no. 9, pp. 3158\u2013 3167, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A coordinate descent margin basedtwin support vector machine for classification", "author": ["Y. Shao", "N. Deng"], "venue": "Neural Networks, vol. 25, pp. 114\u2013121, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A ga-based model selection for smooth twin parametric-margin support vector machine", "author": ["Z. Wang", "Y. Shao", "T. Wu"], "venue": "Pattern Recognition, vol. 46, no. 8, pp. 2267\u20132277, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep twin support vector machine", "author": ["D. Li", "Y. Tian", "H. Xu"], "venue": "Data MiningWorkshop (ICDMW), 2014 IEEE International Conference on. IEEE, 2014, pp. 65\u201373.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Twin support vector machine for clustering", "author": ["Z. Wang", "Y. Shao", "L. Bai", "N. Deng"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 10, pp. 2583\u20132588, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Mltsvm: A novel twin support vector machine to multi-label learning", "author": ["W. Chen", "Y. Shao", "C. Li", "N. Deng"], "venue": "Pattern Recognition, vol. 52, pp. 61\u201374, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear Programming\u0142Theory and Algorithms, second ed", "author": ["M. Bazarra", "H. Sherali", "C. Shetty"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J. Platt"], "venue": "Advances in kernel methods-support vector learning, Cambridge, MA: MIT Press, 1999, pp. 185\u2013208.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods-Support Vector Learning, Cambridge, 1998, pp. 169\u2013184.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "LIBSVM: A library for support vector machines, http://www.csie.ntu.edu.tw/\u223ccjlin", "author": ["C. Chang", "C. Lin"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "LIBLINEAR: a library for large linear classification", "author": ["R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1871\u20131874, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1871}, {"title": "Large-scale linear nonparallel support vector machine solver", "author": ["Y. Tian", "Y. Ping"], "venue": "Neural Networks, vol. 50, pp. 166\u2013174, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "Signal Processing, IEEE Transactions on, vol. 52, no. 8, pp. 2165\u20132176, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the twenty-first international conference onMachine learning. ACM, 2004, p. 116.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm,\u201dMathematical", "author": ["S. Shai", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "programming, vol. 127,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["W. Xu"], "venue": "arXiv preprint arXiv:1107.2490, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Almost sure convergence of a stochastic approximation process in a convex set", "author": ["A. Bennar", "J. Monnez"], "venue": "International Journal of Applied Mathematics, vol. 20, no. 5, pp. 713\u2013722, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Multisurface proximal support vector classification via generalize eigenvalues", "author": ["O. Mangasarian", "E. Wild"], "venue": "IEEE Trans.PatternAnal. Machine Intell, vol. 28, no. 1, pp. 69\u201374, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning with rigorous support vector machines", "author": ["J. Bi", "V. Vapnik"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Learning with kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Optimal kernel selection in twin support vector machines", "author": ["R. Khemchandani", "Jayadeva", "S. Chandra"], "venue": "Optimization Letters, vol. 3, pp. 77\u201388, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "RSVM: Reduced support vector machines", "author": ["Y. Lee", "O. Mangasarian"], "venue": "First SIAM International Conference on Data Mining, Chicago, IL, USA, 2001, pp. 5\u20137.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Proximal parametric-margin support vector classifier and its applications", "author": ["Z. Wang", "Y. Shao", "T. Wu"], "venue": "Neural Computing and Applications, vol. 24, no. 3-4, pp. 755\u2013764, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix Computations", "author": ["G. Golub", "L. Van"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1996}, {"title": "Principles of mathematical analysis", "author": ["W. Rudin"], "venue": "McGraw-Hill New York,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1964}, {"title": "On the generalization ability of online strongly convex programming algorithms", "author": ["S. Kakade", "A. Tewari"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 801\u2013808.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Least squares support vector machine classifiers", "author": ["J. Suykens", "J. Vandewalle"], "venue": "Neural Process Letter, vol. 9, no. 3, pp. 293\u2013 300, 1999.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "Pattern Classification, 2nd Edition", "author": ["R. Duda", "P. Hart", "D. Stork"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION Support vector machine (SVM), being powerful tool for classification [1], [2], [3], has already outperformed most other classifiers in a wide variety of applications [4], [5], [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "1 INTRODUCTION Support vector machine (SVM), being powerful tool for classification [1], [2], [3], has already outperformed most other classifiers in a wide variety of applications [4], [5], [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "1 INTRODUCTION Support vector machine (SVM), being powerful tool for classification [1], [2], [3], has already outperformed most other classifiers in a wide variety of applications [4], [5], [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 3, "context": "Different from SVM with a pair of parallel hyperplanes, twin support vector machine (TSVM) [7] with a pair of nonparallel hyperplanes has been proposed and developed, e.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": ", twin bounded support vector machine (TBSVM) [8], twin parametric margin support vector machine (TPMSVM) [9], and weighted Lagrangian twin support vector machine (WLTSVM) [10].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": ", twin bounded support vector machine (TBSVM) [8], twin parametric margin support vector machine (TPMSVM) [9], and weighted Lagrangian twin support vector machine (WLTSVM) [10].", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": ", twin bounded support vector machine (TBSVM) [8], twin parametric margin support vector machine (TPMSVM) [9], and weighted Lagrangian twin support vector machine (WLTSVM) [10].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "In the training stage, SVM solves a quadratic programming problem (QPP), whereas TSVM solve two smaller QPPs by traditional solver such as interior method [2], [16], [7].", "startOffset": 160, "endOffset": 164}, {"referenceID": 3, "context": "In the training stage, SVM solves a quadratic programming problem (QPP), whereas TSVM solve two smaller QPPs by traditional solver such as interior method [2], [16], [7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 13, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 234, "endOffset": 237}, {"referenceID": 8, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 239, "endOffset": 243}, {"referenceID": 17, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 245, "endOffset": 249}, {"referenceID": 18, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "It has been proved that SGSVM is almost sure convergent, and thus is able to find an approximation of the desired solution with high probability [26], [23], [24].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "It has been proved that SGSVM is almost sure convergent, and thus is able to find an approximation of the desired solution with high probability [26], [23], [24].", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "It has been proved that SGSVM is almost sure convergent, and thus is able to find an approximation of the desired solution with high probability [26], [23], [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "Compared with SVM, it is significant that TSVM is more stable for sampling and does not strongly depend on some special samples such as the SVs [7], [8], which indicates SGD is more suitable for TSVM.", "startOffset": 144, "endOffset": 147}, {"referenceID": 4, "context": "Compared with SVM, it is significant that TSVM is more stable for sampling and does not strongly depend on some special samples such as the SVs [7], [8], which indicates SGD is more suitable for TSVM.", "startOffset": 149, "endOffset": 152}, {"referenceID": 23, "context": ", \u201ccross plane\u201d dataset [27] and preferential classification [7].", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": ", \u201ccross plane\u201d dataset [27] and preferential classification [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 20, "context": "The main contributions of this paper includes: (i) a SGD-based TSVM (SGTSVM) is first proposed, and it is very easy to be extended to other TSVM-type classifiers; (ii) we prove that the proposed SGTSVM is convergent, instead of almost sure convergence in SGSVM; (iii) for the uniformly sampling, it is proved that the original objective of the solution to SGTSVM is bounded by the optimum of TBSVM, which indicates the solution to SGTSVM is an approximation of the optimal solution to TBSVM, while SGSVM only has an opportunity to obtain an approximation of the optimal solution to SVM (see Corollaries 1 and 2 in [24]); (iv) the nonlinear case of SGTSVM is obtained directly based on its original problem, whereas the nonlinear case of SGSVM is derived from SVM\u2019s dual problem; (v) each iteration of SGTSVM includes no more than 8n + 4 multiplications without additional storage, so it is the fastest one than other proposed TSVM-type classifiers.", "startOffset": 614, "endOffset": 618}, {"referenceID": 24, "context": "C-support vector machine (CSVM) [28], one formulation of the standard SVM, searches for a separating hyperplane", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "where || \u00b7 || denotes the L2 norm, c > 0 is a parameter with some quantitative meanings [28], e is a vector of ones with an appropriate dimension, \u03be \u2208 R is the slack vector, and D = diag(y1, .", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "And the structural risk minimization principle is implemented in this problem [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "2 SGSVM SGSVM (or PEGASOS as an alias) [23], [24] considers a strongly convex problem by modifying (2) as follow", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "2 SGSVM SGSVM (or PEGASOS as an alias) [23], [24] considers a strongly convex problem by modifying (2) as follow", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "t=1 wt is bounded by the optimal solution w \u2217 to (4) with o(1), and thus SGSVM has with a probability of at least 1/2 to find a good approximation of w [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 20, "context": "The authors of [24] also pointed out that wT is often used instead of w\u0304 in practice.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "The sample xt which is selected randomly can be replaced with a small subset belonging to the whole dataset, and the subset only including a sample is often used in practice [23], [24], [25].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "The sample xt which is selected randomly can be replaced with a small subset belonging to the whole dataset, and the subset only including a sample is often used in practice [23], [24], [25].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "The sample xt which is selected randomly can be replaced with a small subset belonging to the whole dataset, and the subset only including a sample is often used in practice [23], [24], [25].", "startOffset": 186, "endOffset": 190}, {"referenceID": 20, "context": "(8) However, this modification would lead to the function not to be strongly convex and thus yield a slow convergence rate [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 4, "context": "3 TBSVM TBSVM [8], a representative of TSVM, seeks a pair of nonparallel hyperplanes in R which can be expressed as w 1 x+ b1 = 0 and w \u22a4 2 x+ b2 = 0, (9)", "startOffset": 14, "endOffset": 17}, {"referenceID": 23, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 97, "endOffset": 100}, {"referenceID": 25, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 102, "endOffset": 106}, {"referenceID": 26, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 27, "context": "However, the reduced kernel strategy, which has been successfully applied for SVM and TSVM [31], [32], [12], can also be applied for our SGTSVM.", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "However, the reduced kernel strategy, which has been successfully applied for SVM and TSVM [31], [32], [12], can also be applied for our SGTSVM.", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "However, the reduced kernel strategy, which has been successfully applied for SVM and TSVM [31], [32], [12], can also be applied for our SGTSVM.", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "In practice, X\u0303 just needs 1% \u2212 10% samples from X to get a well performance, reducing the learning time without loss of generalization [32].", "startOffset": 136, "endOffset": 140}, {"referenceID": 29, "context": "(31) For i \u2265 N + 1, ||At+N+1\u2212iuN+1|| \u2264 \u03bbi||uN+1|| \u2264 i\u22121 i ||uN+1|| [33].", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "Note that an infinite series of vectors is convergent if its norm series is convergent [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "However, Kakade and Tewari [35] have shown a way to obtain a similar bounds with high probability.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 67, "endOffset": 70}, {"referenceID": 32, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 90, "endOffset": 94}, {"referenceID": 4, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "CSVM was implemented by Libsvm [19] based on SMO algorithm on small sample size datasets (i.", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": ", m \u2264 10, 000), while Liblinear [20] was implemented for CSVM based on trust region algorithm on large scale datasets (i.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "TBSVM was solved by SOR algorithm [8].", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "1 Artificial datasets We first test our SGTSVM compared with TBSVM on two artificial datasets [9], [27] in R (see Figures ?? and ??).", "startOffset": 94, "endOffset": 97}, {"referenceID": 23, "context": "1 Artificial datasets We first test our SGTSVM compared with TBSVM on two artificial datasets [9], [27] in R (see Figures ?? and ??).", "startOffset": 99, "endOffset": 103}, {"referenceID": 33, "context": "For the optimal parameters, we depicted the iteration, accuracy (by ten-fold cross validation [39]), and learning time along with the parameter tol in Figures 3 and 4 for linear and nonlinear cases, respectively.", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "3 Large scale datasets To test the feasibility of these methods on large scale datasets, we ran them on three large scale datasets: \u201cCODRNA\u201d, \u201cSKIN\u201d, and \u201cSUSY\u201d [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "Moreover, since LSSVM, TBSVM, and WLTSVM are out of memory on all of these datasets, the comparisons only include CSVM, SGSVM, and our SGTSVM, where CSVM is implemented by Liblinear [20].", "startOffset": 182, "endOffset": 186}], "year": 2017, "abstractText": "For classification problems, twin support vector machine (TSVM) with nonparallel hyperplanes has been shown to be more powerful than support vector machine (SVM). However, it is time consuming and insufficient memory to deal with large scale problems due to calculating the inverse of matrices. In this paper, we propose an efficient stochastic gradient twin support vector machine (SGTSVM) based on stochastic gradient descent algorithm (SGD). As far as now, it is the first time that SGD is applied to TSVM though there have been some variants where SGD was applied to SVM (SGSVM). Compared with SGSVM, our SGTSVM is more stable, and its convergence is also proved. In addition, its simple nonlinear version is also presented. Experimental results on several benchmark and large scale datasets have shown that the performance of our SGTSVM is comparable to the current classifiers with a very fast learning speed.", "creator": "LaTeX with hyperref package"}}}