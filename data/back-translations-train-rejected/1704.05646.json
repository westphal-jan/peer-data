{"id": "1704.05646", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Effects of the optimisation of the margin distribution on generalisation in deep architectures", "abstract": "Despite being so vital to success of Support Vector Machines, the principle of separating margin maximisation is not used in deep learning. We show that minimisation of margin variance and not maximisation of the margin is more suitable for improving generalisation in deep architectures. We propose the Halfway loss function that minimises the Normalised Margin Variance (NMV) at the output of a deep learning models and evaluate its performance against the Softmax Cross-Entropy loss on the MNIST, smallNORB and CIFAR-10 datasets.", "histories": [["v1", "Wed, 19 Apr 2017 08:31:20 GMT  (185kb,D)", "http://arxiv.org/abs/1704.05646v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lech szymanski", "brendan mccane", "wei gao", "zhi-hua zhou"], "accepted": false, "id": "1704.05646"}, "pdf": {"name": "1704.05646.pdf", "metadata": {"source": "CRF", "title": "Effects of the optimisation of the margin distribution on generalisation in deep architectures", "authors": ["Lech Szymanski", "Brendan McCane"], "emails": ["lechszym@cs.otago.ac.nz", "mccane@cs.otago.ac.nz", "gaow@lamda.nju.edu.cn", "zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "We show that minimizing margin variance, not maximizing margin, is better suited to improving generalization in deep architectures. We propose the halfway loss feature, which minimizes normalized margin variance (NMV) when issuing a deep learning model, and evaluate its performance versus Softmax cross-entropy loss on the MNIST, smallNORB, and CIFAR-10 datasets."}, {"heading": "1 Introduction", "text": "Support Vector Machines (SVM) guarantee the best generalization in a classification task for a selected function-extraction function (Cortes & Vapnik, 1995; Vapnik, 1995) While the question of the choice of suitable functionality (or its parameters) remains open, the training will guarantee the optimal answer for the choice made. This assurance of generalization stems from the principle of maximizing the margin. Upgrading methods build a trait space during the training process from a combination of weak classifiers (Schapire, 1990). However, it has been shown that their resistance to overadjustment is due to the effect that these methods have on the distribution of points around the margin (Schapire et al., 1998; Reyzin & Schapire, 2006; Wang et al., 2011). Gao & Zhou (2013) demonstrated that their resistance to overadjustment to overadjustment to the margin-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge-Marge is theoretically due to the effect that they have on the distribution of points around the margin (Schapire et al., 1998; Reyzin & Schapire, 2006; Wang et al., 2011)."}, {"heading": "2 Previous work", "text": "Jayadeva et al. (2002) combined it with a decision-tree-based training, and Nishikawa & Abe (2002) integrated it into the CARVE algorithm (Young & Downs, 1998), both of which are based on a boosting-like training plan in which the feature space builds a neuron (hypothesis) at a given time, focusing on the remaining, misclassified subset of training data. Although these methods are in the spirit of neural networks, the increase in performance they achieve by maximizing margin probably has more to do with the promoting aspects of feature building than with the deep nature of the neural network used. The meticulously named Maximum Margin Gradient Descent with the Adaptive Learning Rate (MMGDX) algorithm, which is derived from the conventional rigid connectivity architecture with the proposed universality function."}, {"heading": "3 Margin", "text": "Suppose that we have a set S = {(x1, y1), (x2, y2),..., (xm, ym) of m points drawn identically and independently of D. Suppose that we have a function of character extraction \u03c6 (x), (x1, y1), (x2, y2),..., (xm, ym) so that we can define the difference of the instance (xi, yi), which is in reality a distance of the point from the classification boundary, such as: \u03b3i = yi [wT\u03c6 (xi) + b]. (1) Given the classification error as (\u03b3i) = {0 \u03b3i > 01 otherwise, the goal of binary classification is to seek the direction of the unity vector and the value of the expectation that the difference (1) is designated as a minimum."}, {"heading": "3.1 Margin across different feature spaces", "text": "The learning process for a given selection of \u03c6 (x) is the search for w and b that maximizes the geometric margin while providing a correct classification to the degree dictated by the choice of the margin margin parameter. However, the challenge with SVMs is to determine the best margin (x) by selecting the function of the right kernel and its parameters, as well as an appropriate soft margin factor for the likelihood of misclassification. Given our intention to apply margin theory to deep learning, we are encouraged to study the behavior of the margin in an SVM while we vary. Lanckriet et al (2004) demonstrated that, with some margin limitations and constraints (x), maximizing the margin."}, {"heading": "3.2 Margin in deep architectures", "text": "The simple experiment from the previous section suggests that maximizing the margin in architectures where \u03c6 (x) is not constant cannot lead to a better generalization; we can go even further and show that a simple linear transformation enabled by \u03c6 (x) is sufficient to generate an arbitrary margin value without changing the relative position of the points in relation to the delimiter given by w.Lemma. the mean margin of a set of points provided by the unit vector w, bias \u03b2b and a feature that extracts function (x) = margin margin (x), so that \u00b5 > 0 can be arbitrarily made large by varying the value of \u03b2 > 1.Proof. The problem is fairly obvious, since\u03b22) i = yi [wT (\u03b22) + \u03b22], which generates an average margin difference \u03b23 (x), if \u00b5 > 0 and \u03b2 > 1 means a change in the margin differentiation of the plane x."}, {"heading": "4 Margin variance", "text": "Following the theory (Gao & Zhou, 2013) and (Zhang & Zhou, 2016), we next consider the effects of minimizing the deviation of the marginal area in deep architectures. The deviation of the marginal area becomes as\u03c3 = 1m m \u2211 i = 1 (\u03b3i \u2212 \u00b5) 2. (4) In order to increase the mean deviation as shown in Figure 2a, it is sufficient that the characteristic space \u03c6 (x) changes in such a way that the points move away from the separating hyperplane defined by w. This can easily be facilitated by a linear transformation as defined in Figure 3.1. Figure 2b shows the type of transformation that \u03c6 (x) must undergo in order to reduce the deviation of the marginal area. In addition to the deviation from the separating hyperplane, space must squeeze by two separate hyperplanes on the positive and negative deviation. It is evident that it is a slightly less linear non-vibratory boundary area, and is therefore more suitable for generalization changes."}, {"heading": "4.1 Normalised margin", "text": "If our hypothesis is correct that the mean margin value for the change of \u03c6 (x) is arbitrary, it stands to reason that the variance value could also be arbitrary for different \u03c6 (x). If we repeat the experiment with SVM with respect to the two-class aspect of SmallNORB and the margin function for feature extraction defined in Equation 3, we can clearly see (Figure 3) that the minimum margin variance does not exactly correspond to the minimum test error with respect to the margin curvature of \u03c6 (x). However, it is not unreasonable to assume that the SVM formation does not seek to minimize the variance of margin variance, but rather to maximize the geometric margin variance. Considering that the margin variance for different designations (x) is not consistent, it is not unreasonable to assume that the variance will not be normal either. Therefore, we propose normalized margin variance (where NV) is defined."}, {"heading": "4.2 Halfway loss function", "text": "In order to make an empirical assessment of the effect of minimizing the normalization of variance on generalization in deep architectures, we propose the function of the half-way loss, the asJ = 1m m \u2211 i = 1 (\u03c3i \u2212 1 2) 2. (7) It is difficult not to notice the similarity of Equation 7 to the Mean Squared Error (MSE) loss function. Indeed, the MSE training seeks to minimize the variance of the production performance of the model by the value given by the target label. The point of difference between half-way loss and MSE loss is the normalization of the margin, which is ultimately the same as the normalization of the production value of the model. The motivation for normalization, as discussed in the previous section, is to achieve the consistency of the margin variance between the different margin values (x). However, one consequence of this normalization is that the optimization does not force an absolute target value for production, but rather a relative value to other outputs."}, {"heading": "4.3 Halfway loss for multi-class classification", "text": "For the multi-class classification, in which the label yi = {\u2212 1, 1} K from the label K is defined as follows, we propose a one-on-one training program with a cost-sensitive, learning-like multi-class weighting factor (Elkan, 2001) to correct the natural imbalance of the positive to negative label ratio. In the note-point dataset with an even distribution of the K classes, i.e. mK examples of each class, a certain output is trained on m K-positive and (K \u2212 1) m-K-negative labels, which would mean that negative labels receive more reduction of variance than positive. To correct this, we propose the following half-way loss for output k: Jk = 1 m \u00b2 i = 1 yki (Ki \u2212 1 2) 2, (8) where eyki = {1 yki = 11 K \u2212 1 yki = \u2212 1 yki = \u2212 1 yki = \u2212 1. The symbols and kyki weighting can be derived from the target marker as follows:"}, {"heading": "5 Empirical evaluation of Halfway loss", "text": "In fact, most of them are able to move to another world, in which they can move to another world, but in which they are not able to integrate."}, {"heading": "6 Discussion", "text": "When it comes to the larger models, FC-500-500-2000 and CNN, those trained with the Halfway Loss consistently perform better than those trained with the Softmax Cross-Entropy in terms of both the mean and standard deviation of the test error over several attempts with different starting conditions. At the same time, the Halfway Training for the small FC-128-32 network consistently performs worse (although it is only slightly worse apart from the small NORB).An intuitive explanation is that the Halfway Loss in terms of what it requires of the distribution of points in \u03c6 (x) is more limited than Softmax. While these limitations have been shown to be beneficial for generalization in representative rich models, they may get in the way of the goal of dividing classes into representative limited models. In other words, Halfway Loss can provide a better goal for classification, but a goal that is a bit more difficult to achieve in models with limited transformation dynamics."}, {"heading": "7 Conclusion", "text": "The driving hypothesis of our work was that margin maximization alone is not a viable goal for architectures where the function of feature extraction changes during optimization, but this could be the minimization of margin variance. We have provided some theoretical evidence that margin maximization in a neural network may be trivial, and we followed this up with empirical studies of the importance of margin variance. We proposed margin variance midway as a training goal that minimizes normalized margin variance. It is an MSE-like training goal with cost-sensitive learning that aims to reduce the variance by the middle of 0 to the maximum margin value (as calculated from the training dataset), and our empirical evaluation of known image sets shows that the superiority of margin variance halfway through the Softmax cross-entropy generalization could serve as evidence for possible networking, as well as neural, and evolutionary networks."}, {"heading": "Acknowledgements", "text": "Thank you for supporting NVIDIA Corporation with the donation of the Titan X GPU used for this research."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Kunal", "Tucker", "Paul", "Vanhoucke", "Vincent", "Vasudevan", "Vijay", "Vi\u00e9gas", "Fernanda", "Vinyals", "Oriol", "Warden", "Pete", "Wattenberg", "Martin", "Wicke", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": null, "citeRegEx": "Kunal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kunal et al\\.", "year": 2015}, {"title": "Prediction games and arcing algorithms", "author": ["Breiman", "Leo"], "venue": "Neural Computation,", "citeRegEx": "Breiman and Leo.,? \\Q1999\\E", "shortCiteRegEx": "Breiman and Leo.", "year": 1999}, {"title": "Support vector networks", "author": ["Cortes", "Corinna", "Vapnik", "Vladimir N"], "venue": "Machine Learning,", "citeRegEx": "Cortes et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 1995}, {"title": "An Introduction to Support Vector Machines : and other Kernel-based Learning Methods", "author": ["Cristianini", "Nello", "Shawe-Taylor", "John"], "venue": null, "citeRegEx": "Cristianini et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2000}, {"title": "The foundations of cost-sensitive learning", "author": ["Elkan", "Charles"], "venue": "In Proceedings of the 17th International Joint Conference on Artificial Intelligence, volume 2 of IJCAI\u201901,", "citeRegEx": "Elkan and Charles.,? \\Q2001\\E", "shortCiteRegEx": "Elkan and Charles.", "year": 2001}, {"title": "On the doubt about margin explanation of boosting", "author": ["Gao", "Wei", "Zhou", "Zhi-Hua"], "venue": "Artificial Intelligence,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Permitted and forbidden sets in symmetric threshold-linear networks", "author": ["Hahnloser", "Richard H. R", "Seung", "H. Sebastian", "Slotine", "Jean-Jacques"], "venue": "Neural Computation,", "citeRegEx": "Hahnloser et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hahnloser et al\\.", "year": 2003}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Binary classification by svm based tree type neural networks", "author": ["Jayadeva", "Deb", "Alok Kanti", "Chandra", "Suresh"], "venue": "In Neural Networks,", "citeRegEx": "Jayadeva et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Jayadeva et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Lanckriet", "Gert R. G", "Cristianini", "Nello", "Bartlett", "Peter", "Ghaoui", "Laurent El", "Jordan", "Michael I"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun", "Yann", "Huang", "Fu Jie", "Bottou", "L\u00e9on"], "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Novel maximum-margin training algorithms for supervised neural networks", "author": ["Ludwig", "Oswaldo", "Nunes", "Urbano"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Ludwig et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ludwig et al\\.", "year": 2010}, {"title": "Maximizing margins of multilayer neural networks", "author": ["Nishikawa", "Takahiro", "Abe", "Shigeo"], "venue": "In Neural Information Processing,", "citeRegEx": "Nishikawa et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Nishikawa et al\\.", "year": 2002}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["Reyzin", "Lev", "Schapire", "Robert E"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Reyzin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Reyzin et al\\.", "year": 2006}, {"title": "The strength of weak learnability", "author": ["Schapire", "Robert E"], "venue": "Machine Learning,", "citeRegEx": "Schapire and E.,? \\Q1990\\E", "shortCiteRegEx": "Schapire and E.", "year": 1990}, {"title": "Boosting the margin: a new explanation for the effectiveness of voting methods", "author": ["Schapire", "Robert E", "Freund", "Yoav", "Bartlett", "Peter", "Lee", "Wee Sun"], "venue": "The Annals of Statistics,", "citeRegEx": "Schapire et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1998}, {"title": "The Nature of Statistical Learning", "author": ["Vapnik", "Vladimir N"], "venue": null, "citeRegEx": "Vapnik and N.,? \\Q1995\\E", "shortCiteRegEx": "Vapnik and N.", "year": 1995}, {"title": "A refined margin analysis for boosting algorithms via equilibrium margin", "author": ["Wang", "Liwei", "Sugiyama", "Masashi", "Jing", "Zhaoxiang", "Yang", "Cheng", "Zhou", "Zhi-Hua", "Feng", "Jufu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Carve-a constructive algorithm for real-valued examples", "author": ["Young", "Steven", "Downs", "Tom"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Young et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Young et al\\.", "year": 1998}, {"title": "Large margin distribution machine", "author": ["Zhang", "Teng", "Zhou", "Zhi-Hua"], "venue": "CoRR, abs/1311.0989,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Optimal margin distribution machine", "author": ["Zhang", "Teng", "Zhou", "Zhi-Hua"], "venue": "CoRR, abs/1604.03348,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "On multi-class cost-sensitive learning", "author": ["Zhou", "Zhi-Hua", "Liu", "Xu-Ying"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence,", "citeRegEx": "Zhou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 19, "context": "It has been shown that their resistance to overfitting is due to the effect these methods have on the distribution of points around the margin (Schapire et al., 1998; Reyzin & Schapire, 2006; Wang et al., 2011).", "startOffset": 143, "endOffset": 210}, {"referenceID": 21, "context": "It has been shown that their resistance to overfitting is due to the effect these methods have on the distribution of points around the margin (Schapire et al., 1998; Reyzin & Schapire, 2006; Wang et al., 2011).", "startOffset": 143, "endOffset": 210}, {"referenceID": 19, "context": "It has been shown that their resistance to overfitting is due to the effect these methods have on the distribution of points around the margin (Schapire et al., 1998; Reyzin & Schapire, 2006; Wang et al., 2011). Gao & Zhou (2013) theoretically showed that AdaBoost is resistant to overfitting because it implicitly optimises the classification margin distribution by maximising average margin and minimising margin variance simultaneously.", "startOffset": 144, "endOffset": 230}, {"referenceID": 19, "context": "It has been shown that their resistance to overfitting is due to the effect these methods have on the distribution of points around the margin (Schapire et al., 1998; Reyzin & Schapire, 2006; Wang et al., 2011). Gao & Zhou (2013) theoretically showed that AdaBoost is resistant to overfitting because it implicitly optimises the classification margin distribution by maximising average margin and minimising margin variance simultaneously. In particular, they emphasised that the minimisation of margin variance is very important, which was ignored by most previous studies on learning algorithm design. Zhang & Zhou (2013) proposed the LDM which maximises average margin and minimises margin variance simultaneously, and achieved consistently better performance than SVMs; later, Zhang & Zhou (2016) proposed Optimal Margin Machine (ODM) which demonstrates even better performance.", "startOffset": 144, "endOffset": 624}, {"referenceID": 19, "context": "It has been shown that their resistance to overfitting is due to the effect these methods have on the distribution of points around the margin (Schapire et al., 1998; Reyzin & Schapire, 2006; Wang et al., 2011). Gao & Zhou (2013) theoretically showed that AdaBoost is resistant to overfitting because it implicitly optimises the classification margin distribution by maximising average margin and minimising margin variance simultaneously. In particular, they emphasised that the minimisation of margin variance is very important, which was ignored by most previous studies on learning algorithm design. Zhang & Zhou (2013) proposed the LDM which maximises average margin and minimises margin variance simultaneously, and achieved consistently better performance than SVMs; later, Zhang & Zhou (2016) proposed Optimal Margin Machine (ODM) which demonstrates even better performance.", "startOffset": 144, "endOffset": 801}, {"referenceID": 7, "context": "We can test it on fully connected, as well as convolutional, neural networks with Rectifier Linear Unit (ReLU) activation function (Hahnloser et al., 2003; Glorot et al., 2011) and compare its performance to Softmax Central-Entropy loss on multi-class image recognition datasets.", "startOffset": 131, "endOffset": 176}, {"referenceID": 6, "context": "We can test it on fully connected, as well as convolutional, neural networks with Rectifier Linear Unit (ReLU) activation function (Hahnloser et al., 2003; Glorot et al., 2011) and compare its performance to Softmax Central-Entropy loss on multi-class image recognition datasets.", "startOffset": 131, "endOffset": 176}, {"referenceID": 7, "context": "Jayadeva et al. (2002) combined it with a decision tree-based training and Nishikawa & Abe (2002) incorporated it into the CARVE algorithm (Young & Downs, 1998).", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Jayadeva et al. (2002) combined it with a decision tree-based training and Nishikawa & Abe (2002) incorporated it into the CARVE algorithm (Young & Downs, 1998).", "startOffset": 0, "endOffset": 98}, {"referenceID": 12, "context": "Lanckriet et al. (2004) demonstrated that with some constraints and restrictions on \u03c6(x), maximisation of the margin still provides an upper bound on probability of misclassification.", "startOffset": 0, "endOffset": 24}, {"referenceID": 14, "context": "Figure 1 shows how the test error relates to the maximum geometric and mean margin values over a range of different \u03b4\u2019s in \u03c6(x) for a two-class subset problem from the smallNORB dataset (LeCun et al., 2004).", "startOffset": 186, "endOffset": 206}, {"referenceID": 12, "context": "However, given that current proofs for existence of the bound require certain constraints on the structure of \u03c6(x) (Lanckriet et al., 2004), the result of our simple experiment prompts us to hypothesise that in general it is the relative value of the margin within given \u03c6(x) and not its absolute value across different realisations of \u03c6(x) that needs to be maximised in order to improve generalisation.", "startOffset": 115, "endOffset": 139}, {"referenceID": 13, "context": "The three datasets that we will use for evaluation of the Halfway loss are the MNIST (LeCun et al., 1998), smallNORB (LeCun et al.", "startOffset": 85, "endOffset": 105}, {"referenceID": 14, "context": ", 1998), smallNORB (LeCun et al., 2004) and CIFAR-10 (Krizhevsky, 2009) datasets.", "startOffset": 19, "endOffset": 39}], "year": 2017, "abstractText": "Despite being so vital to success of Support Vector Machines, the principle of separating margin maximisation is not used in deep learning. We show that minimisation of margin variance and not maximisation of the margin is more suitable for improving generalisation in deep architectures. We propose the Halfway loss function that minimises the Normalised Margin Variance (NMV) at the output of a deep learning models and evaluate its performance against the Softmax Cross-Entropy loss on the MNIST, smallNORB and CIFAR-10 datasets.", "creator": "LaTeX with hyperref package"}}}