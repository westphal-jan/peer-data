{"id": "1704.05665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "CNN based music emotion classification", "abstract": "Music emotion recognition (MER) is usually regarded as a multi-label tagging task, and each segment of music can inspire specific emotion tags. Most researchers extract acoustic features from music and explore the relations between these features and their corresponding emotion tags. Considering the inconsistency of emotions inspired by the same music segment for human beings, seeking for the key acoustic features that really affect on emotions is really a challenging task. In this paper, we propose a novel MER method by using deep convolutional neural network (CNN) on the music spectrograms that contains both the original time and frequency domain information. By the proposed method, no additional effort on extracting specific features required, which is left to the training procedure of the CNN model. Experiments are conducted on the standard CAL500 and CAL500exp dataset. Results show that, for both datasets, the proposed method outperforms state-of-the-art methods.", "histories": [["v1", "Wed, 19 Apr 2017 09:28:39 GMT  (2887kb,D)", "http://arxiv.org/abs/1704.05665v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.MM cs.LG", "authors": ["xin liu", "qingcai chen", "xiangping wu", "yan liu", "yang liu"], "accepted": false, "id": "1704.05665"}, "pdf": {"name": "1704.05665.pdf", "metadata": {"source": "CRF", "title": "CNN BASED MUSIC EMOTION CLASSIFICATION", "authors": ["Xin Liu", "Qingcai Chen", "Xiangping Wu", "Yan Liu", "Yang Liu"], "emails": ["wxpleduole}@gmail.com,", "csyliu@comp.polyu.edu.hk,", "csygliu@comp.hkbu.edu.hk"], "sections": [{"heading": null, "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "2.1. CNN framework for music emotion classification", "text": "Fig.1 shows the CNN framework for the classification of music emotions. Unique from existing methods is that this framework uses only the spectrogram of the audio signal as input, and a network with few folding pooling layers, hidden layers and a SOFTMAX classifier is then constructed to extract properties and classify the emotions of a given music segment. It is also known that emotions inspired by a segment of music are relevant to both the rhythm and the melody of music [17], which are mainly determined by the distribution and variance of signal energy in time and frequency ranges. Spectrogram, as a way of displaying variations of the frequency spectrum over time, represents not only a visualization tool, but also an important type of rich information characteristics for the analysis of audio signals. Spectrogram is calculated via the short-time fourier transformation of audio signals."}, {"heading": "2.2. Preprocessing of music spectrogram", "text": "The pre-processing of the spectrogram is a key point for the successful application of CNN to the music spectrogram. It is because input to CNN is required as a fixed-dimensional matrix, while the length of the music segment specified for the classification of emotions is usually variable duration.As equivalenty.( 1) shows, the spectrogram of a music segment is generated by calculating a set of values of the discrete time segment STFT (t, f) at different times of the music segment along the timeline. At any time, the number of values produced along the frequency axis is determined by the needs of the frequency resolution for a given application. Therefore, we first determine the frequency points K of the STFT (t, f). Then, the number of time points is set as equal to the frequency points in order to generate a K \u00d7 K matrix. Since we directly use Matlab to generate the spectrogram, we first select in this paper to skip the same time of the music segment by generating the same overdimensional time of the two music matrix."}, {"heading": "2.3. Convolutional neural network for music spectrogram", "text": "As mentioned in the previous section, the emotions are formed when people listen to music, usually going through a cumulative process as time passes. Then, this is another reason why we choose spectrogram to represent a music. In the Convolutionary Neural Network, the input feature maps are calculated by the operation called local field to generate new feature maps. Concrete implementation in the network is that the nodes for computation are adjacent in the same layers and the nodes in two adjective layers are not fully connected. Thus, this operation can satisfy the need to calculate the relevance of the adjacent time and frequency."}, {"heading": "2.4. Emotion classification based on CNN of music spectrogram", "text": "In the Hidden Level and Classification Level, the input is the same as the transformed vectors in the previous level. The nodes in the Hidden Level are fully connected. After a few fully connected layers, we then receive a fixed vector as Softmax input for classification. In Softmax, the dimension of the output corresponds to the number of emotion tags. Each dimension corresponds to one day. As soon as the numerical value exceeds a certain threshold, we conclude that the tag belongs to this music. We also train our neural network in some valid techniques, such as dropouts [22], activation function, and so on."}, {"heading": "3.1. Dataset", "text": "Wang et al. [16] published the dataset. CAL500exp labels are commented on the segment level and not on the track level in CAL500. In other words, in CAL500exp, each song contains several segments that split off from each other, and each segment is commented on as dependent data from 18 emotion tags. Thus, in the CAL500exp dataset, there are a total of 3223 items. Whereas in CAL500, each song is considered a traction or test data, and each song is rated on 18 emotions from 1 to 5 by different listeners, we confirm the emotion labels of this song by the mean that at least 80 percent of all listeners agree with the score [15]. The number in CAL500 is significantly lower than CAL500exp. There are about 502 items."}, {"heading": "3.2. Evaluation criterion", "text": "Most researchers use macro averages and micro averages to evaluate overall performance across multiple labels.In this paper, we focus on the precision (P), recall (R), and F1 criteria, which take into account both precision and recall. In contrast, some other criteria such as hammer loss, AUC value, average precision (AP), and an error are briefly presented, and the calculation method can be found using Python Sklearn, which is a Python machine learning and data mining module at [23].For some criteria such as F1 value, AUC value, and average precision, the greater the metric value, the better performance it displays.For another two metrics that hamming losses and an error, it is the opposite. Smaller values indicate better performance.For each train set, we stop training until CNN reaches a certain epoch."}, {"heading": "3.3. Cross validation", "text": "We constructed traction set, validation set and test set based on these segments and whole songs. We suggested ten-fold cross-validation for train, validation and test. In both sets of data, we performed ten-fold random cross-validation. 3223 segments and 502 songs were first placed in a random order, then we selected 10 percent of the same disordered set as a test set and the rest as a move or validation set for ten times each for each record. Ten-fold cross-validation aims to create a contrast with those already known states of art. In CAL500exp, each fold contains the same number of moves and test sets with the average differences in 5, namely about 2902 and 321. While in CAL500, each fold contains 452 songs for train and 50 for test. Since CNN model generates a set of parameters in each era, each represents a set of parameters and each yields different results. Thus, we search for suitable parts for both train sets based on the validation."}, {"heading": "3.4. Model structure", "text": "As shown in Fig. 1, the original input is the fixed-size spectrogram. Compared to the general input size of CNN, our input structure may be slightly more complex. But, such a scale may contain more information of a long music. In the price include more calculations and disk space costs. Besides, we need to know that the size is not the more complex the better. If more complex, there will be more redundant information that will lead to a negative impact on the end result. To compensate for the loss of information and the cost of time and space complexity, we need to choose a balanced size. In our model, there are two key points that we need to plan. One is the size of the input spectrum, the other is the structure of CNN.First, we need to know that the size of the spectrogram is associated with the length of the music. After analyzing music in both datasets, we find out that one song is long in CAL500 minutes, while 5 seconds are performed more or less for a segment in CAL500exp."}, {"heading": "3.5. Results", "text": "In previous years we have shown the size of spectra and the structure of CNN that we do not have to adjust all the parameters of the network. We have developed the model with iterations (also called epoch) and in any case we will get a price that indicates the processing of the current epoch. Costs can not only tell us if the model works, but also help us to select the parameters for the test. There is an example of cost reduction with the increase of iterations that affect the model. There are hundreds of epochs that we choose, such as the average cost of ten epochs. The trend of costs shows the validity of models and allows us to find the optimal parameters. After a series of certain iterations we finish the move."}], "references": [{"title": "What strikes the strings of your heart?\u2014feature mining for music emotion analysis", "author": ["Yang Liu", "Yan Liu", "Yu Zhao", "Kien A Hua"], "venue": "IEEE Transactions on Affective Computing, vol. 6, no. 3, pp. 247\u2013260, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Musical brain revealed by high-field (3 tesla) functional mri", "author": ["Teruhiro Nakada", "Yukihiko Fujii", "Kiyotaka Suzuki", "Ingrid L Kwee"], "venue": "Neuro report, vol. 9, no. 17, pp. 3853\u20133856, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Psychological perspectives on music and emotion", "author": ["John A Sloboda", "Patrik N Juslin"], "venue": "2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening", "author": ["Patrik N Juslin", "Petri Laukka"], "venue": "Journal of New Music Research, vol. 33, no. 3, pp. 216\u2013237, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Music emotion recognition: From content- to contextbased models", "author": ["Mathieu Barthet", "Gyorgy Fazekas", "Mark Sandler"], "venue": "Computer music modeling and retrieval, pp. 228\u2013252, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label classification of emotions in music", "author": ["Alicja A Wieczorkowska", "Piotr Synak", "Zbigniew W Ra\u015b"], "venue": "Proc. of Intelligent Information Processing and Web Mining, vol. 35, pp. 307\u2013315, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "An empirical study of multi-label classifiers for music tag annotation", "author": ["Chris Sanden", "John Z Zhang"], "venue": "Proc. of the 12th International Society for Music Information Retrieval (ISMIR) Conference, pp. 717\u2013722, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Emotion in the singing voice\u2014a deeperlook at acoustic features in the light of automatic classification", "author": ["Florian Eyben", "Glaucia L Salomao", "Johan Sundberg", "Klaus R Scherer", "Bjorn Schuller"], "venue": "Eurasip Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, pp. 1\u20139, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Music emotion classification (mec): Exploiting vocal and instrumental sound features", "author": ["Mudiana Mokhsin Misron", "Nurlaila Binti Rosli", "Norehan Abdul Manaf", "Hamizan Abdul Halim"], "venue": "Recent Advances on Soft Computing and Data Mining, pp. 539\u2013549, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["Beth Logan"], "venue": "Proc. of the 1st International Symposium on Music Information Retrieval (ISMIR 2000)Conference, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Feature selection for content-based, time-varying musical emotion regression", "author": ["Erik M Schmidt", "Douglas Turnbull", "Youngmoo E Kim"], "venue": "Proc. of the 11th ACM SIGMM International Conference on Multimedia Information Retrieval (MIR), pp. 267\u2013274, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Multilabel classification via calibrated label ranking", "author": ["Johannes Furnkranz", "Eyke Hullermeier", "Eneldo Loza Mencia", "Klaus Brinker"], "venue": "Machine Learning, vol. 73, no. 2, pp. 133\u2013153, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Random k-labelsets for multilabel classification", "author": ["Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis Vlahavas"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 23, no. 7, pp. 1079\u20131089, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["Minling Zhang", "Zhihua Zhou"], "venue": "Pattern Recognition, vol. 40, no. 7, pp. 2038\u20132048, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards musical query-by-semanticdescription using the cal500 data set", "author": ["Douglas Turnbull", "Luke Barrington", "David Torres", "Gert Lanckriet"], "venue": "Proc. of Annual International Acm Sigir Conference on Research Development in Information Retrieval, pp. 439\u2013446, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards time-varying music autotagging based on cal500 expansion", "author": ["Shuoyang Wang", "Juchiang Wang", "Yihsuan Yang", "Hsinmin Wang"], "venue": "IEEE International Coference on Multimedia and Expo(ICME), pp. 1\u20136, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Emotions, arousal, and frontal alpha rhythm asymmetry during beethoven\u2019s 5th symphony", "author": ["Christian Alexander Mikutta", "Andreas Altorfer", "Werner Strik", "Thomas Koenig"], "venue": "Brain Topography, vol. 25, no. 4, pp. 423\u2013 430, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Music genre recognition using spectrograms", "author": ["Yandre M G Costa", "Luiz S Oliveira", "Alessandro L Koericb", "Fabien Gouyon"], "venue": "IEEE International Conference on Systems, Signals and Image Processing(IWSSIP), pp. 1\u2013 4, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectrograms: turning signals into pictures", "author": ["Mark French", "Rod Handy"], "venue": "Journal of Engineering Technology, vol. 24, pp. 32\u201335, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Real time implementation of audio spectrogram on field programmable gate array(fpga)", "author": ["Akshay Krishnegowda Hebbal"], "venue": "Dissertations Theses Gradworks, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Yoshua Bengio", "Yann Lecun", "Yann Lecun"], "venue": "1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["George E Dahl", "Tara N Sainath", "Geoffrey E Hinton"], "venue": "IEEE International Conference on Acoustics, Speech adn Signal Processing, pp. 8609\u20138613, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Gael Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron J Weiss", "Vincent Dubourg"], "venue": "Journal of Machine Learning Research, vol. 12, no. 10, pp. 2825\u20132830, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Researchers have shown that, music, explained as organized sound, can resonate with our nerve tissue [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "It is more likely a biological instinct, just the interaction between sound rhythm or melody and their brain [2, 3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 2, "context": "It is more likely a biological instinct, just the interaction between sound rhythm or melody and their brain [2, 3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 3, "context": "In fact, the delicate relationship between music and emotion has already been explored by numerous researchers for a long period [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "[5] gave a detail introduction about music emotion recognition (MER) task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] firstly formulated MER as a multi-label classification problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Though there is still no standard guidance for selecting features that contribute most to the representation of music [7], acoustic features are still most prevalent in the feature selecting procedure [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "Though there is still no standard guidance for selecting features that contribute most to the representation of music [7], acoustic features are still most prevalent in the feature selecting procedure [8].", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "Acoustic features mainly consist of rhythmic features, timbre features and spectral features [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Rhythmic features are derived by extracting periodic changes from a beat histogram [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "Timbre features consist of a series of Zero Crossing Rate, MFCC [10] and Chroma [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Timbre features consist of a series of Zero Crossing Rate, MFCC [10] and Chroma [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "For example, Calibrated Label Ranking classifier using a Support Vector Machine (CLRSVM) [12], Random k-Labelsets (RAkEL) [13], Back-propagation for Multi-Label Learning (BPMLL), Multi-Label K-Nearest Neighbor (MLkNN) [14] and Binary Relevance kNN (BRkNN) etc.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "For example, Calibrated Label Ranking classifier using a Support Vector Machine (CLRSVM) [12], Random k-Labelsets (RAkEL) [13], Back-propagation for Multi-Label Learning (BPMLL), Multi-Label K-Nearest Neighbor (MLkNN) [14] and Binary Relevance kNN (BRkNN) etc.", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "For example, Calibrated Label Ranking classifier using a Support Vector Machine (CLRSVM) [12], Random k-Labelsets (RAkEL) [13], Back-propagation for Multi-Label Learning (BPMLL), Multi-Label K-Nearest Neighbor (MLkNN) [14] and Binary Relevance kNN (BRkNN) etc.", "startOffset": 218, "endOffset": 222}, {"referenceID": 4, "context": "Among these classifiers, in most cases, CLRSVM outperforms the rest [5].", "startOffset": 68, "endOffset": 71}, {"referenceID": 14, "context": "In Section 3 we discuss the experiments on the dataset CAL500 [15] and CAL500exp [16] and compare the results with state-of-the-arts algorithms.", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "In Section 3 we discuss the experiments on the dataset CAL500 [15] and CAL500exp [16] and compare the results with state-of-the-arts algorithms.", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "As well known that emotions inspired by a segment of music are close relevant to both the rhythm and melody of music [17], which are mainly determined by the distribution and variance of signal energy on time and frequency domains.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Spectrogram, as a type of representation for variances of frequency spectrum with time, not only presents a visualization tool, but also an important type of rich-information feature for audio signal analysis [18, 19].", "startOffset": 209, "endOffset": 217}, {"referenceID": 18, "context": "Spectrogram, as a type of representation for variances of frequency spectrum with time, not only presents a visualization tool, but also an important type of rich-information feature for audio signal analysis [18, 19].", "startOffset": 209, "endOffset": 217}, {"referenceID": 19, "context": "The spectrogram is computed via the Short-Time-Fourier-Transformation of audio signal along the time axis as below formula [20]:", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "nals into their spectrogram, the power variance information along both frequency and time axis are well presented [19].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "The convolution calculation [21] is", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": ", Sigmoid or Relu [22]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "We also train our neural network in some valid techniques, such as dropout [22], activation function and so on.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "[16] published the dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "While in CAL500, each whole song is regarded as one train or test data and each song is scored on 18 emotions form 1 to 5 by different listeners, then we confirm the emotion labels of this song by the means that at least 80 percent of all listeners agree the score [15].", "startOffset": 265, "endOffset": 269}, {"referenceID": 22, "context": "The computational method can be found from python-sklearn which is a python module for machine learning and data mining at [23].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "The first line of numerical parts shows the result of [16].", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "[16] Macro average 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] report the results when they published the CAL500exp dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "And on CAL500, the results are shown in Table 4, compared with [16] and [1].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "And on CAL500, the results are shown in Table 4, compared with [16] and [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 15, "context": "The first line of numerical parts shows the result of [16].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "The second and third line of numerical parts shows the result of [1].", "startOffset": 65, "endOffset": 68}, {"referenceID": 15, "context": "[16] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "417 Macro average [1] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "444 Micro average [1] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "423 CAL500 [1] 0.", "startOffset": 11, "endOffset": 14}], "year": 2017, "abstractText": "Music emotion recognition (MER) is usually regarded as a multi-label tagging task, and each segment of music can inspire specific emotion tags. Most researchers extract acoustic features from music and explore the relations between these features and their corresponding emotion tags. Considering the inconsistency of emotions inspired by the same music segment for human beings, seeking for the key acoustic features that really affect on emotions is really a challenging task. In this paper, we propose a novel MER method by using deep convolutional neural network (CNN) on the music spectrograms that contains both the original time and frequency domain information. By the proposed method, no additional effort on extracting specific features required, which is left to the training procedure of the CNN model. Experiments are conducted on the standard CAL500 and CAL500exp dataset. Results show that, for both datasets, the proposed method outperforms state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}