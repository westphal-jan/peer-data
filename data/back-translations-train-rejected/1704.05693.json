{"id": "1704.05693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Unsupervised Creation of Parameterized Avatars", "abstract": "We study the problem of mapping an input image to a tied pair consisting of a vector of parameters and an image that is created using a graphical engine from the vector of parameters. The mapping's objective is to have the output image as similar as possible to the input image. During training, no supervision is given in the form of matching inputs and outputs.", "histories": [["v1", "Wed, 19 Apr 2017 11:19:45 GMT  (7045kb,D)", "http://arxiv.org/abs/1704.05693v1", null], ["v2", "Sun, 9 Jul 2017 16:10:53 GMT  (7045kb,D)", "http://arxiv.org/abs/1704.05693v2", "v2 -- a change in the references due to a request from authors"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["lior wolf", "yaniv taigman", "adam polyak"], "accepted": false, "id": "1704.05693"}, "pdf": {"name": "1704.05693.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Creation of Parameterized Avatars", "authors": ["Lior Wolf", "Yaniv Taigman", "Adam Polyak"], "emails": [], "sections": [{"heading": null, "text": "This learning problem extends to two literature problems: unattended domain fitting and cross-domain transfer. We define a generalization boundary based on discrepancy and use a GAN to implement a network solution that meets that boundary. Experimentally, our method has been shown to solve the problem of automatic avatar creation."}, {"heading": "1. Introduction", "text": "The artist Hanoch Piven creates caricatures by arranging household items and scrap material in a frame and photographing the result, see Figure 1 (a). How can a computer produce such images? Given a training set consisting of Piven's images, generative Adversarial Networks (GANs) can be used to create images that are as indistinguishable as possible. However, common sense tells us that for any reasonably large training set, without knowledge of the physical world, the generated images are easily recognized by humans as synthetic. As a second motivating example, the problem of generating computer avatars based on the user is considered."}, {"heading": "1.1. Background", "text": "In fact, it is so that most of them are able to abide by the rules which they have imposed on themselves. (...) Indeed, it is so that they are able to determine themselves. (...) In fact, it is so that they are able to determine themselves. (...) It is as if they are able to do it. (...) It is not as if they do it. (...) It is as if they do it. (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...\") \"((...)\" (() \"(()\") ((()) \"() (()\" () () \"() () (()\" () () \"() ()\" () () () () () () () () () () ()) (()) () () ()) () () () () ()) () () () ()) () () () () () ()) () () ()) () () () ()) () () () ()) () () () () ())) () () ()) () () ()) () ()) () () ()) () () () () () ()) () ()) () () () () () ()) () () () () () ()) () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () (() () () () () () () ()"}, {"heading": "2. Problem Formulation", "text": "Issues related to domain shifts are getting more and more attention as the focus of machine learning shifts away from vanilla-monitored learning scenarios to new combinations of monitored, unsupervised, and transferred learning. In this section, we formulate the new arithmetic problem we call Tied Output Synthesis (TOS) and put it into a theoretical context. In the next section, we redefine the problem as a concrete deep learning problem. To maximize clarity, the two sections are kept as independent as possible."}, {"heading": "2.1. Related Problems", "text": "In the unattended domain adaptation problem [2, 16, 1], the algorithm maps a hypothesis on one source domain and tests the hypothesis on another target domain. Therefore, the algorithm is evaluated with a labeled dataset of the source domain and an unlabeled dataset of the target domain. The conventional approach to dealing with this problem is to learn a function card that (i) allows accurate classification in the source domain and (ii) has significant invariant relationships between the source domain and the target domain. Let X be the input room and Y the output room (mathematical notation is also conveniently tabulated in the appendix). The source domain is a distribution DS along a function yS: X \u2192 Y. Similarly, the target domain is specified by (DT, yT). \""}, {"heading": "2.2. The Tied Output Synthesis Problem", "text": "The problem examined in this paper is a third variant of domain shift, which can be seen as a mixture of the two problems: unattended domain customization and the cross-domain transfer problem. Similar to the unattended domain transfer problem, which imposes a number of verified domain transfer problems on us."}, {"heading": "3. The Tied Output Synthesis Network", "text": "We are investigating the problem of projecting an image in one domain onto an image in another domain where the images follow a series of specifications. Considering a domain, X, a mapping e, and a function f, we would like to learn a generative function G so that f is invariant under G, i.e. f, G = f, and that for all examples x-X there is a configuration u-Y2, so that G (x) = e (u). Unlike functions f and e, the training data is unattended and consists of a series of samples from the source domain X and a second group from the target domain e, which we call Y1. Compared to the domain transfer method presented in [22], domain Y1 is limited to being the image of a mapping. DTN cannot meet this requirement because it requires a strong recovery of the real world, since it does not have sufficient synthesis of images."}, {"heading": "3.1. The interplay between the trained networks", "text": "In a general view of GANs, we assume a loss function '(G, d, x) for a function d that receives input in domain Y1. G, which maps an input x to units in Y1, minimizes the following loss: LGAN = maxd \u2212 Ex' (G, d, x). This optimization is successful if for each function d is small the expectation of '(G, d, x) for the learned G. Consider the case where G and a second function c work hand in hand to minimize the expectation of another loss (G, c, x). The two learned networks d and G provide a training signal for each other. In this case, two networks can also provide a mutual signal by working together on a common task. Consider the case where G and a second function c work hand in order to minimize the expectation of another loss."}, {"heading": "3.2. The complete network solution", "text": "The learning algorithm is, in addition to the two mappings e and f, a training set s-X and a training protocol t-Y1. Similar to [22], we define G from f and a second function g, which results from the output space of f (x), i.e., G = g samples f-samples (Lc of Eq. 3): Lc = x samples g-samples (f (x)))) \u2212 e (c (f (x)))), 2 (G = g-samples f. In addition, we minimize LCONST, which advocates for each input x-sets that f-samples unmodified asGmaps make it to Y1: LCONST = x-sets f (x) \u2212 f (G (x)) \u2212 f (6) a GAN term is added to ensure that the samples generated by G can be indexed."}, {"heading": "4. Experiments", "text": "The Tied Output Synthesis (TOS) method is evaluated based on a toy problem of reversing a polygon synthesis engine and avatar generation based on a photo for two different CG engines; the first problem is presented as a mere illustration of the method, while the second is an unsolved challenge in the real world."}, {"heading": "4.1. Polygons", "text": "The first experiment examines TOS in a context independent of f-constancy. In view of a series of images t-Y1 and a picture e from any vector space on Y1, we learn a figure c and a generative function G that generates random images in Y1 that are e-conform (Equation 4). We generate binary 64 x 64 images of regular polygons by consistently scanning three parameters: the number of vertices (3-6), the radius of the enclosing circle (15-30) and a rotation angle in the range [\u2212 10, 10]. Some polygons are shown in Fig. 5 (a). 10,000 training images were created and used to train a CNN e that maps the three parameters to the output, with very low loss (MSE of 0.1) and a rotation angle in the range [\u2212 10, 10]."}, {"heading": "4.2. Face Emoji", "text": "The proposed TOS method is evaluated for the task of generating specification-compliant emojis. In this task, we transfer an in-the-wild facial image to a set of parameters that define an emoji. However, since the unlabeled training data is generated from facial images (Domain X), we use a set of one million random images without identity information. The set consists of different facial images created by an online service (bitmoji.com). The emoji images were processed through an automatic process that generates one million random images without identity information, the center of the iris and the tip of the nose [22]. Based on these coordinates, the emojis were centered and scaled into 152 \u00d7 152 RGB images. The emoji engine of the online service is largely additive. To train the TOS, we imitate them and have created a neural network that includes characteristics such as gender, hair length, and so on."}, {"heading": "4.3. VR Avatars", "text": "Next, we apply the proposed TOS method to a commercial avatar generator engine, see Fig. 6 (c). We try random parameterization and automatically align their frontal rendered avatars into 64 x 64 RGB images to form the training set t. We then train a CNN e to mimic this engine and generate such images based on their parameterization. Using the same architectures and configurations as in Fig. 4.2, including the same training sets, we train g and c to assign natural facial photos to their motor-compliant parameters. In addition, we repeat the same identification experiment and report on the ranking of the analog experiments, see Tab. 1 (right). The 3D avatar machine is not as detailed in its construction as the 2D emoji engine, with elements such as facial hair still missing and fewer subforms available. Furthermore, the overall model is too focused on the avatar style and therefore appears to be lower in the puppets only, while the avatoning style and puppets appear to be lower."}, {"heading": "5. Conclusions", "text": "With the advent of better computer graphics engines and the abundance of available models, and the ability of neural networks to compare cross-domain units, the ability to link image data with appropriate parameterization is lacking. DTN demonstrated a remarkable ability to generate analogies without explicit monitoring, such as creating highly identifiable emojis. However, emoji applications require parameterized characters that can then be transformed by artists into different views and new expressions, and the emojis created by DTN cannot be converted into a configuration. The TOS method we present is capable of generating identifiable emojis coupled with a valid configuration vector. While TOS has been presented in a way that requires the differentiability of the rendering function e, working with black box renderers using gradient estimation techniques is common practice, such as cleaning."}, {"heading": "A. Summary of Notations", "text": "Tab. 2 lists the symbols used in this work. Figure 2,3,4 of the main text illustrates many of these symbols."}, {"heading": "B. DANN results", "text": "Fig. 8 shows side by side examples of the original image and the emoji generated by the method of [6]. As can be seen, these results do not preserve the identity particularly well despite considerable efforts to find suitable architectures."}, {"heading": "C. Multiple Images Per Person", "text": "Following [22], we evaluated the visual quality achieved per person, not just per image, by testing TOS on the Facescrub dataset [18]. For each person p, we looked at the set of their images Xp and selected the emoji that was most similar to their source image, i.e., the one for which:"}, {"heading": "D. Detailed Architecture of the Various Networks", "text": "This year, it is so far that it is only a matter of time before it is so far, until it is so far, until it is so far."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We study the problem of mapping an input image to a<lb>tied pair consisting of a vector of parameters and an image<lb>that is created using a graphical engine from the vector of<lb>parameters. The mapping\u2019s objective is to have the output<lb>image as similar as possible to the input image. During<lb>training, no supervision is given in the form of matching<lb>inputs and outputs. This learning problem extends two literature problems:<lb>unsupervised domain adaptation and cross domain trans-<lb>fer. We define a generalization bound that is based on dis-<lb>crepancy, and employ a GAN to implement a network so-<lb>lution that corresponds to this bound. Experimentally, our<lb>method is shown to solve the problem of automatically cre-<lb>ating avatars.", "creator": "LaTeX with hyperref package"}}}