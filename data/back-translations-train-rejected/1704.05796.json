{"id": "1704.05796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations", "abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.", "histories": [["v1", "Wed, 19 Apr 2017 16:10:38 GMT  (7585kb,D)", "http://arxiv.org/abs/1704.05796v1", "First two authors contributed equally. Oral presentation at CVPR 2017"]], "COMMENTS": "First two authors contributed equally. Oral presentation at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["david bau", "bolei zhou", "aditya khosla", "aude oliva", "antonio torralba"], "accepted": false, "id": "1704.05796"}, "pdf": {"name": "1704.05796.pdf", "metadata": {"source": "CRF", "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations", "authors": ["David Bau", "Bolei Zhou", "Aditya Khosla", "Aude Oliva", "Antonio Torralba"], "emails": ["torralba}@csail.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "Observation of hidden units in large, deep neural networks has shown that human interpretable concepts sometimes occur as individual latent variables within these networks: For example, object detector units appear within networks designed to detect locations [40]; partial detectors appear in object classifiers [11]; and object detectors appear in generative video networks [32]. This internal structure occurs in situations where networks are not limited to decompressing problems in any way. The emergence of interpretable structures suggests that deep networks can spontaneously learn unraveled representations. While it is generally understood that a network can learn efficient coding that allows the economic use of hidden variables to distinguish its states, lamps appear in object networks that represent humans in video networks equally."}, {"heading": "1.1. Related Work", "text": "A growing number of techniques have been developed to understand the internal representations of Convolutionary Neural Networks through visualization; the behavior of a CNN can be represented by scanning image fields that maximize the activation of hidden units [37, 40], or by using variants of back propagation to identify or generate prominent image features [17, 26, 37]; the discriminatory power of hidden layers of CNN features can also be understood by isolating, transmitting, or limiting portions of networks, and testing their capabilities on specific problems [35, 24, 2]. Visualizations digest the mechanisms of a network to the point where images themselves need to be interpreted; this motivates our work, which aims to directly and automatically match representations of CNNs with designated interpretations."}, {"heading": "2. Network Dissection", "text": "How can we quantify the clarity of an idea? The notion of an untangled representation is based on the human perception of what it means for a concept that needs to be mixed up. So, when we quantify the ability to interpret, we define it in terms of alignment with a set of humanly interpretable concepts. Our measurement of the ability to interpret deep visual representations proceeds in three steps: 1. Identify a broad range of humanly described visual concepts. 2. Collect hidden variables \"response to known concepts. 3. Quantify the alignment of hidden variable concept pairs. This three-step process of network dissection is reminiscent of the procedures that neuroscientists use to understand similar questions of representation in biological neurons. [23] Since our purpose is to measure the level at which a representation is unraveled, we focus on quantifying the correlation between a single latent variable and a visual concept."}, {"heading": "2.1. Broden: Broadly and Densely Labeled Dataset", "text": "To determine consistency with both low-level concepts such as colors and higher-level concepts such as objects, we have compiled a new heterogeneous dataset. The Broadly and Densely Labeled Dataset (Broden) brings together several densely labeled image datasets: ADE [43], OpenSurfaces [4], Pascal context [19], Pascal part [6], and the Describable Textures Dataset [7]. These datasets contain examples of a wide range of objects, scenes, object parts, textures, and materials in a variety of contexts. Most examples are segmented down to the pixel level, except for textures and scenes given for full frames. In addition, each image pixel in the dataset is provided with one of the eleven common color descriptions that are identical according to the human perceptions classified by van de Weijer."}, {"heading": "2.2. Scoring Unit Interpretability", "text": "The aforementioned driving forces are able to hide themselves, namely in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they are able to behave, in the way they react, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they live, in the way they are able to behave, in the way they are able to behave, in the way they behave, in the way they are able to behave, in the way they react, in the way they react, in the way they react, in the way they react, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they act, in the way they live, in the way they live, in the way they live, how they act, how they act, in the way they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act, how they act"}, {"heading": "3. Experiments", "text": "For the test, we prepare a collection of CNN models with different network architectures and monitoring of primary tasks as listed in Table 2. Net architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12]. For supervised training, the models are trained from the ground up (i.e. not pre-trained) on ImageNet [25], Places205 [42], and Places365 [41]. ImageNet is an object-oriented dataset containing 1.2 million images from 1000 classes. Places205 and Places365 are two subsets of the Places Database that represent a scenographic set of data with categories such as kitchen, living room, and coast. Places205 contains images from 205 scenarios in which we move."}, {"heading": "3.1. Human Evaluation of Interpretations", "text": "We evaluate the quality of unit interpretations found by our method using Amazon Mechanical Turk (AMT). Assessors were shown 15 images with highlighted fields showing the most activating regions for each unit in AlexNet trained at locations 205, and they were asked to decide (yes / no) whether a particular phrase describes the most image fields. Table 3 summarizes the results. First, we determined the set of interpretable units as those units for which the raters agreed with the ground truth interpretations from [40]. About this set of units, we report the percentage of interpretations generated by our method that were rated as descriptive. Within this sentence, we also compare the percentage of soil truth labels described by a second group of raters. The proposed method may find semantic labels for units comparable to the descriptions written by human annotators at the top level."}, {"heading": "3.2. Measurement of Axis-Aligned Interpretability", "text": "We are conducting an experiment to determine whether it makes sense to assign an interpretable concept to an individual entity. Two possible hypotheses can explain the emergence of interpretability in individual hidden layer units: Hypothesis 1. Interpretable units arise because interpretable concepts appear in most directions in the space of representation. If representation localizes related concepts in an axis-independent manner, the projection in each direction could indicate a concept capable of interpretation, and interpretations of individual units in the natural base cannot be a meaningful method to understand a representation.Hypothesis 2. Interpretable alignments are unusual and interpretable units arise because learning has convergences to a specific basis that aligns explanatory factors with individual units. In this model, the natural basis represents a meaningful decomposition learned through the network.Hypothesis 1 is the standard assumption: In the past, there was found to be no differences in terms of interpretability. \""}, {"heading": "3.3. Disentangled Concepts by Layer", "text": "Using network dissection, we analyze and compare the interpretability of units within all the winding layers of Places-AlexNet and ImageNet-AlexNet. Places-AlexNet is trained for scene classification at Places205 [42], while ImageNet-AlexNet is the identical architecture that was trained for object classification at ImageNet [15]. Results are summarized in Fig. 5. a sample of units is presented together with automatically derived interpretations and manually assigned interpretations from [40]. We can see that the predicted labels correspond well with human remark, although they sometimes contain a different description of a visual concept, such as the \"Way of the Cross\" predicted by the algorithm compared to the \"horizontal lines\" that humans make for the third unit in Convex 4 of PlacesAlexNet in Fig. 5. Confirm intuition, color and texture concepts, more than the object detectors - in the convex and sub-detector layers."}, {"heading": "3.4. Network Architectures and Supervisions", "text": "For simplicity, the following experiments focus on the last revolutionary layer of each CNN, where semantic detectors appear most frequently. Results showing the number of unique detectors resulting from different network architectures trained on ImageNet and Places are presented in Fig. 7, with examples shown in Fig. 6. Regarding the network architecture, we find that the interpretability of ResNet > VGG > GoogLeNet > AlexNet. Deeper architectures seem to allow greater interpretability. When comparing training data, we find Places > ImageNet. As discussed in [40], a scene is composed of multiple objects, so it may be beneficial for more object detectors tested in CNNs to detect scenarios. Results from networks trained on different monitored and self-monitored tasks will be epotated."}, {"heading": "3.5. Training Conditions vs. Interpretability", "text": "We have the same way in which we analyze the impact of the training processes on the interpretative ability of people in individual countries that we have taken as a starting point to prepare several variants of which all use the same architecture. To analyze the impact of the training processes on interpretative ability, we take the Places205AlexNet as a starting point and prepare several variants of which all use the same architecture. To analyze the impact of the training processes on interpretative conditions, we must be 0.091 dotted (texture) 0.140 chequered (texture) 0.167 car (texture) 0.063 perforated (structure) 0.085 scrps (part) 0.056 grass (object) 0.12010 210 6Training iteration010203040N."}, {"heading": "3.6. Discrimination vs. Interpretability", "text": "Activations from the higher layers of CNNs are often used as generic visual features that exhibit a high discriminatory and generalizable capability [42, 24]. Here, we measure deep features from multiple networks trained on multiple standardized image classification datasets for their discriminatory capability in a new task. For each trained model, we determine the representation at the highest revolutionary level and form a linear SVM with C = 0.001 on the training data for the action40 action detection task [34]. We calculate the classification accuracy that is averaged across all classes on the test split.Fig.12 averages the number of unique object detectors for each representation compared to the classification accuracy of this representation on the action40 test set. We see that there is a positive correlation between them. Therefore, the monitoring tasks that promote the emergence of more concept detectors may also improve the discriminatory capability of deep features. Interestingly, the best discriminatory net representation for the image detection number 15240 represents the unique representation of the image action365."}, {"heading": "3.7. Layer Width vs. Interpretability", "text": "From AlexNet to ResNet, the CNNs for visual detection have grown deeper in the quest for higher classification accuracy; depth has proven to be important for a high discriminatory ability, and we have seen in Section 3.4 that interpretability can also increase with depth. However, the width of layers (the number of units per layer) is less researched, one reason being that increasing the number of revolutionary units on a layer significantly increases computing costs, while resulting in only marginal improvements in classification accuracy. Nevertheless, some recent work [36] shows that a carefully designed wide residual network can achieve a classification accuracy superior to the commonly used thin and deep counterparts. To investigate how the width of layers affects the interpretability of CNNs, we are conducting a preliminary experiment to test how the formation of interpretable detectors affects: We remove the FC layers from the number of Alex768 units to the number of 253."}, {"heading": "4. Conclusion", "text": "This work proposed a general framework for quantifying the interpretability of CNNs. We applied network dissection to measure whether interpretability is an axis-independent phenomenon, and we found that it is not. This is consistent with the hypothesis that interpretable units indicate a partially unraveled representation; we applied network dissection to investigate the impact on the interpretation of modern CNN training techniques; we confirmed that representations at different levels unravel different categories of meaning; and that different training techniques can have a significant impact on the interpretability of the representation learned from hidden units; this work was partially supported by the National Science Foundation under grant number 1524817 to A.T.; the Vannevar Bush Faculty Fellowship Program, sponsored by the Basic Research Office of the Assistant Secretary of Defense for Research and Engineering; and the Naval Research Office-01-01-16 grants to NN."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "Proc. ECCV,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding intermediate layers using linear classifier probes", "author": ["G. Alain", "Y. Bengio"], "venue": "arXiv:1610.01644,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Intrinsic images in the wild", "author": ["S. Bell", "K. Bala", "N. Snavely"], "venue": "ACM Trans. on Graphics (SIGGRAPH),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Detect what you can: Detecting and representing objects using holistic models and body parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing textures in the wild", "author": ["M. Cimpoi", "S. Maji", "I. Kokkinos", "S. Mohamed", "A. Vedaldi"], "venue": "Proc. CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "What is a random matrix", "author": ["P. Diaconis"], "venue": "Notices of the AMS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "Proc. CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Object-centric representation learning from unlabeled videos", "author": ["R. Gao", "D. Jayaraman", "K. Grauman"], "venue": "arXiv:1612.00500,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Do semantic parts emerge in convolutional neural networks", "author": ["A. Gonzalez-Garcia", "D. Modolo", "V. Ferrari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": "Proc. ICCV,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Convergent learning: Do different neural networks learn the same representations", "author": ["Y. Li", "J. Yosinski", "J. Clune", "H. Lipson", "J. Hopcroft"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "Proc. CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Shuffle and learn: unsupervised learning using temporal order verification", "author": ["I. Misra", "C.L. Zitnick", "M. Hebert"], "venue": "Proc. ECCV,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["A. Nguyen", "A. Dosovitskiy", "J. Yosinski", "T. Brox", "J. Clune"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "Proc. ECCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba"], "venue": "Proc. ECCV,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Invariant visual representation by single neurons in the human brain", "author": ["R.Q. Quiroga", "L. Reddy", "G. Kreiman", "C. Koch", "I. Fried"], "venue": "Nature, 435(7045):1102\u20131107,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "arXiv:1403.6382,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Int\u2019l Journal of Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "International Conference on Learning Representations Workshop,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u2013 1958,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv:1312.6199,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning color names for real-world applications", "author": ["J. Van De Weijer", "C. Schmid", "J. Verbeek", "D. Larlus"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Generating videos with scene dynamics", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "arXiv:1609.02612,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "Proc. CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei-Fei"], "venue": "Proc. ICCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv:1605.07146,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Proc. ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "Proc. ECCV. Springer,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Split-brain autoencoders: Unsupervised learning by cross-channel prediction", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "Proc. CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Places: An image database for deep scene understanding", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Torralba", "A. Oliva"], "venue": "arXiv:1610.02055,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Scene parsing through ade20k dataset", "author": ["B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba"], "venue": "Proc. CVPR,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 39, "context": "Observations of hidden units in large deep neural networks have revealed that human-interpretable concepts sometimes emerge as individual latent variables within those networks: for example, object detector units emerge within networks trained to recognize places [40]; part detectors emerge in object classifiers [11]; and object detectors emerge in generative video networks [32] (Fig.", "startOffset": 264, "endOffset": 268}, {"referenceID": 10, "context": "Observations of hidden units in large deep neural networks have revealed that human-interpretable concepts sometimes emerge as individual latent variables within those networks: for example, object detector units emerge within networks trained to recognize places [40]; part detectors emerge in object classifiers [11]; and object detectors emerge in generative video networks [32] (Fig.", "startOffset": 314, "endOffset": 318}, {"referenceID": 31, "context": "Observations of hidden units in large deep neural networks have revealed that human-interpretable concepts sometimes emerge as individual latent variables within those networks: for example, object detector units emerge within networks trained to recognize places [40]; part detectors emerge in object classifiers [11]; and object detectors emerge in generative video networks [32] (Fig.", "startOffset": 377, "endOffset": 381}, {"referenceID": 39, "context": "Unit 13 in [40] (classifying places) detects table lamps.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "Unit 246 in [11] (classifying objects) detects bicycle wheels.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "A unit in [32] (self-supervised for generating videos) detects people.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "A disentangled representation aligns its variables with a meaningful factorization of the underlying problem structure, and encouraging disentangled representations is a significant area of research [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 27, "context": "We further examine how interpretability is affected by training data sets, training techniques like dropout [28] and batch normalization [13], and supervision by different primary tasks.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "We further examine how interpretability is affected by training data sets, training techniques like dropout [28] and batch normalization [13], and supervision by different primary tasks.", "startOffset": 137, "endOffset": 141}, {"referenceID": 36, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 107, "endOffset": 115}, {"referenceID": 39, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 107, "endOffset": 115}, {"referenceID": 16, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 204, "endOffset": 216}, {"referenceID": 25, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 204, "endOffset": 216}, {"referenceID": 36, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 204, "endOffset": 216}, {"referenceID": 34, "context": "The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2].", "startOffset": 207, "endOffset": 218}, {"referenceID": 23, "context": "The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2].", "startOffset": 207, "endOffset": 218}, {"referenceID": 1, "context": "The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2].", "startOffset": 207, "endOffset": 218}, {"referenceID": 39, "context": "In [40] human evaluation was used to determine that individual units behave as object detectors in a network that was trained to classify scenes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "[20] automatically generated prototypical images for individual units by learning a feature inversion mapping; this contrasts with our approach of automatically assigning concept labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Recently [3] suggested an approach to testing the intermediate layers by training simple linear probes, which analyzes the information dynamics among layers and its effect on the final prediction.", "startOffset": 9, "endOffset": 12}, {"referenceID": 22, "context": "This three-step process of network dissection is reminiscent of the procedures used by neuroscientists to understand similar representation questions in biological neurons [23].", "startOffset": 172, "endOffset": 176}, {"referenceID": 4, "context": "Although we expect a network to learn partially nonlocal representations in interior layers [5], and past experience shows that an emergent concept will often align with a combination of a several hidden units [11, 2], street (scene) flower (object) headboard (part)", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "Although we expect a network to learn partially nonlocal representations in interior layers [5], and past experience shows that an emergent concept will often align with a combination of a several hidden units [11, 2], street (scene) flower (object) headboard (part)", "startOffset": 210, "endOffset": 217}, {"referenceID": 1, "context": "Although we expect a network to learn partially nonlocal representations in interior layers [5], and past experience shows that an emergent concept will often align with a combination of a several hidden units [11, 2], street (scene) flower (object) headboard (part)", "startOffset": 210, "endOffset": 217}, {"referenceID": 42, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 201, "endOffset": 204}, {"referenceID": 30, "context": "In addition, every image pixel in the data set is annotated with one of the eleven common color names according to the human perceptions classified by van de Weijer [31].", "startOffset": 165, "endOffset": 169}, {"referenceID": 42, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 50, "endOffset": 54}, {"referenceID": 42, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 94, "endOffset": 98}, {"referenceID": 42, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 167, "endOffset": 170}, {"referenceID": 6, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 192, "endOffset": 195}, {"referenceID": 14, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 26, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": ", not pretrained) on ImageNet [25], Places205 [42], and Places365 [41].", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": ", not pretrained) on ImageNet [25], Places205 [42], and Places365 [41].", "startOffset": 46, "endOffset": 50}, {"referenceID": 40, "context": ", not pretrained) on ImageNet [25], Places205 [42], and Places365 [41].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 212, "endOffset": 215}, {"referenceID": 17, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 259, "endOffset": 263}, {"referenceID": 32, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 287, "endOffset": 291}, {"referenceID": 9, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 344, "endOffset": 348}, {"referenceID": 37, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 383, "endOffset": 387}, {"referenceID": 38, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 429, "endOffset": 433}, {"referenceID": 21, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 484, "endOffset": 488}, {"referenceID": 14, "context": "Next, we analyze all the convolutional layers of AlexNet as trained on ImageNet [15] and as trained on Places [42], and confirm that our method reveals detectors for higher-level concepts at higher layers and lower-level concepts at lower layers; and that more detectors for higher-level concepts emerge under scene training.", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "Next, we analyze all the convolutional layers of AlexNet as trained on ImageNet [15] and as trained on Places [42], and confirm that our method reveals detectors for higher-level concepts at higher layers and lower-level concepts at lower layers; and that more detectors for higher-level concepts emerge under scene training.", "startOffset": 110, "endOffset": 114}, {"referenceID": 39, "context": "First, we determined the set of interpretable units as those units for which raters agreed with ground-truth interpretations from [40].", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "Hypothesis 1 is the default assumption: in the past it has been found [30] that with respect to interpretability \u201cthere is no distinction between individual high level units and random linear combinations of high level units.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "The rotation Q is drawn uniformly from SO(256) by applying GramSchmidt on a normally-distributed QR = A \u2208 R2562 with positive-diagonal right-triangular R, as described by [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 41, "context": "Places-AlexNet is trained for scene classification on Places205 [42], while ImageNet-AlexNet is the identical architecture trained for object classification on ImageNet [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Places-AlexNet is trained for scene classification on Places205 [42], while ImageNet-AlexNet is the identical architecture trained for object classification on ImageNet [15].", "startOffset": 169, "endOffset": 173}, {"referenceID": 39, "context": "A sample of units are shown together with both automatically inferred interpretations and manually assigned interpretations taken from [40].", "startOffset": 135, "endOffset": 139}, {"referenceID": 39, "context": "As discussed in [40], one scene is composed of multiple objects, so it may be beneficial for more object detectors to emerge in CNNs trained to recognize scenes.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "Training conditions such as the number of training iterations, dropout [28], batch normalization [13], and random initialization [16], are known to affect the representation learning of neural networks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "Training conditions such as the number of training iterations, dropout [28], batch normalization [13], and random initialization [16], are known to affect the representation learning of neural networks.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Training conditions such as the number of training iterations, dropout [28], batch normalization [13], and random initialization [16], are known to affect the representation learning of neural networks.", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "We find several effects: 1) Comparing different random initializations, the models converge to similar levels of interpretability, both in terms of the unique detector number and the total detector number; this matches observations of convergent learning discussed in [16].", "startOffset": 268, "endOffset": 272}, {"referenceID": 41, "context": "and generalization ability [42, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 23, "context": "and generalization ability [42, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 33, "context": "001 on the training data for action40 action recognition task [34].", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "Nevertheless, some recent work [36] shows that a carefully designed wide residual network can achieve classification accuracy superior to the commonly used thin and deep counterparts.", "startOffset": 31, "endOffset": 35}], "year": 2017, "abstractText": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.", "creator": "LaTeX with hyperref package"}}}