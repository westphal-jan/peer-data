{"id": "1704.05958", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Global Relation Embedding for Relation Extraction", "abstract": "Recent studies have shown that embedding textual relations using deep neural networks greatly helps relation extraction. However, many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data. In this work, we generalize textual relation embedding to the distant supervision setting, where much larger-scale but noisy training data is available. We propose leveraging global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus, to embed textual relations. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embeddings can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.", "histories": [["v1", "Wed, 19 Apr 2017 23:54:46 GMT  (3279kb)", "http://arxiv.org/abs/1704.05958v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yu su", "honglei liu", "semih yavuz", "izzeddin gur", "huan sun", "xifeng yan"], "accepted": false, "id": "1704.05958"}, "pdf": {"name": "1704.05958.pdf", "metadata": {"source": "CRF", "title": "Global Relation Embedding for Relation Extraction", "authors": ["Yu Su", "Honglei Liu", "Semih Yavuz", "Izzeddin G\u00fcr", "Xifeng Yan"], "emails": ["ysu@cs.ucsb.edu", "honglei@cs.ucsb.edu", "syavuz@cs.ucsb.edu", "izzeddingur@cs.ucsb.edu", "sun.397@osu.edu", "xyan@cs.ucsb.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.05 958v 1 [cs.C L] 19 Apr 201 7Text relationships using deep neural networks help with relationship extraction, but many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data. In this work, we generalize textual embedding in the remote supervision environment, where much more extensive but noisy training data is available. We propose to use global relationship statistics, i.e. the statistics on the occurrence of text and knowledge base relationships collected from the entire corpus to embed textual relationships. This approach proves to be more robust than the training noise introduced by remote monitoring. Based on a popular relationship extraction dataset, we show that the learned textual embedding can be used to expand existing relationship models and significantly improve their performance, 3. On the basis of a popular relationship extraction dataset, we suggest that the learned textual embedding can be used to significantly enhance the training noise introduced by remote monitoring. 3."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to change the world, in which they are able to integrate themselves, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they live in which they live, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they live in which they live, in which they live, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, she lives, in which she lives, in which she lives, in which she lives, she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which"}, {"heading": "2 Related Work", "text": "In recent years, it has been shown that the problem is not only a problem, but also a complex problem that affects people. (In recent years, it has been shown that the problem is a complex problem.) (In recent years, it has been shown that it is a complex problem.) In recent years, it has been shown that it is a complex problem. (In the last ten years, it has been shown that it is a complex problem.) In the last ten years, the number of cases in which it is a complex problem has doubled. (In the last ten years, it has been shown that it is a complex problem.) In the last ten years, the number of cases in which it is a complex problem has increased. (In the last ten years, it has been shown that it is a complex problem.)"}, {"heading": "3 Global Statistics of Relations", "text": "When we use a corpus to train statistical models, there are two levels of statistics that we can exploit: local and global. Let's take Word embedding as an example. The Skip-gram model (Mikolov et al., 2013) is based on local statistics: During the training, we sweep through the corpus and slightly fine-tune the embedding model based on each local window (e.g. 10 consecutive words). In contrast, in global statistical methods illustrated by latent semantic analysis (Deerwester et al., 1990) and GloVe (Pennington et al., 2014), we process the entire corpus to collect global statistics such as word-word coexistence numbers, normalize the raw statistics, and train an embedding model directly on the normalized global statistics. Most existing studies on relation extraction are based on local statistics of relationships, i.e., models are coached on individual statistical examples from this section and coding examples from KB."}, {"heading": "3.1 Relation Graph Construction", "text": "In the face of a corpus and a KB, we first do an entity that links on each sentence, and do pairs of dependencies if at least two entities are identified.2 For each entity pair (e, e) in the sentence, we extract the fully lexicalized shortest dependency path as a textual relationship t, creating a relational fact (e, t, e \u2032).From this step, two results emerge: a set of textual relations T = {ti} and support S (ti) for each ti. Support of a textual relationship is a multiset containing the entity pairs of the textual relationship. The plurality of an entity pair, mS (ti) (e, e \u2032), is the number of occurrences of the corresponding relational fact (e, ti, e \u2032) in the corpus."}, {"heading": "3.2 Normalization", "text": "The raw co-occurrences include a highly distorted distribution spanning several orders of magnitude, so that most co-occurrences occur very frequently, while most co-occurrences occur only a few times. For example, a textual relationship, SUBJECT nsubjpass \u2190 \u2212 \u2212 born nmod: in \u2212 \u2212 born nmod: in \u2212 \u2212 \u2212 \u2212 -city nmod: in \u2212 \u2212 \u2212 city nmod: in Chicago \"), while a synonymous but somewhat more compositional text relationship, SUBJECT nsubjpass: in \u2212 born nmod: in the city nmod: from \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 can be found, coinciding with the KB relationship only several times throughout the corpus (e.g.:\" Michelle Obama was born in the city of Chicago \")."}, {"heading": "4 Textual Relation Embedding", "text": "Next, we will discuss how to learn how to embed textual relationships using the constructed relationship graph. We call our approach Global Relation Embedding (GloRE) in the light of global relationship statistics."}, {"heading": "4.1 Embedding via RNN", "text": "Considering the ratio graph, a simple way of embedding relationships is matrix factorization, similar to latent semantic analysis (Deerwester et al., 1990) for embedding words. However, textual relationships of words differ in that they are sequences of words and typed dependency relationships. Therefore, we use recursive neural networks (RNNs) for embedding, which respect the compositionality of textual relationships and can learn the common substructures of different textual relationships (Toutanova et al., 2015). For the examples in Figure 1, an RNN can learn from both textual relationships that the common dependency relationship \"nmod: in\" is indicative of location changes. For a textual relationship, we first decompose it into a sequence of tokens {x1,..., xm} that includes lexical words and directional dependencies."}, {"heading": "4.2 Training Objective", "text": "We use global statistics in the relation graph to train the embedding model. Specifically, we model the semantics of a textual relationship as its random distribution of KB relationships and learn textual embedding to reconstruct the corresponding co-event distributions.We use a separate GRU cell followed by softmax to map a textual relationship embedded in a distribution across KB relationships (Figure 3); the complete model thus resembles the sequence-to-sequence architecture (Sutskever et al., 2014).Given a textual relationship ti and its embedding hm, the predicted conditional probability of a KB relationship rj is thus: p (rj | ti) = softmax (Woho + bo) j, (3) where () j denotes the j element of a vector, and ho is the state of the output GRU cell: ho = GRU cell: > GRU-embedding (GO)."}, {"heading": "5 Augmenting Relation Extraction", "text": "This year, the number of job-related redundancies has fallen by 20 per cent compared to the previous year, while the number of job-related redundancies has increased by 20 per cent."}, {"heading": "6 Experiments", "text": "In this experimental study, we demonstrate the effectiveness of GloRE by showing that it could significantly improve the performance of several newer methods of relation extraction."}, {"heading": "6.1 Experimental Setup", "text": "In fact, most people are able to determine for themselves what they want and what they want to do. (...) Most people in the world have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They have no idea what they can do. (...) They don't do it. \"(...) They have no idea.\""}, {"heading": "6.2 Held-out Evaluation", "text": "Existing models + GloRE. We first show that our approach, GloRE, can improve the performance of the previous most powerful model, 4 https: / / code.google.com / archive / p / word2vec / PCNN + ATT, leading to a new state of the art on the NYT dataset. As shown in Figure 5, when adding GloRE to PCNN + ATT, a consistent improvement is observed along the precision callback curve. It is also worth noting that although PCNN + ATT + GloRE appears to be worse at callback than PCNN + ATT < 0.05, as we will show from manual evaluation, it is actually based on false negatives. We show in Figure 4 that the improvement that GloRE brings is general, not only for PCNN + ATT; the other three models also receive a consistent improvement when supplemented with GloRE."}, {"heading": "6.3 Manual Evaluation", "text": "Due to the incompleteness of the knowledge base, sustained evaluation leads to some false negatives. Accuracy of sustained evaluation is therefore a lower limit of true precision. To get a more accurate evaluation of model performance, we have human experts who manually review the false relational facts assessed by heldout evaluation in the top 1000 predictions of three models: PCNN + ATT, PCNN + ATT + LoRE and PCNN + ATT + GloRE and the corrected results in Table 2. Under manual evaluation, PCNN + ATT + GloRE achieves the best performance in the entire range of N. In particular, for the top 1000 predictions, GloRE improves the accuracy of the previous best model PCNN + ATT from 83.9% to 89.3%. The manual evaluation results confirm the earlier observations from sustained evaluation."}, {"heading": "6.4 Case Study", "text": "Table 3 shows two examples. For a better illustration, we choose pairs of entities that have only one contextual sentence. For the first example, PCNN + ATT predicts that there is most likely no KB relationship between the entity pair, while both LoRE and GloRE identify the correct relationship with high confidence.The textual relationship clearly indicates that the main unit (appos) is a criminologist at (nmod: at) the entity. For the second example, there is no KB relationship between the entity pair, and PCNN + ATT is actually able to rank NA at the top. However, it is still quite confused about nationality, probably because it has learned that sentences about a person and a country with many words out of work (\"poet,\" \"playwright\" and \"novelist\") are likely to express the nationality of the person. As a result, its prediction of NA is not very certain, on the other hand, that if a person learns a GloRE at a place, as well as a place of birth, it is a problem."}, {"heading": "7 Conclusion", "text": "Our results show that the textual embedding of relationships based on global coexistence statistics with KB relationships can gather useful information for the extraction of relationships and thus improve existing models for the extraction of relationships. Extensive training data for embedding can easily be obtained from remote monitoring, and global relationship statistics provide a natural way to combat the problem of the mislabeling of relationships remotely. The idea of embedding relationships based on global statistics can be expanded further in several directions. In this work, we have focused on the embedding of text relationships, but it is in principle beneficial for the joint embedding of relationships in the knowledge base (KB) as well as units. Recently, a common embedding approach has been attempted in connection with the completion of the knowledge base (Toutanova et al., 2015), but it is still based on local statistics, i.e. on individual relational facts. The common embedding approach in connection with the completion of the knowledge base has been attempted."}, {"heading": "A Word and Relation Distribution", "text": "Since textual relationships are sequences of words and dependency relationships, they collide less frequently than words. As a result, most textual relationships occur only a few times. We collect statistics on the occurrence and occurrence of words and textual relationships from the corpus of The New York Times. Two words occur side by side when they occur in the same sentence. The simultaneous relationship of text and KB relationships is created by remote monitoring, as described in Section 3. As can be seen, the relationship distributions are more distorted toward the bottom end of the axis than the word distributions. Very few textual relationships occur more than 100 times. Rare the norm, not the exception, but it is worth noting that the most common textual relationship still occurs more than 40,000 times (not shown in the illustrations)."}], "references": [{"title": "Tensorflow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the ACM SIGMOD International conference on Management of", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["John A Bullinaria", "Joseph P Levy."], "venue": "Behavior research methods 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Razvan C Bunescu", "Raymond J Mooney."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 724\u2013731.", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Dependency tree kernels for relation extraction", "author": ["Aron Culotta", "Jeffrey Sorensen."], "venue": "Proceedings of the AnnualMeeting of the Association for Computational Linguistics. Association for Computational Linguistics, page 423.", "citeRegEx": "Culotta and Sorensen.,? 2004", "shortCiteRegEx": "Culotta and Sorensen.", "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman."], "venue": "Journal of the American society for information science 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Thomas G Dietterich", "Richard H Lathrop", "Tom\u00e1s Lozano-P\u00e9rez."], "venue": "Artificial intelligence 89(1):31\u201371.", "citeRegEx": "Dietterich et al\\.,? 1997", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou GuoDong", "Su Jian", "Zhang Jie", "Zhang Min."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 427\u2013", "citeRegEx": "GuoDong et al\\.,? 2005", "shortCiteRegEx": "GuoDong et al\\.", "year": 2005}, {"title": "Knowledgebased weak supervision for information extraction", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": null, "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["Nanda Kambhatla."], "venue": "Proceedings of the ACL on Interactive poster and demonstration sessions. Association for Computational Linguistics,", "citeRegEx": "Kambhatla.,? 2004", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Word embeddings through hellinger PCA", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "European Chapter of the Association for Computational Linguistics page 482.", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Compu-", "citeRegEx": "Lin et al\\.,? 2016", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Relation classification via modeling augmented dependency paths", "author": ["Yang Liu", "Sujian Li", "Furu Wei", "Heng Ji."], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 24(9):1585\u20131594.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems. pages", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguis-", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 148\u2013163.", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M Marlin."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Riedel et al\\.,? 2013", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "An improved model of semantic similarity based on lexical co-occurrence", "author": ["Douglas LT Rohde", "Laura M Gonnerman", "David C Plaut."], "venue": "Communications of the ACM 8:627\u2013633.", "citeRegEx": "Rohde et al\\.,? 2005", "shortCiteRegEx": "Rohde et al\\.", "year": 2005}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher DManning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Multiinstance multi-label learning for relation extraction", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Surdeanu et al\\.,? 2012", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Toutanova et al\\.,? 2015", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Asso-", "citeRegEx": "Verga et al\\.,? 2016", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "arXiv:1506.07650 .", "citeRegEx": "Xu et al\\.,? 2015a", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Improved relation classification by deep recurrent neural networkswith data augmentation", "author": ["Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin."], "venue": "arXiv:1601.03651 .", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Xu et al\\.,? 2015b", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Kernel methods for relation extraction", "author": ["Dmitry Zelenko", "Chinatsu Aone", "Anthony Richardella."], "venue": "Journal of machine learning research 3(Feb):1083\u20131106.", "citeRegEx": "Zelenko et al\\.,? 2003", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of the International Conference on Computational Linguistics", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Exploring syntactic features for relation extraction using a convolution tree kernel", "author": ["Min Zhang", "Jie Zhang", "Jian Su."], "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the As-", "citeRegEx": "Zhang et al\\.,? 2006", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Early studies mainly use hand-crafted features (Kambhatla, 2004; GuoDong et al., 2005), and later kernel methods are introduced to automatically generate fea-", "startOffset": 47, "endOffset": 86}, {"referenceID": 9, "context": "Early studies mainly use hand-crafted features (Kambhatla, 2004; GuoDong et al., 2005), and later kernel methods are introduced to automatically generate fea-", "startOffset": 47, "endOffset": 86}, {"referenceID": 30, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 6, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 3, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 33, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 22, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 32, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 29, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 31, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 14, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 3, "context": "Because of their exact feature matching, early kernel based models (Bunescu and Mooney, 2005) can hardly exploit fine-grained word similarities.", "startOffset": 67, "endOffset": 93}, {"referenceID": 15, "context": "More recent studies (Xu et al., 2015a,b, 2016; Liu et al., 2016) have explored embedding textual relations via neural networks.", "startOffset": 20, "endOffset": 64}, {"referenceID": 15, "context": "eral thousands of annotated sentences and around 10 target relations (Liu et al., 2016).", "startOffset": 69, "endOffset": 87}, {"referenceID": 17, "context": "In contrast, we embed textual relations with distant supervision (Mintz et al., 2009), which provides much larger-scale training data without the need of manual annotation.", "startOffset": 65, "endOffset": 85}, {"referenceID": 25, "context": "Statistics are based on the annotated ClueWeb data released in (Toutanova et al., 2015).", "startOffset": 63, "endOffset": 87}, {"referenceID": 19, "context": "On a popular dataset introduced by Riedel et al. (2010), we show that a number of recent relation extraction models, which are based on local statistics, can be significantly improved using our textual relation embeddings.", "startOffset": 35, "endOffset": 56}, {"referenceID": 11, "context": "Early relation extraction methods are mainly feature-based (Kambhatla, 2004; GuoDong et al., 2005), where features at various levels, including POS tags, constituency and dependency parses, are integrated in a max entropy model.", "startOffset": 59, "endOffset": 98}, {"referenceID": 9, "context": "Early relation extraction methods are mainly feature-based (Kambhatla, 2004; GuoDong et al., 2005), where features at various levels, including POS tags, constituency and dependency parses, are integrated in a max entropy model.", "startOffset": 59, "endOffset": 98}, {"referenceID": 30, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 6, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 3, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 33, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 3, "context": ", 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006). The most related work to ours is by Bunescu and Mooney (2005), where the authors point out and demonstrate the importance of shortest dependency paths for relation extraction.", "startOffset": 36, "endOffset": 145}, {"referenceID": 22, "context": ", (Socher et al., 2012; Zeng et al., 2014)).", "startOffset": 2, "endOffset": 42}, {"referenceID": 32, "context": ", (Socher et al., 2012; Zeng et al., 2014)).", "startOffset": 2, "endOffset": 42}, {"referenceID": 15, "context": "Among those, the most related are the ones embedding shortest dependency paths with neural networks (Xu et al., 2015a,b, 2016; Liu et al., 2016).", "startOffset": 100, "endOffset": 144}, {"referenceID": 27, "context": "(2015b) use a recurrent neural network (RNN) with LSTM units to embed shortest dependency paths without typed dependency relations, while a convolutional neural network is used in (Xu et al., 2015a).", "startOffset": 180, "endOffset": 198}, {"referenceID": 15, "context": ", 2015a,b, 2016; Liu et al., 2016). For example, Xu et al. (2015b) use a recurrent neural network (RNN) with LSTM units to embed shortest dependency paths without typed dependency relations, while a convolutional neural network is used in (Xu et al.", "startOffset": 17, "endOffset": 67}, {"referenceID": 17, "context": "Distant supervision (Mintz et al., 2009) has emerged as an appealing way to solicit largescale training data for relation extraction.", "startOffset": 20, "endOffset": 40}, {"referenceID": 8, "context": "(2012) have attempted a multi-instance learning (Dietterich et al., 1997) framework to soften the assumption of distant supervision, but their", "startOffset": 48, "endOffset": 73}, {"referenceID": 15, "context": "Distant supervision (Mintz et al., 2009) has emerged as an appealing way to solicit largescale training data for relation extraction. Various efforts have been put to combat the longcriticized wrong labeling problem. Riedel et al. (2010), Hoffmann et al.", "startOffset": 21, "endOffset": 238}, {"referenceID": 9, "context": "(2010), Hoffmann et al. (2011), and Surdeanu et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 9, "context": "(2010), Hoffmann et al. (2011), and Surdeanu et al. (2012) have attempted a multi-instance learning (Dietterich et al.", "startOffset": 8, "endOffset": 59}, {"referenceID": 31, "context": "Zeng et al. (2015) combine multi-instance learning with neural networks, with the assumption that at least one of the contextual sentences of an entity pair is express-", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Instead, Lin et al. (2016) use all the contextual sentences, and", "startOffset": 9, "endOffset": 27}, {"referenceID": 20, "context": "In universal schema (Riedel et al., 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 25, "context": ", 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al., 2015; Verga et al., 2016), a binary matrix is constructed from the entire corpus, with entity pairs as rows and textual/KB relations as columns.", "startOffset": 76, "endOffset": 120}, {"referenceID": 26, "context": ", 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al., 2015; Verga et al., 2016), a binary matrix is constructed from the entire corpus, with entity pairs as rows and textual/KB relations as columns.", "startOffset": 76, "endOffset": 120}, {"referenceID": 16, "context": "The skip-gram model (Mikolov et al., 2013) is based on local statistics: During training, we sweep through the corpus and slightly tune the embedding model based on each local window (e.", "startOffset": 20, "endOffset": 42}, {"referenceID": 7, "context": "In contrast, in global statistics based methods, exemplified by latent semantic analysis (Deerwester et al., 1990) and GloVe (Pennington et al.", "startOffset": 89, "endOffset": 114}, {"referenceID": 18, "context": ", 1990) and GloVe (Pennington et al., 2014), we process the entire corpus to collect global statistics like", "startOffset": 18, "endOffset": 43}, {"referenceID": 4, "context": "In the experiments entity linking is assumed given, and dependency parsing is done using Stanford Parser (Chen and Manning, 2014) with universal dependencies.", "startOffset": 105, "endOffset": 129}, {"referenceID": 21, "context": "A number of normalization strategies have been proposed in the context of word embedding, including correlation- and entropy-based normalization (Rohde et al., 2005), positive pointwise mutual information (Bullinaria and Levy, 2007), and some square root type transformation (Lebret and Collobert, 2014).", "startOffset": 145, "endOffset": 165}, {"referenceID": 2, "context": ", 2005), positive pointwise mutual information (Bullinaria and Levy, 2007), and some square root type transformation (Lebret and Collobert, 2014).", "startOffset": 47, "endOffset": 74}, {"referenceID": 13, "context": ", 2005), positive pointwise mutual information (Bullinaria and Levy, 2007), and some square root type transformation (Lebret and Collobert, 2014).", "startOffset": 117, "endOffset": 145}, {"referenceID": 7, "context": "Given the relation graph, a straightforward way of relation embedding is matrix factorization, similar to latent semantic analysis (Deerwester et al., 1990) for word embedding.", "startOffset": 131, "endOffset": 156}, {"referenceID": 25, "context": "Therefore, we use recurrent neural networks (RNNs) for embedding, which respect the compositionality of textual relations and can learn the shared sub-structures of different textual relations (Toutanova et al., 2015).", "startOffset": 193, "endOffset": 217}, {"referenceID": 5, "context": "An RNN with gated recurrent units (GRUs) (Cho et al., 2014) is then applied to consecutively process the sequence as shown in Figure 3.", "startOffset": 41, "endOffset": 59}, {"referenceID": 5, "context": "GRU follows the definition in Cho et al. (2014):", "startOffset": 30, "endOffset": 48}, {"referenceID": 24, "context": "We use a separate GRU cell followed by softmax to map a textual relation embedding to a distribution over KB relations (Figure 3); the full model thus resembles the sequence-to-sequence architecture (Sutskever et al., 2014).", "startOffset": 199, "endOffset": 223}, {"referenceID": 18, "context": "It is modeled as a regression problem, similar to GloVe (Pennington et al., 2014).", "startOffset": 56, "endOffset": 81}, {"referenceID": 14, "context": "But it will lose the useful signals from those neglected sentences (Lin et al., 2016).", "startOffset": 67, "endOffset": 85}, {"referenceID": 30, "context": ", only using the largest score maxs\u2208C G(z|s), similar to the at-least-one strategy used by Zeng et al. (2015). But it will lose the useful signals from those neglected sentences (Lin et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 10, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 23, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 31, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 14, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 19, "context": ", 2016), we use the relation extraction dataset introduced in (Riedel et al., 2010), which was generated by aligning New York Times (NYT) articles with Freebase (Bollacker et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 1, "context": ", 2010), which was generated by aligning New York Times (NYT) articles with Freebase (Bollacker et al., 2008).", "startOffset": 85, "endOffset": 109}, {"referenceID": 31, "context": "\u2022 CNN+ONE and PCNN+ONE (Zeng et al., 2015): A convolutional neural network (CNN) is used to embed contextual sentences for relation classification.", "startOffset": 23, "endOffset": 42}, {"referenceID": 14, "context": "\u2022 CNN+ATT and PCNN+ATT (Lin et al., 2016): Different from the at-least-one assumption which loses information in the ne-", "startOffset": 23, "endOffset": 41}, {"referenceID": 19, "context": "Similar to previous work (Riedel et al., 2010; Zeng et al., 2015), we use two settings for evaluation: (1) Held-out evaluation, where a subset of relational facts in KB is held out from training (Table 1), and is later used to compare against newly discovered relational facts.", "startOffset": 25, "endOffset": 65}, {"referenceID": 31, "context": "Similar to previous work (Riedel et al., 2010; Zeng et al., 2015), we use two settings for evaluation: (1) Held-out evaluation, where a subset of relational facts in KB is held out from training (Table 1), and is later used to compare against newly discovered relational facts.", "startOffset": 25, "endOffset": 65}, {"referenceID": 12, "context": "We use Adam (Kingma and Ba, 2014) with parameters suggested by the authors for optimization.", "startOffset": 12, "endOffset": 33}, {"referenceID": 16, "context": "Word embeddings are initialized with the 300-dimensional word2vec (Mikolov et al., 2013) vectors pre-trained on the Google News corpus.", "startOffset": 66, "endOffset": 88}, {"referenceID": 0, "context": "Our model is implemented using Tensorflow (Abadi et al., 2016), and the source code is available at https://github.", "startOffset": 42, "endOffset": 62}, {"referenceID": 25, "context": "Recently a joint embedding approach has been attempted in the context of knowledge base completion (Toutanova et al., 2015), but it is still based on local statistics, i.", "startOffset": 99, "endOffset": 123}], "year": 2017, "abstractText": "Recent studies have shown that embedding textual relations using deep neural networks greatly helps relation extraction. However, many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data. In this work, we generalize textual relation embedding to the distant supervision setting, where much largerscale but noisy training data is available. We propose leveraging global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus, to embed textual relations. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embeddings can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.", "creator": "LaTeX with hyperref package"}}}