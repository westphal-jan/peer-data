{"id": "1704.05972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours", "abstract": "Media is full of false claims. Even Oxford Dictionaries named \"post-truth\" as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the kind of discourse there is around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics - each having their own families of claims and replies - and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.", "histories": [["v1", "Thu, 20 Apr 2017 01:21:20 GMT  (28kb)", "http://arxiv.org/abs/1704.05972v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["leon derczynski", "kalina bontcheva", "maria liakata", "rob procter", "geraldine wong sak hoi", "arkaitz zubiaga"], "accepted": false, "id": "1704.05972"}, "pdf": {"name": "1704.05972.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Maria Liakata", "Geraldine Wong Sak Hoi"], "emails": ["leon.d@shef.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.05 972v 1 [cs.C L] 20 Apr 201 7ford dictionaries named \"post-truth\" as Word of the Year 2016. This makes it more important than ever to develop systems that can detect the veracity of a story and the nature of the discourse around it. RumourEval is a joint task of SemEval, which aims to identify and process rumors and responses to it in the text. We present an annotation scheme, a large dataset covering several topics - each of which has its own family of assertions and answers - and use it to raise two concrete challenges as well as the results achieved by participants on these challenges."}, {"heading": "1 Introduction and Motivation", "text": "False assertions influence the perception of events and their behavior, sometimes in harmful ways. As individuals, news professionals, and automated systems increasingly rely on the Web - especially social media - as a source of information and news updates, the potential destructive effects of rumors are further accentuated. The task of analyzing and determining the veracity of social media content has recently been of interest to the field of natural language processing. After initial work (Qazvinian et al., 2011), increasingly advanced systems and annotation schemes have been developed to support the analysis of rumors and misinformation in the text (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016b). Verification judgments can be intuitively dissected in relation to a comparison between assertions in - and analogies from - a candidate text and external world knowledge."}, {"heading": "1.1 Subtask A - SDQC Support/ Rumour stance classification", "text": "In terms of the goal of predicting the veracity of a rumor, Subtask A addresses the complementary goal of tracking how other sources either orient themselves on the accuracy of the rumourous story. An essential step in analyzing the surrounding discourse is to determine how other social media users view the rumor (Procter et al., 2013b). We propose to address this analysis by looking at the conversation that arises from direct and nested responses to the tweet that produces the rumor (Source Tweet). To this effect, RumourEval has provided participants with a tree-structured conversation of tweets that responds to the emergence of rumors, directly or indirectly. Each tweet presents its own kind of support with respect to the rumor (see Figure 1). We frame this in terms of supporting, querying, or commenting on tweets."}, {"heading": "1.2 Subtask B - Veracity prediction", "text": "The purpose of this sub-task is to predict the veracity of a particular rumor. However, in light of such a tweet / allegation and a number of other resources provided, systems should return a label that describes the expected veracity of the rumor as true or false - see Figure 2. The basic veracity of this task was determined manually by journalists who identified official statements or other trusted sources of confidence that resolved the veracity of the rumor. Examples of tweets that were commented on for veracity are shown in Figure 2. Participants in this sub-task chose between two variants. In the first case - the closed variant - the veracity of a rumor had to be predicted solely from the tweet itself (for example (Liu et al., 2015)."}, {"heading": "1.3 Impact", "text": "Decision-making, digital journalism, and disaster response are already based on identifying such claims (Procter et al., 2013b). Furthermore, the web and social media are a more challenging environment than, for example, newswire, which has traditionally been the mainstay of similar tasks (such as RTE (Bentivogli et al., 2011). Last year, we hosted a workshop at WWW 2015, Rumors and Deception in Social Media: Detection, Tracking, and Visualization (RDSM 2015) 1, which piqued the interest of researchers from a variety of fields, including natural language processing, web science, and computational journalism."}, {"heading": "2 Data & Resources", "text": "In order to understand the web assertions and the response of the community to them, we take data from the \"model organism\" of social media, Twitter (Tufekci, 2014), and the data for the task are available in the form of online discussion guides, each referring to a particular event and the rumors surrounding it. These topics form a tree to which each tweet has a parent tweet to which it responds. Together, they form a conversation triggered by a source tweet (see Figure 1). The data has already been commented on according to a published annotation scheme (Zubiaga et al., 2016b) as part 1http: / / www.pheme.eu / events / rdsm2015 / of the PHEME project (Derczynski and Bontcheva, 2014), in which the task organisers are involved, for veracity and SDQC."}, {"heading": "2.1 Training Data", "text": "Our training dataset includes 297 rumors collected for a total of 8 events, including 297 source tweets and 4,222 response tweets for a total of 4,519 tweets. These events include well-known breaking news such as the Charlie Hebdo attack in Paris, the riots in Ferguson, USA, and the Germanwings plane crash in the French Alps. Due to the size of the dataset, it can be distributed as JSON files without modification in accordance with current Twitter data usage policy. This dataset is already publicly available (Zubiaga et al., 2016a) and represents training and development data."}, {"heading": "2.2 Test Data", "text": "For the test data, we commented on 28 additional threads, including 20 threads from the same events as the training set, and 8 threads from two newly collected events: (1) a rumor that Hillary Clinton contracted pneumonia during the 2016 US election campaign, and (2) a rumor that YouTuber Marina Joyce was kidnapped. A total of 1,080 tweets, 28 of which are source tweets and 1,052 responses, are in the test data set. Table 1 summarizes the distribution of labels in the training and test data."}, {"heading": "2.3 Context Data", "text": "In addition to the tweet threads, we also provided additional context for participants to use. We provided two contexts: (1) Wikipedia articles related to the event in question. We provided the last revision of the article before the source tweet was published, and (2) content from linked URLs, using the Internet archive to retrieve the last revision before the link was published, where available."}, {"heading": "2.4 Data Annotation", "text": "Commenting on rumors and their subsequent interactions was done in two steps. In the first step, we sampled a subset of probable rumor tweets from all tweets related to the event, using the high number of retweets as an indication that a tweet is potentially rumor. These sampled tweets were fed to a comment tool that allowed our expert commentators to manually identify those tweets that actually reported unverified updates and were considered rumors. Whenever possible, they also commented on rumors that ultimately turned out to be true or those that were unmasked as false stories; the rest was commented as \"unverified.\" In the second step, we collected conversations associated with those rumor-related updates, including any responses that followed a rumor-source tweet. The type of support (SDQC) expressed by each participant in the conversation was subsequently crowdsourced."}, {"heading": "3 Evaluation", "text": "The two subtasks were evaluated as follows: SDQC Classification: The evaluation of the SDQC required careful consideration, since the distribution of categories is clearly distorted by comments. Evaluation is done by classification accuracy. Verity Forecast: The evaluation of the predicted veracity, which is either true or false for each instance, was done by means of macro averaged accuracy, which measures the ratio of the instances for which a correct prediction was made. In addition, we calculated RMSE \u03c1 for the difference between system and reference security in correct examples and gave the mean of these values. False examples have an RMSE of 1. This is normalized and combined with the macro averaged accuracy to give an end result; e.g. acc = (1 \u2212 \u03c1) acc.The baseline is the most common class. For task A, we also introduce a baseline that excludes the general, low-impact \"comment class,\" taking into account only the denial class, and taking into account the accuracy of DQ."}, {"heading": "4 Participant Systems and Results", "text": "We had 13 system submissions to RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Singh et al., 2017; Garc\u0131 \"a Lozano et al., 2017; Enayet et al., 2017; Chen et al., 2017; Enayet and El-Beltagy et al., 2017; Enayet and El-Beltagy, 2017), the identification of the meaning of rumors, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumor reliability classification task, with participating teams from four continents (Europe, Sweden, UK; North America: Canada; Asia, India, Taiwan; Africa: Egypt), showing the global reach of social media rumors."}, {"heading": "5 Conclusion", "text": "This shared task brought together many approaches to correcting the veracity of rumors in real media by working through community interactions and assertions on the Internet. Many systems were able to achieve good results when it came to unraveling the arguments surrounding various assertions and determining whether a discussion supports, denies, questions, or commentaries.2The systems did not reach the most common baseline, although the data is not exceptionally meagre. Even the best systems may have the wrong level of confidence in a true / false judgment, so this progress is a big positive thing. However, accurately verifying whether a story is false or true remains really difficult. This tells us that we are making progress, but that the problem is very persistent.RumourEval leaves behind competitive outcomes, that large number of approaches for determining good news and setting future benchmark steps for everyone."}, {"heading": "Acknowledgments", "text": "This work is supported by the 7th Research Framework Programme of the European Commission under funding number 611223 PHEME. This work is also supported by the research and innovation programme Horizon 2020 of the European Union under funding agreement number 687847 COMRADES. We are grateful to Swissinfo.ch for their extended support in the form of journalistic advice, the maintenance of a sound brief as well as the efforts to provide comments and tasks. We would also like to thank the SemEval organisers for their sustained hard work and our participants who have supported us in the first joint task of this kind and all the joy and effort that goes with it."}], "references": [{"title": "Cats rule and dogs drool!: Classifying stance in online debate", "author": ["Pranav Anand", "Marilyn Walker", "Rob Abbott", "Jean E. Fox Tree", "Robeson Bowmani", "Michael Minor."], "venue": "Proceedings of the 2Nd Workshop on Compu-", "citeRegEx": "Anand et al\\.,? 2011", "shortCiteRegEx": "Anand et al\\.", "year": 2011}, {"title": "UWaterloo at SemEval-2017 Task 8: Detecting Stance towards Rumours with Topic Independent Features", "author": ["Hareesh Bahuleyan", "Olga Vechtomova."], "venue": "Proceedings of SemEval. ACL.", "citeRegEx": "Bahuleyan and Vechtomova.,? 2017", "shortCiteRegEx": "Bahuleyan and Vechtomova.", "year": 2017}, {"title": "IKM at SemEval-2017 Task 8: Convolutional Neural Networks for Stance Detection and Rumor Verification", "author": ["Yi-Chin Chen", "Zhao-Yand Liu", "Hung-Yu Kao."], "venue": "Proceedings of SemEval. ACL.", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Pheme: Veracity in digital social networks", "author": ["Leon Derczynski", "Kalina Bontcheva."], "venue": "UMAP Workshops.", "citeRegEx": "Derczynski and Bontcheva.,? 2014", "shortCiteRegEx": "Derczynski and Bontcheva.", "year": 2014}, {"title": "NileTMRG at SemEval-2017 Task 8: Determining Rumour and Veracity Support for Rumours on Twitter", "author": ["Omar Enayet", "Samhaa R. El-Beltagy."], "venue": "Proceedings of SemEval. ACL.", "citeRegEx": "Enayet and El.Beltagy.,? 2017", "shortCiteRegEx": "Enayet and El.Beltagy.", "year": 2017}, {"title": "Mama Edha at SemEval-2017 Task 8: Stance Classification with CNN and Rules", "author": ["Marianela Garc\u0131\u0301a Lozano", "Hanna Lilja", "Edward Tj\u00f6rnhammar", "Maja Maja Karasalo"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Lozano et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lozano et al\\.", "year": 2017}, {"title": "Tweetcred: Real-time credibility assessment of content on twitter", "author": ["Aditi Gupta", "Ponnurangam Kumaraguru", "Carlos Castillo", "Patrick Meier."], "venue": "SocInfo. pages 228\u2013243. https://doi.org/10.1007/978-3-319-13734-6 16.", "citeRegEx": "Gupta et al\\.,? 2014", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "Tweets and truth: Journalism as a discipline of collaborative verification", "author": ["Alfred Hermida."], "venue": "Journalism Practice 6(5-6):659\u2013668.", "citeRegEx": "Hermida.,? 2012", "shortCiteRegEx": "Hermida.", "year": 2012}, {"title": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM", "author": ["Elena Kochkina", "Maria Liakata", "Isabelle Augenstein."], "venue": "Proceedings of SemEval. ACL.", "citeRegEx": "Kochkina et al\\.,? 2017", "shortCiteRegEx": "Kochkina et al\\.", "year": 2017}, {"title": "Detecting misinformation in online social networks using cognitive psychology", "author": ["KP Krishna Kumar", "G Geethakumari."], "venue": "Human-centric Computing and Information Sciences 4(1):1\u201322.", "citeRegEx": "Kumar and Geethakumari.,? 2014", "shortCiteRegEx": "Kumar and Geethakumari.", "year": 2014}, {"title": "Real-time rumor debunking on twitter", "author": ["Xiaomo Liu", "Armineh Nourbakhsh", "Quanzhi Li", "Rui Fang", "Sameena Shah."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, pages 1867\u2013", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Classifying tweet level judgements of rumours in social media", "author": ["Michal Lukasik", "Trevor Cohn", "Kalina Bontcheva."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. volume 2, pages 2590\u20132595.", "citeRegEx": "Lukasik et al\\.,? 2015", "shortCiteRegEx": "Lukasik et al\\.", "year": 2015}, {"title": "SemEval-2016 Task 6: Detecting Stance in Tweets", "author": ["Saif M Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry."], "venue": "Proceedings of the Workshop on Semantic Evaluation.", "citeRegEx": "Mohammad et al\\.,? 2016", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "Semeval-2015 task 3: Answer selection", "author": ["Alessandro Moschitti", "Preslav Nakov", "Llu\u0131s Marquez", "Walid Magdy", "James Glass", "Bilal Randeree"], "venue": null, "citeRegEx": "Moschitti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moschitti et al\\.", "year": 2015}, {"title": "Cantijoch. 2013a. Reading the riots: What were the Police doing on Twitter? Policing and Society 23(4):413\u2013436", "author": ["Rob Procter", "Jeremy Crump", "Susanne Karstedt", "Alex Voss", "Marta"], "venue": null, "citeRegEx": "Procter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Reading the riots on twitter: methodological innovation for the analysis of big data", "author": ["Rob Procter", "Farida Vis", "Alex Voss."], "venue": "International journal of social research methodology 16(3):197\u2013214.", "citeRegEx": "Procter et al\\.,? 2013b", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Rumor has it: Identifying misinformation in microblogs", "author": ["Vahed Qazvinian", "Emily Rosengren", "Dragomir R Radev", "Qiaozhu Mei."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Qazvinian et al\\.,? 2011", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2011}, {"title": "Hoaxy: A platform for tracking online misinformation", "author": ["Chengcheng Shao", "Giovanni Luca Ciampaglia", "Alessandro Flammini", "Filippo Menczer."], "venue": "arXiv preprint arXiv:1603.01511 .", "citeRegEx": "Shao et al\\.,? 2016", "shortCiteRegEx": "Shao et al\\.", "year": 2016}, {"title": "IITP at SemEval-2017 Task 8: A Supervised Approach for Rumour Evaluation", "author": ["Vikram Singh", "Sunny Narayan", "Md Shad Akhtar", "Asif Ekbal", "Pushpak Bhattacharya."], "venue": "Proceedings of SemEval. ACL.", "citeRegEx": "Singh et al\\.,? 2017", "shortCiteRegEx": "Singh et al\\.", "year": 2017}, {"title": "DFKI-DKT at SemEval2017 Task 8: Rumour Detection and Classification using Cascading Heuristics", "author": ["Ankit Srivastava", "Rehm Rehm", "Julian Moreno Schneider."], "venue": "Proceedings of SemEval. ACL.", "citeRegEx": "Srivastava et al\\.,? 2017", "shortCiteRegEx": "Srivastava et al\\.", "year": 2017}, {"title": "Big questions for social media big data: Representativeness, validity and other methodological pitfalls", "author": ["Zeynep Tufekci."], "venue": "Proceedings of the AAAI International Conference on Weblogs and Social Media.", "citeRegEx": "Tufekci.,? 2014", "shortCiteRegEx": "Tufekci.", "year": 2014}, {"title": "A work-in-process literature review: Incorporating social media in risk and crisis communication", "author": ["Shari R Veil", "Tara Buehner", "Michael J Palenchar."], "venue": "Journal of contingencies and crisis management 19(2):110\u2013122.", "citeRegEx": "Veil et al\\.,? 2011", "shortCiteRegEx": "Veil et al\\.", "year": 2011}, {"title": "ECNU at SemEval-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models", "author": ["Feixiang Wang", "Man Lan", "Yuanbin Wu."], "venue": "Proceedings of SemEval. ACL.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Automatic detection of rumor on social network", "author": ["Qiao Zhang", "Shuiyuan Zhang", "Jian Dong", "Jinhua Xiong", "Xueqi Cheng."], "venue": "Natural Language Processing and Chinese Computing, Springer, pages 113\u2013122.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Detection and resolution of rumours in social media: A survey", "author": ["Arkaitz Zubiaga", "Ahmet Aker", "Kalina Bontcheva", "Maria Liakata", "Rob Procter."], "venue": "arXiv preprint arXiv:1704.00656 .", "citeRegEx": "Zubiaga et al\\.,? 2017", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2017}, {"title": "Crowdsourcing the annotation of rumourous conversations in social media", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Kalina Bontcheva", "Peter Tolmie."], "venue": "Proceedings of the 24th International Conference on World Wide Web: Companion", "citeRegEx": "Zubiaga et al\\.,? 2015a", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}, {"title": "Towards detecting rumours in social media", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Kalina Bontcheva", "Peter Tolmie."], "venue": "Proceedings of the AAAI Workshop on AI for Cities.", "citeRegEx": "Zubiaga et al\\.,? 2015b", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}, {"title": "PHEME rumour scheme dataset: Journalism use case", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Peter Tolmie."], "venue": "doi:10.6084/m9.figshare.2068650.v1.", "citeRegEx": "Zubiaga et al\\.,? 2016a", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}, {"title": "Analysing how people orient to and spread rumours in social media", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Peter Tolmie"], "venue": null, "citeRegEx": "Zubiaga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "After initial work (Qazvinian et al., 2011), increasingly advanced systems and annotation schemas have been developed to", "startOffset": 19, "endOffset": 43}, {"referenceID": 9, "context": "support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016b).", "startOffset": 58, "endOffset": 150}, {"referenceID": 23, "context": "support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016b).", "startOffset": 58, "endOffset": 150}, {"referenceID": 17, "context": "support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016b).", "startOffset": 58, "endOffset": 150}, {"referenceID": 7, "context": "Critically, based on recent work the task appears deeply nuanced and very challenging, while having important applications in, for example, journalism and disaster mitigation (Hermida, 2012; Procter et al., 2013a; Veil et al., 2011).", "startOffset": 175, "endOffset": 232}, {"referenceID": 21, "context": "Critically, based on recent work the task appears deeply nuanced and very challenging, while having important applications in, for example, journalism and disaster mitigation (Hermida, 2012; Procter et al., 2013a; Veil et al., 2011).", "startOffset": 175, "endOffset": 232}, {"referenceID": 26, "context": "We define a rumour as a \u201ccirculating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient scepticism and/or anxiety so as to motivate finding out the actual truth\u201d (Zubiaga et al., 2015b).", "startOffset": 216, "endOffset": 239}, {"referenceID": 15, "context": "in the analysis of the surrounding discourse is to determine how other users in social media regard the rumour (Procter et al., 2013b).", "startOffset": 111, "endOffset": 134}, {"referenceID": 12, "context": "We note that superficially this subtask may bear similarity to SemEval-2016 Task 6 on stance detection from tweets (Mohammad et al., 2016), where participants are asked to determine whether a tweet is in favour, against or neither, of a given target entity (e.", "startOffset": 115, "endOffset": 138}, {"referenceID": 0, "context": "This is more closely aligned with stance classification as defined in other domains, such as public debates (Anand et al., 2011).", "startOffset": 108, "endOffset": 128}, {"referenceID": 13, "context": "Task 3 on Answer Selection in Community Question Answering (Moschitti et al., 2015), where the task was to determine the quality of responses in", "startOffset": 59, "endOffset": 83}, {"referenceID": 14, "context": "Each tweet in the tree-structured thread is categorised into one of the following four categories, following Procter et al. (2013b):", "startOffset": 109, "endOffset": 132}, {"referenceID": 11, "context": "Prior work in the area has found the task difficult, compounded by the variety present in language use between different stories (Lukasik et al., 2015; Zubiaga et al., 2017).", "startOffset": 129, "endOffset": 173}, {"referenceID": 24, "context": "Prior work in the area has found the task difficult, compounded by the variety present in language use between different stories (Lukasik et al., 2015; Zubiaga et al., 2017).", "startOffset": 129, "endOffset": 173}, {"referenceID": 10, "context": "In the first case \u2013 the closed variant \u2013 the veracity of a rumour had to be predicted solely from the tweet itself (for example (Liu et al., 2015) rely only on the content of tweets to assess the veracity of tweets in real time, while systems such as Tweet-Cred (Gupta et al.", "startOffset": 128, "endOffset": 146}, {"referenceID": 6, "context": ", 2015) rely only on the content of tweets to assess the veracity of tweets in real time, while systems such as Tweet-Cred (Gupta et al., 2014) follow a tweet level analysis for a similar task where the credibility of a tweet is predicted).", "startOffset": 123, "endOffset": 143}, {"referenceID": 26, "context": "Identifying the veracity of claims made on the web is an increasingly important task (Zubiaga et al., 2015b).", "startOffset": 85, "endOffset": 108}, {"referenceID": 15, "context": "Decision support, digital journalism and disaster response already rely on picking out such claims (Procter et al., 2013b).", "startOffset": 99, "endOffset": 122}, {"referenceID": 20, "context": "tion around them, we take data from the \u201cmodel organism\u201d of social media, Twitter (Tufekci, 2014).", "startOffset": 82, "endOffset": 97}, {"referenceID": 3, "context": "of the PHEME project (Derczynski and Bontcheva, 2014), in which the task organisers are partners.", "startOffset": 21, "endOffset": 53}, {"referenceID": 27, "context": "This dataset is already publicly available (Zubiaga et al., 2016a) and constitutes the training and development data.", "startOffset": 43, "endOffset": 66}, {"referenceID": 25, "context": "The methodology for performing this crowdsourced annotation process has been previously assessed and validated (Zubiaga et al., 2015a), and is further detailed in (Zubiaga et al.", "startOffset": 111, "endOffset": 134}, {"referenceID": 25, "context": "For Subtask A, the extra annotation for support / deny / question / comment at the tweet level within the conversations were performed through crowdsourcing \u2013 as performed to satisfactory quality already with the existing training data (Zubiaga et al., 2015a).", "startOffset": 236, "endOffset": 259}, {"referenceID": 8, "context": "We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc\u0131\u0301a Lozano et al., 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al.", "startOffset": 81, "endOffset": 277}, {"referenceID": 1, "context": "We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc\u0131\u0301a Lozano et al., 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al.", "startOffset": 81, "endOffset": 277}, {"referenceID": 19, "context": "We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc\u0131\u0301a Lozano et al., 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al.", "startOffset": 81, "endOffset": 277}, {"referenceID": 22, "context": "We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc\u0131\u0301a Lozano et al., 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al.", "startOffset": 81, "endOffset": 277}, {"referenceID": 18, "context": "We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc\u0131\u0301a Lozano et al., 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al.", "startOffset": 81, "endOffset": 277}, {"referenceID": 2, "context": "We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc\u0131\u0301a Lozano et al., 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al.", "startOffset": 81, "endOffset": 277}, {"referenceID": 4, "context": "We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc\u0131\u0301a Lozano et al., 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al.", "startOffset": 81, "endOffset": 277}, {"referenceID": 19, "context": ", 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour", "startOffset": 119, "endOffset": 231}, {"referenceID": 22, "context": ", 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour", "startOffset": 119, "endOffset": 231}, {"referenceID": 18, "context": ", 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour", "startOffset": 119, "endOffset": 231}, {"referenceID": 2, "context": ", 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour", "startOffset": 119, "endOffset": 231}, {"referenceID": 4, "context": ", 2017; Enayet and El-Beltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour", "startOffset": 119, "endOffset": 231}], "year": 2017, "abstractText": "Media is full of false claims. Even Oxford Dictionaries named \u201cpost-truth\u201d as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics \u2013 each having their own families of claims and replies \u2013 and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.", "creator": "LaTeX with hyperref package"}}}