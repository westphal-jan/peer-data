{"id": "1704.05973", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Call Attention to Rumors: Deep Attention Based Recurrent Neural Networks for Early Rumor Detection", "abstract": "The proliferation of social media in communication and information dissemination has made it an ideal platform for spreading rumors. Automatically debunking rumors at their stage of diffusion is known as \\textit{early rumor detection}, which refers to dealing with sequential posts regarding disputed factual claims with certain variations and highly textual duplication over time. Thus, identifying trending rumors demands an efficient yet flexible model that is able to capture long-range dependencies among postings and produce distinct representations for the accurate early detection. However, it is a challenging task to apply conventional classification algorithms to rumor detection in earliness since they rely on hand-crafted features which require intensive manual efforts in the case of large amount of posts. This paper presents a deep attention model on the basis of recurrent neural networks (RNN) to learn \\textit{selectively} temporal hidden representations of sequential posts for identifying rumors. The proposed model delves soft-attention into the recurrence to simultaneously pool out distinct features with particular focus and produce hidden representations that capture contextual variations of relevant posts over time. Extensive experiments on real datasets collected from social media websites demonstrate that (1) the deep attention based RNN model outperforms state-of-the-arts that rely on hand-crafted features; (2) the introduction of soft attention mechanism can effectively distill relevant parts to rumors from original posts in advance; (3) the proposed method detects rumors more quickly and accurately than competitors.", "histories": [["v1", "Thu, 20 Apr 2017 01:22:57 GMT  (2366kb,D)", "http://arxiv.org/abs/1704.05973v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["tong chen", "lin wu", "xue li", "jun zhang", "hongzhi yin", "yang wang"], "accepted": false, "id": "1704.05973"}, "pdf": {"name": "1704.05973.pdf", "metadata": {"source": "META", "title": "Call Attention to Rumors: Deep Attention Based Recurrent Neural Networks for Early Rumor Detection", "authors": ["Tong Chen", "Lin Wu", "Xue Li", "Jun Zhang", "Hongzhi Yin", "Yang Wang"], "emails": ["tong.chen@uq.edu.au", "lin.wu@uq.edu.au", "xue.li@itee.uq.edu.au", "jun.zhang@deakin.edu.au", "h.yin1@uq.edu.au", "wangy@cse.unsw.edu.au", "@mantrackerone:", "@TheHamptonKid:", "@Evertxn:"], "sections": [{"heading": null, "text": "Key concepts Early detection of rumors, recurrent neural networks, deep-a-ention models"}, {"heading": "1 INTRODUCTION", "text": "The explosive use of contemporary social media in communications has seen the spread of rumors that can pose a threat to cyber security and social stability."}, {"heading": "1.1 Challenges and Our Approach", "text": "In summary, there are three challenges in early detection of rumors that need to be addressed: (1) automatically learning representations of rumors instead of using labor-intensive hand-picked features; (2) the need to maintain the extensive dependence between variable-length post series to build their internal representations; (3) the problem of high duplication, exacerbated by different contextual focal points. To address these challenges, we propose a novel deep neural network (RNN) for early detection of rumors, namely CallAtRumors (Call Attention to Rumors). Overview of our framework is illustrated in Figure 2. Our model processes disperse text sequences that are constructed into a set of feature matrices by coding contextual information from posts related to an event."}, {"heading": "1.2 Contributions", "text": "The main contributions of our work are summarized as follows: \u2022 We propose a profound model that learns to automatically perform rumor detection in the morning. e model is based on RNN and is able to learn continuous hidden representations by capturing long-term dependencies and contextual variations of posting series. \u2022 e deterministic so-a de-entitlement mechanisms are embedded in repetition to allow for a unique feature extraction from high duplication and advanced meaning focus that varies over time. \u2022 We quantitatively validate the effectiveness of an entity in terms of detection accuracy and seriousness by comparing it with the state of the art on two real social media datasets: Twi er and Weibo.e The rest of the paper is organized as follows. Section 2 and Section 3 represent the relationship to existing work and preparatory work with RNN. We introduce the main intuition, formulating a section 4. in Section 3."}, {"heading": "2 RELATEDWORK", "text": "Our work is closely linked to the early detection of rumors and a mechanism for detecting rumors, and we will explore these two aspects in more detail in this section."}, {"heading": "2.1 Early Rumor Detection", "text": "The problem of detection of rumors [4] can be considered a binary classification task. Extraction and selection of discriminatory characteristics clearly influences the performance of the classifier. Hu et al. rst conducted a study to analyze the sensation differences between spammers and normal users and then presented an optimization formulation that incorporates sentiment information into a novel framework for detecting rumors [10]. Also, the distribution structure of rumors was developed by Wu et al. using a message propagation tree in which each node represents a text message to classify whether the root of the tree is a rumor or or not [37]. In [18] a dynamic time series structure was proposed to capture the temporal characteristics based on the context information generated in each lifecycle of a rumor. However, these approaches require powerful manual measures in the feature engineering and rumors may be limited by the data structure of their early detection [49]."}, {"heading": "2.90 0.75 0 \u2026", "text": "Contributions about One Event Textual Feature Extraction Deep Attention Based Neural Network Latent Additional Classified asRepresentations Hidden Rumor orLayer Non-rumor0 1.15 0.97... 3.14 0...... 2.46 0.38 0... but informative question phrases play an important role in feature engineering when combined with clusters and a class on the clusters, as they shorten the time for spewing rumors. Manually modified features have shown their importance in the research of real-time rumor debunking by Liu et al. [16]. In contrast, Wu et al. proposed a sparse learning method to automatically select discriminatory features and train the class for emerging rumors [39]. As these methods neglect the temporal property of social media data, a time series-based feature structure [18] is introduced to capture context variations over time."}, {"heading": "2.2 Attention Mechanism", "text": "As an emerging technique for natural language processing (NLP) problems [22, 28, 46], Bahdanau et al. extended the basic encoder architecture of neural machine translation by an encoder mechanism that allows the model to automatically search for parts of a source set relevant to predicting a target word [2], achieving comparable performance in the translation task between English and French. Vinyals et al. improved the encoder model in [2] so that their model calculated an encoder vector that determines how much an extension should be placed over the input words, and thus increased performance in large-scale translations [29]. Furthermore, Sharma et al. applied a so-max function [25] to the hidden states of the LSTM layer (Long Short-Term Memory), recognizing more valuable elements in sequencing actions."}, {"heading": "3 RECURRENT NEURAL NETWORKS", "text": "Recursive neural networks, or RNNs [23], are a family of feedforward neural networks for processing sequential data, such as a sequence of values x1,..., x\u03c4. RNNs process one input sequence after another, update the hidden units ht, a \"state vector\" that implicitly contains information about the history of all past elements of the sequence (x < t), and generate an output vector ot [13]. e Forward propagation begins with a speculation of the initial state h0, then for each time step t = 1 to t = \u03c4 the following update equations are applied [6]: ht = tanh (Uxt + Wht \u2212 1 + b), ot = Vht + c, (1) where the parameters U, V and W are weight matrices for the input tohidden, hidden-to-hidden-boiled connections."}, {"heading": "4 CALLATRUMORS: EARLY RUMOR DETECTIONWITH DEEP ATTENTION BASED RNN", "text": "In this section, we present the details of our framework with deep insight to classify social textual events into rumor and non-rumor. First, we present a strategy that converts incoming streams of social contributions into continuous, variable time series. We describe the so-called evolution mechanism that can be embedded in recurring neural networks to focus on selective textual cues to learn different representations for the rumor and / or non-rumor binary classification."}, {"heading": "4.1 Problem Statement", "text": "On the other hand, an assertion is usually associated with a number of posts relevant to the assertion. these relevant posts to an assertion can easily be collected to more accurately describe the central content. Therefore, we are interested in identifying rumors on an overall level, rather than identifying each individual post [17]. In other words, we focus on tracking rumors at the event level, summarizing successive posts on the same topics into one event, and our model determines whether the event is a rumor or or not. Let E = {egg} designate a series of specific events, with each event Ei = {(pi, j, ti, j)} nij = 1 consisting of all relevant posts, each with the timestamp ti, j, and the task is to classify each event as a rumor or or not."}, {"heading": "4.2 Constructing Variable-Length Post Series", "text": "Within each event, posts are divided into time intervals, each of which is considered a batch. This is because it is not practical to treat each post individually in the large number scale. To ensure a similar word density for each time step within an event, we group items in batches according to a specified amount of mail N rather than cutting the event span evenly. Algorithm 1 describes the construction of variable post series. Specifically, for each event, Ei = {(pi, j, ti, j)} nij = 1, post series are constructed with variable length of posts relevant to the respective events. We use a minimum word length to maintain the sequential property for all events. For events that contain no less than N-min posts, we iteratively take the first N posts from the egg and feed them into a time interval."}, {"heading": "4.3 Long Short-Term Memory (LSTM) with Deterministic So Attention Mechanism", "text": "To capture the long-term time dependencies between the continuous time series, we use Long Short-Term Memory (LSTM) unit [7, 44, 48] to capture high-level discriminatory representations for rumors; thus, the structure of LSTM is asit = \u03c3 (Uiht \u2212 1 + Wixt + Vict \u2212 1 + bi), ft = \u03c3 (Uf ht \u2212 1 + Wf xt + Vf ct \u2212 1 + bf ct), ct = ftct \u2212 1 + it tanh (Uht \u2212 1 + Wcxt + bc), ot = \u03c3 (Uoht \u2212 1 + Woxt + bo), ht = ot tanh (ct), (2) where the logistic sigmoid function is, and it, ot, ct are the input gates, forget gate, output gate and cell input vector, respectively."}, {"heading": "4.4 Loss Function and Model Training", "text": "In model building, we use a lateral entropy loss coupled with the double stochastic regularization [45], which encourages the model to impose an additional constraint on each element of the input word matrix, so that the loss function works as follows: L = \u2212 Looking t = 1 c = 1 yt, i Log y t, i + \u03bb K \u2211 i = 1 (1 \u2212 Looking t = 1 at, i) 2 + 2, (5) where yt is the only hot label vector, y \u0448t is the vector of the probabilities of the binary class at the time of the timestamp, \u03c4 is the total number of timestamps, C = 2 is the number of output classes (rumors or non-rumors), \u03bb is the a penalty coe cient, q is the weight decay coefficient, and vice versa represents all model parameters."}, {"heading": "5 EXPERIMENTS", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "5.2 Self Evaluations", "text": "All parameters are set by cross-validation. To generate variable length input of the mail series, we set the number of posts N for each time step as 2www.twi er.com 3www.weibo.com 4www.snopes.com 5h p: / / service.account.weibo.com 6 e web page in this file downloaded from h p: / / www.snopes.com / hillaryclinton-accidentally-gave-isis-400-million / 7h p: / deeplearning.net / so ware / theano / 50 and the minimum row length. We chose K = 10,000 spikes for constructing tf-idf matrices. Apart from lowercasing, we do not use any other special pre-processing such as containment [2]. e recurring neural networks with an de-tification mechanism can automatically learn to ignore these unimportant or irrelevant expressions in the training process."}, {"heading": "5.3 Settings and Baselines", "text": "We evaluate the effectiveness and frequency of CallAtRumors by comparing it with the following state-of-the-art approaches in terms of precision, recall and F-measurement. \u2022 DT-Rank [49]: is a decision-tree-based ranking model, and is able to identify trending rumors by re-evaluating the problem as an entire cluster of posts whose subject is a controversial factual claim. We implement their query phrases and features to make them comparable to our method. \u2022 SVM-TS [18]: is an SVM (Support Vector Machine) model that uses time series structures to capture the variation of social context characteristics. SVM-TS can use the temporal characteristics of these characteristics based on the time series of rumor-based life cycles with time series modeling techniques that are applied to integrate carious social contexts."}, {"heading": "5.4 E ectiveness Validation", "text": "Table 2 and Table 3 show the performance of all approaches in the Twi er dataset and Weibo dataset, respectively. DT-Rank cannot clearly distinguish between rumors and normal events when double-content and conciseness datasets are sighted in textual characteristics. LK-RBF and SVM-TS achieve better results, indicating the feature engineering's ability to help detect rumors. However, both LK-RBF and SVM-TS show a lack of adequate memory, which shows how sensitive models are to rumors. By collectively selecting discriminatory features and training the topic-independent class based on selected characteristics, CERT achieves better results than the previous three approaches in our datasets. e ML-GRU is competitive both in terms of precision and recall due to its ability to process sequential data and determine hidden states from raw data."}, {"heading": "5.5 More Comparison with the State-of-the-art:", "text": "CERT [39] To show how the conditions of the datasets affect the performance of rumor detection, we compare the performance of CallAtRumors with that of CERT using different datasets. To reproduce the same experimental conditions as [39], we have also organized a sample dataset based on the criteria described in the thesis. We use queries generated from 220 reported rumors of snopes and regular expressions on the same topics to search through 7,580 tweets and manually label each tweet by reading the contents and referring to the Snopes article. Finally, we lead to a sample dataset containing 1,193 rumor tweets and 6,387 non-rumor tweets, which also have a similar ratio of rumor to non-rumor as the dataset in [39]. Table 4 shows the results when CallAtRumors and CERT are applied to this sample dataset and our Ter datasets."}, {"heading": "5.6 Earliness Analysis", "text": "In this experiment, we examine the property of our approach in its early detection. In order to have a fair comparison, we allow outgoing methods for detecting rumors to be trained on rumors to be evaluated. Broadly speaking, by adding training data in chronological order, we are able to estimate the time in which our method can detect emerging rumors. E-results for early detection are presented in Figure 7. At an early stage, with training data ranging from 10% to 60%, CallAtRumor outperforms four comparison methods by a significant margin. In particular, when compared to the most relevant method of ML-GRU, since the data share is between 10% and 20%, CallAtRumor's ML-GRU outperforms all data in terms of accuracy by 5% and 4% when retrieving both Twier and Weibo datasets. e result shows that a detection mechanism in early detection of ML-GRU is more effective, as the data ratio of 10% to 20% is applied to the most distinct features in the report."}, {"heading": "6 CONCLUSION", "text": "In this paper, we introduce CallAtRumors, a novel recurring neural network model based on a so-called de-tification mechanism for automatic detection of rumors by learning latent representations from successive social contributions. We have conducted experiments with state-of-the-art methods for detecting rumors to demonstrate that CallAtRumors responds sensitively to distinguishable words, outperforming competitors even when textual features are sparse in the early stages of a rumor. Furthermore, we demonstrate our model's ability to handle duplicate data with further comparison. In our future work, it would be attractive to explore the possibility of combining more complex features with our deep de-tification model. For example, we can model the propagation of rumors as sequential inputs for RNNs to improve detection accuracy."}], "references": [{"title": "Learning wake-sleep recurrent a\u008aention models", "author": ["Jimmy Ba", "Roger Grosse", "Ruslan Salakhutdinov", "Brendan Frey"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning long-term dependencies with gradient decent is di\u0081cult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Information credibility on twi\u008aer", "author": ["Carlos Castillo", "Marcelo Mendoza", "Barbara Poblete"], "venue": "In Proceedings of the 20th international conference on World wide web", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bo\u008aou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Gre", "Rupesh K Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE transactions on neural networks and learning systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural computation", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Social spammer detection with sentiment information", "author": ["Xia Hu", "Jiliang Tang", "Huiji Gao", "Huan Liu"], "venue": "In Data Mining (ICDM),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Prominent features of rumor propagation in online social media", "author": ["Sejeong Kwon", "Meeyoung Cha", "Kyomin Jung", "Wei Chen", "Yajun Wang"], "venue": "In Data Mining (ICDM),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Mining of massive datasets", "author": ["Jure Leskovec", "Anand Rajaraman", "Je\u0082rey David Ullman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Presenting diverse location views with real-time near-duplicate photo elimination", "author": ["Jiajun Liu", "Zi Huang", "Hong Cheng", "Yueguo Chen", "Heng Tao Shen", "Yanchun Zhang"], "venue": "In ICDE", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Real-time rumor debunking on twi\u008aer", "author": ["Xiaomo Liu", "Armineh Nourbakhsh", "\u008banzhi Li", "Rui Fang", "Sameena Shah"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Detecting rumors from microblogs with recurrent neural networks", "author": ["Jing Ma", "Wei Gao", "Prasenjit Mitra", "Sejeong Kwon", "Bernard J Jansen", "Kam-Fai Wong", "Meeyoung Cha"], "venue": "Proceedings of IJCAI", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Detect rumors using time series of social context information on microblogging websites", "author": ["Jing Ma", "Wei Gao", "Zhongyu Wei", "Yueming Lu", "Kam-Fai Wong"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Recurrent models of visual a\u008aention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Collective opinion spam detection: Bridging review networks and metadata", "author": ["Shebuti Rayana", "Leman Akoglu"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Collective opinion spam detection using active inference", "author": ["Shebuti Rayana", "Leman Akoglu"], "venue": "In Proceedings of the 2016 SIAM International Conference on Data Mining", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Reasoning about entailment with neural a\u008aention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenste\u008ae", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geo\u0082rey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling 5,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Leveraging the Implicit Structure within Social Media for Emergent Rumor Detection", "author": ["Justin Sampson", "Fred Morsta\u008aer", "Liang Wu", "Huan Liu"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Action recognition using visual a\u008aention", "author": ["Shikhar Sharma", "Ryan Kiros", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.04119", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Multiple feature hashing for real-time large scale near-duplicate video retrieval", "author": ["Jingkuan Song", "Yi Yang", "Zi Huang", "Heng Tao Shen", "Richang Hong"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from over\u0080\u008aing", "author": ["Nitish Srivastava", "Geo\u0082rey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "\u008boc V Le"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geo\u0082rey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "E\u0082ective Multi-\u008bery Expansions: Robust Landmark Retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "In ACM Multimedia", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "E\u0082ective Multi-\u008bery Expansions: Collaborative Deep Networks for Robust Landmark Retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "IEEE Trans. Image Processing 26,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "LBMCH: Learning Bridging Mapping for Cross-modal Hashing", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang"], "venue": "In ACM SIGIR", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Robust Subspace Clustering for Multi-View Data by Exploiting Correlation Consensus", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang", "Xiaodi Huang"], "venue": "IEEE Trans. Image Processing 24,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Towards metric fusion on multi-view data: a cross-view based graph random walk approach", "author": ["Yang Wang", "Xuemin Lin", "Qing Zhang"], "venue": "In ACM CIKM", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Shi\u0089ing Hypergraphs by Probabilistic Voting", "author": ["Yang Wang", "Xuemin Lin", "Qing Zhang", "Lin Wu"], "venue": "In PAKDD", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Unsupervised Metric Fusion Over Multiview Data by Graph Random Walk-Based Cross-View Di\u0082usion", "author": ["Yang Wang", "Wenjie Zhang", "Lin Wu", "Xuemin Lin", "Xiang Zhao"], "venue": "IEEE Trans. Neural Netw. Learning Syst 28,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2017}, {"title": "False rumors detection on sina weibo by propagation structures", "author": ["Ke Wu", "Song Yang", "Kenny Q Zhu"], "venue": "In Data Engineering (ICDE),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Geo-location estimation from two shadow trajectories", "author": ["Lin Wu", "Xiaochun Cao"], "venue": "In CVPR", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Gleaning Wisdom from the Past: Early Detection of Emerging Rumors in Social Media", "author": ["Liang Wu", "Jundong Li", "Xia Hu", "Huan Liu"], "venue": "In SDM", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Deep linear discriminant analysis on \u0080sher networks: A hybrid architecture for person reidenti\u0080cation", "author": ["Lin Wu", "Chunhua Shen", "Anton van den Hengel"], "venue": "Pa\u0088ern Recognition", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Robust hashing for multi-view data: Jointly learning low-rank kernelized similarity consensus and hash functions", "author": ["Lin Wu", "Yang Wang"], "venue": "Image Vision Comput", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2017}, {"title": "Exploiting A\u008aribute Correlations: A Novel Trace Lasso based Weakly Supervised Dictionary Learning Method", "author": ["Lin Wu", "Yang Wang", "Shirui Pan"], "venue": "IEEE Trans. Cybernetics", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "E\u0081cient image and tag co-ranking: a bregman divergence optimization method", "author": ["Lin Wu", "Yang Wang", "John Shepherd"], "venue": "In ACM Multimedia", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Answer Selection in Community \u008bestion Answering via A\u008aentive Neural Networks", "author": ["Yang Xiang", "Qingcai Chen", "Xiaolong Wang", "Yang Qin"], "venue": "IEEE Signal Processing Le\u0088ers 24,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2017}, {"title": "Show, A\u008aend and Tell: Neural Image Caption Generation with Visual A\u008aention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Hierarchical a\u008aention networks for document classi\u0080cation", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "10 bits of surprise: Detecting malicious users with minimum information", "author": ["Reza Zafarani", "Huan Liu"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Enquiring minds: Early detection of rumors in social media from enquiry posts", "author": ["Zhe Zhao", "Paul Resnick", "Qiaozhu Mei"], "venue": "In Proceedings of the 24th International Conference on World Wide Web", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "E\u0082ective and E\u0081cient Global Context Veri\u0080cation for Image Copy Detection", "author": ["Zhili Zhou", "Yunlong Wang", "Q.M. Jonathan Wu", "Ching-Nung Yang", "Xingming Sun"], "venue": "IEEE Transactions on Information Forensics and Security 12,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2017}, {"title": "Brand-Related Twi\u008aer Sentiment Analysis Using Feature Engineering and the Dynamic Architecture for Arti\u0080cial Neural Networks", "author": ["David Zimbra", "M Ghiassi", "Sean Lee"], "venue": "In System Sciences (HICSS),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}], "referenceMentions": [{"referenceID": 48, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 64, "endOffset": 68}, {"referenceID": 27, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 83, "endOffset": 95}, {"referenceID": 28, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 83, "endOffset": 95}, {"referenceID": 44, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 83, "endOffset": 95}, {"referenceID": 10, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 13, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 15, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 31, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 33, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 34, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 17, "context": "Embedding social graphs into a classi\u0080cation model also helps distinguish malicious user comments from normal ones [20, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "Embedding social graphs into a classi\u0080cation model also helps distinguish malicious user comments from normal ones [20, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 46, "context": "1 (b), users\u2019 posts exhibit high duplication in their textual phrases due to the repeated forwarding, reviews, and/or inquiry behavior [49].", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "One exception is [17] where Ma et al.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "Although some studies on duplication detection are available and e\u0082ective in di\u0082erent tasks [15, 26, 50], these approaches are not applicable in our case where the duplication cannot be determined beforehand but is rather varied across post series over time.", "startOffset": 92, "endOffset": 104}, {"referenceID": 23, "context": "Although some studies on duplication detection are available and e\u0082ective in di\u0082erent tasks [15, 26, 50], these approaches are not applicable in our case where the duplication cannot be determined beforehand but is rather varied across post series over time.", "startOffset": 92, "endOffset": 104}, {"referenceID": 47, "context": "Although some studies on duplication detection are available and e\u0082ective in di\u0082erent tasks [15, 26, 50], these approaches are not applicable in our case where the duplication cannot be determined beforehand but is rather varied across post series over time.", "startOffset": 92, "endOffset": 104}, {"referenceID": 0, "context": "Our framework is premised on the RNNs which are proved to be e\u0082ective in recent machine learning tasks [1, 7] in handling sequential data.", "startOffset": 103, "endOffset": 109}, {"referenceID": 5, "context": "Our framework is premised on the RNNs which are proved to be e\u0082ective in recent machine learning tasks [1, 7] in handling sequential data.", "startOffset": 103, "endOffset": 109}, {"referenceID": 3, "context": "1 Early Rumor Detection \u008ce problem of rumor detection [4] can be cast as binary classi\u0080cation tasks.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "\u0080rst conducted a study to analyze the sentiment di\u0082erences between spammers and normal users and then presented an optimization formulation that incorporates sentiment information into a novel social spammer detection framework [10].", "startOffset": 228, "endOffset": 232}, {"referenceID": 34, "context": "through utilizing a message propagation tree where each node represents a text message to classify whether the root of the tree is a rumor or not [37].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "In [18], a dynamic time series structure was proposed to capture the temporal features based on the time series context information generated in every rumor\u2019s life-cycle.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Early rumor detection is to detect viral rumors in their formative stages in order to take early action [24].", "startOffset": 104, "endOffset": 108}, {"referenceID": 46, "context": "In [49], some very rare", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "proposed a sparse learning method to automatically select discriminative features as well as train the classi\u0080er for emerging rumors [39].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "As those methods neglect the temporal trait of social media data, a time-series based feature structure[18] is introduced to seize context variation over time.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "[17], utilizing sequential data to spontaneously capture temporal textual characteristics of rumor di\u0082usion which helps detecting rumor earlier with accuracy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "2 Attention Mechanism As a rising technique in NLP (natural language processing) problems [22, 28, 46], Bahdanau et al.", "startOffset": 90, "endOffset": 102}, {"referenceID": 25, "context": "2 Attention Mechanism As a rising technique in NLP (natural language processing) problems [22, 28, 46], Bahdanau et al.", "startOffset": 90, "endOffset": 102}, {"referenceID": 43, "context": "2 Attention Mechanism As a rising technique in NLP (natural language processing) problems [22, 28, 46], Bahdanau et al.", "startOffset": 90, "endOffset": 102}, {"referenceID": 1, "context": "extended the basic encoderdecoder architecture of neural machine translation with a\u008aention mechanism to allow the model to automatically search for parts of a source sentence that are relevant to predicting a target word [2], achieving a comparable performance in the English-to-French translation task.", "startOffset": 221, "endOffset": 224}, {"referenceID": 1, "context": "improved the a\u008aention model in [2], so their model computed an a\u008aention vector re\u0083ecting how much a\u008aention should be put over the input words and boosted the performance on large scale translation [29].", "startOffset": 31, "endOffset": 34}, {"referenceID": 26, "context": "improved the a\u008aention model in [2], so their model computed an a\u008aention vector re\u0083ecting how much a\u008aention should be put over the input words and boosted the performance on large scale translation [29].", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "applied a location so\u0089max function [25] to the hidden states of the LSTM (Long Short-Term Memory) layer, thus recognizing more valuable elements in sequential inputs for action recognition.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "3 RECURRENT NEURAL NETWORKS Recurrent neural networks, or RNNs [23], are a family of feedforward neural networks for processing sequential data, such as a sequence of values x1, .", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "\u008ce gradient computation of RNNs involves performing backpropagation through time (BPTT) [23].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "In practice, a standard RNN is di\u0081cult to be trained due to the well-known vanishing or exploding gradients caused by the incapability of RNN in capturing the long-distance temporal dependencies for the gradient based optimization [3, 40].", "startOffset": 231, "endOffset": 238}, {"referenceID": 37, "context": "In practice, a standard RNN is di\u0081cult to be trained due to the well-known vanishing or exploding gradients caused by the incapability of RNN in capturing the long-distance temporal dependencies for the gradient based optimization [3, 40].", "startOffset": 231, "endOffset": 238}, {"referenceID": 5, "context": "To tackle this training di\u0081culty, an e\u0082ective solution is to includes \u201cmemory\u201d cells to store information over time, which are known as Long Short-Term Memory (LSTM) [7, 9].", "startOffset": 166, "endOffset": 172}, {"referenceID": 7, "context": "To tackle this training di\u0081culty, an e\u0082ective solution is to includes \u201cmemory\u201d cells to store information over time, which are known as Long Short-Term Memory (LSTM) [7, 9].", "startOffset": 166, "endOffset": 172}, {"referenceID": 14, "context": "Hence, we are interested in detecting rumor on an aggregate level instead of identifying each single posts [17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Proved to be an e\u0082ective and lightweight textual feature, tf-idf is a numerical statistic that is intended to re\u0083ect how important a word is to a document in a collection or corpus [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "3 Long Short-Term Memory (LSTM) with Deterministic So\u0085 Attention Mechanism To capture the long-distance temporal dependencies among continuous time post series, we employ Long Short-Term Memory (LSTM) unit [7, 44, 48] to learn high-level discriminative representations for rumors.", "startOffset": 206, "endOffset": 217}, {"referenceID": 41, "context": "3 Long Short-Term Memory (LSTM) with Deterministic So\u0085 Attention Mechanism To capture the long-distance temporal dependencies among continuous time post series, we employ Long Short-Term Memory (LSTM) unit [7, 44, 48] to learn high-level discriminative representations for rumors.", "startOffset": 206, "endOffset": 217}, {"referenceID": 45, "context": "3 Long Short-Term Memory (LSTM) with Deterministic So\u0085 Attention Mechanism To capture the long-distance temporal dependencies among continuous time post series, we employ Long Short-Term Memory (LSTM) unit [7, 44, 48] to learn high-level discriminative representations for rumors.", "startOffset": 206, "endOffset": 217}, {"referenceID": 6, "context": "\u008ce LSTM architecture is essentially a memory cell which can maintain its state over time, and non-linear gating units can regulate the information \u0083ow into and out of the cell [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 22, "context": "\u008ce location so\u0089max [25] is thus, applied over the hidden states of the last LSTM layer to calculate at+1, the a\u008aention weight for the next input matrix dt+1:", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "A\u0089er calculating these probabilities, the so\u0085 deterministic attention mechanism [2] computes the expected value of the input at the next time step xt+1 by taking expectation over the word matrix at di\u0082erent positions:", "startOffset": 80, "endOffset": 83}, {"referenceID": 16, "context": "So\u0089 a\u008aention models are shown to be deterministic and can be trained by using back-propagation whereas hard a\u008aention models are stochastic and the training requires the REINFORCE algorithm [19] or by maximizing a variational lower bound or using importance sampling [1, 45].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "So\u0089 a\u008aention models are shown to be deterministic and can be trained by using back-propagation whereas hard a\u008aention models are stochastic and the training requires the REINFORCE algorithm [19] or by maximizing a variational lower bound or using importance sampling [1, 45].", "startOffset": 266, "endOffset": 273}, {"referenceID": 42, "context": "So\u0089 a\u008aention models are shown to be deterministic and can be trained by using back-propagation whereas hard a\u008aention models are stochastic and the training requires the REINFORCE algorithm [19] or by maximizing a variational lower bound or using importance sampling [1, 45].", "startOffset": 266, "endOffset": 273}, {"referenceID": 42, "context": "4 Loss Function and Model Training In model training, we employ cross-entropy loss coupled with the doubly stochastic regularization [45] that encourages the model to pay a\u008aention to every element of the input word matrix.", "startOffset": 133, "endOffset": 137}, {"referenceID": 14, "context": "1 Datasets We use two public datasets published by [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "It also contains 494 normal events from Snopes and two public datasets [4, 12].", "startOffset": 71, "endOffset": 78}, {"referenceID": 10, "context": "It also contains 494 normal events from Snopes and two public datasets [4, 12].", "startOffset": 71, "endOffset": 78}, {"referenceID": 14, "context": "For each event, the keywords are extracted and manually re\u0080ned until the composed queries can have precise Twi\u008aer search results [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "In addition, to balance the ration of rumors and non-romors, we follow the criteria from [17] to manually gather 4 non-rumors from Twi\u008aer and 38 rumors from Weibo to achieve a 1:1 ratio of rumors to non-rumors.", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Apart from lowercasing, we do not apply any other special preprocessing like stemming [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 24, "context": "45 and we apply a dropout [27] of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "Our model is trained by measuring the derivative of the loss through back-propagation [5] algorithm, namely the Adam optimization algorithm [11].", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "Our model is trained by measuring the derivative of the loss through back-propagation [5] algorithm, namely the Adam optimization algorithm [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 46, "context": "\u2022 DT-Rank [49]: \u008cis is a decision-tree based ranking model, and is able to identify trending rumors by recasting the problem as \u0080nding entire clusters of posts whose topic is a disputed factual claim.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "\u2022 SVM-TS [18]: \u008cis is a SVM (support vector machine) model that uses time-series structures to capture the variation of social context features.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "\u2022 LK-RBF [24]: To tackle the problem of implicit data without explicit links and jointed conversations, Sampson et al.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "\u2022 ML-GRU [17]: \u008cis method utilizes recurrent neural networks to automatically discover deep data representations for e\u0081cient rumor detection.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "\u2022 CERT [39]: \u008cis is a cross-topic emerging rumor detection model which can jointly cluster data, select features and train classi\u0080ers by using the abundant labeled data from prior rumors to facilitate the detection of an emerging rumor.", "startOffset": 7, "endOffset": 11}, {"referenceID": 36, "context": "Since CERT can jointly select discriminative features and train the topic-independent classi\u0080er with selected features [39], it achieves be\u008aer results than the former three approaches in our datasets.", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "5 More Comparison with the State-of-the-art: CERT [39] To demonstrate how the conditions of datasets a\u0082ect the performance of rumor detection, we compare the performance of CallAtRumors with CERT using di\u0082erent datasets.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "To reproduce the same experimental conditions as [39], we have also organized a sample dataset using the criteria described in the work.", "startOffset": 49, "endOffset": 53}, {"referenceID": 36, "context": "8038 Table 4: More comparison with CERT [39] on the Sample and Twitter datasets to the Snopes article.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "At last we result in a sample dataset containing 1,193 rumor Tweets and 6,387 non-rumor Tweets, which also has a similar ratio of rumors to non-rumors as the dataset in [39].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "\u008cis result is promising because the average report time over the rumors given by Snopes and Sina Community Management Center is 54 hours and 72 hours respectively [17], and we can save much manual e\u0082ort with the help of our deep a\u008aention based early rumor detection technique.", "startOffset": 163, "endOffset": 167}], "year": 2017, "abstractText": "\u008ce proliferation of social media in communication and information dissemination has made it an ideal platform for spreading rumors. Automatically debunking rumors at their stage of di\u0082usion is known as early rumor detection, which refers to dealing with sequential posts regarding disputed factual claims with certain variations and highly textual duplication over time. \u008cus, identifying trending rumors demands an e\u0081cient yet \u0083exible model that is able to capture long-range dependencies among postings and produce distinct representations for the accurate early detection. However, it is a challenging task to apply conventional classi\u0080cation algorithms to rumor detection in earliness since they rely on hand-cra\u0089ed features which require intensive manual e\u0082orts in the case of large amount of posts. \u008cis paper presents a deep a\u008aention model on the basis of recurrent neural networks (RNN) to learn selectively temporal hidden representations of sequential posts for identifying rumors. \u008ce proposed model delves so\u0089-a\u008aention into the recurrence to simultaneously pool out distinct features with particular focus and produce hidden representations that capture contextual variations of relevant posts over time. Extensive experiments on real datasets collected from social media websites demonstrate that (1) the deep a\u008aention based RNN model outperforms state-of-thearts that rely on hand-cra\u0089ed features; (2) the introduction of so\u0089 a\u008aention mechanism can e\u0082ectively distill relevant parts to rumors from original posts in advance; (3) the proposed method detects rumors more quickly and accurately than competitors.", "creator": "LaTeX with hyperref package"}}}