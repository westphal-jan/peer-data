{"id": "1704.06036", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "End-to-end representation learning for Correlation Filter based tracking", "abstract": "The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.", "histories": [["v1", "Thu, 20 Apr 2017 07:51:27 GMT  (2976kb,D)", "http://arxiv.org/abs/1704.06036v1", "To appear at CVPR 2017"]], "COMMENTS": "To appear at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jack valmadre", "luca bertinetto", "jo\\~ao f henriques", "rea vedaldi", "philip h s torr"], "accepted": false, "id": "1704.06036"}, "pdf": {"name": "1704.06036.pdf", "metadata": {"source": "CRF", "title": "End-to-end representation learning for Correlation Filter based tracking", "authors": ["Jack Valmadre", "Luca Bertinetto", "Jo\u00e3o F. Henriques", "Andrea Vedaldi", "Philip H. S. Torr"], "emails": ["name.surname@eng.ox.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "The basic challenge, however, is the lack of a-priori knowledge of the target, which can be viewed from any classroom. However, the simplest approach is to re-detect the absence of an object via a video with sole supervision of a limited recording box at the beginning of the sequence and adapt a pre-formed profound revolutionary learning method. The biggest challenge is the lack of a-priori knowledge of the target, which can be viewed from any classroom. The simplest approach is to ignore the lack of a-priori knowledge and to adapt a pre-formed profound revolutionary learning method (CNN) to the target, for example by using stochastic gradients, the workhorse of deep network optimization [32, 26, 36]. The extremely limited training data and a large number of parameters make this a difficult learning problem."}, {"heading": "2. Related work", "text": "Since the secondary work of Bolme et al. [4], the correlation filter has gained great popularity within the community. Notable efforts have been made to mitigate the effects of periodic limits [10, 16, 8] to achieve multi-resolution [22, 9] and to extend the goal with a more robust loss. To achieve simplicity, we adopt the basic formulation of correlation filters. Recently, several methods based on Siamese networks have been introduced."}, {"heading": "3. Method", "text": "Before introducing the CFNet architecture (Section 3.3), we briefly present a framework for embedding in Siamese networks (Section 3.1) and explain the use of such embedding for object tracking (Section 3.2), and then derive the terms used to evaluate and feedback on the most important new ingredient in our networks, the correlation filter layer that online learning performs in the run-up (Section 3.4)."}, {"heading": "3.1. Fully-convolutional Siamese networks", "text": "Our starting point is a network similar to that of [3], which we later modify so that the model can be interpreted as a correlation filter tracker. The fully revolutionary Siamese framework considers pairs (x, \"z\") that comprise a training image x \"and a test image z\" 2. \"The image x\" represents the object of interest (e.g. an image area centered on the target in the first video image), while z \"is typically larger and represents the search area (e.g. the next video image). Both inputs are processed by a CNN image with learnable parameters \u03c1. This results in two function cards that then correlate with each other: g\u03c1 (x,\" z \") = f\u043e (x\")? f\u0435 (z \"). (1) Equation 1 corresponds to an exhaustive search of the pattern x\" over the test image z. \"The goal is the maximum value of the response card (left-sided\" 1 \"to correspond to the target object)."}, {"heading": "3.2. Tracking algorithm", "text": "The network itself only provides a function to measure the similarity of two image fields. To apply this network to object tracking, it is necessary to combine this with a method that describes the logic of the tracker. Similar2Note that this differs from [3], in which the target object and the search area are designated z or x instead. [3] We use a simplifying tracking algorithm to assess the usefulness of the similarity function.Online tracking is performed simply by evaluating the network in forward mode. The property representation of the target object is compared with that of the search area achieved in each new frame, by extracting a window centered at the previously estimated position, with a range four times the size of the object. The new position of the object is taken as the position with the highest number of points.The original fully revolutionary network simply compares each frame with the previous appearance of the original object, and in contrast to this we compute a new one in the template."}, {"heading": "3.3. Correlation Filter networks", "text": "The resulting architecture is illustrated in Figure 1. This change can be formalized as follows: h\u03c1, s, b (x \u2032, z \u2032) = s \u03c9 (f\u03c1 (x \u2032)? f\u03c1 (z \u2032) + b (3) The CF block w = \u03c9 (x) calculates a standard CF template w from the Training Characteristics Card x = f\u03c1 (x \u2032) by solving a regression problem in the Fourier area [14]. Its effect can be understood as creating a discriminatory template that is robust to translations. It is necessary to introduce scalar parameters s and b (scale and bias) to make the score range suitable for logistic regression. Offline training is then carried out in the same way as for a Siamese network (Section 3.1), which replaces g with a cosmic template."}, {"heading": "3.4. Correlation Filter", "text": "We show how to translate gradients by the correlation filter solution efficiently and in closed form via the Fourier system. (...) We show how to reverse gradients. (...) We show how to translate gradients by the correlation filter solution efficiently and in closed form via the Fourier system. (...) We show how to reverse gradients. (...) The correlation filter solution is the template w > Rm \u00b7 m, whose inner product is as close as possible to a desired answer y with each circular shift of the image. (...) Here, U = {... 1} 2 is the domain of the image, y Rm \u00b7 m is a signal of which u-th element is y [u], and the translated Dirac delta function is [t]. (...) In this section we use gradients for denoted convolution and denotarized circuits."}, {"heading": "4. Experiments", "text": "The main objective of our experiments is to study the effects of the installation of the correlation filter during training. We first compare it with the symmetrical Siamese architecture of Bertinetto et al. [3]. Then we compare the endto-end trained CFNet with a variant in which the characteristics are replaced by characteristics trained for another task. Finally, we show that our method achieves state-of-the-art results."}, {"heading": "4.1. Evaluation criteria", "text": "Popular tracking benchmarks such as VOT [17] and OTB [33, 34] have provided all the basic truth notes and do not enforce validation / assignment. However, to avoid consistency with the test set in design selection and hyperparameter selection, we consider OTB-2013, OTB-50 and OTB-100 as our test set and 129 videos from VOT2014, VOT-2016 and Temple-Color [20] as our validation set, excluding any videos that have already been assigned to the test set. We run all our tracking experiments in Sections 4.2, 4.3 and 4.4 on the validation set using the same set of \"natural\" hyperparameters that are reasonable for all methods and not aligned to a particular method. As in the OTB benchmark [33, 34], we quantify the performance of the tracker based on a sequential overlapping of the basic truth values."}, {"heading": "4.2. Comparison to Siamese baseline", "text": "Figures 3 and 4 compare the accuracy of both methods on the validation set for networks of varying depth. The feature extraction network n is terminated after the n-th linear layer, including the following ReLU, but not the following pooling layer (if any). Our baseline differs slightly from [3] in two respects. First, we reduce the overall progress of the network from 8 to 4 (2 for conven1, 2 for pool1) to avoid correlation filters with small feature cards. Second, we always limit the last layer to 32 output channels in order to maintain the high speed of the method with larger feature cards. These changes had no negative impact on the tracking performance of SiamFC. Results show that CFNet is significantly better than the baseline when flat networks are used to calculate features. Specifically, it brings a 31% or 13% improvement for networks of depth one and less than two, with a significant difference of four and three."}, {"heading": "4.3. Feature transfer experiment", "text": "The motivation for this work was the hypothesis that the inclusion of CF during the training will lead to characteristics that are better suited for tracking with CF. In order to obtain the curve Baseline + CF, we trained a Siamese base network of the desired depth and then combined these characteristics with a CF during the tracking. Results show that the consideration of CF during the offline training in depth one and two is of crucial importance. However, it appears redundant if more sinuous layers are added, since the use of characteristics from the baseline in connection with CF achieves a similar performance. ImageNet + CF variant uses characteristics from a network that has been trained to solve the challenge of the ImageNet classification [28]. Results show that these characteristics, which are often the first choice for combining CF with CNNs [7, 9, 22, 22, 32, 32, 32 and 32 are clearly expected for the tracking layer."}, {"heading": "4.4. Importance of adaptation", "text": "For multi-channel CF, each channel p of the template can be obtained as wp = \u03b1? xp, where \u03b1 itself is a function of copy x (Appendix C, supplementary material).In order to verify the importance of the online adjustment, which should provide the solution of a burr regression problem at the test date, we propose a \"constant\" version of the correlation filter (CFNet-Const), where the vector of the Lagrange multipliers \u03b1 is instead a parameter of the network, which is learned offline and remains fixed at the test date. Figure 6 compares CFNet with its constant variant. CFNet is consistently better, which shows that it is crucial for improvement over the baseline of the Siamese network to reproduce itself by solving the reverse conversion problem, which defines the Lagrange multipliers."}, {"heading": "4.5. Comparison with the state-of-the-art", "text": "We use the OTB 2013 / 50 / 100 benchmarks to confirm that our results are on par with the state of the art. All figures in this section are determined using the OTB toolkit [33]. We report on the results for the three best instances of CFNet from Figure 5 (CFNet-conven2, CFNet-conven5, Baseline + CF-conven3), the best variant of baseline (Baseline-conven5) and the most promising single layer network (CFNet-conven1). We compare our methods with the most modern trackers that can operate in real time: SiamFC-3s [3], Staple [2] and LCT [23]. We also use the most recent SAMF [19] and DSST [6] for references. To evaluate this section, we use a different set of trackingtype parameters per architecture that have been selected to maximize performance on the validation."}, {"heading": "4.6. Speed and practical benefits", "text": "The previous sections have shown that integrating correlation filters into Siamese networks offers a clear advantage when the feature extraction network is relatively flat. Shallow networks are convenient because they require fewer operations and less disk space to evaluate and store. To understand the trade-off, Figure 7 shows the speed and accuracy of both the CF network and the baseline for different network depths. This diagram suggests that the two-layer CFNet may be the most interesting variant for practitioners who need a precise tracking algorithm that operates at high frame rates. It runs at 75 frames per second and has less than 4% of the parameters of the five-layer baseline and requires only 600 kB to store. This may be of particular interest for embedded devices with limited memory. In contrast, methods such as DeepSRDCF [7] and C-COT [9], which require basic features for the correlation filter to be less than five, run a different size filter."}, {"heading": "5. Conclusion", "text": "This paper proposes the correlation filter network, an asymmetrical architecture that propagates progressions backwards through an online learning algorithm to optimize the underlying representation of the characteristics. This is accomplished by setting up an efficient traceability map for solving a system of circular equations. However, our empirical study shows that adding a correlation filter layer for a sufficiently deep Siamese network does not significantly improve tracking accuracy. We believe that this is evidence of the performance of deep learning, as sufficient training data is available. The integration of the correlation filter into a similarity network during training allows flat networks to compete with their slower, deeper counterparameters. Future research may include enhancements to accommodate the adaptation over time, and backward propagating progressions through learning problems for related tasks such as one-time learning and domain adaptation to avoid networking [38] following the loss of X."}, {"heading": "B. Back-propagation for the Correlation Filter", "text": "As described in Appendix D (supplementary material), the backpropagation map is the juxtaposition of linear maps representing the difference. These linear maps for the correlation filter are represented in Appendix 9. We are free to obtain these adjunct cards in the Fourier area, since parseval theorem provides for the conservation of internal products. Let J1 denote the map dx 7 \u2192 dk in Appendix 9a. Therefore, the manipulation of internal product < Fdk, FJ1 (dx) > = d, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k,"}, {"heading": "C. Correlation Filter formulation", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "D. Adjoint of the differential", "text": "Consider an arithmetic diagram that calculates a scalar loss. \"Within this network, we consider an intermediate function that calculates y = f (x), where x-X = Rm and y-Y = Rn. Reproduction calculates the gradient relative to the input value of x-x-x from the gradient relative to the output of y-y-y. The derivative of x-x-x-x-x-x-x-x-x-x-x-x-x-x-x is the partial derivative of fi (x) / x-xj. This matrix relates the gradients to (x) T-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x."}, {"heading": "E. Back-propagation for multi-channel case", "text": "The differentials of the equations that define the Multichannel CF in eq. 37 are dk = 1n \u00b2 p (dxp? xp + xp? dxp) dk \u00b2 + k \u00b2 p = 1ndy dwp = d\u03b1? xp + \u03b1? dxp \u00b2 p, (42) and if you take the Fourier transformations of these equations, the result is d \u00b2 k = 1n \u00b2 p (d \u00b2 x \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p \u00b2 p p p p p \u00b2 p p p p p p p \u00b2 p p p p p \u00b2 p p p p p p p \u00b2 p p p p p \u00b2 p p p p p \u00b2 p p p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p p p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p p p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p p p \u00b2 p p p \u00b2 p p \u00b2 p p p \u00b2 p p \u00b2 p p p p \u00b2 p p \u00b2 p p p p \u00b2 p \u00b2 p p \u00b2 p p p p p \u00b2 p \u00b2 p p p \u00b2 p p \u00b2 p \u00b2 p p p p \u00b2 p p p p p \u00b2 p \u00b2 p p p \u00b2 p p p \u00b2 p p p p \u00b2 p p p p \u00b2 p p p \u00b2 p p p p p p \u00b2 p p p p p p p \u00b2 p p p"}, {"heading": "F. Hyperparameter optimization", "text": "The hyperparameters that define the simplistic tracking algorithm have a significant impact on tracking accuracy, including parameters such as the penalty for changes in scale and position, and the learning rate of the template average. Choosing hyperparameters is a difficult optimization problem: we cannot use the parentage of gradients because the function is highly discontinuous, and any function evaluation is expensive because it involves a tracker for each sequence from multiple starting points. Therefore, for the experiments of the main paper, where we seek a fair comparison of different architectures, we used a natural choice of hyperparameters that were not optimized for a particular architecture. Ideally, we would use the optimal hyperparameters for each variant, unless it would have been prohibitively expensive to perform this optimization for each item in each main paper (multiple error bar points)."}, {"heading": "G. Detailed results on the OTB benchmarks", "text": "Figures 9 to 14 show the OTB Toolkit4 curves for OTB-2013 / 50 / 100, of which we have a summary in the main paper Acknowledgements. This research was supported by Apical Ltd., EPSRC grant Seebibyte EP / M013774 / 1 and ERC grant ERC-2012-AdG 321162-HELIOS, HELIOSDFR00200, Integrated and Detailed Image Understanding (EP / L024683 / 1) and ERC 677195-IDIU."}], "references": [{"title": "Learning feed-forward one-shot learners", "author": ["L. Bertinetto", "J.F. Henriques", "J. Valmadre", "P.H.S. Torr", "A. Vedaldi"], "venue": "NIPS, pages 523\u2013531,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Staple: Complementary learners for real-time tracking", "author": ["L. Bertinetto", "J. Valmadre", "S. Golodetz", "O. Miksik", "P.H.S. Torr"], "venue": "CVPR, pages 1401\u20131409,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully-convolutional Siamese networks for object tracking", "author": ["L. Bertinetto", "J. Valmadre", "J.F. Henriques", "A. Vedaldi", "P.H.S. Torr"], "venue": "ECCV Workshops, pages 850\u2013865,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual object tracking using adaptive correlation filters", "author": ["D.S. Bolme", "J.R. Beveridge", "B.A. Draper", "Y.M. Lui"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Once for all: A two-flow convolutional neural network for visual tracking", "author": ["K. Chen", "W. Tao"], "venue": "arXiv preprint arXiv:1604.07507,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Accurate scale estimation for robust visual tracking", "author": ["M. Danelljan", "G. H\u00e4ger", "F. Khan", "M. Felsberg"], "venue": "BMVC,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional features for correlation filter based visual tracking", "author": ["M. Danelljan", "G. Hager", "F. Shahbaz Khan", "M. Felsberg"], "venue": "ICCV Workshops, pages 58\u201366,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning spatially regularized correlation filters for visual tracking", "author": ["M. Danelljan", "G. Hager", "F. Shahbaz Khan", "M. Felsberg"], "venue": "ICCV, pages 4310\u20134318,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Beyond correlation filters: Learning continuous convolution operators for visual tracking", "author": ["M. Danelljan", "A. Robinson", "F.S. Khan", "M. Felsberg"], "venue": "ECCV, pages 472\u2013488,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Zero-aliasing correlation filters", "author": ["J.A. Fernandez", "B. Vijayakumar"], "venue": "International Symposium on Image and Signal Processing and Analysis 2013, pages 101\u2013106,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "On differentiating parameterized argmin and argmax problems with application to bi-level optimization", "author": ["S. Gould", "B. Fernando", "A. Cherian", "P. Anderson", "R.S. Cruz", "E. Guo"], "venue": "arXiv preprint arXiv:1607.05447,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluating derivatives: Principles and techniques of algorithmic differentiation", "author": ["A. Griewank", "A. Walther"], "venue": "SIAM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to track at 100 fps with deep regression networks", "author": ["D. Held", "S. Thrun", "S. Savarese"], "venue": "ECCV, pages 749\u2013765. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Highspeed tracking with kernelized correlation filters", "author": ["J.F. Henriques", "R. Caseiro", "P. Martins", "J. Batista"], "venue": "IEEE TPAMI, 37(3):583\u2013596,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Matrix backpropagation for deep networks with structured layers", "author": ["C. Ionescu", "O. Vantzos", "C. Sminchisescu"], "venue": "ICCV, pages 2965\u20132973,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Correlation filters with limited boundaries", "author": ["H. Kiani Galoogahi", "T. Sim", "S. Lucey"], "venue": "CVPR, pages 4630\u20134638,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "The Visual Object Tracking VOT2016 challenge results", "author": ["M. Kristan", "A. Leonardis", "J. Matas", "M. Felsberg", "R. Pflugfelder", "L. \u010cehovin", "T. Voj\u0131\u0301r", "G. H\u00e4ger", "A. Luke\u017ei\u010d", "G. Fern\u00e1ndez"], "venue": "In ECCV Workshops. Springer,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning by tracking: Siamese CNN for robust target association", "author": ["L. Leal-Taix\u00e9", "C. Canton-Ferrer", "K. Schindler"], "venue": "CVPR Workshops, pages 33\u201340,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A scale adaptive kernel correlation filter tracker with feature integration", "author": ["Y. Li", "J. Zhu"], "venue": "ECCV, pages 254\u2013265,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Encoding color information for visual tracking: Algorithms and benchmark", "author": ["P. Liang", "E. Blasch", "H. Ling"], "venue": "IEEE Transactions on Image Processing, 24(12):5630\u20135644,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, pages 3431\u2013 3440,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical convolutional features for visual tracking", "author": ["C. Ma", "J.-B. Huang", "X. Yang", "M.-H. Yang"], "venue": "ICCV, pages 3074\u20133082,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term correlation tracking", "author": ["C. Ma", "X. Yang", "C. Zhang", "M.-H. Yang"], "venue": "CVPR, pages 5388\u20135396,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradientbased hyperparameter optimization through reversible learning", "author": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "ICML,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Differentiation of the Cholesky decomposition", "author": ["I. Murray"], "venue": "arXiv preprint arXiv:1602.07527,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "CVPR 2016, pages 4293\u20134302,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Maximum margin correlation filter: A new approach for localization and classification", "author": ["A. Rodriguez", "V.N. Boddeti", "B.V.K.V. Kumar", "A. Mahalanobis"], "venue": "IEEE Transactions on Image Processing, 22(2):631\u2013643,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), 115(3):211\u2013252,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Siamese instance search for tracking", "author": ["R. Tao", "E. Gavves", "A.W.M. Smeulders"], "venue": "CVPR, pages 1420\u20131429,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning detectors quickly with stationary statistics", "author": ["J. Valmadre", "S. Sridharan", "S. Lucey"], "venue": "ACCV, pages 99\u2013114. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Matching networks for one shot learning", "author": ["O. Vinyals", "C. Blundell", "T. Lillicrap", "D. Wierstra"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Transferring rich feature hierarchies for robust visual tracking", "author": ["N. Wang", "S. Li", "A. Gupta", "D.-Y. Yeung"], "venue": "arXiv preprint arXiv:1501.04587,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Online object tracking: A benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "CVPR, pages 2411\u20132418,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Object tracking benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "TPAMI, 37(9):1834\u20131848,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "CVPR, pages 2528\u20132535,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning of appearance models for online object tracking", "author": ["M. Zhai", "M.J. Roshtkhari", "G. Mori"], "venue": "arXiv preprint arXiv:1607.02568,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H.S. Torr"], "venue": "ICCV, pages 1529\u20131537,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "The simplest approach is to disregard the lack of a-priori knowledge and adapt a pre-trained deep convolutional neural network (CNN) to the target, for example by using stochastic gradient descent (SGD), the workhorse of deep network optimization [32, 26, 36].", "startOffset": 247, "endOffset": 259}, {"referenceID": 25, "context": "The simplest approach is to disregard the lack of a-priori knowledge and adapt a pre-trained deep convolutional neural network (CNN) to the target, for example by using stochastic gradient descent (SGD), the workhorse of deep network optimization [32, 26, 36].", "startOffset": 247, "endOffset": 259}, {"referenceID": 35, "context": "The simplest approach is to disregard the lack of a-priori knowledge and adapt a pre-trained deep convolutional neural network (CNN) to the target, for example by using stochastic gradient descent (SGD), the workhorse of deep network optimization [32, 26, 36].", "startOffset": 247, "endOffset": 259}, {"referenceID": 31, "context": "Furthermore, SGD is quite expensive for online adaptation [32, 26].", "startOffset": 58, "endOffset": 66}, {"referenceID": 25, "context": "Furthermore, SGD is quite expensive for online adaptation [32, 26].", "startOffset": 58, "endOffset": 66}, {"referenceID": 2, "context": "Recent works have focused on learning deep embeddings that can be used as universal object descriptors [3, 13, 29, 18, 5].", "startOffset": 103, "endOffset": 121}, {"referenceID": 12, "context": "Recent works have focused on learning deep embeddings that can be used as universal object descriptors [3, 13, 29, 18, 5].", "startOffset": 103, "endOffset": 121}, {"referenceID": 28, "context": "Recent works have focused on learning deep embeddings that can be used as universal object descriptors [3, 13, 29, 18, 5].", "startOffset": 103, "endOffset": 121}, {"referenceID": 17, "context": "Recent works have focused on learning deep embeddings that can be used as universal object descriptors [3, 13, 29, 18, 5].", "startOffset": 103, "endOffset": 121}, {"referenceID": 4, "context": "Recent works have focused on learning deep embeddings that can be used as universal object descriptors [3, 13, 29, 18, 5].", "startOffset": 103, "endOffset": 121}, {"referenceID": 3, "context": "The CF is an efficient algorithm that learns to discriminate an image patch from the surrounding patches by solving a large ridge regression problem extremely efficiently [4, 14].", "startOffset": 171, "endOffset": 178}, {"referenceID": 13, "context": "The CF is an efficient algorithm that learns to discriminate an image patch from the surrounding patches by solving a large ridge regression problem extremely efficiently [4, 14].", "startOffset": 171, "endOffset": 178}, {"referenceID": 5, "context": "[6, 19, 23, 2]), where its efficiency enables a tracker to adapt its internal model of the object on the fly at every frame.", "startOffset": 0, "endOffset": 14}, {"referenceID": 18, "context": "[6, 19, 23, 2]), where its efficiency enables a tracker to adapt its internal model of the object on the fly at every frame.", "startOffset": 0, "endOffset": 14}, {"referenceID": 22, "context": "[6, 19, 23, 2]), where its efficiency enables a tracker to adapt its internal model of the object on the fly at every frame.", "startOffset": 0, "endOffset": 14}, {"referenceID": 1, "context": "[6, 19, 23, 2]), where its efficiency enables a tracker to adapt its internal model of the object on the fly at every frame.", "startOffset": 0, "endOffset": 14}, {"referenceID": 21, "context": "[22, 7, 9, 32]), which have shown that CNNs and CFs are complementary and their combination results in improved performance.", "startOffset": 0, "endOffset": 14}, {"referenceID": 6, "context": "[22, 7, 9, 32]), which have shown that CNNs and CFs are complementary and their combination results in improved performance.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "[22, 7, 9, 32]), which have shown that CNNs and CFs are complementary and their combination results in improved performance.", "startOffset": 0, "endOffset": 14}, {"referenceID": 31, "context": "[22, 7, 9, 32]), which have shown that CNNs and CFs are complementary and their combination results in improved performance.", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], the Correlation Filter has enjoyed great popularity within the tracking community.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries [10, 16, 8], incorporating multi-resolution feature maps [22, 9] and augmenting the objective with a more robust loss [27].", "startOffset": 114, "endOffset": 125}, {"referenceID": 15, "context": "Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries [10, 16, 8], incorporating multi-resolution feature maps [22, 9] and augmenting the objective with a more robust loss [27].", "startOffset": 114, "endOffset": 125}, {"referenceID": 7, "context": "Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries [10, 16, 8], incorporating multi-resolution feature maps [22, 9] and augmenting the objective with a more robust loss [27].", "startOffset": 114, "endOffset": 125}, {"referenceID": 21, "context": "Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries [10, 16, 8], incorporating multi-resolution feature maps [22, 9] and augmenting the objective with a more robust loss [27].", "startOffset": 171, "endOffset": 178}, {"referenceID": 8, "context": "Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries [10, 16, 8], incorporating multi-resolution feature maps [22, 9] and augmenting the objective with a more robust loss [27].", "startOffset": 171, "endOffset": 178}, {"referenceID": 26, "context": "Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries [10, 16, 8], incorporating multi-resolution feature maps [22, 9] and augmenting the objective with a more robust loss [27].", "startOffset": 232, "endOffset": 236}, {"referenceID": 28, "context": "Recently, several methods based on Siamese networks have been introduced [29, 13, 3], raising interest in the tracking community for their simplicity and competitive performance.", "startOffset": 73, "endOffset": 84}, {"referenceID": 12, "context": "Recently, several methods based on Siamese networks have been introduced [29, 13, 3], raising interest in the tracking community for their simplicity and competitive performance.", "startOffset": 73, "endOffset": 84}, {"referenceID": 2, "context": "Recently, several methods based on Siamese networks have been introduced [29, 13, 3], raising interest in the tracking community for their simplicity and competitive performance.", "startOffset": 73, "endOffset": 84}, {"referenceID": 2, "context": "For our method, we prefer to build upon the fully-convolutional Siamese architecture [3], as it enforces the prior that the appearance similarity function should commute with translation.", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "At its core, the Correlation Filter layer that we introduce amounts to computing the solution to a regularized deconvolution problem, not to be confused with upsampling convolution layers that are sometimes referred to as \u201cdeconvolution layers\u201d [21].", "startOffset": 245, "endOffset": 249}, {"referenceID": 34, "context": "[35] introduced a deep architecture in which each layer solves a convolutional sparse coding problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] and Murray [25] have presented back-propagation forms for the SVD and Cholesky decomposition respectively, enabling gradient descent to be applied to a network that computes the solution to either a system of linear equations or an eigenvalue problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[15] and Murray [25] have presented back-propagation forms for the SVD and Cholesky decomposition respectively, enabling gradient descent to be applied to a network that computes the solution to either a system of linear equations or an eigenvalue problem.", "startOffset": 16, "endOffset": 20}, {"referenceID": 36, "context": "When the solution to the optimization problem is obtained iteratively, an alternative is to treat the iterations as a Recurrent Neural Network, and to explicitly unroll a fixed number of iterations [37].", "startOffset": 198, "endOffset": 202}, {"referenceID": 23, "context": "[24] go further and back-propagate gradients through an entire SGD learning procedure, although this is computationally demanding and requires judicious bookkeeping.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] have recently considered differentiating the solution to general argmin problems without restricting themselves to iterative procedures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Recent works [31, 1] have proposed feed-forward architectures that can be interpreted as learning algorithms, enabling optimization by gradient", "startOffset": 13, "endOffset": 20}, {"referenceID": 0, "context": "Recent works [31, 1] have proposed feed-forward architectures that can be interpreted as learning algorithms, enabling optimization by gradient", "startOffset": 13, "endOffset": 20}, {"referenceID": 2, "context": "Our starting point is a network similar to that of [3], which we later modify in order to allow the model to be interpreted as a Correlation Filter tracker.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "2Note that this differs from [3], in which the target object and search area were instead denoted z and x respectively.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "to [3], we employ a simplistic tracking algorithm to assess the utility of the similarity function.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "The CF block w = \u03c9(x) computes a standard CF template w from the training feature map x = f\u03c1(x) by solving a ridge regression problem in the Fourier domain [14].", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "[8] and Kiani et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window [4] and the final template is cropped [30].", "startOffset": 100, "endOffset": 103}, {"referenceID": 29, "context": "To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window [4] and the final template is cropped [30].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [14, 6, 23, 3] with CNN features, as proposed in previous work [22, 7].", "startOffset": 123, "endOffset": 137}, {"referenceID": 5, "context": "Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [14, 6, 23, 3] with CNN features, as proposed in previous work [22, 7].", "startOffset": 123, "endOffset": 137}, {"referenceID": 22, "context": "Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [14, 6, 23, 3] with CNN features, as proposed in previous work [22, 7].", "startOffset": 123, "endOffset": 137}, {"referenceID": 2, "context": "Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [14, 6, 23, 3] with CNN features, as proposed in previous work [22, 7].", "startOffset": 123, "endOffset": 137}, {"referenceID": 21, "context": "Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [14, 6, 23, 3] with CNN features, as proposed in previous work [22, 7].", "startOffset": 186, "endOffset": 193}, {"referenceID": 6, "context": "Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [14, 6, 23, 3] with CNN features, as proposed in previous work [22, 7].", "startOffset": 186, "endOffset": 193}, {"referenceID": 13, "context": "as close as possible to a desired response y[u] [14], minimizing\u2211", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "6 can be computed efficiently in the Fourier domain [14], \uf8f4\uf8f2\uf8f4\uf8f3 k\u0302 = 1 n (x\u0302 \u2217 \u25e6 x\u0302) + \u03bb1 \u03b1\u0302 = 1 n k\u0302 \u22121 \u25e6 \u0177 \u0175 = \u03b1\u0302\u2217 \u25e6 x\u0302 (7a)", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "[15] to compute backpropagation maps using matrix differential calculus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "However, in practice we did not find learning this parameter to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response [4, 14].", "startOffset": 159, "endOffset": 166}, {"referenceID": 13, "context": "However, in practice we did not find learning this parameter to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response [4, 14].", "startOffset": 159, "endOffset": 166}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Popular tracking benchmarks like VOT [17] and OTB [33, 34] have made all ground truth annotations available and do not enforce a validation/test split.", "startOffset": 37, "endOffset": 41}, {"referenceID": 32, "context": "Popular tracking benchmarks like VOT [17] and OTB [33, 34] have made all ground truth annotations available and do not enforce a validation/test split.", "startOffset": 50, "endOffset": 58}, {"referenceID": 33, "context": "Popular tracking benchmarks like VOT [17] and OTB [33, 34] have made all ground truth annotations available and do not enforce a validation/test split.", "startOffset": 50, "endOffset": 58}, {"referenceID": 19, "context": "However, in order to avoid overfitting to the test set in design choices and hyperparameter selection, we consider OTB-2013, OTB-50 and OTB-100 as our test set and 129 videos from VOT2014, VOT-2016 and Temple-Color [20] as our validation set, excluding any videos which were already assigned to the test set.", "startOffset": 215, "endOffset": 219}, {"referenceID": 32, "context": "As in the OTB benchmark [33, 34], we quantify the performance of the tracker on a sequence in terms of the average overlap (intersection over union) of the predicted and ground truth rectangles in all frames.", "startOffset": 24, "endOffset": 32}, {"referenceID": 33, "context": "As in the OTB benchmark [33, 34], we quantify the performance of the tracker on a sequence in terms of the average overlap (intersection over union) of the predicted and ground truth rectangles in all frames.", "startOffset": 24, "endOffset": 32}, {"referenceID": 2, "context": "Our baseline diverges slightly from [3] in two ways.", "startOffset": 36, "endOffset": 39}, {"referenceID": 27, "context": "The ImageNet+CF variant employs features taken from a network trained to solve the ImageNet classification challenge [28].", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "The results show that these features, which are often the first choice for combining CFs with CNNs [7, 9, 22, 26, 32, 36], are significantly worse than those learned by CFNet and the Baseline experiment.", "startOffset": 99, "endOffset": 121}, {"referenceID": 8, "context": "The results show that these features, which are often the first choice for combining CFs with CNNs [7, 9, 22, 26, 32, 36], are significantly worse than those learned by CFNet and the Baseline experiment.", "startOffset": 99, "endOffset": 121}, {"referenceID": 21, "context": "The results show that these features, which are often the first choice for combining CFs with CNNs [7, 9, 22, 26, 32, 36], are significantly worse than those learned by CFNet and the Baseline experiment.", "startOffset": 99, "endOffset": 121}, {"referenceID": 25, "context": "The results show that these features, which are often the first choice for combining CFs with CNNs [7, 9, 22, 26, 32, 36], are significantly worse than those learned by CFNet and the Baseline experiment.", "startOffset": 99, "endOffset": 121}, {"referenceID": 31, "context": "The results show that these features, which are often the first choice for combining CFs with CNNs [7, 9, 22, 26, 32, 36], are significantly worse than those learned by CFNet and the Baseline experiment.", "startOffset": 99, "endOffset": 121}, {"referenceID": 35, "context": "The results show that these features, which are often the first choice for combining CFs with CNNs [7, 9, 22, 26, 32, 36], are significantly worse than those learned by CFNet and the Baseline experiment.", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": "SiamFC-3s [3] 60.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "5 Staple [2] 60.", "startOffset": 9, "endOffset": 12}, {"referenceID": 22, "context": "9 LCT [23] 61.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "5 SAMF [19] \u2013 \u2013 \u2013 \u2013 46.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "6 DSST [6] 55.", "startOffset": 7, "endOffset": 10}, {"referenceID": 32, "context": "All numbers in this section are obtained using the OTB toolkit [33].", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s [3], Staple [2] and LCT [23].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s [3], Staple [2] and LCT [23].", "startOffset": 109, "endOffset": 112}, {"referenceID": 22, "context": "We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s [3], Staple [2] and LCT [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "We also include the recent SAMF [19] and DSST [6] for reference.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "We also include the recent SAMF [19] and DSST [6] for reference.", "startOffset": 46, "endOffset": 49}, {"referenceID": 33, "context": "Both overlap (IoU) and precision scores [34] are reported for OPE (one pass) and TRE (temporal robustness) evaluations.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "bounding box regression [26], ensembling of multiple cues [23, 2], optical flow [29]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "bounding box regression [26], ensembling of multiple cues [23, 2], optical flow [29]).", "startOffset": 58, "endOffset": 65}, {"referenceID": 1, "context": "bounding box regression [26], ensembling of multiple cues [23, 2], optical flow [29]).", "startOffset": 58, "endOffset": 65}, {"referenceID": 28, "context": "bounding box regression [26], ensembling of multiple cues [23, 2], optical flow [29]).", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "In contrast, methods like DeepSRDCF [7] and C-COT [9], which use out-of-the-box deep features for the Correlation Filter, run orders of magnitude slower.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "In contrast, methods like DeepSRDCF [7] and C-COT [9], which use out-of-the-box deep features for the Correlation Filter, run orders of magnitude slower.", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "We follow the procedure of [3] to minimize the loss (equation 2) through SGD, with the Xavier-improved parameters initialization and using mini-batches of size 8.", "startOffset": 27, "endOffset": 30}, {"referenceID": 27, "context": "We use all the 3862 training videos of ImageNet Video [28], containing more than 1 million annotated frames, with multiple objects per frame.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "However, the dual formulation is much more efficient in the multi-channel case [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "This is the core of reverse-mode differentiation [12].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "This technique has previously been used for matrix structured back-propagation [15].", "startOffset": 79, "endOffset": 83}], "year": 2017, "abstractText": "The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.", "creator": "LaTeX with hyperref package"}}}