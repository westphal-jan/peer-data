{"id": "1704.06191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Softmax GAN", "abstract": "Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of $N$ real training samples and $M$ generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability $\\frac{1}{M}$, and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability $\\frac{1}{M+N}$. While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training.", "histories": [["v1", "Thu, 20 Apr 2017 15:35:14 GMT  (4438kb,D)", "http://arxiv.org/abs/1704.06191v1", "NIPS 2017 submission"]], "COMMENTS": "NIPS 2017 submission", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["min lin"], "accepted": false, "id": "1704.06191"}, "pdf": {"name": "1704.06191.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Softmax GAN", "Min Lin"], "emails": ["mavenlin@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Generative Adversarial Networks (GAN) [4] has achieved great success due to its ability to generate realistic samples. GAN consists of a discriminator and a generator. The discriminator tries to distinguish real samples from generated samples, while the generator falsifies real samples using information from the discriminator. GAN differs from many other generative models. Instead of explicitly trying out on the basis of a probability distribution, GAN uses a deep neural network as a direct generator that generates samples from random sounds. GAN has proven to be good at several realistic tasks, such as image painting, blurring and imitation. Despite its success in many applications, GAN is highly unstable in training. Careful selection of hyperparameters is often necessary to allow the training process to converge [11]. It is often assumed that this instability is caused by unbalanced discriminators and generator training. As the discriminator uses maximum logistic loss when the samples are quickly separated from graded samples, it will be a problem if the sample is sawed out."}, {"heading": "2 Related Works", "text": "There is much work related to improving the stability of GAN training. DCGAN proposed by Radford et. al. [11] comes with several empirical techniques that work well, including the application of batch normalization, how the input should be normalized, and the activation function to use. [13] One of them is minibatch discrimination. The idea is to introduce a layer that works across the samples to introduce coordination between the gradients from different samples in a minibatch. In this work we achieve a similar effect with Softmax on the samples. We argue that Softmax is more natural and explainable, but does not require additional parameters. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.ar Xiv: 170 4.06 191v 1 [cs.L G] 20 Apr 201 7Nowozin etal."}, {"heading": "3 Softmax GAN", "text": "We call the minibatch sample from the training data and the generated data B + + and B \u2212 respectively. B = B + + + B \u2212 is the union of B + and B \u2212. \"The output of the discriminator is represented by the parameterization of sample material in B \u2212. As in GAN, the generated samples are not sampled directly from a distribution. Instead, they are generated directly from a random variable (z) with a detectable generator. We use x for samples from B + and x \u00b2 for generated samples in B \u2212.\" As in GAN, the generated samples are not sampled directly from a distribution. Instead, they are generated directly from a random variable (e.g. with a detectable generator). We normalize the energy of all data points within B \u2212 and use the cross entropy loss for both the discriminator and the generator. The goal of the discriminator is to distribute the probability mass equally across all samples in B +."}, {"heading": "4 Relationship to Importance Sampling", "text": "In the original GAN paper, it was pointed out that GAN resembles NCE [6] in that both use a binary logistic classification loss as a substitute function for the formation of a generative model. GAN improves on NCE by using a trained generator for sound samples instead of fixing the sound distribution. In the same sense as GAN is related to NCE [5], this work can be considered the importance sampling version of GAN [2]. We prove it as follows."}, {"heading": "4.1 Discriminator", "text": "The \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (x) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "4.2 Generator", "text": "We replace equation 10 with 4. The lhs of equation 4 gives \u2212 \u2211 x-B 1 | B | ln pD pD + pG 2 \u2212 lnC = KL (pD + pG 2-pD) (11) The gradient of the Rhs can also be considered as a distorted meaning sample, \u2212 \u2211 x-B pD pD + pG2-pG pD + pG * x-B pDpD + pG 2 \u2248 \u2212 Ex-pD-pGpD + pG (12), which \u2212 Ex-pD ln (pD + pG) = KL (pD-pD + pG 2) \u2212 Ex-pD ln 2pD optimized. After removing the constants, we get LG = KL (pD + pG2-pD) + KL (pD-pD + pG 2) (13)."}, {"heading": "4.3 Importance Sampling\u2019s link to NCE", "text": "As can be seen from [7] and [12], both Importance Sampling and NCE train the underlying generative model with a classification surrogate. The difference is that in NCE a binary classification task is defined between real and noise samples with logistical loss, while Importance Sampling replaces the logistical loss with a multiclass softmax and cross entropy loss. We show the relationship between NCE, Importance Sampling, GAN and this work in Figure 1. Softmax-GAN fills the table with the missing element."}, {"heading": "4.4 Infinite batch size", "text": "As shown in [3], the distorted estimation of the importance of samples converges with the real partition function when the number of samples in a stack reaches infinity. In practice, we have found that the setting | B + | = | B \u2212 | = 5 is sufficient to produce images that are visually realistic."}, {"heading": "5 Experiments", "text": "We conduct image generation experiments with the celebA database. We show that Softmax GAN, while minimizing Jensen-Shannon divergence between the generated data and the real data, is more stable than the original GAN and less prone to mode breakdowns. We implement Softmax GAN by modifying the loss function of the DCGAN code (https: / / github.com / carpedm20 / DCGAN-tensorflow). Since DCGAN is quite stable, we remove the empirical techniques used in DCGAN and observe instability in training. On the contrary, Softmax GAN is stable against these changes."}, {"heading": "5.1 Stablized training", "text": "We follow the WGAN paper by removing the batch normalization layers and using a constant number of filters in the generator network. We compare the results of GAN and Softmax GAN. The results are shown in Figure 2."}, {"heading": "5.2 Mode collapse", "text": "In the DCGAN paper, the pixels of the input images are normalized to [\u2212 1, 1). We remove this constraint and normalize the pixels to [0, 1) instead. At the same time, we replace the leaky Relu with Relu, which makes the gradients thinner (which, according to the DCGAN authors, is unfavorable for GAN training). Under this setting, the original GAN suffers from a significant degree of mode collape and poor image quality. Softmax GAN, on the other hand, is resilient to this change. Examples can be seen in Figure 3."}, {"heading": "5.3 Balance of generator and discriminator", "text": "In [11] it is claimed that it is a bad idea to manually balance the number of iterations of the discriminator and generator. However, the WGAN paper gives the discriminator phase more iterations to get a better discriminator for training the generator. We set the discriminator-generator ratio to 5: 1 and 1: 5 and investigate the effects of this ratio on DCGAN and Softmax GAN. Results are shown in Figure 4 and 5 respectively."}, {"heading": "6 Conclusions and future work", "text": "We propose a variant of GAN that performs cross-sample Softmax in a minibatch, using crossentropy losses for both discriminator and generator. The goal is to match all probabilities to real data for discriminator, and to assign the same probability to all samples for generator. We have proven that this goal roughly optimizes JS divergence using import sampling. We form the connections between GAN, NCE, import sampling, and Softmax GAN. We demonstrate with experiments that Softmax GAN consistently achieves good results when GAN fails to eliminate empirical techniques. In our future work, we will perform a more systematic comparison between Softmax GAN and other GAN variants, and check whether it works for tasks other than image generation."}], "references": [{"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Yoshua Bengio", "Jean-S\u00e9bastien Sen\u00e9cal"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "On distinguishability criteria for estimating generative models", "author": ["Ian J Goodfellow"], "venue": "arXiv preprint arXiv:1412.6515,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Least squares generative adversarial networks", "author": ["Xudong Mao", "Qing Li", "Haoran Xie", "Raymond YK Lau", "Zhen Wang"], "venue": "arXiv preprint ArXiv:1611.04076,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Loss-sensitive generative adversarial networks on lipschitz densities", "author": ["Guo-Jun Qi"], "venue": "arXiv preprint arXiv:1701.06264,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "On word embeddings - part 2: Approximating the softmax", "author": ["Sebastian Ruder"], "venue": "http://sebastianruder.com/word-embeddings-softmax/index.html# similaritybetweennceandis", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "1 Introduction Generative Adversarial Networks(GAN) [4] has achieved great success due to its ability to generate realistic samples.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "Careful selection of hyperparameters is often necessary to make the training process converge [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "[11] comes up with several empirical techniques that works well, including how to apply batch normalization, how the input should be normalized, and which activation function to use.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] generalizes the GAN training loss from Jensen-Shannon divergence to any f-divergence function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "For example, the Least Square GAN [8] uses l2 loss function as the objective, which achieves faster training and improves generation quality.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "A related work to WGAN is Loss-Sensitive GAN [10], whose objective is to minimize the loss for real data and maximize it for the fake ones.", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "It has been pointed out in the original GAN paper that GAN is similar to NCE [6] in that both of them use a binary logistic classification loss as the surrogate function for training of a generative model.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "In the biased Importance Sampling [3], the above is converted to the following biased estimation which can be calculated without knowing p:", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "4 Infinite batch size As pointed out by [3], biased Importance Sampling estimation converges to the real partition function when the number of samples in one batch goes to infinity.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "3 Balance of generator and discriminator It is claimed in [11] that manually balancing the number of iterations of the discriminator and generator is a bad idea.", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of N real training samples and M generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability 1 M , and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability 1 M+N . While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training.", "creator": "LaTeX with hyperref package"}}}