{"id": "1704.06217", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Reinforcement Learning with External Knowledge and Two-Stage Q-functions for Predicting Popular Reddit Threads", "abstract": "This paper addresses the problem of predicting popularity of comments in an online discussion forum using reinforcement learning, particularly addressing two challenges that arise from having natural language state and action spaces. First, the state representation, which characterizes the history of comments tracked in a discussion at a particular point, is augmented to incorporate the global context represented by discussions on world events available in an external knowledge source. Second, a two-stage Q-learning framework is introduced, making it feasible to search the combinatorial action space while also accounting for redundancy among sub-actions. We experiment with five Reddit communities, showing that the two methods improve over previous reported results on this task.", "histories": [["v1", "Thu, 20 Apr 2017 16:30:39 GMT  (1085kb,D)", "http://arxiv.org/abs/1704.06217v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ji he", "mari ostendorf", "xiaodong he"], "accepted": false, "id": "1704.06217"}, "pdf": {"name": "1704.06217.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning with External Knowledge and Two-Stage Q-functions for Predicting Popular Reddit Threads", "authors": ["Ji He", "Mari Ostendorf", "Xiaodong He"], "emails": ["ostendor}@uw.edu", "xiaohe@microsoft.com"], "sections": [{"heading": null, "text": "This paper addresses the problem of predicting the popularity of comments in an online discussion forum using reinforcement learning, in particular two challenges arising from the availability of natural linguistic states and spaces of action. First, state representation, which characterizes the history of comments, is expanded to include the global context represented by discussions of world events in an external source of knowledge. Second, a two-tier Q-Learning framework is introduced that makes it possible to search the combinatorial space of action while taking into account redundancy in sub-actions. We experiment with five Reddit communities and show that the two methods improve on previous reported outcomes in this task."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them will be able to abide by the rules that they have imposed on themselves, and that they will be able to break the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to break the rules. (...) In fact, it is the case that they are able to break the rules. (...) In fact, it is the case that they are able to obey the rules. (...) In fact, it is the case that they are able to break the rules. (...)"}, {"heading": "2 Task", "text": "On Reddit, users respond to posts and other comments in a summarized (tree-structured) discussion. Comments (and posts) are associated with a karma score, which is a combination of positive and negative voices from registered users indicating the popularity of the comment. In previous work (He et al., 2016b), popularity prediction in Reddit discussions (comment recommendation) is proposed to study enhancing learning with a large-scale natural linguistic action space. In a real-time scenario, the actor receives a text sequence describing the state of st and multiple text strings describing the potential actions. In (re-considered comments), attempts are made to select the best measures to maximize long-term reward. In a real-time scenario, the final karma of a comment is not immediately available, so predicting popularity is based on the text in the comment as well as the context of the discussion story. Ultimately, a lower karma is more common in the discussion, and it becomes a lower karma."}, {"heading": "3 Related Work", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves in the world, and we are able to assert ourselves in the world, we are able to assert ourselves, we are able to assert ourselves in the world, and we are able to assert ourselves in the world, we are in the world we are in."}, {"heading": "4 Incorporating External Knowledge into the State Representation", "text": "In fact, most of us are able to play by the rules."}, {"heading": "5 Two-Stage Q-learning for a Combinatorial Action Space", "text": "There are two challenges associated with a combinatorial scope for action."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Data set and preprocessing", "text": "We conduct experiments to predict popular discussion lines on Reddit, as proposed by He et al. (2016b). Specifically, we conduct experiments with data from 5 subreddits, including askscience, askmen, todayilearned, askwomen, and politics, covering various genres and topics. To have sufficiently long discussion lines, we filter out discussion trees with less than 100 comments. For each of the 5 subreddits, we randomly distribute 90% of the data for online training and 10% of the data for testing. Our evaluation metric is aggregated karma scores. For each setting, we obtain averages (average reward) and standard deviations (represented as error bars or numbers in brackets) of over 10,000 episodes each. In all of our Ex-3The whole two-stage Q framework is summary in Algorithm 1 in Appendix.periments, we lay N = 10. The basic subreddit statistics are presented in Table 1."}, {"heading": "6.2 Incorporating external knowledge", "text": "This year, it's so far that we'll be able to leave the country to save it, \"he said.\" We've never lost as much time as this year, \"he said.\" But it's too early to do it, \"he said."}, {"heading": "6.3 Two-stage Q-learning for a combinatorial action space", "text": "In this subsection, we examine the effect of two-step Q-learning without taking external knowledge into account. We first train DRRN (K = 1) and copy the parameters on DRRN-Sum as Q1. We then train Q2 = DRRN-BiLSTM as before, except that we use Q1 = DRRN-Sum to explore the entire action space to Bt.On askscience, we try several settings with K = 2, 3, 4, 5 and the results are shown in Table 2. We compare the proposed two-step Q-Learning with two single-step Q-Learning baselines. The first baseline, following the method in He et al. (2016b), uses a random subsampling approach to obtain Bt (with m = 10) and takes the maximum to compare it with DRRN-BiLSTM. The second baseline uses DRRN-Sum and examines the entire action space."}, {"heading": "6.4 Combined results", "text": "In Figure 4, we present an ablation study on the effects of the inclusion of external knowledge and / or two-level Q-Learning (with N = 10, K = 3) through different speakers. The two contributions we propose each contribute to improving learning performance in a natural language scenario with a combinatorial scope for action. Moreover, the combination of these two approaches further improves performance. In our task, two-level Q-Learning provides greater benefit. In all cases, however, the inclusion of external knowledge leads to a more consistent additional gain compared to two-level Q-Learning. [7] We conduct case studies in Table 4. We show examples of the most / least visited documents in the external knowledge that correspond to the status description."}, {"heading": "7 Conclusion", "text": "In this paper, we present two approaches to improving natural language decision-making in a combinatorial action space: the first is to improve the state representation of the environment by incorporating external knowledge through a learnable attention mechanism; the second is to use a two-step Q-Learning framework to explore the entire combinatorial action space, avoiding enumerating all possible action combinations; and our experimental results show that both proposed approaches improve performance in predicting popular Reddit threads."}, {"heading": "A Algorithm table for two-stage Q-learning", "text": "As shown in algorithm 1."}, {"heading": "B URLs for subreddits used in this paper", "text": "\u00b7 Algorithm 1: Two-level Q-learning in the combinatorial action space (Q1: DRRN-Sum, Q2: DRRNBiLSTM) 1: Initialize Reddit Popularity Forecast Environment and Stress Dictionary. 2: Initialize DRRN Q0 (st, cit; 1) (equivalent to DRRN-Sum with K = 1) with small random weight and pull sequences. The DRRN-Sum Q1 (st, at; 1) = Q1 (st, {c1t, c2t, \u00b7 \u00b7, cKt}; K i = 1Q0 (st, c i t) shares the same parameters as DRRN. 3: Initialize Replay Memory D to capacity."}], "references": [{"title": "Snowball: Extracting relations from large plain-text collections", "author": ["E. Agichtein", "L. Gravano."], "venue": "Proceedings of the fifth ACM conference on Digital libraries. ACM, pages 85\u201394.", "citeRegEx": "Agichtein and Gravano.,? 2000", "shortCiteRegEx": "Agichtein and Gravano.", "year": 2000}, {"title": "Knowledge collection for natural language spoken dialog systems", "author": ["E. Ammicht", "A. L Gorin", "T. Alonso."], "venue": "EUROSPEECH.", "citeRegEx": "Ammicht et al\\.,? 1999", "shortCiteRegEx": "Ammicht et al\\.", "year": 1999}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Besting the quiz master: Crowdsourcing incremental classification games", "author": ["J. Boyd-Graber", "B. Satinoff", "H. He", "H. Daum\u00e9 III."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Boyd.Graber et al\\.,? 2012", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2012}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["S. Branavan", "D. Silver", "R. Barzilay."], "venue": "Journal of Artificial Intelligence Research 43:661\u2013704.", "citeRegEx": "Branavan et al\\.,? 2012", "shortCiteRegEx": "Branavan et al\\.", "year": 2012}, {"title": "Search-based structured prediction", "author": ["H. Daum\u00e9 III", "J. Langford", "D. Marcu."], "venue": "Machine learning 75(3):297\u2013325.", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["B. Dhingra", "L. Li", "X. Li", "J. Gao", "Y-N Chen", "F. Ahmed", "L. Deng."], "venue": "arXiv preprint arXiv:1609.00777 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Deep reinforcement learning in large discrete action spaces", "author": ["G. Dulac-Arnold", "R. Evans", "H. Van Hasselt", "P. Sunehag", "T. Lillicrap", "J. Hunt."], "venue": "arXiv preprint arXiv:1512.07679 .", "citeRegEx": "Dulac.Arnold et al\\.,? 2016", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2016}, {"title": "Open information extraction: The second generation", "author": ["O. Etzioni", "A. Fader", "J. Christensen", "S. Soderland", "M. Mausam."], "venue": "IJCAI. volume 11, pages 3\u201310.", "citeRegEx": "Etzioni et al\\.,? 2011", "shortCiteRegEx": "Etzioni et al\\.", "year": 2011}, {"title": "Learning latent local conversation modes for predicting community endorsement in online discussions", "author": ["H. Fang", "H. Cheng", "M. Ostendorf."], "venue": "Proc. Int. Workshop Natural Language Processing for Social Media. page 55.", "citeRegEx": "Fang et al\\.,? 2016", "shortCiteRegEx": "Fang et al\\.", "year": 2016}, {"title": "Building watson: An overview of the DeepQA project. AI magazine 31(3):59\u201379", "author": ["D. Ferrucci", "E. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A. A Kalyanpur", "A. Lally", "J W. Murdock", "E. Nyberg", "J. Prager"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Double Q-learning", "author": ["Hado V. Hasselt."], "venue": "J. D. Lafferty, C. K. I. Williams, J. ShaweTaylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23. Curran Associates, Inc., pages 2613\u2013", "citeRegEx": "Hasselt.,? 2010", "shortCiteRegEx": "Hasselt.", "year": 2010}, {"title": "Deep reinforcement learning with a natural language action space", "author": ["J. He", "J. Chen", "X. He", "J. Gao", "L. Li", "L. Deng", "M. Ostendorf."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL).", "citeRegEx": "He et al\\.,? 2016a", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "2016b. Deep reinforcement learning with a combinatorial action space for predicting popular", "author": ["J. He", "M. Ostendorf", "X. He", "J. Chen", "J. Gao", "L. Li", "L. Deng"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "External knowledge sources for question answering", "author": ["B. Katz", "G. Marton", "G. C Borchardt", "A. Brownell", "S. Felshin", "D. Loreto", "J. Louis-Rosenberg", "B. Lu", "F. Mora", "S. Stiller"], "venue": null, "citeRegEx": "Katz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Katz et al\\.", "year": 2005}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "D. Jurafsky", "M. Galley", "J. Gao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "The web as a resource for question answering: Perspectives and challenges", "author": ["J. J Lin."], "venue": "LREC.", "citeRegEx": "Lin.,? 2002", "shortCiteRegEx": "Lin.", "year": 2002}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L-J Lin."], "venue": "Machine Learning 8(3\u20134):293\u2013321.", "citeRegEx": "Lin.,? 1992", "shortCiteRegEx": "Lin.", "year": 1992}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. A Rusu", "J. Veness", "M. G Bellemare", "A. Graves", "M. Riedmiller", "A. K Fidjeland", "G. Ostrovski"], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay."], "venue": "Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1\u201311.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["K. Narasimhan", "A. Yala", "R. Barzilay."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Narasimhan et al\\.,? 2016", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "End-to-end goal-driven web navigation", "author": ["R. Nogueira", "K. Cho."], "venue": "Advances in Neural Information Processing Systems 29. pages 1903\u20131911.", "citeRegEx": "Nogueira and Cho.,? 2016", "shortCiteRegEx": "Nogueira and Cho.", "year": 2016}, {"title": "Introduction to modern information retrieval", "author": ["G. Salton", "M. J McGill."], "venue": "McGraw-Hill, Inc.", "citeRegEx": "Salton and McGill.,? 1986", "shortCiteRegEx": "Salton and McGill.", "year": 1986}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C. J Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus."], "venue": "Advances in neural information processing systems. pages 2440\u20132448.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver."], "venue": "CoRR, abs/1509.06461 .", "citeRegEx": "Hasselt et al\\.,? 2015", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton."], "venue": "Advances in Neural Information Processing Systems. pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Q-learning", "author": ["C. JCH Watkins", "P. Dayan."], "venue": "Machine learning 8(3-4):279\u2013292.", "citeRegEx": "Watkins and Dayan.,? 1992", "shortCiteRegEx": "Watkins and Dayan.", "year": 1992}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "L. M Rojas-Barahona", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young."], "venue": "arXiv preprint arXiv:1604.04562 .", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes."], "venue": "arXiv preprint arXiv:1410.3916 .", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Open information extraction using wikipedia", "author": ["F. Wu", "D. S Weld."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 118\u2013127.", "citeRegEx": "Wu and Weld.,? 2010", "shortCiteRegEx": "Wu and Weld.", "year": 2010}, {"title": "Docchat: An information retrieval approach for chatbot engines using unstructured documents", "author": ["Z. Yan", "N. Duan", "J. Bao", "P. Chen", "M. Zhou", "Z. Li", "J. Zhou."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Yan et al\\.,? 2016", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Structured use of external knowledge for eventbased open domain question answering", "author": ["H. Yang", "T-S Chua", "S. Wang", "C-K Koh."], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in infor-", "citeRegEx": "Yang et al\\.,? 2003", "shortCiteRegEx": "Yang et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 19, "context": "Recently, several tasks that involve states and actions described by natural language have been studied, such as text-based games (Narasimhan et al., 2015; He et al., 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al.", "startOffset": 130, "endOffset": 173}, {"referenceID": 12, "context": "Recently, several tasks that involve states and actions described by natural language have been studied, such as text-based games (Narasimhan et al., 2015; He et al., 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al.", "startOffset": 130, "endOffset": 173}, {"referenceID": 21, "context": ", 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al.", "startOffset": 25, "endOffset": 49}, {"referenceID": 20, "context": ", 2016a), web navigation (Nogueira and Cho, 2016), information extraction (Narasimhan et al., 2016), Reddit popularity prediction and tracking (He et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 28, "context": ", 2016b), and human-computer dialogue systems (Wen et al., 2016; Li et al., 2016).", "startOffset": 46, "endOffset": 81}, {"referenceID": 15, "context": ", 2016b), and human-computer dialogue systems (Wen et al., 2016; Li et al., 2016).", "startOffset": 46, "endOffset": 81}, {"referenceID": 32, "context": "External knowledge \u2013 both general and domainspecific \u2013 has been shown to be useful in many natural language tasks, such as in question answering (Yang et al., 2003; Katz et al., 2005; Lin, 2002),", "startOffset": 145, "endOffset": 194}, {"referenceID": 14, "context": "External knowledge \u2013 both general and domainspecific \u2013 has been shown to be useful in many natural language tasks, such as in question answering (Yang et al., 2003; Katz et al., 2005; Lin, 2002),", "startOffset": 145, "endOffset": 194}, {"referenceID": 16, "context": "External knowledge \u2013 both general and domainspecific \u2013 has been shown to be useful in many natural language tasks, such as in question answering (Yang et al., 2003; Katz et al., 2005; Lin, 2002),", "startOffset": 145, "endOffset": 194}, {"referenceID": 0, "context": "information extraction (Agichtein and Gravano, 2000; Etzioni et al., 2011; Wu and Weld, 2010), computer games (Branavan et al.", "startOffset": 23, "endOffset": 93}, {"referenceID": 8, "context": "information extraction (Agichtein and Gravano, 2000; Etzioni et al., 2011; Wu and Weld, 2010), computer games (Branavan et al.", "startOffset": 23, "endOffset": 93}, {"referenceID": 30, "context": "information extraction (Agichtein and Gravano, 2000; Etzioni et al., 2011; Wu and Weld, 2010), computer games (Branavan et al.", "startOffset": 23, "endOffset": 93}, {"referenceID": 4, "context": ", 2011; Wu and Weld, 2010), computer games (Branavan et al., 2012), and dialog systems (Ammicht et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 1, "context": ", 2012), and dialog systems (Ammicht et al., 1999; Yan et al., 2016).", "startOffset": 28, "endOffset": 68}, {"referenceID": 31, "context": ", 2012), and dialog systems (Ammicht et al., 1999; Yan et al., 2016).", "startOffset": 28, "endOffset": 68}, {"referenceID": 18, "context": "Atari games (Mnih et al., 2015) and the game of Go (Silver et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 23, "context": ", 2015) and the game of Go (Silver et al., 2016).", "startOffset": 27, "endOffset": 48}, {"referenceID": 12, "context": "Our study is conducted on the task of Reddit popularity prediction proposed in He et al. (2016b), which is a sequential decision-making problem based on a large-scale real-world natural language data set.", "startOffset": 79, "endOffset": 97}, {"referenceID": 12, "context": "In He et al. (2016b), two deep Q-learning architectures are proposed, both with separate networks for the state and action spaces yielding embeddings hs and ha, respectively.", "startOffset": 3, "endOffset": 21}, {"referenceID": 12, "context": "In He et al. (2016b), two deep Q-learning architectures are proposed, both with separate networks for the state and action spaces yielding embeddings hs and ha, respectively. Those embeddings are combined with a general interaction function g(\u00b7) to approximate the Q-values, Q(st, at) = g ( hs, h i a ) , as in He et al. (2016a), where the approach of using separate networks for natural language state and action spaces is termed a Deep Reinforcement Relevance Network (DRRN).", "startOffset": 3, "endOffset": 329}, {"referenceID": 18, "context": "To control agents directly given high-dimensional sensory inputs, a Deep Q-Network (Mnih et al., 2015) has been proposed and shown high capacity and scalability for handling a large state space.", "startOffset": 83, "endOffset": 102}, {"referenceID": 2, "context": "Another stream of work in recent deep learning research is the attention mechanism (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vinyals et al., 2015), where a probability distribution is computed to pay attention to certain parts of a collec-", "startOffset": 83, "endOffset": 153}, {"referenceID": 24, "context": "Another stream of work in recent deep learning research is the attention mechanism (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vinyals et al., 2015), where a probability distribution is computed to pay attention to certain parts of a collec-", "startOffset": 83, "endOffset": 153}, {"referenceID": 26, "context": "Another stream of work in recent deep learning research is the attention mechanism (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vinyals et al., 2015), where a probability distribution is computed to pay attention to certain parts of a collec-", "startOffset": 83, "endOffset": 153}, {"referenceID": 18, "context": "Narasimhan et al. (2016) presents a framework of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce.", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "Narasimhan et al. (2016) presents a framework of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. In task-oriented human-computer dialogue interactions, Wen et al. (2016) introduce a neural network-based trainable dialogue system with a database operator module.", "startOffset": 0, "endOffset": 255}, {"referenceID": 6, "context": "Dhingra et al. (2016) proposed a dialogue agent that provides users with an entity from a knowledge base by", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "In question answering, knowledge representation and reasoning also plays a central role (Ferrucci et al., 2010; Boyd-Graber et al., 2012).", "startOffset": 88, "endOffset": 137}, {"referenceID": 3, "context": "In question answering, knowledge representation and reasoning also plays a central role (Ferrucci et al., 2010; Boyd-Graber et al., 2012).", "startOffset": 88, "endOffset": 137}, {"referenceID": 11, "context": "To address overestimations of action values, double Q-learning (Hasselt, 2010; Van Hasselt et al., 2015) has been proposed and it leads to better performance gains on several Atari games.", "startOffset": 63, "endOffset": 104}, {"referenceID": 7, "context": "Dulac-Arnold et al. (2016) present a policy architecture that works efficiently with a large number of actions.", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "\u2022 Semantic similarity: we use the standard tf-idf (term-frequency inverse-documentfrequency) (Salton and McGill, 1986)", "startOffset": 93, "endOffset": 118}, {"referenceID": 12, "context": "Q0 is the simple DRRN introduced in He et al. (2016a).", "startOffset": 36, "endOffset": 54}, {"referenceID": 12, "context": "We carry out experiments on the task of predicting popular discussion threads on Reddit, as proposed by He et al. (2016b). Specifically, we conduct experiments on data from 5 subreddits including askscience, askmen, todayilearned,", "startOffset": 104, "endOffset": 122}, {"referenceID": 17, "context": "use experience replay (Lin, 1992) and the memory size is set to 10,000.", "startOffset": 22, "endOffset": 33}, {"referenceID": 9, "context": "Unlike in Fang et al. (2016), the summed karma scores do not follow a Zipfian distribution, so we do not use quantization or any nonlinear transformation.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "The first baseline, following the method in He et al. (2016b), uses a random subsampling approach to obtain Bt (with m = 10) and takes the max over them using DRRN-BiLSTM.", "startOffset": 44, "endOffset": 62}], "year": 2017, "abstractText": "This paper addresses the problem of predicting popularity of comments in an online discussion forum using reinforcement learning, particularly addressing two challenges that arise from having natural language state and action spaces. First, the state representation, which characterizes the history of comments tracked in a discussion at a particular point, is augmented to incorporate the global context represented by discussions on world events available in an external knowledge source. Second, a two-stage Q-learning framework is introduced, making it feasible to search the combinatorial action space while also accounting for redundancy among sub-actions. We experiment with five Reddit communities, showing that the two methods improve over previous reported results on this task.", "creator": "LaTeX with hyperref package"}}}