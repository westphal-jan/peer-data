{"id": "1704.06259", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "A Semantic QA-Based Approach for Text Summarization Evaluation", "abstract": "Many Natural Language Processing and Computational Linguistics applications involves the generation of new texts based on some existing texts, such as summarization, text simplification and machine translation. However, there has been a serious problem haunting these applications for decades, that is, how to automatically and accurately assess quality of these applications. In this paper, we will present some preliminary results on one especially useful and challenging problem in NLP system evaluation: how to pinpoint content differences of two text passages (especially for large pas-sages such as articles and books). Our idea is intuitive and very different from existing approaches. We treat one text passage as a small knowledge base, and ask it a large number of questions to exhaustively identify all content points in it. By comparing the correctly answered questions from two text passages, we will be able to compare their content precisely. The experiment using 2007 DUC summarization corpus clearly shows promising results.", "histories": [["v1", "Fri, 21 Apr 2017 15:32:01 GMT  (372kb)", "http://arxiv.org/abs/1704.06259v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ping chen", "fei wu", "tong wang"], "accepted": false, "id": "1704.06259"}, "pdf": {"name": "1704.06259.pdf", "metadata": {"source": "CRF", "title": "A Semantic QA-Based Approach for Text Summarization Evaluation", "authors": ["Ping Chen", "Fei Wu", "Tong Wang"], "emails": ["Ping.chen@umb.edu"], "sections": [{"heading": null, "text": "Computer linguistic applications involve generating new texts based on existing texts such as abstract, text simplification and machine translation. However, there has been a serious problem in these applications for decades, namely how to automatically and accurately assess the quality of these applications. In this article, we present some preliminary results on a particularly useful and challenging problem in assessing NLP systems - how to localize the content differences between two passages of text (especially in large passages such as articles and books). Our idea is intuitive and very different from existing approaches. We treat a passage of text as a small knowledge base and ask it a large number of questions in order to fully identify all the content points it contains. By comparing the correctly answered questions from two passages of text, we will be able to accurately compare their contents."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most people who are able to move are able to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move, to move and to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move, to move and to move."}, {"heading": "2 Related Work", "text": "Although our evaluation method is applicable to any application requiring semantic comparison of texts, our current experiments unfortunately focus on the evaluation of text summaries. In this section we will only discuss the existing work on the evaluation of summaries. Automatic summary of texts is the process of finding the most important contents from a document and creating a summary. How summaries are automatically evaluated remains a difficult problem [Jones 1995, Jing 1998, Steinberger 2012]. [Donaway 2000] introduced content-based measurement: Comparing the term frequency (tf) vectors of the machine summary with the tf vectors of the full text or human summary. The evaluation is based on \"bags of words\" or \"tf-idf\" model with cosmic similarity. However, it is likely that the summary vector is sparse compared to the document vector, and a summary of terms that are not frequently used in the complete document."}, {"heading": "3 A QA-Based method for semantic comparison of texts", "text": "Our automated evaluation method uses two NLP fields: Question Generation (QG), and Question Answer (QA). Its architecture is illustrated in Error! Reference source not found. First, the main idea is to generate a large number of questions from an original text passage in order to comprehensively cover its contents. In order to evaluate the contents of a newly created text passage, this means that this text passage (e.g. a summary, a simplified version, or a translation into another language) will use the new passage as the only source of knowledge to answer these questions. If a question is answered correctly, it means that this new text passage contains this piece of information. By examining all the correct answers, we can have an accurate measurement of the information contained in the new passage. By comparing the questions that can be answered by the original text passage, but we cannot be answered by a new passage, we can precisely determine the substantive differences between these two text passages Generation (QG) (question has been used in many areas)."}, {"heading": "4 Experiment", "text": "To test our idea, we built a proof-of-concept system using some existing QG and QA systems. For the QG component, we adapted the system developed in [Heilman 2011]. For the QA component, we adapted an open source QA framework, OpenEphyra, by replacing the passage retrieval component with a text search component that searches only within a document. We used the Document Understanding Conference (DUC) 2007 corpus, which contains 2 sets of text passages, the first set being the original documents divided into 45 topics. Each topic consists of 25 original documents. The second set of texts is the summaries of the individual topics. The summaries were generated by 2 baseline summarization systems, 30 participating summarization systems, and 4 human summarizers. All of these summarizers were evaluated by human applicants."}, {"heading": "5 Conclusion", "text": "In this article we present an innovative semantic evaluation method for different NLP applications by using QG and QA fields. Our method does not require any manual effort, is easy to interpret and illustrates details about evaluated NLP systems. Initial experiments to evaluate text summaries showed promising results."}], "references": [{"title": "A comparison of rankings produced by summarization evaluation measures.", "author": ["Robert L. Donaway", "Kevin W. Drummey", "Laura A. Mather"], "venue": "In Proceedings of the 2000 NAACL-ANLPWorkshop on Automatic summarization-Volume", "citeRegEx": "Donaway et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Donaway et al\\.", "year": 2000}, {"title": "Towards Automatic Topical Question Generation.", "author": ["Hasan", "Yllias Chali Sadid A"], "venue": "Proceedings of COLING", "citeRegEx": "Hasan and A.,? \\Q2012\\E", "shortCiteRegEx": "Hasan and A.", "year": 2012}, {"title": "Automated summarization evaluation with basic elements.", "author": ["Eduard Hovy", "Chin-Yew Lin", "Liang Zhou", "Junichi Fukumoto"], "venue": "In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Evaluating natural language processing systems: An analysis and review", "author": ["KS Jones", "Julia R. Galliers"], "venue": "Vol. 1083. Springer Science & Business Media,", "citeRegEx": "Jones and Galliers.,? \\Q1995\\E", "shortCiteRegEx": "Jones and Galliers.", "year": 1995}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q. Le", "T. Mikolov"], "venue": null, "citeRegEx": "Le and Mikolov,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Automatic factual question generation from text", "author": ["Heilman", "Michael"], "venue": "Diss. Carnegie Mellon University,", "citeRegEx": "Heilman and Michael.,? \\Q2011\\E", "shortCiteRegEx": "Heilman and Michael.", "year": 2011}, {"title": "Evaluating Content Selection in Summarization: The Pyramid Method.", "author": ["Ani Nenkova", "Rebecca J. Passonneau"], "venue": "In HLT-NAACL,", "citeRegEx": "Nenkova and Passonneau.,? \\Q2004\\E", "shortCiteRegEx": "Nenkova and Passonneau.", "year": 2004}, {"title": "Better summarization evaluation with word embeddings for rouge.\" arXiv preprint", "author": ["Jun-Ping Ng", "Viktoria Abrecht"], "venue": null, "citeRegEx": "Ng and Abrecht.,? \\Q2015\\E", "shortCiteRegEx": "Ng and Abrecht.", "year": 2015}, {"title": "Evaluation Measures for Text Summarization", "author": ["Josef Steinberger", "Karel Jeek."], "venue": "In: Computing and Informatics, vol 28.2, pp 251\u2013275", "citeRegEx": "Steinberger and Jeek.,? 2012", "shortCiteRegEx": "Steinberger and Jeek.", "year": 2012}, {"title": "A new evaluation measure using compression dissimilarity on text summarization", "author": ["Tong Wang", "Ping Chen", "Dan Simovici."], "venue": "Applied Intelligence (2016): 1-8.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Many Natural Language Processing and Computational Linguistics applications involves the generation of new texts based on some existing texts, such as summarization, text simplification and machine translation. However, there has been a serious problem haunting these applications for decades, that is, how to automatically and accurately assess quality of these applications. In this paper, we will present some preliminary results on one especially useful and challenging problem in NLP system evaluation \u2013 how to pinpoint content differences of two text passages (especially for large passages such as articles and books). Our idea is intuitive and very different from existing approaches. We treat one text passage as a small knowledge base, and ask it a large number of questions to exhaustively identify all content points in it. By comparing the correctly answered questions from two text passages, we will be able to compare their content precisely. The experiment using 2007 DUC summarization corpus clearly shows promising results.", "creator": "Microsoft\u00ae Word 2016"}}}