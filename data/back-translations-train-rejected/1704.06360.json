{"id": "1704.06360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "SwellShark: A Generative Model for Biomedical Named Entity Recognition without Labeled Data", "abstract": "We present SwellShark, a framework for building biomedical named entity recognition (NER) systems quickly and without hand-labeled data. Our approach views biomedical resources like lexicons as function primitives for autogenerating weak supervision. We then use a generative model to unify and denoise this supervision and construct large-scale, probabilistically labeled datasets for training high-accuracy NER taggers. In three biomedical NER tasks, SwellShark achieves competitive scores with state-of-the-art supervised benchmarks using no hand-labeled training data. In a drug name extraction task using patient medical records, one domain expert using SwellShark achieved within 5.1% of a crowdsourced annotation approach -- which originally utilized 20 teams over the course of several weeks -- in 24 hours.", "histories": [["v1", "Thu, 20 Apr 2017 23:02:14 GMT  (1047kb,D)", "http://arxiv.org/abs/1704.06360v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jason fries", "sen wu", "alex ratner", "christopher r\\'e"], "accepted": false, "id": "1704.06360"}, "pdf": {"name": "1704.06360.pdf", "metadata": {"source": "CRF", "title": "SWELLSHARK: A Generative Model for Biomedical Named Entity Recognition without Labeled Data", "authors": ["Jason Fries", "Sen Wu", "Alex Ratner", "Christopher R\u00e9"], "emails": ["jfries@cs.stanford.edu", "senwu@cs.stanford.edu", "ajratner@cs.stanford.edu", "chrismre@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Related Work", "text": "In the biomedical field, monitored methods that use CRFs are the standard (Settles, 2004; Leaman et al., 2008), although RNNs / LSTMs are becoming more common (Sahu and Anand, 2016; Dernoncourt et al., 2016); semi-monitored methods that extend labeled records by word embedding (Tang et al., 2014; Kuksa and Qi, 2010); or bootstrapping techniques (Vlachos and Gasperin, 2006) have shown that they exceed monitored baselines in tasks such as gene name recognition. Contrary to these existing approaches, SWELLSHARK does not require hand-labeled training data and is agnostic about the choice of discriminatory model."}, {"heading": "3 Background", "text": "Identification of designated entities is a core component of applied biomedical information extraction systems and a critical sub-task in normalization, where entities are assigned to canonical identifiers, and relationship extraction, where we identify semantic connections between entities. Ontologies are key artifacts in formalizing biological concepts for normalization and use in computerized systems. Biomedical NER focuses on identifying these concepts. For example, we would use a phrase like: \"Primary pulmonary hypertension is a rare, progressive, and incurable disease.\" To identify a disease, we must adopt laboratory laboratories as a binary classification task, i.e., each tagger predicts an entity type, although our method of generalizing multi-class settings without major changes."}, {"heading": "4 Methods", "text": "The SWELLSHARK pipeline is outlined in Figure 1 and consists of the following steps: 1) provision of unlabeled documents and definition of weak monitoring inputs; 2) use generators to transform documents into a set of classification candidates; 3) automatic generation of labeling functions using structured resources or user heuristics; 4) adaptation to a multinomial generative model using the output of all labeling functions applied to candidates; and 5) generation of probably labeled data that can then be used with any commercially available classification model. Details of each phase are described below."}, {"heading": "4.1 SwellShark Input", "text": "SWELLSHARK requires entering a collection of unlabeled documents and some form of weak oversight. This is typically a collection of lexicographs, ontologies, and optional heuristic rules. Oversight consists largely of indicating positive and negative lexicographs. A toy example could be a minimal specification of drug labels (1: antibotic, -1: amino acid, peptide, or protein, gene, or genome), with each semantic category assignment containing lexicographs in the Unified Medical Language System (UMLS) or other external dictionaries."}, {"heading": "4.2 Candidate Generators", "text": "Our approach first requires identifying a number of potential or candidate mentions in documents. We define a candidate generator as a function that converts a document collection D into a candidate set. Each candidate x is defined as a span at character level within a document set. Candidate generators are heuristics that can be restrictive, such as the amount of all dictionary matches or free, such as all overlapping grammar widths. The choice of heuristics affects overall performance, as candidates define the space in which we provide both monitoring and learning. We examine the following simple automated generators: \u2022 Noun phrases All noun phrases are compared with regular expressions versus POS tags. \u2022 Dictionary dictionaries with domain-specific stop word lists or other lexical curators. Each heuristic generates K-gram candidates."}, {"heading": "4.3 Labeling Function Generators", "text": "Labeling functions are a generalization of strategies used in remote monitoring. * In the marking of disease names, for example, we can define a labeling function that outputs 1 when a candidate occurs in a disease or syndrome lexicon, and another function that outputs -1 when it is found in a gene or genome lexicon. Several examples are shown in Figure 2.def LF _ in _ lexicon (c): t = c.text () return 1 when t is found in umls _ disease else 0def LF _ idf _ filter (c): return -1 when idf (c) < = 4.0 else 0def LF _ temporal _ modifiers (c): head = c.tokens (\"words\") i.i.else 0def LF _ idf _ filter (c): return -1 when idf _ idf _ id _ id _ filter (c): -labens ltens, c = temporary return (i.i.f)."}, {"heading": "4.4 Multinomial Generative Model", "text": "In this example, all variants of \"hypertension\" are found in a single lexi-con that overestimates the positive label probabilities for the ninth candidate. To address these, we extend the generative model to learn mutual exclusion. Crucially, for this change, it allows us to explore entity boundaries while maintaining the simple label semantics, i.e., we choose these label chances for the ninth candidate."}, {"heading": "4.5 Sampling for Dataset Construction", "text": "After the training, each chipset defines a multinomial distribution si = {(cij, pij); j = 1... k}, where p is the probability of each candidate within the chipset. We treat all chipsets in a set, S, as a sample distribution S \u0445 s1 \u00b7 s2 \u00b7... sN to create noisily labeled tag sequences. We assume that chipsets are independent and generate 10 samples per observed set, sampling once per chipset."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Weakly-supervised Taggers", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5.2 Materials and Setup", "text": "Datasets: We evaluate the performance of three bioinformatics datasets: (1) the NCBI Disease corpus (Dog an et al., 2014); (2) the BioCreative V Chemical Disease Relation task (CDR) corpus (Wei et al., 2015); and the i2b2-2009 Medication Extraction Challenge dataset (Uzuner et al., 2010a). NCBI Disease contains 792 PubMed Abstracts separated into Education, Development, and Test Subsets (n = 592 / 100 / 100); CDR contains 1,500 PubMed Abstracts (n = 500 / 500 / 500); and i2b2-2009 contains the Electronic Health Record (EHR) blurred summaries (n = 1000 / 124 / 125)."}, {"heading": "6 Results / Discussion", "text": "6.1 Candidate generation Figure 4 shows performance compromises on the scale of our three heuristics in CDR diseases with advanced labeling features. The use of a hand-controlled candidate generator allows our system to exceed monitored performance, but also fully automated methods perform very well. The bump in performance seen in 1K documents is an artifact of scale-up with an embedded CRF where there are some early matches. Table 2 contains values for PubMed taggers and compares manual and automated candidate generation heuristics on the scale. In both CDR tasks, we can closely match or exceed benchmarks reported by TaggerOne (from + 0.5 to -0.1 F1 points). In chemistry, NounPhrase candidate generation shows better memories and improves tuner handling by partially 0.7 points under the NCR disease comparison group."}, {"heading": "6.2 Autogenerating Supervision", "text": "Table 3 shows performance benchmarks for our models when only lexical resources are used for monitoring, without annotation guidelines or dataset-specific labeling features. In all cases, we find that the LSTM-CRF / emb models outperform the majority vote by 1.7 to 5.4 F1 points. In chemical labeling, we achieve a 1-1 point of the published TaggerOne benchmark; in NCBI, we perform much worse due to the candidate issues outlined above. Scaling & Automatic Feature Extraction: Figure 5 provides a more comprehensive picture of the scaling curve and the convergence differences between the human-generated feature library used for our CRF and LSTM-CRF models."}, {"heading": "6.3 A Tagger in 24 Hours", "text": "We are expanding this core system to include 21 custom rules of regular expression and other policy-specific labeling features for an overall score within 7% (6 F1 points) of the same model trained on hand-labeled data. Our approach quickly performed well and would most likely improve with more unlabeled training materials that are unfortunately not available for this task. Comparing our performance with the same task as crowdsourcing (Uzuner et al., 2010b), we are within 5.1% (4.4 F1 points) of the crowd macro average achieved by 79 commenters. This required two phases of labeling and adjustment over several weeks, although an accurate time estimate for drug names alone is difficult as it is one of 7 commented subtasks."}, {"heading": "7 Conclusion", "text": "In this work, we have shown that programmatic supervision provided by biomedical lexicographs and other heuristics can achieve competitive performance over state-of-the-art systems trained on hand-labeled data. SWELLSHARK accepts much weaker forms of supervision, allowing NER taggers to be built in a shorter time and in a more intuitive manner for domain experts. Our approach automatically scales large training sets, enabling SWELLSHARK to train high-performance taggers using the latest deep learning models."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Mobilize Center, a Big Data to Knowledge (BD2K) center of excellence at the National Institutes of Health, which is supported by the U54EB020405 grant."}, {"heading": "8 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Materials", "text": "Feature Library: Our CRF models use feature templates defined above the mention and its parent dependency parse tree. Features include context window features, parts of language tags, word form, word embedding, character N grams, morphology, domain dictionary membership, and the lemmatized form of the parent dependency tree word of a mention. We expand abbreviations in the document and share features across each identical mention that is linked within a document by a parenthetical mention."}, {"heading": "8.2 Results", "text": "Candidate Generation Precision / Recall Curves: Figure 6 shows the precision recall curves of candidate generation methods in detail. Note how domain engineering matchers suffer from recall problems, with only a small amount generalized beyond dictionaries and 20% of all mentions missing. Multinomial CRF Model: Figure 7 shows the power difference between multinomial and binary generative models when sample data is used to train noise-conscious implementations of a CRF and logistical regression. We see that the multinomial CRF model as a whole performs best in all k-gram decisions. k = 6 performs best in sequence models. Scaling: Tables 5 and 6 show some of the economies of scale where we see gains of up to 3.1 F1 points or 7.6 points improvement over majority decisions. Scaling: Tables 5 and 6 show some of the economies of scale advantages, where we see gains of up to 3.1 F1 points or 7.6 points improvement over majority decisions, are better than those seen in other automated systems, but still less than in majority decisions."}], "references": [{"title": "The unified medical language system (UMLS): integrating biomedical terminology", "author": ["Olivier Bodenreider."], "venue": "Nucleic acids research 32(suppl 1):D267\u2013D270.", "citeRegEx": "Bodenreider.,? 2004", "shortCiteRegEx": "Bodenreider.", "year": 2004}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Mark Craven", "Johan Kumlien"], "venue": "In ISMB. volume", "citeRegEx": "Craven and Kumlien,? \\Q1999\\E", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "The Comparative Toxicogenomics Database\u2019s 10th year anniversary: update", "author": ["Allan Peter Davis", "Cynthia J Grondin", "Kelley LennonHopkins", "Cynthia Saraceni-Richards", "Daniela Sciaky", "Benjamin L King", "Thomas C Wiegers", "Carolyn J Mattingly"], "venue": null, "citeRegEx": "Davis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2015}, {"title": "De-identification of patient notes with recurrent neural networks", "author": ["Franck Dernoncourt", "Ji Young Lee", "Ozlem Uzuner", "Peter Szolovits."], "venue": "Journal of the American Medical Informatics Association page ocw156.", "citeRegEx": "Dernoncourt et al\\.,? 2016", "shortCiteRegEx": "Dernoncourt et al\\.", "year": 2016}, {"title": "Ncbi disease corpus: a resource for disease name recognition and concept normalization", "author": ["Rezarta Islamaj Do\u011fan", "Robert Leaman", "Zhiyong Lu."], "venue": "Journal of biomedical informatics 47:1\u201310.", "citeRegEx": "Do\u011fan et al\\.,? 2014", "shortCiteRegEx": "Do\u011fan et al\\.", "year": 2014}, {"title": "Corleone: Hands-off crowdsourcing for entity matching", "author": ["Chaitanya Gokhale", "Sanjib Das", "AnHai Doan", "Jeffrey F Naughton", "Narasimhan Rampalli", "Jude Shavlik", "Xiaojin Zhu."], "venue": "Proceedings of the 2014 ACM SIGMOD international conference", "citeRegEx": "Gokhale et al\\.,? 2014", "shortCiteRegEx": "Gokhale et al\\.", "year": 2014}, {"title": "Mimic-iii, a freely accessible critical care database", "author": ["Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "Liwei H Lehman", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark."], "venue": "Scientific", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Semi-supervised bio-named entity recognition with word-codebook learning", "author": ["Pavel P Kuksa", "Yanjun Qi."], "venue": "Proceedings of the 2010 SIAM International Conference on Data Mining. SIAM, pages 25\u201336.", "citeRegEx": "Kuksa and Qi.,? 2010", "shortCiteRegEx": "Kuksa and Qi.", "year": 2010}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "arXiv preprint arXiv:1603.01360 .", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Banner: an executable survey of advances in biomedical named entity recognition", "author": ["Robert Leaman", "Graciela Gonzalez"], "venue": "In Pacific symposium on biocomputing", "citeRegEx": "Leaman and Gonzalez,? \\Q2008\\E", "shortCiteRegEx": "Leaman and Gonzalez", "year": 2008}, {"title": "Taggerone: Joint named entity recognition and normalization with semi-markov models", "author": ["Robert Leaman", "Zhiyong Lu."], "venue": "Bioinformatics page btw343.", "citeRegEx": "Leaman and Lu.,? 2016", "shortCiteRegEx": "Leaman and Lu.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Deep distant supervision: Learning statistical relational models for weak supervision in natural language extraction", "author": ["Sriraam Natarajan", "Ameet Soni", "Anurag Wazalwar", "Dileep Viswanathan", "Kristian Kersting."], "venue": "Solving Large Scale Learning", "citeRegEx": "Natarajan et al\\.,? 2016", "shortCiteRegEx": "Natarajan et al\\.", "year": 2016}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs)", "author": ["Naoaki Okazaki."], "venue": "http://www.chokkan.org/software/crfsuite/.", "citeRegEx": "Okazaki.,? 2007", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Representation of rare diseases in health information systems: the orphanet approach to serve a wide range of end users", "author": ["Ana Rath", "Annie Olry", "Ferdinand Dhombres", "Maja Mili\u010di\u0107 Brandt", "Bruno Urbero", "Segolene Ayme."], "venue": "Human mutation", "citeRegEx": "Rath et al\\.,? 2012", "shortCiteRegEx": "Rath et al\\.", "year": 2012}, {"title": "Data programming: Creating large training sets, quickly", "author": ["Alexander Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher R\u00e9."], "venue": "arXiv preprint arXiv:1605.07723 .", "citeRegEx": "Ratner et al\\.,? 2016", "shortCiteRegEx": "Ratner et al\\.", "year": 2016}, {"title": "Crowdsourcing research opportunities: lessons from natural language processing", "author": ["Marta Sabou", "Kalina Bontcheva", "Arno Scharl."], "venue": "Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technolo-", "citeRegEx": "Sabou et al\\.,? 2012", "shortCiteRegEx": "Sabou et al\\.", "year": 2012}, {"title": "Recurrent neural network models for disease name recognition using domain invariant features", "author": ["Sunil Kumar Sahu", "Ashish Anand."], "venue": "arXiv preprint arXiv:1606.09371 .", "citeRegEx": "Sahu and Anand.,? 2016", "shortCiteRegEx": "Sahu and Anand.", "year": 2016}, {"title": "Disease ontology: a backbone for disease semantic integration", "author": ["Lynn Marie Schriml", "Cesar Arze", "Suvarna Nadendla", "Yu-Wei Wayne Chang", "Mark Mazaitis", "Victor Felix", "Gang Feng", "Warren Alden Kibbe."], "venue": "Nucleic acids research 40(D1):D940\u2013D946.", "citeRegEx": "Schriml et al\\.,? 2012", "shortCiteRegEx": "Schriml et al\\.", "year": 2012}, {"title": "Biomedical named entity recognition using conditional random fields and rich feature sets", "author": ["Burr Settles."], "venue": "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Association for", "citeRegEx": "Settles.,? 2004", "shortCiteRegEx": "Settles.", "year": 2004}, {"title": "Evaluating word representation features in biomedical named entity recognition tasks", "author": ["Buzhou Tang", "Hongxin Cao", "Xiaolong Wang", "Qingcai Chen", "Hua Xu."], "venue": "BioMed research international 2014.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "An overview of the BIOASQ", "author": ["George Tsatsaronis", "Georgios Balikas", "Prodromos Malakasiotis", "Ioannis Partalas", "Matthias Zschunke", "Michael R Alvers", "Dirk Weissenborn", "Anastasia Krithara", "Sergios Petridis", "Dimitris Polychronopoulos"], "venue": null, "citeRegEx": "Tsatsaronis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsatsaronis et al\\.", "year": 2015}, {"title": "Extracting medication information from clinical text", "author": ["\u00d6zlem Uzuner", "Imre Solti", "Eithon Cadag."], "venue": "Journal of the American Medical Informatics Association 17(5):514\u2013518.", "citeRegEx": "Uzuner et al\\.,? 2010a", "shortCiteRegEx": "Uzuner et al\\.", "year": 2010}, {"title": "Community annotation experiment for ground truth generation for the i2b2 medication challenge", "author": ["\u00d6zlem Uzuner", "Imre Solti", "Fei Xia", "Eithon Cadag."], "venue": "Journal of the American Medical Informatics Association 17(5):519\u2013523.", "citeRegEx": "Uzuner et al\\.,? 2010b", "shortCiteRegEx": "Uzuner et al\\.", "year": 2010}, {"title": "Bootstrapping and evaluating named entity recognition in the biomedical domain", "author": ["Andreas Vlachos", "Caroline Gasperin."], "venue": "Proceedings of the HLTNAACL BioNLP Workshop on Linking Natural Language and Biology. Association for Computational", "citeRegEx": "Vlachos and Gasperin.,? 2006", "shortCiteRegEx": "Vlachos and Gasperin.", "year": 2006}, {"title": "Overview of the biocreative v chemical disease relation (cdr) task", "author": ["Chih-Hsuan Wei", "Yifan Peng", "Robert Leaman", "Allan Peter Davis", "Carolyn J Mattingly", "Jiao Li", "Thomas C Wiegers", "Zhiyong Lu."], "venue": "Proceedings of the fifth BioCre-", "citeRegEx": "Wei et al\\.,? 2015", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Bioportal: enhanced functionality via new web services from the national center for biomedical ontology to access", "author": ["Patricia L Whetzel", "Natalya F Noy", "Nigam H Shah", "Paul R Alexander", "Csongor Nyulas", "Tania Tudorache", "Mark A Musen"], "venue": null, "citeRegEx": "Whetzel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Whetzel et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "Recent success in deep learning for NER (Lample et al., 2016) suggests that automatic feature extraction will largely replace this process.", "startOffset": 40, "endOffset": 61}, {"referenceID": 17, "context": "How do we obtain enough training data to fit these complex models? Crowdsourcing offers one way of generating large-scale labeled data, but the process is expensive, especially when annotators require specialized domain knowledge or data has privacy concerns preventing distribution (Sabou et al., 2012; Gokhale et al., 2014).", "startOffset": 283, "endOffset": 325}, {"referenceID": 6, "context": "How do we obtain enough training data to fit these complex models? Crowdsourcing offers one way of generating large-scale labeled data, but the process is expensive, especially when annotators require specialized domain knowledge or data has privacy concerns preventing distribution (Sabou et al., 2012; Gokhale et al., 2014).", "startOffset": 283, "endOffset": 325}, {"referenceID": 12, "context": "In NLP, another common approach is distant supervision (Mintz et al., 2009) where structured resources like ontologies and knowledge bases are used to heuristically label training data.", "startOffset": 55, "endOffset": 75}, {"referenceID": 1, "context": "Distant supervision is commonly used with a few, canonical structured resources like Freebase (Bollacker et al., 2008), weighting each resource equally when labeling data.", "startOffset": 94, "endOffset": 118}, {"referenceID": 27, "context": "rated resources; NCBO Bioportal (Whetzel et al., 2011) currently houses 541 distinct biomedical ontologies.", "startOffset": 32, "endOffset": 54}, {"referenceID": 20, "context": "vised methods using CRFs are the standard (Settles, 2004; Leaman et al., 2008) though RNNs/LSTMs are increasingly common (Sahu and Anand, 2016; Dernoncourt et al.", "startOffset": 42, "endOffset": 78}, {"referenceID": 18, "context": ", 2008) though RNNs/LSTMs are increasingly common (Sahu and Anand, 2016; Dernoncourt et al., 2016).", "startOffset": 50, "endOffset": 98}, {"referenceID": 4, "context": ", 2008) though RNNs/LSTMs are increasingly common (Sahu and Anand, 2016; Dernoncourt et al., 2016).", "startOffset": 50, "endOffset": 98}, {"referenceID": 21, "context": "Semi-supervised methods that augment labeled datasets with word embeddings (Tang et al., 2014; Kuksa and Qi, 2010) or bootstrapping techniques (Vlachos and Gasperin, 2006) have been shown to outperform supervised baselines in tasks like gene name recognition.", "startOffset": 75, "endOffset": 114}, {"referenceID": 8, "context": "Semi-supervised methods that augment labeled datasets with word embeddings (Tang et al., 2014; Kuksa and Qi, 2010) or bootstrapping techniques (Vlachos and Gasperin, 2006) have been shown to outperform supervised baselines in tasks like gene name recognition.", "startOffset": 75, "endOffset": 114}, {"referenceID": 25, "context": ", 2014; Kuksa and Qi, 2010) or bootstrapping techniques (Vlachos and Gasperin, 2006) have been shown to outperform supervised baselines in tasks like gene name recognition.", "startOffset": 56, "endOffset": 84}, {"referenceID": 12, "context": "Distant supervision (Craven et al., 1999; Mintz et al., 2009) uses knowledge bases to supervise relation extraction tasks.", "startOffset": 20, "endOffset": 61}, {"referenceID": 12, "context": ", 1999; Mintz et al., 2009) uses knowledge bases to supervise relation extraction tasks. Recent methods incorporate more generalized knowledge into extraction systems. Natarajan et al.(2016) used Markov Logic Networks to encode commonsense domain knowledge like \u201chome teams are more likely to win a game\u201d and generate weak training examples.", "startOffset": 8, "endOffset": 191}, {"referenceID": 16, "context": "Data programming: Ratner et al. (2016) proposed data programming as a method for programmatic training set creation.", "startOffset": 18, "endOffset": 39}, {"referenceID": 0, "context": "As a toy example, a minimal drug tagger specification could be (1: antibotic, -1: amino acid, peptide, or protein, gene or genome), with each semantic category mapping to source lexicons in the Unified Medical Language System (UMLS) (Bodenreider, 2004) or other external dictionaries.", "startOffset": 233, "endOffset": 252}, {"referenceID": 11, "context": "Comparison Systems: For our baseline comparison system, we use reported benchmarks from TaggerOne (Leaman and Lu, 2016) a state-of-theart general purpose biomedical NER tagger.", "startOffset": 98, "endOffset": 119}, {"referenceID": 5, "context": "Datasets: We evaluate performance on three bioinformatics datasets: (1) the NCBI Disease corpus (Do\u011fan et al., 2014); (2) the BioCreative V Chemical Disease Relation task (CDR) corpus (Wei et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 26, "context": ", 2014); (2) the BioCreative V Chemical Disease Relation task (CDR) corpus (Wei et al., 2015); and the i2b2-2009 Medication Ex-", "startOffset": 75, "endOffset": 93}, {"referenceID": 23, "context": "traction Challenge dataset (Uzuner et al., 2010a).", "startOffset": 27, "endOffset": 49}, {"referenceID": 22, "context": "For unlabeled PubMed data, we use a 100K document sample chosen uniformly at random from the BioASQ Task 4a challenge (Tsatsaronis et al., 2015) dataset.", "startOffset": 118, "endOffset": 144}, {"referenceID": 7, "context": "4M clinical narratives made available as part of the MIMIC-III critical care database (Johnson et al., 2016).", "startOffset": 86, "endOffset": 108}, {"referenceID": 19, "context": "Ontologies used in this work include those provided as part of the 2014AB release of the UMLS, and various other disease/chemical ontologies (Schriml et al., 2012; Davis et al., 2015; Rath et al., 2012; K\u00f6hler et al., 2013).", "startOffset": 141, "endOffset": 223}, {"referenceID": 3, "context": "Ontologies used in this work include those provided as part of the 2014AB release of the UMLS, and various other disease/chemical ontologies (Schriml et al., 2012; Davis et al., 2015; Rath et al., 2012; K\u00f6hler et al., 2013).", "startOffset": 141, "endOffset": 223}, {"referenceID": 15, "context": "Ontologies used in this work include those provided as part of the 2014AB release of the UMLS, and various other disease/chemical ontologies (Schriml et al., 2012; Davis et al., 2015; Rath et al., 2012; K\u00f6hler et al., 2013).", "startOffset": 141, "endOffset": 223}, {"referenceID": 14, "context": "Discriminative Models: We use two external sequence models: CRFsuite (Okazaki, 2007) and a bidirectional LSTM-CRF hybrid (Lample et al.", "startOffset": 69, "endOffset": 84}, {"referenceID": 9, "context": "Discriminative Models: We use two external sequence models: CRFsuite (Okazaki, 2007) and a bidirectional LSTM-CRF hybrid (Lample et al., 2016) which makes use of both word and characterlevel embeddings.", "startOffset": 121, "endOffset": 142}, {"referenceID": 24, "context": "Comparing our performance to the same task as done with crowdsourcing (Uzuner et al., 2010b), we are within 5.", "startOffset": 70, "endOffset": 92}], "year": 2017, "abstractText": "We present SWELLSHARK, a framework for building biomedical named entity recognition (NER) systems quickly and without hand-labeled data. Our approach views biomedical resources like lexicons as function primitives for autogenerating weak supervision. We then use a generative model to unify and denoise this supervision and construct large-scale, probabilistically labeled datasets for training high-accuracy NER taggers. In three biomedical NER tasks, SWELLSHARK achieves competitive scores with state-of-the-art supervised benchmarks using no hand-labeled training data. In a drug name extraction task using patient medical records, one domain expert using SWELLSHARK achieved within 5.1% of a crowdsourced annotation approach \u2013 which originally utilized 20 teams over the course of several weeks \u2013 in 24 hours.", "creator": "LaTeX with hyperref package"}}}