{"id": "1704.06380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Improving Context Aware Language Models", "abstract": "Increased adaptability of RNN language models leads to improved predictions that benefit many applications. However, current methods do not take full advantage of the RNN structure. We show that the most widely-used approach to adaptation (concatenating the context with the word embedding at the input to the recurrent layer) is outperformed by a model that has some low-cost improvements: adaptation of both the hidden and output layers. and a feature hashing bias term to capture context idiosyncrasies. Experiments on language modeling and classification tasks using three different corpora demonstrate the advantages of the proposed techniques.", "histories": [["v1", "Fri, 21 Apr 2017 02:27:26 GMT  (92kb,D)", "http://arxiv.org/abs/1704.06380v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaron jaech", "mari ostendorf"], "accepted": false, "id": "1704.06380"}, "pdf": {"name": "1704.06380.pdf", "metadata": {"source": "CRF", "title": "Improving Context Aware Language Models", "authors": ["Aaron Jaech", "Mari Ostendorf"], "emails": ["ostendor}@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "The dominant paradigm for language model fitting is based on the concept of a domain. Domains are in many ways inadequate context representations because they are poorly defined, discrete, and incomparable, and do not reflect the diversity of human language (Ruder et al., 2016). In context-conscious language models, the notion of a domain is replaced by a series of context variables, each describing an aspect of the associated language such as the subject, time, or language. These variables can be dynamically combined to create a continuous representation of the context as a low-dimensional embedding (Tang et al., 2016). Context variables and context embedding can then be used to adapt a recursive neural language model (RNNLM).The standard approach to using a context as a low-dimensional embedding (Tang et al.)."}, {"heading": "2 Model", "text": "Our model is based on a standard RNN language model. There are three key elements that we will discuss below: how to present the context using low-dimensional embedding, the mechanism to use context embedding to adjust the recurring layer, and the mechanisms to adjust the output layer."}, {"heading": "2.1 Representing outside context", "text": "We assume that accessing one or more indicator variables, c1: n = c1, c2,.. cn, which contain information about the outer context for each sentence, can be indicators for topic, geographical region, period or other metadata. In (Mikolov and Zweig, 2012), LDA subject vectors are used for the outer context. In (Tang et al., 2016), the outer context is a sentiment score and a product ID for a product review dataset. we use their method of combining information from multiple context variables using a simple neural network. This strategy is well suited for the types of context variables we will see in our experiments, such as the identity of the speaker. In other cases, it may be more appropriate to use topic models (Chen et al., 2015; Ghosh et al., 2016) or an RNN (Hoang et al., 2016) to learn the context ~ for each context."}, {"heading": "2.2 Adapting the hidden layer", "text": "The equation for the hidden layer of an RNN eats = \u03c3 (U ~ wt + Sst \u2212 1 + b1), where ~ wt is the word embedding the t-word, st \u2212 1 is the hidden state from the previous time step and \u03c3 is the activation function. To use the embedding of the context, ~ c, to adapt the hidden layer, the term F ~ c is inserted as an additive adaptation of the hidden layer, except for a customized bias term. It can be implemented simply by linking the context vector ~ c with the word embedding ~ wt at each time step to the input to the recursive layer. To increase the adaptability of the hidden layer, we use a context-dependent multiplicative recalculation of the hidden layer weights."}, {"heading": "2.3 Adapting the output layer", "text": "In fact, it is that most people are able to survive themselves, and that they are able to survive themselves. (...) Most people who are able to survive themselves are able to survive themselves. (...) Most people who are able to survive themselves are not able to survive themselves. (...) Most people who are able to survive themselves are not able to survive themselves. (...) The others who are able to survive themselves. (...) The others who are able to survive themselves. (...) The others, who are able to survive themselves, are not able to survive themselves. (...)"}, {"heading": "3 Data", "text": "In recent years, the number of people who are able to survive in the US has decreased significantly."}, {"heading": "4 Experiments", "text": "In these experiments, we specify the size of the word embedding dimensions and recurring layers so as not to exhaust our computing resources and then vary the various mechanisms for adjusting the model. We used an LSTM with coupled input and forgetfulness gates to reduce computing time by 20% (Greff et al., 2016). Dropout was used as the regulator for the inputs and outputs of the recurring layer, as described in Zaremba et al. (2014). If the vocabulary is large, calculating the complete entropy loss can be prohibitively expensive. For the large vocabulary experiments, we used a sampled Softmax strategy with a unigram distribution to accelerate training (Jean et al., 2015). A summary of the most important hyperparameters for each class of experiments can be prohibitive if it is given in Table 2 million. The total parameters are based on the unadjusted model, depending on which one of these old models is adapted from the table."}, {"heading": "4.1 Reddit Experiments", "text": "The aforementioned subredtieirVnree\u00fcb dme eeisrVnlrtee\u00fcgn rf\u00fc ide eeisrVnlrtee\u00fcgn rf\u00fc ide eeisrVnlrtee\u00fcgn rf\u00fc ide eeisrVnlrtee\u00fcgn rf\u00fc ide eeisrVnlrtee\u00fcgn rf\u00fc ide eeisrteeVnlrteeeign rf\u00fc ide eeisrdne eeisrtee\u00fcgn rf\u00fc ide eeisrteeVnlrteeeee\u00fcgn rf\u00fc ide eeisdne eeisdne eBnlrtee\u00fcgn rf\u00fc the eeisne eBnlrteeu"}, {"heading": "4.2 Twitter experiments", "text": "The language context embedding the vector dimensionality has been set to 8. If both the vocabulary and the number of contexts are small, as in this case, there is no risk of hash collusion. We disable the bloom filter, which essentially equates the hash adaptation with context-dependent bias vectors. Table 6 reports on the results of the experiments on the Twitter corpus. We calculate both the perplexity and the performance of the models on a language identification task. In terms of perplexity, the best models do not use the multiplicative hidden layer adaptation, which is consistent with the results of the Reddit corpus. In general, the improvement in the perplexity of the adaptation on this corpus is small (less than 5%) compared to our other experiments where we saw relative improvements of two to four times larger."}, {"heading": "4.3 SCOTUS experiments", "text": "Table 7 lists the results of the experiments on the SCOTUS corpus. The size of the context embedding is 9, 15 and 8 for the case, the speaker and the role variables. To calculate the confusion, we used 60,000 sentences of assessment set. For the classification experiment, we selected 4,000 sentences from the test data of eleven different judges and tried to classify the identity of justice. The confusion of the distribution of judges over these sentences is 8.9 (11.0 would be uniform). So the data is roughly balanced. In classifying judges, the case context is given to the model variable, but we make no specific effort to filter candidates based on who served on the court during that time, i.e. all eleven judges are considered for each case.For both perplexity and classification metrics, the hash fit makes a big difference. The model, where only hash fit and no hidden layer is used, each model has a better perplexity."}, {"heading": "5 Related Work", "text": "Several studies cover the early history of the adaptation of language models (DeMori and Federico, 1999; Bellegarda, 2004). We mention only the recent, closely related work. Multiplicative recalculation of recurring layer weights is used in the hypernetwork model (Ha et al., 2016). The focus of this model is to allow the LSTM to automatically adapt depending on the context of the previous words. This differs from our work in that we adapt to contextual information outside the word sequence. Gangireddy et al. (2016) also use a recalculation of the hidden layer for adaptation, but it is fine-tuned and not during training like our model. The RNME model by Mikolov et al. (2011) uses hashing to train a maximum entropy model alongside an RNN language model. The setup is similar to our method to learn context-dependent biases."}, {"heading": "6 Conclusions & Future Work", "text": "Our results suggest that there is no unified approach to the adaptation of language models, but it is clear that we are improving on the standard adaptation approach. Mikolov and Zweig's model (2012), which corresponds only to the use of additive adjustments at the hidden level and the slight adjustment of the output layer, is surpassed by all three datasets in both language modeling and classification tasks. In language modeling, the multiplicative adjustment of the hidden layer was helpful only for the SCTOUS dataset. However, the combined adjustment of the low-rank output layer and hash consistently yielded the best confusion. For the classification tasks, the multiplicative adjustment of the hidden layer could clearly be useful, as well as the combined adjustment of the low-rank output layer and hash. It is important that there is not always a strong relationship between perplexity and classification results. For the classification tasks, the multiplicative adjustment of the hidden layer could clearly be useful, as well as the combined adjustment of the low-rank output layer and hash output. It is important that there is not always a strong relationship between perplexity and classification results."}], "references": [{"title": "Statistical language model adaptation: review and perspectives", "author": ["Jerome R Bellegarda."], "venue": "Speech Communication 42(1):93\u2013108.", "citeRegEx": "Bellegarda.,? 2004", "shortCiteRegEx": "Bellegarda.", "year": 2004}, {"title": "Natural Language Processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Space/time trade-offs in hash coding with allowable errors", "author": ["Burton H Bloom."], "venue": "Communications of the ACM 13(7):422\u2013426.", "citeRegEx": "Bloom.,? 1970", "shortCiteRegEx": "Bloom.", "year": 1970}, {"title": "Recurrent neural network language model adaptation for multi-genre broadcast speech recognition", "author": ["Xie Chen", "Tian Tan", "Xunying Liu", "Pierre Lanchantin", "Moquan Wan", "Mark JF Gales", "Philip C Woodland."], "venue": "Proceedings of InterSpeech.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Language model adaptation", "author": ["Renato DeMori", "Marcello Federico."], "venue": "Computational models of speech pattern processing, Springer, pages 280\u2013 303.", "citeRegEx": "DeMori and Federico.,? 1999", "shortCiteRegEx": "DeMori and Federico.", "year": 1999}, {"title": "Sparse additive generative models of text", "author": ["Jacob Eisenstein", "Amr Ahmed", "Eric P Xing"], "venue": null, "citeRegEx": "Eisenstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Unsupervised adaptation of recurrent neural network language models", "author": ["Siva Reddy Gangireddy", "Pawel Swietojanski", "Peter Bell", "Steve Renals."], "venue": "Interspeech 2016 pages 2333\u20132337.", "citeRegEx": "Gangireddy et al\\.,? 2016", "shortCiteRegEx": "Gangireddy et al\\.", "year": 2016}, {"title": "Contextual LSTM (CLSTM) models for large scale NLP tasks", "author": ["Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Tom Dean", "Larry Heck."], "venue": "arXiv preprint arXiv:1602.06291 .", "citeRegEx": "Ghosh et al\\.,? 2016", "shortCiteRegEx": "Ghosh et al\\.", "year": 2016}, {"title": "LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems", "author": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Hypernetworks", "author": ["David Ha", "Andrew Dai", "Quoc V Le."], "venue": "arXiv preprint arXiv:1609.09106 .", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Incorporating side information into recurrent neural network language models", "author": ["Cong Duy Vu Hoang", "Trevor Cohn", "Gholamreza Haffari."], "venue": "HLTNAACL.", "citeRegEx": "Hoang et al\\.,? 2016", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Exceptions in language as learned by the multi-factor sparse plus low-rank language model", "author": ["Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel."], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, pages", "citeRegEx": "Hutchinson et al\\.,? 2013", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2013}, {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01462 .", "citeRegEx": "Inan et al\\.,? 2016", "shortCiteRegEx": "Inan et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein."], "venue": "CoRR abs/1511.03962.", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei."], "venue": "arXiv preprint arXiv:1506.02078 .", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "preprint arXiv:1603.06155 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Strategies for training large scale neural network language models", "author": ["Tom\u00e1\u0161 Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE,", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig."], "venue": "SLT . pages 234\u2013239.", "citeRegEx": "Mikolov and Zweig.,? 2012", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Generalizing and hybridizing count-based and neural language models", "author": ["Graham Neubig", "Chris Dyer."], "venue": "arXiv preprint arXiv:1606.00499 .", "citeRegEx": "Neubig and Dyer.,? 2016", "shortCiteRegEx": "Neubig and Dyer.", "year": 2016}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf."], "venue": "arXiv preprint arXiv:1608.05859 .", "citeRegEx": "Press and Wolf.,? 2016", "shortCiteRegEx": "Press and Wolf.", "year": 2016}, {"title": "Learning to generate reviews and discovering sentiment", "author": ["Alec Radford", "Rafal Jozefowicz", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1704.01444 .", "citeRegEx": "Radford et al\\.,? 2017", "shortCiteRegEx": "Radford et al\\.", "year": 2017}, {"title": "Towards a continuous modeling of natural language domains", "author": ["Sebastian Ruder", "Parsa Ghaffari", "John G Breslin."], "venue": "arXiv preprint arXiv:1610.09158 .", "citeRegEx": "Ruder et al\\.,? 2016", "shortCiteRegEx": "Ruder et al\\.", "year": 2016}, {"title": "Randomized language models via perfect hash functions", "author": ["David Talbot", "Thorsten Brants."], "venue": "ACL. volume 8, pages 505\u2013513.", "citeRegEx": "Talbot and Brants.,? 2008", "shortCiteRegEx": "Talbot and Brants.", "year": 2008}, {"title": "Context-aware natural language generation with recurrent neural networks", "author": ["Jian Tang", "Yifan Yang", "Sam Carton", "Ming Zhang", "Qiaozhu Mei."], "venue": "arXiv preprint arXiv:1611.09900 .", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Characterizing the language of online communities and its relation to community reception", "author": ["Trang Tran", "Mari Ostendorf."], "venue": "arXiv preprint arXiv:1609.04779 .", "citeRegEx": "Tran and Ostendorf.,? 2016", "shortCiteRegEx": "Tran and Ostendorf.", "year": 2016}, {"title": "Recurrent neural network based language model personalization by social network crowdsourcing", "author": ["Tsung-Hsien Wen", "Aaron Heidel", "Hung-yi Lee", "Yu Tsao", "Lin-Shan Lee."], "venue": "INTERSPEECH. pages 2703\u20132707.", "citeRegEx": "Wen et al\\.,? 2013", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Randomized maximum entropy language models", "author": ["Puyang Xu", "Sanjeev Khudanpur", "Asela Gunawardana."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE, pages 226\u2013230.", "citeRegEx": "Xu et al\\.,? 2011", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Generative and discriminative text classification with recurrent neural networks", "author": ["Dani Yogatama", "Chris Dyer", "Wang Ling", "Phil Blunsom"], "venue": null, "citeRegEx": "Yogatama et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2017}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Domains are in many ways inadequate representations of context due to being ill-defined, discrete and incomparable, and not reflective of the diversity of human language (Ruder et al., 2016).", "startOffset": 170, "endOffset": 190}, {"referenceID": 24, "context": "These variables can be dynamically combined to create a continuous representation of context as a lowdimensional embedding (Tang et al., 2016).", "startOffset": 123, "endOffset": 142}, {"referenceID": 18, "context": "The standard approach for using a context embedding to adapt an RNNLM is to simply concatenate the context representation with the word embedding at the input to the RNN (Mikolov and Zweig, 2012).", "startOffset": 170, "endOffset": 195}, {"referenceID": 3, "context": "2016), adapting an LM to different genres of television shows (Chen et al., 2015), adapting to long range dependencies in a document (Ji et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 14, "context": ", 2015), adapting to long range dependencies in a document (Ji et al., 2015), sharing information in generative text classifiers (Yogatama et al.", "startOffset": 59, "endOffset": 76}, {"referenceID": 28, "context": ", 2015), sharing information in generative text classifiers (Yogatama et al., 2017), and in other cases as well.", "startOffset": 60, "endOffset": 83}, {"referenceID": 18, "context": "The method from Mikolov and Zweig (2012) of using the lowdimensional context embedding to adapt the output layer avoids the excessive memory issue of context-dependent bias vectors but our experiments show that it does not capture isolated but important details.", "startOffset": 16, "endOffset": 41}, {"referenceID": 18, "context": "In (Mikolov and Zweig, 2012) LDA topic vectors are used for the outside context.", "startOffset": 3, "endOffset": 28}, {"referenceID": 24, "context": "In (Tang et al., 2016) the outside context is a sentiment score and a product id for a product review dataset.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "more appropriate to use topic models (Chen et al., 2015; Ghosh et al., 2016) or an RNN (Hoang et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 7, "context": "more appropriate to use topic models (Chen et al., 2015; Ghosh et al., 2016) or an RNN (Hoang et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 10, "context": ", 2016) or an RNN (Hoang et al., 2016) to build the context representation.", "startOffset": 18, "endOffset": 38}, {"referenceID": 9, "context": "The method is borrowed from Ha et al. (2016) where it is used for dynamically adjusting the parameters of a language model in response to the previous words in the sentence.", "startOffset": 28, "endOffset": 45}, {"referenceID": 20, "context": "In our case, we tie the weights between the word embeddings in the input and output layer: WT = V (Press and Wolf, 2016; Inan et al., 2016).", "startOffset": 98, "endOffset": 139}, {"referenceID": 12, "context": "In our case, we tie the weights between the word embeddings in the input and output layer: WT = V (Press and Wolf, 2016; Inan et al., 2016).", "startOffset": 98, "endOffset": 139}, {"referenceID": 18, "context": "Mikolov and Zweig (2012) use a low-rank factorization of of the adaptation matrix, replacing the |V |\u00d7|C|matrix with the product of a matrix G of size |V | \u00d7 k and a context embedding ~c of size k.", "startOffset": 0, "endOffset": 25}, {"referenceID": 17, "context": "Hash collusions are known to negatively effect the perplexity (Mikolov et al., 2011).", "startOffset": 62, "endOffset": 84}, {"referenceID": 2, "context": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).", "startOffset": 135, "endOffset": 190}, {"referenceID": 23, "context": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).", "startOffset": 135, "endOffset": 190}, {"referenceID": 27, "context": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).", "startOffset": 135, "endOffset": 190}, {"referenceID": 1, "context": "The Reddit and SCOTUS data are tokenized and lower-cased using the standard NLTK tokenizer (Bird et al., 2009).", "startOffset": 91, "endOffset": 110}, {"referenceID": 8, "context": "We used an LSTM with coupled input and forget gates for a 20% reduction in computation time (Greff et al., 2016).", "startOffset": 92, "endOffset": 112}, {"referenceID": 13, "context": "For the large vocabulary experiments, we used a sampled softmax strategy with a unigram distribution to speed up training (Jean et al., 2015).", "startOffset": 122, "endOffset": 141}, {"referenceID": 8, "context": "We used an LSTM with coupled input and forget gates for a 20% reduction in computation time (Greff et al., 2016). Dropout was used as a regularizer on the input and outputs of the recurrent layer as described in Zaremba et al. (2014). When the vocabulary is large, computing the full cross-entropy loss can be prohibitively expensive.", "startOffset": 93, "endOffset": 234}, {"referenceID": 25, "context": "These are the same subreddit used in Tran and Ostendorf (2016) for a related but not comparable classification task.", "startOffset": 37, "endOffset": 63}, {"referenceID": 15, "context": "Furthermore, we find that a single dimension in the hidden state of the unadapted model is often enough to distinguish between different languages even though the model was not given any supervision signal (Karpathy et al., 2015; Radford et al., 2017).", "startOffset": 206, "endOffset": 251}, {"referenceID": 21, "context": "Furthermore, we find that a single dimension in the hidden state of the unadapted model is often enough to distinguish between different languages even though the model was not given any supervision signal (Karpathy et al., 2015; Radford et al., 2017).", "startOffset": 206, "endOffset": 251}, {"referenceID": 4, "context": "Multiple survey papers cover the early history of language model adaptation (DeMori and Federico, 1999; Bellegarda, 2004).", "startOffset": 76, "endOffset": 121}, {"referenceID": 0, "context": "Multiple survey papers cover the early history of language model adaptation (DeMori and Federico, 1999; Bellegarda, 2004).", "startOffset": 76, "endOffset": 121}, {"referenceID": 9, "context": "layer weights is used in the Hypernetwork model (Ha et al., 2016).", "startOffset": 48, "endOffset": 65}, {"referenceID": 6, "context": "Gangireddy et al. (2016) also use a rescaling of the hidden layer for adaptation but it is done as a fine-tuning step and not during training like our model.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "The RNNME model from Mikolov et al. (2011) uses feature hashing to train a maximum entropy model alongside an RNN language model.", "startOffset": 21, "endOffset": 43}, {"referenceID": 16, "context": "The RNNME model from Mikolov et al. (2011) uses feature hashing to train a maximum entropy model alongside an RNN language model. The setup is similar to our method of using hashing to learn context-dependent biases. However, there are a number of differences. The motivation for the RNNME model was to speed-up training of the RNN not to compensate for the inadequacy of low-rank output layer adaptation, which had yet to be invented. Furthermore, Mikolov et al. (2011) do not use context dependent features in the maxent component of the RNNME model nor do they have a method for dealing with hash collusions such as our use of Bloom filters.", "startOffset": 21, "endOffset": 471}, {"referenceID": 5, "context": "The idea of having one part of a language model be low-rank and another part to be an additive correction to the low-rank model has been investigated in other work (Eisenstein et al., 2011; Hutchinson et al., 2013).", "startOffset": 164, "endOffset": 214}, {"referenceID": 11, "context": "The idea of having one part of a language model be low-rank and another part to be an additive correction to the low-rank model has been investigated in other work (Eisenstein et al., 2011; Hutchinson et al., 2013).", "startOffset": 164, "endOffset": 214}, {"referenceID": 19, "context": "The hybrid LSTM and count based language model is an alternative way of correcting for a lowrank approximation (Neubig and Dyer, 2016).", "startOffset": 111, "endOffset": 134}, {"referenceID": 11, "context": "not consider combining multiple context factors but there are some exceptions (Hutchinson et al., 2013; Tang et al., 2016; Hoang et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 24, "context": "not consider combining multiple context factors but there are some exceptions (Hutchinson et al., 2013; Tang et al., 2016; Hoang et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 10, "context": "not consider combining multiple context factors but there are some exceptions (Hutchinson et al., 2013; Tang et al., 2016; Hoang et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 18, "context": "The model from Mikolov and Zweig (2012), equivalent to using just additive", "startOffset": 15, "endOffset": 40}], "year": 2017, "abstractText": "Increased adaptability of RNN language models leads to improved predictions that benefit many applications. However, current methods do not take full advantage of the RNN structure. We show that the most widely-used approach to adaptation (concatenating the context with the word embedding at the input to the recurrent layer) is outperformed by a model that has some low-cost improvements: adaptation of both the hidden and output layers. and a feature hashing bias term to capture context idiosyncrasies. Experiments on language modeling and classification tasks using three different corpora demonstrate the advantages of the proposed techniques.", "creator": "LaTeX with hyperref package"}}}