{"id": "1704.06440", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Equivalence Between Policy Gradients and Soft Q-Learning", "abstract": "Two of the leading approaches for model-free reinforcement learning are policy gradient methods and $Q$-learning methods. $Q$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the $Q$-values they estimate are very inaccurate. A partial explanation may be that $Q$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between $Q$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that \"soft\" (entropy-regularized) $Q$-learning is exactly equivalent to a policy gradient method. We also point out a connection between $Q$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of $Q$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a $Q$-learning method that closely matches the learning dynamics of A3C without using a target network or $\\epsilon$-greedy exploration schedule.", "histories": [["v1", "Fri, 21 Apr 2017 08:33:59 GMT  (878kb,D)", "http://arxiv.org/abs/1704.06440v1", null], ["v2", "Sun, 27 Aug 2017 23:43:20 GMT  (878kb,D)", "http://arxiv.org/abs/1704.06440v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john schulman", "pieter abbeel", "xi chen"], "accepted": false, "id": "1704.06440"}, "pdf": {"name": "1704.06440.pdf", "metadata": {"source": "CRF", "title": "Equivalence Between Policy Gradients and Soft Q-Learning", "authors": ["John Schulman", "Xi Chen", "Pieter Abbeel"], "emails": ["pieter}@openai.com"], "sections": [{"heading": null, "text": "Experimentally, we examine the entropy-regulated versions of Q-Learning and political gradients and find that they perform just as well (or slightly better than) the standard variants of the Atari benchmark. We also show that equivalence endures in practical environments by constructing a Q-Learning method that approaches the learning dynamics of A3C without using a target network or a greedy exploration plan."}, {"heading": "1 Introduction", "text": "In both cases, if the return after an action is at high, then this action is amplified: in the political gradient methods, the probability is increased that the costs of entropy are added to the returns, while the optimal policy has the form of expo (s, a); therefore, political gradient methods for the optimal Q function are solved, up to an additive constant (s, a). The optimal policy has the form of expo (s, a)."}, {"heading": "2 Bandit Setting", "text": "Consider a bandit problem with a discrete or continuous scope for action: At each point, the agent selects an action a, and the reward r is sampled according to P (r), with P (r) unknown to the agent. Letar Xiv: 170 4.06 440v 1 [cs.L G] 21 Apr 201 7r (a) = E [r | a], and let it come to a policy where p (a) the probability of the action is a. Then the expected pertimestep reward of politics \u03c0 [r] [r] [r] [r] is one (a) r [r] one (a) or one (a) r [r] one (a). Suppose that we are a maximizing (p), an entropy-regulated version of this goal: p (g) = Ea)."}, {"heading": "3 Entropy-Regularized Reinforcement Learning", "text": "We will consider an entropy-regulated version of the entropy learning problem, following various previous work (Ziebart [2010], Fox et al. [2015], Haarnoja et al. [2017], Nachum et al. [2017]). Let us define the entropy-increased yield more precisely by defining it as p = 0 \u03b3t (rt \u2212 \u03c4 KLt), where rt is the reward, p = 0, 1] the discount factor, p = a scalar temperature coefficient, and KLt the kullback-leibler divergence between current policy and a reference policy p = p t: KLt = DKL (\u00b7 | st). We will sometimes use the notation KL (s) = DKL (s) = p = p (\u00b7 | s) to mimic the effect of a standard entropy bonus (\u00b7 | st)."}, {"heading": "3.1 Value Functions", "text": "We are obliged to change our definitions of value functions to include the new KL penalty terms. We will define the state value function as expected return: V\u03c0 (s) = E [s] [s, a) = E [r0 + s [t = 0 \u03b3t (rt \u2212 \u03c4 KLt), a = s] (24) and we will define the Q function asQ\u03c0 (s, a) = E [r0 + 3 [t = 1 \u03b3t (rt \u2212 \u03c4 KLt), a = s, a0 = a] (25). Note that this Q function does not include the first KL penalty terms that do not depend on the action a0. This definition simplifies some later expressions and leads to the following relationship between Q\u03c0 and V\u03c0: V\u03c0 (s) = Ea \u0445 \u03c0 (s, a) \u2212 KL (s), (26) resulting from concordances in the sums in the equations (25) and (24)."}, {"heading": "3.2 Boltzmann Policy", "text": "In standard reinforcement learning, the \"greedy politics\" for Q is defined as [GQ] (s) = arg maxaQ (s, a). With entropy regulation, we must change our conception of greedy politics, since the optimal politics is stochastic. Since Q\u03c0 omitted the first entropy term, it is natural to define the following stochastic politics, called Boltzmann politics and analogous to greedy politics: \u03c0BQ (s, a) = arg max \u03c0 (Ea, a). (Q (s, a) \u2212 \u03c4DKL (\u03c0, p). (27) = \u03c0 (a, a) exp (Q (s, a) / Ea (exp). (Q (s, a)."}, {"heading": "3.3 Fixed-Policy Backup Operators", "text": "The T\u03c0 operators (for Q and V) in the standard amplification learning correspond to the calculation of the expected yield with a single step forward: They take the expectation over one step of the dynamics and then revert to the value function at the next time step. We can easily generalize these operators to the entropy-regulated constellation. We define [T\u03c0V] (s) = Ea \u00b2 \u03c0, (r, s \u00b2) \u0445 P (r, s \u00b2 | s, a) [r \u2212 \u03c4 KL (s \u00b2, a \u00b2 KL (s \u00b2)] (32) [T\u03c0Q] (s, a) = E (r, s \u00b2 s \u00b2 s \u00b2 s \u00b2 P (r, s \u00b2 s \u00b2, a) [r + \u03b3 (Ea \u00b2, a \u00b2 KL (s \u00b2, a \u00b2)]] (32) [T\u03c0Q] (s \u00b2 (s \u00b2, a) (r, s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s, a \u00b2 s \u00b2)."}, {"heading": "3.4 Boltzmann Backups", "text": "s guideline, \u03c0 (a | s) Q = Q (a | Q = Q (a | s) exp (Q (s, a) / 4). We define the following Boltzmann safety operator: [T Q] (s, a) = E (r, s \u00b2, s, a) r + 4 (s \u00b2, a) r + 4 (s \u00b2) - 2 (s \u00b2, a) - 2 (s \u00b2) - 2 (s \u00b2, a \u00b2) (s \u00b2) (38) = E (r, s \u00b2) \u00b2 P (r, s \u00b2 s \u00b2 s, a) r + 5 (s \u00b2) logEa \u00b2 (Q \u00b2, a \u00b2 s \u00b2) - 4 (s \u00b2)."}, {"heading": "3.6 Policy Gradients", "text": "The entropie regularization is often used in reference to the empirical yield, with graduation estimates of formEt (st, at, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "6 Experiments", "text": "To complement our theoretical analyses, we developed experiments to investigate the following questions: 1. Although PG methods use single-stage entropy bonuses for neural network strategies (Williams [1992], Mnih et al. [2016]), how do the entropy-regulated RL versions of policy gradients and Q-learning described in Section 3 perform in demanding RL benchmark problems? How does the \"correct\" entropy-regulated policy gradient method (with entropy in yields) compare to the naive method (with single-stage entropy bonus)? (Section 6.1) 2. How do the entropy-regulated versions of Q-Learning (with logsumexp) differ in expectation, but the actual sample size differs slightly from the standard DQN by Mnih et al. [2015] (Section 6.2) 3."}, {"heading": "6.1 A2C on Atari: Naive vs Proper Entropy Bonuses", "text": "Here we examined whether there is an empirical effect of the inclusion of entropy terms in the calculation of returns, as described in Section 3. In this section, we compare the na\u00efve and correct estimates of the policy gradient: naive / 1-step: \"Log\" (at | st) (n \u2212 1 \u2211 d = 0 \u03b3drt + d \u2212 V (st))) + calculating tactic: \"Log\" (at | st) (n \u2212 1 \u0445 d = 0 \u03b3d (rt + d \u2212 \u03c4DKL [\u03c0\u03b8 \u03c0] (st + d))) \u2212 V (st) + calculating tactic:. \"We start with a well-tuned (synchronous, deterministic) version of A3C (Mnih et al)."}, {"heading": "6.2 DQN on Atari: Standard vs Soft", "text": "Here we investigated whether soft Q-learning (which optimizes entropy augmented return) defines the results of two games. Q = Q-Learning results on Atari are different. We investigated whether soft Q-Learning (which simply constitutes the entropy bonus and the KL penalty) is a constant, but this constant made a big difference in the experiments, as a positive constant encourages the reward to longer episodes."}, {"heading": "7 Related Work", "text": "Three recent papers have shown the connection between policy-based methods and value-based methods that are closely linked to the entropy regularization of Q functions. \u2022 O'Donoghue et al. [2016] begin with a similar motivation to the current work: that one possible explanation for Q-Learning and SARSA is that their updates are similar to political gradient updates. They split the Q function into a political part and a value part, inspired by the dueling of Q networks (Wang et al. [2015]: Q (s) = V (s) + (logbook) + \u03c4S [\u03c0)])) (87) This form is chosen so that the term is multiplied so that the expectation is zero below \u03c0 zero, which is a property that satisfies the true advantage function: E\u03c0 [A\u03c0] = 0. Note that our work ignores this S-term because it is most natural not to include the Q-function in order to include the first entropy."}, {"heading": "8 Conclusion", "text": "We examine the connection between two of the leading families of RL algorithms used with deep neural networks. In the framework of an entropy-regulated RL, we show that soft Q-Learning corresponds to a method of policy gradient (with value function adjustment) in terms of expected gradients (first-order view). Furthermore, we also analyze how a subdued Q-Learning method can be interpreted as an implementation of a natural policy gradient (second-order view). Empirically, we show that the entropically regulated formulation considered in our theoretical analysis is in practice based on the Atari-RL benchmark and that equivalence applies in a practically relevant regime."}], "references": [{"title": "Taming the noise in reinforcement learning via soft updates", "author": ["Roy Fox", "Ari Pakman", "Naftali Tishby"], "venue": "arXiv preprint arXiv:1512.08562,", "citeRegEx": "Fox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2015}, {"title": "Reinforcement learning with deep energy-based policies", "author": ["Tuomas Haarnoja", "Haoran Tang", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1702.08165,", "citeRegEx": "Haarnoja et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Haarnoja et al\\.", "year": 2017}, {"title": "A natural policy gradient", "author": ["Sham Kakade"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Kakade.,? \\Q2002\\E", "shortCiteRegEx": "Kakade.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Bridging the gap between value and policy based reinforcement learning", "author": ["Ofir Nachum", "Mohammad Norouzi", "Kelvin Xu", "Dale Schuurmans"], "venue": "arXiv preprint arXiv:1702.08892,", "citeRegEx": "Nachum et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Nachum et al\\.", "year": 2017}, {"title": "Pgq: Combining policy gradient and q-learning", "author": ["Brendan O\u2019Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "venue": "arXiv preprint arXiv:1611.01626,", "citeRegEx": "O.Donoghue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 2016}, {"title": "Should one compute the temporal difference fix point or minimize the bellman residual? the unified oblique projection view", "author": ["Bruno Scherrer"], "venue": "arXiv preprint arXiv:1011.4362,", "citeRegEx": "Scherrer.,? \\Q2010\\E", "shortCiteRegEx": "Scherrer.", "year": 2010}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["Brian D Ziebart"], "venue": null, "citeRegEx": "Ziebart.,? \\Q2010\\E", "shortCiteRegEx": "Ziebart.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Section 4 shows that the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016].", "startOffset": 197, "endOffset": 216}, {"referenceID": 8, "context": "With an entropy cost added to the returns, the optimal policy has the form \u03c0(a | s) \u221d exp(Q(s, a)); hence policy gradient methods solve for the optimal Q-function, up to an additive constant (Ziebart [2010]).", "startOffset": 192, "endOffset": 207}, {"referenceID": 3, "context": "O\u2019Donoghue et al. [2016] also discuss the connection between the fixed points and updates of PG and QL methods, though the discussion of fixed points is restricted to the tabular setting, and the discussion comparing updates is informal and shows an approximate equivalence.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Altogether, the update matches what is typically done in \u201cactor-critic\u201d policy gradient methods such as A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and n-step Q-learning.", "startOffset": 128, "endOffset": 147}, {"referenceID": 3, "context": "Altogether, the update matches what is typically done in \u201cactor-critic\u201d policy gradient methods such as A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and n-step Q-learning. Section 2 uses the bandit setting to provide the reader with a simplified version of our main calculation. (The main calculation applies to the MDP setting.) Section 3 discusses the entropy-regularized formulation of RL, which is not original to this work, but is included for the reader\u2019s convenience. Section 4 shows that the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016]. Section 5 draws a connection between QL methods that use batch updates or replay-buffers, and natural policy gradient methods. Some previous work on entropy regularized reinforcement learning (e.g., O\u2019Donoghue et al. [2016], Nachum et al.", "startOffset": 128, "endOffset": 976}, {"referenceID": 3, "context": "Altogether, the update matches what is typically done in \u201cactor-critic\u201d policy gradient methods such as A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and n-step Q-learning. Section 2 uses the bandit setting to provide the reader with a simplified version of our main calculation. (The main calculation applies to the MDP setting.) Section 3 discusses the entropy-regularized formulation of RL, which is not original to this work, but is included for the reader\u2019s convenience. Section 4 shows that the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016]. Section 5 draws a connection between QL methods that use batch updates or replay-buffers, and natural policy gradient methods. Some previous work on entropy regularized reinforcement learning (e.g., O\u2019Donoghue et al. [2016], Nachum et al. [2017]) uses entropy bonuses, whereas we use a penalty on Kullback-Leibler (KL) divergence, which is a bit more general.", "startOffset": 128, "endOffset": 998}, {"referenceID": 9, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al.", "startOffset": 118, "endOffset": 133}, {"referenceID": 0, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al.", "startOffset": 134, "endOffset": 152}, {"referenceID": 0, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al. [2017], Nachum et al.", "startOffset": 134, "endOffset": 176}, {"referenceID": 0, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al. [2017], Nachum et al. [2017]).", "startOffset": 134, "endOffset": 198}, {"referenceID": 3, "context": "Following Mnih et al. [2015], modern implementations of Q-learning, and nstep Q-learning (see Mnih et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 3, "context": "Following Mnih et al. [2015], modern implementations of Q-learning, and nstep Q-learning (see Mnih et al. [2016]) update the Q-function incrementally to compute the backup against a fixed target Q-function, which we\u2019ll call Q.", "startOffset": 10, "endOffset": 113}, {"referenceID": 9, "context": "(Williams [1992], Mnih et al.", "startOffset": 1, "endOffset": 17}, {"referenceID": 3, "context": "(Williams [1992], Mnih et al. [2016]).", "startOffset": 18, "endOffset": 37}, {"referenceID": 8, "context": "This result is obtained directly by considering the stochastic computation graph for the loss (Schulman et al. [2015a]), shown in the figure on the right.", "startOffset": 95, "endOffset": 119}, {"referenceID": 3, "context": "These approximations can take the form of n-step methods (Mnih et al. [2016]) or TD(\u03bb)-like methods (Schulman et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 3, "context": "These approximations can take the form of n-step methods (Mnih et al. [2016]) or TD(\u03bb)-like methods (Schulman et al. [2015b]), though we will focus on n-step returns here.", "startOffset": 58, "endOffset": 125}, {"referenceID": 3, "context": "Effectively, the value function error has a coefficient of \u03c4\u22121, which is larger than what is typically used in practice (Mnih et al. [2016]).", "startOffset": 121, "endOffset": 140}, {"referenceID": 2, "context": "As pointed out by Kakade [2002], the natural gradient step can be computed as the solution to a least squares problem.", "startOffset": 18, "endOffset": 32}, {"referenceID": 9, "context": "Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992], Mnih et al.", "startOffset": 84, "endOffset": 100}, {"referenceID": 3, "context": "Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992], Mnih et al. [2016]), how do the entropy-regularized RL versions of policy gradients and Q-learning described in Section 3 perform on challenging RL benchmark problems? How does the \u201cproper\u201d entropy-regularized policy gradient method (with entropy in the returns) compare to the naive one (with one-step entropy bonus)? (Section 6.", "startOffset": 101, "endOffset": 120}, {"referenceID": 3, "context": "Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992], Mnih et al. [2016]), how do the entropy-regularized RL versions of policy gradients and Q-learning described in Section 3 perform on challenging RL benchmark problems? How does the \u201cproper\u201d entropy-regularized policy gradient method (with entropy in the returns) compare to the naive one (with one-step entropy bonus)? (Section 6.1) 2. How do the entropy-regularized versions of Q-learning (with logsumexp) compare to the standard DQN of Mnih et al. [2015]? (Section 6.", "startOffset": 101, "endOffset": 558}, {"referenceID": 3, "context": "We start with a well-tuned (synchronous, deterministic) version of A3C (Mnih et al. [2016]), henceforth called A2C (advantage actor critic), to optimize the entropy-regularized return.", "startOffset": 72, "endOffset": 91}, {"referenceID": 10, "context": "The Q-function was represented as: Q\u03b8(s, a) = V\u03b8(s) + \u03c4 log \u03c0\u03b8(a | s), which can be seen as a form of dueling architecture with \u03c4 log \u03c0\u03b8(a | s) being the \u201cadvantage stream\u201d (Wang et al. [2015]).", "startOffset": 174, "endOffset": 193}, {"referenceID": 5, "context": "\u2022 O\u2019Donoghue et al. [2016] begin with a similar motivation as the current paper: that a possible explanation for Q-learning and SARSA is that their updates are similar to policy gradient updates.", "startOffset": 2, "endOffset": 27}, {"referenceID": 5, "context": "\u2022 O\u2019Donoghue et al. [2016] begin with a similar motivation as the current paper: that a possible explanation for Q-learning and SARSA is that their updates are similar to policy gradient updates. They decompose the Q-function into a policy part and a value part, inspired by dueling Q-networks (Wang et al. [2015]): Q(s, a) = V (s) + \u03c4(log \u03c0(a | s) + \u03c4S[\u03c0(\u00b7 | s)]) (87) This form is chosen so that the term multiplying \u03c4 has expectation zero under \u03c0, which is a property that the true advantage function satisfies: E\u03c0 [A\u03c0] = 0.", "startOffset": 2, "endOffset": 314}, {"referenceID": 5, "context": "\u2022 Nachum et al. [2017] also discuss the entropy-regularized reinforcement learning setting, and develop an off-policy method that applies in this setting.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": "The resulting algorithm is a kind of Bellman residual minimization\u2014it optimizes with respect to the future target values, rather than treating them as fixed Scherrer [2010]. \u2022 Haarnoja et al.", "startOffset": 157, "endOffset": 173}, {"referenceID": 1, "context": "\u2022 Haarnoja et al. [2017] work in the same setting of soft Q-learning as the current paper, and they are concerned with tasks with high-dimensional action spaces, where we would like to learn stochastic policies that are multi-modal, and we would like to use Q-functions for which there is no closed-form way of sampling from the Boltzmann distribution \u03c0(a | s) \u221d \u03c0(a | s) exp(Q(s, a)/\u03c4).", "startOffset": 2, "endOffset": 25}], "year": 2017, "abstractText": "Two of the leading approaches for model-free reinforcement learning are policy gradient methods and Q-learning methods. Q-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the Q-values they estimate are very inaccurate. A partial explanation may be that Q-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between Q-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that \u201csoft\u201d (entropy-regularized) Q-learning is exactly equivalent to a policy gradient method. We also point out a connection between Q-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of Q-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a Q-learning method that closely matches the learning dynamics of A3C without using a target network or -greedy exploration schedule.", "creator": "LaTeX with hyperref package"}}}