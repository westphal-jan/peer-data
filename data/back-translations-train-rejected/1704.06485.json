{"id": "1704.06485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Attend to You: Personalized Image Captioning with Context Sequence Memory Networks", "abstract": "We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.", "histories": [["v1", "Fri, 21 Apr 2017 11:29:07 GMT  (3876kb,D)", "http://arxiv.org/abs/1704.06485v1", "Accepted paper at CVPR 2017"], ["v2", "Tue, 25 Apr 2017 23:30:43 GMT  (3879kb,D)", "http://arxiv.org/abs/1704.06485v2", "Accepted paper at CVPR 2017"]], "COMMENTS": "Accepted paper at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["cesc chunseong park", "byeongchang kim", "gunhee kim"], "accepted": false, "id": "1704.06485"}, "pdf": {"name": "1704.06485.pdf", "metadata": {"source": "META", "title": "Attend to you: Personalized Image Captioning with Context Sequence Memory Networks", "authors": ["Cesc Chunseong Park", "Byeongchang Kim", "Gunhee Kim"], "emails": ["cspark@lunit.io", "byeongchang.kim@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2. Related work", "text": "Caption: Much work on captioning has been published in recent years, including [3, 4, 9, 12, 20, 22, 28, 31, 33] to name a few. Many proposed captioning models use RNN-based decoders to generate a sequence of words from encoded input images. They et al. [33] use semantic attention to combine top-down and bottom-down recurring revolutionary networks [3] to extract richer information from images, and pair it with an LSTM decoder. Compared to such recent advances in captioning research, replacing an RNN-based decoder with semantic attention is novel."}, {"heading": "3. Dataset", "text": "We present our newly collected Instagram dataset, the central statistics of which are shown in Table 1. We create separate datasets for completion after completion and hashtag prediction."}, {"heading": "3.1. Collection of Instagram Posts", "text": "We collect picture posts from Instagram, one of the fastest-growing photo-sharing social networks. As a postcrawler, we use the built-in hashtag search function of the Instagram APIs. We select 270 search terms that consist of the 10 most common hashtags for each of the 27 general categories of Pinterest (e.g. design, food, style). We use the Pinterest categories because they are well-defined topics to get picture posts from different users. We then collect a total of 3,455,021 raw posts from 17,813 users. Next, we edit a number of filters. First, we apply language filtering to include only English posts; we exclude posts in which more than 20% of the words are not based on English. We then dictate the posts that embed hyperlinks in the body text because they are likely to be advertisements. Finally, if users maximize more than (15, 0.15 x # user posts) that are not English or advertisements, we remove all text that embed their posts because they are likely to include a minimum number of hyperlinks."}, {"heading": "3.2. Preprocessing", "text": "We create a dictionary V separately for each of the two tasks by selecting the most common V-words in our dataset. For example, the dictionary for hashtag prediction contains only the most common hashtags as vocabulary. We set V to 40K for completion and 60K for hash prediction after thorough testing. Before building the dictionary, we first remove all URLs, Unicodes except emojis and special characters. We then downsize words and change usernames to @ username token."}, {"heading": "4. The Context Sequence Memory Network", "text": "Figure 2 illustrates the proposed Context Sequence Memory Network (CSMN) model. The input is a query image Iq of a specific user, and the output is a sequence of words: {yt} = y1,.., yT, each of which is a symbol taken from the V dictionary. That is, {yt} corresponds to a list of hashtags in the hashtag prediction and a postsentence in the post generation. Optionally, the context information to be added to the memory is entered, e.g. active vocabulary of a given user. Since both tasks can be formulated as word sequence predictions for a particular image, we use the same CSMN model and only change the dictionary. Specifically, we also consider hashtag prediction as a sequence prediction rather than a prediction of a bag of orderless keywords. Since hashtags in a post tend to have strong coincident relationships, it is better to anticipate the next one."}, {"heading": "4.1. Construction of Context Memory", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "4.2. State-Based Sequence Generation", "text": "It's about the candidates. (...) It's about the candidates. (...) It's about the candidates. (...) It's about the candidates. (...) It's about the candidates. (...) \"It's about the candidates.\" (...) \"It's about the candidates.\" (...) \"It's about the candidates.\" (...) \"It's about the candidates.\" (...) \"It's about the candidates.\" \"\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...)\" It's about the candidates. \"(...\" It's about the candidates. \"(...)\" It's about the candidates. \"(...\" It's about the candidates. \"(...)\" It's about the candidates. \"(...) It's about the candidates.\" (... \"It's about the candidates.\" (...) It's about the candidates. \"(...\" It's about the candidates. \"It's about the candidates.\" (...) It's about the candidates. (...) It's about the candidates. (...) It's about the candidates. (It's about the candidates. \""}, {"heading": "4.3. Training", "text": "We use the softmax entropy loss as a cost function for each time step forecast, minimizing the negative log probability from the estimated yt to the corresponding target word yGT, t. We choose the Adam Optimizer [11] with \u03b22 = 0.9, \u03b22 = 0.999 and \u0441 = 1e \u2212 08. To speed up the training, we use four GPUs for data parallelism and set a batch size of 200 for each GPU. We achieve the best results with the initial learning rate of 0.001 for all models. At all 5 epochs, we divide the learning rate by 1.2 to gradually decrease it."}, {"heading": "5. Experiments", "text": "We compare the performance of our approach with other state-of-the-art models using quantitative metrics and Amazon Mechanical Turk (AMT) studies."}, {"heading": "5.1. Experimental Setting", "text": "We use the image of a test post as a query and the associated hashtags and text descriptions as Groundtruth (GT). For hashtag prediction evaluation metrics, we calculate the F1 score as a balanced average metric between precision and retrieval between predicted hashtag sentences and GT18650018 _ @ _ 56299423158975190sets: 2 (1 / precision + 1 / retrieval) \u2212 1. For post-generation evaluation metrics, we calculate the linguistic similarity between predicted sentences and GTs. We use BLEU [21], CIDEr [26], METEOR [14] and ROUGE-r [15] values. In all measurements, higher values indicate better performance. We randomly divide the dataset into 90% for training, 5K contributions for the test, and the rest for validation. We divide the dataset by the user, so that both the COK test and user strength can be separated from each other (e.g., if the GTS can be separated into the GTS)."}, {"heading": "5.2. Baselines", "text": "As baselines, we select multiple next-neighbor approaches, a language generation algorithm, two state-of-the-art caption methods, and several variants of our model. As simple baselines, we first test the 1-next search by images designated by (1NN-Im); for a query image, we find its nearest image using the 1-2 distance on ResNet Pool5 descriptors, and then return its text as a prediction. Second, we test the 1-next search by users designated by (1NN-UsrIm); we find the next user whose 60 active vocabularies are most overlapped with those of the query user, and then randomly select a user contribution. The third next neighbor variant, which we name by (1NN-UsrIm), is to find the 1-next image among the images of the next user, and return its text as a prediction."}, {"heading": "5.3. Quantitative Results", "text": "This is mainly due to the fact that the predictions of the post-generation and the hashtag are getting bigger. Since algorithms show similar patterns in both tasks, we analyze the experimental results to each other below. Firstly, according to most metrics in both tasks, our approach (CSMN- *) clearly exceeds the basics. We can divide the algorithms into two groups with or without personalization; the latter includes (ShowTell), while the (1NN-Im), and (CSMN-NoUC-P5), while the former include the other methods. Ours (CSMN-NoUC-P5) ranks the first among the methods without personalization, while the (CSMN-P5) achieves the best total."}, {"heading": "5.4. User Studies via Amazon Mechanical Turk", "text": "We perform AMT tests to observe the preferences of general users between different algorithms for the two post-automation tasks. For each task, we perform 100 sample tests. In the test, we show a query image and three randomly selected complete contributions from the user as a personalization clue, as well as two text descriptions generated by our method and a baseline in a random order. We ask the Turks to choose a more relevant one from the two. We receive responses from three different Turks for each query. We choose the variant (CSMN-P5) as representative of our method for its best quantitative performance. We compare the results with three baseline tests, selecting the best method in each group of 1NNNNs, captions and pure language methods: (1NN-UsrIm), (ShowTell) and (seq2seq).Table 4 summarizes the results of the AMT tests, confirming that human commentators clearly match our baseline results with those of the 2qq (most base lines)."}, {"heading": "5.5. Qualitative Results", "text": "Figure 3 illustrates selected examples of post-generation. In each sentence, we show a query image, GT, and generated text descriptions using our method and baselines. In many Instagram examples, GT comments are difficult to accurately predict because they are extremely diverse, subjective, and private, and talk about a variety of topics. However, most of the predicted text descriptions are relevant to the query images. In addition, our CSMN model is able to appropriately use normal words, emojis, and even mentions to other users (anonymized by @ username). Figure 4 shows examples of hashtag predictions. We observe that our hashtag prediction is stable even on a variety of topics, including profiles, food, fashion, and interior design. Figure 5 shows examples of how much hashtag and post predictions vary depending on different users for the same query images. Although the predicted results in terms of the text used vary, they are most relevant to the initial query images and the correspondingly illustrated for each of the six queries."}, {"heading": "6. Conclusions", "text": "We proposed the Context Sequence Memory Network (CSMN) as the first personalized captioning approach. We covered two post-automation tasks: hashtag prediction and post-generation. Using quantitative evaluation and AMT user studies on nearly collected Instagram data sets, we showed that our CSMN approach outperformed other state-of-the-art captioning models. There are several promising future directions that go beyond this work. First, we can add another interesting task to the CSMN model, such as commenting on posts that generate a series of responses for a particular post. Second, since we have only been working on Instagram posts in this work, we can examine data on other social networks such as Flickr, Pinterest or Tumblr that have different post types, metadata, and text and hashtag uses. This research is partially supported by Hancom and the National Research Foundation Korea (RC120562), which is the corresponding author."}, {"heading": "A. Why using memory CNN can be helpful?", "text": "While traditional storage networks cannot model the structural order if no temporal embedding is added to the model, we suggest using the memory CNN to model the structural order with much stronger representational power. Specifically, CNN is useful for the word output memory to represent the sequential order of the words generated; for the user's context memory, CNN can correctly capture the order of the context words, since we feed the frequent words of the user into the context memory in decreasing order of the TF-IDF weighted values (rather than putting them in a random order).Suppose that a user can have active words related to fashion, street and landscape in the context memory of the user. If the fashion-related words are at the top of the user context memory and the street-modeled mode, the user may be interested in modelling that way."}, {"heading": "B. Details of Dataset Collection", "text": "27 generic categories of Pinterest. For the collection of records, we select 270 search terms by collecting the 10 most common hashtags for each of the following 27 generic categories of Pinterest: celebrities, design, education, food, drinking, gardening, hair, health, fitness, history, humor, decor, outdoor, illustration, quotes, product, sport, technology, travel, wedding, tour, car, football, animal, pet, fashion and World Cup.Database statistics. We show the cumulative distribution functions (CDFs) of the words for captions and hashtags in Figure 7. Based on the statistics, we set the vocabulary size for captions as 40K and for hashtags as 60K. For captions, the most commonly used words account for 97.3% of all word occurrences, providing sufficient coverage for the vocabulary and hashtags in Figure 7. Based on the statistics, we show the font size for captions as 40K and 60K for hashtags."}, {"heading": "C. Model Details", "text": "In the main draft, we describe our model on the assumption that we use the res5c features Ir5c-R2.048-7-7 for image representation. Here, we discuss some formulation changes when we use the pool5 features Ip5-R2.048. For memory CNNs, we present the memory output for res5c features as follows (i.e. Equation (10) of the main draft).chim, t = maxpool (ReLU (w h im, 1: 49 + bhim))) (13) When we use the pool5 features, Equation (13) is changed, t = w h imm o im, 1 + b h im. (14), where bhim-R1.800, wimh-R1.024-1.800 and chim, t-R1.800. After experiments, we found that the addition of features ReLu to Eq. (14) slightly improves performance. Finally, the memory bed is shown for Rcc = 5cc-5c1 features."}, {"heading": "D. Training Details", "text": "Although our model can use variable-length sequences as input to speed up training, it is better if a minibatch consists of sets of the same length. Therefore, we randomly group training samples into a series of minibatches, each of which has the same length as possible. Afterwards, we randomly mix a batch order to mix short and long minibatches. It empirically provides a better education than a curriculum suggests [35]."}], "references": [{"title": "Learning to Transfer: Transferring Latent Task Structures and Its Application to Person-specific Facial Action Unit Detection", "author": ["T. Almaev", "B. Martinez", "M. Valstar"], "venue": "ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "User Conditional Hashtag Prediction for Images", "author": ["E. Denton", "J. Weston", "M. Paluri", "L. Bourdev", "R. Fergus"], "venue": "KDD,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "From Captions to Visual Concepts and Back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Turing Machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv:1410.5401,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid Computing Using a Neural Network with Dynamic", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u0144ska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou"], "venue": "External Memory. Nature,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Unconstrained Realtime Facial Performance Capture", "author": ["P.-L. Hsieh", "C. Ma", "J. Yu", "H. Li"], "venue": "CVPR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Personalized Handwriting Recognition via Biased Regularization", "author": ["W. Kienzle", "K. Chellapilla"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "ADAM: AMethod For Stochastic Optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "ICLR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal Neural Language Models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author": ["S.B.A. Lavie"], "venue": "ACL,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["C.-Y. Lin"], "venue": "WAS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Microsoft COCO: Common Objects in Context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Key-Value Memory Networks for Directly Reading Documents", "author": ["A. Miller", "A. Fisch", "J. Dodge", "A.-H. Karimi", "A. Bordes", "J. Weston"], "venue": "EMNLP,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Motivating Personality-aware Machine Translation", "author": ["S. Mirkin", "S. Nowson", "C. Brun", "J. Perez"], "venue": "EMNLP,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Im2Text: Describing Images Using 1 Million Captioned Photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Expressing an Image Stream with a Sequence of Natural Sentences", "author": ["C.C. Park", "G. Kim"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Personalized Mathematical Word Problem Generation", "author": ["O. Polozov", "E. ORourke", "A.M. Smith", "L. Zettlemoyer", "S. Gulwani", "Z. Popovic"], "venue": "IJCAI,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to- End Memory Networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "CIDEr: Consensus-based Image Description Evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Grammar as a Foreign Language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE TPMAI, 39(4):652\u2013663,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Memory Networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "ICLR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks", "author": ["R.J. Williams", "D. Zipser"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Gesture Recognition Portfolios for Personalization", "author": ["A. Yao", "L. Van Gool", "P. Kohli"], "venue": "CVPR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Image Captioning with Semantic Attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to Execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv:1410.4615,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 3, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 8, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 11, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 19, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 21, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 27, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 30, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 32, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 4, "context": "Our model is inspired by memory networks [5, 24, 29], which explicitly include memory components to which neural networks read and write data for capturing long-term information.", "startOffset": 41, "endOffset": 52}, {"referenceID": 23, "context": "Our model is inspired by memory networks [5, 24, 29], which explicitly include memory components to which neural networks read and write data for capturing long-term information.", "startOffset": 41, "endOffset": 52}, {"referenceID": 28, "context": "Our model is inspired by memory networks [5, 24, 29], which explicitly include memory components to which neural networks read and write data for capturing long-term information.", "startOffset": 41, "endOffset": 52}, {"referenceID": 2, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 24, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 30, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 32, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "By using teacherforced learning [30], our model has a markov property at training time; predicting a previous word yt\u22121 has no effect on predicting a next word yt, which depends on only the current memory state.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "Original memory networks [24, 29] leverage time embedding to model the memory order.", "startOffset": 25, "endOffset": 33}, {"referenceID": 28, "context": "Original memory networks [24, 29] leverage time embedding to model the memory order.", "startOffset": 25, "endOffset": 33}, {"referenceID": 26, "context": "[27, 28, 31]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "[27, 28, 31]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 30, "context": "[27, 28, 31]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 20, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "(4) With quantitative evaluation and user studies via AMT, we demonstrate the effectiveness of three novel features of CSMN and its performance superiority over stateof-the-art captioning models, including [27, 28, 31].", "startOffset": 206, "endOffset": 218}, {"referenceID": 27, "context": "(4) With quantitative evaluation and user studies via AMT, we demonstrate the effectiveness of three novel features of CSMN and its performance superiority over stateof-the-art captioning models, including [27, 28, 31].", "startOffset": 206, "endOffset": 218}, {"referenceID": 30, "context": "(4) With quantitative evaluation and user studies via AMT, we demonstrate the effectiveness of three novel features of CSMN and its performance superiority over stateof-the-art captioning models, including [27, 28, 31].", "startOffset": 206, "endOffset": 218}, {"referenceID": 2, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 3, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 8, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 11, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 19, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 21, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 27, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 30, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 32, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 2, "context": "For example, long-term recurrent convolutional networks [3] are one of earliest model to use RNNs for modeling the relations between sequential inputs and outputs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 32, "context": "[33] exploit semantic attention to combine top-down and bottom-up strategies to extract richer information from images, and couples it with an LSTM decoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 7, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 0, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 31, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 9, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 22, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 1, "context": "[2] develop a CNN model that predicts hashtags from image content and user information.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[32] propose a domain adaptation approach to classify user-specific human gestures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] adopt a transfer learning framework to detect person-specific facial action unit detection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] enhance machine translation performance by exploiting personal traits.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] generate personalized mathematical word problem for a given tutor/student specification by logic programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Neural Turing Machines [5] use external memory to solve algorithmic problems such as sorting and copying.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Later, this architecture is extended to Differential Neural Computer (DNC) [6] to solve more complicated algorithmic problems such as finding shortest path and graph traversal.", "startOffset": 75, "endOffset": 78}, {"referenceID": 28, "context": "[29] propose one of the earliest memory network models for natural language question answering (QA), and later Sukhbaatar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] modify the network to be trainable in an end-to-end manner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] and Milleret al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] address language QA tasks proposing novel memory networks such as dynamic networks with episodic memory in [13] and key-value memory networks in [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[17] address language QA tasks proposing novel memory networks such as dynamic networks with episodic memory in [13] and key-value memory networks in [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "[17] address language QA tasks proposing novel memory networks such as dynamic networks with episodic memory in [13] and key-value memory networks in [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 28, "context": "Following [29], each input to the memory is embedded into input and output memory representation, for which we use superscript a and c, respectively.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "We represent images using ResNet101 [7] pretrained on the ImageNet 2012 dataset.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "The ReLU indicates an element-wise ReLU activation [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 30, "context": "to which part of memory the attention turns at time t [31]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "We define a set of three filters whose depth is 300 by changing window sizes h = [3, 4, 5].", "startOffset": 81, "endOffset": 90}, {"referenceID": 3, "context": "We define a set of three filters whose depth is 300 by changing window sizes h = [3, 4, 5].", "startOffset": 81, "endOffset": 90}, {"referenceID": 4, "context": "We define a set of three filters whose depth is 300 by changing window sizes h = [3, 4, 5].", "startOffset": 81, "endOffset": 90}, {"referenceID": 2, "context": "For h = [3, 4, 5],", "startOffset": 8, "endOffset": 17}, {"referenceID": 3, "context": "For h = [3, 4, 5],", "startOffset": 8, "endOffset": 17}, {"referenceID": 4, "context": "For h = [3, 4, 5],", "startOffset": 8, "endOffset": 17}, {"referenceID": 2, "context": "Parameters include biases bim \u2208 R49\u00d7300 and filters w im \u2208 R[3,4,5]\u00d71,024\u00d7300.", "startOffset": 60, "endOffset": 67}, {"referenceID": 3, "context": "Parameters include biases bim \u2208 R49\u00d7300 and filters w im \u2208 R[3,4,5]\u00d71,024\u00d7300.", "startOffset": 60, "endOffset": 67}, {"referenceID": 4, "context": "Parameters include biases bim \u2208 R49\u00d7300 and filters w im \u2208 R[3,4,5]\u00d71,024\u00d7300.", "startOffset": 60, "endOffset": 67}, {"referenceID": 0, "context": "Via max-pooling, each cim,t is reduced from (300\u00d7 [47, 46, 45]) to (300\u00d7 [1, 1, 1]).", "startOffset": 73, "endOffset": 82}, {"referenceID": 0, "context": "Via max-pooling, each cim,t is reduced from (300\u00d7 [47, 46, 45]) to (300\u00d7 [1, 1, 1]).", "startOffset": 73, "endOffset": 82}, {"referenceID": 0, "context": "Via max-pooling, each cim,t is reduced from (300\u00d7 [47, 46, 45]) to (300\u00d7 [1, 1, 1]).", "startOffset": 73, "endOffset": 82}, {"referenceID": 10, "context": "We select the Adam optimizer [11] with \u03b22 = 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 59, "endOffset": 63}, {"referenceID": 33, "context": "Flickr30K [34] and MS COCO [16]) have multiple GTs (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "Flickr30K [34] and MS COCO [16]) have multiple GTs (e.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "[27], denoted by (seq2seq).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The first baseline is (ShowTell) of [28], which is a multi-modal CNN and LSTM model.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "The second baseline is the attention-based captioning model of [31] denoted by (AttendTell).", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "(seq2seq) [27] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "065 (ShowTell)\u2217 [28] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "081 (AttendTell)\u2217 [31] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "(seq2seq) [27] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "085 (ShowTell)\u2217 [28] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "011 (AttendTell)\u2217 [31] 0.", "startOffset": 18, "endOffset": 22}], "year": 2017, "abstractText": "We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user\u2019s active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.", "creator": "LaTeX with hyperref package"}}}