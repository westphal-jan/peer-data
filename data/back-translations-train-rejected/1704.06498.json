{"id": "1704.06498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces", "abstract": "Graph models are relevant in many fields, such as distributed computing, intelligent tutoring systems or social network analysis. In many cases, such models need to take changes in the graph structure into account, i.e. a varying number of nodes or edges. Predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics, e.g. with respect to user behaviour. However, predictive techniques in the past have almost exclusively focused on single edges or nodes. In this contribution, we attempt to predict the future state of a graph as a whole. We propose to phrase time series prediction as a regression problem and apply dissimilarity- or kernel-based regression techniques, such as 1-nearest neighbor, kernel regression and Gaussian process regression, which can be applied to graphs via graph kernels. The output of the regression is a point embedded in a pseudo-Euclidean space, which can be analyzed using subsequent dissimilarity- or kernel-based processing methods. We discuss strategies to speed up Gaussian Processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems. We find that simple regression methods, such as kernel regression, are sufficient to capture the dynamics in the theoretical models, but that Gaussian process regression significantly improves the prediction error for real-world data.", "histories": [["v1", "Fri, 21 Apr 2017 12:08:30 GMT  (130kb,D)", "http://arxiv.org/abs/1704.06498v1", "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')"], ["v2", "Tue, 6 Jun 2017 10:21:36 GMT  (136kb,D)", "http://arxiv.org/abs/1704.06498v2", "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')"], ["v3", "Fri, 11 Aug 2017 11:47:42 GMT  (138kb,D)", "http://arxiv.org/abs/1704.06498v3", "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')"]], "COMMENTS": "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["benjamin paa{\\ss}en", "christina g\\\"opfert", "barbara hammer"], "accepted": false, "id": "1704.06498"}, "pdf": {"name": "1704.06498.pdf", "metadata": {"source": "CRF", "title": "Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces\u2217\u2020", "authors": ["Benjamin Paa\u00dfen", "Christina G\u00f6pfert", "Barbara Hammer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to seek a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution."}, {"heading": "2 Background and Related Work", "text": "Dynamically changing graphs are relevant in many different areas, such as transport [42], distributed computing [11], social networks [34], or intelligent tutoring systems [31, 37]. Due to the breadth of the field, we will focus here on relatively general concepts that can be applied to a variety of areas. We will begin with two formalisms for modelling graph dynamics, namely temporally varying graphs [11] and sequential dynamic systems [6]. Next, we will address our research question: predicting graph dynamics, which has been dealt with mainly in the area of social networks under the umbrella of link prediction [34, 55] and in graph growth models [23]. Finally, in preparation for our own approach, we will discuss graph nuclei and differences, as well as previous work on nuclear-based approaches to predicting time series."}, {"heading": "2.1 Models of Graph Dynamics", "text": "The authors point out that in all of these areas, the graph topology changes over time, and the influence of such changes is too strong to be represented only in terms of system anomalies. Rather, dynamics must be considered an \"integral part of the nature of the system.\" [11] We assume a slightly varied version of the notation developed in their work to provide an overview of the types of problems associated with dynamics in graphs. [11] A time-varying graph is defined as a five-tuple graph G = (V, E, T, E), where \u2022 V is called an arbitrary set."}, {"heading": "2.2 Predicting Changes in Graphs", "text": "In accordance with the classical prediction of time series, predicting time series in graphs can be described as predicting the next temporal subgraph Gt + 1, which is associated with a sequence of temporal subgraphs G0,.., Gt in a temporally varying graph. However, to our knowledge there is no approach that addresses this problem in this general form. However, more specific subproblems have been addressed in the literature.Link Prediction: In the field of social network analysis [34] we have formulated the linking problem of linkage, which is: Facing a sequence of temporal subgraphs G0,. Gt for a time-varying graph G, whose edges are added to the graph in the next time step, i.e. for which edges we find a block (e, t) = 0, but a sequence of temporal subgraphs G0, [e, t + 1) = 1 [55], as a deviation between the two, as we can predict in the light of a recent scientific example."}, {"heading": "2.3 Graph Dissimilarities and Kernels", "text": "Instead of directly predicting the next graph in a sequence, one can consider indirect approaches that base a prediction on paired differences d (G, G \u2032) or kernels k (G, G \u2032). As a simple example, we consider an approach with 1-nearest neighbors: However, we first aggregate a database of training data consisting of graphs Gt + 1. When confronted with a new graph G \u2032, we simply predict the graph Gt + 1 in such a way that d (Gt, G \u2032) is minimized or k (Gt, G \u2032) a database of training data consisting of graphs Gt + 1. But how can such a difference or kernel be formulated on graphs? In the literature, we observe two main streams of research: Graph Edit Distance: The graph edit Distance between two graphs G and G \u2032 is traditionally defined as the minimum number of processing operations required to transform G into G."}, {"heading": "2.4 Kernel-Based approaches for Vectorial Time Series Prediction", "text": "The idea of using nuclear-based methods for predicting time series as such is not new. A popular example is the use of supportive vector regression in wide-ranging applications in financial, economic, environmental, and engineering sciences [49]. Another example is Gaussian processes for predicting chemical processes [21], motion data [53], and physical data [45]. To our knowledge, however, all applications of predicting time series have so far focused on vector data. Our work, on the other hand, deals with structured data such as sequences, trees, and graphs."}, {"heading": "3 Time Series Prediction for Graphs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Time Series Prediction as a Regression Problem", "text": "Based on the notation of temporally varying graphs as presented above, we can describe a time series of graphs as a sequence of temporal subgraphs G0,..., Gt.Time series prediction. We first turn this problem into a regression problem, as suggested by Sapankecych and colleagues, i.e. we try to learn a function f that maps the past K states of a time series to a subsequent state [49]. For simplicity's sake, we assume that K = 1 is what the Markov assumption is. Note that this assumption can be easily dropped by considering deviations or similarities of graphs instead of simple graphs. By this simplification, we can express our training data as tuples {(xi, yi)} without these temporal regularities to i {1,., N}, where yi is the successor of xi in some time series."}, {"heading": "3.2 Non-Parametric Regression techniques", "text": "In this section, we present three non-parametric regression techniques in their standard form, assuming vector input and output data. Below, we explain how these methods can be applied to structured input and output data. We assume that a training data set of tuples {(~ xi, ~ yi)} is available, where ~ yi is the desired output for input and output data (xi.1-NN): We define the prediction function for the most insensitive neighboring regression (1-NN) as follows: f (~ x): = Regi + wo i + = argmin i = argmin i i."}, {"heading": "3.3 Dissimilarities, Similarities and Kernels", "text": "Each of the methods described above is based either on a dissimilarity (in the case of 1-NN) or on a core (in the case of KR and GPR). Note that we have not yet strictly defined these concepts. In fact, dissimilarity and similarities are inherently poorly defined concepts. A dissimilarity for the set X is any function of the form d: X-X-R that increases when the two input arguments are more closely related in any sense [43]. Conversely, a similarity for the set X is a function: X-X-R that increases when the two input arguments are related to each other."}, {"heading": "3.4 Prediction in Kernel and Dissimilarity Spaces", "text": "When introducing the three regression techniques above us, we assumed that each data point is a vector, so that algebraic operations such as scalar multiplication (for kernel regression) are allowed (except matrix vector multiplication and vector addition (for Gauss process regression). In the case of graphs, such operations are not well defined. However, we can rely on previous work on vector embedding of differences and similarities to define our regression in a latent space. In particular: Theorem 2 (pseudo-euclidean embeddings [43]). For differences: Let x1,. xN be some points from a set X, and let d be a symmetric and reflexive dissimilarity to X. Then there is a pseudo-euclidean space Y and a function."}, {"heading": "3.5 Linear Time Predictions", "text": "GP regression involves the inversion of the matrix (K + \u03c3 = 2 \u00b7 IN), resulting in O (N3) complexity. A variety of efficient approximation schemes exist [44]. Recently, the robust Bayesian Committee Machine (rBCM) was introduced as a particularly fast and precise approximation. rBCM's approach consists of distributing the examples into C disjoint sets, based e.g. on clusters in the input data space. For each of these sets, a separate GP regression is used, which distributes the predictive distributions N (~ \u00b5c, \u03c32c) for c disjoint sets. \u2212 These distributions are separated to the final predictive distribution N (~ \u00b5rBCM, \u03c32rBCM) \u2212 2rBCM = C-2 \u00b0 C-2 \u00b0 C = \u03c32c \u00b2."}, {"heading": "4 Experiments", "text": "In our experimental evaluation, we apply the pipeline presented in the previous section to four sets of data, two of which are theoretical models and two of which are real Java program datasets. In all cases, we evaluate the mean square error (RMSE) of the prediction for each method in a leave-oneout cross-validation of the sequences in our dataset. We designate the current test history as x-1,..., x-T, the training paths as {xj1,.,.,.) and the matrix of square imbalances (including the test data) as the predicted affinity coefficients for the point x-t \"independent tests as ~ \u03b1t\" = (\u03b1 1 t, \"1,.,.,.) and the matrix of the quadrix of quadritic inequalities (including the test data) as D2. Accordingly, the RMSE has the following form for each fold (see Equation 16)."}, {"heading": "4.1 Theoretical Data Sets", "text": "We examine the following theoretical datasets: Barab\u00e1si-Albert Model: This is a simple stochastic model of graph growth in undirected graphs [4]. We examine the following theoretical datasets: Barab\u00e1si-Albert Model: This is a simple stochastic model of graph growth in undirected graphs [4]. Starting from a fully connected initial graph of m0 nodes, nodes are added one by one. In each iteration, the newly added node is connected to k of the existing nodes randomly selected with probability P (u, v). It has been shown that the edge distribution resulting from this growth model is scale-free, more specifically the probability of a particular degree node degree is P (k) = m \u00b7 k \u2212 3, where m is the number of nodes [4]. Our dataset consists of 20 graphs with m = 27 nodes."}, {"heading": "4.2 Java Programs", "text": "The motivation for predicting such data is to help students find a correct solution to their program, since they have already solved the problem."}, {"heading": "5 Discussion and Conclusion", "text": "Our results indicate that it is possible to make predictions about time series in nuclear and deviation spaces, especially for graphs. In all our experiments, even simple prediction models (1-nearest neighbor and kernel regression) have exceeded the baseline of where you are. In real data, we could further improve the prediction error by applying a more complex prediction model, the rugged Bayesian Committee Machine (rBCM). This suggests a trade-off in model selection: simpler models are faster, and in the case of 1-nearest neighbors, the result is easier to infer. However, in the case of real data, it is likely that a more complex prediction model is required to accurately describe the underlying dynamics in nuclear space. Fortunately, the runtime overhead is only a constant factor, as rBCM can be applied in linear time."}], "references": [{"title": "Collision-Based Computing", "author": ["A. Adamatzky"], "venue": "Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A review on applications of ANN and SVM for building electrical energy consumption forecasting", "author": ["A. Ahmad", "M. Hassan", "M. Abdullah", "H. Rahman", "F. Hussin", "H. Abdullah", "R. Saidur"], "venue": "Renewable and Sustainable Energy Reviews, 33:102\u2013109", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An efficient topological distance-based tree kernel", "author": ["F. Aiolli", "G.D.S. Martino", "A. Sperduti"], "venue": "IEEE Trans. Neural Netw. Learning Syst., 26(5):1115\u20131120", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of scaling in random networks", "author": ["A.-L. Barab\u00e1si", "R. Albert"], "venue": "Science, 286(5439):509\u2013512", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Toward automatic hint generation for logic proof tutoring using historical student data", "author": ["T. Barnes", "J. Stamper"], "venue": "B. P. Woolf, E. A\u00efmeur, R. Nkambou, and S. Lajoie, editors, Intelligent Tutoring Systems, volume  Preprint as provided by the authors.  22 5091 of Lecture Notes in Computer Science, pages 373\u2013382. Springer Berlin Heidelberg", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Elements of a theory of simulation II: sequential dynamical systems", "author": ["C.L. Barrett", "H. Mortveit", "C.M. Reidys"], "venue": "Applied Mathematics and Computation, 107(2-3):121\u2013136", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "ETS IV: Sequential dynamical systems: fixed points", "author": ["C.L. Barrett", "H.S. Mortveit", "C.M. Reidys"], "venue": "invertibility and equivalence. Applied Mathematics and Computation, 134(1):153\u2013171", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Elements of a theory of computer simulation I: Sequential ca over random graphs", "author": ["C.L. Barrett", "C.M. Reidys"], "venue": "Applied Mathematics and Computation, 98(2-3):241\u2013259", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "A Survey on Metric Learning for Feature Vectors and Structured Data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "ArXiv e-prints, ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1306}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H.P. Kriegel"], "venue": "In Fifth IEEE International Conference on Data Mining (ICDM\u201905),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Timevarying graphs and dynamic networks", "author": ["A. Casteigts", "P. Flocchini", "W. Quattrociocchi", "N. Santoro"], "venue": "International Journal of Parallel, Emergent and Distributed Systems, 27(5):387\u2013408", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative models for complex network structure", "author": ["A. Clauset"], "venue": "NetSci 2013 - Complex Networks meets Machine Learning", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 20(3):273\u2013297", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Mining structured data", "author": ["G. Da San Martino", "A. Sperduti"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Distributed gaussian processes", "author": ["M.P. Deisenroth", "J.W. Ng"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 1481\u20131490", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["A. Feragen", "N. Kasenburg", "J. Petersen"], "venue": "de Bruijne, and K. Borgwardt. Scalable kernels for graphs with continuous attributes. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 216\u2013224. Curran Associates, Inc.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, 41(1):176\u2013190", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A survey of graph edit distance", "author": ["X. Gao", "B. Xiao", "D. Tao", "X. Li"], "venue": "Pattern Analysis and Applications, 13(1):113\u2013129", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Mathematical games \u2013 the fantastic combinations of John Conway\u2019s new solitaire game \u201clife", "author": ["M. Gardner"], "venue": "Scientific American, 223:120\u2013123", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1970}, {"title": "Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting", "author": ["A. Girard", "C.E. Rasmussen", "J.Q. Candela", "R. Murray-Smith"], "venue": "Advances in neural information processing systems, pages 545\u2013552", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Metric and non-metric proximity transformations at linear costs", "author": ["A. Gisbrecht", "F.-M. Schleif"], "venue": "Neurocomputing, 167:643\u2013657", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of statistical network models", "author": ["A. Goldenberg", "A.X. Zheng", "S.E. Fienberg", "E.M. Airoldi"], "venue": "Foundations and Trends in Machine Learning, 2(2):129\u2013233", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P. Flach", "S. Wrobel"], "venue": "CONFERENCE ON LEARNING THEORY, pages 129\u2013143", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Topographic mapping of large dissimilarity data sets", "author": ["B. Hammer", "A. Hasenfuss"], "venue": "Neural Computation, 22(9):2229\u20132284", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning vector quantization for (dis-)similarities", "author": ["B. Hammer", "D. Hofmann", "F.-M. Schleif", "X. Zhu"], "venue": "NeuroComputing, 131:43\u201351", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent space approaches to social network analysis", "author": ["P.D. Hoff", "A.E. Raftery", "M.S. Handcock"], "venue": "Journal of the American Statistical Association, 97(460):1090\u20131098", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel Robust Soft Learning Vector Quantization", "author": ["D. Hofmann", "B. Hammer"], "venue": "pages 14\u201323. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning interpretable kernelized prototype-based models", "author": ["D. Hofmann", "F.-M. Schleif", "B. Paa\u00dfen", "B. Hammer"], "venue": "Neurocomputing, 141:84\u201396", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic block models: First steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social Networks, 5:109\u2013137", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1983}, {"title": "New potentials for data-driven intelligent tutoring system development and optimization", "author": ["K.R. Koedinger", "E. Brunskill", "R.S. Baker", "E.A. McLaughlin", "J. Stamper"], "venue": "AI Magazine, 34(3):27\u201341", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "The pre-image problem in kernel methods", "author": ["J.-Y. Kwok", "I. Tsang"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Binary codes capable of correcting deletions", "author": ["V.I. Levenshtein"], "venue": "insertions, and reversals. Soviet Physics Doklady, 10(8):707\u2013710", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1965}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "Journal of the American Society for Information Science and Technology, 58(7):1019\u20131031", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "New perspectives and methods in link prediction", "author": ["R.N. Lichtenwalter", "J.T. Lussier", "N.V. Chawla"], "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201910, pages 243\u2013252, New York, NY, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Link prediction in complex networks: A survey", "author": ["L. L\u00fc", "T. Zhou"], "venue": "Physica A: Statistical Mechanics and its Applications, 390(6):1150 \u2013 1170", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Domainindependent proximity measures in intelligent tutoring systems", "author": ["B. Mokbel", "S. Gross", "B. Paa\u00dfen", "N. Pinkwart", "B. Hammer"], "venue": "Proceedings of the 6th International Conference on Educational Data Mining (EDM), pages 334\u2013335", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning for sequences in relational lvq", "author": ["B. Mokbel", "B. Paa\u00dfen", "F.-M. Schleif", "B. Hammer"], "venue": "Neurocomputing, 169:306\u2013322", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability & Its Applications, 9(1):141\u2013142", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1964}, {"title": "Gaussian process prediction for time series of structured data", "author": ["B. Paa\u00dfen", "C. G\u00f6pfert", "B. Hammer"], "venue": "M. Verleysen, editor, 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN), pages 41\u201346. i6doc.com", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive structure metrics for automated feedback provision in intelligent tutoring systems", "author": ["B. Paa\u00dfen", "B. Mokbel", "B. Hammer"], "venue": "Neurocomputing, pages 3\u201313", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic modeling", "author": ["M. Papageorgiou"], "venue": "assignment, and route guidance in traffic networks. Transportation Research Part B: Methodological, 24(6):471\u2013495", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1990}, {"title": "The dissimilarity representation for pattern recognition: foundations and applications", "author": ["E. P\u0119kalska"], "venue": "PhD thesis, Delft University of Technology", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Gaussian processes for time-series modelling", "author": ["S. Roberts", "M. Osborne", "M. Ebden", "S. Reece", "N. Gibson", "S. Aigrain"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 371", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1984}, {"title": "Edit distance from graph spectra", "author": ["A. Robles-Kelly", "E.R. Hancock"], "venue": "In Proceedings Ninth IEEE International Conference on Computer Vision,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2003}, {"title": "Graph edit distance from spectral seriation", "author": ["A. Robles-Kelly", "E.R. Hancock"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(3):365\u2013378", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "A distance measure between attributed relational graphs for pattern recognition", "author": ["A. Sanfeliu", "K.S. Fu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1983}, {"title": "Time series prediction using support vector machines: A survey", "author": ["N.I. Sapankevych", "R. Sankar"], "venue": "IEEE Computational Intelligence Magazine, 4(2):24\u201338", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Description and simulation of dynamic mobility networks", "author": ["A. Scherrer", "P. Borgnat", "E. Fleury", "J.-L. Guillaume", "C. Robardet"], "venue": "Computer Networks, 52(15):2842 \u2013 2858", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning in indefinite proximity spaces - recent trends", "author": ["F.-M. Schleif", "P. Tino", "Y. Liang"], "venue": "M. Verleysen, editor, 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN), pages 113\u2013122. i6doc.com", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Time series analysis and its applications", "author": ["R.H. Shumway", "D.S. Stoffer"], "venue": "Springer Science & Business Media", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian process dynamical models", "author": ["J. Wang", "A. Hertzmann", "D.M. Blei"], "venue": "Advances in neural information processing systems, pages 1441\u2013 1448", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep graph kernels", "author": ["P. Yanardag", "S. Vishwanathan"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1365\u20131374, New York, NY, USA", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating link prediction methods", "author": ["Y. Yang", "R.N. Lichtenwalter", "N.V. Chawla"], "venue": "Knowledge and Information Systems, 45(3):751\u2013782", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparing stars: On approximating graph edit distance", "author": ["Z. Zeng", "A.K.H. Tung", "J. Wang", "J. Feng", "L. Zhou"], "venue": "Proc. VLDB Endow., 2(1):25\u201336", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Simple fast algorithms for the editing distance between trees and related problems", "author": ["K. Zhang", "D. Shasha"], "venue": "SIAM Journal on Computing, 18(6):1245\u20131262", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 40, "context": "to model traffic connections between cities [42], data lines between \u2217Funding by the DFG under grant number HA 2719/6-2 and the CITEC center of excellence (EXC 277) is gratefully acknowledged.", "startOffset": 44, "endOffset": 48}, {"referenceID": 38, "context": "\u2020This contribution is an extension of the work presented at ESANN 2016 under the title \u201cGaussian process prediction for time series of structured data\u201d [40].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 70, "endOffset": 74}, {"referenceID": 35, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 170, "endOffset": 178}, {"referenceID": 39, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 170, "endOffset": 178}, {"referenceID": 40, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 371, "endOffset": 375}, {"referenceID": 32, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 529, "endOffset": 533}, {"referenceID": 35, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 649, "endOffset": 657}, {"referenceID": 29, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 649, "endOffset": 657}, {"referenceID": 47, "context": "Traditionally, predicting the future development based on knowledge of the past is the topic of time series prediction, which has wide-ranging applications in physics, sociology, medicine, engineering, finance and other fields [49, 52].", "startOffset": 227, "endOffset": 235}, {"referenceID": 50, "context": "Traditionally, predicting the future development based on knowledge of the past is the topic of time series prediction, which has wide-ranging applications in physics, sociology, medicine, engineering, finance and other fields [49, 52].", "startOffset": 227, "endOffset": 235}, {"referenceID": 50, "context": "However, classic models in time series prediction, such as ARIMA, NARX, Kalman filters, recurrent networks or reservoir models focus on vectorial data representations, and they are not equipped to handle time series of graphs [52].", "startOffset": 226, "endOffset": 230}, {"referenceID": 1, "context": "predicting the overall load in an energy network [2] or predicting the appearance of single edges in a social network [34].", "startOffset": 49, "endOffset": 52}, {"referenceID": 32, "context": "predicting the overall load in an energy network [2] or predicting the appearance of single edges in a social network [34].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 9, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 13, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 15, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 39, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 52, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 37, "context": "Second, within this space, we can apply similarity- and kernel-based regression methods, such as nearest neighbor regression, kernel regression [39] or Gaussian processes [44] to predict the next position in the kernel space given the current position.", "startOffset": 144, "endOffset": 148}, {"referenceID": 42, "context": "Second, within this space, we can apply similarity- and kernel-based regression methods, such as nearest neighbor regression, kernel regression [39] or Gaussian processes [44] to predict the next position in the kernel space given the current position.", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "Fortunately, Deisenroth and Ng have suggested a simple strategy to permit predictions in linear time, namely distributing the prediction to multiple Gaussian processes, each of which handles only a constant-sized subset of the data [15].", "startOffset": 232, "endOffset": 236}, {"referenceID": 20, "context": "can be avoided using the well-known Nystr\u00f6m approximation as investigated by [22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 157, "endOffset": 160}, {"referenceID": 18, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 187, "endOffset": 191}, {"referenceID": 35, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 308, "endOffset": 316}, {"referenceID": 36, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 308, "endOffset": 316}, {"referenceID": 40, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 170, "endOffset": 178}, {"referenceID": 35, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 170, "endOffset": 178}, {"referenceID": 10, "context": "We begin with two formalisms to model dynamics in graphs, namely time-varying graphs [11], and sequential dynamical systems [6].", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "We begin with two formalisms to model dynamics in graphs, namely time-varying graphs [11], and sequential dynamical systems [6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 32, "context": "This has mainly been addressed in the domain of social networks under the umbrella of link prediction [34, 55], as well as in models of graph growth [23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 53, "context": "This has mainly been addressed in the domain of social networks under the umbrella of link prediction [34, 55], as well as in models of graph growth [23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 21, "context": "This has mainly been addressed in the domain of social networks under the umbrella of link prediction [34, 55], as well as in models of graph growth [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 10, "context": "Time-Varying Graphs: Time-varying graphs have been introduced by Casteigts and colleagues in an effort to integrate different notations found in the fields of delay-tolerant networks, opportunistic-mobility networks or social networks [11].", "startOffset": 235, "endOffset": 239}, {"referenceID": 10, "context": "Rather, dynamics have to be regarded as an \u201cintegral part of the nature of the system\u201d [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Definition 1 (Time-Varying Graph [11]).", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": ",GtK [11, 50].", "startOffset": 5, "endOffset": 13}, {"referenceID": 48, "context": ",GtK [11, 50].", "startOffset": 5, "endOffset": 13}, {"referenceID": 5, "context": "Sequential Dynamical Systems: Sequential dynamical systems (SDS) have been introduced by Barret, Reidys and Mortvart to generalize cellular automata to arbitrary graphical structures [6, 8].", "startOffset": 183, "endOffset": 189}, {"referenceID": 7, "context": "Sequential Dynamical Systems: Sequential dynamical systems (SDS) have been introduced by Barret, Reidys and Mortvart to generalize cellular automata to arbitrary graphical structures [6, 8].", "startOffset": 183, "endOffset": 189}, {"referenceID": 5, "context": "This induces a discrete dynamical system on graphs (where edges and neighborhoods stay fixed) [6, 7, 8].", "startOffset": 94, "endOffset": 103}, {"referenceID": 6, "context": "This induces a discrete dynamical system on graphs (where edges and neighborhoods stay fixed) [6, 7, 8].", "startOffset": 94, "endOffset": 103}, {"referenceID": 7, "context": "This induces a discrete dynamical system on graphs (where edges and neighborhoods stay fixed) [6, 7, 8].", "startOffset": 94, "endOffset": 103}, {"referenceID": 32, "context": "Link Prediction: In the realm of social network analysis, [34] have formulated the link prediction problem, which can be stated as: Given a sequence of temporal subgraphs G0, .", "startOffset": 58, "endOffset": 62}, {"referenceID": 53, "context": "for which edges do we find \u03c1(e, t) = 0 but \u03c1(e, t + 1) = 1 [55]? For example, given all past collaborations in a scientific community, can we predict new collaborations in the future? The simplest approaches to address this challenge derive a similarity index between nodes, such as the number of common neighbors, rank all non-existing edges", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "according to the similarity index of their nodes and predict all edges which are above a certain threshold [34, 35].", "startOffset": 107, "endOffset": 115}, {"referenceID": 33, "context": "according to the similarity index of their nodes and predict all edges which are above a certain threshold [34, 35].", "startOffset": 107, "endOffset": 115}, {"referenceID": 33, "context": "However, more complex models exist as well, such as supervised models [35], and probabilistic as well as stochastic models [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "However, more complex models exist as well, such as supervised models [35], and probabilistic as well as stochastic models [36].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Growth models: In a seminal paper, Barab\u00e1si and Albert described a simple model to incrementally grow an undirected graph node by node from a small, fully connected seed graph [4].", "startOffset": 176, "endOffset": 179}, {"referenceID": 11, "context": "Since then, many other models of graph growth have emerged, most notably stochastic block models and latent space models [12, 23].", "startOffset": 121, "endOffset": 129}, {"referenceID": 21, "context": "Since then, many other models of graph growth have emerged, most notably stochastic block models and latent space models [12, 23].", "startOffset": 121, "endOffset": 129}, {"referenceID": 28, "context": "Stochastic block models assign each node to a block and model the probability of an edge between two nodes only dependent on their respective blocks [30].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "Latent space models embed all nodes in an underlying, latent space and model the probability of an edge depending on the distance in this space [27].", "startOffset": 144, "endOffset": 148}, {"referenceID": 46, "context": "Permitted edit operations include node insertion, node deletion, edge insertion, edge deletion, and the substitution of labels in nodes or edges [48].", "startOffset": 145, "endOffset": 149}, {"referenceID": 31, "context": "This problem is a generalization of the classic string or tree edit distance, which is defined as the minimum number of operations required to transform a string into another or a tree into another respectively [33, 57].", "startOffset": 211, "endOffset": 219}, {"referenceID": 55, "context": "This problem is a generalization of the classic string or tree edit distance, which is defined as the minimum number of operations required to transform a string into another or a tree into another respectively [33, 57].", "startOffset": 211, "endOffset": 219}, {"referenceID": 54, "context": "Unfortunately, while the string edit distance and the tree edit distance can be efficiently computed in O(n2) and O(n4) respectively, computing the exact graph edit distance is NP-hard [56].", "startOffset": 185, "endOffset": 189}, {"referenceID": 17, "context": "relying on self-organizing maps, Gaussian mixture models, graph kernels or binary linear programming [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 39, "context": "to [41, 46, 47]).", "startOffset": 3, "endOffset": 15}, {"referenceID": 44, "context": "to [41, 46, 47]).", "startOffset": 3, "endOffset": 15}, {"referenceID": 45, "context": "to [41, 46, 47]).", "startOffset": 3, "endOffset": 15}, {"referenceID": 39, "context": "For our experiments on real-world java data we will rely on a kernel over such an approximated graph distance as suggested in [41].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "Learning such edit costs from data is the topic of structure metric learning, which has mainly been investigated for string edit distances [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 39, "context": "However, if string edit distance is applied as a substitute for graph edits, the results apply to graphs as well [41].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The only formal requirement on a graph kernel is that it implicitly or explicitly maps a graph G to a vectorial feature representation \u03c6(G) and computes the pairwise kernel values of two graphs G and G\u2032 as the dot-product of their feature vectors k(G,G\u2032) = \u03c6(G) \u00b7 \u03c6(G\u2032) [14].", "startOffset": 270, "endOffset": 274}, {"referenceID": 22, "context": "If each graph has a unique representation (that is, \u03c6 is injective) computing such a kernel is at least as hard as the graph isomorphy problem [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Thus, efficient graph kernels rely on a non-injective feature embedding, which is still expressive enough to capture important differences between graphs [14].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "A particularly popular class of graph kernels are walk kernels which decompose the kernel between two graphs into kernels between paths which can be taken in the graphs [14, 10, 16].", "startOffset": 169, "endOffset": 181}, {"referenceID": 9, "context": "A particularly popular class of graph kernels are walk kernels which decompose the kernel between two graphs into kernels between paths which can be taken in the graphs [14, 10, 16].", "startOffset": 169, "endOffset": 181}, {"referenceID": 15, "context": "A particularly popular class of graph kernels are walk kernels which decompose the kernel between two graphs into kernels between paths which can be taken in the graphs [14, 10, 16].", "startOffset": 169, "endOffset": 181}, {"referenceID": 9, "context": "In our experiments, we will apply the shortest-path kernel suggested by Borgwardt and colleagues, which compares the lengths of shortest paths in both graphs to construct an overall graph kernel [10].", "startOffset": 195, "endOffset": 199}, {"referenceID": 47, "context": "One popular example is the use of support vector regression with wide-ranging applications in finance, business, environmental research and engineering [49].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "Another example are gaussian processes to predict chemical processes [21], motion data [53] and physics data [45].", "startOffset": 69, "endOffset": 73}, {"referenceID": 51, "context": "Another example are gaussian processes to predict chemical processes [21], motion data [53] and physics data [45].", "startOffset": 87, "endOffset": 91}, {"referenceID": 43, "context": "Another example are gaussian processes to predict chemical processes [21], motion data [53] and physics data [45].", "startOffset": 109, "endOffset": 113}, {"referenceID": 47, "context": "We first transform this problem into a regression problem as suggested by Sapankecych and colleagues, that is, we try to learn a function f which maps the past K states of a time series to a successor state [49].", "startOffset": 207, "endOffset": 211}, {"referenceID": 37, "context": "Kernel Regression (KR): Kernel regression was first proposed by Nadaraya and Watson and can be seen as a generalization of 1-nearest neighbor to a smooth predictive function f using a kernel k instead of a dissimilarity [39].", "startOffset": 220, "endOffset": 224}, {"referenceID": 42, "context": "Gaussian Process Regression (GPR): In Gaussian process regression (GPR) we assume that the output points (training as well as test) are a realization of a multivariate random variable with a Gaussian distribution [44].", "startOffset": 213, "endOffset": 217}, {"referenceID": 41, "context": "A dissimilarity for the set X is any function of the form d : X \u00d7 X \u2192 R, such that d decreases if the two input arguments are in some sense more related to each other [43].", "startOffset": 167, "endOffset": 171}, {"referenceID": 41, "context": "Conversely, a similarity for the set X is any function s : X \u00d7 X \u2192 R which increases if the two input arguments are in some sense more related to each other [43].", "startOffset": 157, "endOffset": 161}, {"referenceID": 49, "context": "While in many cases, more rigorous criteria apply as well (such as non-negativity, symmetry, or that any element is most similar to itself) any or all of them may be violated in practice [51].", "startOffset": 187, "endOffset": 191}, {"referenceID": 41, "context": "Such a similarity matrix S is a kernel matrix if and only if it is positive semi-definite [43].", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "The resulting matrix \u039b\u0303 can then be applied to obtain a kernel matrix K = U\u039b\u0303V [22, 43, 51].", "startOffset": 79, "endOffset": 91}, {"referenceID": 41, "context": "The resulting matrix \u039b\u0303 can then be applied to obtain a kernel matrix K = U\u039b\u0303V [22, 43, 51].", "startOffset": 79, "endOffset": 91}, {"referenceID": 49, "context": "The resulting matrix \u039b\u0303 can then be applied to obtain a kernel matrix K = U\u039b\u0303V [22, 43, 51].", "startOffset": 79, "endOffset": 91}, {"referenceID": 20, "context": "However, linear-time approximations have recently been discovered based on the Nystr\u00f6m method [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 41, "context": "Theorem 2 (Pseudo-Euclidean Embeddings [43]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "Refer to [43].", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "Indeed, as graph kernels are generally not injective [24], there might be multiple graphs that correspond to the same affine combination.", "startOffset": 53, "endOffset": 57}, {"referenceID": 30, "context": "Finding such a graph is called the kernel pre-image problem and is hard to solve even for vectorial data [32].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "This poses a challenge with respect to further processing: How do we interpret a data point for which we have no explicit representation [29]? Fortunately, we can still address many classical questions of data analysis for such a representation relying on our already existing implicit embedding in Y.", "startOffset": 137, "endOffset": 141}, {"referenceID": 23, "context": "Refer to [25] for a proof of 16.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "For dissimilarities, this includes relational learning vector quantization for classification [26] or relational neural gas for clustering [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "For dissimilarities, this includes relational learning vector quantization for classification [26] or relational neural gas for clustering [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 26, "context": "For kernels, this includes the non-parametric regression techniques discussed here, but also techniques like kernel vector quantization [28] or support vector machines for classification [13] and kernel variants of k-means, SOM and neural gas for clustering [17].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "For kernels, this includes the non-parametric regression techniques discussed here, but also techniques like kernel vector quantization [28] or support vector machines for classification [13] and kernel variants of k-means, SOM and neural gas for clustering [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 16, "context": "For kernels, this includes the non-parametric regression techniques discussed here, but also techniques like kernel vector quantization [28] or support vector machines for classification [13] and kernel variants of k-means, SOM and neural gas for clustering [17].", "startOffset": 258, "endOffset": 262}, {"referenceID": 20, "context": "We transform S to a kernel matrixK via Eigenvalue correction [22, 29, 43].", "startOffset": 61, "endOffset": 73}, {"referenceID": 27, "context": "We transform S to a kernel matrixK via Eigenvalue correction [22, 29, 43].", "startOffset": 61, "endOffset": 73}, {"referenceID": 41, "context": "We transform S to a kernel matrixK via Eigenvalue correction [22, 29, 43].", "startOffset": 61, "endOffset": 73}, {"referenceID": 42, "context": "A variety of efficient approximation schemes exist [44].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Recently, the robust Bayesian Committee Machine (rBCM) has been introduced as a particularly fast and accurate approximation [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "As suggested by the authors, we use the differential entropy, given as \u03b2c = 12 \u00b7 ( log(\u03c3 prior) \u2212 log(\u03c3 c ) ) [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "Within this work, we address the latter issue by applying relational neural gas only on a constant-sized subset of the data and extending the clustering to the full data set later on, resulting in overall linear time [25].", "startOffset": 217, "endOffset": 221}, {"referenceID": 3, "context": "Barab\u00e1si-Albert model: This is a simple stochastic model of graph growth in undirected graphs [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "It has been shown that the edge distribution resulting from this growth model is scale-free, more specifically the probability of a certain degree k is P (k) = m \u00b7k\u22123, where m is the number of nodes [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 18, "context": "Conway\u2019s Game of Life: John Conway\u2019s Game of Life [20] is a simple, 2-dimensional cellular automaton model.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "Note that Conway\u2019s Game of Life is turing-complete and its evolution is, in general, unpredictable without computing every single step according to the rules [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 9, "context": "As data representation for the theoretical data set we use an explicit feature embedding inspired by the shortest-path-kernel of Borgwardt and colleagues [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "This scheme is inspired by prior work in intelligent tutoring systems, such as the hint factory by Barnes and colleagues [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 35, "context": "MiniPalindrome: This dataset consists of 48 Java programs, each realizing one of eight different strategies to recognize palindromic input (see figure 4) [37] 2.", "startOffset": 154, "endOffset": 158}, {"referenceID": 35, "context": "The programs come in eight different variations described in [37].", "startOffset": 61, "endOffset": 65}, {"referenceID": 36, "context": "Sorting: This is a benchmark dataset of 64 Java sorting programs taken from the web, implementing one of two sorting algorithms, namely BubbleSort or InsertionSort (see figure 4) [38] 3.", "startOffset": 179, "endOffset": 183}, {"referenceID": 45, "context": "Then, we computed a sequence alignment distance on the resulting node sequenes, similar to the method described by Robles-Kelly and Hancock [47].", "startOffset": 140, "endOffset": 144}, {"referenceID": 39, "context": "In particular, we used an affine sequence alignment with learned node dissimilarity as suggested in [41].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "We transformed the dissimilarity so a similarity via the radial basis function transformation and obtained a kernel via clip Eigenvalue correction [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "However, two problems remain open: First, usual hyperparameter optimization techniques depend on a vectorial data representation [15] and one has to adapt them for a relational case.", "startOffset": 129, "endOffset": 133}], "year": 2017, "abstractText": "Graph models are relevant in many fields, such as distributed computing, intelligent tutoring systems or social network analysis. In many cases, such models need to take changes in the graph structure into account, i.e. a varying number of nodes or edges. Predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics, e.g. with respect to user behaviour. However, predictive techniques in the past have almost exclusively focused on single edges or nodes. In this contribution, we attempt to predict the future state of a graph as a whole. We propose to phrase time series prediction as a regression problem and apply dissimilarityor kernel-based regression techniques, such as 1-nearest neighbor, kernel regression and Gaussian process regression, which can be applied to graphs via graph kernels. The output of the regression is a point embedded in a pseudo-Euclidean space, which can be analyzed using subsequent dissimilarityor kernel-based processing methods. We discuss strategies to speed up Gaussian Processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems. We find that simple regression methods, such as kernel regression, are sufficient to capture the dynamics in the theoretical models, but that Gaussian process regression significantly improves the prediction error for real-world data.", "creator": "LaTeX with hyperref package"}}}