{"id": "1704.06616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Accurately and Efficiently Interpreting Human-Robot Instructions of Varying Granularities", "abstract": "Humans can ground natural language commands to tasks at both abstract and fine-grained levels of specificity. For instance, a human forklift operator can be instructed to perform a high-level action, like \"grab a pallet\" or a lowlevel action like \"tilt back a little bit.\" While robots are also capable of grounding language commands to tasks, previous methods implicitly assume that all commands and tasks reside at a single, fixed level of abstraction. Additionally, those approaches that do not use abstraction experience inefficient planning and execution times due to the large, intractable state-action spaces, which closely resemble real world complexity. In this work, by grounding commands to all the tasks or subtasks available in a hierarchical planning framework, we arrive at a model capable of interpreting language at multiple levels of specificity ranging from coarse to more granular. We show that the accuracy of the grounding procedure is improved when simultaneously inferring the degree of abstraction in language used to communicate the task. Leveraging hierarchy also improves efficiency: our proposed approach enables a robot to respond to a command within one second on 90% of our tasks, while baselines take over twenty seconds on half the tasks. Finally, we demonstrate that a real, physical robot can ground commands at multiple levels of abstraction allowing it to efficiently plan different subtasks within the same planning hierarchy.", "histories": [["v1", "Fri, 21 Apr 2017 16:15:19 GMT  (1338kb,D)", "http://arxiv.org/abs/1704.06616v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["dilip arumugam", "siddharth karamcheti", "nakul gopalan", "lawson l s wong", "stefanie tellex"], "accepted": false, "id": "1704.06616"}, "pdf": {"name": "1704.06616.pdf", "metadata": {"source": "CRF", "title": "Accurately and Efficiently Interpreting Human-Robot Instructions of Varying Granularities", "authors": ["Dilip Arumugam", "Siddharth Karamcheti", "Nakul Gopalan", "Stefanie Tellex"], "emails": ["arumugam@,", "karamcheti@,", "ngopalan@cs.,", "lsw@,", "stefie10@cs.}brown.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "II. RELATED WORK", "text": "In fact, most people who are able to put themselves in a situation in which they are able, in which they are able to put themselves in a situation in which they are able, in which they are able to put themselves in a situation in which they are able to put themselves in a situation in which they are in a situation in which they are able, in which they are able to put themselves in a situation in which they are able to put themselves in a situation in which they are in."}, {"heading": "III. TECHNICAL APPROACH", "text": "In order to interpret a variety of natural speech commands, there must be representation for all kinds of tasks and sub-tasks. \"We define an object-oriented Markov decision method (OOMDP) to represent the actions of the robot.\" An MDP is a quintuple of < S, T, R, G, where S represents the series of tasks that define a particular environment, A represents the action frame that an agent can execute to transition between states, T defines the transitional probability distribution across all possible next states and actions performed, R defines the numerical reward earned for a particular transition, and it represents the discount factor or effective time horizon taking into account. Planning in an MDP produces a mapping between states and actions, or a policy that maximizes the total expected reward. In our framework, as in MacGlashan et al, we will map between words in the language and specific reward functions."}, {"heading": "IV. LANGUAGE MODELS", "text": "We compare four language models: an IBM Model 2 translation model (similar to MacGlashan et al. [17]), a deep neural network language model, and two sets of recursive neural network language models with different architectures. For detailed descriptions of all models presented, refer to the supplementary material. IBM Model 2As a starting point, we use the well-known IBM Model 2 (IBM2) machine translation model [4, 5] as a statistical language model for evaluating reward functions based on an input command. IBM2 is a generative model that solves the following objective, which corresponds to Equation 7 by the Bayes rule: l, m \u00b2 = argmax l, mPr (l, m) \u00b7 Pr (c | l, m) (3)."}, {"heading": "B. Neural Network Language Models", "text": "We evaluate three classes of neural network architectures (see Fig. 2): a network that requires a natural language command structure encoded as a bag-of-words with different parameters for each level of abstraction (Multi-NN); a recursive network that takes into account the order of words in the sequence, even with different parameters (Multi-RNN); and a recursive network that takes into account the order of words in the sequence and a common parameter space across the levels of abstraction (Single-RNN).1) Multi-NN: Multiple Output Feed-Forward Network: We propose a neural network to be introduced that takes into account the order of words in the sequence and a common parameter space across the levels of abstraction (Single-RNN).1) that includes both the probability of each level of abstraction as well as the probability of each reward function and the probability of each reward function."}, {"heading": "V. EVALUATION", "text": "The aim of our evaluation is to test the hypothesis that hierarchical structure improves the speed and accuracy of speech search at multiple levels of abstraction. We evaluate our method with a corpus-based simulation evaluation and evaluate the speed and accuracy of our approach. Additionally, we demonstrate our system on a mobile Turtlebot robot."}, {"heading": "A. Mobile Robot Domain", "text": "The Cleanup world is a mobile manipulator-robot domain, divided into spaces (designated by unique colors) with open doors. In addition, each room can contain a number of objects that can be moved (pushed) by the robot. This problem is modeled after a mobile robot that must move objects around an environment, analogous to a robot forklift that can operate in a warehouse, or a pick-and-place robot in a home environment. We use an AMDP that comes directly from Gopalan et al. [11] for the Cleanup World domain, which imposes a three-level abstraction hierarchy that can be used for planning. The combinatorial large state space of the Cleanup world simulates complexity and is ideal for exploiting abstractions. At the lowest level of abstraction (which we call L0) is the (primitive) action available to the robot agent."}, {"heading": "B. Procedure", "text": "In light of the AMDP for the Cleanup World domain, we conducted an Amazon Mechanical Turk (AMT) user study to collect natural language samples at different levels of abstraction for the Cleanup World domain [15, 17] (see Fig. 3a). Noters were shown video demonstrations of ten tasks using a single starting instance of the Cleanup World domain shown in Fig. 3a. For each task, we asked them to give an order that they would ask a robot to perform the action they saw in the video, while limiting their language to one of three possible levels in a designated abstraction hierarchy: fine-grained, medium, and coarse. This data was used to construct multiple parallel companies for the task grounding. We measured the performance of our system by passing each command to the language system and evaluating the correct level of abstraction and reward function."}, {"heading": "C. Robot Task Grounding", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "D. Robot Response Time", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "E. Robot Demonstration", "text": "Using the trained grounding model and the corresponding AMDP hierarchy, we tested a Turtlebot in a small version of the Cleanup World domain. To accommodate the Turtlebot's continuous action space, the low, primitive actions at L0 of the AMDP were replaced by forward, reverse and bi-directional rotation actions; all other levels of the AMDP remained unchanged. These commands were implemented using low, closed loop commands sent to the robot using the Robot Operating System [26]. The spoken commands were provided by a human expert who instructed the robot to navigate from one room to another. These verbal commands were converted from speech to text using Google's Speech API [1] before being grounded using the trained Single RNN model. The resulting grounding, where both the AMDP hierarchy level and the reward function were fed directly into the AMDP planner, resulting in almost immediate planning and execution."}, {"heading": "VI. DISCUSSION", "text": "We present the results of the exploration of the area where the people reside."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we presented a system for interpretation and grounding of natural speech commands to a pick-and-place mobile robot at multiple levels of abstraction. To our knowledge, our system is not only the first to ground language at multiple levels of abstraction, but also the first to use deep neural networks to ground speech on robots. We demonstrate that the integration of such a language education system with a hierarchical planner allows the specification and efficient execution of a wide range of robotic tasks, thereby fostering a very natural interaction between humans and robots. Furthermore, through our turtlebot demonstrations, we demonstrate that this system works well in real environments. Future work should extend this system to a wide variety of real scenarios. Such a system would be effective in any environment where multiple levels of abstraction are useful; for example, in surgical and domestic robotics. Furthermore, it would be incredibly fruitful to expand the proposed work to a more natural level to expand upon this abstraction and to facilitate natural language abstraction."}, {"heading": "VIII. ACKNOWLEDGEMENTS", "text": "This work is supported by the National Science Foundation under grant number IIS-1637614, the US Army / DARPA under grant number W911NF-15-1-0503 and the National Aeronautics and Space Administration under grant number NNX16AR61G.Lawson L.S. Wong was supported by a grant from the Croucher Foundation."}, {"heading": "B. Multi-NN - Multiple Output Feed-Forward Network:", "text": "A breakdown of the exact network transformations is as follows: ~ e = Lookup (E, ~ c) ~ s = ReLU (~ e \u00b7 Ws + bs) ~ t = ReLU (~ s \u00b7 Wkt + bkt) ~ o = Softmax (~ t \u00b7 Wkt + bkt) Here, the layer-specific weight and bias parameters are each expressed by W, b. Superscripts denote output-specific parameters, and the (\u00b7) operation denotes matrix vector product. To generate high-dimensional, fixed-size representations of each word in the finite vocabulary of the natural language, the initial embedding layer contains a reference matrix E, which is trained using back propagation with the rest of the model, with each line denoting a single word as embedding."}, {"heading": "C. Gated Recurrent Units", "text": "Both the multi-RNN and single-RNN models use gated recurrent unit (GRU) cells, a special type of recursive neural network cells. GRU cells only maintain a single hidden state h, and the update rules are as follows: ~ zt = \u03c3 (Wz \u00b7 ~ xt + Uz \u00b7 ht \u2212 1 + bz) ~ rt = \u03c3 (Wr \u00b7 xt + Ur \u00b7 ht \u2212 1 + br) ~ nt = Tanh (Wh \u00b7 xt + Uh \u00b7 (~ rt \u2212 1) + bz) ~ ht = (~ 1 \u2212 zt) ~ ht \u2212 1 + ~ zt ~ ntHere, the (\u00b7) operation denotes matrix vector product, while the () operation denotes an elementary product. The intermediate vectors ~ z, ~ r act as refresh and reset gates, dictating how much of the hidden state should be overwritten with the new information in the ZW. The parameters are tucked together with the rest of the ZW = the ZW."}, {"heading": "D. Multi-RNN - Multiple Output Recurrent Network:", "text": "We now give a detailed breakdown of the exact network transformations that make up the multi-RNN: ~ e1, ~ e2... ~ en = Lookup (E, c1, c2.. cn) ~ h = GRU (~ e1, ~ e2,... ~ en) ~ s = ReLU (~ h \u00b7 Ws + bs) ~ t = ReLU (~ s \u00b7 Wkt + bkt) ~ o = Softmax (~ t \u00b7 Wkt + bkt) Here too, the layer parameters are specified by W, b, where the superscripts denote the output-specific parameters."}, {"heading": "E. Single-RNN - Single Output Recurrent Network:", "text": "A detailed breakdown of the single RNN transformations is: ~ e1, ~ e2... ~ en = Lookup (E, c1, c2... cn) ~ h = GRU (~ e1, ~ e2,... ~ en) ~ s = ReLU (~ h \u00b7 Ws + bs) ~ t = ReLU (~ s \u00b7 Wt + bt) ~ o = Softmax (~ t \u00b7 Wt + bt) Note that these transformations are exactly the same as the multi-RNN, with the only exception that there is only one output and not several. Since there is only one output, the new loss is merely the cross-entropy loss of the predicted common level reward function distribution."}], "references": [{"title": "Dynamic Programming", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1957}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "A statistical approach to machine translation", "author": ["Peter F. Brown", "John Cocke", "Stephen Della Pietra", "Vincent J. Della Pietra", "Frederick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["David L. Chen", "Raymond J. Mooney"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fal\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fal\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G. Dieterrich"], "venue": "Journal on Artificial Intelligence Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["Carlos Diuk", "Andre Cohen", "Michael L. Littman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Planning with abstract markov decision processes", "author": ["Nakul Gopalan", "Marie desJardins", "Michael L. Littman", "James MacGlashan", "Shawn Squire", "Stefanie Tellex", "John Winder", "Lawson L.S. Wong"], "venue": "In International Conference on Machine Learning Workshop on Abstraction in Reinforcement Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A natural language planner interface for mobile manipulators", "author": ["Thomas M. Howard", "Stefanie Tellex", "Nicholas Roy"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L. Boyd-Graber", "Hal Daum\u00e9"], "venue": "In Conference of the Association for Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Hierarchical modelbased reinforcement learning: R-max + MAXQ", "author": ["Nicholas K. Jong", "Peter Stone"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Sokoban: a challenging single-agent search problem", "author": ["Andreas Junghanns", "Jonathan Schaeeer"], "venue": "In International Joint Conference on Artificial Intelligence Workshop on Using Games as an Experimental Testbed for AI Reasearch,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Grounding English commands to reward functions", "author": ["James MacGlashan", "Monica Babe\u015f-Vroman", "Marie des- Jardins", "Michael L. Littman", "Smaranda Muresan", "Shawn Squire", "Stefanie Tellex", "Dilip Arumugam", "Lei Yang"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Walk the talk: Connecting language, knowledge, and action in route instructions", "author": ["Matt MacMahon", "Brian Stankiewicz", "Benjamin Kuipers"], "venue": "In National Conference on Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["Cynthia Matuszek", "Evan Herbst", "Luke Zettlemoyer", "Dieter Fox"], "venue": "In International Symposium on Experimental Robotics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Roles of macro-actions in accelerating reinforcement learning", "author": ["Amy McGovern", "Richard S. Sutton", "Andrew H Fagg"], "venue": "Grace Hopper Celebration of Women in Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Bounded real-time dynamic programming: RTDP with monotone upper bounds and per-  formance guarantees", "author": ["H. Brendan McMahan", "Maxim Likhachev", "Geoffrey J. Gordon"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Efficient grounding of abstract spatial concepts for natural language interaction with robot manipulators", "author": ["Rohan Paul", "Jacob Arkin", "Nicholas Roy", "Thomas M. Howard"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "ROS: an open-source robot operating system", "author": ["Morgan Quigley", "Josh Faust", "Tully Foote", "Jeremy Leibs"], "venue": "In IEEE International Conference on Robotics and Automation Workshop on Open Source Software,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder P. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R. Walter", "Ashis Gopal Banerjee", "Seth Teller", "Nicholas Roy"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "abstraction [6, 19, 30].", "startOffset": 12, "endOffset": 23}, {"referenceID": 17, "context": "abstraction [6, 19, 30].", "startOffset": 12, "endOffset": 23}, {"referenceID": 28, "context": "abstraction [6, 19, 30].", "startOffset": 12, "endOffset": 23}, {"referenceID": 15, "context": "[17] decouple the problem and use a statistical language model to map between language and robot goals, expressed as reward functions in a Markov Decision Process (MDP).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "environments face an inherent tradeoff between including lowlevel task representations and increasing the time needed to plan in the presence of both low- and high-level reward functions [11].", "startOffset": 187, "endOffset": 191}, {"referenceID": 4, "context": "One of the earliest works in this area mapped tasks to another planning language, which then grounded to the actions performed by the robots [6].", "startOffset": 141, "endOffset": 144}, {"referenceID": 10, "context": "More recent methods ground natural language commands to tasks using features that describe correspondences between natural language phrases present in the task description and physical objects and actions available in the world [12, 19, 30].", "startOffset": 228, "endOffset": 240}, {"referenceID": 17, "context": "More recent methods ground natural language commands to tasks using features that describe correspondences between natural language phrases present in the task description and physical objects and actions available in the world [12, 19, 30].", "startOffset": 228, "endOffset": 240}, {"referenceID": 28, "context": "More recent methods ground natural language commands to tasks using features that describe correspondences between natural language phrases present in the task description and physical objects and actions available in the world [12, 19, 30].", "startOffset": 228, "endOffset": 240}, {"referenceID": 23, "context": "[25] ground to abstract spatial concepts like rows, columns and middle before learning correspondences between them to solve tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] proposed grounding natural language commands to reward functions associated with certain tasks, allowing robot agents to plan in stochastic environments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] treated the goal reward function as a sequence of propositional functions, much like a machine language, to which a natural language task can be translated, using an IBM Model 2 [4, 5] (IBM2) language model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[17] treated the goal reward function as a sequence of propositional functions, much like a machine language, to which a natural language task can be translated, using an IBM Model 2 [4, 5] (IBM2) language model.", "startOffset": 183, "endOffset": 189}, {"referenceID": 3, "context": "[17] treated the goal reward function as a sequence of propositional functions, much like a machine language, to which a natural language task can be translated, using an IBM Model 2 [4, 5] (IBM2) language model.", "startOffset": 183, "endOffset": 189}, {"referenceID": 0, "context": "Planning in domains with large state-action spaces is computationally expensive as planners like value iteration and bounded RTDP need to explore the domain at the lowest, \u201cflat\u201d level of abstraction [2, 21].", "startOffset": 200, "endOffset": 207}, {"referenceID": 19, "context": "Planning in domains with large state-action spaces is computationally expensive as planners like value iteration and bounded RTDP need to explore the domain at the lowest, \u201cflat\u201d level of abstraction [2, 21].", "startOffset": 200, "endOffset": 207}, {"referenceID": 18, "context": "A common method to describe subtasks is by using temporal abstraction in the form of Macro-Actions [20] or Options [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 27, "context": "A common method to describe subtasks is by using temporal abstraction in the form of Macro-Actions [20] or Options [29].", "startOffset": 115, "endOffset": 119}, {"referenceID": 18, "context": "These methods achieve subgoals using either a fixed sequence of actions [20] or a subgoal based policy [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "These methods achieve subgoals using either a fixed sequence of actions [20] or a subgoal based policy [29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 7, "context": "Other methods for abstraction, like MAXQ [9], R-MAXQ [14] and Abstract Markov Decision Processes (AMDPs) [11] involve providing a hierarchy of subtasks.", "startOffset": 41, "endOffset": 44}, {"referenceID": 12, "context": "Other methods for abstraction, like MAXQ [9], R-MAXQ [14] and Abstract Markov Decision Processes (AMDPs) [11] involve providing a hierarchy of subtasks.", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "Other methods for abstraction, like MAXQ [9], R-MAXQ [14] and Abstract Markov Decision Processes (AMDPs) [11] involve providing a hierarchy of subtasks.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "In these methods, a subtask is associated with a subgoal and a state abstraction relevant to achieving the subgoal [9, 11, 14].", "startOffset": 115, "endOffset": 126}, {"referenceID": 9, "context": "In these methods, a subtask is associated with a subgoal and a state abstraction relevant to achieving the subgoal [9, 11, 14].", "startOffset": 115, "endOffset": 126}, {"referenceID": 12, "context": "In these methods, a subtask is associated with a subgoal and a state abstraction relevant to achieving the subgoal [9, 11, 14].", "startOffset": 115, "endOffset": 126}, {"referenceID": 7, "context": "Both MAXQ [9] and R-MAXQ [14] are bottom-up planners, they back up each individual action\u2019s reward across the hierarchy.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "Both MAXQ [9] and R-MAXQ [14] are bottom-up planners, they back up each individual action\u2019s reward across the hierarchy.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "We chose AMDPs [11] for our approach because they plan in a \u201ctop-down\u201d fashion.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "An AMDP hierarchy itself is an acyclic graph in which each node is a primitive action or an AMDP that solves a subtask defined by its parent [11]; the states of the subtask AMDP are abstract representations of the environment state.", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "AMDPs have been shown to achieve faster planning performance than other hierarchical methods [11]", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "Deep neural networks have had great success in a variety of natural language tasks, like traditional language modeling [3, 22, 23], machine translation [7, 8], and text categorization [13].", "startOffset": 119, "endOffset": 130}, {"referenceID": 20, "context": "Deep neural networks have had great success in a variety of natural language tasks, like traditional language modeling [3, 22, 23], machine translation [7, 8], and text categorization [13].", "startOffset": 119, "endOffset": 130}, {"referenceID": 21, "context": "Deep neural networks have had great success in a variety of natural language tasks, like traditional language modeling [3, 22, 23], machine translation [7, 8], and text categorization [13].", "startOffset": 119, "endOffset": 130}, {"referenceID": 5, "context": "Deep neural networks have had great success in a variety of natural language tasks, like traditional language modeling [3, 22, 23], machine translation [7, 8], and text categorization [13].", "startOffset": 152, "endOffset": 158}, {"referenceID": 6, "context": "Deep neural networks have had great success in a variety of natural language tasks, like traditional language modeling [3, 22, 23], machine translation [7, 8], and text categorization [13].", "startOffset": 152, "endOffset": 158}, {"referenceID": 11, "context": "Deep neural networks have had great success in a variety of natural language tasks, like traditional language modeling [3, 22, 23], machine translation [7, 8], and text categorization [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 1, "context": "[3] learn distributed representations of words in tandem with the rest of their language model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[24] propose a system solely dedicated to learning these representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "commands) to a fixed-size vector representation, which have been widely used in NLP [7, 8].", "startOffset": 84, "endOffset": 90}, {"referenceID": 6, "context": "commands) to a fixed-size vector representation, which have been widely used in NLP [7, 8].", "startOffset": 84, "endOffset": 90}, {"referenceID": 8, "context": "We define an Object-oriented Markov Decision Process (OOMDP) to represent the robot\u2019s actions [10].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "[17], we will map between words in language and specific reward functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "For our evaluation, we use the Cleanup World [15, 17] OO-MDP, which models a mobile manipulator robot; this domain is defined in Sec.", "startOffset": 45, "endOffset": 53}, {"referenceID": 15, "context": "For our evaluation, we use the Cleanup World [15, 17] OO-MDP, which models a mobile manipulator robot; this domain is defined in Sec.", "startOffset": 45, "endOffset": 53}, {"referenceID": 9, "context": "In our work, fast planning and the ability to ground and solve individual subtasks without needing to solve the entire planning problem make AMDPs a reliable choice for the hierarchical planner [11].", "startOffset": 194, "endOffset": 198}, {"referenceID": 15, "context": "[17]), a deep neural network bag-of-words language model, and two sets of recurrent neural network language models, with varying architectures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "We use the well known IBM Model 2 (IBM2) machine translation model [4, 5] as a statistical language model for scoring reward functions based on some input command.", "startOffset": 67, "endOffset": 73}, {"referenceID": 3, "context": "We use the well known IBM Model 2 (IBM2) machine translation model [4, 5] as a statistical language model for scoring reward functions based on some input command.", "startOffset": 67, "endOffset": 73}, {"referenceID": 15, "context": "[17] and we continue in an identical fashion training the IBM2 using the standard EM algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "1) Multi-NN: Multiple Output Feed-Forward Network: We propose a feed-forward neural network [3, 13, 24] that takes in a natural language command as a bag-of-words, and outputs both the probability of each of the different levels of abstraction, as well as the probability of each reward function.", "startOffset": 92, "endOffset": 103}, {"referenceID": 11, "context": "1) Multi-NN: Multiple Output Feed-Forward Network: We propose a feed-forward neural network [3, 13, 24] that takes in a natural language command as a bag-of-words, and outputs both the probability of each of the different levels of abstraction, as well as the probability of each reward function.", "startOffset": 92, "endOffset": 103}, {"referenceID": 22, "context": "1) Multi-NN: Multiple Output Feed-Forward Network: We propose a feed-forward neural network [3, 13, 24] that takes in a natural language command as a bag-of-words, and outputs both the probability of each of the different levels of abstraction, as well as the probability of each reward function.", "startOffset": 92, "endOffset": 103}, {"referenceID": 14, "context": "We train the network via backpropagation, using the Adam Optimizer [16], with a minibatch size of 16, and a learning rate of 0.", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "Furthermore, to better regularize the model and encourage robustness, we use Dropout [27] after the initial embedding layer, as well as after the output-specific hidden layers with probability p = 0.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "2) Multi-RNN: Multiple Output Recurrent Network: Inspired by the success of recurrent neural networks in NLP tasks [7, 22, 23, 28], we propose a recurrent neural network language model that takes in a command as a sequence of words and, like the Multi-NN bag-of-words model, outputs both the probability of each of the different levels of abstraction, as well as the probability of each reward function, at each of the different levels of abstraction.", "startOffset": 115, "endOffset": 130}, {"referenceID": 20, "context": "2) Multi-RNN: Multiple Output Recurrent Network: Inspired by the success of recurrent neural networks in NLP tasks [7, 22, 23, 28], we propose a recurrent neural network language model that takes in a command as a sequence of words and, like the Multi-NN bag-of-words model, outputs both the probability of each of the different levels of abstraction, as well as the probability of each reward function, at each of the different levels of abstraction.", "startOffset": 115, "endOffset": 130}, {"referenceID": 21, "context": "2) Multi-RNN: Multiple Output Recurrent Network: Inspired by the success of recurrent neural networks in NLP tasks [7, 22, 23, 28], we propose a recurrent neural network language model that takes in a command as a sequence of words and, like the Multi-NN bag-of-words model, outputs both the probability of each of the different levels of abstraction, as well as the probability of each reward function, at each of the different levels of abstraction.", "startOffset": 115, "endOffset": 130}, {"referenceID": 26, "context": "2) Multi-RNN: Multiple Output Recurrent Network: Inspired by the success of recurrent neural networks in NLP tasks [7, 22, 23, 28], we propose a recurrent neural network language model that takes in a command as a sequence of words and, like the Multi-NN bag-of-words model, outputs both the probability of each of the different levels of abstraction, as well as the probability of each reward function, at each of the different levels of abstraction.", "startOffset": 115, "endOffset": 130}, {"referenceID": 5, "context": "[7], a particular type of Recurrent Neural Network cell that is characterized by a hidden state incrementally updated with new inputs (i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "We utilize them specifically as they have been shown to work well on natural language sequence modeling tasks [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "[11] for the Cleanup World domain which imposes a three-level abstraction hierarchy that can be utilized for planning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "at all levels and their corresponding propositional functions are defined by [11] however Fig.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "tion for the Cleanup World domain [15, 17] (see Fig.", "startOffset": 34, "endOffset": 42}, {"referenceID": 15, "context": "tion for the Cleanup World domain [15, 17] (see Fig.", "startOffset": 34, "endOffset": 42}, {"referenceID": 15, "context": "[17] results, as we see that without accounting for abstractions in language, there is a noticeable effect on grounding accuracy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17], the demonstrations shown were not only limited to simple robot navigation and object", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] observed, we note that we do not utilize a task or behavior model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "\u2022 BASE: A state-of-the-art flat (non-hierarchical) planner, bounded real-time dynamic programming (BRTDP [21]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "\u2022 AMDP: A hierarchical planner for MDPs [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "Additionally, NH suffered from two outliers, where the planning problem became more complex because the solution was constrained to conform to the hierarchy; this is a well-known tradeoff in hierarchical planning [9].", "startOffset": 213, "endOffset": 216}, {"referenceID": 24, "context": "These commands were implemented using low level, closed loop control policies, which were sent to the robot using the Robot Operating System [26].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "[18].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Humans can ground natural language commands to tasks at both abstract and fine-grained levels of specificity. For instance, a human forklift operator can be instructed to perform a high-level action, like \u201cgrab a pallet\u201d or a lowlevel action like \u201ctilt back a little bit.\u201d While robots are also capable of grounding language commands to tasks, previous methods implicitly assume that all commands and tasks reside at a single, fixed level of abstraction. Additionally, those approaches that do not use abstraction experience inefficient planning and execution times due to the large, intractable state-action spaces, which closely resemble real world complexity. In this work, by grounding commands to all the tasks or subtasks available in a hierarchical planning framework, we arrive at a model capable of interpreting language at multiple levels of specificity ranging from coarse to more granular. We show that the accuracy of the grounding procedure is improved when simultaneously inferring the degree of abstraction in language used to communicate the task. Leveraging hierarchy also improves efficiency: our proposed approach enables a robot to respond to a command within one second on 90% of our tasks, while baselines take over twenty seconds on half the tasks. Finally, we demonstrate that a real, physical robot can ground commands at multiple levels of abstraction allowing it to efficiently plan different subtasks within the same planning hierarchy.", "creator": "LaTeX with hyperref package"}}}