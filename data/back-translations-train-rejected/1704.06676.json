{"id": "1704.06676", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Multi-Objective Deep Q-Learning with Subsumption Architecture", "abstract": "In this work we present a method for using Deep Q-Networks (DQNs) in multi-objective tasks. Deep Q-Networks provide remarkable performance in single objective tasks learning from high-level visual perception. However, in many scenarios (e.g in robotics), the agent needs to pursue multiple objectives simultaneously. We propose an architecture in which separate DQNs are used to control the agent's behaviour with respect to particular objectives. In this architecture we use signal suppression, known from the (Brooks) subsumption architecture, to combine outputs of several DQNs into a single action. Our architecture enables the decomposition of the agent's behaviour into controllable and replaceable sub-behaviours learned by distinct modules. To evaluate our solution we used a game-like simulator in which an agent - provided with high-level visual input - pursues multiple objectives in a 2D world. Our solution provides benefits of modularity, while its performance is comparable to the monolithic approach.", "histories": [["v1", "Fri, 21 Apr 2017 18:42:02 GMT  (261kb,D)", "http://arxiv.org/abs/1704.06676v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["tomasz tajmajer"], "accepted": false, "id": "1704.06676"}, "pdf": {"name": "1704.06676.pdf", "metadata": {"source": "CRF", "title": "Multi-Objective Deep Q-Learning with Subsumption Architecture", "authors": ["Tomasz Tajmajer"], "emails": ["t.tajmajer@mimuw.edu.pl"], "sections": [{"heading": null, "text": "Keywords: Deep Q-Learning, Deep Reinforcement Learning, Subsumption Architecture, Neural Networks"}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Single Objective Reinforcement Learning", "text": "In each objective amplification method, an agent interacts with the environment by perceiving the state of st-S and performing an action for each step t. The objective of the agent is to maximize the expected discounted reward for performing an action. After performing an action, the agent receives a reward rt. Then, the agent observes the next state st + 1 and the process repeats itself. The agent's goal is to maximize the expected discounted reward for performing an action a in a state s. For certain states, at = arg maxa Q (s, a) is the optimal action. Deep Q-Learning uses deep neural networks to approximate Q (s, a) values, enabling the use of this method in many real-world applications. Deep Q-Networks [2] can be used with high-grade inputs such as video games."}, {"heading": "2.2 Multi-Objective Reinforcement Learning", "text": "We can consider a more complex reinforcement learning scenario in which multiple goals are pursued by the agent. Let O be the totality of an agent's goals. We can assign a priority to each target o-O to form a linear sequence [o1, o2,..., on] so that ok has a lower priority than oj if k < j, where n is the number of targets. Instead of a single reward at each time step, the agent receives t in relation to each target oi, i.e.: rt = [r1, t, r2, t,... rn, t] where ri, t corresponds to the target oi. For each target oi and step t, we can define the discounted return as: Ri, t = long k = 0 Kri, t + k (1) In addition, for each target oi, we can determine a function oi (s, a) representing the expected discounted return, i.e. Qi, k = 0 Kri, t (optimum + 1)."}, {"heading": "3 Subsumption and Deep Q-Networks", "text": "We have looked at an agent who has multiple goals, receives rewards for those goals, and has a Q function for each target. In this section we describe a) how actions from different Q functions can be merged, b) how an agent can switch between pursued goals, and c) how all Q functions in the Q vector can be trained simultaneously. We refer to our method Multiple Deep Q-Network with Subsumption (mDQNS)."}, {"heading": "3.1 Combining multiple DQNs", "text": "In the case of a multi-objective agent, we can use a separate DQN as an approximate value for each q value, and we want to use q values from all DQNs to select a single action a that is executed by the agent. Let us define a vector qi that consists of q values provided by Qi (s, a) for each possible action a and a single target oi, i.e.: qi = [Qi (s, a0), Qi (s, a1), (4) In a single object case, the optimal action a would be equal aj for such action a and a single target oi, i.e.: qi = max qi. Here we have a qi vector for each target oi, i.e. also a vector for optimal actions a."}, {"heading": "3.2 Suppression", "text": "We now have a method of combining outputs from multiple DQNs. However, such a combination of higher priority QQNi priority will hardly result in a meaningful choice of action. So, if we return to the previous example and say that we are driving a car; actions a1, a2, a3 correspond to the left turn, go straight and turn right respectively. So, if we approach a wall and perform the action proposed in Q1, we will turn right if we agree with the statement in Q1, then we will turn left. As each DQN output in our model corresponds to a specific target, we can say that if DQNi provides q values with respect to objective oi, then that DQNi priority has the same priority i as objective oi. DQNs with priorities we can be suppressed to form a hierarchy."}, {"heading": "3.3 Objectives, goals and rewards", "text": "A multi-objective agent receives rewards in relation to each target. An agent's objectives can be derived from some of the behaviours presented by the agent. Here, we will consider how a complex agent's behaviour can be broken down into several objectives. First, we should think about how to define the agent's objectives. For example, we may want a mobile robot to behave in such a way that it automatically charges its batteries and moves without colliding with other objects in the vicinity. Thus, the robot can have two objectives: a) to search for energy sources and b) to avoid collisions with obstacles.In the standard situation, the robot consumes energy for movement and performing other tasks. The state in which the robot is charged is attractive to the robot as it can achieve a goal in this state (a). On the contrary, colliding with an obstacle is a state that the robot should avoid by being in this state, clearly realizing a negative state."}, {"heading": "3.4 Learning", "text": "In our solution we use Deep Q-Networks to approximate the values of the Qfunctions. Following the state of the art in this area, a DQN provides the approximate function Q (s, a; \u03b8) in which we represent the approximate parameters of the neural network. However, as in our model we use several DQNs, there is a function Qi (s, j) = E (s, i) for a DQNi in relation to objective oi. Each DQNi is iteratively optimized, using the following reward function for each iteration j, but there is a function Qi (s, j) = E (s, i) for a DQNi in relation to objective oi. Each DQNi is iteratively optimized, using the following reward function for each iteration j, j (j, j, j) there is a function Qi (s, j) = E (s, ri, s)."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Eater - a 2D game-like virtual environment", "text": "In fact, most of us are able to move to another world in which we are able to live, in which we want to live."}, {"heading": "4.2 mDQNS implementation", "text": "Our implementation of the mDQNS was based on a DQN implementation for TensorFlow [16] of [17]. We extended the standard DQN with additional suppression outputs and mechanisms for stacking several such extended DQNs into one mDQNS. Each single DQN in an mDQNS consists of a folding network with three folding layers and no pooling layers, followed by a fully bonded layer and the output layer. The suppression output is a sigmoid layer connected to the fully bonded layer. The parameters of the folding network are based on the specification provided in [2], except for the size of the fully bonded layer, which in our experiments is 64, and the size of the input image, which in our case is 60x60x4. Our implementation has not been optimized for execution speed, but to make it less memory intensive."}, {"heading": "4.3 Experiments", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 Conclusions", "text": "In this paper, we introduced a method of using multiple Deep Q networks to address multi-objective problems. Furthermore, we introduced suppression signals in DQNs to create a hierarchy of multiple DQNs that is very similar to the subsumption architecture. In the experimental part, we demonstrated that mDQNS is capable of successfully learning different behaviors related to specific objectives. The results of our experiments are promising and justify the further development of our approach. The solution presented has several advantages that are useful in practical applications: it can learn multiple behaviors at once based on high-grade visual inputs; learned behaviors occur after learning DQNs that are responsible for manifesting certain sub-behaviors."}], "references": [{"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature 518(7540)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "1st edn. MIT Press, Cambridge, MA, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Multiobjective reinforcement learning: A comprehensive overview", "author": ["C. Liu", "X. Xu", "D. Hu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems 45(3)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalarized multi-objective reinforcement learning: Novel design techniques", "author": ["K.V. Moffaert", "M.M. Drugan", "A. Now"], "venue": "2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL).", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning all optimal policies with multiple criteria", "author": ["L. Barrett", "S. Narayanan"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908, New York, NY, USA, ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-objective reinforcement learning using sets of pareto dominating policies", "author": ["K.V. Moffaert", "A. Now\u00e9"], "venue": "Journal of Machine Learning Research 15", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple-goal reinforcement learning with modular sarsa(o)", "author": ["N. Sprague", "D. Ballard"], "venue": "Proceedings of the 18th International Joint Conference on Artificial Intelligence. IJCAI\u201903, San Francisco, CA, USA, Morgan Kaufmann Publishers Inc.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Parallel reinforcement learning using multiple reward signals", "author": ["P. Raicevic"], "venue": "Neurocomputing 69(1618) (2006) 2171 \u2013 2179 Brain Inspired Cognitive SystemsSelected papers from the 1st International Conference on Brain Inspired Cognitive Systems (BICS 2004)1st International Conference on Brain Inspired Cognitive Systems", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-objectivization of reinforcement learning problems by reward shaping", "author": ["T. Brys", "A. Harutyunyan", "P. Vrancx", "M.E. Taylor", "D. Kudenko", "A. Nowe"], "venue": "2014 International Joint Conference on Neural Networks (IJCNN).", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-objective deep reinforcement learning", "author": ["H. Mossalam", "Y.M. Assael", "D.M. Roijers", "S. Whiteson"], "venue": "CoRR abs/1610.02707", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Understanding Intelligence", "author": ["R. Pfeifer", "C. Scheier"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "A robust layered control system for a mobile robot", "author": ["R. Brooks"], "venue": "IEEE Journal on Robotics and Automation 2(1)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1986}, {"title": "Q learning behavior on autonomous navigation of physical robot", "author": ["H. Wicaksono"], "venue": "2011 8th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI).", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Behavior hierarchy learning in a behavior-based system using reinforcement learning", "author": ["A.M. Farahmand", "M.N. Ahmadabadi", "B.N. Araabi"], "venue": "2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566). Volume 2.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems (2015) Software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Many recent works on Reinforcement Learning focus on single-objective methods such as Deep Q-learning [1,2].", "startOffset": 102, "endOffset": 107}, {"referenceID": 1, "context": "Many recent works on Reinforcement Learning focus on single-objective methods such as Deep Q-learning [1,2].", "startOffset": 102, "endOffset": 107}, {"referenceID": 2, "context": "Methods such as Q-learning should converge to optimal policies [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "However in case of multiobjective problems, there may be many such optimal policies depending on the trade-offs between satisfying particular objectives [4].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "The simplest single-policy method uses a scalarization function [5], which converts multiple objectives into a single objective.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "Some techniques assign linear priorities to objectives [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Their aim is to approximate the Pareto front of policies [4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 6, "context": "In multi-policy methods, the preference of objectives does not need to be set a priori as a Pareto optimal policy for any preference may be obtained at runtime [7].", "startOffset": 160, "endOffset": 163}, {"referenceID": 7, "context": "A natural approach in MORL is to use separate learning modules for each objective [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 8, "context": "are to some extent independent [9]; modularity may be required for providing features desired in practical applications that were listed earlier.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "Some works deal with transforming complex single-objective problems to many simpler objectives [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "Very recently authors of [11] proposed a multi-policy learning framework that utilizes Deep QNetworks.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "In embodied artificial intelligence, the idea of parallel, loosely coupled processes [12] is proposed as a principle for designing embodied agents.", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "The subsumption architecture [13] is a very successful realization of the principle of parallel, loosely coupled processes.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "However, several methods for utilizing RL for behaviour learning were proposed [14,15].", "startOffset": 79, "endOffset": 86}, {"referenceID": 14, "context": "However, several methods for utilizing RL for behaviour learning were proposed [14,15].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "The goal of the agent is to maximize the expected discounted reward Rt = \u2211\u221e k=0 \u03b3 rt+k, where \u03b3 \u2208 [0, 1] is the discount factor.", "startOffset": 98, "endOffset": 104}, {"referenceID": 1, "context": "Deep Q-Networks [2] may be used used with high-level visual inputs such as those provided by video games.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "The most common method for selecting a single action is scalarization [5], which uses a weight vector w to retrieve a single scalar from vector at.", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "If we want them to represent votes for particular actions, each qi vector needs to be rescaled to [0, 1] \u2286 IR, where the min(qi) is mapped to 0 and max(qi) to 1.", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "The suppression signal S \u2208 [0, 1] \u2286 IR is a value that is used as a weight for summing the q-vectors.", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "As introduced in [1], there are in fact two neural networks involved in the learning process of a single DQN.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "We may perceive the suppression value as if it was a state-value function [3], returning the value of the state s under policy \u03c0:", "startOffset": 74, "endOffset": 77}, {"referenceID": 15, "context": "Our implementation of the mDQNS was based on a DQN implementation for TensorFlow[16] provided by [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 1, "context": "The parameters of the convolution network were based on the specification provided in [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "2-objective multiple DQN without subsumption (2-mDQNn) and 2-objective multiple DQN with subsumption (2-mDQNS), using hyperparameters resembling those in [2] and presented in Table 1.", "startOffset": 154, "endOffset": 157}], "year": 2017, "abstractText": "In this work we present a method for using Deep Q-Networks (DQNs) in multi-objective tasks. Deep Q-Networks provide remarkable performance in single objective tasks learning from high-level visual perception. However, in many scenarios (e.g in robotics), the agent needs to pursue multiple objectives simultaneously. We propose an architecture in which separate DQNs are used to control the agent\u2019s behaviour with respect to particular objectives. In this architecture we use signal suppression, known from the (Brooks) subsumption architecture, to combine outputs of several DQNs into a single action. Our architecture enables the decomposition of the agent\u2019s behaviour into controllable and replaceable sub-behaviours learned by distinct modules. To evaluate our solution we used a game-like simulator in which an agent provided with high-level visual input pursues multiple objectives in a 2D world. Our solution provides benefits of modularity, while its performance is comparable to the monolithic approach.", "creator": "LaTeX with hyperref package"}}}