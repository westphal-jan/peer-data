{"id": "1704.06731", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Batch-Expansion Training: An Efficient Optimization Paradigm for Machine Learning", "abstract": "We propose Batch-Expansion Training (BET), a framework for running a batch optimizer on a gradually expanding dataset. As opposed to stochastic approaches, batches do not need to be resampled i.i.d. at every iteration, thus making BET more resource efficient in a distributed setting, and when disk-access is constrained. Moreover, BET can be easily paired with most batch optimizers, does not require any parameter-tuning, and compares favorably to existing stochastic and batch methods. We show that when the batch size grows exponentially with the number of outer iterations, BET achieves optimal $\\tilde{O}(1/\\epsilon)$ data-access convergence rate for strongly convex objectives.", "histories": [["v1", "Sat, 22 Apr 2017 01:26:11 GMT  (169kb)", "http://arxiv.org/abs/1704.06731v1", null], ["v2", "Sun, 15 Oct 2017 22:19:28 GMT  (119kb)", "http://arxiv.org/abs/1704.06731v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michal derezinski", "dhruv mahajan", "s sathiya keerthi", "s v n vishwanathan", "markus weimer"], "accepted": false, "id": "1704.06731"}, "pdf": {"name": "1704.06731.pdf", "metadata": {"source": "META", "title": "Batch-Expansion Training: An Efficient Optimization Paradigm for Machine Learning", "authors": ["Micha\u0142 Derezi\u0144ski"], "emails": ["MDEREZIN@UCSC.EDU", "DHRUVM@FB.COM", "KEERTHI@MICROSOFT.COM", "VISHY@UCSC.EDU", "MWEIMER@MICROSOFT.COM"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.06 731v 1 [csa framework for running a batch optimizer on a gradually growing dataset. Unlike stochastic approaches, batches do not need to be re-sampled with each iteration, making BET more resource efficient in a distributed environment and limiting disk access. In addition, BET can easily be paired with most batch optimizers, requires no parameter setting, and compares favorably with existing stochastic and batch methods. We show that when batch size exponentially grows with the number of external iterations, BET achieves an optimal convergence rate of data access for highly convex targets."}, {"heading": "1. Introduction", "text": "In this context, it should be noted that this project is a project, which is a project, which is primarily a project, which aims to put the needs of people at the centre of society."}, {"heading": "2. Related Work", "text": "Algorithms such as the Stochastic Variance Reduced Gradient Method (Johnson & Zhang, 2013) and related approaches (Schmidt et al., 2013; Defazio et al., 2014) mix SGD-like steps with some batch calculations to control stochastic noise. Others have suggested paralleling stochastic training with large mini-batches (Li et al., 2014; Dekel et al., 2012), but these methods do not address the problems of calculation and data availability delays we discussed above. Furthermore, two-step approaches have been proposed (Agarwal et al., 2014; Shalev-Shwartz & Zhang, 2013) that start with SGD optimizers and then provide a batch optimizer (e.g. L-BFGS)."}, {"heading": "3. Batch-Expansion Training", "text": "Considering a dataset Z = {(xi, yi)} Ni = 1, we strive to approximate minimize the average regulated loss. < w,? (xi) >, yi) the loss of the prediction with a linear model < w,? (xi) > is against a target label yi. Any iterative optimization algorithm in this setting will produce a sequence of models {wt,? Tt = 1 with the goal that wT has a small optimization error g (wT) with respect to the exact optimum w < w (xi) > against a target label yi."}, {"heading": "3.1. Linear convergence of the batch optimizer", "text": "In this setting, many popular batch optimization algorithms enjoy a linear convergence rate (Nocedal & Wright, 2006), because, given any model w and any c > 1 iterations by a maximum of O (log (c)) iterations - where a single iteration can view the entire dataset - we can reduce the optimization error by a multiplier factor of c, with only a constant number of batch iterations required to achieve a linear convergence of optimization methods. Note that the runtime of a single iteration depends on the data size N, but not the number of iterations needed. If we set c = 2, we find that only a constant number of batch iterations is needed for a linear convergence of optimization methods to halve the optimization error of w. This convergence depends on the convergence rate enjoyed by the method."}, {"heading": "3.2. Dataset size selection", "text": "The general task of an optimization algorithm is to return a model with small optimization errors."}, {"heading": "3.3. Exponentially increasing batches", "text": "Suppose our goal is to return a model with optimization errors that represents reasonable implementation practice in many web applications.wT = w Note: Each of the intermediate models (say, model wt for t < T) has a large optimization error in relation to w, so we can minimize an objective function with appropriately large estimation errors. Thus, we will first obtainw1 with optimization errors 2, which have a smaller error tolerance 2 < 1 and larger data size n2 > n1, so that we achieve a better model w2, etc., so that we end up with a simple optimization of this strategy."}, {"heading": "3.4. Two-track algorithm", "text": "How many iterations of the proposed enhancement procedure should be performed at each stage? From our high-level analysis, we conclude that an approximately constant number of updates should suffice for each stage (using a linear optimizer), because each time we seek the same multiplicative improvement in the optimization error that our model has suffered. However, this constant may depend on the type of loss function, the dataset, and the optimizer used, and, in practice, the correct number of iterations may vary to some extent between the stages. Therefore, we need a practical method to determine the right time for the duplication of the data. Consider the following experiment: We run two optimization tracks in parallel, one for the batch of the size nt, the other for half of this batch. For a slower step on the larger batch, two faster steps are performed on the smaller batch."}, {"heading": "3.5. Discussion", "text": "The choice to increase the data size in each phase by a factor of 2 (and not by another factor) is not decisive for the optimization performance (both theoretically and in practice), so this parameter does not require any adjustment. The initial partial quantity size n0 also does not affect the performance of the sig algorithm 2 Double-track optimization Initializew1.0 = w \u00b2 0.0 arbitrarily Choose any 2 \u2264 n1 = 2n0 < N Initialize s 0 and t \u00b2 1, while n < N dowt, s + 1 \u2190 Update (wt, s, nt) w \u00b2 t \u2212 1, s + 1 \u2190 Update (w \u00b2, n \u2212 1) s + 1, if f \u00b2 t (wt, s / 2) < N dowt, s + 1 \u2190 Update (w \u00b2 t, t \u2212 1, s) n \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t t \u00b2 t t \u00b2 t t \u00b2 t \u00b2 t t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t (t, n \u2212 1) s + 1, if f \u00b2 t t t t (wt, s / 2) < N dowt, s + 1 \u2190 Update (wt, s < N < N < N dowt, s dowt, s, s + 1 \u2190 Update) n \u00b2 t, s t, s t + 1 \u2190 Update (wt, s < N < N) n < N \u00b2 t; N \u00b2 t; N \u00b2 t; N) t t t t t \u00b2 t t t t t t \u00b2 t \u00b2 t t t t \u00b2 t t t t \u00b2 t t t t t \u00b2 t t t t t t t \u00b2 t t t t t t t t t t t t \u00b2 t t t t t t t t \u00b2 t t t t t t \u00b2 t t t t t t \u00b2 t t t t t t \u00b2 t t t t t t t \u00b2 t t t t t t \u00b2 t t \u00b2 t t t t t \u00b2 t t t t t t t \u00b2 t \u00b2 t t t t \u00b2 t t t t t t t t t t \u00b2 t t t t t t t t t t."}, {"heading": "4. Complexity Analysis", "text": "In this section we offer theoretical guarantees for the time complexity of the batch expansion training and compare it with other approaches. For the rest of this section we assume that the internal optimizer for any area inconvergence between \u0445 > 1 and foreach t, w has a linear convergence: g-t (Update (w, nt) \u2264 (1 \u2212 (1 / \u0445)) g-t (w)."}, {"heading": "4.1. Data-Access Complexity", "text": "For the sake of complexity analysis, we discuss a parameterized variant of our approach, described in algorithm 3, and determine complexity results for it. Here, the number of updates required in each phase is a fixed parameter. \u2212 Algorithm 3 Optimal BET Input: Target tolerance: 0, n0 Initializew0 \u2190 0, t \u2190 0 and 0 and 0 and 0 and 0 = Log (6) while 3 \u00b7 Experience values are not + 1 \u2190 2nt wt, 0 \u2190 wt for s = 1... \u00b7 WE do wt, s \u2190 Update (wt, s \u2212 1, nt + 1) End for wt + 1. \u2212 WE expect appendices t / 2 t + 1end, while T \u00b7 t wTAssuptions are better. We assume that the feature mapping of this data (\u00b7) (see (1) and the discussion below) is B-limited, i.e. it does not satisfy an optimization rate."}, {"heading": "4.2. Time Complexity", "text": "We will now discuss a simple model of time complexity for BET that includes such aspects as hardware acceleration and data loading. (Our goal is to highlight certain systems in the computational architecture that allow a meaningful comparison of stochastic and batch methods.) We compare BET with three other paradigms for operating an internal batch optimizer. (Note: the internal optimizer is used as a regular batch method for the entire datasette.2. Dynamic Sample Method (DSM): Applying stochastic sampling with dynamically increasing sample sizes, proposed in Byrd et al., 2012), 3. Mini-Batch: Applying stochastic sampling with fixed-size mini-batches b (Li et al., 2014). First, we compare data access complexity of the methods with their sample complexity N (number of unique data points needed to achieve a generalization error)."}, {"heading": "5. Experiments", "text": "In this section, we present experimental results using the theoretical time complexity analysis from Section 4. As an optimization problem, we use square hinges loss for SVM with 2-standard regularization trained on multiple standard LIBSVM datasets (see Table 2.) The regularization parameter for each dataset was selected by tuning to achieve the highest accuracy of the test sets. All algorithms start with the original model vector w, which is set on all zeros. BET was implemented as in Algorithm 2. The initial subset n0 was tested in the range between 100 and 2000 data points, with minimal impact on performance. As the main internal optimizer for BET, we used sub-sampled Newton-CG (SN) (Byrd et al., 2011). Additional experiments in Appendix A.1 of (Authors, 2017) amplify how our sample sampleter algorithm behaves with samplexed sampleleleeters when not configured with Reejuxed sampleeters."}, {"heading": "5.1. Optimization Time", "text": "In this section, we evaluate algorithms that use the time complexity model described in Section 4.2. We present the results in the form of Log Relative Functional Value Differential og RFVD: log ((f (w) \u2212 f (w))) / f (w). (6) Figure 2 records the simulated runtime at a = 1, p = 10 and s = 5, based on the system characteristics of a standard hardware setup. As expected from the time complexity analysis, DSM and Adagrad pay a significant cost benefit to restore the data at each iteration. We also show that wall clock times leading to achieving test setting accuracy of 0.985 (within 1% of the optimum) and 0.995 (within 0.05% of the optimum) for BET, DSM and Batch, when executed on the same platform, result in a reduction in computation time."}, {"heading": "5.2. Parallel Experiments", "text": "In this section, we present preliminary results for parallel experiments comparing batch expansion training with the standard batch strategy, using L-BFGS as an internal optimizer. Both methods were implemented within the framework of PETSc (Balay et al., 2016a; b; 1997), splitting the data between multiple computing cores. Figure 5 shows the runtime results with respect to the training goal to optimize the regulated logistical loss in the SUSY dataset. Dashed lines correspond to sequential experiments, whereas solid lines describe convergence when two cores perform parallel computations. In this experiment, BET achieved an acceleration of 1.84x, while Batch achieved an improvement of 1.78x. In addition, we can see that the advantage of BET over batch increases at large sample sizes (4 million instances), following the asymptotic analysis in Section 4.2."}, {"heading": "5.3. Test Set Accuracy", "text": "Figure 6 shows the accuracy of the test set over the simulated runtime under the same time complexity model as Figure 2. We observe similar results because BET performs better than other methods. On the diagrams, we mark the point in time when BET reaches the full dataset. In most cases, the algorithm reaches the optimum test accuracy at that point. Therefore, we can end optimization in many practical applications by relying on this simple stop criterion. In addition, in problems where the full dataset is abundant, BET will achieve optimal performance long before the full dataset size is reached."}, {"heading": "6. Conclusions", "text": "We suggested Batch Expansion Training, a simple, parameter-free method of running batch optimizers in a way that competes both theoretically and experimentally with stochastic approaches. BET does not require coordination and can be paired with different optimizers while offering advantages in parallel and distributed settings."}, {"heading": "A. Additional Experiments", "text": "The question which arises is whether this is a way in which the people of the USA, Europe, Europe, the USA, the USA, the EU, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the"}, {"heading": "B. Proof Details for Theorem 4.1", "text": "B. 1. Proof of Theorem 4.1Recall that we are the approximation error estimation at stage t (w), f."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Agarwal", "Alekh", "Chapelle", "Olivier", "Dud\u0131\u0301k", "Miroslav", "Langford", "John"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "PETSc Web page. http:// www.mcs.anl.gov/petsc, 2016a. URL http:// www.mcs.anl.gov/petsc", "author": ["Hong", "Zhang"], "venue": null, "citeRegEx": "Hong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2016}, {"title": "PETSc users manual. Technical Report ANL-95/11 - Revision 3.7, Argonne National Laboratory, 2016b", "author": ["Hong", "Zhang"], "venue": "URL http://www.mcs. anl.gov/petsc", "citeRegEx": "Hong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2016}, {"title": "ISSN 1532-4435", "author": ["July"], "venue": "URL http://dl.acm.", "citeRegEx": "July,? 2011", "shortCiteRegEx": "July", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Additionally, two-stage approaches have been proposed (Agarwal et al., 2014; Shalev-Shwartz & Zhang, 2013), which employ SGD at the beginning followed by a batch optimizer (e.", "startOffset": 54, "endOffset": 106}], "year": 2017, "abstractText": "We propose Batch-Expansion Training (BET), a framework for running a batch optimizer on a gradually expanding dataset. As opposed to stochastic approaches, batches do not need to be resampled i.i.d. at every iteration, thus making BET more resource efficient in a distributed setting, and when disk-access is constrained. Moreover, BET can be easily paired with most batch optimizers, does not require any parameter-tuning, and compares favorably to existing stochastic and batch methods. We show that when the batch size grows exponentially with the number of outer iterations, BET achieves optimal \u00d5(1/\u01eb) data-access convergence rate for strongly convex objectives.", "creator": "LaTeX with hyperref package"}}}