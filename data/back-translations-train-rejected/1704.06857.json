{"id": "1704.06857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation", "abstract": "Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.", "histories": [["v1", "Sat, 22 Apr 2017 23:37:43 GMT  (8639kb,D)", "http://arxiv.org/abs/1704.06857v1", "Submitted to TPAMI on Apr. 22, 2017"]], "COMMENTS": "Submitted to TPAMI on Apr. 22, 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["alberto garcia-garcia", "sergio orts-escolano", "sergiu oprea", "victor villena-martinez", "jose garcia-rodriguez"], "accepted": false, "id": "1704.06857"}, "pdf": {"name": "1704.06857.pdf", "metadata": {"source": "CRF", "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation", "authors": ["A. Garcia-Garcia", "S. Orts-Escolano", "S.O. Oprea", "V. Villena-Martinez", "J. Garcia-Rodriguez"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Semantic Segmentation, Deep Learning, Scene Labeling, Object SegmentationF"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in fact, are able"}, {"heading": "2 TERMINOLOGY AND BACKGROUND CONCEPTS", "text": "To properly understand how semantic segmentation is tackled by modern deep-learning architectures, it is important to know that it is not an isolated field, but rather a natural step in the progression from coarse to fine inference. The origin may lie in classification, which consists of making a prediction for an entire input, i.e. predicting the object in an image, or even generating a ranking list when there are many of them. Localization or detection is the next step toward fine-grained inference, which provides not only the classes, but also additional information about the spatial location of those classes, e.g. centrifuges or bottom boxes. Provided that semantic segmentation is the natural step to achieve fine-grained inference Labels Labels-Condensed predictions for each pixel label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label-label"}, {"heading": "2.1 Common Deep Network Architectures", "text": "As we have already mentioned, certain deep networks have made such significant contributions to this area that they have become widely known standards. It is the case of AlexNet, VGG-16, GoogLeNet and ResNet. Their importance has been so great that they are currently used as building blocks for many segmentation architectures, which is why we will devote this section to their review."}, {"heading": "2.1.1 AlexNet", "text": "AlexNet was the ground-breaking deep CNN that won the ILSVRC-2012 with a TOP-5 test accuracy of 84.6%, while the nearest competitor, which used traditional techniques instead of deep architectures, achieved an accuracy of 73.8% in the same challenge. Krizhevsky et al. [14] \"s architecture was relatively simple, consisting of five revolutionary layers, max-pooling layers, non-linear rectified units (ReLUs), three fully connected layers, and suspensions. Figure 2 shows that CNN architecture works."}, {"heading": "2.1.2 VGG", "text": "The Visual Geometry Group (VGG) is a CNN model introduced by the Visual Geometry Group (VGG) of the University of Oxford. They proposed various models and configurations of deep CNNs [15], one of which was submitted to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) -2013. This model, also known as VGG-16 due to the fact that it consists of 16 weight layers, became popular thanks to its achievement of 92.7% TOP-5 test accuracy. Figure 3 shows the configuration of VGG-16. The main difference between VGG-16 and its predecessors is the use of a stack of conversion layers with small receptive fields in the first layers instead of some layers with big3 receptive fields. This results in fewer parameters and more non-linearity in between, making the decision function more differentiated and the model easier to train."}, {"heading": "2.1.3 GoogLeNet", "text": "GoogLeNet is a network introduced by Szegedy et al. [16] that won the ILSVRC 2014 challenge with a test accuracy of 93.3%. This CNN architecture is characterized by its complexity, underscored by the fact that it consists of 22 layers and a newly introduced building block called the Incention Module (see Figure 4).This new approach proved that CNN layers can be stacked in more than one typical sequential way. In fact, these modules consist of a Network-in-Network Layer (NiN), a pooling operation, a large-area folding layer and a small-area folding layer. All layers are calculated in parallel and followed by 1 \u00d7 1 folding operations to reduce dimensionality. Thanks to these modules, this network takes into account special storage and computing costs by significantly reducing the number of parameters and operations."}, {"heading": "2.1.4 ResNet", "text": "Microsoft's ResNet [17] is particularly noteworthy because it won ILSVRC-2016 with an accuracy of 96.4%. Apart from that, the network is well known for its depth (152 layers) and introduction of residual blocks (see Figure 5).The residual blocks address the problem of training a really deep architecture by introducing identity skip links so that layers can copy their input to the next layer. The intuitive idea behind this approach is that it ensures that the next layer learns something new and different than the input has already encoded (since it contains both the output of the previous layer and its unchanged input)."}, {"heading": "2.1.5 ReNet", "text": "In order to extend the architecture of Recurrent Neural Networks (RNNs) to multi-dimensional tasks, Graves et al. [18] proposed a multi-dimensional Recurrent Neural Network (MDRNN) architecture that replaces every single recursive connection of standard RNNNs with d-connections, where d is the number of spatio-temporal data dimensions. Based on this initial approach, Visin el. [19] proposed a ReNet architecture that used sequence RNNNs instead of multidimensional RNNs. Thus, the number of RNNNs scales linearly on each level with respect to the number of dimensions d of the input image (2d). In this approach, each revolutionary layer (convolution + pooling) is replaced by four RNNNNs that spring the image vertically and horizontally in both directions, as we can see in Figure 6.4."}, {"heading": "2.2 Transfer Learning", "text": "Rebuilding a deep neural network from scratch is often impractical for a number of reasons: A sufficiently large data set is required (and usually not available), and achieving convergence can take too long to pay off. Even if a sufficiently large data set is available and convergence does not take that long, it is often helpful to start with pre-trained weights rather than randomly initialized weights [20] [21]. Fine-tuning the weights of a pre-trained network by continuing the training process is one of the most important transfer learning scenarios. Yosinski et al. [22] has proven that transferring features even from remote tasks can be better than using random initialization, bearing in mind that the transferability of features decreases as the difference between the pre-trained task and the target increases. However, applying this transfer learning method is not entirely straightforward, as it is a reassessment of the weighting."}, {"heading": "2.3 Data Preprocessing and Augmentation", "text": "Data augmentation is a common technique that has been shown to benefit the training of machine learning models in general and deep architectures in particular; it either accelerates convergence or acts as a regulator, thereby avoiding over-adaptation and increasing generalization capabilities [25]. Typically, it consists of applying a series of transformations to either data or feature spaces, or even both. The most common augmentations are performed in data space. This type of augmentation generates new samples by applying transformations to existing data. There are many transformations that can be applied: translation, rotation, distortion, scaling, farmspace shifts, harvests, etc. The goal of these transformations is to generate more samples in order to generate a larger dataset, to prevent overhauling and presumably regulation of the model, to balance the classes within this database, and even to synthesize 1,500 samples per task, which are particularly representative of the 1,500 or for each task."}, {"heading": "3 DATASETS AND CHALLENGES", "text": "Two types of readers are expected for this type of review: Either they initiate themselves into the problem, or they are experienced enough, and they are just looking for the recent advances made by other researchers in recent years. Although the second type usually knows two of the most important aspects to know before starting to investigate this problem, it is crucial for newcomers to gain an understanding of what the highest-quality data sets and challenges are. Therefore, the purpose of this section is to start new scientists by providing them with a brief summary of data sets that might suit their needs, as well as data augmentation and pre-processing tips. Nevertheless, it can also be useful for hardened researchers who want to review the basics or perhaps discover new information, data is one of the most - if not the most important part of any machine learning system."}, {"heading": "3.1 2D Datasets", "text": "Over the years, semantic segmentation has been focused primarily on two-dimensional images, which is why 2D datasets are the most common. In this section, we describe the most popular large-scale 2D semantic segmentation datasets, where this challenge includes any type of two-dimensional representation such as gray scale or red green blue (RGB) images. \u2022 PASCAL Visual Object Classes (VOC) 1: This challenge consists of a down-to-earth database containing an annotated dataset of images and five different competitions: classification, detection, segmentation, action classification and person layout. Segmentation is particularly interesting because its goal is to predict the object class of each pixel for each test image. There are 21 classes that are categorized into vehicles, pets and other vehicles: aeroplanes, bicycles, boats, motorcycles, bottles, chairs, tables, sofas, sofas, dogs, sofas tables, and other vehicles."}, {"heading": "3.2 2.5D Datasets", "text": "In fact, most of them will be able to put themselves in the world they live in, and they will be able to change the world, \"he said in an interview with The New York Times:\" I don't think they will be able to change the world. \""}, {"heading": "3.3 3D Datasets", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "4 METHODS", "text": "In this context, it should be noted that this is a very complex matter, which is a very complex, complex, complex and complex matter."}, {"heading": "4.1 Decoder Variants", "text": "Apart from the FCN architecture, other variants have been developed to transform a network whose purpose was classification to make it suitable for segmentation. FCN-based architectures are arguably more popular and successful, but other alternatives are also noteworthy. In general, they all take a network for classification, such as VGG16, and remove its fully connected layers. This part of the new segmentation network is often called encoder and generates low-resolution images or feature maps. The problem is that you learn to pixel-point these low-resolution images to decode them for segmentation. This part is called decoder and is usually the divergence point in this type of architecture. SegNet [66] is a clear example of this divergence (see Figure 9). SegNet's decoder level is composed by a series of upsampling and convolution layers followed by a soft-max classification."}, {"heading": "4.2 Integrating Context Knowledge", "text": "Semantic segmentation is a problem that requires the integration of information from different spatial scales. It also implies balancing local and global information. On the one hand, fine-grained or local information is crucial to achieve good accuracy at the pixel level. On the other hand, it is also important to integrate information from the global context of the image to resolve local ambiguities. Fig.10: Comparison of SegNet (left) and FCN decoders (right). While SegNet uses indices from the corresponding encoder stage for upsample, FCN learns deconvolution filters for upsample (addition of the corresponding function card from the encoder level). Figure [66]. Vanilla CNNs struggle with this equilibrium. Pooling layers that allow networks to achieve a certain degree of spatial inventory and keep computing costs within limits, dispose of global context information. Even pure CNs without NCRF layers, the number of CNF layers are limited as they can only be filled in CNF layers."}, {"heading": "4.2.1 Conditional Random Fields", "text": "As we have already mentioned, the inherent invariance of spatial transformations of CNN architectures is limited to the same spatial accuracy for segmentation tasks. A possible and common approach to refining the output of a segmentation system and its ability to capture fine-grained details is to apply a conditional edge field (CRF) processing level. CRFs enable the combination of low image information - such as pixel interactions [92] - with the production of inference systems per pixel. This combination is especially important to capture long-lasting dependencies that CNNs do not take into account, and to ensure fine local detail. DeepLab models make use of the fully networked systems that lead to class outcomes."}, {"heading": "4.2.2 Dilated Convolutions", "text": "Extended coils, also known as \"trous convolutions,\" are a generalization of Kronecker-factored convolutional filters [96] that support exponentially expanding receptive fields without losing resolution. In other words, extended coils are regular coils that use upsampling filters. Dilation rate l controls this upsampling factor. As shown in Figure 12, stacking L-dilated convolution causes the receptive fields to grow exponentially, while the number of parameters for the filters exhibits linear growth. This means that dilated convolutions allow efficient extraction of dense characteristics at any resolution. As a side note, it is important to mention that typical coils only have 1-dilated convolutions.In practice, it is equivalent to expand the filter before the usual convolution is performed. This means that its size corresponds to the dilation rate when the filters are larger than the filters are filled with 69 words."}, {"heading": "4.2.3 Multi-scale Prediction", "text": "Another way to deal with the integration of contextual knowledge is to use multi-scale predictions. Almost every single parameter of a CNN affects the scale of the function cards generated. In other words, exactly the same architecture will have an impact on the number of pixels of the input image corresponding to one pixel of the function card. This means that the filters implicitly learn to detect features at specific scales (presumably with a certain degree of inventory). Furthermore, these parameters are usually closely linked to the problem, making it difficult for models to generalize to different scales. One possible way to overcome this obstacle is to use multi-scale networks, which generally target multiple scales and then merge the predictions to generate a single output.Raj et al. [73] suggest a multi-scale version of a fully evolutionary network VGG-16. This network has two ways, one that processes the input in the original resolution and the other that duplicates it."}, {"heading": "4.2.4 Feature Fusion", "text": "Another way to add context information to a fully revolutionary architecture for segmentation is to merge a global feature (extracted from a previous layer in a network) with a more local feature map extracted from a subsequent layer. Common architectures such as the original FCN use skip connections to perform a late fusion by combining the feature maps from different layers (see Figure 15). Another approach is to perform an early fusion. This approach is followed by ParseNet [77] in its context module. The global feature is split to the same spatial size as the local feature and then linked together to generate a combined feature used in the next layer or to learn a classifier. Figure 16 shows a representation of this process. Figure 15: Skip-connection-like architecture, which merges a late fusion of a feature into context, which is performed as a separate feature maps for each previous pin formation. Figure 16 shows a representation of this process."}, {"heading": "4.2.5 Recurrent Neural Networks", "text": "In fact, it is such that the majority of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a"}, {"heading": "4.3 Instance Segmentation", "text": "Instance segmentation is considered the next step after semantic segmentation and at the same time the most difficult problem compared to the rest of the low-level pixel segmentation techniques. Its main purpose is to represent objects of the same class that are divided into different instances. Automating this process is not easy, so the number of instances is initially unknown and evaluating the predictions made is not pixel by pixel as in semantic segmentation. Consequently, this problem remains partially unsolved, but the interest in this field is motivated by its potential applicability. Instance labeling provides us with additional information for arguing about occlusion situations, including the number of elements that belong to the same class and for detecting a specific object for robotics tasks, among many other fields of application. To this end, Hariharan et al. [10] proposed a Simultaneous Detection and Segmentation (SDS) method to improve performance over existing works."}, {"heading": "4.4 RGB-D Data", "text": "As we noted, a significant amount of work has been done in semantic segmentation through the use of photometric data. Nevertheless, the use of structural information has been spurred by the advent of low-cost RGB-D sensors that provide useful geometric clues extracted from depth information. Several papers that focused on RGB-D scene segmentation reported an improvement in fine-grained marking through the use of depth information and not just photometric data. Depth information is used due to the unpredictable variation in scene illumination alongside incomplete objects due to complex events. However, several papers have successfully used depth information to increase accuracy. The use of depth images with approaches to photometric data is not easy."}, {"heading": "4.5 3D Data", "text": "3D geometric data such as dot clouds or polygonal meshes are useful representations thanks to their added dimension, which provides methods with rich spatial information that are intuitively useful for segmentation. However, the vast majority of successful deep learning segmentation architectures - especially CNNs - are not originally designed to deal with unstructured or disordered inputs such as the above. To enable weight-sharing 15 and other optimizations in revolutionary architectures, most researchers have resorted to 3D voxel grids or projections to transform unstructured and disordered dot clouds or meshes into regular representations before they are fed into the networks. For example, Huang et al. [86] (see Figure 19) take a dot cloud and analyze it through a dense voxel grid that generates a series of occupancy voxels that generate a series of vox grids that are used as a 3D structure."}, {"heading": "4.6 Video Sequences", "text": "As we have seen, there has been significant progress in frame-by-frame segmentation. However, when dealing with image sequences, many systems rely on the application of these same algorithms in a frame-by-frame manner. This approach works and often yields remarkable results. However, applying these methods frame-by-frame is usually impractical, as they completely ignore the temporal continuity and coherence of cubes, which could help increase the accuracy of the system while reducing its execution time. The most notable work in this regard is the FCN clockwork by Shelter et al."}, {"heading": "5 DISCUSSION", "text": "In the previous section, we reviewed the existing methods from a literary and qualitative point of view, i.e. we did not consider a quantitative result. In this section, we will discuss the same methods from a numerical point of view. First, we will describe the most popular evaluation metrics that can be used to measure the performance of semantic segmentation systems under three aspects: execution time, memory requirements and accuracy. 16 Figure 20: The PointNet unit architecture for point cloud classification and segmentation. Figure from [87]. Next, we will collect the results of the methods on the most representative data sets using the metrics described above. Afterwards, we will summarize these results and draw conclusions. Finally, we will list possible future research lines that we consider relevant to the field."}, {"heading": "5.1 Evaluation Metrics", "text": "For a segmentation system to be useful and indeed make a significant contribution to the field, its performance needs to be evaluated with care. In addition, this evaluation needs to be based on common and known metrics that allow fair comparisons with existing methods. In addition, many aspects need to be evaluated to confirm the validity and usefulness of a system: execution time, memory requirements and accuracy. Depending on the purpose or context of the system, some metrics may be more important than others, i.e., accuracy may be dispensable to a certain point in favor of execution speed for a real-time application. Nevertheless, for reasons of scientific rigor, it is of utmost importance to provide all possible metrics for a proposed method."}, {"heading": "5.1.1 Execution Time", "text": "Speed or runtime is an extremely valuable metric, as the vast majority of systems have to meet tough requirements on how much time they can spend on the inference pass. In some cases, it may be useful to know the time needed to train the system, but it is usually not that important unless it is excessively slow, as it is an offline process. In any case, providing accurate timelines for the methods can be considered meaningless, as they are extremely dependent on the hardware and backend implementation, making some comparisons pointless. However, for reasons of reproducibility and to help other researchers, it is useful to provide timelines with a thorough description of the hardware on which the system was running, as well as the conditions for benchmarking. If done correctly, it can help others to assess whether the method is useful for application or not, as well as make fair comparisons under the same conditions to check which are the fastest methods.17"}, {"heading": "5.1.2 Memory Footprint", "text": "Memory usage is another important factor for segmentation methods. Although it may be less restrictive than execution time - memory scaling is usually possible - it can also be a limiting element. In some situations, such as robotic platform onboard chips, memory is not as abundant as a high-performance server. Even high-end graphics processing units (GPUs), which are often used to accelerate deep networks, do not pack as much memory. In this respect, and bearing in mind the same implementation-dependent aspects as runtime, documenting the peak and average storage requirements of a method with a full description of execution conditions can be extremely helpful."}, {"heading": "5.1.3 Accuracy", "text": "Many evaluation criteria have been suggested and are often used to judge the accuracy of any kind of semantic segmentation technique. \u2022 These metrics are usually variations of pixel accuracy and IoU. We give the most popular metrics for semantic segmentation that are currently used to measure how per-pixel markup methods perform this task. \u2022 To get the explanation, we mention the following notation details: We assume a total number of k + 1 classes (from L0 to Lk including a void class or background) and Pij is the number of pixels of class i derived to belong to class j. In other words, Pii represents the number of true positives, while Pij and Pji are usually interpreted as false positives or false negatives (although each of them can be the sum of both false positives and false negatives). \u2022 Pixel accuracy of the MPi-PA (total unit): It is the simplest ratio of the metric to the unit of the fictitious MPxels, giving a metric to the unit of the MPxels."}, {"heading": "5.2 Results", "text": "As we have already mentioned, Section 4 contains a functional description of the methods under review according to their objectives. Now, we have collected all the quantitative results for these methods, as indicated by their authors in their respective papers. These results are divided into three parts depending on the input data used by the methods: 2D RGB or 2.5D RGB D images, volumetric 3D images or video sequences. The most commonly used data sets were selected for this purpose. It is important to mention the heterogeneity of on-site work when reporting results. Although most of them try to evaluate their methods in standard data sets and provide enough information to reproduce their results, even in terms of well-known metrics, many others do not. This leads to a situation where it is difficult or even impossible to fairly compare methods. Furthermore, we came across the fact that few authors provide information on other metrics rather than on accuracy. Despite the importance of other metrics, most of the work do not include execution time or do not require the most of the data."}, {"heading": "5.2.1 RGB", "text": "For each 2D image category we have selected seven datasets: PASCAL VOC2012, PASCAL Context, PASCAL Person-Part, CamVid, CityScapes, Stanford Background, and SiftFlow. This selection takes into account a wide range of situations and objectives. The first and arguably most important dataset in which the vast majority of methods are evaluated is PASCAL VOC-2012. Table 3 shows the results of these verified methods that accurately deliver results on the PASCAL VOC-2012 test set. This set of results shows a clear improvement trend from the first proposed methods (SegNet18 and the original FCN) to the most complex models such as CRFasRNN and the winner (DeepLab) with 79.70 IoU.Apart from the well-known VOC dataset that we have collected, VOC datasets from VOC dataset of VOU.U4 and later on, we will consider the results of VASC4 in the OAL Dataset only (the results of VAS6AL). For each 2D image category we have selected seven datasets: PASCAL VOC2012, PASCAL Context, PASCAL Context, PASCAL Person-Part, PASCAL Person-Part, CamVid, CityScapes, Stanford Background, and SiftFlow. This selection takes into account a wide range of situations and objectives."}, {"heading": "5.2.2 2.5D", "text": "With regard to the 2.5D category, i.e. datasets that contain depth information in addition to the typical RGB channels, we have selected three of them for analysis: SUNRGB-D and NYUDv2. Table 10 shows the results for SUNRGB-D that are provided only by LSTM-CF that reach 48.10 IOU. Table 11 shows the results for NYUDv2 that are also exclusive to LSTM-CF. This method reaches 49.40 IOU. Finally, Table 12 collects results for the last 2.5D dataset: SUN-3D. Again, LSTM-CF is the only one to provide information for this database, in this case an accuracy of 58.50."}, {"heading": "5.2.3 3D", "text": "For this discussion, two 3D datasets were selected: ShapeNet Part and Stanford-2D-3D-S. In both cases, only one of the analyzed methods actually scored on them. Thus, PointNet reached 83.80 and 47.71 IoU respectively on ShapeNet Part (Table 13) and Stanford-2D-3D-S (Table 14).19"}, {"heading": "5.2.4 Sequences", "text": "The last category in this discussion is videos or sequences. For this part, we have collected results for two sets of data suitable for sequence segmentation: CityScapes and YouTube objects. Only one of the verified methods for video segmentation provides quantitative results for these data sets: Clockwork Convnet. This method reaches 64.40 IoU for CityScapes (Table 15) and 68.50 for YouTube objects (Table 16)."}, {"heading": "5.3 Summary", "text": "Considering the results, we can draw several conclusions, the most important of which is reproducibility. As we have observed, many methods report non-standard data sets or are not tested at all, making comparisons impossible. Furthermore, some of them do not describe the setup for the experiments or provide the source code for the implementation, thereby significantly impairing reproducibility. Methods should report their results on standard data sets, describe the training procedure comprehensively, and also make their models and weights publicly available in order to facilitate advance.Another important fact discovered thanks to this study is the lack of information on other metrics such as execution time and storage requirements. Almost no paper reports on this type of information, and those who suffer from the aforementioned reproducibility problems are empty due to the fact that most methods focus on accuracy without worrying about time or space. Nevertheless, it is important to think about where these methods are used."}, {"heading": "5.4 Future Research Directions", "text": "This year it is more than ever before."}, {"heading": "6 CONCLUSION", "text": "To the best of our knowledge, this is the first review in the literature to focus on semantic segmentation through deep learning. Compared to other surveys, this work is dedicated to such an emerging topic as deep learning and covers the most advanced and recent work on this topic. We formulated the semantic segmentation problem and provided the reader with the background knowledge needed about deep learning for such a task. We covered the contemporary literature of data sets and methods and provided a comprehensive overview of 28 data sets and 27 methods. Data sets were carefully described and classified according to their purposes and characteristics so that researchers can easily select the one that best suits their needs. Methods were considered from two perspectives: contributions and raw results, i.e. accuracy. Finally, we presented a comparative summary of data sets and methods in tabular forms and classified them according to various criteria. At the end, we discussed the results and provided useful insights into future research directions in this field and open problems."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was financed by the Spanish government TIN2016-76515-R for the project COMBAHO, which is supported by Feder. It was also supported by a Spanish national scholarship for doctoral studies FPU15 / 04516. In addition, it was financed by the scholarship Ayudas para Estudios de Ma \u0301 ster e Iniciacio \u0301 n a la Investigacio \u0301 n of the University of Alicante."}], "references": [{"title": "Segmentationbased urban traffic scene understanding.", "author": ["A. Ess", "T. M\u00fcller", "H. Grabner", "L.J. Van Gool"], "venue": "in BMVC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition, June 2012, pp. 3354\u20133361.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3213\u20133223.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Hands deep in deep learning for hand pose estimation", "author": ["M. Oberweger", "P. Wohlhart", "V. Lepetit"], "venue": "arXiv preprint arXiv:1502.06807, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a deep convolutional network for light-field image superresolution", "author": ["Y. Yoon", "H.-G. Jeon", "D. Yoo", "J.-Y. Lee", "I. So Kweon"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, 2015, pp. 24\u201332.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for content-based image retrieval: A comprehensive study", "author": ["J. Wan", "D. Wang", "S.C.H. Hoi", "P. Wu", "J. Zhu", "Y. Zhang", "J. Li"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 157\u2013166.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward automatic phenotyping of developing embryos from videos", "author": ["F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P.E. Barbano"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 9, pp. 1360\u20131371, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep neural networks segment neuronal membranes in electron microscopy images", "author": ["D. Ciresan", "A. Giusti", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2012, pp. 2843\u20132851.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1915\u2013 1929, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1915}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 297\u2013312.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning rich features from rgb-d images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbel\u00e1ez", "J. Malik"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 345\u2013360.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation", "author": ["H. Zhu", "F. Meng", "J. Cai", "S. Lu"], "venue": "Journal of Visual Communication and Image Representation, vol. 34, pp. 12 \u2013 27, 2016. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S1047320315002035", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A survey of semantic segmentation", "author": ["M. Thoma"], "venue": "CoRR, vol. abs/1602.06541, 2016. [Online]. Available: http://arxiv.org/abs/ 1602.06541", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Multidimensional recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "CoRR, vol. abs/0705.2011, 2007. [Online]. Available: http://arxiv.org/ abs/0705.2011  21", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Renet: A recurrent neural network based alternative to convolutional networks", "author": ["F. Visin", "K. Kastner", "K. Cho", "M. Matteucci", "A.C. Courville", "Y. Bengio"], "venue": "CoRR, vol. abs/1505.00393, 2015. [Online]. Available: http://arxiv.org/abs/1505.00393", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks", "author": ["A. Ahmed", "K. Yu", "W. Xu", "Y. Gong", "E. Xing"], "venue": "European Conference on Computer Vision. Springer, 2008, pp. 69\u201382.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 1717\u20131724.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "How transferable are features in deep neural networks?", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 248\u2013255.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding data augmentation for classification: when to warp?", "author": ["S.C. Wong", "A. Gatt", "V. Stamatescu", "M.D. McDonnell"], "venue": "CoRR, vol. abs/1609.08764,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Automatic portrait segmentation for image stylization", "author": ["X. Shen", "A. Hertzmann", "J. Jia", "S. Paris", "B. Price", "E. Shechtman", "I. Sachs"], "venue": "Computer Graphics Forum, vol. 35, no. 2. Wiley Online Library, 2016, pp. 93\u2013102.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, vol. 111, no. 1, pp. 98\u2013136, Jan. 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Detect what you can: Detecting and representing objects using holistic models and body parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "2011 International Conference on Computer Vision. IEEE, 2011, pp. 991\u2013998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 740\u2013755.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", "author": ["G. Ros", "L. Sellart", "J. Materzynska", "D. Vazquez", "A.M. Lopez"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3234\u20133243.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "The cityscapes dataset", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Scharw\u00e4chter", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR Workshop on The Future of Datasets in Vision, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic object classes in video: A high-definition ground truth database", "author": ["G.J. Brostow", "J. Fauqueur", "R. Cipolla"], "venue": "Pattern Recognition Letters, vol. 30, no. 2, pp. 88\u201397, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Combining appearance and structure from motion features for road scene understanding", "author": ["P. Sturgess", "K. Alahari", "L. Ladicky", "P.H. Torr"], "venue": "BMVC 2012-23rd British Machine Vision Conference. BMVA, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Road scene segmentation from a single image", "author": ["J.M. Alvarez", "T. Gevers", "Y. LeCun", "A.M. Lopez"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 376\u2013389.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised image transformation for outdoor semantic labelling", "author": ["G. Ros", "J.M. Alvarez"], "venue": "Intelligent Vehicles Symposium (IV), 2015 IEEE. IEEE, 2015, pp. 537\u2013542.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Vision-based offline-online perception paradigm for autonomous driving", "author": ["G. Ros", "S. Ramos", "M. Granados", "A. Bakhtiary", "D. Vazquez", "A.M. Lopez"], "venue": "Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on. IEEE, 2015, pp. 231\u2013 238.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Sensor fusion for semantic segmentation of urban scenes", "author": ["R. Zhang", "S.A. Candra", "K. Vetter", "A. Zakhor"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015, pp. 1850\u20131857.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 1\u20138.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonparametric scene parsing: Label transfer via dense scene alignment", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 1972\u20131979.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervoxel-consistent foreground propagation in video", "author": ["S.D. Jain", "K. Grauman"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 656\u2013671.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Material recognition in the wild with the materials in context database", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3479\u20133487.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A benchmark dataset and evaluation methodology for video object segmentation", "author": ["F. Perazzi", "J. Pont-Tuset", "B. McWilliams", "L. Van Gool", "M. Gross", "A. Sorkine-Hornung"], "venue": "Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "The 2017 davis challenge on video object segmentation", "author": ["J. Pont-Tuset", "F. Perazzi", "S. Caelles", "P. Arbel\u00e1ez", "A. Sorkine- Hornung", "L. Van Gool"], "venue": "arXiv:1704.00675, 2017.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2017}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 746\u2013760.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Sun3d: A database of big spaces reconstructed using sfm and object labels", "author": ["J. Xiao", "A. Owens", "A. Torralba"], "venue": "2013 IEEE International Conference on Computer Vision, Dec 2013, pp. 1625\u2013 1632.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Sun rgb-d: A rgb-d scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 567\u2013 576.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "A large-scale hierarchical multi-view rgb-d object dataset", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "Robotics and Automation (ICRA), 2011 IEEE International Conference on. IEEE, 2011, pp. 1817\u20131824.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "A scalable active framework for region annotation in 3d shape collections", "author": ["L. Yi", "V.G. Kim", "D. Ceylan", "I.-C. Shen", "M. Yan", "H. Su", "C. Lu", "Q. Huang", "A. Sheffer", "L. Guibas"], "venue": "SIGGRAPH Asia, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint 2D-3D- Semantic Data for Indoor Scene Understanding", "author": ["I. Armeni", "A. Sax", "A.R. Zamir", "S. Savarese"], "venue": "ArXiv e-prints, Feb. 2017.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "A benchmark for 3D mesh segmentation", "author": ["X. Chen", "A. Golovinskiy", "T. Funkhouser"], "venue": "ACM Transactions on Graphics (Proc. SIGGRAPH), vol. 28, no. 3, Aug. 2009.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "An occlusionaware feature for range images", "author": ["A. Quadros", "J. Underwood", "B. Douillard"], "venue": "Robotics and Automation, 2012. ICRA\u201912. IEEE International Conference on. IEEE, May 14-18 2012.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Contour detection in unstructured 3d point clouds", "author": ["T. Hackel", "J.D. Wegner", "K. Schindler"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1610\u20131618.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Segmentation and recognition using structure from motion point clouds", "author": ["G.J. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla"], "venue": "European Conference on Computer Vision. Springer, 2008, pp. 44\u201357.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Vision meets robotics: The kitti dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "The International Journal of Robotics Research, vol. 32, no. 11, pp. 1231\u20131237, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning object class detectors from weakly annotated video", "author": ["A. Prest", "C. Leistner", "J. Civera", "C. Schmid", "V. Ferrari"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 3282\u20133289.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "OpenSurfaces: A richly annotated catalog of surface appearance", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "ACM Trans. on Graphics (SIGGRAPH), vol. 32, no. 4, 2013.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Labelme: a database and web-based tool for image annotation", "author": ["B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "International journal of computer vision, vol. 77, no. 1, pp. 157\u2013173, 2008.  22", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2008}, {"title": "Perceptual organization and recognition of indoor scenes from rgb-d images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 564\u2013571.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "A Category-Level 3D Object Dataset: Putting the Kinect to Work", "author": ["A. Janoch", "S. Karayev", "Y. Jia", "J.T. Barron", "M. Fritz", "K. Saenko", "T. Darrell"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "The object segmentation database (osd)", "author": ["A. Richtsfeld"], "venue": "2012.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Shapenet: An information-rich 3d model repository", "author": ["A.X. Chang", "T. Funkhouser", "L. Guibas", "P. Hanrahan", "Q. Huang", "Z. Li", "S. Savarese", "M. Savva", "S. Song", "H. Su"], "venue": "arXiv preprint arXiv:1512.03012, 2015.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "3d semantic parsing of large-scale indoor spaces", "author": ["I. Armeni", "O. Sener", "A.R. Zamir", "H. Jiang", "I. Brilakis", "M. Fischer", "S. Savarese"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1534\u20131543.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u20133440.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2015}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561, 2015.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding", "author": ["A. Kendall", "V. Badrinarayanan", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.02680, 2015.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062, 2014.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["\u2014\u2014"], "venue": "arXiv preprint arXiv:1606.00915, 2016.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1529\u20131537.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122, 2015.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2015}, {"title": "Enet: A deep neural network architecture for real-time semantic segmentation", "author": ["A. Paszke", "A. Chaurasia", "S. Kim", "E. Culurciello"], "venue": "arXiv preprint arXiv:1606.02147, 2016.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-scale convolutional architecture for semantic segmentation", "author": ["A. Raj", "D. Maturana", "S. Scherer"], "venue": "2015.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2650\u20132658.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-scale cnn for affordance segmentation in rgb images", "author": ["A. Roy", "S. Todorovic"], "venue": "European Conference on Computer Vision. Springer, 2016, pp. 186\u2013201.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiscale fully convolutional network with application to industrial inspection", "author": ["X. Bian", "S.N. Lim", "N. Zhou"], "venue": "Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on. IEEE, 2016, pp. 1\u20138.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2016}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "arXiv preprint arXiv:1506.04579, 2015.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2015}, {"title": "Reseg: A recurrent neural network-based model for semantic segmentation", "author": ["F. Visin", "M. Ciccone", "A. Romero", "K. Kastner", "K. Cho", "Y. Bengio", "M. Matteucci", "A. Courville"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2016.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2016}, {"title": "LSTM-CF: Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene Labeling", "author": ["Z. Li", "Y. Gan", "X. Liang", "Y. Yu", "H. Cheng", "L. Lin"], "venue": "Cham: Springer International Publishing,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2016}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3547\u20133555.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural networks for scene labeling.", "author": ["P.H. Pinheiro", "R. Collobert"], "venue": "in ICML,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2014}, {"title": "Dag-recurrent neural networks for scene labeling", "author": ["B. Shuai", "Z. Zuo", "G. Wang", "B. Wang"], "venue": "CoRR, vol. abs/1509.00552, 2015. [Online]. Available: http://arxiv.org/abs/1509.00552", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to segment object candidates", "author": ["P.O. Pinheiro", "R. Collobert", "P. Dollar"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1990\u20131998.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to refine object segments", "author": ["P.O. Pinheiro", "T.-Y. Lin", "R. Collobert", "P. Doll\u00e1r"], "venue": "European Conference on Computer Vision. Springer, 2016, pp. 75\u201391.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2016}, {"title": "A multipath network for object detection", "author": ["S. Zagoruyko", "A. Lerer", "T.-Y. Lin", "P.O. Pinheiro", "S. Gross", "S. Chintala", "P. Doll\u00e1r"], "venue": "arXiv preprint arXiv:1604.02135, 2016.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2016}, {"title": "Point cloud labeling using 3d convolutional neural network", "author": ["J. Huang", "S. You"], "venue": "Proc. of the International Conf. on Pattern Recognition (ICPR), vol. 2, 2016.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2016}, {"title": "Pointnet: Deep learning on point sets for 3d classification and segmentation", "author": ["C.R. Qi", "H. Su", "K. Mo", "L.J. Guibas"], "venue": "arXiv preprint arXiv:1612.00593, 2016.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2016}, {"title": "Clockwork convnets for video semantic segmentation", "author": ["E. Shelhamer", "K. Rakelly", "J. Hoffman", "T. Darrell"], "venue": "Computer Vision\u2013 ECCV 2016 Workshops. Springer, 2016, pp. 852\u2013868.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep end2end voxel2voxel prediction", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2016, pp. 17\u201324.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 2018\u20132025.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European conference on computer vision. Springer, 2014, pp. 818\u2013833.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2014}, {"title": "Grabcut: Interactive foreground extraction using iterated graph cuts", "author": ["C. Rother", "V. Kolmogorov", "A. Blake"], "venue": "ACM transactions on graphics (TOG), vol. 23, no. 3. ACM, 2004, pp. 309\u2013314.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2004}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "International Journal of Computer Vision, vol. 81, no. 1, pp. 2\u201323, 2009.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["V. Koltun"], "venue": "Adv. Neural Inf. Process. Syst, vol. 2, no. 3, p. 4, 2011.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter learning and convergent inference for dense random fields.", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2013}, {"title": "Exploiting local structures with the kronecker layer in convolutional networks", "author": ["S. Zhou", "J.-N. Wu", "Y. Wu", "X. Zhou"], "venue": "arXiv preprint arXiv:1512.09194, 2015.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2014}, {"title": "RGB-D scene labeling with long short-term memorized fusion model", "author": ["Z. Li", "Y. Gan", "X. Liang", "Y. Yu", "H. Cheng", "L. Lin"], "venue": "CoRR, vol. abs/1604.05000, 2016. [Online]. Available: http://arxiv.org/abs/1604.05000", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep contrast learning for salient object detection", "author": ["G. Li", "Y. Yu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 478\u2013487.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiscale combinatorial grouping", "author": ["P. Arbel\u00e1ez", "J. Pont-Tuset", "J.T. Barron", "F. Marques", "J. Malik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 328\u2013335.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580\u2013587.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge", "author": ["A. Zeng", "K. Yu", "S. Song", "D. Suo", "E.W. Jr.", "A. Rodriguez", "J. Xiao"], "venue": "CoRR, vol. abs/1609.09475, 2016. [Online]. Available: http: //arxiv.org/abs/1609.09475", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-view deep learning for consistent semantic mapping with rgb-d cameras", "author": ["L. Ma", "J. Stuckler", "C. Kerl", "D. Cremers"], "venue": "arXiv:1703.08866, Mar 2017.  23", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2017}, {"title": "Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture", "author": ["C. Hazirbas", "L. Ma", "C. Domokos", "D. Cremers"], "venue": "Proc. ACCV, vol. 2, 2016.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative feature learning for video semantic segmentation", "author": ["H. Zhang", "K. Jiang", "Y. Zhang", "Q. Li", "C. Xia", "X. Chen"], "venue": "Virtual Reality and Visualization (ICVRV), 2014 International Conference on. IEEE, 2014, pp. 321\u2013326.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on pattern analysis and machine intelligence, vol. 23, no. 11, pp. 1222\u20131239, 2001.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 4489\u20134497.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv preprint arXiv:1506.05163, 2015.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "arXiv preprint arXiv:1609.02907, 2016.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "Proceedings of the 33rd annual international conference on machine learning. ACM, 2016.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2016}, {"title": "Structured pruning of deep convolutional neural networks", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1512.08571, 2015.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "arXiv preprint arXiv:1510.00149, 2015.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 264, "endOffset": 267}, {"referenceID": 7, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 268, "endOffset": 271}, {"referenceID": 8, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 272, "endOffset": 275}, {"referenceID": 9, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 276, "endOffset": 280}, {"referenceID": 10, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 281, "endOffset": 285}, {"referenceID": 11, "context": "[12] and Thoma [13], which do a great work summarizing and classifying existing methods, discussing datasets and metrics, and providing design choices for future research directions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] and Thoma [13], which do a great work summarizing and classifying existing methods, discussing datasets and metrics, and providing design choices for future research directions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "[14] was relatively simple.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Figure reproduced from [14].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "They proposed various models and configurations of deep CNNs [15], one of them was submitted to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)-2013.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "[16] which won the ILSVRC-2014 challenge with a TOP-5 test accuracy of 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Figure reproduced from [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Microsoft\u2019s ResNet [17] is specially remarkable thanks to winning ILSVRC-2016 with 96.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "Figure reproduced from [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "[18] proposed a Multi-dimensional Recurrent Neural Network (MDRNN) architecture which replaces each single recurrent connection from standard RNNs with d connections, where d is the number of spatio-temporal data dimensions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposed ReNet architecture in which instead of multidimensional RNNs, they have been using usual sequence RNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Extracted from [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Even if a dataset large enough is available and convergence does not take that long, it is often helpful to start with pre-trained weights instead of random initialized ones [20] [21].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "Even if a dataset large enough is available and convergence does not take that long, it is often helpful to start with pre-trained weights instead of random initialized ones [20] [21].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "[22] proved that transferring features even from distant tasks can be better than using random initialization, taking into account that the transferability of features decreases as the difference between the pre-trained task and the target one increases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Due to the inherent difficulty of gathering and creating per-pixel labelled segmentation datasets, their scale is not as large as the size of classification datasets such as ImageNet [23] [24].", "startOffset": 183, "endOffset": 187}, {"referenceID": 23, "context": "Due to the inherent difficulty of gathering and creating per-pixel labelled segmentation datasets, their scale is not as large as the size of classification datasets such as ImageNet [23] [24].", "startOffset": 188, "endOffset": 192}, {"referenceID": 24, "context": "Data augmentation is a common technique that has been proven to benefit the training of machine learning models in general and deep architectures in particular; either speeding up convergence or acting as a regularizer, thus avoiding overfitting and increasing generalization capabilities [25].", "startOffset": 289, "endOffset": 293}, {"referenceID": 25, "context": "For instance, in [26], a dataset of 1500 portrait images is augmented synthesizing four new scales (0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "\u2022 PASCAL Visual Object Classes (VOC) [27]1: this challenge consists of a ground-truth annotated dataset of images and five different competitions: classification, detection, segmentation, action classification, and person layout.", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "\u2022 PASCAL Context [28]3: this dataset is an extension of the PASCAL VOC 2010 detection challenge which contains pixel-wise labels for all training images (10103).", "startOffset": 17, "endOffset": 21}, {"referenceID": 28, "context": "\u2022 PASCAL Part [29]4: this database is an extension of the PASCAL VOC 2010 detection challenge which goes beyond that task to provide per-pixel segmentation masks for each part of the objects (or at least silhouette annotation if the object does not have a", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "\u2022 Semantic Boundaries Dataset (SBD) [30]5: this dataset is an extended version of the aforementioned PASCAL VOC which provides semantic segmentation ground truth for those images that were not labelled in VOC.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "\u2022 Microsoft Common Objects in Context (COCO) [31]6: is another image recognition, segmentation, and captioning large-scale dataset.", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "\u2022 SYNTHetic Collection of Imagery and Annotations (SYNTHIA) [32]8: is a large-scale collection of photorealistic renderings of a virtual city, semantically segmented, whose purpose is scene understanding in the context of driving or urban scenarios.", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "Name and Reference Purpose Year Classes Data Resolution Sequence Synthetic/Real Samples (training) Samples (validation) Samples (test) PASCAL VOC 2012 Segmentation [27] Generic 2012 21 2D Variable 7 R 1464 1449 Private PASCAL-Context [28] Generic 2014 540 (59) 2D Variable 7 R 10103 N/A 9637", "startOffset": 164, "endOffset": 168}, {"referenceID": 27, "context": "Name and Reference Purpose Year Classes Data Resolution Sequence Synthetic/Real Samples (training) Samples (validation) Samples (test) PASCAL VOC 2012 Segmentation [27] Generic 2012 21 2D Variable 7 R 1464 1449 Private PASCAL-Context [28] Generic 2014 540 (59) 2D Variable 7 R 10103 N/A 9637", "startOffset": 234, "endOffset": 238}, {"referenceID": 28, "context": "PASCAL-Part [29] Generic-Part 2014 20 2D Variable 7 R 10103 N/A 9637", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "SBD [30] Generic 2011 21 2D Variable 7 R 8498 2857 N/A Microsoft COCO [31] Generic 2014 +80 2D Variable 7 R 82783 40504 81434", "startOffset": 4, "endOffset": 8}, {"referenceID": 30, "context": "SBD [30] Generic 2011 21 2D Variable 7 R 8498 2857 N/A Microsoft COCO [31] Generic 2014 +80 2D Variable 7 R 82783 40504 81434", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "SYNTHIA [32] Urban (Driving) 2016 11 2D 960\u00d7 720 7 S 13407 N/A N/A Cityscapes (fine) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 2975 500 1525", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "SYNTHIA [32] Urban (Driving) 2016 11 2D 960\u00d7 720 7 S 13407 N/A N/A Cityscapes (fine) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 2975 500 1525", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": "Cityscapes (coarse) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 22973 500 N/A CamVid [34] Urban (Driving) 2009 32 2D 960\u00d7 720 3 R 701 N/A N/A CamVid-Sturgess [35] Urban (Driving) 2009 11 2D 960\u00d7 720 3 R 367 100 233", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "Cityscapes (coarse) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 22973 500 N/A CamVid [34] Urban (Driving) 2009 32 2D 960\u00d7 720 3 R 701 N/A N/A CamVid-Sturgess [35] Urban (Driving) 2009 11 2D 960\u00d7 720 3 R 367 100 233", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "Cityscapes (coarse) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 22973 500 N/A CamVid [34] Urban (Driving) 2009 32 2D 960\u00d7 720 3 R 701 N/A N/A CamVid-Sturgess [35] Urban (Driving) 2009 11 2D 960\u00d7 720 3 R 367 100 233", "startOffset": 155, "endOffset": 159}, {"referenceID": 35, "context": "KITTI-Layout [36] [37] Urban/Driving 2012 3 2D Variable 7 R 323 N/A N/A KITTI-Ros [38] Urban/Driving 2015 11 2D Variable 7 R 170 N/A 46", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "KITTI-Layout [36] [37] Urban/Driving 2012 3 2D Variable 7 R 323 N/A N/A KITTI-Ros [38] Urban/Driving 2015 11 2D Variable 7 R 170 N/A 46", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "KITTI-Layout [36] [37] Urban/Driving 2012 3 2D Variable 7 R 323 N/A N/A KITTI-Ros [38] Urban/Driving 2015 11 2D Variable 7 R 170 N/A 46", "startOffset": 82, "endOffset": 86}, {"referenceID": 38, "context": "KITTI-Zhang [39] Urban/Driving 2015 10 2D/3D 1226\u00d7 370 7 R 140 N/A 112", "startOffset": 12, "endOffset": 16}, {"referenceID": 39, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 20, "endOffset": 24}, {"referenceID": 40, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 77, "endOffset": 81}, {"referenceID": 41, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 229, "endOffset": 233}, {"referenceID": 42, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 284, "endOffset": 288}, {"referenceID": 43, "context": "DAVIS [44] [45] Generic 2016 4 2D 480p 3 R 4219 2023 2180", "startOffset": 6, "endOffset": 10}, {"referenceID": 44, "context": "DAVIS [44] [45] Generic 2016 4 2D 480p 3 R 4219 2023 2180", "startOffset": 11, "endOffset": 15}, {"referenceID": 45, "context": "NYUDv2 [46] Indoor 2012 40 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 46, "context": "5D 480\u00d7 640 7 R 795 654 N/A SUN3D [47] Indoor 2013 \u2013 2.", "startOffset": 34, "endOffset": 38}, {"referenceID": 47, "context": "5D 640\u00d7 480 3 R 19640 N/A N/A SUNRGBD [48] Indoor 2015 37 2.", "startOffset": 38, "endOffset": 42}, {"referenceID": 48, "context": "RGB-D Object Dataset [49] Household objects 2011 51 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 49, "context": "5D 640\u00d7 480 3 R 207920 N/A N/A ShapeNet Part [50] Object/Part 2016 16/50 3D N/A 7 S 31, 963 N/A N/A Stanford 2D-3D-S [51] Indoor 2017 13 2D/2.", "startOffset": 45, "endOffset": 49}, {"referenceID": 50, "context": "5D 640\u00d7 480 3 R 207920 N/A N/A ShapeNet Part [50] Object/Part 2016 16/50 3D N/A 7 S 31, 963 N/A N/A Stanford 2D-3D-S [51] Indoor 2017 13 2D/2.", "startOffset": 117, "endOffset": 121}, {"referenceID": 51, "context": "5D/3D 1080\u00d7 1080 3 R 70469 N/A N/A 3D Mesh [52] Object/Part 2009 19 3D N/A 7 S 380 N/A N/A Sydney Urban Objects Dataset [53] Urban (Objects) 2013 26 3D N/A 7 R 41 N/A N/A Large-Scale Point Cloud Classification Benchmark [54] Urban/Nature 2016 8 3D N/A 7 R 15 N/A 15", "startOffset": 43, "endOffset": 47}, {"referenceID": 52, "context": "5D/3D 1080\u00d7 1080 3 R 70469 N/A N/A 3D Mesh [52] Object/Part 2009 19 3D N/A 7 S 380 N/A N/A Sydney Urban Objects Dataset [53] Urban (Objects) 2013 26 3D N/A 7 R 41 N/A N/A Large-Scale Point Cloud Classification Benchmark [54] Urban/Nature 2016 8 3D N/A 7 R 15 N/A 15", "startOffset": 120, "endOffset": 124}, {"referenceID": 53, "context": "5D/3D 1080\u00d7 1080 3 R 70469 N/A N/A 3D Mesh [52] Object/Part 2009 19 3D N/A 7 S 380 N/A N/A Sydney Urban Objects Dataset [53] Urban (Objects) 2013 26 3D N/A 7 R 41 N/A N/A Large-Scale Point Cloud Classification Benchmark [54] Urban/Nature 2016 8 3D N/A 7 R 15 N/A 15", "startOffset": 220, "endOffset": 224}, {"referenceID": 32, "context": "\u2022 Cityscapes [33]9: is a large-scale database which focuses on semantic understanding of urban street scenes.", "startOffset": 13, "endOffset": 17}, {"referenceID": 54, "context": "\u2022 CamVid [55] [34]10: is a road/driving scene understanding database which was originally captured as five video sequences with a 960 \u00d7 720 resolution camera mounted on the dashboard of a car.", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "\u2022 CamVid [55] [34]10: is a road/driving scene understanding database which was originally captured as five video sequences with a 960 \u00d7 720 resolution camera mounted on the dashboard of a car.", "startOffset": 14, "endOffset": 18}, {"referenceID": 34, "context": "[35] which divided the dataset into 367/100/233 training, validation, and testing images respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "\u2022 KITTI [56]: is one of the most popular datasets for use in mobile robotics and autonomous driv-", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": "[36] [37] generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[36] [37] generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky.", "startOffset": 5, "endOffset": 9}, {"referenceID": 38, "context": "[39] annotated 252 (140 for training and 112 for testing) acquisitions \u2013 RGB and Velodyne scans \u2013 from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "\u2022 Youtube-Objects [57] is a database of videos collected from YouTube which contain objects from ten PASCAL VOC classes: aeroplane, bird, boat, car, cat, cow, dog, horse, motorbike, and train.", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "[42] manually annotated a subset of 126 sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "\u2022 Adobe\u2019s Portrait Segmentation [26]11: this is a dataset of 800 \u00d7 600 pixels portrait images collected from Flickr, mainly captured with mobile frontfacing cameras.", "startOffset": 32, "endOffset": 36}, {"referenceID": 42, "context": "\u2022 Materials in Context (MINC) [43]: this work is a dataset for patch material classification and full scene material segmentation.", "startOffset": 30, "endOffset": 34}, {"referenceID": 57, "context": "The main source for these images is the OpenSurfaces dataset [58], which was augmented using other sources of imagery such as Flickr or Houzz.", "startOffset": 61, "endOffset": 65}, {"referenceID": 43, "context": "\u2022 Densely-Annotated VIdeo Segmentation (DAVIS) [44] [45]12: this challenge is purposed for video object segmentation.", "startOffset": 47, "endOffset": 51}, {"referenceID": 44, "context": "\u2022 Densely-Annotated VIdeo Segmentation (DAVIS) [44] [45]12: this challenge is purposed for video object segmentation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 39, "context": "\u2022 Stanford background [40]13: dataset with outdoor scene images imported from existing public datasets: LabelMe, MSRC, PASCAL VOC and Geometric Context.", "startOffset": 22, "endOffset": 26}, {"referenceID": 40, "context": "\u2022 SiftFlow [41]: contains 2688 fully annotated images which are a subset of the LabelMe database [59].", "startOffset": 11, "endOffset": 15}, {"referenceID": 58, "context": "\u2022 SiftFlow [41]: contains 2688 fully annotated images which are a subset of the LabelMe database [59].", "startOffset": 97, "endOffset": 101}, {"referenceID": 45, "context": "gz \u2022 NYUDv2 [46]14: this database consists of 1449 indoor RGB-D images captured with a Microsoft Kinect device.", "startOffset": 12, "endOffset": 16}, {"referenceID": 59, "context": "[60] for both training (795 images) and testing (654) splits.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "\u2022 SUN3D [47]15: similar to the NYUDv2, this dataset contains a large-scale RGB-D video database, with 8 annotated sequences.", "startOffset": 8, "endOffset": 12}, {"referenceID": 47, "context": "\u2022 SUNRGBD [48]16: captured with four RGB-D sensors, this dataset contains 10000 RGB-D images, at a similar scale as PASCAL VOC.", "startOffset": 10, "endOffset": 14}, {"referenceID": 45, "context": "It contains images from NYU depth v2 [46], Berkeley B3DO [61], and SUN3D [47].", "startOffset": 37, "endOffset": 41}, {"referenceID": 60, "context": "It contains images from NYU depth v2 [46], Berkeley B3DO [61], and SUN3D [47].", "startOffset": 57, "endOffset": 61}, {"referenceID": 46, "context": "It contains images from NYU depth v2 [46], Berkeley B3DO [61], and SUN3D [47].", "startOffset": 73, "endOffset": 77}, {"referenceID": 61, "context": "\u2022 The Object Segmentation Database (OSD) [62]17 this database has been designed for segmenting unknown objects from generic scenes even under partial occlusions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 48, "context": "\u2022 RGB-D Object Dataset [49]18: this dataset is composed by video sequences of 300 common household objects organized in 51 categories arranged using WordNet hypernym-hyponym relationships.", "startOffset": 23, "endOffset": 27}, {"referenceID": 49, "context": "\u2022 ShapeNet Part [50]19: is a subset of the ShapeNet [63] repository which focuses on fine-grained 3D object segmentation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 62, "context": "\u2022 ShapeNet Part [50]19: is a subset of the ShapeNet [63] repository which focuses on fine-grained 3D object segmentation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 50, "context": "\u2022 Stanford 2D-3D-S [51]20: is a multi-modal and largescale indoor spaces dataset extending the Stanford 3D Semantic Parsing work [64].", "startOffset": 19, "endOffset": 23}, {"referenceID": 63, "context": "\u2022 Stanford 2D-3D-S [51]20: is a multi-modal and largescale indoor spaces dataset extending the Stanford 3D Semantic Parsing work [64].", "startOffset": 129, "endOffset": 133}, {"referenceID": 51, "context": "\u2022 A Benchmark for 3D Mesh Segmentation [52]21: this benchmark is composed by 380 meshes classified in 19 categories (human, cup, glasses, airplane, ant, chair, octopus, table, teddy, hand, plier, fish, bird, armadillo, bust, mech, bearing, vase, fourleg).", "startOffset": 39, "endOffset": 43}, {"referenceID": 52, "context": "\u2022 Sydney Urban Objects Dataset [53]22: this dataset contains a variety of common urban road objects scanned with a Velodyne HDK-64E LIDAR.", "startOffset": 31, "endOffset": 35}, {"referenceID": 53, "context": "\u2022 Large-Scale Point Cloud Classification Benchmark [54]23: this benchmark provides manually annotated 3D point clouds of diverse natural and urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles among others.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "The relentless success of deep learning techniques in various high-level computer vision tasks \u2013 in particular, supervised approaches such as Convolutional Neural Networks (CNNs) for image classification or object detection [14] [15] [16] \u2013 motivated researchers to explore the capabilities of such networks for pixel-level labelling problems like semantic segmentation.", "startOffset": 224, "endOffset": 228}, {"referenceID": 14, "context": "The relentless success of deep learning techniques in various high-level computer vision tasks \u2013 in particular, supervised approaches such as Convolutional Neural Networks (CNNs) for image classification or object detection [14] [15] [16] \u2013 motivated researchers to explore the capabilities of such networks for pixel-level labelling problems like semantic segmentation.", "startOffset": 229, "endOffset": 233}, {"referenceID": 15, "context": "The relentless success of deep learning techniques in various high-level computer vision tasks \u2013 in particular, supervised approaches such as Convolutional Neural Networks (CNNs) for image classification or object detection [14] [15] [16] \u2013 motivated researchers to explore the capabilities of such networks for pixel-level labelling problems like semantic segmentation.", "startOffset": 234, "endOffset": 238}, {"referenceID": 64, "context": "[65].", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 159, "endOffset": 163}, {"referenceID": 65, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 210, "endOffset": 214}, {"referenceID": 66, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 285, "endOffset": 289}, {"referenceID": 67, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 346, "endOffset": 350}, {"referenceID": 68, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 351, "endOffset": 355}, {"referenceID": 42, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 439, "endOffset": 443}, {"referenceID": 69, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 514, "endOffset": 518}, {"referenceID": 70, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 580, "endOffset": 584}, {"referenceID": 71, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 638, "endOffset": 642}, {"referenceID": 72, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 733, "endOffset": 737}, {"referenceID": 73, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 817, "endOffset": 821}, {"referenceID": 74, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 903, "endOffset": 907}, {"referenceID": 75, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1010, "endOffset": 1014}, {"referenceID": 76, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1085, "endOffset": 1089}, {"referenceID": 77, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1153, "endOffset": 1157}, {"referenceID": 78, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1242, "endOffset": 1246}, {"referenceID": 79, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1352, "endOffset": 1356}, {"referenceID": 80, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1410, "endOffset": 1414}, {"referenceID": 81, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1487, "endOffset": 1491}, {"referenceID": 9, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1574, "endOffset": 1578}, {"referenceID": 82, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1665, "endOffset": 1669}, {"referenceID": 83, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1745, "endOffset": 1749}, {"referenceID": 84, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1823, "endOffset": 1827}, {"referenceID": 85, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1930, "endOffset": 1934}, {"referenceID": 86, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 2007, "endOffset": 2011}, {"referenceID": 87, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 2102, "endOffset": 2106}, {"referenceID": 88, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 2271, "endOffset": 2275}, {"referenceID": 64, "context": "[65].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 137, "endOffset": 141}, {"referenceID": 89, "context": "Those maps are upsampled using fractionally strided convolutions (also named deconvolutions [90] [91]) to produce dense per-pixel labeled outputs.", "startOffset": 92, "endOffset": 96}, {"referenceID": 90, "context": "Those maps are upsampled using fractionally strided convolutions (also named deconvolutions [90] [91]) to produce dense per-pixel labeled outputs.", "startOffset": 97, "endOffset": 101}, {"referenceID": 65, "context": "SegNet [66] is a clear example of this divergence (see Figure 9).", "startOffset": 7, "endOffset": 11}, {"referenceID": 65, "context": "Figure extracted from [66].", "startOffset": 22, "endOffset": 26}, {"referenceID": 65, "context": "Figure reproduced from [66].", "startOffset": 23, "endOffset": 27}, {"referenceID": 91, "context": "CRFs enable the combination of lowlevel image information \u2013 such as the interactions between pixels [92] [93] \u2013 with the output of multi-class inference systems that produce per-pixel class scores.", "startOffset": 100, "endOffset": 104}, {"referenceID": 92, "context": "CRFs enable the combination of lowlevel image information \u2013 such as the interactions between pixels [92] [93] \u2013 with the output of multi-class inference systems that produce per-pixel class scores.", "startOffset": 105, "endOffset": 109}, {"referenceID": 67, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 19, "endOffset": 23}, {"referenceID": 68, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 24, "endOffset": 28}, {"referenceID": 93, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 99, "endOffset": 103}, {"referenceID": 94, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 104, "endOffset": 108}, {"referenceID": 42, "context": "[43] makes use of various CNNs trained to identify patches in the MINC database.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "11: CRF refinement per iteration as shown by the authors of DeepLab [68].", "startOffset": 68, "endOffset": 72}, {"referenceID": 69, "context": "[70].", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "[81] which employed RNNs to model large spatial dependencies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 95, "context": "Dilated convolutions, also named \u00e0-trous convolutions, are a generalization of Kronecker-factored convolutional filters [96] which support exponentially expanding receptive fields without losing resolution.", "startOffset": 120, "endOffset": 124}, {"referenceID": 70, "context": "12: As shown in [71], dilated convolution filters with various dilation rates: (a) 1-dilated convolutions in which each unit has a 3\u00d7 3 receptive fields, (b) 2-dilated ones with 7 \u00d7 7 receptive fields, and (c) 3-dilated convolutions with 15\u00d7 15 receptive fields.", "startOffset": 16, "endOffset": 20}, {"referenceID": 70, "context": "[71], the already mentioned DeepLab (its improved version) [69], and the real-time network ENet [72].", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "[71], the already mentioned DeepLab (its improved version) [69], and the real-time network ENet [72].", "startOffset": 59, "endOffset": 63}, {"referenceID": 71, "context": "[71], the already mentioned DeepLab (its improved version) [69], and the real-time network ENet [72].", "startOffset": 96, "endOffset": 100}, {"referenceID": 72, "context": "[73] propose a multi-scale version of a fully convolutional VGG-16.", "startOffset": 0, "endOffset": 4}, {"referenceID": 74, "context": "[75] take a different approach using a network composed by four multi-scale CNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "[74].", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "[74].", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "Figure extracted from [74].", "startOffset": 22, "endOffset": 26}, {"referenceID": 75, "context": "[76].", "startOffset": 0, "endOffset": 4}, {"referenceID": 76, "context": "This approach is taken by ParseNet [77] in their context module.", "startOffset": 35, "endOffset": 39}, {"referenceID": 83, "context": "Figure extracted from [84].", "startOffset": 22, "endOffset": 26}, {"referenceID": 76, "context": "Figure extracted from [77].", "startOffset": 22, "endOffset": 26}, {"referenceID": 83, "context": "in their SharpMask network [84], which introduced a progressive refinement module to incorporate features from the previous layer to the next in a top-down architecture.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "[19] proposed an architecture for semantic segmentation called ReSeg [78] represented in Figure 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 77, "context": "[19] proposed an architecture for semantic segmentation called ReSeg [78] represented in Figure 17.", "startOffset": 69, "endOffset": 73}, {"referenceID": 77, "context": "Figure extracted from [78].", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "VGG-16 network [15], feeding the resulting feature maps into one or more ReNet layers for fine-tuning.", "startOffset": 15, "endOffset": 19}, {"referenceID": 96, "context": "Several derived models such as Long Short-Term Memory (LSTM) networks [97] and GRUs [98] are the stateof-art in this field to avoid such problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 97, "context": "Several derived models such as Long Short-Term Memory (LSTM) networks [97] and GRUs [98] are the stateof-art in this field to avoid such problem.", "startOffset": 84, "endOffset": 88}, {"referenceID": 98, "context": "Inspired on the same ReNet architecture, a novel Long Short-Term Memorized Context Fusion (LSTM-CF) model for scene labeling was proposed by [99].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "The RGB pipeline relies on a variant of the DeepLab architecture [29] concatenating features at three different scales to enrich feature representation (inspired by [100]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 99, "context": "The RGB pipeline relies on a variant of the DeepLab architecture [29] concatenating features at three different scales to enrich feature representation (inspired by [100]).", "startOffset": 165, "endOffset": 170}, {"referenceID": 79, "context": "[80] purposed a simple 2D LSTMbased architecture in which the input image is divided into non-overlapping windows which are fed into four separate LSTMs memory blocks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "[81] introduced Recurrent Convolutional Neural Networks (rCNNs) which recurrently train with different input window sizes taking into account previous predictions by using a different input window sizes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 81, "context": "Undirected cyclic graphs (UCGs) were also adopted to model image contexts for semantic segmentation [82].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "[10] proposed a Simultaneous Detection and Segmentation (SDS) method in order to improve performance over already existing works.", "startOffset": 0, "endOffset": 4}, {"referenceID": 100, "context": "Their pipeline uses, firstly, a bottom-up hierarchical image segmentation and object candidate generation process called Multi-scale COmbinatorial Grouping (MCG) [101] to obtain region proposals.", "startOffset": 162, "endOffset": 167}, {"referenceID": 101, "context": "For each region, features are extracted by using an adapted version of the Region-CNN (R-CNN) [102], which is fine-tuned using bounding boxes provided by the MCG method instead of selective search and also alongside region foreground features.", "startOffset": 94, "endOffset": 99}, {"referenceID": 82, "context": "[83] presented DeepMask model, an object proposal approach based on a single ConvNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 83, "context": "Based on the DeepMask architecture as a starting point due to its effectiveness, the same authors presented a novel architecture for object instance segmentation implementing a top-down refinement process [84] and achieving a better performance in terms of accuracy and speed.", "startOffset": 205, "endOffset": 209}, {"referenceID": 82, "context": "Figure extracted from [83].", "startOffset": 22, "endOffset": 26}, {"referenceID": 84, "context": "Another approach, based on Fast R-CNN as a starting point and using DeepMask object proposals instead of Selective Search was presented by Zagoruyko et al [85].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "Different techniques such as Horizontal Height Angle (HHA) [11] are used for encoding the depth into three channels as follows: horizontal disparity, height above ground, and the angle between local surface normal and the inferred gravity direction.", "startOffset": 59, "endOffset": 63}, {"referenceID": 98, "context": "Several works such as [99] are based on this encoding technique.", "startOffset": 22, "endOffset": 26}, {"referenceID": 102, "context": "[103] present an object segmentation approach that leverages multi-view RGB-D data and deep learning techniques.", "startOffset": 0, "endOffset": 5}, {"referenceID": 13, "context": "Moreover, in this work, multiple networks for feature extraction were trained (AlexNet [14] and VGG-16 [15]), evaluating the benefits of using depth information.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "Moreover, in this work, multiple networks for feature extraction were trained (AlexNet [14] and VGG-16 [15]), evaluating the benefits of using depth information.", "startOffset": 103, "endOffset": 107}, {"referenceID": 103, "context": "[104] propose a novel approach for object-class segmentation using a multi-view deep learning technique.", "startOffset": 0, "endOffset": 5}, {"referenceID": 104, "context": "The proposed approach is based on FuseNet [105], which combines RGB and depth images for semantic segmentation, and improves the original work by adding multi-scale loss minimization.", "startOffset": 42, "endOffset": 47}, {"referenceID": 85, "context": "[86] (see Figure 19 take a point cloud and parse it through a dense voxel grid, generating a set of occupancy voxels which are used as input to a 3D CNN to produce one label per voxel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "[86] for semantic labeling of point clouds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "Figure extracted from [86].", "startOffset": 22, "endOffset": 26}, {"referenceID": 86, "context": "PointNet [87] is a pioneering work which presents a deep neural network that takes raw point clouds as input, providing a unified architecture for both classification and segmentation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 87, "context": "[88].", "startOffset": 0, "endOffset": 4}, {"referenceID": 105, "context": "[106] took a different approach and made use of a 3DCNN, which was originally created for learning features from volumes, to learn hierarchical spatio-temporal features from multi-channel inputs such as video clips.", "startOffset": 0, "endOffset": 5}, {"referenceID": 106, "context": "The final segmentation is obtained by applying graph-cut [107] on the supervoxel graph.", "startOffset": 57, "endOffset": 62}, {"referenceID": 88, "context": "[89].", "startOffset": 0, "endOffset": 4}, {"referenceID": 107, "context": "In that work, they make use of the Convolutional 3D (C3D) network introduced by themselves on a previous work [108], and extend it for semantic segmentation by adding deconvolutional layers at the end.", "startOffset": 110, "endOffset": 115}, {"referenceID": 86, "context": "Figure reproduced from [87].", "startOffset": 23, "endOffset": 27}, {"referenceID": 87, "context": "Figure extracted from [88].", "startOffset": 22, "endOffset": 26}, {"referenceID": 87, "context": "[88].", "startOffset": 0, "endOffset": 4}, {"referenceID": 87, "context": "Figure extracted from [88].", "startOffset": 22, "endOffset": 26}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 79.", "startOffset": 34, "endOffset": 38}, {"referenceID": 70, "context": "2 Dilation [71] 75.", "startOffset": 11, "endOffset": 15}, {"referenceID": 69, "context": "3 CRFasRNN [70] 74.", "startOffset": 11, "endOffset": 15}, {"referenceID": 76, "context": "4 ParseNet [77] 69.", "startOffset": 11, "endOffset": 15}, {"referenceID": 64, "context": "5 FCN-8s [65] 67.", "startOffset": 9, "endOffset": 13}, {"referenceID": 73, "context": "6 Multi-scale-CNN-Eigen [74] 62.", "startOffset": 24, "endOffset": 28}, {"referenceID": 66, "context": "7 Bayesian SegNet [67] 60.", "startOffset": 18, "endOffset": 22}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 45.", "startOffset": 34, "endOffset": 38}, {"referenceID": 69, "context": "2 CRFasRNN [70] 39.", "startOffset": 11, "endOffset": 15}, {"referenceID": 64, "context": "3 FCN-8s [65] 39.", "startOffset": 9, "endOffset": 13}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 64.", "startOffset": 34, "endOffset": 38}, {"referenceID": 81, "context": "# Method Accuracy (IoU) 1 DAG-RNN [82] 91.", "startOffset": 34, "endOffset": 38}, {"referenceID": 66, "context": "2 Bayesian SegNet [67] 63.", "startOffset": 18, "endOffset": 22}, {"referenceID": 65, "context": "3 SegNet [66] 60.", "startOffset": 9, "endOffset": 13}, {"referenceID": 77, "context": "4 ReSeg [78] 58.", "startOffset": 8, "endOffset": 12}, {"referenceID": 71, "context": "5 ENet [72] 55.", "startOffset": 7, "endOffset": 11}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 70.", "startOffset": 34, "endOffset": 38}, {"referenceID": 70, "context": "2 Dilation10 [71] 67.", "startOffset": 13, "endOffset": 17}, {"referenceID": 64, "context": "3 FCN-8s [65] 65.", "startOffset": 9, "endOffset": 13}, {"referenceID": 69, "context": "4 CRFasRNN [70] 62.", "startOffset": 11, "endOffset": 15}, {"referenceID": 71, "context": "5 ENet [72] 58.", "startOffset": 7, "endOffset": 11}, {"referenceID": 80, "context": "# Method Accuracy (IoU) 1 rCNN [81] 80.", "startOffset": 31, "endOffset": 35}, {"referenceID": 79, "context": "2 2D-LSTM [80] 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 81, "context": "# Method Accuracy (IoU) 1 DAG-RNN [82] 85.", "startOffset": 34, "endOffset": 38}, {"referenceID": 80, "context": "2 rCNN [81] 77.", "startOffset": 7, "endOffset": 11}, {"referenceID": 79, "context": "3 2D-LSTM [80] 70.", "startOffset": 10, "endOffset": 14}, {"referenceID": 78, "context": "# Method Accuracy (IoU) 1 LSTM-CF [79] 48.", "startOffset": 34, "endOffset": 38}, {"referenceID": 78, "context": "# Method Accuracy (IoU) 1 LSTM-CF [79] 49.", "startOffset": 34, "endOffset": 38}, {"referenceID": 78, "context": "# Method Accuracy (IoU) 1 LSTM-CF [79] 58.", "startOffset": 34, "endOffset": 38}, {"referenceID": 86, "context": "# Method Accuracy (IoU) 1 PointNet [87] 83.", "startOffset": 35, "endOffset": 39}, {"referenceID": 86, "context": "# Method Accuracy (IoU) 1 PointNet [87] 47.", "startOffset": 35, "endOffset": 39}, {"referenceID": 87, "context": "# Method Accuracy (IoU) 1 Clockwork Convnet [88] 64.", "startOffset": 44, "endOffset": 48}, {"referenceID": 87, "context": "# Method Accuracy (IoU) 1 Clockwork Convnet [88] 68.", "startOffset": 44, "endOffset": 48}, {"referenceID": 108, "context": "One promising line of research aims to treat point clouds as graphs and apply convolutions over them [109] [110] [111].", "startOffset": 101, "endOffset": 106}, {"referenceID": 109, "context": "One promising line of research aims to treat point clouds as graphs and apply convolutions over them [109] [110] [111].", "startOffset": 107, "endOffset": 112}, {"referenceID": 110, "context": "One promising line of research aims to treat point clouds as graphs and apply convolutions over them [109] [110] [111].", "startOffset": 113, "endOffset": 118}, {"referenceID": 111, "context": "Pruning is a promising research line that aims to simplify a network, making it lightweight while keeping the knowledge, and thus the accuracy, of the original network architecture [112] [113] [114].", "startOffset": 181, "endOffset": 186}, {"referenceID": 112, "context": "Pruning is a promising research line that aims to simplify a network, making it lightweight while keeping the knowledge, and thus the accuracy, of the original network architecture [112] [113] [114].", "startOffset": 187, "endOffset": 192}], "year": 2017, "abstractText": "Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.", "creator": "LaTeX with hyperref package"}}}