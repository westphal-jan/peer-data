{"id": "1704.06913", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Learning weakly supervised multimodal phoneme embeddings", "abstract": "Recent works have explored deep architectures for learning multimodal speech representation (e.g. audio and images, articulation and audio) in a supervised way. Here we investigate the role of combining different speech modalities, i.e. audio and visual information representing the lips movements, in a weakly supervised way using Siamese networks and lexical same-different side information. In particular, we ask whether one modality can benefit from the other to provide a richer representation for phone recognition in a weakly supervised setting. We introduce mono-task and multi-task methods for merging speech and visual modalities for phone recognition. The mono-task learning consists in applying a Siamese network on the concatenation of the two modalities, while the multi-task learning receives several different combinations of modalities at train time. We show that multi-task learning enhances discriminability for visual and multimodal inputs while minimally impacting auditory inputs. Furthermore, we present a qualitative analysis of the obtained phone embeddings, and show that cross-modal visual input can improve the discriminability of phonological features which are visually discernable (rounding, open/close, labial place of articulation), resulting in representations that are closer to abstract linguistic features than those based on audio only.", "histories": [["v1", "Sun, 23 Apr 2017 11:27:53 GMT  (1072kb,D)", "http://arxiv.org/abs/1704.06913v1", null], ["v2", "Wed, 18 Oct 2017 12:21:22 GMT  (1082kb,D)", "http://arxiv.org/abs/1704.06913v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rahma chaabouni", "ewan dunbar", "neil zeghidour", "emmanuel dupoux"], "accepted": false, "id": "1704.06913"}, "pdf": {"name": "1704.06913.pdf", "metadata": {"source": "CRF", "title": "Learning weakly supervised multimodal phoneme embeddings", "authors": ["Rahma Chaabouni", "Ewan Dunbar", "Neil Zeghidour", "Emmanuel"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "The ability of many people to hear and not hear, lip reading, shows that speech perception is not just a purely auditory ability. Audio-visual integration is clearly illustrated by the McGurk effect [1]: the lip movements corresponding to [g], together with audio corresponding to one [b], are perceived by many subjects as an intermediate sound. Such interactions between modalities have been documented in 5-month-old infants [2, 3]. The visual channel for speech is poorer than the auditory channel, but provides information that can be complementary, especially in relation to the location of articulation [4] (for example, between the coronal and labial consonants [d] and [b]. Previous research has examined the use of intermodal information for speech recognition and focused on systems trained with verified learning (phoneme labels)."}, {"heading": "2. Related work", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "3. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Dataset", "text": "We use the Blue Lips database [19], an audiovisual speech corpus composed of 238 French sentences read by 16 loudspeakers in hexagonal French. Recordings of each loudspeaker last about 20 minutes. We represent the audio signal with 40 MFCCs, computed from 40 filter banks on a mel scale, sampled at 100 frames per second, resulting in a 40-dimensional vector. These 40 coefficients are the first 40 cepstral coefficients and contain neither delta / delta nor energy characteristics. We use two types of visual characteristics: First, we use dimensionally reduced pixels from the region of the image corresponding to the mouth, identified with a hair cascade classifier [20], to recognize the mouth in the first image, and the meanshift algorithm [21] to track the mouth in the rest of the video."}, {"heading": "3.2. Siamese network and ABnet", "text": "A Siamese network is an architecture that contains two identical copies of the same network, so that the two subnetworks have the same configuration and share the same parameters. In our experiments, we use ABnet, a specific Siamese network architecture. The model is shown in Figure 1. It uses pairs of words to learn a representation of telephones. Input to the network during the training consists of pairs of stacked frames of MFCC features x1 and x2 and a caption y: 0, 1}, where y = 1 if x1 and x2 represent the same word, and y = 0 otherwise. x1 and x2 represent frames that are in agreement between the two words: if the two pairs are identical, this match is the result of an alignment using dynamic time warping (DTW) [23], and if the word pairs are different, xxxx2, an alignment along the diagonal.The idea of this architecture is that we can represent a large representation between D and D (if we have an abstract representation of 1)."}, {"heading": "3.3. Evaluation", "text": "We evaluate two aspects of speech representation: its ability to distinguish between phonemes and its internal structure, which we study to determine whether phonological features are uniquely encoded."}, {"heading": "3.3.1. ABX task", "text": "To evaluate the phonetic discriminability of our embedding, we use an ABX discrimination task [24, 25]. The task is to present three stimuli A, B and X, with A and B belonging to two different phonetic categories and X belonging to one of these categories (concretely always A). On a measure of divergence D (not necessarily a correct distance measurement), if D (A, X) < D (B, X), then the rating is 1 (success), and otherwise the rating is 0 (failure). In our experiments, A and B are minimal pairs of triphons: they are pairs of sounds each composed of three phonemes and differ only in their central phoneme (for example, begging, / bEg / and bag, / b\u00e6g /, although the triphons do not have to be words)."}, {"heading": "3.3.2. Parallelism", "text": "The second analysis measures how well the learned representations encode individual phonological characteristics by assessing the parallelism of the representations. [26] For a particular phonological characteristic, say [voice], representations of the characteristic are extracted by taking subtractions of phonemes that differ only in this characteristic (for [voice], [d] \u2212 [t], [z] \u2212 [s] etc.) In a space with perfect parallelism in pronunciation, these vectors will be exactly parallel (see Figure 2); the parallelism value we use here measures relative parallelism, so that all subtraction vectors corresponding to a single characteristic need only be more parallel (have higher cosines) than pairs of subtraction vectors that do not coincide with changes to the same individual characteristic in order to obtain a maximum score (1)."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. ABX discriminability", "text": "Table 2 shows the ABX error rates within and between loudspeakers for audio (A), visual (V) and concatenated representation (A & V. The distance used for the ABX tasks is the cosine distance. We first note that the ABnet embedding has a lower error than the raw characteristics in all modalities. This has already been shown for the acoustic modality [6, 27, 28]; we show that ABnet also improves the visual and concatenated input representations. Tested on the visual modality, ABnet improves the ABX cross-sound discriminability. In particular, the multitask ABnet results in an embedding of visual information that is more discriminatory than the (raw) audio functions themselves, is much more discriminatory than the audio signal itself than the visual one. In addition, we achieve the best performance with the multitask model for the visual inputs and show that this model does not use the audio learning models available in the training phase to achieve a better representation of the two."}, {"heading": "4.2. Parallelism", "text": "Figure 4 shows the parallelism values for the different embeddings. As before, representations with visual information considerably improve the values for visual characteristics, suggesting that these characteristics have a more uniform representation in these embeddings. In the case of non-visual characteristics, multi-task audio generally performs better than mono-task audio. However, in contrast to ABX analysis, acoustic embeddings only show the best value for three characteristics (nasal vowel, nasal consonant, middle / high). Therefore, even with \"non-visual\" phonological characteristics - where no obvious lip movement is expected - representations that include visual information show slightly more consistent encodings of these characteristics. On average, mono-task embeddings exhibit the best parallelism and are thus the best approximation of a phonological characterization."}, {"heading": "4.3. McGurk effect", "text": "The representations with audiovisual and visual input (monotask and multi-task concatenation) learn audiovisual integration, so we can also see if they have the same patterns of audiovisual integration as humans: We test whether the presentation of an audio signal corresponding to [b] with a different visual signal corresponding to [g] is perceived by the model as [d] (the McGurk effect); we perform an ABX discrimination task to determine whether these deviating multimodal inputs are unexpectedly similar (difficult to distinguish from) [d] (audio and video match); to assess this, we construct three sets of multimodal inputs: the first corresponds to the acoustic [b] with the same generic visual suppression [b]; the second replaces this with a generic visual suppression [p]; the first provides a baseline score; and the second results in a combination that does not match, but does not show McGurk's lip effects (the last lip effects should be the same)."}, {"heading": "5. Discussion", "text": "This study introduces methods for learning multimodal language representations in a poorly monitored environment. We use metrics for speech representation in telephone discrimination and introduce analyses of the internal structure of representations. Discrimination analysis shows that weakly monitored learning using ABnet improves phoneme discriminability over input characteristics in all cases and that for certain phonemic contrasts (those with strong visual corrections) the addition of visual information contributes to discrimination. Furthermore, it changes the structure of representation in order to provide more coherent representations of relevant phonological characteristics; the model can use the visual information even if it is only present during training; for phonological contrasts that do not benefit from visual information, the methods developed here for adding visual information in some cases reduce discriminability; this shows that visual information should only be used selectively and discarded if it does not provide discriminatory information; this indicates that future research would include in our respective attention system, or if it would ignore any of the attentiveness or attention systems."}, {"heading": "6. Acknowledgment", "text": "This work was supported by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX0001-02 PSL *), the Ecole de Neurosciences de Paris, the Ile de France DIM Cerveau et pense and an Amazon Web Services in Education Research Grant."}, {"heading": "7. References", "text": "[1] M. J. McGurk H, \"Hearing lips and seeing voices,\" Nature, vol.264, p. 746748, 1976. [2] P. K. Kuhl and A. N. Meltzoff, \"The intermodal representation of speech in infants,\" Infant Behavior and Development, vol. 7, no. 3, pp. 361-381, jul 1984. [Online] Available: http: / / doi.org / 10.1016% 2Fs0163% 2884% 2980050-8 [3]. L. D. Rosenblum, M. A. Schmuckler, and J. A. Johnson, \"The mcgurk effect in infants,\" Perception & Psychophysics, vol. 59, no. 347-357, 1997. [Online]."}], "references": [{"title": "Hearing lips and seeing voices", "author": ["H M.J. McGurk"], "venue": "Nature, vol. 264, p. 746748, 1976.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "The intermodal representation of speech in infants", "author": ["P.K. Kuhl", "A.N. Meltzoff"], "venue": "Infant Behavior and Development, vol. 7, no. 3, pp. 361\u2013381, jul 1984. [Online]. Available: https://doi.org/10.1016%2Fs0163-6383%2884%2980050-8", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "The mcgurk effect in infants", "author": ["L.D. Rosenblum", "M.A. Schmuckler", "J.A. Johnson"], "venue": "Perception & Psychophysics, vol. 59, no. 3, pp. 347\u2013357, 1997. [Online]. Available: http://dx.doi.org/10.3758/BF03211902", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Lipreading and audio-visual speech perception", "author": ["Q. Summerfield"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 335, no. 1273, pp. 71\u201378, jan 1992. [Online]. Available: https://doi.org/10.1098%2Frstb.1992.0009", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Signature verification using a\u201dsiamese\u201d time delay neural network", "author": ["J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, vol. 7, no. 4, pp. 669\u2013688, 1993. [Online]. Available: http://oro.open.ac.uk/35662/", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Weakly supervised multiembeddings learning of acoustic models", "author": ["G. Synnaeve", "E. Dupoux"], "venue": "CoRR, vol. abs/1412.6645, 2014. [Online]. Available: http://arxiv.org/ abs/1412.6645", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling.", "author": ["R. Thiolliere", "E. Dunbar", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "in INTER- SPEECH,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4950\u20134954.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5828\u20135832.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "See me, hear me: integrating automatic speech recognition and lip-reading.", "author": ["P. Duchnowski", "U. Meier", "A. Waibel"], "venue": "in ICSLP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Audio-visual speech recognition using deep learning", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "Applied Intelligence, vol. 42, no. 4, pp. 722\u2013737, 2015. [Online]. Available: http://dx.doi.org/10.1007/s10489-014-0629-7", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel lip descriptor for audio-visual keyword spotting based on adaptive decision fusion", "author": ["P. Wu", "H. Liu", "X. Li", "T. Fan", "X. Zhang"], "venue": "IEEE Transactions on Multimedia, vol. 18, no. 3, pp. 326\u2013338, mar 2016. [Online]. Available: https://doi.org/10.1109%2Ftmm.2016.2520091", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Lip reading sentences in the wild", "author": ["J.S. Chung", "A.W. Senior", "O. Vinyals", "A. Zisserman"], "venue": "CoRR, vol. abs/1611.05358, 2016. [Online]. Available: http://arxiv.org/abs/1611.05358", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "ABX-Discriminability Measures and Applications", "author": ["T. Schatz"], "venue": "Ph.D. dissertation, Ecole Normale Suprieure, Paris, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep canonical correlation analysis", "author": ["J.B.K.L. Galen Andrew", "Raman Arora"], "venue": "vol. 28, p. 12471255, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["W. Wang", "R. Arora", "K. Livescu", "J.A. Bilmes"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Institute of Electrical and Electronics Engineers (IEEE), apr 2015. [Online]. Available: https://doi.org/10.1109%2Ficassp.2015.7178840", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "BL-Database: A French audiovisual database for speech driven lip animation systems", "author": ["Y. Benezeth", "G. Bachman", "G. Le-Jan", "N. Souvira\u00e0-Labastie", "F. Bimbot"], "venue": "INRIA, Research Report RR-7711, Aug. 2011. [Online]. Available: https: //hal.inria.fr/inria-00614761", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust real-time object detection", "author": ["P. Viola", "M. Jones"], "venue": "International Journal of Computer Vision, vol. 4, no. 34\u201347, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Real time face and object tracking as a component of a perceptual user interface", "author": ["G.R. Bradski"], "venue": "Proceedings of the 4th IEEE Workshop on Applications of Computer Vision (WACV\u201998), ser. WACV \u201998. Washington, DC, USA: IEEE Computer Society, 1998, pp. 214\u2013. [Online]. Available: http://dl.acm.org/citation.cfm?id=521384.836819", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Interpreting face images using active appearance models", "author": ["G. Edwards", "C. Taylor", "T. Cootes"], "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition. Institute of Electrical and Electronics Engineers (IEEE). [Online]. Available: https://doi.org/10.1109%2Fafgr. 1998.670965", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Readings in speech recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "A. Waibel and K.-F. Lee, Eds. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1990, ch. Dynamic Programming Algorithm Optimization for Spoken Word Recognition, pp. 159\u2013165. [Online]. Available: http://dl.acm.org/citation.cfm?id= 108235.108244", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "Evaluating speech features with the minimal-pair abx task: Analysis of the classical mfc/plp pipeline", "author": ["T. Schatz", "V. Peddinti", "F. Bach", "A. Jansen", "H. Hermansky", "E. Dupoux"], "venue": "INTER- SPEECH 2013: 14th Annual Conference of the International Speech Communication Association, 2013, pp. 1\u20135.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating speech features with the minimal-pair abx task (ii): Resistance to noise", "author": ["X.-N.C.F.H.T. Schatz", "V. Peddinti", "E.Dupoux"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantitative methods for comparing featural representations", "author": ["E. Dunbar", "G. Synnaeve", "E. Dupoux"], "venue": "Proceedings of the 18th International Congress of Phonetic Sciences, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep scattering spectrum \u2014 deep siamese network pipeline for unsupervised acoustic modeling", "author": ["N. Zeghidour", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Institute of Electrical and Electronics Engineers (IEEE), mar 2016. [Online]. Available: https://doi.org/10.1109% 2Ficassp.2016.7472622", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint learning of speaker and phonetic similarities with siamese networks", "author": ["N. Zeghidour", "G. Synnaeve", "N. Usunier", "E. Dupoux"], "venue": "Interspeech 2016, pp. 1295\u20131299, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1409.0473, 2014. [Online]. Available: http://arxiv.org/abs/ 1409.0473", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Audio-visual integration is illustrated clearly by the McGurk effect [1]: the lip movements corresponding to [g], presented together with audio corresponding to a [b], is perceived as an intermediate sound, identified as [d], by many subjects.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Such interactions between modalities have been documented in 5-month-old infants [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 2, "context": "Such interactions between modalities have been documented in 5-month-old infants [2, 3].", "startOffset": 81, "endOffset": 87}, {"referenceID": 3, "context": "The visual channel for speech is poorer than the auditory channel, but provides information which can be complementary, especially regarding place of articulation [4] (for example, between the coronal and labial consonants [d] and [b]).", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "In particular, we will use a Siamese DNN architecture which feeds on word-level side information (the fact that two words are the same or different: [5, 6, 7, 8]), which demonstrably can be discovered automatically from continuous speech using spoFigure 1: Multi-task setting.", "startOffset": 149, "endOffset": 161}, {"referenceID": 5, "context": "In particular, we will use a Siamese DNN architecture which feeds on word-level side information (the fact that two words are the same or different: [5, 6, 7, 8]), which demonstrably can be discovered automatically from continuous speech using spoFigure 1: Multi-task setting.", "startOffset": 149, "endOffset": 161}, {"referenceID": 6, "context": "In particular, we will use a Siamese DNN architecture which feeds on word-level side information (the fact that two words are the same or different: [5, 6, 7, 8]), which demonstrably can be discovered automatically from continuous speech using spoFigure 1: Multi-task setting.", "startOffset": 149, "endOffset": 161}, {"referenceID": 7, "context": "In particular, we will use a Siamese DNN architecture which feeds on word-level side information (the fact that two words are the same or different: [5, 6, 7, 8]), which demonstrably can be discovered automatically from continuous speech using spoFigure 1: Multi-task setting.", "startOffset": 149, "endOffset": 161}, {"referenceID": 6, "context": "ken term discovery ([7, 9]).", "startOffset": 20, "endOffset": 26}, {"referenceID": 8, "context": "ken term discovery ([7, 9]).", "startOffset": 20, "endOffset": 26}, {"referenceID": 9, "context": "Classical audio-visual ASR systems use an audio-visual corpus to build complex supervised classifiers able to have an efficient phoneme representation [10, 11].", "startOffset": 151, "endOffset": 159}, {"referenceID": 10, "context": "Classical audio-visual ASR systems use an audio-visual corpus to build complex supervised classifiers able to have an efficient phoneme representation [10, 11].", "startOffset": 151, "endOffset": 159}, {"referenceID": 11, "context": "Some other studies involve both geometry-based and appearance-based features to build less complex models [12] but rely on an important upfront knowledge of optimal features.", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "A more recent architecture is the WLAS [13], which yields statear X iv :1 70 4.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "As discussed in [15], the supervised classification performance obtained on features is not a reliable indicator of the performance of unsupervised algorithms.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Another relevant model is the DCCA [16, 17] which can learn complex non-linear transformations of two data views to give a highly correlated embedding representation.", "startOffset": 35, "endOffset": 43}, {"referenceID": 15, "context": "Another relevant model is the DCCA [16, 17] which can learn complex non-linear transformations of two data views to give a highly correlated embedding representation.", "startOffset": 35, "endOffset": 43}, {"referenceID": 5, "context": "We use an ABnet type architecture [6], a particular type of Siamese network that allows learning phonetic-level embeddings from word-level annotations.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Such an architecture previously showed good performance [7] in the context of the Zero Resource Speech Challenge 2015 [18].", "startOffset": 56, "endOffset": 59}, {"referenceID": 16, "context": "We use the Blue-Lips database [19], an audio-visual speech corpus composed of 238 French sentences read by 16 speakers of Hexagonal French.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "First, we use dimensionality-reduced pixels from the region of the image corresponding to the mouth, identified using a Haar cascade classifier [20] to detect the mouth at the first frame and the meanshift algorithm [21] to track the mouth throughout the rest of the video.", "startOffset": 144, "endOffset": 148}, {"referenceID": 18, "context": "First, we use dimensionality-reduced pixels from the region of the image corresponding to the mouth, identified using a Haar cascade classifier [20] to detect the mouth at the first frame and the meanshift algorithm [21] to track the mouth throughout the rest of the video.", "startOffset": 216, "endOffset": 220}, {"referenceID": 9, "context": "We found that this did not reduce performance, consistent with [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Second, we concatenate these video features with lip landmarks, extracted using the active apperance model [22], a facial alignment algorithm which gives the shape of the mouth in 20 two-dimensional points.", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "x1 and x2 represent stacks of frames that are in correspondence across the two words: for same-word pairs, this corresponence is the result of an alignment using dynamic time warping (DTW) [23], and, for different-word pairs, an alignment along the diagonal.", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": "To evaluate the phonetic discriminability of our embeddings, we use an ABX discrimination task [24, 25].", "startOffset": 95, "endOffset": 103}, {"referenceID": 22, "context": "To evaluate the phonetic discriminability of our embeddings, we use an ABX discrimination task [24, 25].", "startOffset": 95, "endOffset": 103}, {"referenceID": 23, "context": "The second analysis measures how well the learned representations code individual phonological features, assessing the parallelism of the representations [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "See [26] for details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 5, "context": "This was already demonstrated for the acoustic modality [6, 27, 28]; we demonstrate that ABnet also improves the visual and concatenated input representations.", "startOffset": 56, "endOffset": 67}, {"referenceID": 24, "context": "This was already demonstrated for the acoustic modality [6, 27, 28]; we demonstrate that ABnet also improves the visual and concatenated input representations.", "startOffset": 56, "endOffset": 67}, {"referenceID": 25, "context": "This was already demonstrated for the acoustic modality [6, 27, 28]; we demonstrate that ABnet also improves the visual and concatenated input representations.", "startOffset": 56, "endOffset": 67}, {"referenceID": 26, "context": "This suggests future research incorporating into our models a gating or attention system [29] that would learn when to use each modality, or both, in order to dynamically ignore uninformative features at test time.", "startOffset": 89, "endOffset": 93}], "year": 2017, "abstractText": "Recent works have explored deep architectures for learning multimodal speech representation (e.g. audio and images, articulation and audio) in a supervised way. Here we investigate the role of combining different speech modalities, i.e. audio and visual information representing the lips? movements, in a weakly supervised way using Siamese networks and lexical same-different side information. In particular, we ask whether one modality can benefit from the other to provide a richer representation for phone recognition in a weakly supervised setting. We introduce mono-task and multi-task methods for merging speech and visual modalities for phone recognition. The mono-task learning consists in applying a Siamese network on the concatenation of the two modalities, while the multi-task learning receives several different combinations of modalities at train time. We show that multi-task learning enhances discriminability for visual and multimodal inputs while minimally impacting auditory inputs. Furthermore, we present a qualitative analysis of the obtained phone embeddings, and show that cross-modal visual input can improve the discriminability of phonological features which are visually discernable (rounding, open/close, labial place of articulation), resulting in representations that are closer to abstract linguistic features than those based on audio only.", "creator": "LaTeX with hyperref package"}}}