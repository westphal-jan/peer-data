{"id": "1704.06942", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Population Seeding Techniques for Rolling Horizon Evolution in General Video Game Playing", "abstract": "While Monte Carlo Tree Search and closely related methods have dominated General Video Game Playing, recent research has demonstrated the promise of Rolling Horizon Evolutionary Algorithms as an interesting alternative. However, there is little attention paid to population initialization techniques in the setting of general real-time video games. Therefore, this paper proposes the use of population seeding to improve the performance of Rolling Horizon Evolution and presents the results of two methods, One Step Look Ahead and Monte Carlo Tree Search, tested on 20 games of the General Video Game AI corpus with multiple evolution parameter values (population size and individual length). An in-depth analysis is carried out between the results of the seeding methods and the vanilla Rolling Horizon Evolution. In addition, the paper presents a comparison to a Monte Carlo Tree Search algorithm. The results are promising, with seeding able to boost performance significantly over baseline evolution and even match the high level of play obtained by the Monte Carlo Tree Search.", "histories": [["v1", "Sun, 23 Apr 2017 15:53:29 GMT  (871kb,D)", "http://arxiv.org/abs/1704.06942v1", "Proceedings of the IEEE Conference on Evolutionary Computation 2017"]], "COMMENTS": "Proceedings of the IEEE Conference on Evolutionary Computation 2017", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["rauca d gaina", "simon m lucas", "diego perez-liebana"], "accepted": false, "id": "1704.06942"}, "pdf": {"name": "1704.06942.pdf", "metadata": {"source": "CRF", "title": "Population Seeding Techniques for Rolling Horizon Evolution in General Video Game Playing", "authors": ["Raluca D. Gaina", "Simon M. Lucas"], "emails": ["rdgain@essex.ac.uk", "sml@essex.ac.uk", "dperez@essex.ac.uk"], "sections": [{"heading": null, "text": "The authors all seem to agree that this is a major challenge, the importance of which cannot be denied, and which focuses on the development of artificial intelligence agents that are able to achieve high performance in any previously unknown environment. Games make an excellent domain for testing AI techniques, which are due to their changing complexity and wide range of problems. In addition, experiments can be carried out in a limited scenario, with minimal cost for testing AI techniques."}, {"heading": "II. RELEVANT RESEARCH", "text": "In fact, it is the case that most people who have lived in the United States for the last ten years are unable to understand the world and what it is all about. (...) In fact, it is the case that the people of the United States and other parts of the world are able to understand the world. (...) It is the case that the people of the United States and other parts of the world are not able to understand and understand the world. (...) It is as if they are able to understand and understand the world. (...) It is as if they do not understand the world what they are doing. (...) It is as if they are in the world and as if they are in the world and as if they are in the world and in the world and in the world and in the world, in the world and in the world, in the world and in the world, in the world and in the world, in the world and in the world, in the world and in the world, in the world, in the world and in the world."}, {"heading": "III. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. General Video Game AI", "text": "GVGAI aims to provide a framework for benchmarking general Artificial Intelligence Agents. It currently offers a total of 140 games, of which 100 are singles and 40 are two-player, some of which are stochastical and all of which are in real time. The study focuses on the single-player framework. The games are played by the agents in black box mode, without knowledge of the rules (e.g. different scoring systems, conditions for terminating the game or types of objects in the game - NPCs, portals, resources), but are able to query the current game status for information on game objects. In addition, future possible states can be simulated using a forward model (FM) that requires an action that the agent wants to perform and returns the game state resulting from this action. However, it should be noted that all states sent back by FM may not be an accurate representation of the real game, as they are assigned such due to stoicity."}, {"heading": "B. Evolutionary Algorithms", "text": "The algorithms used in this study are based on the Rolling Horizon Evolutionary Algorithm (RHEA) [10], which encodes individuals as sequences of action; the term \"Rolling Horizon\" refers to the fact that the first action of the developed plan is performed in one game step, then the plan is re-evaluated and adapted, looking one step further into the future and thus slowly broadening the \"horizon.\" Each individual in the EA is evaluated in a similar way: the actions are simulated using a forward model (FM) that follows the sequence; the value associated with the state reached at the end (approximately through a heuristic function) is evaluated as the fitness value of the individual.The evolutionary process of the RHEA consists of several iterations (dictated by a fixed number or a time or storage budget, for example) starting with population initialization; then mutations, crossovers and other evolutionary methods are used to change individuals and bring about new decisions at the end of the game, based on the best outcome of the population."}, {"heading": "C. Monte Carlo Tree Search", "text": "Monte Carlo Tree Search (MCTS) is a search-based technique that consists of four steps that are repeated repeatedly until a predefined budget is reached (e.g., a number of iterations, saving or time).The action returned at the end of the process is the child of the root node, which is considered the best by a recommendation policy (e.g., the most visited child).In the first step of the process, MCTS selects a non-terminal and not yet fully extended leaf of the tree over a tree guideline. Second, a child of the selected node is added to the tree. Third, it predicts by using the new child as the root of a Monte Carlo simulation using the FM provided by the system. Finally, heuristics are used to evaluate the state reached at the end of the simulation step and all parents of the selected node down to the root of the tree are updated with this value. Finally, the commentary algorithm used in this game implements the flow technology, effectively, with the slime-open nodes."}, {"heading": "IV. APPROACH AND EXPERIMENTAL SETUP", "text": "The aim of this paper is to investigate whether initializing the population of an evolutionary algorithm with individuals who are better than random results in an improvement in performance when applied to general video gaming. This hypothesis was tested by using 2 different initialization techniques to design variants of vanilla RHEA, the basic algorithm in this study, A-Vanilla. The B-1SLA-S algorithm is a seeding variant that uses a one-step look-upside technique to select a better starting point in the search space. C-MCTS-S algorithm uses Monte Carlo tree search to use the RHEA for better analysis of the search space. The performance of a fourth algorithm was compared with the RHEA variants, an open-loop tree search (D-MCTS algorithm), in which the simple implementation of the GVGAI competition test was tested to calculate the total CTS (The HEMTS = different value of the HEMP algorithm)."}, {"heading": "A. Games", "text": "All the algorithms were tested on the same subset of 20 single players of the current GVGAI Corpus. Since the goal was to observe performance in different game types, two classifications were used to determine a set of games suitable for this experiment. Mark Nelson presented a large-scale analysis of similar games in 62 games based on the performance of this algorithm. Bontrager and al. used clustering techniques in 49 GVGAI games. The 20 games selected for this experiment were lined up by both works for a balanced set of 10 stochastic and 10 deterministic games."}, {"heading": "C. One Step Look Ahead Seeding (Algorithm B-1SLA-S)", "text": "The One Step Look Ahead (1SLA) algorithm is a simple technique that comprehensively scans the actions available from the current state and assigns each one a Q value corresponding to the approximate value of the game state reached after each action is performed (the value is defined by the same heuristics used by the RHEA), and then selects the action with the highest Q value to execute iteration.The B-1SLA-S algorithm uses the same evolutionary process as A-Vanilla described above, but the first person in the initial population is the solution recommended by the 1SLA technique. Iterations of the algorithm are performed, one for each gene in the individual: An exhaustive search is performed by all actions available from the current state, the game state is advanced using the forward model, the best action found is repeated and the process is repeated until either the end of the individual or the end of the game is reached. In the second case, the rest of the algorithm is randomly added to the current action, if the first action is the current one, and the action is not the current one."}, {"heading": "D. Monte Carlo Tree Search Seeding (Algorithm C-MCTS-S)", "text": "The C-MCTS-S algorithm splits the obtained budget and uses half of it to perform a Monte Carlo tree search in the current game state first, according to the steps described in Section III-C. The roll-out depth is set to the same value as the individual length in A-Vanilla, and the UCB1 formula (where the constant C assumes the value \u221a 2) is applied as a tree guideline (see Equation 1). a) = argmax a-A (s) {Q (s, a) + C \u221a lnN (s) N (s, a)} (1) The first person in the original RHEA population is then seeded with the solution recommended by MCTS. Only the first K-relevant nodes are selected by traversing the tree with the most frequently visited actions (the same method used by the D-MCTS algorithm in selecting its final action to play). One node is relevant if no one person has visited the rest of the GCTS (at least 3 times)."}, {"heading": "V. RESULTS AND DISCUSSION", "text": "The analysis in this section measures the statistical significance of the results for each game (p-value = 0.05) on two performance indicators: win rate and game outcomes. In general, both seeding techniques improve the performance of the vanilla algorithm much more when the population size and individual length are small than when they increased. This is in line with the results of the study conducted by Gaina et. al [23], in which Random Search (RS) was found to be the best algorithm within the limited budget offered. The more the parameter values increase towards the RS, the less impact of the seeding can be observed. Table V provides an overall win comparison between the two seeding variants and vanilla RHEA across all games and configurations. The further down the table, the number of games in which one algorithm was significantly better than the other two, resulting in a total number of unique games in which a significant improvement was noted."}, {"heading": "A. Overall Seeding Comparison", "text": "The general trend observed in this study is that the MCTS seed variant performs significantly better in all configurations than the other two algorithms, A-Vanilla and B-1SLA-S, in only 4 games, both in terms of win percentage and score. It should be noted that there have been a lower number of games in which A-Vanilla or B-1SLA-S consistently performed significantly better than C-MCTS-S: games with indexes 36 and 91 (Escape and Wait for Breakfast) both in terms of win percentage and score and game with index 50 (Intersection) for score. This is due to the poor performance of MCTS in these games, which is improved in the seeding algorithm over D-MCTS. In addition, the MCTS seeding shows a steady improvement and the game with indexes 0, 13 and 22 (Aliens, butterflies and a gain rate of only 0.0001%)."}, {"heading": "B. Pair-wise Seeding Comparison", "text": "In this context, it should be noted that the persons in question are people who are capable, who are capable, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who are in a position, who is in a position, who is in a position, who is in a position to invest the situation."}, {"heading": "C. MCTS Comparison", "text": "Although the results suggest that the C-MCTS-S algorithm is the best in this situation, an analysis has been carried out against a pure Monte Carlo tree search technique to validate the results. This comparison can be seen in Table III, where the values show the number of games in which an algorithm was significantly better than the other two in terms of win rate (and results in parentheses), resulting in the sum of unique games across all configurations.The bottom line of Table III shows the number of games in which C-MCTS-S was not the best algorithm, but nevertheless achieved higher performance than A-Vanilla. While A-Vanilla consistently achieves significantly more wins and higher scores in its best games (indexes 36, 91 and 50), it should be stressed that the comparison between these games MCTS-MTS and MTS-S performs significantly better than the comparison group MTS-MTS-S."}, {"heading": "VI. CONCLUSION", "text": "This paper presented an experiment that focused on observing how a better than random population initialization technique influences the performance of Rolling Horizon Evolutionary Algorithms (RHEA) in General Video Game Playing.First, a one-step look ahead method that simply performs a comprehensive search through all available actions and selects the best one at each game step. Second, a Monte Carlo Tree Search (MCTS) that took half the budget to process the game from its current state and recommend a solution that serves as a starting point for the evolutionary process. Experiments were conducted in a balanced set of 20 games from the General Video Game AI Framework and used various configurations of RHEA parameters (Population Size (P) and Individual Length (L).The results suggest that both seeding variants provide a significant improvement in performance, with both the win rate and in-game results being zero, especially when the P- and L values are low."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was funded by the EPSRC Centre for Doctoral Training in Intelligent Games and Game Intelligence (IGGI) EP / L015846 / 1."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "While Monte Carlo Tree Search and closely related<lb>methods have dominated General Video Game Playing, recent<lb>research has demonstrated the promise of Rolling Horizon<lb>Evolutionary Algorithms as an interesting alternative. However,<lb>there is little attention paid to population initialization techniques<lb>in the setting of general real-time video games. Therefore, this<lb>paper proposes the use of population seeding to improve the<lb>performance of Rolling Horizon Evolution and presents the<lb>results of two methods, One Step Look Ahead and Monte Carlo<lb>Tree Search, tested on 20 games of the General Video Game<lb>AI corpus with multiple evolution parameter values (population<lb>size and individual length). An in-depth analysis is carried out<lb>between the results of the seeding methods and the vanilla<lb>Rolling Horizon Evolution. In addition, the paper presents a<lb>comparison to a Monte Carlo Tree Search algorithm. The<lb>results are promising, with seeding able to boost performance<lb>significantly over baseline evolution and even match the high<lb>level of play obtained by the Monte Carlo Tree Search.", "creator": "LaTeX with hyperref package"}}}