{"id": "1704.06945", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "General Video Game AI: Learning from Screen Capture", "abstract": "General Video Game Artificial Intelligence is a general game playing framework for Artificial General Intelligence research in the video-games domain. In this paper, we propose for the first time a screen capture learning agent for General Video Game AI framework. A Deep Q-Network algorithm was applied and improved to develop an agent capable of learning to play different games in the framework. After testing this algorithm using various games of different categories and difficulty levels, the results suggest that our proposed screen capture learning agent has the potential to learn many different games using only a single learning algorithm.", "histories": [["v1", "Sun, 23 Apr 2017 16:08:06 GMT  (1372kb,D)", "http://arxiv.org/abs/1704.06945v1", "Proceedings of the IEEE Conference on Evolutionary Computation 2017"]], "COMMENTS": "Proceedings of the IEEE Conference on Evolutionary Computation 2017", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kamolwan kunanusont", "simon m lucas", "diego perez-liebana"], "accepted": false, "id": "1704.06945"}, "pdf": {"name": "1704.06945.pdf", "metadata": {"source": "CRF", "title": "General Video Game AI: Learning from Screen Capture", "authors": ["Kamolwan Kunanusont", "Simon M. Lucas"], "emails": ["kkunan@essex.ac.uk", "sml@essex.ac.uk", "dperez@essex.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTION: AGI IN GAMES The main goal of Artificial Intelligence is to develop automated Agent Q, which can solve real-world problems on the same level as humans. These agents can be divided into two major types: domain-specific agents and general agents. Domain-specific agents focus on solving only one problem, or a few, problems at the same or better level than qualified or trained people. For video games, there have been several competitions that promoted the development of AI players for specific games. Examples are the Ms PacMan [1] contest, which took place between 2008 and 2011, and the Mario AI contest [2], which took place between 2009 and 2012. Apart from being excellent at performing a single qualified task, people are also able to solve several different types of problems efficiently. For example, in video game terms, people limit their ability to be experts at not being able to solve just one or a few types of video games."}, {"heading": "II. RELATED WORK", "text": "The first attempt to apply AGI within the gaming area is General Game Playing (GGP) [11], which is an artificial intelligence platform for games. Later, in 2013, ALE was proposed by Bellemere et al. [10] as a framework for assessing video games, with some of the video games considered tasks in real time. In the same year, General Video Game Playing (GVGP) was defined by extending from GGP [5]. In contrast, GVGP focuses on general representatives of video games that require more interaction with players."}, {"heading": "III. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Convolutional neural network", "text": "Convolutionary neural networks are a type of neural network designed to extract image-like data. It was first introduced in 1998 [22], but gained interest after successful application as part of the ImageNet classification algorithm [23]. For each folding layer, each neuron is responsible for a value of the input data (i.e. one pixel RGB value for image input).The idea is that images of the same category often have the same characteristics in certain local areas: For example, images of dogs most likely contain dog ears in a particular place.The image pixels representing dog ears exhibit the same or similar characteristics even though they are not located in the same places in the images. Folding layers extract this by passing data from the same adjacent areas into the same neurons as shown in Figure 1. Each block size is referred to as a \"core size\" and the gap between two blocks is called a \"stripe size.\" Neursize 1 is used in each of these images (max x and max x in each resolution)."}, {"heading": "B. Q-learning", "text": "Q-Learning is a non-political time difference learning algorithm in Reinforcement Learning [19] (RL), a machine learning paradigm that aims to find the best policy to respond to the problem to be solved by maximizing the reward signals given from the environment for each available action in a given situation. Q-Learning is a model-free RL technique that enables online learning and updates the current state values according to the maximum return values of all states after available action is provided. A key challenge in Reinforcement Learning is the balance between exploitation and exploration. Exploitation selects the best measures found so far, while exploration selects another alternative option to improve current policy. An easy solution is the application of a greedy policy that selects a random action. In our implementation, it was started with 1 and decreases by 0.1 in each time step, stabilizing it at 0.1."}, {"heading": "C. Deep Q-Network", "text": "Deep Q-Network consists of two components: a deep convolution neural network and Q-Learning. A CNN is responsible for extracting functions for the images and determining the best action to take. The Q-Learning component takes these extracted functions and evaluates state-action values of each frame.1) Network structure: The Deep CNN proposed by Mnih et. al. [9] consists of 6 layers: 1 input layer, 4 hidden layers and 1 output layer, sequentially connected. The input layer receives pre-processed screenshots and forwards them to the first hidden layer. The first three hidden layers are Convolutionary layers and the last is a fully connected dense layer. The output layer has the same number of neurons as the number of possible actions. 2) Method: The method has two main units: preprocessing and training. Pre-processing turns a 210 x 160 pixel screen Y-84 x into a pixel."}, {"heading": "D. GVG-AI", "text": "1) GVG AI Framework: The GVG AI Frame contains a total of 140 games, 100 of which are single-player games and the rest, 2-player games. In this paper, the developed agent was tested with some of the single-player games alone. Video games within this framework were all implemented using a Java port from Py-vgdl developed by T. Schaul [6]. All game components, including avatars and physical objects, are located within a 2-dimensional rectangular frame. The 2014-2016 GVG AI Competition featured only the \"planning track,\" which is divided into single-player and 2-player settings. Submitted agents are not allowed to replay games, instead a forward model is given for future state simulation. All government information is encapsulated in this model and the agent can select an action before performing the action in-game."}, {"heading": "IV. PROPOSED METHOD", "text": "We present a GVG-AI Screen Capture Learning Agent based on a Deep Q network. We modified the framework to allow repetitions for all created agents, and another change was to disable all time limits, including 1 second for constructing an agent and up to 40 ms action determination for each time step. Therefore, our agent is granted unlimited time for initialization and learning steps."}, {"heading": "A. Deep Q-Network for GVG-AI", "text": "In fact, it is so that it is a matter of a way in which one blames oneself and others, in which one blames oneself. (...) It is not as if one wants to look into the cards. (...) It is not as if one can look into the cards. (...) It is as if one wants to look into the cards. \"(...)\" It is as if one wants to look into the cards. \"(...)\" It is as if one wants to look into the cards. \"(...)\" It is as if one wants to look into the cards. \"(...)\" It is as if one wants to look into the cards. \"(...)\" It is as if one wants to look into the cards. \"(...)\" (...) \"it is like that.\" (...) \"it is like that.\" (...). \"It is like that.\" (...). \"It is.\" It is like that. \"(...).\" It is. \"It is like that.\" (...). \"It is like that.\" (...). \"It is.\" It is like that. \"(...\" it. \"It is.\"). \"It is.\" It is. \"It is.\" (...). \"it.\" (). \"It is like.\" (). \"It is.\" (). \"It is.\" it. \"it.\" (). \"It is.\" (). \"it.\" It is. (). (). \"It is. ().\" it. \"It is.\" it. \"it.\" (). \"it. (). ().\" It is. (). \"it.\" It is. \"). (). (It is. (It is.\"). (It is. \")."}, {"heading": "V. EXPERIMENT RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Parameter tuning", "text": "All matched parameters are described in Table II. Escape level 0 was selected to make parameter adjustments, because the nature of the game is simple, the size of the game screen is not large, and only 16 moves are needed to win the game optimally. To measure performance, three criteria were used in each set: the number of steps taken to win, winning percentages for each episode number, and winning percentages calculated since the first training. For each criterion, the average, the best value, and the optimal (if applicable) value were measured and compared. We found that among the selected values, the batch size = 400, the core size = 5 x 5 connected by 3 x 3, dropout = 0, and no subsampling level yielded the best performance in this game. Next, we applied this parameter set to create a CNN embedded in the player agent to play 6 selected games."}, {"heading": "B. Testing with games", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "This paper introduces for the first time a screen-forming learning agent for the Artificial Intelligence for General Video Games (GVG-AI) framework that operates within the GVG-AI framework, such as support for any screen size and non-visualizable game modes. Convolution neural network parameters were tuned as a pre-experiment before being implemented as part of the game agent in the real experiment. Results indicate that our learning agent was able to solve both static and stochastic games, as the cumulative win percentage in static games and the accumulative average score in stochastic games increased with increasing number of games played, suggesting that the agent acquired during the previous games applied knowledge to adapt to later replays in the same game. Several future work is possible in the improvements of the agent. Currently, the same CNN structure has been applied to each game tried."}], "references": [{"title": "Ms Pac-man Competition", "author": ["S.M. Lucas"], "venue": "ACM SIGEVOlution, vol. 2, no. 4, pp. 37\u201338, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "The 2009 Mario AI Competition", "author": ["J. Togelius", "S. Karakovskiy", "R. Baumgarten"], "venue": "IEEE Congress on Evolutionary Computation, pp. 1\u20138, IEEE, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "The 2014 General Video Game Playing Competition", "author": ["D. Perez", "S. Samothrakis", "J. Togelius", "T. Schaul", "S. Lucas", "A. Cou\u00ebtoux", "J. Lee", "C.-U. Lim", "T. Thompson"], "venue": "2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "General Video Game Playing", "author": ["J. Levine", "C.B. Congdon", "M. Ebner", "G. Kendall", "S.M. Lucas", "R. Miikkulainen", "T. Schaul", "T. Thompson"], "venue": "Dagstuhl Follow-Ups, vol. 6, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "A Video Game Description Language for Model-based or Interactive Learning", "author": ["T. Schaul"], "venue": "Computational Intelligence in Games (CIG), 2013 IEEE Conference on, pp. 1\u20138, IEEE, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Ms. Pac-man Competition: Screen Capture Version.", "author": ["S.M. Lucas"], "venue": "http://cswww.essex.ac.uk/staff/sml/pacman/PacManContest.html,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "ViZ- Doom: A Doom-based AI Research Platform for Visual Reinforcement Learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Ja\u015bkowski"], "venue": "arXiv preprint arXiv:1605.02097, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level Control through Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "General Game Playing", "author": ["M. Genesereth", "M. Thielscher"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 8, no. 2, pp. 1\u2013229, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Trans. on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Knowledge-based Fast Evolutionary MCTS for General Video Game Playing", "author": ["D. Perez", "S. Samothrakis", "S. Lucas"], "venue": "2014 IEEE Conference on Computational Intelligence and Games, pp. 1\u20138, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Open Loop Search for General Video Game Playing", "author": ["D. Perez Liebana", "J. Dieskau", "M. Hunermund", "S. Mostaghim", "S. Lucas"], "venue": "Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, pp. 337\u2013344, ACM, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "General Video Game AI: Competition, Challenges and Opportunities", "author": ["D. Perez-Liebana", "S. Samothrakis", "J. Togelius", "S.M. Lucas", "T. Schaul"], "venue": "30 AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Neuroevolution for General Video Game Playing", "author": ["S. Samothrakis", "D. Perez-Liebana", "S.M. Lucas", "M. Fasli"], "venue": "2015 IEEE Conference on Computational Intelligence and Games (CIG), pp. 200\u2013207, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Networks and Deep Learning", "author": ["M.A. Nielsen"], "venue": "2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Auto-encoder Neural Networks in Reinforcement Learning", "author": ["S. Lange", "M. Riedmiller"], "venue": "The 2010 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138, IEEE, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "General Video Game Playing with Goal Orientation", "author": ["B. Ross"], "venue": "PhD thesis, Master\u2019s thesis, University of Strathclyde,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pp. 1097\u20131105, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Reuse of neural modules for general video game playing", "author": ["A. Braylan", "M. Hollenbeck", "E. Meyerson", "R. Miikkulainen"], "venue": "arXiv preprint arXiv:1512.01537, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Object-model transfer in the general video game domain", "author": ["A. Braylan", "R. Miikkulainen"], "venue": "Twelfth Artificial Intelligence and Interactive Digital Entertainment Conference, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Examples are the Ms PacMan [1] competition, which took place between 2008 and 2011, and the Mario AI competition [2], held between 2009 and 2012.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Examples are the Ms PacMan [1] competition, which took place between 2008 and 2011, and the Mario AI competition [2], held between 2009 and 2012.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "General Video Game Artificial Intelligence or GVG-AI [4] is a General Video Game Playing [5] framework with this characteristic.", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "General Video Game Artificial Intelligence or GVG-AI [4] is a General Video Game Playing [5] framework with this characteristic.", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "Video Game Description Language was applied [6] to easily design and develop new video games, increasing the number of games from 30 games (when it was first started in 2014) to 140 games at the time this paper is written (January 2017).", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "This inspired attempts to develop video game automated players using mainly screen information as an input, such as the PacMan screen capture competition [7] and VizDoom [8].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "This inspired attempts to develop video game automated players using mainly screen information as an input, such as the PacMan screen capture competition [7] and VizDoom [8].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The developed agent was evaluated using the Arcade Learning Environment (ALE) [10] framework.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "The first attempt to apply AGI within the game domain is General Game Playing (GGP) [11], which is a platform for Artificial General Intelligence for games.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "[10] as a framework to evaluate Artificial General Intelligence, using some of the Atari 2600 video games as tasks to solve.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "In the same year, General Video Game Playing (GVGP) was defined by extending from GGP [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "The first GVGP competition and framework is General Video Game Artificial Intelligence or GVG-AI [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "The most popular algorithm applied was Monte Carlo Tree Search (MCTS) [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "There have been attempts to modify MCTS to work more efficiently with GVG-AI, such as using evolutionary algorithm with knowledge-based fitness function to guide rollouts [13], or storing statistical information in tree nodes instead of pure state details [14].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "There have been attempts to modify MCTS to work more efficiently with GVG-AI, such as using evolutionary algorithm with knowledge-based fitness function to guide rollouts [13], or storing statistical information in tree nodes instead of pure state details [14].", "startOffset": 256, "endOffset": 260}, {"referenceID": 13, "context": "There was a claim that GVG-AI will operate a new track called learning track to encourage learning agent development in near future [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Only one learning agent has been proposed so far, based on neuro-evolution [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "PacMan screen capture competition [7] and ALE, have screen capture tools embedded.", "startOffset": 34, "endOffset": 37}, {"referenceID": 15, "context": "Also, recently flourish attention in Deep learning [17], especially a spatial-based deep neural network called Convolutional Neural Network (CNN) [18], encouraged more adaptations of CNN usages in auto image feature extraction.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "Also, recently flourish attention in Deep learning [17], especially a spatial-based deep neural network called Convolutional Neural Network (CNN) [18], encouraged more adaptations of CNN usages in auto image feature extraction.", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "The first framework idea that combines deep learning and reinforcement learning in a visual learning task was proposed by Lange and Riedmiller [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "[9] proposed a breakthrough learning general agent for ALE.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Ross [21] applies sprite location information in grid observation to guide MCTS towards a new sprite type that never explored , although it is still a planning agent.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "The concept was first introduced in 1998 [22], but gained more interest after being successfully applied as a part of classifier algorithm for ImageNet [23].", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "The concept was first introduced in 1998 [22], but gained more interest after being successfully applied as a part of classifier algorithm for ImageNet [23].", "startOffset": 152, "endOffset": 156}, {"referenceID": 7, "context": "[9] consists of 6 layers: 1 input layer, 4 hidden layers and 1 output layer, connected sequentially.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Schaul [6].", "startOffset": 7, "endOffset": 10}, {"referenceID": 10, "context": "3) MCTS: Monte Carlo Tree Search (MCTS; [12]) is a tree search technique that builds an asymmetric tree in memory, biased towards the most promising parts of the search space, via sampling available actions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "[24] mentioned an idea of using immediate reward as a guidance of Reinforcement Learning, which inspired us to add collectible items to Labyrinth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25], where an agent trained to play one game could apply the knowledge learned when playing other similar games faster than learning from scratch.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "There was an attempt to use this same idea in GVG-AI framework to improve the accuracy of the forward model [26].", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "General Video Game Artificial Intelligence is a general game playing framework for Artificial General Intelligence research in the video-games domain. In this paper, we propose for the first time a screen capture learning agent for General Video Game AI framework. A Deep Q-Network algorithm was applied and improved to develop an agent capable of learning to play different games in the framework. After testing this algorithm using various games of different categories and difficulty levels, the results suggest that our proposed screen capture learning agent has the potential to learn many different games using only a single learning algorithm.", "creator": "LaTeX with hyperref package"}}}