{"id": "1704.07055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "k-FFNN: A priori knowledge infused Feed-forward Neural Networks", "abstract": "Recurrent neural network (RNN) are being extensively used over feed-forward neural networks (FFNN) because of their inherent capability to capture temporal relationships that exist in the sequential data such as speech. This aspect of RNN is advantageous especially when there is no a priori knowledge about the temporal correlations within the data. However, RNNs require large amount of data to learn these temporal correlations, limiting their advantage in low resource scenarios. It is not immediately clear (a) how a priori temporal knowledge can be used in a FFNN architecture (b) how a FFNN performs when provided with this knowledge about temporal correlations (assuming available) during training. The objective of this paper is to explore k-FFNN, namely a FFNN architecture that can incorporate the a priori knowledge of the temporal relationships within the data sequence during training and compare k-FFNN performance with RNN in a low resource scenario. We evaluate the performance of k-FFNN and RNN by extensive experimentation on MediaEval 2016 audio data (\"Emotional Impact of Movies\" task). Experimental results show that the performance of k-FFNN is comparable to RNN, and in some scenarios k-FFNN performs better than RNN when temporal knowledge is injected into FFNN architecture. The main contributions of this paper are (a) fusing a priori knowledge into FFNN architecture to construct a k-FFNN and (b) analyzing the performance of k-FFNN with respect to RNN for different size of training data.", "histories": [["v1", "Mon, 24 Apr 2017 06:54:23 GMT  (169kb,D)", "http://arxiv.org/abs/1704.07055v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sri harsha dumpala", "rupayan chakraborty", "sunil kumar kopparapu"], "accepted": false, "id": "1704.07055"}, "pdf": {"name": "1704.07055.pdf", "metadata": {"source": "CRF", "title": "K-FFNN: USING A PRIORI KNOWLEDGE IN FEED-FORWARD NEURAL NETWORKS", "authors": ["Sri Harsha Dumpala", "Rupayan Chakraborty", "Sunil Kumar Kopparapu"], "emails": ["d.harsha@tcs.com", "rupayan.chakraborty@tcs.com", "sunilkumar.kopparapu@tcs.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country, in which it is a country, in which it is a country and in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country and in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country,"}, {"heading": "2. HYPOTHESIS", "text": "We start with the hypothesis that FFNN, which is interspersed with prior knowledge of the temporal relationship between data, is similar to RNN in terms of performance.As seen in Figure 1, the primary difference between an RNN and an FFNN is the presence of the hidden layer feedback self-loop in RNN, which adds memory to the RNN network over time. The question we have to address is whether a regular FFNN fed with the sequence can function as well as a priori knowledge (i.e. k-FFNN). In other words, if we have some a priori knowledge of the sequence, we can use it without having to rely on RNN to learn it through its hidden layer feedback loop. This is particularly useful in scenarios where the training data is sparse and if we are aware of the sequential relationship between the FFNN and the performance of the RNN are similar to the extensive RNN hypothesis by confirming the media data."}, {"heading": "2.1. Background", "text": "We validate our hypothesis by considering a simple network configuration and deriving from it a series of expressions showing the relationship between k-FFNN and RNN. For simplicity, we are looking at a 4 \u2212 2 \u2212 \u2212 1 (input-hidden-output nodes) network configuration. In addition, we are looking at a data sequence of length 3. Let's look at the data presented in Table 1. Let's look more closely at the data presented in Table 1, if each ~ gi, j was dimension 4, then we would have used Table 2 as input-output data to train RNN. Let's assume that there is some temporal relationship between ~ g1.1, ~ g1.2, ~ g2.1 and ~ g2.2, ~ g2.3 that can be captured as shown in Table 3. Output v1 associated with ~ g1,1, ~ g1.2, ~ g1,3 is actually f (1) v1, f (2) v1, v1, v1, 1."}, {"heading": "2.2. RNN", "text": "In RNNs, the length of the input sequence (here T = 3), apart from the values of the input sequence, is always considered as a unit of measurement for the nets. In the case of RNNs, the output of the hidden layer is considered as ashtk = 1 1 + exp \u2212 \u03bb (\u2211 4 j = 1 (g j 1t) ((ih) wjk) + \u2211 2 h \u2032 = 1 (h t \u2212 1 h) (hh) wh \u2032 k))) (7) The output of RNN is considered as byo1 = 1 + exp \u2212 \u03bb (p \u2212 2 k = 1 (h T k) (ho) wk \u00b2 h)) (8) The error in the output estimation is = (o1 \u2212 v1) 2 (9) The error is propagated back by the length of the sequence, i.e. the backpropagation by time (BPTT) is used as a unit of measurement to modify the weights of RNNN."}, {"heading": "3. WORKING SCENARIO AND DATASET PREPARATION", "text": "It is. It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) In fact it is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (... \"it is.\" It is. \"It is.\" It is. \"It is.\" It is. \"It is.\" It is. \"It is. (...) It is. (... It is. It is. (...) It is. It is. It is. It is. (... It is. (...) It is. It is. It is. It is. It is. It is. (... It is. It is. It is. It is. (... It is. (...) It is. It is. It is. It is. It is. It is. It is. It is. (... It is. (...) it is. It is. It is. It is. It is. It is. It is. () It is. () it is. It is. It is. It is."}, {"heading": "4. EXPERIMENTAL VALIDATION", "text": "In all our experiments, we used the audio extract from 7571 video clips (MediaEval database), each lasting approximately n (n = 8 \u2212 10) seconds [14]. The database has a value (and excitation) value in the range [0, 5] for all 7571 videos, namely (ck; vk) for k = 1, 2, \u00b7 \u00b7 \u00b7, 7571. For each {ckj} k = 7571, j = nk = 1, j = 1, we extracted 384 features used for the Interspeech 2009 Emotion Challenge [15] using the openSMILE toolkit [16]. We used WEKA Toolkit [17] to reduce the feature method to 21 using the feature selection."}, {"heading": "4.1. Experimental Analysis", "text": "The performance of the proposed k-FFNN function on the hidden units is compared with the popular RNN architecture, i.e., simple FFSE systems (RNN with long-term short-term memory (LSTM) units and bidirectional RNN with LSTM (BLSTM) units).In this analysis, all k-FFNN and RNN systems are selected from 11 (half the sum of the input (i.e. 21) and output units (i.e., 1) to 44 (twice the number of input and output units) by varying the number of units from 11 (half the sum of the input and output values).Sigmoid (S in Table 8) is used as non-linear activation."}, {"heading": "5. CONCLUSIONS", "text": "FFNN Architecture looks at the temporal relationship that emerges in a data sequence, not as in the case of a voice signal. RNN Architecture through its design is able to implicitly learn the temporal correlations that exist between the data sequence. Whereas RNNs are beneficial if (a) one is not explicitly aware of the temporal relationship between the sequential data and (b) if there is a large amount of training data. In this paper we deal with the scenario when there is insufficient training data and when a priori temporal knowledge of the training data is explicitly known. In this paper we have shown how to incorporate explicitly known a priori temporal knowledge of the sequential data in order to improve the performance of the FFNN Architecture. We first compared the differences between a simple RNN and a FFNN and showed that the a priori knowledge of the NNN knowledge can in a certain sense contribute to the hidden layer feedback weights that can compensate for the training time capturing."}, {"heading": "6. REFERENCES", "text": "[1] Teuvo Kohonen, \"An introduction to neural computing,\" Neural Networks, vol. 1, no. 1, pp. 3-16, 1988. [2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\" IEEE Signal Processing Magazine, vol. 29, no. 2016, pp. 82-97, Nov 2012. [3] A. Graves, A. r. Mohamed, and G. Hinton \"Speech recognition with deep recurrent neural networks,\" in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, May 2013, pp. 6645- 6649."}], "references": [{"title": "An introduction to neural computing", "author": ["Teuvo Kohonen"], "venue": "Neural Networks, vol. 1, no. 1, pp. 3 \u2013 16, 1988.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. r. Mohamed", "G. Hinton"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, May 2013, pp. 6645\u2013 6649.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "ICML, 2014, vol. 14, pp. 1764\u20131772.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio", "author": ["Felix Weninger", "Fabien Ringeval", "Erik Marchi", "Bj\u00f6rn W. Schuller"], "venue": "Proceedings of the Twenty- Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, 2016, pp. 2196\u20132202.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Networks: A Comprehensive Foundation (3rd Edition), Prentice-Hall, Inc", "author": ["Simon Haykin"], "venue": "Upper Saddle River, NJ,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Neurocomputing: Foundations of research", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "chapter Learning Representations by Back-propagating Errors, pp. 696\u2013699. MIT Press, Cambridge, MA, USA, 1988.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Paul J. Werbos"], "venue": "Neural Networks, vol. 1, no. 4, pp. 339 \u2013 356, 1988.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Liris-accede: A video database for affective content analysis", "author": ["Y. Baveye", "E. Dellandra", "C. Chamaret", "L. Chen"], "venue": "IEEE Transactions on Affective Computing, vol. 6, no. 1, pp. 43\u201355, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "From crowdsourced rankings to affective ratings", "author": ["Y. Baveye", "E. Dellandra", "C. Chamaret", "L. Chen"], "venue": "2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW), 2014, pp. 1\u20136.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "The INTERSPEECH 2009 emotion challenge", "author": ["Bj\u00f6rn W. Schuller", "Stefan Steidl", "Anton Batliner"], "venue": "IN- TERSPEECH, 2009, pp. 312\u2013315.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Artificial neural networks are extensively used in all types of classification problems [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Subsequent advancements in the field of neural networks was adapted to build better speech processing systems [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "Recurrent neural network (RNN) is one such advancement in neural networks, which is being used extensively to solve various problems in speech processing [3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 3, "context": "Recurrent neural network (RNN) is one such advancement in neural networks, which is being used extensively to solve various problems in speech processing [3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 4, "context": "RNN has been successfully applied in speech emotion recognition [5].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "It is evident that a feed-forward neural network (FFNN) discriminatingly learns the patterns within the inputs, even from a low resource dataset [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "On the other side, deep neural networks like RNN has an inherent characteristic of learning and exploiting temporal relationships amongst the sequences [7, 8, 9], however RNN requires large training data to capture those correlations.", "startOffset": 152, "endOffset": 161}, {"referenceID": 7, "context": "On the other side, deep neural networks like RNN has an inherent characteristic of learning and exploiting temporal relationships amongst the sequences [7, 8, 9], however RNN requires large training data to capture those correlations.", "startOffset": 152, "endOffset": 161}, {"referenceID": 8, "context": "On the other side, deep neural networks like RNN has an inherent characteristic of learning and exploiting temporal relationships amongst the sequences [7, 8, 9], however RNN requires large training data to capture those correlations.", "startOffset": 152, "endOffset": 161}, {"referenceID": 9, "context": "This dataset is part of the LIRIS-ACCEDE dataset [11, 12] and consists of video clips of duration 8-12 seconds which have been annotated by viewers for their perceived emotion, in terms of arousal and valance.", "startOffset": 49, "endOffset": 57}, {"referenceID": 4, "context": "Note that the perceived emotion annotation is for the entire video clip in terms of valance and arousal value in the range [0, 5].", "startOffset": 123, "endOffset": 129}, {"referenceID": 10, "context": "According to the [13, 12], each video clip in the dataset has a fade in at the beginning of the video clip and and a fade out at the end of the video clip.", "startOffset": 17, "endOffset": 25}, {"referenceID": 9, "context": "According to the [13, 12], each video clip in the dataset has a fade in at the beginning of the video clip and and a fade out at the end of the video clip.", "startOffset": 17, "endOffset": 25}, {"referenceID": 7, "context": "For testing the hypothesis, we first extracted the audio from the original video clip and then segmented the audio into smaller non-overlapping 1 second duration, so a movie clip of n seconds (n \u2208 [8, 12]) duration, resulted in n audio clips each of 1 second duration.", "startOffset": 197, "endOffset": 204}, {"referenceID": 9, "context": "For testing the hypothesis, we first extracted the audio from the original video clip and then segmented the audio into smaller non-overlapping 1 second duration, so a movie clip of n seconds (n \u2208 [8, 12]) duration, resulted in n audio clips each of 1 second duration.", "startOffset": 197, "endOffset": 204}, {"referenceID": 4, "context": "Let {ck; ok} be the input output pair; where ok \u2208 [0, 5] can be either valence (vk) or arousal (ak) associated with the audio ck.", "startOffset": 50, "endOffset": 56}, {"referenceID": 4, "context": "The database has a valence (and arousal) value in the range [0, 5] for all the 7571 videos, namely (ck; vk) for k = 1, 2, \u00b7 \u00b7 \u00b7 , 7571 is available.", "startOffset": 60, "endOffset": 66}, {"referenceID": 11, "context": "For each {ckj} k=1,j=1 we extracted 384 features, which were used for Interspeech 2009 Emotion Challenge [15] using the openSMILE toolkit [16].", "startOffset": 105, "endOffset": 109}], "year": 2017, "abstractText": "Recurrent neural network (RNN) are being extensively used over feed-forward neural networks (FFNN) because of their inherent capability to capture temporal relationships that exist in the sequential data such as speech. This aspect of RNN is advantageous especially when there is no a priori knowledge about the temporal correlations within the data. However, RNNs require large amount of data to learn these temporal correlations, limiting their advantage in low resource scenarios. It is not immediately clear (a) how a priori temporal knowledge can be used in a FFNN architecture (b) how a FFNN performs when provided with this knowledge about temporal correlations (assuming available) during training. The objective of this paper is to explore k-FFNN, namely a FFNN architecture that can incorporate the a priori knowledge of the temporal relationships within the data sequence during training and compare k-FFNN performance with RNN in a low resource scenario. We evaluate the performance of k-FFNN and RNN by extensive experimentation on MediaEval 2016 audio data (Emotional Impact of Movies task). Experimental results show that the performance of k-FFNN is comparable to RNN, and in some scenarios k-FFNN performs better than RNN when temporal knowledge is injected into FFNN architecture. The main contributions of this paper are (a) fusing a priori knowledge into FFNN architecture to construct a k-FFNN and (b) analyzing the performance of k-FFNN with respect to RNN for different size of training data.", "creator": "LaTeX with hyperref package"}}}