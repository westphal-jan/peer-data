{"id": "1704.07075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Analysis of Vanilla Rolling Horizon Evolution Parameters in General Video Game Playing", "abstract": "Monte Carlo Tree Search techniques have generally dominated General Video Game Playing, but recent research has started looking at Evolutionary Algorithms and their potential at matching Tree Search level of play or even outperforming these methods. Online or Rolling Horizon Evolution is one of the options available to evolve sequences of actions for planning in General Video Game Playing, but no research has been done up to date that explores the capabilities of the vanilla version of this algorithm in multiple games. This study aims to critically analyse the different configurations regarding population size and individual length in a set of 20 games from the General Video Game AI corpus. Distinctions are made between deterministic and stochastic games, and the implications of using superior time budgets are studied. Results show that there is scope for the use of these techniques, which in some configurations outperform Monte Carlo Tree Search, and also suggest that further research in these methods could boost their performance.", "histories": [["v1", "Mon, 24 Apr 2017 08:01:39 GMT  (420kb,D)", "http://arxiv.org/abs/1704.07075v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["raluca d gaina", "jialin liu", "simon m lucas", "diego perez-liebana"], "accepted": false, "id": "1704.07075"}, "pdf": {"name": "1704.07075.pdf", "metadata": {"source": "CRF", "title": "Analysis of Vanilla Rolling Horizon Evolution Parameters in General Video Game Playing", "authors": ["Raluca D. Gaina", "Jialin Liu", "Simon M. Lucas", "Diego P\u00e9rez-Li\u00e9bana"], "emails": ["rdgain@essex.ac.uk", "jialin.liu@essex.ac.uk", "sml@essex.ac.uk", "dperez@essex.ac.uk"], "sections": [{"heading": null, "text": "Keywords: general video games, rolling horizon evolution, games, Montcarlo tree search, random search"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules that they have set themselves, and they will be able to play by the rules that they have set themselves."}, {"heading": "2 Relevant Research", "text": "The popularity of General Game Playing (GGP) has increased over the last decade, since M. Genesereth et al. [8] organized the first GGP competition, in which participants could submit game agents to play in a diverse collection of board games. Sharma et al. [25] motivates research in this area by highlighting techniques such as agents trained without prior knowledge of the game and excelling in certain games, such as TD gammon backgammon [26] and Blondie24 in Checkers [1], cannot be successfully applied in other scenarios or environments. However, the problem is being extended to video games in General Video Game Playing (GVGP [12]), which provides agents with new and possibly more complex challenges that are applied in practice due to a higher and more continuous action rate. One of the first frameworks to enable the testing of such general agents was the Arcade Learning Environment (ALE)."}, {"heading": "3 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The GVGAI Framework", "text": "The experiments outlined in this paper are within the general framework of video games1, which are often used in secondary intelligence, due to their large and ever increasing number of games. This framework currently comprises 100 single-player and 40 two-player games, which are limited to a maximum of 5, although they cannot take any action (ACTION NIL). It is important that these measures are not able to initiate a movement available to agents."}, {"heading": "3.2 Rolling Horizon Evolutionary Algorithms", "text": "Rolling Horizon Evolutionary Algorithms (RHEA) [21] are a subset of EAs that use populations of individuals representing action plans or action sequences; individuals are evaluated by simulating steps forward using a forward model; from the current state of the games, all actions (genes of the individual) are performed in sequence until a terminal state or length of the individual is reached; the state reached at that point is then evaluated using a heuristic function and the value assigned as the fitness of the individual. Generally, the algorithm starts with a random population of individuals; at each game step, it applies traditional genetic operators (such as mutation, random alteration of some actions in the sequence and cross-over, combination of individuals in different ways) to obtain new individuals for the next generation of the population; each of them is then evaluated and assigned to a fitness by which the population is sorted and only the best are passed on to subsequent generations."}, {"heading": "3.3 Open Loop Monte Carlo Tree Search (OLMCTS)", "text": "The Open Loop Monte Carlo Tree Search (OLMCTS) is an MCTS implementation for the GVGAI framework. This special agent does not store the states of the game in the tree nodes, but instead uses the forward model to re-evaluate each action. OLMCTS uses four simple steps to achieve a high level of play: selection (using a tree policy to select one of the current leaves of the tree that is not yet fully extended), expansion (adding a new child of the selected node to the tree), simulation (a Monte Carlo process that uses the forward model to get through random actions through the game) and backpropagation (the state that is reached after the MC simulation has been evaluated using a heuristic and its value underpinned the tree to the root node and updated all other parent nodes).The steps of the MCTS algorithm are illustrated in Figure 3.when the limit of the selected algorithm's executing budget is reached, such as the GAI's execution budget (or the number of requests for the GAI)."}, {"heading": "4 Approach and Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Methods", "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they have been moving."}, {"heading": "4.2 Games", "text": "All combinations studied in this study were performed 20 times on 20 games in the GVGAI corpus at all 5 levels, resulting in 100 games per configuration.The games were selected on the basis of two classifications available in the literature to balance the game set and analyze performance on the basis of a selected selection of different games. The first classification, compiled by Mark Nelson [17] in his analysis of the Vanille Monte Carlo Tree Search algorithm in 62 games in the framework, sorted by the MCTS win rate as a simple criterion. The second classification considered for this study was the summary of 49 games by Bontrager et al. [4], which divided the games into groups due to their similarity in playing characteristics. Combining these two lists and the uniform sample from both yielded a varied subset suitable for this experiment containing 10 stochastic and 10 deterministic games. See Table 1 for the name of these games and the later indices used in this document."}, {"heading": "5 Results and Discussion", "text": "In this section, the results are presented and analyzed from different angles, taking into account the nature of the game and variations in population size and individual length. In Section 5.1, performance is compared using smaller or larger populations, while in Section 5.2, the effects of individual length are discussed. Later, the performance of the RHEA is also compared with the performance of the RS, which uses different budgets (Section 5.3) and OLMCTS (Section 5.4) as provided by the GVGAI framework. Since the game set used is equally divided into deterministic and stochastic games, an in-depth analysis is performed for each game type, although it is not implied that the trend would continue in other games of the same type. Additionally, a non-parametric Mann-Whitney test has been used to measure the statistical significance of the results for each game (p-value = 0.05). Table 4 summarizes the winning rates of all the configurations in this study."}, {"heading": "5.1 Population Variation", "text": "Figure 4 shows the change in the win rate as population size, for L = 6 and L = 14 (numbers for other individual lengths have been omitted for reasons of space). Each of the 20 games that these algorithm configurations were tested showed different performances and variations. There is a trend that is observed in most games, with the win rate increasing, regardless of the game type (c.f. Table 4). Exceptions are games where the win rate starts at 100%, leaving no room for improvement (games with indexes 0 and 50, aliens and intersection, respectively, on the contrary, when the win rate is very close to 0%, due to the outstanding difficulty (game index 75, Roguelike)."}, {"heading": "5.2 Individual Variation", "text": "Figure 5 illustrates the change in the win rate in each of the 20 games with increasing individual length, for population sizes P = 1 = 97 = 91. Full results using a variety of population sizes and individual length are given in Table 4. If there is only one individual in the population, i.e. no crossover is involved, the win rate experiences a significant increase followed by a decrease with simultaneous increase in individual length. This is due to the fact that the size of the search space for solutions exponentially increases with individual length. For a few individuals evaluated, the algorithm struggles to find optimal solutions. This problem can be solved by increasing the population size, as shown in Figure 5 (top). For example, the game with index 67, Plaque Attack, is a variation from 68% to 83% with simultaneous increase in population."}, {"heading": "5.3 Random Search", "text": "The version of RHEA, which uses large values for population size and individual length, is reminiscent of the Random Search (RS) algorithm. We run an RS on the same set of games with P = 24 individuals and simulation depth L = 20. Since a budget of 480 calls is assigned to the forward model, RHEA corresponds to this population size and individual length. The average win rate in each of the games tested is summarized in the last row of Table 5.RS and performs no worse than any previously investigated variant of the RHEA. This result supports one of the most important findings on this paper: the vanilla version of RHEA is not able to explore the search space better than (and in most cases not even as good as) RS in the framework, which is tested when the budget is very limited. To test the limitations and potential benefits of development, an additional series of experiments is carried out, with the conversion using the same P = 24, L = 20 configuration, but increasing the Forward model to 940 calls for the budget and the budget is recommended."}, {"heading": "5.4 RHEA vs OLMCTS", "text": "Table 5 also includes the performance of the OLMCTS agent in the GVGAI sample. The OLMCTS agent uses a playout depth of 10, so the comparisons presented here refer to RHEA configurations with an individual length L = 10. The results show that, although RHEA is significantly worse than OLMCTS when the number of individuals per population is increased (P > 5), it is possible to create an RHEA that is capable of achieving a higher level of play than OLMCTS, which is the basis of most dominant algorithms in GVGAI literacy. Furthermore, when comparing with RS, OLMCTS also falls short in terms of the average percentage of victories. However, in these games, it manages to achieve a higher number of ranking points against the other 4 agents. Considering that points are awarded for each game in order to evaluate their overall abilities, the overall version of the OLTS does not suggest that the individual results are worse than those of the MOLTS."}, {"heading": "6 Conclusions and Future Work", "text": "This paper presents an analysis of the population size and individual length of Rolling Time (RHEA). The performance of this algorithm is measured in terms of the win rate in a subset of 20 games of the general game AI. One of the most important results of this research is that RHEA is not able to find a better solution (RS)."}], "references": [{"title": "The Importance of a Piece Difference Feature to Blondie 24", "author": ["B. Al-Khateeb", "G. Kendall"], "venue": "UK Workshop on Computational Intelligence (UKCI). pp. 1\u20136", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "EnHiC: An Enforced Hill Climbing Based System for General Game Playing", "author": ["A. Babadi", "B. Omoomi", "G. Kendall"], "venue": "IEEE Conference on Computational Intelligence and Games (CIG). vol. 1, pp. 193\u2013199", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research 47, 253\u2013279", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Matching Games and Algorithms for General Video Game Playing", "author": ["P. Bontrager", "A. Khalifa", "A. Mendes", "J. Togelius"], "venue": "Twelfth Artificial Intelligence and Interactive Digital Entertainment Conference. pp. 122\u2013128", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games. vol. 4, pp. 1\u201343", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining Pathfmding Algorithm with Knowledge-based Monte-Carlo Tree Search in General Video Game Playing", "author": ["C.Y. Chu", "H. Hashizume", "Z. Guo", "T. Harada", "R. Thawonmas"], "venue": "IEEE Conference on Computational Intelligence and Games (CIG). vol. 1, pp. 523\u2013529", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "General Video Game for 2 Players: Framework and Competition", "author": ["R.D. Gaina", "D. Perez-Liebana", "S.M. Lucas"], "venue": "Proceedings of the IEEE Computer Science and Electronic Engineering Conference (CEEC). p. to appear", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "General Game Playing: Overview of the AAAI Competition", "author": ["M. Genesereth", "N. Love", "B. Pell"], "venue": "AI Magazine. vol. 26, p. 62", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "MCTS/EA Hybrid GVGAI Players and Game Difficulty Estimation", "author": ["H. Horn", "V. Volz", "D. Perez-Liebana", "M. Preuss"], "venue": "Proceedings of the IEEE Conference on Computational intelligence and Games (CIG). p. to appear", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Online evolution for multi-action adversarial games", "author": ["N. Justesen", "T. Mahlmann", "J. Togelius"], "venue": "European Conference on the Applications of Evolutionary Computation. pp. 590\u2013 603. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "General Video Game Level Generation", "author": ["A. Khalifa", "D. Perez-Liebana", "S. Lucas", "J.T."], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference (GECCO). p. to appear", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "General Video Game Playing", "author": ["J. Levine", "S.M. Lucas", "M. Mateas", "M. Preuss", "P. Spronck", "J. Togelius"], "venue": "Artificial and Computational Intelligence in Games, Dagstuhl FollowUps. vol. 6, pp. 1\u20137", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Bandit-Based Random Mutation Hill-Climbing", "author": ["J. Liu", "D.P. Liebana", "S.M. Lucas"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Fast Evolutionary Adaptation for Monte Carlo Tree Search", "author": ["S.M. Lucas", "S. Samothrakis", "D. Perez"], "venue": "EvoGames", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "An Introduction to Genetic Algorithms", "author": ["M. Mitchell"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Human-level Control Through Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature 518(7540), 529\u2013533", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating Vanilla MCTS Scaling on the GVG-AI Game Corpus", "author": ["M.J. Nelson"], "venue": "Proceedings of the 2016 IEEE Conference on Computational Intelligence and Games", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "MCTS with Influence Map for General Video Game Playing", "author": ["H. Park", "K.J. Kim"], "venue": "IEEE Conference on Computational Intelligence and Games (CIG). vol. 1, pp. 534\u2013535", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge-based Fast Evolutionary MCTS for General Video Game Playing", "author": ["D. Perez", "S. Samothrakis", "S.M. Lucas"], "venue": "IEEE Conference on Computational Intelligence and Games. pp. 1\u20138", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Open Loop Search for General Video Game Playing", "author": ["D. Perez-Liebana", "J. Dieskau", "M. Hnermund", "S. Mostaghim", "S.M. Lucas"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference (GECCO). pp. 337\u2013344", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Rolling Horizon Evolution versus Tree Search for Navigation in Single-Player Real-Time Games", "author": ["D. Perez-Liebana", "S. Samothrakis", "S.M. Lucas", "P. Rolfshagen"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference (GECCO). pp. 351\u2013358", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "General video game ai: Competition, challenges and opportunities", "author": ["D. Perez-Liebana", "S. Samothrakis", "J. Togelius", "S.M. Lucas", "T. Schaul"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "The 2014 General Video Game Playing Competition", "author": ["D. Perez-Liebana", "S. Samothrakis", "J. Togelius", "T. Schaul", "S. Lucas", "A. Couetoux", "J. Lee", "C.U. Lim", "T. Thompson"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games. vol. PP, p. 1", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Rolling Horizon methods for Games with Continuous States and Actions", "author": ["S. Samothrakis", "S.A. Roberts", "D. Perez", "S. Lucas"], "venue": "Proceedings of the Conference on Computational Intelligence and Games (CIG)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "General Game Playing: An Overview and Open Problems", "author": ["S. Sharma", "Z. Kobti", "S.D. Goodwin"], "venue": "IEEE International Conference on Computing, Engineering and Information. pp. 257\u2013260", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Temporal Difference Learning and TD-Gammon", "author": ["G.J. Tesauro"], "venue": "IEEE Conference on Computational Intelligence and Games. pp. 58\u201368", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Portfolio online evolution in starcraft", "author": ["C. Wang", "P. Chen", "Y. Li", "C. Holmg\u00e5rd", "J. Togelius"], "venue": "Twelfth Artificial Intelligence and Interactive Digital Entertainment Conference", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "The General Video Game AI Competition (GVGAI) [22,23] offers a large corpus of games described in a plain text language, making it easy to run general AI agents in several different environments and analyse their performance.", "startOffset": 46, "endOffset": 53}, {"referenceID": 22, "context": "The General Video Game AI Competition (GVGAI) [22,23] offers a large corpus of games described in a plain text language, making it easy to run general AI agents in several different environments and analyse their performance.", "startOffset": 46, "endOffset": 53}, {"referenceID": 6, "context": "The competition has already completed three editions of its single player track (starting in 2014), with two additional tracks running in 2016 for two player games [7] and level generation [11].", "startOffset": 164, "endOffset": 167}, {"referenceID": 10, "context": "The competition has already completed three editions of its single player track (starting in 2014), with two additional tracks running in 2016 for two player games [7] and level generation [11].", "startOffset": 189, "endOffset": 193}, {"referenceID": 1, "context": "This competition is becoming a popular way of benchmarking AI algorithms such as enforced hill climbing [2], algorithms employing advanced path finding or using the knowledge gained during the game in interesting ways [19,6], or dominant Monte Carlo Tree Search techniques [18].", "startOffset": 104, "endOffset": 107}, {"referenceID": 18, "context": "This competition is becoming a popular way of benchmarking AI algorithms such as enforced hill climbing [2], algorithms employing advanced path finding or using the knowledge gained during the game in interesting ways [19,6], or dominant Monte Carlo Tree Search techniques [18].", "startOffset": 218, "endOffset": 224}, {"referenceID": 5, "context": "This competition is becoming a popular way of benchmarking AI algorithms such as enforced hill climbing [2], algorithms employing advanced path finding or using the knowledge gained during the game in interesting ways [19,6], or dominant Monte Carlo Tree Search techniques [18].", "startOffset": 218, "endOffset": 224}, {"referenceID": 17, "context": "This competition is becoming a popular way of benchmarking AI algorithms such as enforced hill climbing [2], algorithms employing advanced path finding or using the knowledge gained during the game in interesting ways [19,6], or dominant Monte Carlo Tree Search techniques [18].", "startOffset": 273, "endOffset": 277}, {"referenceID": 19, "context": "The way they are applied to the domain of GVGP is by encoding sequences of in-game actions as individuals, using heuristics to analyse the value of each sequence [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 7, "context": "[8] organised the first GGP competition allowing participants to submit game agents to play in a diverse collection of board games.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[25] motivates research in this area by bringing to attention how agents trained without prior knowledge of the game and excelling in specific games, such as TD-Gammon", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "in Backgammon [26] and Blondie24 in Checkers [1], cannot be successfully applied in other scenarios or environments.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "in Backgammon [26] and Blondie24 in Checkers [1], cannot be successfully applied in other scenarios or environments.", "startOffset": 45, "endOffset": 48}, {"referenceID": 11, "context": "The problem is further expanded to video games in General Video Game Playing (GVGP [12]), which provide the agents with new and possibly more complex challenges due to a higher and continuous, in practice, rate of actions.", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "One of the first frameworks to allow testing of such general agents was the Arcade Learning Environment (ALE) [3], later used as benchmark for applying Deep Q-Learning to achieve human level of play on the Atari 2600 collection [16].", "startOffset": 110, "endOffset": 113}, {"referenceID": 15, "context": "One of the first frameworks to allow testing of such general agents was the Arcade Learning Environment (ALE) [3], later used as benchmark for applying Deep Q-Learning to achieve human level of play on the Atari 2600 collection [16].", "startOffset": 228, "endOffset": 232}, {"referenceID": 4, "context": "Monte Carlo Tree Search methods have dominated GVGP so far, and their variations have been explored in various works [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 20, "context": "[21] compare EA techniques with tree search on the Physical Salesman Travelling Problem, and their results are satisfactory, encouraging research in the area.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] compare two variations of the Rolling Horizon setting of EAs in a number of continuous environments, including a Lunar Lander game.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] used online evolution for action decision in Hero Academy, a game in which each player counts on multiple units to move in a single turn, presenting a branching factor of a million actions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] employed a modified version of online evolution using a portfolio of script to play Starcraft micro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Other different approaches to EAs have been explored in the past, such as combining them with other techniques in order to produce hybrids, and take advantage of the benefits of each algorithm [9].", "startOffset": 193, "endOffset": 196}, {"referenceID": 18, "context": "[19], or, for a different effect, the MCTS parameters were adjusted with evolutionary methods [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[19], or, for a different effect, the MCTS parameters were adjusted with evolutionary methods [14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "There has been recent work that has attempted to give more focus to the evolutionary process and instead integrates tree structures into EAs, or uses N-armed bandit techniques and Upper Confidence Bounds (UCB) for informing and guiding the evolution process [13].", "startOffset": 258, "endOffset": 262}, {"referenceID": 20, "context": "Rolling Horizon Evolutionary Algorithms (RHEA) [21] are a subset of EAs which use populations of individuals representing action plans or sequences of actions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 4, "context": "Monte Carlo Tree Search steps [5]", "startOffset": 30, "endOffset": 33}, {"referenceID": 14, "context": "The most basic form this algorithm can take is that of a Random Mutation Hill Climber [15], where the population size is only 1, using the mutation operator as the only way to navigate through the search space.", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "For an in depth description of Monte Carlo Tree Search, variants, improvements, and applications, the reader is referred to [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 22, "context": "The winner of the first edition of the competition in 2014, Adrien Cou\u00ebtoux [23], employed an Open Loop technique quite similar to this algorithm.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "The first classification was that generated by Mark Nelson [17] in his analysis of the vanilla Monte Carlo Tree Search algorithm in 62 of the games in the framework, sorted using the win rate of MCTS as a simple criterion.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "[4], which separated the games into groups based on their similarity in terms of game features.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Monte Carlo Tree Search techniques have generally dominated General Video Game Playing, but recent research has started looking at Evolutionary Algorithms and their potential at matching Tree Search level of play or even outperforming these methods. Online or Rolling Horizon Evolution is one of the options available to evolve sequences of actions for planning in General Video Game Playing, but no research has been done up to date that explores the capabilities of the vanilla version of this algorithm in multiple games. This study aims to critically analyse the different configurations regarding population size and individual length in a set of 20 games from the General Video Game AI corpus. Distinctions are made between deterministic and stochastic games, and the implications of using superior time budgets are studied. Results show that there is scope for the use of these techniques, which in some configurations outperform Monte Carlo Tree Search, and also suggest that further research in these methods could boost their performance.", "creator": "LaTeX with hyperref package"}}}