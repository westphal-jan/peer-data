{"id": "1704.07121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets", "abstract": "Visual question answering (QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or the both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular visual QA datasets as well as to create a new visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via", "histories": [["v1", "Mon, 24 Apr 2017 10:05:19 GMT  (774kb,D)", "http://arxiv.org/abs/1704.07121v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["wei-lun chao", "hexiang hu", "fei sha"], "accepted": false, "id": "1704.07121"}, "pdf": {"name": "1704.07121.pdf", "metadata": {"source": "CRF", "title": "Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets", "authors": ["Wei-Lun Chao", "Hexiang Hu", "Fei Sha"], "emails": ["weilunc@usc.edu,", "hexiang.frank.hu@gmail.com,", "feisha@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in fact, live, in which they, in which they, in fact, live, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in fact, in which they, in fact, are able to put themselves, in a different world, in which they, in which they are able to live, in which they, in which they, in which they are able to live, in which they are able to live, in which they, in which they, in which they, in which they, in which they, in which they, in which they are able to live in"}, {"heading": "2 Related Work", "text": "There are about two dozen data sets for the task. Most of them use images from the real world, while some are based on synthetic ones. We focus on 3 data sets: VQA [3], Visual7W [30], and Visual Genome [16]. All of them use images from MSCOCO [18]. In addition to the pairs of questions and correct answers, VQA, and Visual Madlibs [28] provide decoy answers for each pair, allowing the task to be evaluated in multiple-choice selection."}, {"heading": "3 Analysis of Decoy Answers\u2019 Effects", "text": "In this section we will examine in detail the Visual7W [30] dataset, a popular choice for the visual QA task. We will show how the deficiencies in the design of decoy questions affect the performance of learning algorithms. In multiple choice QA datasets, a training or test example is a triplet consisting of a Figure I, a Question Q and a Candidate Answer Group A. Group A contains a Target T (the right answer) and K decoy (wrong answers), which are indicated by D. Thus, an IQA treble is {I, Q, A = {T, D1, \u00b7 \u00b7 \u00b7, DK}. We will use C to denote either the target or a decoy."}, {"heading": "3.1 Visual QA models", "text": "We will examine how well a learning algorithm can function when supplied with different information modalities. We will focus on the hidden layer MLP model proposed in [12], which has achieved state-of-the-art results on the Visual7W dataset. The model calculates a scoring function f (c, i) f (c, i) = \u03c3 (U max (0, W g (c, i) + b) (1) via a candidate response c and the multimodal information i, where g is the common feature of (c, i) and \u03c3 (x) = 1 / (1 + exp (\u2212 x). The information i can be zero, the image (I) alone, the question (Q) alone, or the combination of the two (I + Q). In the face of an IQA triplet, we will use the penultimate layer of ResNet-200 [9] as visual features to represent I and the average WORD2VEC embedding as characteristics."}, {"heading": "3.2 Analysis results", "text": "There are a few interesting observations. First, in the series of \"A,\" where only the answers of the candidate (and whether they are right or wrong) are used to learn a learning model, the model performs significantly better than random guesses and people (52.9% vs. 25%) - people are equally likely to answer any of the answers without looking at the picture and the question! Note that in this case, the information I put in eq. (1) does not contain anything that the model learns, the specific statistics of the candidate answers in the datasets and uses those.Adding the information about the image (i.e.), the series of \"I + A\" improves significantly and comes close to performance when all the information is used (62.4% vs. 65.7%)."}, {"heading": "4 Create Better Visual QA Datasets", "text": "In this section, we describe our approaches to correcting design errors in the existing visual QA task datasets. We present two automatic methods for creating new decoy responses that can prevent learning models from analyzing the incident statistics in the datasets."}, {"heading": "4.1 Methods", "text": "In fact, most of us are capable of coming out on top."}, {"heading": "4.2 Comparison to other datasets", "text": "Several authors have noted the design deficiencies in existing databases and proposed \"solutions\" [3, 28, 30, 6]. No data set has used a method for creating IoU baits. We show empirically how IoU baits significantly remedy the design deficiencies for the baits in the data sets. Several previous efforts have produced baits that are similar in spirit to our QoU baits. [28, 6, 7] automatically find baits from similar questions or captions based on question templates and annotated objects, tri-gramms and GLOVE embedding [23] and heel vectors [17] or linguistic surface similarity, the latter two intended for different tasks of visual QA and only [7] consider removing semantically ambiguous baits like ours. [3, 30] Ask people in light of the questions and objectives posed."}, {"heading": "5 Empirical Studies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "VQA Real [3] uses images from MSCOCO [18] within the same training / validation / test splits to construct IQA triplets. In total, 614,163 IQA triplets are generated for 204,721 images. Each question has 18 candidate answers: in general, 3 baits are human-made, 4 are randomly sampled, and 10 are randomly sampled. As the test set does not specify the targets, our studies focus on the training and validation steps. Visual7W Telling (Visual7W) [30] The data set uses 47,300 images from MSCOCO [18] and contains 139,868 IQA triplets."}, {"heading": "5.2 Setup", "text": "We refer to MLP-A, MLP-QA, MLP-IQA as the models that use A (Answers only), Q + A (Question plus Answers), I + A (Picture plus Answers), and I + Q + A (Picture, Question and Answers) for multimodal information, and the hidden layer has 8,192 neurons. We use a 200-layer ResNet [9] to calculate visual features that are 2,048-dimensional. ResNet is pre-trained on ImageNet [24]. The WORD2VEC Q-A function is 300-dimensional, pre-trained on Google News."}, {"heading": "5.3 Results", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5.4 Qualitative Results", "text": "In Fig. 2, we present examples of image question target triplets from V7W, VQA and VG, together with our IoU baits (A, B, C) and QoU baits (D, E, F). G is the target. Predictions of the corresponding MLP-IQA are also included. Ignoring information from images or questions makes it extremely difficult to answer the triplets correctly, even for humans. Our automatic procedures fail for some triplets, resulting in ambiguous baits for the targets. See Fig. 3 for examples. We categorized these error cases in two situations. \u2022 Our filter steps in Fig. 4 fail as observed in the above example. The WUP-based similarity is based on the WordNet hierarchy. For some semantically similar words such as \"lady\" and \"woman,\" the similarity is only 0.632, much lower than that of 0.857 between \"cat\" and \"dog.\" This similarity may be compensated by V2C measures."}, {"heading": "6 Conclusion", "text": "We have found that bait design can unintentionally provide \"shortcuts\" for machines that can exploit it to do the job well. We describe several principles for constructing good baits and propose automatic procedures for fixing existing records. We have also created a new record by applying our procedures to the Visual Genome, which has resulted in the largest multiple choice record for the task with over 1 million triple image question candidates. We are conducting extensive empirical studies to demonstrate the effectiveness of our methods in creating better visual QA datasets. The fixed datasets and the Visual Genome-based dataset will be published and are available at http: / / www.teds.usc.edu / website _ vqa /."}, {"heading": "A Details on the MLP models", "text": "The single-layer MLP model used in our experiments has 8,192 hidden units, exactly according to [12]. It contains a batch normalization layer before ReLU and a dropout layer after ReLU. We set the dropout rate to 0.5.The input to the model is the concatenated characteristics of images, questions and answers. We change all characters in lowercase letters and all integer numbers within [0, 10] to words before calculating WORD2VEC. We perform \"2 normalization of the characteristics of each information before concatenation. We train the model with stochastic gradient lineage with a size of 100, dynamics of 0.9, and the graduated learning rate is divided by 10 after each M batch. We set the initial learning rate to 0.01 (we consider 0.001 for fine tuning in Sect."}, {"heading": "B WUP-based similarity for filtering out ambiguous decoys", "text": "We use the Wu-Palmer (WUP) score [26], which characterizes the word meaning similarity, to filter out ambiguous decoys to the target (correct answer).The WUP score is calculated on the basis of the WordNet hierarchy. Essentially, it measures the similarity of two nodes (i.e. synsets) in the hierarchy. Since a word could correspond to several nodes, we measure the word similarity as follows: WUP (w1, w2) = max (n1, n2)."}, {"heading": "C Details on user studies", "text": "As mentioned in section 5.2 of the main text, we provide details of user studies. Figure 4 shows our user interface. We conduct studies with Amazon Mechanic Turk (AMT) on Visual7W [30], VQA [3] and Visual Genome (VG) [16]. We mainly evaluate on our IoU decoys and QoU decoys (combined). For each data set, we randomly sample 1,000 picture question target triplets along with the corresponding IoU decoys and QoU decoys to evaluate human performance. For each of these triplets, three workers are assigned to select the most correct candidate response according to the image and question. We calculate the average accuracy of these workers and report it in Table 3, 4 and 5 of the main texts. We perform human evaluation with the Visual7W original decoys to examine the difference between human-generated and automatically generated 2.5 percent."}, {"heading": "D Detailed results on VQA w/o QA pairs that have Yes/No as the targets", "text": "As mentioned in Section 5.3 of the main text, the VQA validation set contains 45,478 QA pairs (out of a total of 12,1512 pairs) that have yes or no as the correct answers, and the only reasonable decoy to yes is no, and vice versa - any other decoy could in principle be easily detected. As both are in the top 10 common answers, they are already included in the Orig decoys - our IoU decoys and QoU decoys can hardly cause any noticeable improvement. Therefore, we remove all of these pairs (referred to as yes / no QA pairs) to investigate the improvement of the remaining pairs for which several decisions are useful. We refer to the subset of VQA as VQA \u2212 (we remove yes / no pairs set in both training and validation).We are conducting the same experiments as in Section 5.3 on VQA \u2212 Table 7 summarizing the results of the machines as well as the Qlers."}], "references": [{"title": "Analyzing the behavior of visual question answering models", "author": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh"], "venue": "Clear and sunny. C. Basement windows. D. On both sides of road. E. To left of truck. F. On edge of the sidewalk. G. In front of the building.  A. Certificate. B. Garland. C. Three. D. The man. E. Person in chair. F. The lady. G. The woman", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Spice: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould"], "venue": "In ECCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "author": ["Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Understanding image and text simultaneously: a dual vision-language machine comprehension", "author": ["Nan Ding", "Sebastian Goodman", "Fei Sha", "Radu Soricut"], "venue": "task. arXiv preprint arXiv:1612.07833,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1612.00837,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Focused evaluation for image description with binary forcedchoice tasks", "author": ["Micah Hodosh", "Julia Hockenmaier"], "venue": "In ACL Workshop,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Revisiting visual question answering baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten"], "venue": "In ECCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick"], "venue": "arXiv preprint arXiv:1612.06890,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Visual question answering: Datasets, algorithms, and future challenges", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "arXiv preprint arXiv:1610.01465,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Re-evaluating automatic metrics for image captioning", "author": ["Mert Kilickaya", "Aykut Erdem", "Nazli Ikizler-Cinbis", "Erkut Erdem"], "venue": "arXiv preprint arXiv:1612.07600,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Leveraging visual question answering for image-caption ranking", "author": ["Xiao Lin", "Devi Parikh"], "venue": "In ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Visual question answering: A survey of methods and datasets", "author": ["Qi Wu", "Damien Teney", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Verbs semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer"], "venue": "In ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["Licheng Yu", "Eunbyung Park", "Alexander C Berg", "Tamara L Berg"], "venue": "In ICCV,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["Peng Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "Recently, multimodal information processing tasks such as image captioning [27] and visual question answering (visual QA) [3] have gained a lot of attention.", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "Recently, multimodal information processing tasks such as image captioning [27] and visual question answering (visual QA) [3] have gained a lot of attention.", "startOffset": 122, "endOffset": 125}, {"referenceID": 16, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 50, "endOffset": 57}, {"referenceID": 4, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 50, "endOffset": 57}, {"referenceID": 14, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 80, "endOffset": 91}, {"referenceID": 10, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 80, "endOffset": 91}, {"referenceID": 9, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 8, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 5, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 17, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 10, "context": ", the image), the question, or both [12, 3].", "startOffset": 36, "endOffset": 43}, {"referenceID": 2, "context": ", the image), the question, or both [12, 3].", "startOffset": 36, "endOffset": 43}, {"referenceID": 2, "context": "We apply the procedures to revise both Visual7W [30] and VQA [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "Additionally, we create a multiple-choice based dataset from the recently released Visual Genome dataset [16], resulting in the largest multiple-choice dataset for the visual QA task, with more than one million image-question-candidate answers triplets.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "[14, 25] provide recent overviews of the status quo of the visual QA task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 23, "context": "[14, 25] provide recent overviews of the status quo of the visual QA task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "We concentrate on 3 datasets: VQA [3], Visual7W [30], and Visual Genome [16].", "startOffset": 34, "endOffset": 37}, {"referenceID": 14, "context": "We concentrate on 3 datasets: VQA [3], Visual7W [30], and Visual Genome [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "All of them use images from MSCOCO [18].", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Besides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs [28] provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "Our work is inspired by the experiments in [12] where they observe that machines without looking at images or questions can still perform well on the visual QA task.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 27, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 11, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 0, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 5, "context": "Besides the visual QA task, [7] and VisDial [6] also propose automatic ways to generate decoys for the tasks of selecting the best visual caption and dialog, respectively.", "startOffset": 28, "endOffset": 31}, {"referenceID": 10, "context": "We concentrate on the one hidden-layer MLP model proposed in [12], which had achieved state-of-the-art results on the dataset Visual7W.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Given an IQA triplet, we use the penultimate layer of ResNet-200 [9] as visual features to represent I and the average WORD2VEC embeddings [22] as text features to represent Q and C.", "startOffset": 65, "endOffset": 68}, {"referenceID": 20, "context": "Given an IQA triplet, we use the penultimate layer of ResNet-200 [9] as visual features to represent I and the average WORD2VEC embeddings [22] as text features to represent Q and C.", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "We compute the average WORD2VEC [22] to represent a question, and use the cos similarity to measure the similarity between questions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "Secondly, we utilize the WordNet hierarchy and the Wu-Palmer (WUP) score [26] to eliminate semantically similar decoys.", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "The WUP score measures how similar two word senses are (in the range of [0, 1]),", "startOffset": 72, "endOffset": 78}, {"referenceID": 19, "context": "We compute the similarity of two strings according to the WUP scores in a similar manner to [21], in which the WUP score is used for the evaluation of visual QA performance.", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "We use NLTK toolkit [4] to compute the similarity.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "Several authors have noticed the design deficiencies in the existing databases and have proposed \u201cfixes\u201d [3, 28, 30, 6].", "startOffset": 105, "endOffset": 119}, {"referenceID": 26, "context": "Several authors have noticed the design deficiencies in the existing databases and have proposed \u201cfixes\u201d [3, 28, 30, 6].", "startOffset": 105, "endOffset": 119}, {"referenceID": 26, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 0, "endOffset": 10}, {"referenceID": 5, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 0, "endOffset": 10}, {"referenceID": 21, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 182, "endOffset": 186}, {"referenceID": 5, "context": "The later two are for different tasks from visual QA, and only [7] considers removing semantically ambiguous decoys like ours.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "[3, 30] ask humans to create decoys, given the questions and targets.", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "VQA Real [3] The dataset uses images from MSCOCO [18] under the same training/validation/testing splits to construct IQA triplets.", "startOffset": 9, "endOffset": 12}, {"referenceID": 16, "context": "VQA Real [3] The dataset uses images from MSCOCO [18] under the same training/validation/testing splits to construct IQA triplets.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Visual7W Telling (Visual7W) [30] The dataset uses 47,300 images from MSCOCO [18] and contains 139,868 IQA triplets.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Visual Genome (VG) [16] The dataset uses 101,174 images from MSCOCO [18] and contains 1,445,322 IQT triplets.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "Visual Genome (VG) [16] The dataset uses 101,174 images from MSCOCO [18] and contains 1,445,322 IQT triplets.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "We use a 200-layer ResNet [9] to compute visual features which are 2,048-dimensional.", "startOffset": 26, "endOffset": 29}, {"referenceID": 22, "context": "The ResNet is pre-trained on ImageNet [24].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "The WORD2VEC feature [22] for questions and answers are 300-dimensional, pre-trained on Google News.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "*:taken from [3]", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "5% by [12] where a model pre-trained on VQA is fine-tuned on Visual7W.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "This issue can be alleviated by considering alternative semantic measures by WORD2VEC or by those used in [6, 7] for searching similar questions.", "startOffset": 106, "endOffset": 112}], "year": 2017, "abstractText": "Visual question answering (QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or the both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular visual QA datasets as well as to create a new visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via http://www.teds.usc.edu/website_vqa/.", "creator": "LaTeX with hyperref package"}}}