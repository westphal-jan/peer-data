{"id": "1704.07183", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Stochastic Constraint Programming as Reinforcement Learning", "abstract": "Stochastic Constraint Programming (SCP) is an extension of Constraint Programming (CP) used for modelling and solving problems involving constraints and uncertainty. SCP inherits excellent modelling abilities and filtering algorithms from CP, but so far it has not been applied to large problems. Reinforcement Learning (RL) extends Dynamic Programming to large stochastic problems, but is problem-specific and has no generic solvers. We propose a hybrid combining the scalability of RL with the modelling and constraint filtering methods of CP. We implement a prototype in a CP system and demonstrate its usefulness on SCP problems.", "histories": [["v1", "Mon, 24 Apr 2017 12:44:38 GMT  (17kb)", "http://arxiv.org/abs/1704.07183v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["steven prestwich", "roberto rossi", "armagan tarim"], "accepted": false, "id": "1704.07183"}, "pdf": {"name": "1704.07183.pdf", "metadata": {"source": "CRF", "title": "Stochastic Constraint Programming as Reinforcement Learning", "authors": ["S. D. Prestwich", "R. Rossi", "S. A. Tarim"], "emails": ["s.prestwich@cs.ucc.ie,", "roberto.rossi@ed.ac.uk,", "at@cankaya.edu.tr"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.07 183v 1 [cs.A I] 2 4straint Programming (CP), which is used to model and solve problems with constraints and uncertainty. SCP inherits excellent modeling capabilities and filtering algorithms from CP, but has not yet been applied to major problems. Reinforcement Learning (RL) extends dynamic programming to large stochastic problems, but is problem-specific and has no generic solutions. We propose a hybrid that combines the scalability of RL with the modeling and constraint filtering methods of CP. We implement a prototype in a CP system and demonstrate its usefulness for SCP problems."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to survive on their own."}, {"heading": "2 Stochastic Constraint Programming", "text": "Note-level SCSP is defined as a tuple (V, S, D, P, C, \u03b8, L), where V is a set of decision variables, S is a set of stochastic variables, D is a function that maps each element of V-S to a value domain, P is a function that maps each variable in S to a probability distribution, C is a set of constraints on V-S, \u03b8 is a function that maps each constraint in C to a threshold, each constraint must contain at least one V variable, and L = [< V1, S1 >,. < Vm, Sm >] is a list of decision steps, so that the Vi partition V and the Si partition S. Each constraint must contain at least one V variable, a constraint with threshold thresholds (h) = 1 is a hard constraint that represents such a constraint, and one with the target variable (h) is a random constraint on a random variable."}, {"heading": "3 SCP as RL", "text": "In this section, we describe our hybrid approach to solving SCP problems."}, {"heading": "3.1 Reinforcement Learning", "text": "RL is an area of machine learning rooted in dynamic programming, Monte Carlo methods, optimal control and behavioral psychology. It is one of the three main classes of machine learning, the other two of which are conditioned by supervised and unsupervised learning. RL encompasses the interaction between a decision-making actor and his or her environment. The actor strives to optimize an expected total reward under uncertainty about his or her surroundings. The actor can take measures that may affect the future state of the environment, which in turn can affect the later options of the actor: 1 for a win and 0 for a loss. Therefore, the actor must learn how to respond to a possible game state in order to maximize his or her likelihood of a state. Each state may have a related reward. The actor must learn a policy (a function from states to actions) that maximizes these total reward values, which are calculated taking into account the expected total reward values."}, {"heading": "3.2 Modelling", "text": "In this approach, we can benefit from the constraints that arise from our actions, but it is possible to reach a situation where no action can be taken. We must learn to make decisions that end in dead ends."}, {"heading": "3.3 Solving", "text": "The RL algorithm we will use is a form of tabular TD (0) [18] with a reward calculated at the end of each episode. However, many problems have far too many possible states to use RL in tabular form. To expand RL to address such problems, researchers have used a functional approach, also known as state generalization or state aggregation. This is the key to success of RL in real-world problems, and we will use it below. To apply our algorithm, an SCP model must be provided that includes a real function on total assignments that defines a reward."}, {"heading": "4 Experiments", "text": "We are now conducting experiments to evaluate our approach to stochastic problems, which we call TDCP. It is implemented in the Eclipse Constraint Logic Programming System [1] and all experiments are performed on a 2.8 GHz Pentium 4 with 512 MB RAM."}, {"heading": "4.1 An artificial single-stage problem", "text": "As a first experiment, we design an artificial one-step problem with known optimal solution. > The problem has N decision variables di and N random variables ri all with domain {1,..., N}. We post a very different constraint on decision variables: There are N variables with N values, so the solution must be a permutation of {1,.., N}. All random variables domains have the same uniform probability distribution: Each value has probability 1 / N. The goal is to maximize the sum of probabilities that each decision variable di is not greater than each random variable ri + 1... rN: See Figure 1, where Reify (c) is 1 if it is true and 0 if it is false. The sum of hired terms (without expectation) is the TDL reward."}, {"heading": "4.2 Pre-disaster planning", "text": "This year it is more than ever before."}, {"heading": "5 Conclusion", "text": "We have implemented a simple RL algorithm in a CP solver and obtained a novel algorithm for solving SCP problems. We have shown that this RL / CP hybrid can find high quality solutions to hard problems. We believe that the use of machine learning methods is a good direction for SCP research to make it a practical tool for problems in the real world. In future work we will show that our approach extends to multi-level SCP problems by using different state aggregation techniques (we have preliminary results on a stock control problem).This work should also be of interest from an RL perspective. First, the implementation of RL algorithms in a CP solver allows the user to perform rapid prototyping of RL methods for new problems."}], "references": [{"title": "Constraint Logic Programming Using Eclipse", "author": ["K.R. Apt", "M. Wallace"], "venue": "Cambridge University Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards Stochastic Constraint Programming: A Study of On-Line Multi-Choice Knapsack with Deadlines", "author": ["T. Benoist", "E. Bourreau", "Y. Caseau", "B. Rottembourg"], "venue": "7th International Conference on Principles and Practice of Constraint Programming, Lecture Notes in Computer Science vol. 2239, Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "A Survey on Metaheuristics for Stochastic Combinatorial Optimization", "author": ["L. Bianchi", "M. Dorigo", "L.M. Gambardella", "W.J. Gutjahr"], "venue": "Natural Computing 8(2):239\u2013287,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Introduction to Stochastic Programming", "author": ["J. Birge", "F. Louveaux"], "venue": "Springer Series in Operations Research,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "On the Stochastic Constraint Satisfaction Framework", "author": ["L. Bordeaux", "H. Samulowitz"], "venue": "ACM Symposium on Applied Computing,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "On Relations Between Chance Constrained and Penalty Function Problems Under Discrete Distributions", "author": ["M. Branda"], "venue": "Mathematical Methods of Operations Research 77:265\u2013277,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "W", "author": ["J. Dup\u01cecov\u00e1", "N. Gr\u00f6we-Kuska"], "venue": "R\u00f6misch. Scenario Reduction in Stochastic Programming: an Approach Using Probability Metrics.Mathematical Programming Series A 95:493\u2013 511,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "K", "author": ["S. Peeta", "F.S. Salman", "D. Gunnec"], "venue": "Viswanath. Pre-Disaster Investment Decisions for Strengthening a Highway Network.Computers &Operations Research 37:1708\u20131719,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate Dynamic Programming", "author": ["W.B. Powell"], "venue": "John Wiley & Sons,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolving Parameterised Policies for Stochastic Constraint Programming", "author": ["S.D. Prestwich", "S.A. Tarim", "R. Rossi", "B. Hnich"], "venue": "15th International Conference on Principles and Practice of Constraint Programming, Lecture Notes in Computer Science vol. 5732,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic Constraint Programming by Neuroevolution With Filtering", "author": ["S.D. Prestwich", "S.A. Tarim", "R. Rossi", "B. Hnich"], "venue": "7th International Conference on Integration of Artificial Intelligence and Operations Research Techniques in Constraint Programming, Lecture Notes in Computer Science vol. 6140, Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Value Interchangeability in Scenario Generation", "author": ["S.D. Prestwich", "M. Laumanns", "B. Kawas"], "venue": "19th International Conference on Principles and Practice of Constraint Programming, Lecture Notes in Computer Science Vol. 8124, Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid Metaheuristics for Stochastic Constraint Programming", "author": ["S.D. Prestwich", "S.A. Tarim", "R. Rossi", "B. Hnich"], "venue": "Constraints,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Generalized Arc Consistency for Global Cardinality Constraint", "author": ["J.-C. R\u00e9gin"], "venue": "14th National Conference on Artificial Intelligence pp. 209\u2013215,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Confidence-Based Reasoning in Stochastic Constraint Programming", "author": ["R. Rossi", "B. Hnich", "S.A. Tarim", "S. Prestwich"], "venue": "Artificial Intelligence, 228(1):129\u2013152, Elsevier,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Using Connectionist Systems", "author": ["G.A. Rummery", "M. Niranjan", "M. On-Line Q-Learnin"], "venue": "Technical Report CUED/F-INFENG/TR 166, Cambridge University Engineering Department,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "A Hybrid Bender\u2019s Decomposition Method for Solving Stochastic Constraint Programs with Linear Recourse", "author": ["S.A. Tarim", "I. Miguel"], "venue": "Lecture Notes in Computer Science vol. 3978, Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Practical Issues in Temporal Difference Learning.Machine Learning 8:257\u2013277", "author": ["G. Tesauro"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Commentary \u2014 Perspectives on Stochastic Optimization Over Time", "author": ["J.N. Tsitsiklis"], "venue": "INFORMS Journal on Computing 22(1):18\u201319,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic Constraint Programming", "author": ["T. Walsh"], "venue": "15th European Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "A New Hashing Method with Application for Game Playing", "author": ["A.L. Zobrist"], "venue": "Technical Report 88, Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, 1969. Also: International Computer Chess Association Journal 13(2):69\u201373,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 1, "context": "Stochastic Constraint Programming (SCP) is an extension of Constraint Programming (CP) designed to model and solve complex problems involving uncertainty and probability, a direction of research first proposed in [2,22].", "startOffset": 213, "endOffset": 219}, {"referenceID": 21, "context": "Stochastic Constraint Programming (SCP) is an extension of Constraint Programming (CP) designed to model and solve complex problems involving uncertainty and probability, a direction of research first proposed in [2,22].", "startOffset": 213, "endOffset": 219}, {"referenceID": 4, "context": "They are traditionally tackled by Stochastic Programming [5], but a motivation for SCP is that it should be able to exploit the richer choice of variables and constraints used in CP, leading to more compact models and the use of powerful filtering algorithms.", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "If a problem has many decision variables we can apply metaheuristics [11,12,14,22] but we must still check all scenarios to obtain an exact solution, though in special cases a subset is sufficient [13].", "startOffset": 69, "endOffset": 82}, {"referenceID": 11, "context": "If a problem has many decision variables we can apply metaheuristics [11,12,14,22] but we must still check all scenarios to obtain an exact solution, though in special cases a subset is sufficient [13].", "startOffset": 69, "endOffset": 82}, {"referenceID": 13, "context": "If a problem has many decision variables we can apply metaheuristics [11,12,14,22] but we must still check all scenarios to obtain an exact solution, though in special cases a subset is sufficient [13].", "startOffset": 69, "endOffset": 82}, {"referenceID": 21, "context": "If a problem has many decision variables we can apply metaheuristics [11,12,14,22] but we must still check all scenarios to obtain an exact solution, though in special cases a subset is sufficient [13].", "startOffset": 69, "endOffset": 82}, {"referenceID": 12, "context": "If a problem has many decision variables we can apply metaheuristics [11,12,14,22] but we must still check all scenarios to obtain an exact solution, though in special cases a subset is sufficient [13].", "startOffset": 197, "endOffset": 201}, {"referenceID": 7, "context": "If we are content with an approximate solution we can apply scenario reduction by sampling [8] or approximation [4], but scenario reduction methods can be nontrivial to analyse and apply.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "If we are content with an approximate solution we can apply scenario reduction by sampling [8] or approximation [4], but scenario reduction methods can be nontrivial to analyse and apply.", "startOffset": 112, "endOffset": 115}, {"referenceID": 15, "context": "Confidence intervals can be applied to control approximations [16] but this does not address the issue of scaling up to a huge number of scenarios.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "In contrast, many large stochastic and adversarial problems have been successfully solved by methods from Reinforcement Learning (RL) [18], which is related to Neuro-", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "Dynamic Programming [3] and Approximate Dynamic Programming [10].", "startOffset": 20, "endOffset": 23}, {"referenceID": 9, "context": "Dynamic Programming [3] and Approximate Dynamic Programming [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Perhaps most famously, RL was used to learn how to play the game of Backgammon [20] by trial-and-error self-play and without human intervention, leading to a world-class player.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "Related methods developed in Operations Research to handle exponentially-many actions are able to handle far larger problems, for example the scheduling of tens of thousands of trucks [10].", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "such as that in [15], and we solve it, obtaining a policy.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "For a recent discussion on penalty functions versus chance constraints see [7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 17, "context": "The RL algorithm we shall use is a form of tabular TD(0) [18] with a reward computed at the end of each episode.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "It is implemented in the Eclipse constraint logic programming system [1] and all experiments are performed on a 2.", "startOffset": 69, "endOffset": 72}, {"referenceID": 22, "context": "To handle the exponentially large number of states we use a form of state aggregation based on Zobrist hashing [23] withH hash table entries for some integerH , which works as follows.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "[9] who solved it approximately using a Monte Carlo method with function approximation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "The six problem instances were later solved exactly in [13].", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "However, a symmetry-based technique called scenario bundling was later applied to find exact solutions [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "We show the optimum objective values from [13], the exact evaluation of the plans found by Peeta et al.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "B M optimum [9] TDCP plan estimated actual", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "413 (this is the optimal plan from [13]), both better than the plans of Peeta et al.", "startOffset": 35, "endOffset": 39}], "year": 2014, "abstractText": "Stochastic Constraint Programming (SCP) is an extension of Constraint Programming (CP) used for modelling and solving problems involving constraints and uncertainty. SCP inherits excellent modelling abilities and filtering algorithms from CP, but so far it has not been applied to large problems. Reinforcement Learning (RL) extends Dynamic Programming to large stochastic problems, but is problem-specific and has no generic solvers. We propose a hybrid combining the scalability of RL with the modelling and constraint filtering methods of CP. We implement a prototype in a CP system and demonstrate its usefulness on SCP problems.", "creator": "gnuplot 4.4 patchlevel 0"}}}