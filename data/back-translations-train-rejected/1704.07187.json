{"id": "1704.07187", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Reinforcement Learning Based Dynamic Selection of Auxiliary Objectives with Preserving of the Best Found Solution", "abstract": "Efficiency of single-objective optimization can be improved by introducing some auxiliary objectives. Ideally, auxiliary objectives should be helpful. However, in practice, objectives may be efficient on some optimization stages but obstructive on others. In this paper we propose a modification of the EA+RL method which dynamically selects optimized objectives using reinforcement learning. The proposed modification prevents from losing the best found solution. We analysed the proposed modification and compared it with the EA+RL method and Random Local Search on XdivK, Generalized OneMax and LeadingOnes problems. The proposed modification outperforms the EA+RL method on all problem instances. It also outperforms the single objective approach on the most problem instances. We also provide detailed analysis of how different components of the considered algorithms influence efficiency of optimization. In addition, we present theoretical analysis of the proposed modification on the XdivK problem.", "histories": [["v1", "Mon, 24 Apr 2017 12:52:51 GMT  (106kb,D)", "http://arxiv.org/abs/1704.07187v1", "this is a full version of a paper which has been accepted as a student workshop paper to GECCO conference 2017"]], "COMMENTS": "this is a full version of a paper which has been accepted as a student workshop paper to GECCO conference 2017", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["irina petrova", "arina buzdalova"], "accepted": false, "id": "1704.07187"}, "pdf": {"name": "1704.07187.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning Based Dynamic Selection of Auxiliary Objectives with Preserving of the Best Found Solution", "authors": ["Irina Petrova", "Arina Buzdalova"], "emails": ["irenepetrova@yandex.com,", "abuzdalova@gmail.com"], "sections": [{"heading": null, "text": "In fact, it is in such a way that most of them will be able to be in a position to be in without them being in a position to be in a position to put themselves in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a live in a position to be in a position to be in a position to be in a position to be in a live in a position to be in a live in a live in a live in a live in a life to be a life to be"}, {"heading": "II. PRELIMINARIES", "text": "In this section we describe the EA + RL method. We also define model problems and non-stationary targets used in this study."}, {"heading": "A. EA+RL method", "text": "In reinforcement learning (RL), an agent applies an action to an environment. Then, the environment returns a numerical reward Q Q and a representation of its state and process is repeated. The agent's goal is to maximize the total reward. \"In the EA + RL method, EA is treated as an environment, with the selection of an object to be optimized corresponding to an action. The agent selects a target, EA creates a new population using that target and returns a reward to the agent. The reward depends on the difference in the best target value in two consecutive generations. We look at maximization problems. So, the higher the newly determined target value, the higher the reward. In the recent theoretical analysis of EA + RL with non-stationary targets, Random Local Search (RLS) is used instead of EA, the population consists of a single person [13]. Individuals are presented as bit strings, flip-1-bit mutation."}, {"heading": "B. Model problems", "text": "In this paper, we look at three model problems used in studies on EA + RL [12] - [14]. In all the problems considered, an individual is a bit string of length n. Let x be the number of bits in an individual set to one. Then, the lens ONEMAX is equal to x and the lens ZEROMAX is equal to n \u2212 x. One of the problems considered is GENERALIZED ONEMAX, referred to as OMd. The target of this problem, called OMd, is calculated as the number of bits in an individual of the length n corresponding to a given bitmask. The bitmask has d 0 bits and n \u2212 d 1 bit.Another problem is XDIVK. The target of this problem is calculated as bxk c, where x is the number of bits, k is a constant, k < n and k divides. The last problem considered is LEADINGONES."}, {"heading": "C. Non-stationary objectives", "text": "These auxiliary targets can be both ONEMAX and ZEROMAX at different stages of optimization. Specifically, we consider the auxiliary targets h1 and h2 defined in (1). The parameter p is called a switch point because at this point auxiliary targets change their characteristics. ONEMAX and ZEROMAX are called OM and ZEROMAX respectively ONE ONE E1 (x) = {OM, x \u2264 p ZM, p < x \u2264 n h2 = {ZM, x \u2264 p OM, p < x \u2264 n (1) In the XDIVK problem, a target currently equal to ONEMAX allows to distinguish individuals with the same value of the target lens and gives preference to the individual with a higher x value. Such an individual is more likely to create a disadvantage with a higher target value. In the case of ONEMONX, this problem is equally helpful to the ONE."}, {"heading": "III. MODIFIED EA+RL", "text": "In this section, we propose a modification of the EA + RL method, the EA + RL prevents the best solution found from losing Q. In the EA + RL method, if the newly produced individual is better than the existing individual, because the new individual is better than the existing individual, the new individual can be worse than the existing individual in terms of the objective-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-goal-"}, {"heading": "IV. THEORETICAL ANALYSIS", "text": "In fact, it is as if it were a new way, as it is in the USA, in the USA, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "V. EXPERIMENTAL ANALYSIS OF MODIFIED EA+RL", "text": "In this section we will experimentally examine the efficiency of the two proposed EA + RL modifications to the optimization problems defined in Section II-B."}, {"heading": "A. Description of experiments", "text": "We analyzed the two versions of the proposed modification of EA + RL and the EA + RL method to OMd, XDIVK and LEADINGONES. The non-stationary targets described in (1) were used for all problems. For XDIVK, we analyzed two cases of the switching point position, the first case being the worst [13], when the switching point is at the end of the optimization, p = n \u2212 k + 1. In the second case, the switching point is in the middle of the optimization, p = n / 2. For each algorithm, we analyzed two state definitions: the single state and the target state. We also investigated the application of an \u03b5-greedy strategy. In this strategy, the agent chooses the target with the maximum expected reward with probability 1 \u2212 \u03b5 and with the probability that the agent chooses a random one. This strategy allows the agent to select the target that was inefficient but became efficient after the switching point."}, {"heading": "B. Discussion of experiment results", "text": "In fact, most of them are in a position to be able to move, to move, and indeed in the way in which they are able to move, in which they are able to stay in the world, in which they are able to live, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they"}, {"heading": "VI. CONCLUSION", "text": "We proposed a modification of the EA + RL method that preserves the best solution found. We considered two versions of the proposed modification. In the first version, called the modification of EA + RL without learning for error, the RL agent only learns when the algorithm finds a better solution. In the second version, called the modification of EA + RL with learning for error, the RL agent also learns when the algorithm achieves an inefficient solution. We considered two auxiliary goals that change their efficiency at the switching point. We analyzed the two proposed modifications and the EA + RL method on OMd, LEADINGONES, XDIVK with switching point in the middle of the optimization and XDIVK with switching point at the end of the switching point. Two state definitions were taken into account: the individual state and the destination state."}, {"heading": "VII. ACKNOWLEDGMENTS", "text": "This work was supported by the RFBR within the framework of research project no. 16-31-00380 mol a."}], "references": [{"title": "Runtime analysis of different approaches to select conflicting auxiliary objectives in the generalized onemax problem", "author": ["C. Segura", "C.A.C. Coello", "G. Miranda", "C. L\u00e9on", "\u201cUsing multiobjective evolutionary algorithms for single-objective optimization", "\u201d 4OR", "vol. 3", "no. 11", "pp. 201\u2013228", "2013. [5] J. Handl", "S.C. Lovell", "J.D. Knowles", "\u201cMultiobjectivization by decomposition of scalar cost functions", "\u201d in Parallel Problem Solving from Nature \u2013 PPSN X", "ser. Lecture Notes in Computer Science. Springer", "2008", "no. 5199", "pp. 31\u201340. [6] M. Buzdalov", "A. Buzdalova", "\u201cAdaptive selection of helper-objectives for test case generation", "\u201d in 2013 IEEE Congress on Evolutionary Computation", "vol. 1", "2013", "pp. 2245\u20132250. [7] D.F. Lochtefeld", "F.W. Ciarallo", "\u201cHelper-objective optimization strategies for the Job-Shop scheduling problem", "\u201d Applied Soft Computing", "vol. 11", "no. 6", "pp. 4161\u20134174", "2011. [8] M.T. Jensen", "\u201cHelper-objectives: Using multi-objective evolutionary algorithms for single-objective optimisation: Evolutionary computation combinatorial optimization", "\u201d Journal of Mathematical Modelling", "Algorithms", "vol. 3", "no. 4", "pp. 323\u2013347", "2004. [9] R.S. Sutton", "A.G. Barto", "MA Reinforcement Learning: An Introduction. Cambridge", "USA: MIT Press", "1998. [10] A. Buzdalova", "M. Buzdalov", "\u201cIncreasing efficiency of evolutionary algorithms by choosing between auxiliary fitness functions with reinforcement learning", "\u201d in Proceedings of the International Conference on Machine Learning", "Applications", "vol. 1", "2012", "pp. 150\u2013155. [11] M. Buzdalov", "A. Buzdalova", "\u201cOneMax helps optimizing XdivK: Theoretical runtime analysis for RLS", "EA+RL", "\u201d in Proceedings of Genetic", "Evolutionary Computation Conference Companion. ACM", "2014", "pp. 201\u2013202. [12] \u2014\u2014", "\u201cCan OneMax help optimizing LeadingOnes using the EA+RL method?\u201d in Proceedings of IEEE Congress on Evolutionary Computation", "2015", "pp. 1762\u20131768. [13] I. Petrova", "A. Buzdalova", "G. Korneev", "\u201cRuntime analysis of random local search with reinforcement based selection of non-stationary auxiliary objectives: initial study", "\u201d in Proceedings of 22nd International Conference on Soft Computing MENDEL 2016", "Czech Republic", "2016", "pp. 95\u2013102. [14] A. Buzdalova", "I. Petrova", "M. Buzdalov"], "venue": "Proceedings of IEEE Symposium Series on Computational Intelligence, 2016, pp. 280\u2013286. [15] A. Auger and B. Doerr, Theory of Randomized Search Heuristics: Foundations and Recent Developments. River Edge, NJ, USA: World Scientific Publishing Co., Inc., 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Efficiency of the target objective optimization can be increased by introducing some auxiliary objectives [1]\u2013[5].", "startOffset": 106, "endOffset": 109}], "year": 2017, "abstractText": "Efficiency of single-objective optimization can be improved by introducing some auxiliary objectives. Ideally, auxiliary objectives should be helpful. However, in practice, objectives may be efficient on some optimization stages but obstructive on others. In this paper we propose a modification of the EA+RL method which dynamically selects optimized objectives using reinforcement learning. The proposed modification prevents from losing the best found solution. We analysed the proposed modification and compared it with the EA+RL method and Random Local Search on XdivK, Generalized OneMax and LeadingOnes problems. The proposed modification outperforms the EA+RL method on all problem instances. It also outperforms the single objective approach on the most problem instances. We also provide detailed analysis of how different components of the considered algorithms influence efficiency of optimization. In addition, we present theoretical analysis of the proposed modification on the XdivK problem.", "creator": "LaTeX with hyperref package"}}}