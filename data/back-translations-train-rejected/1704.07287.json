{"id": "1704.07287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Joint Modeling of Text and Acoustic-Prosodic Cues for Neural Parsing", "abstract": "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing a spoken utterance, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and word-based prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together improve parse F1 scores significantly over a strong text-only baseline. For this study with known sentence boundaries, error analysis shows that the main benefit of acoustic-prosodic features is in sentences with disfluencies and that attachment errors are most improved.", "histories": [["v1", "Mon, 24 Apr 2017 15:33:26 GMT  (1214kb,D)", "http://arxiv.org/abs/1704.07287v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["trang tran", "shubham toshniwal", "mohit bansal", "kevin gimpel", "karen livescu", "mari ostendorf"], "accepted": false, "id": "1704.07287"}, "pdf": {"name": "1704.07287.pdf", "metadata": {"source": "CRF", "title": "Joint Modeling of Text and Acoustic-Prosodic Cues for Neural Parsing", "authors": ["Trang Tran", "Shubham Toshniwal", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Mari Ostendorf"], "emails": ["ostendor}@uw.edu,", "mbansal@cs.unc.edu,", "klivescu}@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to feel as if they are able to survive on their own, and that they will be able to survive on their own if they are not able to put themselves in a position to put themselves in a position to survive on their own."}, {"heading": "2 Related Work", "text": "A major challenge in analyzing conversations is the presence of disfluences, which are grammatical (and prosodic) interruptions. Refluencies often include repetitions (\"I am + I am\"), repairs (\"I am + we are\"), and restarts (\"What I am + today is...\"), with the \"+\" corresponding to a break point. Repairs often involve parallel constructions, but they can be more complex in terms of hedging, solving problems, etc. One solution to dealing with disfluencies is to first recognize them automatically, and then analyze the cleaned text. Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu."}, {"heading": "3 Task and Model Description", "text": "Next, we describe our encoder attention decoder model, which maps a sequence of word-level input functions to a linearized parse output sequence.The vector for word-level input functions consists of a concatenation of (learnable) word embeddings and several types of acoustic-prosodic features described in Section 3.3."}, {"heading": "3.1 Task Setup", "text": "We are assuming the availability of a conversational language training tree (in our case SwitchboardNXT (Calhoun et al., 2010)) and corresponding components of parses. Transcriptions are pre-processed by removing punctuation and printing all texts in undercases to better mimic speech recognition settings. According to Vinyals et al. (2015), parse trees are linearized, normalizing preterminals as well as \"XX.\" Figure 1 shows how to convert standard tree parses into encoder inputs and linearized gold standard parses."}, {"heading": "3.2 Encoder-Attention-Decoder Parser", "text": "Our attention-based encoder decoder model is similar to that of Vinyals et al. (2015).The encoder is a deep, long-term recursive neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a wordlevel input feature Sequence1, represented as a sequence of vectors x = (x1, \u00b7, xTs) and outputs of high-level properties h = (h1, \u00b7, hTs), where hi = LSTM (xi, hi \u2212 1).2The parse decoder is also a deep LSTM-RNN that predicts the linearized parse sequence. (2015), the input sequence is processed in reverse order, as in Figure 2.2For Brevity, which we emit the LSTM equations."}, {"heading": "3.3 Acoustic-Prosodic Features", "text": "In the previous work with encoder decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedded ei of the word at position i of the input sentence. Because the characteristics are different in nature (word measurements vs. time series), they are integrated into the encoder decoder, which uses different mechanisms. All the characteristics are extracted from transcriptions that are temporally oriented at the word level. In a small number of cases, the temporal alignment for a certain word boundary is missing, mainly due to tokenization. For example, contradictions, such as not in the original transcript, are referred to as separate words, do and do not, for the parsers and the internal word we are missing."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard-Corpus (Godfrey and Holliman, 1993). Switchboard I - Release 2 (Godfrey et al., 1993) is a collection of approximately 2,400 telephone conversations between strangers; 650 such conversations were later commented on with syntactic parses within the Penn Treebank Release 3 dataset (Marcus et al., 1999), and 642 of these were enhanced with richer layers of annotations made possible by the NITE XML toolkit (Calhoun et al., 2010). Our sentence segmentations and syntactic trees are based on annotations from the Treebank 3 set, with some manual corrections made from the NXT board."}, {"heading": "4.2 Evaluation Metric", "text": "The standard evaluation metric for component parsing is the parseval metric, which uses precision, recall and F1 in parentheses, as in the canonical implementation of EVALB.6 Transcribed speech, but includes disfluences, with language repairs (referred to as \"EDITED\" nodes in the control board parse trees) being particularly problematic for statistical parsers, as Charniak and Johnson (2001) explained. Therefore, our results are often not comparable to previous work on parsing components of the control board, where the removal and reinsertion of EDIT nodes was required during evaluation according to a number of rules (Charniak and Johnson, 2001; Kahn et al.)."}, {"heading": "4.3 Model Parameters and Training Details", "text": "Both the encoder and the decoder are 3-layer deep LSTM RNNNs with 256 hidden units in each layer. For site-conscious attention, convolution operation uses 5 convolution filters of width 40https: / / github.com / syllog1sm / swbd _ tools 6http: / / nlp.cs.nyu.edu / evalb / each. We use 512-dimensional embedding vectors to represent words and linearized parsing symbols, such as \"(S.\" A relatively small number of configurations are examined for the acoustic-prosodic features based on dev parsing performance. Pause embedding is matched to {4, 16, 32} dimensions. For CNN, we try different configurations of filter size combinations: {10, 25, 50], [5, 25, 50], and the number of filters per filter size {16, 128}, 127 and 127}."}, {"heading": "4.4 Inference", "text": "As a consequence, we use a greedy decoder to generate the linearized parse sequence. As shown in Figure 1, we also use a post-processing step that merges the original record marks with the decoder output to obtain 7So if we use filter sizes [10, 25, 50] with 16 of each type, we have a total of 48 filters. Also, note that the filter sizes are actually 6 x 10, 6 x 25 etc., but since the feature dimension is fixed (in our case 6), we only specify the filter width."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Text-only Results", "text": "We are experimenting with the purely content-based attention model proposed by Vinyals et al. (2015) and the Content + Location attention model proposed by Chorowski et al. (2015). For comparison with previous non-neural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), which has been retrained on our switchboard data. Table 2 compares the three purely text-based models. In terms of content + location attention, the Berkeley parser beats the Berkeley parser by about 2.5% and the purely content-based attention by about 4.5%. Interestingly, flat F1 scores for both encoder models are lower than their respective F1 scores."}, {"heading": "5.2 Adding Acoustic-Prosodic Features", "text": "We extend our text-only model with the three types of acoustic-prosodic characteristics: pause (p), word duration (\u03b4) and CNN mappings of basic frequency (f0) and energy (E) characteristics (f0 / E-CNN). There are 23 \u2212 1 = 7 model configurations that correspond to the possible combinations of acoustic-prosodic characteristics. Results of the 7 model configurations and text-only models in our dev set are shown in Table 4. 9. First, we note that adding any combination of acoustic-prosodic characteristics (singly or in sentences) improves performance over the pure text base. However, certain combinations of acoustic-prosodic characteristics are not guaranteed to be better than their subsets, sug-9For about 1% of the sets in both dev and the test, no temporal characteristics are available."}, {"heading": "6 Analysis", "text": "Both models perform worse on longer sentences, which is not surprising since the corresponding parse trees tend to be more complex; the difference in performance between our best model and the purely text-based model increases with sentence length; this can also be expected, since longer sentences are more likely to have several prosodic phrases and disfluences; because sentence limits are given and so many sentences are short in spontaneous speech, it is possible that the benefit of prosody is mainly related to disfluences. Table 6 shows parse values for the subset of flowing and disfluent sentences, suggesting that this may be the case. We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different types of parse."}, {"heading": "7 Conclusion", "text": "We have presented an attention-based conversation sentence parsing encoder model that achieves strong results when parsing text transcriptions, and further improved results when incorporating acoustic-prosodic features at the word level. Unlike previous work, we do not use an explicit step to detect disfluid, and we automatically learn mapping of f0 and energy contours with a CNN that is trained in conjunction with the attention model. Acoustic-prosodic features offer the greatest advantages when sentences are fluent or long, and the analysis of error types shows that these features are particularly helpful in repairing attachment errors. Further analysis may be helpful in identifying more specific contrasts, understanding how fine-grained duration would affect the results, and finding out which aspects of prosody CNN is learning. The significant shortcomings that arise when using only automatically learned features suggest that they may also be useful to others, such as those associated with problems."}, {"heading": "Acknowledgement", "text": "We thank Pranava Swaroop Madhyastha, Hao Tang, Jon Cai, Hao Cheng and Navdeep Jaitly for their help with the initial discussions and code setup. This research was partially funded by a Google Faculty Research Award to Mohit Bansal, Karen Livescu and Kevin Gimpel; and NSF grants the number IIS-1617176. Opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency."}, {"heading": "8 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Tree Examples", "text": "In Figures 6, 8, 9 and 10 we follow the node correction notations as in (Kummerfeld et al., 2012). In particular missing nodes are marked blue on the gold tree, additional nodes are marked red in the predicted tree, and yellow nodes mark intersections."}, {"heading": "8.2 Miscellany", "text": "Figure 7 shows the distribution of pause time in our training data. Our break buckets of 0 < p \u2264 0.05 s, 0.05 s < p \u2264 0.2 s, 0.2 < p \u2264 1 s and p > 1 s, which are described in the main article, are based on this figure. Table 8 shows the comprehensive error counts in all error categories in both flowing and non-flowing subsets."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems", "author": ["Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vi\u00e9gas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vi\u00e9gas et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The NXT-format Switchboard Corpus: a rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue", "author": ["Sasha Calhoun", "Jean Carletta", "Jason M. Brenier", "Neil Mayo", "Dan Jurafsky", "Mark Steedman", "David Beaver."], "venue": "Lan-", "citeRegEx": "Calhoun et al\\.,? 2010", "shortCiteRegEx": "Calhoun et al\\.", "year": 2010}, {"title": "Edit Detection and Parsing for Transcribed Speech", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proc. NAACL.", "citeRegEx": "Charniak and Johnson.,? 2001", "shortCiteRegEx": "Charniak and Johnson.", "year": 2001}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.1602.", "citeRegEx": "Chorowski et al\\.,? 2014", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Attention-Based Models for Speech Recognition", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1506.07503.", "citeRegEx": "Chorowski et al\\.,? 2015", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Exploiting prosody for PCFGs with latent annotations", "author": ["Markus Dreyer", "Izhak Shafran."], "venue": "Proc. Interspeech.", "citeRegEx": "Dreyer and Shafran.,? 2007", "shortCiteRegEx": "Dreyer and Shafran.", "year": 2007}, {"title": "An Introduction to the Bootstrap", "author": ["Bradley Efron", "Robert J. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani.,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1993}, {"title": "Disfluency Detection with a Semi-Markov Model and Prosodic Features", "author": ["James Ferguson", "Greg Durrett", "Dan Klein."], "venue": "Proc. NAACL.", "citeRegEx": "Ferguson et al\\.,? 2015", "shortCiteRegEx": "Ferguson et al\\.", "year": 2015}, {"title": "Articulatory strengthening at edges of prosodic domains", "author": ["C\u00e9cile Fourgeron", "Patricia A. Keating."], "venue": "Journal of the Acoustical Society of America 101(6):3728\u20133740.", "citeRegEx": "Fourgeron and Keating.,? 1997", "shortCiteRegEx": "Fourgeron and Keating.", "year": 1997}, {"title": "Using Integer Linear Programming for Detecting Speech Disfluencies", "author": ["Kallirroi Georgila."], "venue": "Proc. NAACL.", "citeRegEx": "Georgila.,? 2009", "shortCiteRegEx": "Georgila.", "year": 2009}, {"title": "A pitch extraction algorithm tuned for automatic speech recognition", "author": ["Pegah Ghahremani", "Bagher BabaAli", "Daniel Povey", "Korbinian Riedhammer", "Jan Trmal", "Sanjeev Khudanpur."], "venue": "Proc. ICASSP.", "citeRegEx": "Ghahremani et al\\.,? 2014", "shortCiteRegEx": "Ghahremani et al\\.", "year": 2014}, {"title": "Switchboard-1 Release 2", "author": ["John J. Godfrey", "Edward Holliman."], "venue": "Linguistic Data Consortium.", "citeRegEx": "Godfrey and Holliman.,? 1993", "shortCiteRegEx": "Godfrey and Holliman.", "year": 1993}, {"title": "Sentence-Internal Prosody Does not Help Parsing the Way Punctuation Does", "author": ["Michelle L Gregory", "Mark Johnson", "Eugene Charniak."], "venue": "Proc. NAACL.", "citeRegEx": "Gregory et al\\.,? 2004", "shortCiteRegEx": "Gregory et al\\.", "year": 2004}, {"title": "The patterns of silence: Performance structures in sentence production", "author": ["Fran\u0107ois Grosjean", "Lysiane Grosjean", "Harlan Lane."], "venue": "Cognitive Psychology .", "citeRegEx": "Grosjean et al\\.,? 1979", "shortCiteRegEx": "Grosjean et al\\.", "year": 1979}, {"title": "Pcfgs with syntactic and prosodic indicators of speech repairs", "author": ["John Hale", "Izhak Shafran", "Lisa Yung", "Bonnie Dorr", "Mary Harper", "Anna Krasnyanskaya", "Matthew Lease", "Yang Liu", "Brian Roark", "Mathew Snover", "Robin Stewart."], "venue": "Proc. COLING-ACL.", "citeRegEx": "Hale et al\\.,? 2006", "shortCiteRegEx": "Hale et al\\.", "year": 2006}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Joint Incremental Disfluency Detection and Dependency Parsing", "author": ["Matthew Honnibal", "Mark Johnson."], "venue": "TACL .", "citeRegEx": "Honnibal and Johnson.,? 2014", "shortCiteRegEx": "Honnibal and Johnson.", "year": 2014}, {"title": "Appropriately Handled Prosodic Breaks Help PCFG Parsing", "author": ["Zhongqiang Huang", "Mary Harper."], "venue": "Proc. NAACL.", "citeRegEx": "Huang and Harper.,? 2010", "shortCiteRegEx": "Huang and Harper.", "year": 2010}, {"title": "A TAGbased Noisy Channel Model of Speech Repairs", "author": ["Mark Johnson", "Eugene Charniak."], "venue": "Proc. ACL.", "citeRegEx": "Johnson and Charniak.,? 2004", "shortCiteRegEx": "Johnson and Charniak.", "year": 2004}, {"title": "Effective Use of Prosody in Parsing Conversational Speech", "author": ["Jeremy G. Kahn", "Matthew Lease", "Eugene Charniak", "Mark Johnson", "Mari Ostendorf."], "venue": "Proc. HLT/EMNLP.", "citeRegEx": "Kahn et al\\.,? 2005", "shortCiteRegEx": "Kahn et al\\.", "year": 2005}, {"title": "Joint reranking of parsing and word recognition with automatic segmentation", "author": ["Jeremy G. Kahn", "Mari Ostendorf."], "venue": "Computer Speech & Language .", "citeRegEx": "Kahn and Ostendorf.,? 2012", "shortCiteRegEx": "Kahn and Ostendorf.", "year": 2012}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output", "author": ["Jonathan K. Kummerfeld", "David Hall", "James R. Curran", "Dan Klein."], "venue": "Proc. EMNLP.", "citeRegEx": "Kummerfeld et al\\.,? 2012", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2012}, {"title": "Multi-task Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "Proc. ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Treebank-3", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary A. Marcinkiewicz", "Ann Taylor."], "venue": "Technical report, Linguistic Data Consortium.", "citeRegEx": "Marcus et al\\.,? 1999", "shortCiteRegEx": "Marcus et al\\.", "year": 1999}, {"title": "Learning Accurate, Compact, and Interpretable Tree Annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proc. COLINGACL.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour."], "venue": "Proc. International Conference on Frontiers in Handwriting Recognition (ICFHR).", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "The Kaldi Speech Recognition", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": null, "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "The Use of Prosody in Syntactic Disambiguation", "author": ["Patti Price", "Mari Ostendorf", "Stefanie ShattuckHufnagel", "Cynthia Fong."], "venue": "Proc. Workshop on Speech and Natural Language.", "citeRegEx": "Price et al\\.,? 1991", "shortCiteRegEx": "Price et al\\.", "year": 1991}, {"title": "Disfluency Detection Using Multi-step Stacked Learning", "author": ["Xian Qian", "Yang Liu."], "venue": "Proc. NAACL.", "citeRegEx": "Qian and Liu.,? 2013", "shortCiteRegEx": "Qian and Liu.", "year": 2013}, {"title": "Joint Parsing and Disfluency Detection in Linear Time", "author": ["Mohammad Sadegh Rasooli", "Joel Tetreault."], "venue": "Proc. EMNLP.", "citeRegEx": "Rasooli and Tetreault.,? 2013", "shortCiteRegEx": "Rasooli and Tetreault.", "year": 2013}, {"title": "Preliminaries to a theory of speech disfluencies", "author": ["Elizabeth Shriberg."], "venue": "Ph.D. thesis, Department of Psychology, University of California, Berkeley, CA.", "citeRegEx": "Shriberg.,? 1994", "shortCiteRegEx": "Shriberg.", "year": 1994}, {"title": "Tobi: A Standard for Labeling English Prosody", "author": ["Kim Silverman", "Mary Beckman", "John Pitrelli", "Mari Ostendorf", "Colin Wightman", "Patti Price", "Janet Pierrehumbert", "Julia Hirschberg."], "venue": "Proc. ICSLP.", "citeRegEx": "Silverman et al\\.,? 1992", "shortCiteRegEx": "Silverman et al\\.", "year": 1992}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proc. NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Segmental durations in the vicinity of prosodic phrase boundaries", "author": ["Colin W. Wightman", "Stefanie Shattuck-Hufnagel", "Mari Ostendorf", "Patti J. Price."], "venue": "Journal of the Acoustical Society of America 91(3).", "citeRegEx": "Wightman et al\\.,? 1992", "shortCiteRegEx": "Wightman et al\\.", "year": 1992}, {"title": "Recurrent Neural Network Regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "CoRR abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Disfluency Detection using a Bidirectional LSTM", "author": ["Victoria Zayats", "Hannaneh Hajishirzi", "Mari Ostendorf."], "venue": "Proc. Interspeech.", "citeRegEx": "Zayats et al\\.,? 2016", "shortCiteRegEx": "Zayats et al\\.", "year": 2016}, {"title": "Unediting: Detecting Disfluencies Without Careful Transcripts", "author": ["Victoria Zayats", "Mari Ostendorf", "Hannaneh Hajishirzi."], "venue": "Proc. NAACL.", "citeRegEx": "Zayats et al\\.,? 2015", "shortCiteRegEx": "Zayats et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "Joint speech recognition and parsing is useful for dealing with word errors and sentence segmentation (Kahn and Ostendorf, 2012).", "startOffset": 102, "endOffset": 128}, {"referenceID": 3, "context": "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 128, "endOffset": 184}, {"referenceID": 19, "context": "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 128, "endOffset": 184}, {"referenceID": 31, "context": "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 208, "endOffset": 265}, {"referenceID": 17, "context": "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 208, "endOffset": 265}, {"referenceID": 14, "context": "Studies show that speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners are able to use prosodic structure in resolving syntactic ambiguities (Price et al.", "startOffset": 91, "endOffset": 114}, {"referenceID": 29, "context": ", 1979), and listeners are able to use prosodic structure in resolving syntactic ambiguities (Price et al., 1991).", "startOffset": 93, "endOffset": 113}, {"referenceID": 32, "context": "Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994).", "startOffset": 73, "endOffset": 89}, {"referenceID": 10, "context": "Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu, 2013; Ferguson et al., 2015; Zayats et al., 2015, 2016).", "startOffset": 51, "endOffset": 137}, {"referenceID": 30, "context": "Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu, 2013; Ferguson et al., 2015; Zayats et al., 2015, 2016).", "startOffset": 51, "endOffset": 137}, {"referenceID": 8, "context": "Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu, 2013; Ferguson et al., 2015; Zayats et al., 2015, 2016).", "startOffset": 51, "endOffset": 137}, {"referenceID": 3, "context": "Such models have been explored for syntactic constituents (Charniak and Johnson, 2001; Kahn et al., 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 58, "endOffset": 105}, {"referenceID": 20, "context": "Such models have been explored for syntactic constituents (Charniak and Johnson, 2001; Kahn et al., 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 58, "endOffset": 105}, {"referenceID": 31, "context": ", 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 25, "endOffset": 82}, {"referenceID": 17, "context": ", 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).", "startOffset": 25, "endOffset": 82}, {"referenceID": 13, "context": "One study used quantized acousticprosodic cues as tokens in parsing, similar to punctuation, and observed a degradation in performance (Gregory et al., 2004).", "startOffset": 135, "endOffset": 157}, {"referenceID": 33, "context": "ers (prosodic breaks) based on the ToBI prosodic annotation system (Silverman et al., 1992).", "startOffset": 67, "endOffset": 91}, {"referenceID": 15, "context": "In some studies, automatically predicted prosodic breaks are incorporated directly (Hale et al., 2006; Dreyer and Shafran, 2007; Huang and Harper, 2010).", "startOffset": 83, "endOffset": 152}, {"referenceID": 6, "context": "In some studies, automatically predicted prosodic breaks are incorporated directly (Hale et al., 2006; Dreyer and Shafran, 2007; Huang and Harper, 2010).", "startOffset": 83, "endOffset": 152}, {"referenceID": 18, "context": "In some studies, automatically predicted prosodic breaks are incorporated directly (Hale et al., 2006; Dreyer and Shafran, 2007; Huang and Harper, 2010).", "startOffset": 83, "endOffset": 152}, {"referenceID": 20, "context": "Another study uses prosodic break posteriors in parsing (Kahn et al., 2005).", "startOffset": 56, "endOffset": 75}, {"referenceID": 1, "context": "Recently, attention-enabled encoder-decoder models (Bahdanau et al., 2015) have gained traction for constituency parsing, with Vinyals et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 24, "context": "Performance has since been improved by another ensemble of encoder-decoder models trained in a multi-task setting (Luong et al., 2016).", "startOffset": 114, "endOffset": 134}, {"referenceID": 1, "context": "Recently, attention-enabled encoder-decoder models (Bahdanau et al., 2015) have gained traction for constituency parsing, with Vinyals et al. (2015) achieving state-of-the-art results for the Wall Street Journal (WSJ) corpus using an ensemble.", "startOffset": 52, "endOffset": 149}, {"referenceID": 2, "context": "1 Task Setup We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses.", "startOffset": 117, "endOffset": 139}, {"referenceID": 2, "context": "1 Task Setup We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses. The transcriptions are preprocessed by removing punctuation and lower-casing all text to better mimic the speech recognition setting. Following Vinyals et al. (2015), the parse trees are linearized, with pre-terminals also normalized as \u201cXX\u201d.", "startOffset": 118, "endOffset": 345}, {"referenceID": 16, "context": "The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a wordlevel input feature sequence1, represented as a sequence of vectors x = (x1, \u00b7 \u00b7 \u00b7 ,xTs) and outputs high-level features h = (h1, \u00b7 \u00b7 \u00b7 ,hTs) where hi = LSTM(xi,hi\u22121).", "startOffset": 81, "endOffset": 115}, {"referenceID": 33, "context": "2 Encoder-Attention-Decoder Parser Our attention-based encoder-decoder model is similar to the one used by Vinyals et al. (2015). The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a wordlevel input feature sequence1, represented as a sequence of vectors x = (x1, \u00b7 \u00b7 \u00b7 ,xTs) and outputs high-level features h = (h1, \u00b7 \u00b7 \u00b7 ,hTs) where hi = LSTM(xi,hi\u22121).", "startOffset": 107, "endOffset": 129}, {"referenceID": 34, "context": "As in Vinyals et al. (2015), the input sequence is processed in reverse order, as shown in Figure 2.", "startOffset": 6, "endOffset": 28}, {"referenceID": 34, "context": "As in Vinyals et al. (2015), the input sequence is processed in reverse order, as shown in Figure 2. For brevity we omit the LSTM equations. The details can be found, e.g., in Zaremba et al. (2014). (y1, \u00b7 \u00b7 \u00b7 , yTo) as follows: P (y|x) = To \u220f", "startOffset": 6, "endOffset": 198}, {"referenceID": 34, "context": "The attention mechanism used by Vinyals et al. (2015) computes the context vector ct as follows: uit = v > tanh(W 1hi +W 2dt + ba) \u03b1t = softmax(ut) ct = Ts \u2211", "startOffset": 32, "endOffset": 54}, {"referenceID": 4, "context": "The extracted features are then incorporated in the alignment score calculation as: uit = v > tanh(W 1hi +W 2dt +W ff ti + ba) This phenomenon has been observed in encoder-decoder models for speech recognition (Chorowski et al., 2014).", "startOffset": 210, "endOffset": 234}, {"referenceID": 4, "context": "In particular, we use the attention mechanism proposed by Chorowski et al. (2015), in which \u03b1t\u22121 is represented via a feature vector: f t = F \u2217\u03b1t\u22121 where F \u2208 Rk\u00d7r represents k learnable convolution filters of width r.", "startOffset": 58, "endOffset": 82}, {"referenceID": 34, "context": "3 Acoustic-Prosodic Features In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.", "startOffset": 87, "endOffset": 129}, {"referenceID": 24, "context": "3 Acoustic-Prosodic Features In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.", "startOffset": 87, "endOffset": 129}, {"referenceID": 35, "context": "Word-final duration lengthening is a strong cue to prosodic phrase boundaries (Wightman et al., 1992).", "startOffset": 78, "endOffset": 101}, {"referenceID": 28, "context": "The contour features are extracted with 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011), motivated by the success of the associated f0 features in speech recognition (Ghahremani et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 11, "context": ", 2011), motivated by the success of the associated f0 features in speech recognition (Ghahremani et al., 2014).", "startOffset": 86, "endOffset": 111}, {"referenceID": 9, "context": "Multi-band energy features are used as a simple mechanism to capture articulatory strengthening at prosodic constituent onsets (Fourgeron and Keating, 1997).", "startOffset": 127, "endOffset": 156}, {"referenceID": 2, "context": "1 Dataset Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993).", "startOffset": 45, "endOffset": 67}, {"referenceID": 12, "context": ", 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993).", "startOffset": 44, "endOffset": 72}, {"referenceID": 12, "context": "Switchboard I \u2013 Release 2 (Godfrey and Holliman, 1993) is a collection of about 2,400 telephone conversations between strangers; 650 such conversations were later hand-annotated with syntactic parses as part of the Penn Treebank Release 3 dataset (Marcus et al.", "startOffset": 26, "endOffset": 54}, {"referenceID": 25, "context": "Switchboard I \u2013 Release 2 (Godfrey and Holliman, 1993) is a collection of about 2,400 telephone conversations between strangers; 650 such conversations were later hand-annotated with syntactic parses as part of the Penn Treebank Release 3 dataset (Marcus et al., 1999), and 642 of these were further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al.", "startOffset": 247, "endOffset": 268}, {"referenceID": 2, "context": ", 1999), and 642 of these were further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010).", "startOffset": 118, "endOffset": 140}, {"referenceID": 19, "context": "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.", "startOffset": 109, "endOffset": 206}, {"referenceID": 20, "context": "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.", "startOffset": 109, "endOffset": 206}, {"referenceID": 13, "context": "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.", "startOffset": 109, "endOffset": 206}, {"referenceID": 17, "context": "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.", "startOffset": 109, "endOffset": 206}, {"referenceID": 2, "context": "1 Dataset Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993). Switchboard I \u2013 Release 2 (Godfrey and Holliman, 1993) is a collection of about 2,400 telephone conversations between strangers; 650 such conversations were later hand-annotated with syntactic parses as part of the Penn Treebank Release 3 dataset (Marcus et al., 1999), and 642 of these were further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010). Our sentence segmentations and syntactic trees are based on the annotations from the Treebank 3 set, with a few manual corrections from the NXT release. This core dataset consists of 100K sentences, totaling 1M tokens forming a vocabulary of 13.5K words. We follow the sentence boundaries defined by the parsed data available 4. We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al.", "startOffset": 46, "endOffset": 930}, {"referenceID": 3, "context": "Therefore, besides the fact that we are using a larger training data set, our results are not comparable with those of previous work on Switchboard constituent parsing, which involved removing and re-inserting EDIT nodes during evaluation according to a set of rules (Charniak and Johnson, 2001; Kahn et al., 2005).", "startOffset": 267, "endOffset": 314}, {"referenceID": 20, "context": "Therefore, besides the fact that we are using a larger training data set, our results are not comparable with those of previous work on Switchboard constituent parsing, which involved removing and re-inserting EDIT nodes during evaluation according to a set of rules (Charniak and Johnson, 2001; Kahn et al., 2005).", "startOffset": 267, "endOffset": 314}, {"referenceID": 3, "context": "6 Transcribed speech, however, includes disfluencies, with speech repairs (labeled under \u201cEDITED\u201d nodes in Switchboard parse trees) being particularly problematic for statistical parsers, as explained by Charniak and Johnson (2001). For this reason, previous work has often explicitly detected EDIT regions in conjunction with parsing or as a preprocessing step.", "startOffset": 204, "endOffset": 232}, {"referenceID": 22, "context": "For optimization we use Adam (Kingma and Ba, 2014) with a minibatch size of 64.", "startOffset": 29, "endOffset": 50}, {"referenceID": 27, "context": "3 probability is applied on the output of all LSTM layers (Pham et al., 2014).", "startOffset": 58, "endOffset": 77}, {"referenceID": 26, "context": "For comparison with previous non-neural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), retrained on our Switchboard data.", "startOffset": 114, "endOffset": 135}, {"referenceID": 31, "context": "We experiment with the content-only attention model used by Vinyals et al. (2015) and the content+location attention model proposed by Chorowski et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 4, "context": "(2015) and the content+location attention model proposed by Chorowski et al. (2015). For comparison with previous non-neural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al.", "startOffset": 60, "endOffset": 84}, {"referenceID": 7, "context": "The p-values are estimated using a bootstrap test (Efron and Tibshirani, 1993) that simulates 105 random test draws.", "startOffset": 50, "endOffset": 78}, {"referenceID": 23, "context": "We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.", "startOffset": 36, "endOffset": 61}, {"referenceID": 23, "context": "1 Tree Examples In figures 6, 8, 9, and 10, we follow node correction notations as in (Kummerfeld et al., 2012).", "startOffset": 86, "endOffset": 111}], "year": 2017, "abstractText": "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing a spoken utterance, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and word-based prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together improve parse F1 scores significantly over a strong text-only baseline. For this study with known sentence boundaries, error analysis shows that the main benefit of acoustic-prosodic features is in sentences with disfluencies and that attachment errors are most improved.", "creator": "LaTeX with hyperref package"}}}