{"id": "1704.07329", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "A Trie-Structured Bayesian Model for Unsupervised Morphological Segmentation", "abstract": "In this paper, we introduce a trie-structured Bayesian model for unsupervised morphological segmentation. We adopt prior information from different sources in the model. We use neural word embeddings to discover words that are morphologically derived from each other and thereby that are semantically similar. We use letter successor variety counts obtained from tries that are built by neural word embeddings. Our results show that using different information sources such as neural word embeddings and letter successor variety as prior information improves morphological segmentation in a Bayesian model. Our model outperforms other unsupervised morphological segmentation models on Turkish and gives promising results on English and German for scarce resources.", "histories": [["v1", "Mon, 24 Apr 2017 17:07:26 GMT  (400kb,D)", "http://arxiv.org/abs/1704.07329v1", "12 pages, accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics"]], "COMMENTS": "12 pages, accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["murathan kurfal{\\i}", "ahmet \\\"ust\\\"un", "burcu can"], "accepted": false, "id": "1704.07329"}, "pdf": {"name": "1704.07329.pdf", "metadata": {"source": "CRF", "title": "A Trie-Structured Bayesian Model for Unsupervised Morphological Segmentation", "authors": ["Murathan Kurfal\u0131", "Ahmet \u00dcst\u00fcn", "Burcu Can"], "emails": ["kurfali@metu.edu.tr", "ustun.ahmet@metu.edu.tr", "burcucan@cs.hacettepe.edu.tr"], "sections": [{"heading": null, "text": "Keywords: unattended learning, morphology, morphological segmentation, Bayesian learning"}, {"heading": "1 Introduction", "text": "This process is mainly about pre-processing different word forms from a single root. It is impossible to create a dictionary that includes all possible word forms in a language and then use it in an NLP application. Hankamer points out that the number of possible word forms in an agglutinative language is infinite. Instead of developing a model based on word forms that encompasses all possible word forms in a language, it is used in an NLP application. Hankamer points out that the number of possible word forms in an agglutinative language is infinite."}, {"heading": "2 Related Work", "text": "Morphological segmentation, as one of the oldest fields in NLP, has been extensively studied. Deterministic methods are the oldest used in morphological segmentation. Harris [15] defines for the first time the distribution characteristics of letters in a word for unattended morphological segmentation. An example is in Figure 1. The LSV model is named after Harris, which defines the morpheme boundaries based on sequence of letters. When words are inserted into a tri, branches correspond to potential morphology boundaries. An example is in Figure 1. In this example, Re- is a potential prefix, and -s, -ed and -ing are potential suffixes in the tri due to the branching that occurs before these morphemes.The LSV model has been applied in various papers [13,11,1,2,3]. In our study, we also use an LSV-inspired predecessor model, but this time in a Bayesian framework.Stomework.methods have also been applied."}, {"heading": "3 Building Neural Word Embedding-based Tries", "text": "Our model is based on experiments on neural word embedding, which are based on two different methods:"}, {"heading": "3.1 Tries Structured from the Same Stem", "text": "These experiments contain semantically related (morphologically derived or mutually inflected) words that have the same stem. To find the stem of a given word in the training set, we used the algorithm introduced in [21]. In the algorithm, all potential prefixes of a word are extracted. For example, fe, fea, fear, fear, fear are the first valid prefix referring to the first segmentation point. Other segmentation points are found by the cosinal similarity between the word and the first prefix (from the right of the word; i.e., fearful) being higher than a manually set threshold 3 referring to the first segmentation point. Other segmentation points are found by repeating the process towards the word head by checking the cosmic similarity between the currently determined valid prefix and the subsequent terminology on the left side of the word."}, {"heading": "3.2 Tries Based on Semantic Relatedness", "text": "Semantically related 50 words are retrieved for each word in the training set using word2vec [17]. For each word, a Trie is built and 50 similar words are inserted on the Trie of the word. Finally, for each word in the training set, a Trie consisting of 51 words is created. A part of a Trie comprising semantically related words is given in Figure 3."}, {"heading": "4 Bayesian Model Definition", "text": "We define a Bayesian model to find the morpheme boundaries in the corpus: p = DP = p = p = p = p = p = p = p = p = p (model) p (model) p (1), where corpus is a list of raw words and model denotes the segmentation of the corpus. The model that maximizes the given posterior probability is searched for the segmentation task. We apply a uniform model in which the uniform model is used for probability: p (corpus | model) = | W | i p (wi = mi2 + mi2 + mimiti | model). We apply a uniform model in which the biometric word in corpus = {w1, \u00b7 w | W | W, mij is the morpheme in wi, ti is the number of morphemes in word wi, and | W | W is the number of words in word wi."}, {"heading": "5 Inference", "text": "A binary segmentation of the word is sampled from the given posterior distribution: p (wi = mi1 + mi2 | corpus \u2212 wi, model \u2212 wi, \u03b1, \u03bb, \u03b3), p (mi1 | model \u2212 wi, \u03b1, \u03b3) p (mi2 | model \u2212 wi, \u03b1, \u03b3) p (bi1) (11) Once a binary segmentation is sampled, another binary segmentation is sampled for Mix 1. Therefore, a left recursion is applied to the left part of the word, due to the cosine similarity that is calculated between neural word embeddings of word forms and not to suffixes by the original Word2vec. This process is repeated recursively until at least 4 letters are present in the stem or the word itself is sampled from the figure or the word in Figure 4 (i.4) is not sampled in the figure."}, {"heading": "6 Segmentation", "text": "Once the model is learned, each invisible word can be segmented using the learned model. Each word is divided according to the maximum probability in the learned model: arg max mi1, \u00b7 \u00b7 \u00b7, miti p (wi = mi1 + \u00b7 \u00b7 \u00b7 \u00b7 + miti | model, \u03b1, \u03b3) (12) For the segmentation, we apply two different strategies. In both methods, we select the segmentation with the maximum probability, but the amount of possible segmentations for the given word differs. In the first method, we only consider the segmentations that are learned by the model. Since the same word can exist in several attempts, a word can have more than one different segmentation. In the second method, we look at all possible segmentations of a word and select the word with the maximum probability."}, {"heading": "7 Experiments and Results", "text": "In fact, we will be able to go in search of a solution that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us."}, {"heading": "8 Conclusion and Future Work", "text": "We propose a Bayesian model that uses semantically constructed Trie structures, which are constructed by using neural word embedding (i.e., by word2vec [17]) for morphological segmentation in an unattended environment. To this end, our tri-structured model performs well on comparatively smaller datasets. Compared to other available systems, our model outperforms them despite the limited training data, showing that the small size of the data can be compensated to some extent by structured data, which is the main contribution of this paper."}, {"heading": "Acknowledgments", "text": "This research is supported by the Turkish Science and Technology Council (TUBITAK) under project number EEAG-115E464."}], "references": [{"title": "Unsupervised knowledge-free morpheme boundary detection", "author": ["S. Bordag"], "venue": "Proceedings of the RANLP 2005", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Two-step approach to unsupervised morpheme segmentation", "author": ["S. Bordag"], "venue": "Proceedings of 2nd Pascal Challenges Workshop. pp. 25\u201329", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised and knowledge-free morpheme segmentation and analysis", "author": ["S. Bordag"], "venue": "Advances in Multilingual and Multimodal Information Retrieval pp. 881\u2013891", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Statistical Models for Unsupervised Learning of Morphology and POS tagging", "author": ["B. Can"], "venue": "Ph.D. thesis, Department of Computer Science, The University of York", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic hierarchical clustering of morphological paradigms", "author": ["B. Can", "S. Manandhar"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. pp. 654\u2013663. EACL \u201912, Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Explaining the Gibbs sampler", "author": ["G. Casella", "E.I. George"], "venue": "The American Statistician 46(3), 167\u2013174", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised segmentation of words using prior distributions of morph length and frequency", "author": ["M. Creutz"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics. pp. 280\u2013287. Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervised discovery of morphemes", "author": ["M. Creutz", "K. Lagus"], "venue": "Proceedings of the ACL-02 workshop on Morphological and phonological learning. pp. 21\u201330. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Inducing the morphological lexicon of a natural language from unannotated text", "author": ["M. Creutz", "K. Lagus"], "venue": "Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR 2005). pp. 106\u2013113", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["M. Creutz", "K. Lagus"], "venue": "ACM Transactions on Speech Language Processing 4, 1\u201334", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Morphemes as necessary concept for structures discovery from untagged corpora", "author": ["H. D\u00e9jean"], "venue": "Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning. pp. 295\u2013298. Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Interpolating between types and tokens by estimating power-law generators", "author": ["S. Goldwater", "M. Johnson", "T.L. Griffiths"], "venue": "Advances in Neural Information Processing Systems 18, pp. 459\u2013466. MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Word segmentation by letter successor varieties", "author": ["M.A. Hafer", "S.F. Weiss"], "venue": "Information Storage and Retrieval 10(11-12), 371 \u2013 385", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1974}, {"title": "Finite state morphology and left to right phonology", "author": ["J. Hankamer"], "venue": "Proceedings of the West Coast Conference on Formal Linguistics. vol. 5, pp. 41\u201352", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1986}, {"title": "From phoneme to morpheme", "author": ["Z.S. Harris"], "venue": "Language 31(2), 190\u2013222", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1955}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "An unsupervised method for uncovering morphological chains", "author": ["K. Narasimhan", "R. Barzilay", "T.S. Jaakkola"], "venue": "Transactions of the Association for Computational Linguistics 3, 157\u2013167", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["B. Snyder", "R. Barzilay"], "venue": "Proceedings of ACL-08: HLT. pp. 737\u2013745. Association for Computational Linguistics", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["R. Soricut", "F. Och"], "venue": "Proceedings of the Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL. pp. 1627\u20131637. Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised morphological segmentation using neural word embeddings", "author": ["A. \u00dcst\u00fcn", "B. Can"], "venue": "Statistical Language and Speech Processing: 4th International Conference, SLSP 2016, Pilsen, Czech Republic, October 11-12, 2016, Proceedings. pp. 43\u201353. Springer International Publishing", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Hankamer [14] suggests that the number of possible word forms in an agglutinative language such as Turkish is infinite.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "For example, [7] utilize frequency and length information of morphemes as prior information, which provide some orthographic features.", "startOffset": 13, "endOffset": 16}, {"referenceID": 15, "context": "We use orthographic features such as letter successor variety (LSV) counts obtained from tries, semantic information obtained from the neural word embeddings [17] to measure the semantic relatedness between substrings of a word, and we use the presence information of a stem in a dataset after its suffixes are stripped off assuming a concatenative morphology.", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "Harris [15] defines the distributional characteristics of letters in a word for the first time for unsupervised morphological segmentation.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 10, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 0, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 1, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 2, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 7, "context": "Morfessor is the name of the family of a group of unsupervised morphological segmentation systems which are all stochastic [8,10,9].", "startOffset": 123, "endOffset": 131}, {"referenceID": 9, "context": "Morfessor is the name of the family of a group of unsupervised morphological segmentation systems which are all stochastic [8,10,9].", "startOffset": 123, "endOffset": 131}, {"referenceID": 8, "context": "Morfessor is the name of the family of a group of unsupervised morphological segmentation systems which are all stochastic [8,10,9].", "startOffset": 123, "endOffset": 131}, {"referenceID": 11, "context": "Non-parametric Bayesian models have also been applied in morphological segmentation [12,19,5].", "startOffset": 84, "endOffset": 93}, {"referenceID": 17, "context": "Non-parametric Bayesian models have also been applied in morphological segmentation [12,19,5].", "startOffset": 84, "endOffset": 93}, {"referenceID": 4, "context": "Non-parametric Bayesian models have also been applied in morphological segmentation [12,19,5].", "startOffset": 84, "endOffset": 93}, {"referenceID": 16, "context": "[18] use semantic similarity obtained from neural word embeddings by word2vec [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] use semantic similarity obtained from neural word embeddings by word2vec [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "Potential morpheme boundaries on a trie used in LSV model [4]", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "[18] adopt the semantic similarity as a feature in a log-linear model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Soricut and Och [20] use word embeddings to learn morphological rules in an unsupervised setting.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "In order to find the stem of a given word in the training set, we used the algorithm which is introduced in [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "25 as the threshold following [21].", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Among the nearest 50 neighbors of the stem which are obtained from word2vec [17], the ones that begin with the same stem are inserted to the same trie.", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "Semantically related 50 words are retrieved for each word in the training set by using word2vec [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "We use Gibbs sampling [6] for the inference.", "startOffset": 22, "endOffset": 25}, {"referenceID": 15, "context": "Once the tries have been built by recursively augmenting the tries by using word2vec[17], eventually we obtained 2560 English word types, 43884 Turkish word types, and 13747 German word types in the tries structured from similar stems (see Section 3.", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "We used 200-dimensional word embeddings that were obtained by training word2vec [17] on 361 million word tokens and 725.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "We compared our model with Morfessor Baseline [8] (M-Baseline), Morfessor CatMap [9] (M-CatMAP) and MorphoChain System [18].", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "We compared our model with Morfessor Baseline [8] (M-Baseline), Morfessor CatMap [9] (M-CatMAP) and MorphoChain System [18].", "startOffset": 81, "endOffset": 84}, {"referenceID": 16, "context": "We compared our model with Morfessor Baseline [8] (M-Baseline), Morfessor CatMap [9] (M-CatMAP) and MorphoChain System [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "Our trie-structured model (TST) performs better than Morfessor Baseline [8], Morfessor CatMAP [9] and Morphological Chain [18] on Turkish with a Fmeasure of %44.", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Our trie-structured model (TST) performs better than Morfessor Baseline [8], Morfessor CatMAP [9] and Morphological Chain [18] on Turkish with a Fmeasure of %44.", "startOffset": 94, "endOffset": 97}, {"referenceID": 16, "context": "Our trie-structured model (TST) performs better than Morfessor Baseline [8], Morfessor CatMAP [9] and Morphological Chain [18] on Turkish with a Fmeasure of %44.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "obtained from word2vec [17]) for morphological segmentation in an unsupervised setting.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "In this paper, we introduce a trie-structured Bayesian model for unsupervised morphological segmentation. We adopt prior information from different sources in the model. We use neural word embeddings to discover words that are morphologically derived from each other and thereby that are semantically similar. We use letter successor variety counts obtained from tries that are built by neural word embeddings. Our results show that using different information sources such as neural word embeddings and letter successor variety as prior information improves morphological segmentation in a Bayesian model. Our model outperforms other unsupervised morphological segmentation models on Turkish and gives promising results on English and German for scarce resources.", "creator": "LaTeX with hyperref package"}}}