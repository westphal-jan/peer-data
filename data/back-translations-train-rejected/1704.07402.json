{"id": "1704.07402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Towards Instance Segmentation with Object Priority: Prominent Object Detection and Recognition", "abstract": "This manuscript introduces the problem of prominent object detection and recognition. The problem deals with finding the most important region of interest, segmenting the relevant item/object in that area, and assigning it an object class label. In other words, we are solving the three problems of saliency modeling, saliency detection, and object recognition under one umbrella. The motivation behind such a problem formulation is 1) the benefits to the knowledge representation-based vision pipelines, and 2) the potential improvements in emulating bio-inspired vision systems by solving these three problems together. We propose a method as a baseline for further research. The proposed model predicts the most important area in the image, segments the associated objects, and labels them. The proposed problem and method are evaluated against human fixations, annotated segmentation masks, and object class categories. We define a chance level for each of the evaluation criterion to compare the proposed algorithm with. Despite the good performance of the proposed baseline, the overall evaluations indicate that the problem of prominent object detection and recognition is a challenging task that is still worth investigating further.", "histories": [["v1", "Mon, 24 Apr 2017 18:12:12 GMT  (5527kb,D)", "https://arxiv.org/abs/1704.07402v1", null], ["v2", "Fri, 4 Aug 2017 12:15:34 GMT  (1642kb,D)", "http://arxiv.org/abs/1704.07402v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed r tavakoli", "jorma laaksonen"], "accepted": false, "id": "1704.07402"}, "pdf": {"name": "1704.07402.pdf", "metadata": {"source": "CRF", "title": "Towards Instance Segmentation with Object Priority: Prominent Object Detection and Recognition", "authors": ["Hamed R. Tavakoli", "Jorma Laaksonen"], "emails": ["hamed.r-tavakoli@aalto.fi", "jorma.laaksonen@aalto.fi"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to recognize themselves and understand what they are doing. (...) Most of them are able to identify themselves and understand themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. \"(...) Most of them are not able to identify themselves.\" (...) Most of them are not able to identify themselves."}, {"heading": "2. Related studies", "text": "This year, as never before, it will be able to retaliate in order to retaliate."}, {"heading": "3. Method", "text": "The model follows a hierarchical vision pipeline = 24 x 24 x 24. That is, the processed image is first broken down into features defined by Convolutionary Neural Networks (CNNs), which are used in a randomly weighted feedback network (RFNN) to identify the most important area of the image. In the rest of this section, we explain the details of these pipeline.The pipeline consists of two streams of Convolutionary Neural Networks that share their weights. The Convolutionary Layers follow the VG16 [20] architecture. The input to the Saliency Predictions consists of images of different resolutions corresponding to an image pyramid of size."}, {"heading": "4. Experiments", "text": "The evaluation of the total number of segmented object categories is 459, including the 20 object categories of the PASCAL problem 2010.C Assessment of the underlying problem areas \u03b2, segments the associated object and assigns a class label to it, requires data that provides the priority of objects in semantically segmented images. This subset provides eye fixations of 8 subjects in a freeviewing task in which each image was presented for 2 seconds with recalibration between all 25 images. Each image has complete contextual segmentation for all objects and items in the scene. The most important objects are also identified via a mouse click experiment of 12 independent subjects. For an object, the salt value is defined by dividing the number of clicks received on the number of subjects. The total number of segmentation categories includes the 20 object categories of the PASCAL problem 2010.C evaluation."}, {"heading": "5. Results", "text": "Table 1 summarizes the performance of the proposed model in predicting the right region of interest with respect to NSS. The proposed model performs significantly better than chance. However, there is a significant gap between the proposed model and the human upper limit performance. Table 2 reports the F\u03b2 performance for the segments generated by our model. While the performance of the proposed method is above chance, there is room for improvement in this task. A closer look at the mean of precision and recall indicates that compared to the probability model on average 1) the proposed segmentation is less false positive, and 2) the proposed model among the segments is the objects. Overall, the precision and recall indicates that the accuracy of the segmentation pipeline needs to be improved in order to reduce the number of missteps and increase the true positive rate, while preventing false positive detections. The third performance criterion to check is the accuracy of the object category detection of the detection category, summarized in this Confusion Category 3."}, {"heading": "6. Discussion", "text": "In this paper, we have the problem of validity and object detection to formulate the research problem of the most prominent object detection and detection. We have proposed a simple, learnable method that can serve as a basis for further research. However, the performance of the proposed method has been evaluated in terms of consistency with human fixations, and the object segments detected are in a position to require a new problem formulation. The three domains of (1) validity detection and (3) object detection are saturated by various algorithms. Most of the most recent states of art detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-detection-"}, {"heading": "7. Conclusions", "text": "The challenge is to find the most important region of interest, segment the relevant object in this area and assign it an object class naming. The proposed method follows a feed-forward bio-inspired vision system pipeline using deep learning-based features as the main component. The pipeline is modular in design and trained in several steps. We avoid fine-tuning based on the evaluation dataset. Pipeline results include: a predicted fixation density map, the most important item segment and the identified class naming. Therefore, the performance of the method is evaluated based on three types of soil truth data, including human fixation points, human-annotated segmentation masks and object naming. Despite the good performance of the proposed baseline, overall evaluations indicate that the proposed problem poses a challenge. Solving this challenging problem may 1) help us get closer to mimicking the advantages of primate systems and the 2) pipelines."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Salient object detection: A benchmark", "author": ["A. Borji", "M.-M. Cheng", "H. Jiang", "J. Li"], "venue": "IEEE TIP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "State-of-the-art in visual attention modeling", "author": ["A. Borji", "L. Itti"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Saliency based on information maximization", "author": ["N.D.B. Bruce", "J.K. Tsotsos"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results. http://www.pascalnetwork.org/challenges/VOC/voc2010/workshop/index.html", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Performance evaluation of neuromorphic-vision object recognition algorithms", "author": ["R. Kasturi", "D.B. Goldgof", "R. Ekambaram", "G. Pratt", "E. Krotkov", "D.D. Hackett", "Y. Ran", "Q. Zheng", "R. Sharma", "M. Anderson", "M. Peot", "M. Aguilar", "D. Khosla", "Y. Chen", "K. Kim", "L. Elazary", "R.C. Voorhies", "D.F. Parks", "L. Itti"], "venue": "In 2014 22nd International Conference on Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The secrets of salient object segmentation", "author": ["Y. Li", "X. Hou", "C. Koch", "J.M. Rehg", "A.L. Yuille"], "venue": "In 2014 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Rover visual obstacle avoidance", "author": ["H.P. Moravec"], "venue": "Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1981}, {"title": "Components of bottom-up gaze allocation in natural images", "author": ["R.J. Peters", "A. Iyer", "L. Itti", "C. Koch"], "venue": "Vision Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Optimizing binary mrfs via extended roof duality", "author": ["C. Rother", "V. Kolmogorov", "V. Lempitsky", "M. Szummer"], "venue": "In Proc Comp. Vision Pattern Recogn. (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Feedforward neural networks with random weights", "author": ["W.F. Schmidt", "M.A. Kraaijveld", "R.P.W. Duin"], "venue": "In Proceedings.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Exploiting scene context for image captioning", "author": ["R. Shetty", "H.R.-Tavakoli", "J. Laaksonen"], "venue": "In Proceedings of the 2016 ACM Workshop on Vision and Language Integration Meets Multimedia Fusion,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features", "author": ["H.R. Tavakoli", "A. Borji", "J. Laaksonen", "E. Rahtu"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}], "referenceMentions": [{"referenceID": 12, "context": "Comparing the current achievements to the robot visual obstacle avoidance techniques in early 80s [13], they all have one thing in common, that is, they are inspired by the human vision system.", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "To date, we are capable of detecting and recognizing all the objects in a scene using semantic segmentation techniques [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "With a high degree of accuracy, we can localize and recognize the objects of interest using efficient object localization and recognition methods [15, 16].", "startOffset": 146, "endOffset": 154}, {"referenceID": 15, "context": "With a high degree of accuracy, we can localize and recognize the objects of interest using efficient object localization and recognition methods [15, 16].", "startOffset": 146, "endOffset": 154}, {"referenceID": 21, "context": "Examples of such inferences can be found in applications, including, image captioning [22], creative image caption generation [10], visual question answering [1], etc.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Examples of such inferences can be found in applications, including, image captioning [22], creative image caption generation [10], visual question answering [1], etc.", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "Examples of such inferences can be found in applications, including, image captioning [22], creative image caption generation [10], visual question answering [1], etc.", "startOffset": 158, "endOffset": 161}, {"referenceID": 18, "context": "Nonetheless, such end-to-end techniques are not necessarily successful in key object identification as depicted in Figure 1, where the displayed caption is generated by a neural translation technique [19].", "startOffset": 200, "endOffset": 204}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The algorithms of saliency detection are well compared and detailed in [2], we thus avoid going through all of them here.", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "The most relevant work to us is [11], which signifies this connection.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": ", [6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", using a winner-take-all approach as described in [8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "The convolutional layers follow the VGG16 [20] architecture.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "Building on top of the iSEEL [21] framework, we employ an ensemble of neural predictors.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "To estimate the saliency, a mapping based on Randomly-weighted Feedforward Neural Network (RFNN) [18] is used.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "In particular, we employ Extreme Learning Machines (ELM) [7].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "For this purpose, we used the Toronto database [4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 16, "context": "[17] in order to infer the extent of the salient object in an eight-connected pixel array setup.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "special case of pyramid pooling with one pyramid level in consistency with the Fast R-CNN [6] implementation.", "startOffset": 90, "endOffset": 93}, {"referenceID": 20, "context": "Please consult iSEEL model [21] for more details.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "To the best of our knowledge, the best suitable existing data is a subset of the SSOS [11] dataset, consisting of 850 images of the PASCAL VOC 2010 [5] dataset.", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "To the best of our knowledge, the best suitable existing data is a subset of the SSOS [11] dataset, consisting of 850 images of the PASCAL VOC 2010 [5] dataset.", "startOffset": 148, "endOffset": 151}, {"referenceID": 13, "context": "We chose the Normalized Scanpath Score (NSS) [14] to measure the consistency between human and machine.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "Following the saliency segmentation techniques and surveys [2], we use \u03b2 = 0.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "We conducted a similar analysis using the Average Annotation Map (AAM), akin to [2], in order to build a chance model for the segmentation task.", "startOffset": 80, "endOffset": 83}], "year": 2017, "abstractText": "This manuscript introduces the problem of prominent object detection and recognition inspired by the fact that human seems to priorities perception of scene elements. The problem deals with finding the most important region of interest, segmenting the relevant item/object in that area, and assigning it an object class label. In other words, we are solving the three problems of saliency modeling, saliency detection, and object recognition under one umbrella. The motivation behind such a problem formulation is (1) the benefits to the knowledge representation-based vision pipelines, and (2) the potential improvements in emulating bio-inspired vision systems by solving these three problems together. We are foreseeing extending this problem formulation to fully semantically segmented scenes with instance object priority for high-level inferences in various applications including assistive vision. Along with a new problem definition, we also propose a method to achieve such a task. The proposed model predicts the most important area in the image, segments the associated objects, and labels them. The proposed problem and method are evaluated against human fixations, annotated segmentation masks, and object class categories. We define a chance level for each of the evaluation criterion to compare the proposed algorithm with. Despite the good performance of the proposed baseline, the overall evaluations indicate that the problem of prominent object detection and recognition is a challenging task that is still worth investigating further.", "creator": "LaTeX with hyperref package"}}}