{"id": "1704.07415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "abstract": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems.", "histories": [["v1", "Mon, 24 Apr 2017 18:49:38 GMT  (498kb,D)", "http://arxiv.org/abs/1704.07415v1", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yichen gong", "samuel r bowman"], "accepted": false, "id": "1704.07415"}, "pdf": {"name": "1704.07415.pdf", "metadata": {"source": "CRF", "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "authors": ["Yichen Gong", "Samuel R. Bowman"], "emails": ["yichen.gong@nyu.edu", "bowman@nyu.edu"], "sections": [{"heading": null, "text": "To address the problem that the single-pass model cannot reflect and correct its response, we introduce Ruminating Reader. Ruminating Reader adds a second attention cycle and a novel information fusion to the BIDAF model. We propose novel layer structures that construct a query-aware context vector representation and ensure a representation responsible for the encoding with an interim representation based on the BIDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we found that the reader far exceeds the BIDAF baseline and outperforms or outperforms all other published systems."}, {"heading": "1 Introduction", "text": "Most of them are able to play by the rules they have established in recent years."}, {"heading": "2 Question Answering", "text": "The task of the Ruminate Reader is to answer a question by reading and understanding a paragraph of text and selecting a word span within the context. Formally, the training and development data consists of tuples (Q, P, A), where Q = (q1,..., qi,... q | Q | |) is the question, a word sequence with the length | Q |, C = (c1,... cj,..., c | C |) is the context, a word sequence with the length | C |, and A = (ab, ae) is the answer span that marks the beginning and end indexes of the answer in context (1 < = ab < = ae < = = | C |). SQuAD The SQuAD corpus consists of 536 articles randomly selected from the English Wikipedia. Pictures, numbers, tables are removed and all paragraphs shorter than 500 characters are discarded. Unlike other data such as CNN / Daily Mail are put together from a series of 7AD answers (iAD)."}, {"heading": "3 Our Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Ruminating Reader", "text": "In this section we review the BIDAF model (Seo et al., 2017) and introduce our expansion, the QQ-Q-Q. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model. (D) We review the BIDAF model."}, {"heading": "4 Related Work", "text": "Recently, both QA and Cloze-like machine understanding tasks such as CNN / Daily Mail have made rapid progress. Much of this recent work is based on end-to-end trained neural network models, and in this context, most recursive neural networks have used soft attention (Bahdanau et al., 2015), which emphasize some of the data over the others. These models can be roughly divided into two categories: single-pass and multi-pass reasoning systems (Vinyals et al., 2015) to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as the use of a pointer network (Vinyals et al., 2015) to extract the response span from the context. Xiong et al et al. (2017) suggest the dynamic Coattention Network, which uses codependent representations of the question and context, updated and context, and erative."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Implementation details", "text": "Our model configuration largely follows that of Seo et al. (2017): In the character encoding layer, we use 100 filters of width 5. In the rest of the model, we set the dimension of the hidden layer (d) to 100. We use pre-formed 100D GloVe vectors (6B token version) as Word embeddings. Failed tokens are represented by a UNK symbol in the Word embedding layer, but are usually handled by the character embedding layer. The learning rate starts at 0.5 and drops to 0.2 as soon as the model does not improve any more. For optimization, we use the optimizer AdaDelta (Lines, 2012). We selected hyperparameter values by random search (Bergstra and Bengio, 2012). The batch size is 30. The learning rate starts at 0.5 and decreases to 0.2 as the model no longer improves. The L2 regulation weight is 1e-4, the AQSL weight is the drop rate of 1% and the Drop rate is 0.1%."}, {"heading": "5.2 Evaluation Method", "text": "Rajpurkar et al. (2016) provide an official evaluation script that allows us to measure the F1 score and the EM score by comparing the prediction and basic truth answers. Three answers are provided for each question: The prediction is compared with each answer and the best score is selected; the F1 score is defined by retrieval and precision of words; and the EM score is defined as the Exact Match Score as a 100% score in the prediction. We do not use any similarities and compare our results primarily with other individual models (non-ensemble) results; the performance of the test set is evaluated by the administrator in the CodaLab."}, {"heading": "5.3 Results", "text": "At the time of submission, our model is tied to accuracy on the hidden test set with the best performing published single model (Zhang et al., 2017). We achieve an F1 score of 79.5 and an EC score of 70.6. The current ranking is listed in Table 2. The ranking is listed in descending order of the F1 score, but if the F1 score of an entry is better than that of the adjacent entry, while its EM score is worse, then these two entries are considered equivalent."}, {"heading": "5.4 Analysis", "text": "This year it will be so far that it will be able to use the mentionlcihsrcsrteeSe rf\u00fc eid rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "6 Conclusion", "text": "We propose the Ruminating Reader, an extension of the BIDAF model with double attention. The model far exceeds the performance of the original BIDAF model in the Stanford Question Answering Dataset (SQuAD) and builds on the best-published system. These results and our qualitative analysis suggest that the model successfully combines the information from two read-throughs using gating and uses the result to find appropriate answers to Wikipedia questions. An ablation experiment shows that every component of this complex model contributes significantly to this. In future work, we aim to find ways to simplify this model without compromising performance, explore the possibility of even deeper models and extend our study to tasks of machine understanding in a broader sense."}, {"heading": "Acknowledgments", "text": "We thank Pranav Rajpurkar for testing Ruminate Reader on SQuAD hidden test set."}, {"heading": "A Appendix", "text": "In this section we show the setup of the layer Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Q"}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["van", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "van et al\\.,? \\Q2015\\E", "shortCiteRegEx": "van et al\\.", "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Random Search for Hyper-Parameter Optimization", "author": ["James Bergstra", "Yoshua Bengio."], "venue": "Proc. JMLR.", "citeRegEx": "Bergstra and Bengio.,? 2012", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."], "venue": "Proc. ACL.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Reading Wikipedia to Answer OpenDomain Questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."], "venue": "Proc. ACL.", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Gated-Attention Readers for Text Comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "CoRR.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Teaching Machines to Read and Comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Proc. NIPS.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The Goldilocks Principle: Reading Children\u2019s Books with Explicit Memory Representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "Proc. ICLR.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "volume 9, pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text Understanding with the Attention Sum Reader Network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "Proc. ACL.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "Proc. AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "Proc. ACL.", "citeRegEx": "Lee et al\\.,? 2017", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "author": ["Kenton Lee", "Shimi Salant", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das", "Jonathan Berant."], "venue": "ArXiv:1611.01436.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "Proc. CoCo@NIPS.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD - 100, 000+ Questions for Machine Comprehension of Text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proc. EMNLP.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional Attention Flow for Machine Comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "Proc. ICLR.", "citeRegEx": "Seo et al\\.,? 2017", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."], "venue": "Proc. NIPS.", "citeRegEx": "Shen et al\\.,? 2017", "shortCiteRegEx": "Shen et al\\.", "year": 2017}, {"title": "Iterative Alternating Neural Attention for Machine Reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio."], "venue": "CoRR.", "citeRegEx": "Sordoni et al\\.,? 2016", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Highway Networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Proc. ICML.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Proc. NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Pointer Networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proc. NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "ArXiv:1608.07905.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-Perspective Context Matching for Machine Comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "ArXiv:1612.04211.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Making Neural QA as Simple as Possible but not Simpler", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe."], "venue": "ArXiv:1703.04816.", "citeRegEx": "Weissenborn et al\\.,? 2017", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "Dynamic Coattention Networks For Question Answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "Proc. ICLR.", "citeRegEx": "Xiong et al\\.,? 2017", "shortCiteRegEx": "Xiong et al\\.", "year": 2017}, {"title": "Words or Characters? Fine-grained Gating for Reading Comprehension", "author": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "Proc. ICLR.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou."], "venue": "ArXiv:1610.09996.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D Zeiler."], "venue": "ArXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "author": ["Junbei Zhang", "Xiaodan Zhu", "Qian Chen", "Lirong Dai", "Hui Jiang."], "venue": "ArXiv:1703.04617.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "Recently introduced large-scale datasets like CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 15, "context": ", 2015), the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016) and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO; Nguyen et al.", "startOffset": 49, "endOffset": 80}, {"referenceID": 13, "context": ", 2016) and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO; Nguyen et al., 2016) have (a) The high-level structure of BIDAF.", "startOffset": 64, "endOffset": 95}, {"referenceID": 16, "context": "BIDAF (Seo et al., 2017) represents one of ar X iv :1 70 4.", "startOffset": 6, "endOffset": 24}, {"referenceID": 15, "context": "Unlike other datasets that such as CNN/Daily Mail whose questions are synthesized, Rajpurkar et al. (2016) uses a crowdsourcing platform to generate realistic question and answer pairs.", "startOffset": 83, "endOffset": 107}, {"referenceID": 16, "context": "In this section, we review the BIDAF model (Seo et al., 2017) and introduce our extension, the Ruminating Reader.", "startOffset": 43, "endOffset": 61}, {"referenceID": 11, "context": "It does so using a convolutional neural network with max pooling over learned character vectors (Lee et al., 2017; Kim et al., 2016).", "startOffset": 96, "endOffset": 132}, {"referenceID": 10, "context": "It does so using a convolutional neural network with max pooling over learned character vectors (Lee et al., 2017; Kim et al., 2016).", "startOffset": 96, "endOffset": 132}, {"referenceID": 19, "context": "The character embedding and the word embedding are concatenated and passed into a two-layer highway network (Srivastava et al., 2015) to obtain a d dimensional vector representation of each single word.", "startOffset": 108, "endOffset": 133}, {"referenceID": 8, "context": "Sequence Encoding Layers As in BIDAF, we use two LSTM RNNs (Hochreiter and Schmidhuber, 1997) with d-dimensional outputs to encode the context and query representations in both directions.", "startOffset": 59, "endOffset": 93}, {"referenceID": 20, "context": "This approach, while somewhat inefficient, proves to be an valuable addition to the model and allows it to better track position information, loosely following the positional encoding strategy of Sukhbaatar et al. (2015). Hence we obtain S\u0303C \u2208 R2d\u00d7C , which is fused with context encoding C via a gate:", "startOffset": 196, "endOffset": 221}, {"referenceID": 1, "context": "Much of this recent work has been based on end-to-end trained neural network models, and within that, most have used recurrent neural networks with soft attention (Bahdanau et al., 2015), which emphasizes one part of the data over the others.", "startOffset": 163, "endOffset": 186}, {"referenceID": 21, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context.", "startOffset": 240, "endOffset": 262}, {"referenceID": 18, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 18, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context. Xiong et al. (2017) propose the Dynamic Coattention Network, which uses co-dependent representations of the question and the context, and iteratively updates the start and end indices to recover from local maxima and to find the optimal answer span.", "startOffset": 241, "endOffset": 328}, {"referenceID": 18, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context. Xiong et al. (2017) propose the Dynamic Coattention Network, which uses co-dependent representations of the question and the context, and iteratively updates the start and end indices to recover from local maxima and to find the optimal answer span. Wang et al. (2016) propose the Multi-Perspective Context Matching model that matches the encoded context with query by combining various matching strategies, aggregates matching vector with bidirectional LSTM, and predict start and end positions.", "startOffset": 241, "endOffset": 577}, {"referenceID": 7, "context": "In order to merge the entity score during its multiple appearence, Kadlec et al. (2016) propose attention-sum reader who computes dot product between context and query encoding, does a softmax operation over context and sums the probability over the same entity to favor the frequent entities over rare ones.", "startOffset": 67, "endOffset": 88}, {"referenceID": 3, "context": "Chen et al. (2016) propose to use a bilinear term to calculate the attentional alignment between context and query.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "(2015) apply attention on window-based memory, by extending multi-hop end-to-end memory network (Sukhbaatar et al., 2015).", "startOffset": 96, "endOffset": 121}, {"referenceID": 18, "context": "The Iterative Alternative (IA) reader (Sordoni et al., 2016) produces query glimpse and document glimpse in each iterations and uses both glimpses to update recurrent state in each iteration.", "startOffset": 38, "endOffset": 60}, {"referenceID": 6, "context": "Among multi-hop reasoning systems: Hill et al. (2015) apply attention on window-based memory, by extending multi-hop end-to-end memory network (Sukhbaatar et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 5, "context": "Dhingra et al. (2016) extend attention-sum reader to multi-turn reasoning with an added gating mechanism.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "Dhingra et al. (2016) extend attention-sum reader to multi-turn reasoning with an added gating mechanism. The Iterative Alternative (IA) reader (Sordoni et al., 2016) produces query glimpse and document glimpse in each iterations and uses both glimpses to update recurrent state in each iteration. Shen et al. (2017) propose a multi-hop attention model that used reinforcement learning to dynamically determine when to stop digesting intermediate information and produce an answer.", "startOffset": 0, "endOffset": 317}, {"referenceID": 28, "context": "We use the AdaDelta optimizer (Zeiler, 2012) for optimization.", "startOffset": 30, "endOffset": 44}, {"referenceID": 16, "context": "Our model configuration closely follows that of Seo et al. (2017) did: In the character encoding layer, we use 100 filters of width 5.", "startOffset": 48, "endOffset": 66}, {"referenceID": 2, "context": "We selected hyperparameter values through random search (Bergstra and Bengio, 2012).", "startOffset": 56, "endOffset": 83}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al.", "startOffset": 2, "endOffset": 46}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al.", "startOffset": 2, "endOffset": 68}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al.", "startOffset": 2, "endOffset": 93}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al. (2017); f Seo et al.", "startOffset": 2, "endOffset": 116}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al. (2017); f Seo et al. (2017); g Lee et al.", "startOffset": 2, "endOffset": 137}, {"referenceID": 9, "context": "(2017); g Lee et al. (2016); h Wang et al.", "startOffset": 10, "endOffset": 28}, {"referenceID": 9, "context": "(2017); g Lee et al. (2016); h Wang et al. (2016); i Weissenborn et al.", "startOffset": 10, "endOffset": 50}, {"referenceID": 9, "context": "(2017); g Lee et al. (2016); h Wang et al. (2016); i Weissenborn et al. (2017); j Chen et al.", "startOffset": 10, "endOffset": 79}, {"referenceID": 3, "context": "(2017); j Chen et al. (2017); k Shen et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 3, "context": "(2017); j Chen et al. (2017); k Shen et al. (2017); l Zhang et al.", "startOffset": 10, "endOffset": 51}, {"referenceID": 3, "context": "(2017); j Chen et al. (2017); k Shen et al. (2017); l Zhang et al. (2017); \u2021 Unpublished", "startOffset": 10, "endOffset": 74}, {"referenceID": 29, "context": "At the time of submission, our model is tied in accuracy on the hidden test set with the bestperforming published single model (Zhang et al., 2017).", "startOffset": 127, "endOffset": 147}, {"referenceID": 29, "context": "Comparing to the jNet (Zhang et al., 2017) whose success answers occupy 69.", "startOffset": 22, "endOffset": 42}, {"referenceID": 29, "context": "Performance Breakdown Following Zhang et al. (2017), we break down Ruminating Reader\u2019s 79.", "startOffset": 32, "endOffset": 52}], "year": 2017, "abstractText": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BIDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BIDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BIDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems.", "creator": "LaTeX with hyperref package"}}}