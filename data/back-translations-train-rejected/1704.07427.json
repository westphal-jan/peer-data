{"id": "1704.07427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Recognizing Descriptive Wikipedia Categories for Historical Figures", "abstract": "Wikipedia is a useful knowledge source that benefits many applications in language processing and knowledge representation. An important feature of Wikipedia is that of categories. Wikipedia pages are assigned different categories according to their contents as human-annotated labels which can be used in information retrieval, ad hoc search improvements, entity ranking and tag recommendations. However, important pages are usually assigned too many categories, which makes it difficult to recognize the most important ones that give the best descriptions.", "histories": [["v1", "Mon, 24 Apr 2017 19:28:52 GMT  (703kb,D)", "http://arxiv.org/abs/1704.07427v1", "9 pages, 6 tables, 5 figures"]], "COMMENTS": "9 pages, 6 tables, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yanqing chen", "steven skiena"], "accepted": false, "id": "1704.07427"}, "pdf": {"name": "1704.07427.pdf", "metadata": {"source": "CRF", "title": "Recognizing Descriptive Wikipedia Categories for Historical Figures", "authors": ["Yanqing Chen", "Steven Skiena"], "emails": ["cyanqing@cs.stonybrook.edu", "skiena@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "In this paper, we propose an approach to recognizing the descriptive Wikipedia categories. We observe that historical figures in a precise category are likely to be similar to each other, and such categorical coherence could be evaluated via texts or Wikipedia links by corresponding members in the category. We rank at the descriptive level of Wikipedia categories by their coherence and our ranking, which produces a general agreement of 88.27% compared to human wisdom. INTRODUCTIONWikipedia is a useful source of knowledge that favors many applications in language processing and knowledge representation. An important feature of Wikipedia is that the individual categories are designated according to their content as human-listed labels that can be used in information retrieval. Ad-hoc search improvements, entity ranking and tag recommendations. However, important pages are generally assigned to many categories. Figure 1 shows the distribution of categories for historical figures on Wikipedia. On the other hand, most of these categories are not descriptive enough, most of us are writing this category."}, {"heading": "II. RELATED WORK", "text": "Entity ranking is gaining popularity as better rankings boost the performance of search engines, which would lead to a faster and more accurate query of information. Wikipedia seems to be a good playground. The problem of ranking websites could easily be reduced to Wikipedia Entity Ranking, plus that Wikipedia has a large collection of entities of various kinds [2] and Wikipedia contains valuable texts, man-annotated tags, enriched links, and a large structure for analyzing ranking effectiveness. Certain rankings can also serve as a linchpin for extensibility or analysis [3], [4], or can be used to answer queries in designated entity recognition [5]. In addition, retrieving real knowledge about reputations, fame, and historical significance of entity ranking will also be valuable [1]. Traditional ranking algorithms on Wikipedia consider essentially two parts. A part focuses on information provided by raw text, including the length of pre-distributed pages, and word length."}, {"heading": "III. COLLECTING HUMAN WISDOM", "text": "Our experiment focuses mainly on Wikipedia pages with historical numbers. It is clear that not all categories are created with the same descriptive power, most of them are correct, but provide only trivial information. Categories such as \"1961 Births\" and \"Living People\" record a certain status of a person's facts, but are usually meaningless; categories such as \"Harvard Law School Alumni\" provide a better idea of a person, but usually that is not enough to sum up a single aspect of one's life. Here, we define the descriptive power of a category as the ability to construct analogous connections between people within the same categories. The easier we can distinguish a person and other people with a single category, the more descriptive power we have in that category. We expect categories such as \"President of the United States\" to be listed at the top of our ranking of descriptive power, as these categories most accurately represent the historical / cultural meanings of one page."}, {"heading": "IV. CHINESE MENU OF RANKING MODEL", "text": "In this section, we will describe how to calculate categorical coherence, with four main steps: generating character vectors using latent representations from text or links; measuring distances in vector space; defining close neighbors; calculating categorical coherence based on observations of close neighbors. We have several options for each step of the experiment and expect to find the best combinations to solve this problem."}, {"heading": "A. Generating feature vectors", "text": "We focus on the conversion of text / links from Wikipedia pages to vector representations. We experimented with two methods in our tests1) LDA model: The LDA model provides a probability distribution of possible topics. It is based on the simultaneous occurrence of words and has the advantage that the output of each topic would be easy for people to read and understand. We train the LDA model for all available pages in our data collection of 5000 topics, transforming each Wikipedia page in the corpus into a probability distribution of possible topics. Figure 3 shows an example of top-related topics for some historical numbers. Pages are more likely to fall into the same topics, so should be considered more similar than high-dimensional vector spacing. 2) Deepwalk model: Deepwalk is an online algorithm that creates statistical representations of graphs by learning about random walks in the graph. Walks are considered as sentences metaphors and lateral dimensions according to the list of adjacent dimensions."}, {"heading": "B. Measuring distances", "text": "Character vectors are usually considered points in high-dimensional space. Therefore, the distance between the vectors may follow the definition of Manhattan distance (L1 normalization) or Euclidean distance (L1 normalization). Furthermore, cosinal similarities are often used to measure similarities between feature vectors. Since LDA represents probability distributions, we include Kullback-Leibler divergence and Jensen-Shannon divergence in our distance metrics."}, {"heading": "C. Definition of close neighbors", "text": "However, such representations are not linear - pairs with double distance do not necessarily mean half of the similarity. On the other hand, the observation shows that only pairs within a certain range exhibit strong similarity signals. Such ranges are not predefined and usually correlate with the density of characteristic vectors in a certain area of the embedding space. We propose two approaches to the definition of close neighbors: One is to limit the number of close neighbors, with only the closest K neighbors in the vector space being considered \"close\" neighbors. Since the relationship of close neighbors under this definition is not always reversible, this strategy usually leads to asymmetric results. The other is to select close neighbors by distance and mark all neighbors within a certain distance from D. In this strategy, if a point in space is semi-isolated, nothing would be considered similar. Both approaches are reasonable and are considered hyperparameters in our experiment."}, {"heading": "D. Ranking criteria", "text": "We propose two methods to quantify how descriptive a Wikipedia category is: 1) Behaviors: The \"conductivity\" of a category is defined as the ratio of close neighbors within the category and those outside the category. This is an easy and direct measurement. Category with closer neighbors within the category seems to be more stable, since close neighbors have similarities to \"members within that category.\" However, this method does not take into account the size of the category well. In larger categories with more corresponding persons, it is even more difficult to keep all close neighbors within the category / guarantee members of the category that are close, thereby reducing the \"conductivity\" of the category. If the cat is the conductivity of a category, then we have: Ccat = X-cat Y-cat (X, Y-cat) close closures / X-cat (X, Y-cat) close closures of the category (X, Y-cat) close closures of the category (X, Y-cat) close closures of the category (X, Y-cat) close closures of the closures of the category (X)."}, {"heading": "E. Chinese menu combination", "text": "In our experiment, we perform a grid search with the following options: \u2022 Feature vectors: (LDA, Deepwalk) \u2022 Distance function: (L1, L2, Cosine, KL (for LDA) and JS (for LDA)) \u2022 Close neighbours: (Count and Distance). For the counting strategy, we test using parameters of 5, 10, 25, 50 and 100. For the distance strategy, we sort distances in pairs and select thresholds to keep an average of 5, 10, 25, 50 and 100 close neighbours accordingly. \u2022 Measurement: (Conductance and Surprise level)"}, {"heading": "V. EVALUATION RESULTS AND ANALYSIS", "text": "We will classify all Wikipedia categories in our data collection using our Chinese menu combination model, but the final quality of our ranking will be assessed on the basis of conformity with human annotations, i.e. the ranking of 1067 categories that appear in human annotations in at least one question."}, {"heading": "A. Basic evaluation", "text": "A single answer in our collection votes for the most descriptive categories out of 5 possible choices, so that each answer actually gets 4 comparisons between the chosen answer and the remaining 4. If the chosen answer comes first in our ranking compared to the other 4 choices, then our ranking gets 1 point for creating 4 correct judges; if the chosen answer comes second, we miss a comparison and get 0.75 points etc. For each answer we get (5 - i) * 0.25 points over the final result, where i is the relative rank among 5 choices in our ranking. Rough general accuracy is calculated as: Accrough = \u2211 i Score for answeri | Answers | Where the total number of answers is 10,000 (500 questions * 20 answers per question)."}, {"heading": "B. Maximum possible accuracy?", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcehnlrc\u00fceF"}, {"heading": "G. Error analysis", "text": "We are trying to do an in-depth analysis based on the best ranking, which is done using deep walk embedding, L2 normalization, with each person having 25 close neighbors and measured at surprise level. We list all categories with more votes but less surprise level in our data collection. Table V shows the top 10 of them: As we can see, 3 out of 10 categories (Prime Ministers of the United Kingdom, Presidents of the United States, First Ladies of the United States) are political leaders whose titles are so recognizable that they have given this person enough to distinguish them from the others. However, such categories have a long history - it is quite possible that there are fewer similarities between US presidents in the 19th century and US presidents after 2000, except for the title itself and Deepwalk found no supporting evidence from Wikipedia links. \"The Beatles Members\" plays a special role, as the size of the category is too small, while the descriptive power is incredibly large."}, {"heading": "H. Sample Results", "text": "Here we have listed the 100 best Wikipedia categories that our algorithms have discovered in Table VI using the best deepwalk model with 50 close neighbours, L2 normalization and surprise level (represented in the logarithm as S level)."}, {"heading": "VI. CONCLUSION", "text": "We experimented with two models to convert links in Wikipedia to vectors and tried another definition of proximity that reflected similarities between entities. Then, we calculated the descriptive power based on categorical coherence, which reflects how well a category keeps its members close to latent representations of feature vectors. Of course, our models extend to analyzing pages in different languages and also to other categories of entities such as locations (i.e. cities and countries) and organizations (companies and universities), and we are able to identify similar people because they propose friends on social networks or even matching algorithms that bring roommates or romantic partners together. We collected human reviews that suggest descriptive power Wikipedia categories from Crowdflower. We tested our models on about 600,000 pages of historical numbers from Wikipedia."}], "references": [{"title": "Who\u2019s Bigger?: Where Historical Figures Really Rank", "author": ["S. Skiena", "C.B. Ward"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Entity ranking using wikipedia as a pivot", "author": ["R. Kaptein", "P. Serdyukov", "A. De Vries", "J. Kamps"], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management. ACM, 2010, pp. 69\u201378.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Ranking of wikipedia articles in search engines revisited: Fair ranking for reasonable quality?", "author": ["D. Lewandowski", "U. Spree"], "venue": "Journal of the American Society for Information Science and technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Entity ranking in wikipedia", "author": ["A.-M. Vercoustre", "J.A. Thom", "J. Pehcevski"], "venue": "Proceedings of the 2008 ACM symposium on Applied computing. ACM, 2008, pp. 1101\u20131106.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation for tag recommendation", "author": ["R. Krestel", "P. Fankhauser", "W. Nejdl"], "venue": "Proceedings of the third ACM conference on Recommender systems. ACM, 2009, pp. 61\u201368.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "The anatomy of a large-scale hypertextual web  search engine", "author": ["S. Brin", "L. Page"], "venue": "Computer networks and ISDN systems, vol. 30, no. 1, pp. 107\u2013117, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J.M. Kleinberg"], "venue": "Journal of the ACM (JACM), vol. 46, no. 5, pp. 604\u2013632, 1999.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201914. New York, NY, USA: ACM, 2014, pp. 701\u2013710. [Online]. Available: http://doi.acm.org/10.1145/2623330.2623732", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Using wikipedia categories and links in entity ranking", "author": ["A.-M. Vercoustre", "J. Pehcevski", "J.A. Thom"], "venue": "Focused Access to XML Documents. Springer, 2008, pp. 321\u2013335.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Entity ranking in wikipedia: utilising categories, links and topic difficulty prediction", "author": ["J. Pehcevski", "J.A. Thom", "A.-M. Vercoustre", "V. Naumovski"], "venue": "Information retrieval, vol. 13, no. 5, pp. 568\u2013600, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Using wikipedia categories for ad hoc search", "author": ["R. Kaptein", "M. Koolen", "J. Kamps"], "venue": "Proceedings of the 32nd international ACM SIGIR  conference on Research and development in information retrieval. ACM, 2009, pp. 824\u2013825.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting semantics relationships between wikipedia categories.", "author": ["S. Chernov", "T. Iofciu", "W. Nejdl", "X. Zhou"], "venue": "SemWiki, vol", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "We use \u201cranking of significance\u201d described in [1] to decide the most famous people on Wikipedia.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Additionally, retrieving real-life knowledge of reputations, fames and historical significance from entity ranking is also valuable [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "LDA is among the most valuable approaches in such tasks [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "Topics from LDA highly agree with real tags when finding most important feature words of a page [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Representatives include PageRank [8] and HITS [9].", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "Representatives include PageRank [8] and HITS [9].", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "Recent work of Deepwalk [10] uses truncated random walks to learn latent representations by encoding social relations in a continuous vector space, which can be easily exploited by statistical models.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "On Wikipedia, performance of entity ranking is improved by utilizing Wikipedia categories [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Links and topic difficulty prediction together with category information greatly boost the performance of entity ranking [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "Additionally, Wikipedia categories can be used to boost search results in an ad hoc way [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "Researchers also found that it is possible to analyze consistent semantic relationships in the tags [14] and corresponding latent representations of raw text would help tag recommendations [7].", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "Researchers also found that it is possible to analyze consistent semantic relationships in the tags [14] and corresponding latent representations of raw text would help tag recommendations [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "We start a project on Crowdflower, a leading peoplepowered data enrichment platform, to collect human reviews of the most descriptive Wikipedia categories on the top-500 most famous people according to \u201cranking of significance\u201d described in [1].", "startOffset": 241, "endOffset": 244}, {"referenceID": 8, "context": "We use the package described in [10] with 128 output dimensions to train Deepwalk embedding on adjacent lists of all Wikipedia pages.", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "Wikipedia is a useful knowledge source that benefits many applications in language processing and knowledge representation. An important feature of Wikipedia is that of categories. Wikipedia pages are assigned different categories according to their contents as human-annotated labels which can be used in information retrieval, ad hoc search improvements, entity ranking and tag recommendations. However, important pages are usually assigned too many categories, which makes it difficult to recognize the most important ones that give the best descriptions. In this paper, we propose an approach to recognize the most descriptive Wikipedia categories. We observe that historical figures in a precise category presumably are mutually similar and such categorical coherence could be evaluated via texts or Wikipedia links of corresponding members in the category. We rank descriptive level of Wikipedia categories according to their coherence and our ranking yield an overall agreement of 88.27% compared with human wisdom.", "creator": "LaTeX with hyperref package"}}}