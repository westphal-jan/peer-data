{"id": "1704.07434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Paying Attention to Descriptions Generated by Image Captioning Models", "abstract": "To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization ability is, however, observed for the saliency-boosted model on unseen data.", "histories": [["v1", "Mon, 24 Apr 2017 19:51:16 GMT  (8097kb,D)", "http://arxiv.org/abs/1704.07434v1", null], ["v2", "Wed, 28 Jun 2017 10:13:45 GMT  (1069kb,D)", "http://arxiv.org/abs/1704.07434v2", null], ["v3", "Fri, 4 Aug 2017 11:24:45 GMT  (1140kb,D)", "http://arxiv.org/abs/1704.07434v3", "To appear in ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed r tavakoli", "rakshith shetty", "ali borji", "jorma laaksonen"], "accepted": false, "id": "1704.07434"}, "pdf": {"name": "1704.07434.pdf", "metadata": {"source": "CRF", "title": "Can Saliency Information Benefit Image Captioning Models?", "authors": ["Hamed R. Tavakoli", "Rakshith Shetty", "Ali Borji", "Jorma Laaksonen"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Recent advances in machine learning, together with the increase in computing power available, have increased interest in solving high-level problems such as image processing [13, 54, 8, 12], scene and video understanding [46, 47, 36], and answering visual questions [34, 2]. The main goal of these problems is to reach a conclusion that results in a human-like response. Although trivial evaluation techniques facilitate understanding the average performance of algorithms, we still need more detailed studies to understand existing methods, such as perception and sentence planning in describing images. A machine's ability to replicate such interactions is a challenge."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to move to another world, where they will be able to move to another world, where they will be able to move to another world, where they will be able to move, where they will be able to move, where they will be able to move."}, {"heading": "3. Data", "text": "There are several famous datasets for generating images. At this time, the most popular dataset is MS COCO. It consists of over 200K images with at least 5K images per image. It consists of 1K images selected from the PASCAL VOC database."}, {"heading": "4. Analyzing Human Sentences", "text": "The descriptions are written in a way that does not exist in most other countries of the world."}, {"heading": "5. Machine vs. Human", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "6. Saliency-Boosted Captioning Model", "text": "On this point, we have confirmed that there is a degree of agreement between descriptions of humans and machines in terms of attention. We learned that better caption models have a higher attention match with humans. To this end, we have relied on fixations collected from a free viewing task. To this end, we are building a caption model with visual features based on a three-level LSTM network with residual connections. [22] We are using the open implementation of [48], where we switch both caption channels of the LSTM model to visual features and avoid any contextual features for simplicity."}, {"heading": "7. Discussion and Conclusion", "text": "Let us return to the two questions at the beginning of this paper and summarize our findings in relation to them. 1) How well do the descriptions of people or models in a scene match the impact? In summary, all captions show a clear degree of agreement with humans in capturing alienation. We testified that humans captured objects in accordance with [56]. Then we expanded the analysis to include descriptions by machines. The \"Microsoft\" model (the best of all models in this study) has the highest degree of agreement with humans in capturing metrics and attention. This indicates that the captions models become powerful enough to describe the visually existing elements in a similar way to humans. Accordingly with the evidence of importance, e.g. [56, 26] we confirmed that on average the objects move closer to the beginning of the descriptions."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding and predicting importance in images", "author": ["A.C. Berg", "T.L. Berg", "H. Daum", "J. Dodge", "A. Goyal", "X. Han", "A. Mensch", "M. Mitchell", "A. Sood", "K. Stratos", "K. Yamaguchi"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic description generation from images: A survey", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "arXiv:1601.03896,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Minding the clock", "author": ["K. Bock", "D. Irwin", "D. Davidson", "W. Levelt"], "venue": "Journal of Memory and Language, 48,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations", "author": ["A. Borji", "J. Tanner"], "venue": "IEEE Trans Neural Netw Learn Syst., PP(99):1\u201313,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of scores, datasets, and models in visual saliency prediction", "author": ["A. Borji", "H.R. Tavakoli", "D.N. Sihite", "L. Itti"], "venue": "ICCV,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "T.-Y.L. Hao Fang", "R. Vedantam", "S. Gupta", "P. Dollr", "C.L. Zitnick"], "venue": "arXiv:1504.00325,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Giving good directions: order of mention reflects visual salience", "author": ["A.D.F. Clarke", "M. Elsner", "H. Rohde"], "venue": "Frontiers in Psychology, 6(1793),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M. Denkowski", "A. Lavie"], "venue": "EACL,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Language models for image captioning: The quirks and what work", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "ACL,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing images using inferred visual dependency representations", "author": ["D. Elliott", "A.P. de Vries"], "venue": "In ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["D. Elliott", "F. Keller"], "venue": "EMNLP,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, 111(1):98\u2013136, Jan.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, pages 15\u201329,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistics of high-level scene context", "author": ["M.R. Greene"], "venue": "Frontiers in Psychology, 4(777),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "What the eyes say about speaking", "author": ["Z. Griffin", "K. Bock"], "venue": "Psychol Sci., 11(4),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Observing the what and when of language production for different age groups by monitoring speakers eye movements", "author": ["Z.M. Griffin", "D.H. Spieler"], "venue": "Brain and Language, 99(3):272 \u2013 288,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceeding of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation  metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013 899,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "How we focus attention in picture viewing, picture description, and during mental imagery, pages 291\u2013 313", "author": ["J. Holsanova"], "venue": "Bilder - sehen - denken : zum Verhltnis von begrifflichphilosophischen und empirisch-psychologischen Anstzen in der bildwissenschaftlichen Forschung. von Halem,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Extereme learning machine: Theory and applicatons", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomput., 70:489\u2013 501,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Action to Language via the Mirror Neuron System, chapter Attention and the Minimal Subscene", "author": ["L. Itti", "M.A. Arbib"], "venue": "Combridge Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to predict where humans look", "author": ["T. Judd", "K. Ehinger", "F. Durand", "A. Torralba"], "venue": "ICCV,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Referential domains in spoken language comprehension: Using eye movements to bridge the product and action traditions. In The interface of language, vision, and action: Eye movements and visual world", "author": ["M. k. Tanenhaus", "C. Chambers", "J.E. Hanna"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "CoNLL, pages 220\u2013228,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "ACL,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "PAMI,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "The use of eye tracking in studies of sentence generation", "author": ["A.S. Meyer"], "venue": "The interface of language, vision, and action: Eye movements and the visual world. Psychology Press,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "Viewing and naming objects: eye movements during noun phrase production", "author": ["A.S. Meyer", "A.M. Sleiderink", "W.J. Levelt"], "venue": "Cognition, 66(2):B25 \u2013 B33,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.G. Cho", "S.W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR, pages 891\u2013898,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. jing Zhu"], "venue": "In ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "Components of bottom-up gaze allocation in natural images", "author": ["R.J. Peters", "A. Iyer", "L. Itti", "C. Koch"], "venue": "Vision Research, 45:2397\u20132416,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Walking or talking?: Behavioral and neurophysiological correlates of action verb processing", "author": ["F. Pulverm\u00fcller", "M. Hrle", "F. Hummel"], "venue": "Brain and Language, 78(2),", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "The Long-Short Story of Movie Description", "author": ["A. Rohrbach", "M. Rohrbach", "B. Schiele"], "venue": "GCPR,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Video captioning with recurrent networks based on frame- and video-level features and visual content classification", "author": ["R. Shetty", "J. Laaksonen"], "venue": "CVPR Workshops,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploiting scene context for image captioning", "author": ["R. Shetty", "H. R-Tavakoli", "J. Laaksonen"], "venue": "ACMMM Vision and Language Integration Meets Multimedia Fusion Workshop,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv, abs/1409.1556,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Parsing english with a link grammar", "author": ["D.D. Sleator", "D. Temperley"], "venue": "Third International Workshop on Parsing Technologies,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1991}, {"title": "Cognition and Sentence Production: A Cross- Linguistic Study, chapter Models of Sentence Production, pages 7\u201319", "author": ["S.N. Sridhar"], "venue": "Springer New York,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1988}, {"title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features", "author": ["H.R. Tavakoli", "A. Borji", "J. Laaksonen", "E. Rahtu"], "venue": "Neurocomputing,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2017}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL, 2:67\u201378,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring the role of gaze behavior and object detection in scene understanding", "author": ["K. Yun", "Y. Peng", "D. Samaras", "G. Zelinsky", "T. Berg"], "venue": "Frontiers in Psychology, 4,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Studying relationships between human gaze, description, and computer vision", "author": ["K. Yun", "Y. Peng", "D. Samaras", "G.J. Zelinsky", "T.L. Berg"], "venue": "CVPR. IEEE,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "PAMI, PP(99),", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Einhauser. Fixations on objects in natural scenes: dissociating importance from salience", "author": ["B.M. t Hart", "H.C.E.F. Schmidt", "C. Roth"], "venue": "Frontiers in Psychology,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 52, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 6, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 10, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 44, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 242, "endOffset": 254}, {"referenceID": 45, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 242, "endOffset": 254}, {"referenceID": 34, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 242, "endOffset": 254}, {"referenceID": 32, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 286, "endOffset": 293}, {"referenceID": 0, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 286, "endOffset": 293}, {"referenceID": 49, "context": "He proposes that, in a free word positioning scenario, not bound by any traditional rule, the words follow each other according to the degree of emphasis on the concepts [51].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "Griffin and Bock [20] found some empirical supporting evidence by showing that while describing scenes, speakers look at an object before naming it within their description.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 35, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 19, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 22, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 54, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 16, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 39, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 21, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 28, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 29, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 83, "endOffset": 91}, {"referenceID": 13, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 83, "endOffset": 91}, {"referenceID": 52, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 140, "endOffset": 152}, {"referenceID": 15, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 140, "endOffset": 152}, {"referenceID": 12, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 140, "endOffset": 152}, {"referenceID": 2, "context": "In-depth study of these different models is beyond the scope of this article and falls within the surveys such as [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 27, "context": "\u201cNeural Talk\u201d [29] utilizes a vanilla recurrent neural architecture for the language model while using CNNs for encoding.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "\u201cMicrosoft\u201d [17] employs multiple instance learning to learn a visual detector for words in order to utilize them in an exponential language model for sentence generation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 52, "context": "The \u201cGoogle\u201d method [54] is a generative deep model based on recurrent architectures, more specifically long short-term memory (LSTM) networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 45, "context": "\u201cAalto\u201d [47] employs object detection to augment features in order to boost the results in a framework similar to \u201cNeural Talk\u201d, utilizing LSTM networks.", "startOffset": 8, "endOffset": 12}, {"referenceID": 40, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 216, "endOffset": 220}, {"referenceID": 9, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 370, "endOffset": 374}, {"referenceID": 51, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 432, "endOffset": 436}, {"referenceID": 18, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 126, "endOffset": 134}, {"referenceID": 36, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 126, "endOffset": 134}, {"referenceID": 42, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 215, "endOffset": 223}, {"referenceID": 26, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 215, "endOffset": 223}, {"referenceID": 55, "context": "It has been demonstrated that the eye gaze and object descriptions highly correlate [57].", "startOffset": 84, "endOffset": 88}, {"referenceID": 54, "context": "Further, in-depth analysis of gaze behavior in scene understanding and description reveals that people are often describing what they looked at [56], promoting the notion of importance.", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "In [3], the importance of objects is studied in terms of their descriptions where object referral indicates the object\u2019s importance.", "startOffset": 3, "endOffset": 6}, {"referenceID": 57, "context": "On the other hand, obeying natural scene statistics, the importance and saliency of an object are equal [59].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "The saliency, in the form of bottom-up attention, is reported to act as a facilitator whereby salient objects are more likely to be reported in scene descriptions [26].", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "The role of perception and attention is, however, more than the decisive role of referral and can even influence the order of mentioning objects [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "As an alternative to bounding boxes, [19] employed precise hand-labelled object masks to investigate the relation between objects and the scene context.", "startOffset": 37, "endOffset": 41}, {"referenceID": 56, "context": "[58] proposed using abstract images in conjunction with written descriptions for semantic scene analysis, which is impossible for natural images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In this paper, we follow a procedure similar to [19] and rely on precise handlabelled object masks for natural images.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 54, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 55, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 24, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 31, "context": "At the time, the most popular dataset is the MS COCO [33].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Among large datasets, there exists Flicker8K [23] and its extention Flicker30K [55].", "startOffset": 45, "endOffset": 49}, {"referenceID": 53, "context": "Among large datasets, there exists Flicker8K [23] and its extention Flicker30K [55].", "startOffset": 79, "endOffset": 83}, {"referenceID": 43, "context": "One of the earliest wellrecognized datasets is UIUC PASCAL sentences [45].", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "It consists of 1K images selected from the PASCAL-VOC dataset [16] and 5 human-written sentences for each.", "startOffset": 62, "endOffset": 66}, {"referenceID": 51, "context": "The same image set is used in PASCAL-50S [53], where 50 human-written sentences are provided.", "startOffset": 41, "endOffset": 45}, {"referenceID": 38, "context": "The use of the PASCAL-VOC images gives PASCAL-50S the advantage of having rich contextual annotation information [40].", "startOffset": 113, "endOffset": 117}, {"referenceID": 55, "context": "Furthermore, the same image set is used by [57] for gazebased analysis of objects and descriptions, where the gaze is recorded during free-viewing separate from image descriptions.", "startOffset": 43, "endOffset": 47}, {"referenceID": 51, "context": "[53, 57, 40], results in 1K images with 50 sentences, 222 precisely labelled ob-", "startOffset": 0, "endOffset": 12}, {"referenceID": 55, "context": "[53, 57, 40], results in 1K images with 50 sentences, 222 precisely labelled ob-", "startOffset": 0, "endOffset": 12}, {"referenceID": 38, "context": "[53, 57, 40], results in 1K images with 50 sentences, 222 precisely labelled ob-", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 45, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 52, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 15, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "All the models were trained on the MS COCO data set [8] and generated descriptions for the image set of augmented PASCAL-50S.", "startOffset": 52, "endOffset": 55}, {"referenceID": 33, "context": "To obtain such a mapping, we first identified all the nouns in the sentences by running a part of speech (POS) tagging software [35].", "startOffset": 128, "endOffset": 132}, {"referenceID": 37, "context": "18) nouns to each object class label using word2vec [39].", "startOffset": 52, "endOffset": 56}, {"referenceID": 48, "context": "The grammar checking was performed using a link grammar checking syntactic parser [50].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "Among the grammatically correct sentences, using [35], we identified that only 19126 sentences (64%) have a verb, of which 17362 (90%) are active and 1764 (10%) are passive.", "startOffset": 49, "endOffset": 53}, {"referenceID": 55, "context": "3 summarizes these statistics, signifying that some objects are often more attended in agreement with the findings of [57], providing us some idea about the importance and saliency of objects.", "startOffset": 118, "endOffset": 122}, {"referenceID": 54, "context": "We follow the steps of [56] and extend to machine-generated descriptions to compare descriptions by human and machine.", "startOffset": 23, "endOffset": 27}, {"referenceID": 54, "context": "87 in [56], it is worth noting that, in this study, the object categories are obtained from contextual annotation, consisting of 222 classes compared to 20 in [56], and do not discriminate the background from the foreground objects.", "startOffset": 6, "endOffset": 10}, {"referenceID": 54, "context": "87 in [56], it is worth noting that, in this study, the object categories are obtained from contextual annotation, consisting of 222 classes compared to 20 in [56], and do not discriminate the background from the foreground objects.", "startOffset": 159, "endOffset": 163}, {"referenceID": 4, "context": "We put more weight to the centers of objects, as the center of objects are shown to allocate more fixations [6], and slightly smooth the maps.", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "Having a saliency map and fixation information, we employ the trivial fixation prediction evaluation criteria [7] for assessing a sentence in terms of attention.", "startOffset": 110, "endOffset": 113}, {"referenceID": 25, "context": "We utilized area under the curve (AUC) [27], correlation coefficient (CC), and normalized scanpath saliency (NSS) [43].", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "We utilized area under the curve (AUC) [27], correlation coefficient (CC), and normalized scanpath saliency (NSS) [43].", "startOffset": 114, "endOffset": 118}, {"referenceID": 6, "context": "We thus evaluate the generated descriptions using Microsoft COCO caption evaluation code [8] and compared it with the AUC score of models.", "startOffset": 89, "endOffset": 92}, {"referenceID": 20, "context": "In other words, we focus on answering: Can saliency benefit image captioning by machine? For this purpose, we employ a standard captioning model, based on an LSTM network of three layers with residual connections [22] between the layers.", "startOffset": 213, "endOffset": 217}, {"referenceID": 46, "context": "We use the open implementation of [48], where we set both feature input channels of the LSTM model to visual features and avoid any contextual features for simplicity.", "startOffset": 34, "endOffset": 38}, {"referenceID": 46, "context": "We refer the readers to [48] for the details of the language model.", "startOffset": 24, "endOffset": 28}, {"referenceID": 47, "context": "We extract the image features using CNN features of the VGG network [49].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "We follow the filter bank approach of [9] and compute the responses over the input image.", "startOffset": 38, "endOffset": 41}, {"referenceID": 23, "context": "Afterwards, we learn a regression to approximate the human fixations using extreme learning machines [25], following the saliency model of [52].", "startOffset": 101, "endOffset": 105}, {"referenceID": 50, "context": "Afterwards, we learn a regression to approximate the human fixations using extreme learning machines [25], following the saliency model of [52].", "startOffset": 139, "endOffset": 143}, {"referenceID": 31, "context": "We evaluate the saliency boosted model on MS COCO [33] and augmented PASCAL50S, where the model is trained on MS COCO.", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 112, "endOffset": 116}, {"referenceID": 45, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "283 Microsoft [17] \u2013 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 45, "context": "257 Aalto [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 52, "context": "299 Google [54] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "277 Neural Talk [29] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 54, "context": "We testified that humans describe fixated items rather than looking aimlessly, consistent with [56].", "startOffset": 95, "endOffset": 99}, {"referenceID": 54, "context": "[56, 26], we confirmed that more salient objects appear on average closer to the beginning of the descriptions by human.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[56, 26], we confirmed that more salient objects appear on average closer to the beginning of the descriptions by human.", "startOffset": 0, "endOffset": 8}], "year": 2017, "abstractText": "To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliencyboosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization ability is, however, observed for the saliency-boosted model on unseen data.", "creator": "LaTeX with hyperref package"}}}