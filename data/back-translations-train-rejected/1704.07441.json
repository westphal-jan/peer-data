{"id": "1704.07441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Detecting English Writing Styles For Non Native Speakers", "abstract": "This paper presents the first attempt, up to our knowledge, to classify English writing styles on this scale with the challenge of classifying day to day language written by writers with different backgrounds covering various areas of topics.The paper proposes simple machine learning algorithms and simple to generate features to solve hard problems. Relying on the scale of the data available from large sources of knowledge like Wikipedia. We believe such sources of data are crucial to generate robust solutions for the web with high accuracy and easy to deploy in practice. The paper achieves 74\\% accuracy classifying native versus non native speakers writing styles.", "histories": [["v1", "Mon, 24 Apr 2017 20:04:25 GMT  (446kb,D)", "http://arxiv.org/abs/1704.07441v1", "9 figures, 5 tables, 9 pages"]], "COMMENTS": "9 figures, 5 tables, 9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yanging chen", "rami al-rfou'", "yejin choi"], "accepted": false, "id": "1704.07441"}, "pdf": {"name": "1704.07441.pdf", "metadata": {"source": "CRF", "title": "Detecting English Writing Styles For Non Native Speakers", "authors": ["Yanging Chen", "Rami Al-Rfou", "Yejin Choi"], "emails": ["ychoi}@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "In addition, the work shows some interesting observations about the similarity between different languages measured by the similarity of their users \"English writing style. This technique could be used to show some well-known facts about languages, such as when grouping into families that support our experiments."}, {"heading": "1 Introduction", "text": "The Internet is now more diverse than ever, with the advent of social networking, the majority of users are no longer native English speakers, which poses greater challenges for service providers in adapting English content to new users. This paper addresses the challenge of identifying the user's native language by his writing style. We believe that this task will be crucial as a first step in the development of many useful applications. Wikipedia is a well-known source of knowledge. Lately, it has been widely used to help solve various information retrieval tasks, especially those where semantic aspects play a role. Use of Wikipedia can be expanded to help the common NLP tools perform better with the variety of topics and authors of Wikipedia pages. This helps with the problem of data economy. Sustainable growth of Wikipedia content can lead to performance improvements without incurring much additional costs.Recognition of the writer's native language can be helpful in the application that will adapt to the new English language as a second language if it is not adapted to the language of translation."}, {"heading": "2 Related Work", "text": "The first work related to the mother tongue is that of (Koppel et al., 2005a), in which they discussed the profiling of anonymous authors with their home countries; the second group in which they are located is that of the second group in which they are located; the third group in which they are located is that of the first group to which they belong; and the third group in which they are located is that of the second group to which they belong."}, {"heading": "3 Wikipedia", "text": "Wikipedia is the de facto source of knowledge for Internet users. Wikipedia is the fifth most popular website according to Google's ranking, but for researchers, Wikipedia is a vast field of linguistic and social experimentation. The wealth of website content written by users from different backgrounds is a robust example of the current language usage of native speakers and non-native speakers. With more than 90,000 active users and 4.4 million articles, Wikipedia's content covers a wide range of topics. The diversity of authors of these articles, in addition to the records of revisions stored in a database of revisions that the website offers free of charge, represents a realistic source of text. Such a resource represents a higher data quality that is unattainable from other commonly used text sources such as news and scientific papers. Such a successful website has a complex database structure to serve its users. Therefore, extracting data could be a complex process. Our goal is to collect contributions to identify the language users and help them understand their language."}, {"heading": "4 Experiments", "text": "Figure 3 shows that the percentage of users who claimed that their native language was English accounts for about 47% of English Wikipedia users. We analyzed the conversation pages with namespace = 1, representing x% of the conversation pages that produced about 12 million comments. We were able to assign only 2.4 million comments to users with known language skills. In addition, not all users made comments on the conversation pages that we analyzed, so the number of users we have recently made an effort to generate the differences http: / / dumps.wikimedia.org / other / diffdb / is at least one contribution in the extracted contributions. Since we have a large number of comments and users, and because we believe that the data we have is still noisy, we applied the following filtering mechanisms: \u2022 We selected the users of the most popular 19 native languages. \u2022 We selected the users who specified the EN-US as their native tongue, but did not specify the 228 as their native language in order to avoid having the two native languages spoken."}, {"heading": "4.1 Setup", "text": "The following experiments are carried out under the following conditions: \u2022 The accepted comments must have at least 20 characters to avoid short and insignificant comments. \u2022 Correct nouns are replaced by their tags to avoid bias on topics. \u2022 Non-ascii characters are replaced by a special character to avoid the use of foreign language bias in comments. \u2022 The classifier has a balanced number of comments for each of its classes. Therefore, the two basic classifiers; the most common name and the random classifier have an accuracy of 1 / number of classes. \u2022 The logistic regression algorithm is used for the classification task. \u2022 The data set is divided into 70% training set, 10% development set and 20% testset.Figure 3: Distribution of users to native languages in English Wikipedia."}, {"heading": "4.2 Features", "text": "The comments of the training are grouped by classes and the following frequency distribution is calculated for each class: \u2022 1-4 grams above the comment words. \u2022 1-4 grams above the letter of the comments. \u2022 1-4 grams over the part of the language days.For each comment (C), similarity measurements (Sim) are calculated on the basis of each n-gram frequency distribution (f (n) according to the following equations: Count (x, f, n) = FreqDistCount (x, f, n), if x is in f (n) 1, if x is not seen before (C, f, n) = \u2211 x-Ngrams (C, n) log2 (Count (x, f, n)). Thus, if our problem has six classes, this results in 6 x-3-4 = 72 results. Other characteristics include the relative frequency of each of the stops to the size of the comments. The 125 stops are extracted from the NLTK stop corpus."}, {"heading": "4.3 Popular Languages Experiment", "text": "The most popular six languages: US-EN, German, Spanish, French, Russian and Dutch are selected to train a classifier to recognize the user's native language. Figure 4 shows the confusion matrix of the experiment, which is carried out using 100% of the data set, about 150K comment. We can clearly see that Russian users are the easiest to identify. In addition, the classifier is the most confusing, which will affect German and Dutch users with errors > 2.0% and to a lesser extent between (EN-US, French) and (ENUS, Spanish). These figures confirm a basic intuition that users with geographical proximity will have more borrowed words and grammars between their native languages, which will affect their writing style in English. Figure 5 shows that the best accuracy that the classifier has achieved is 50.275%.The learning curves show a typical overpass situation, which can be achieved more in the data available in the classifier, and the size can be used here."}, {"heading": "4.4 Languages Families Experiment", "text": "The confusion in the classification of Dutch and German users suggests that there is a similarity between groups of languages. Referring to the linguistic research history of classification of languages in families according to similar characteristics and evolutionary history, this experiment attempts to examine such a grouping. 17 languages are grouped into 5 families as follows: \u2022 North Germanic German, Dutch, Norwegian, Swedish, Danish \u2022 Roman Spanish, French, Portuguese, Italian \u2022 Urali Russian, Polish, Finnish, Hungarian \u2022 Asian Chinese, Japanese, Korean \u2022 English Figure 6 shows that Asian native speakers have a clear writing style that is relatively easy to recognize. Furthermore, since English belongs to the Anglican language family, which also belongs to the West Germanic language family, we can clearly see in terms of the high error rates > 3.0%. Other trends can be explained according to users who have geographical proximity as in cases (Urdu, Roman)."}, {"heading": "4.5 Native vs Non Native Experiment", "text": "In this experiment, all native English speakers were designated as non-native speakers. Figure 8 and Figure 9 show that the classifier achieved an accuracy of 74.449% by using approximately 322K comments, divided between training, development and test kits. Such high accuracy makes the classifier suitable for practical use."}, {"heading": "5 Writing Styles", "text": "Another type of experiment focuses on the use of PoS ngrams that could never appear in the category we do not define in the category \"Programs\" by comparing the similarity of the PoS ngrams distribution with a candidate language. We use the same definition of \"similarity\" as mentioned in the previous section and (Alpaydin, 2004) and build a 20-classifier based on the total training data (524 MB). After training our language model, the accuracy has about 95% on training data. And in BaselineRandom we have two baselines that we have selected are: Baseline-Max and Baseline-Random Candidate. Where in Baseline-Max we simply put each user in the category of native US speakers, as this class has the greatest accuracy with the probability of about 34% on training data. And in BaselineRandom we randomly select from 20 candidate Figures 8: Native vs Non Native Speakers we learn Confusion of native speakers we should be learning Non Curriculum vs Native Expectant 5: The non-native we should be learning curriculum natives vs 5000."}, {"heading": "6 Conclusions", "text": "Our experiments show that syntactic structures and spellings seem to be different for people from different backgrounds. Even if we only look at these characteristics, we can make a judgment about our native languages, and it will be helpful if we apply them along with other semantic characteristics."}, {"heading": "7 Future Work", "text": "Since our results show promising applications and trends that use Wikipedia data to solve difficult problems with robust means, we are looking for the impact of adding Wikipedia differences, especially the not insignificant ones, as another source of user contributions. In addition, the minimal size of comments affects the performance of our classifiers, the relationship between the quality of the data used and the accuracy of the classification is another interesting aspect. Experiments with language families suggest that it makes sense to use English writing styles to define similarities between different languages, which could lead to interesting explanations and / or observations of the origin of some languages as Korean languages, which remain a mystery. Another direction is to solve the overmatch problem in our learning algorithms by using smarter feature selection and adding more distinct features."}, {"heading": "Acknowledgements", "text": "We would like to thank Steven Skiena for the discussion and advice, which will not be available without the computer resources provided by his lab. We are also indebted to the NLTK and Sklearn team for producing excellent NLP and machine learning resources."}], "references": [{"title": "Introduction to machine learning", "author": ["Ethem Alpaydin"], "venue": null, "citeRegEx": "Alpaydin.,? \\Q2004\\E", "shortCiteRegEx": "Alpaydin.", "year": 2004}, {"title": "Automatically profiling the author of an anonymous text", "author": ["Moshe Koppel", "James W. Pennebaker", "Jonathan Schler"], "venue": "Communications of the ACM,", "citeRegEx": "Argamon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argamon et al\\.", "year": 2009}, {"title": "Author profiling for english emails", "author": ["Tanja Gaustad", "Son-Bao Pham", "Will Radford", "Ben Hutchinson"], "venue": "In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,", "citeRegEx": "Estival et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Estival et al\\.", "year": 2007}, {"title": "Linguistic correlates of style: authorship classification with deep linguistic analysis features", "author": ["Michael Gamon"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Gamon.,? \\Q2004\\E", "shortCiteRegEx": "Gamon.", "year": 2004}, {"title": "Automatically determining an anonymous authors native language", "author": ["Koppel et al.2005a] Moshe Koppel", "Jonathan Schler", "Kfir Zigdon"], "venue": "Intelligence and Security Informatics,", "citeRegEx": "Koppel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koppel et al\\.", "year": 2005}, {"title": "Determining an author\u2019s native language by mining a text for errors", "author": ["Koppel et al.2005b] Moshe Koppel", "Jonathan Schler", "Kfir Zigdon"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "Koppel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koppel et al\\.", "year": 2005}, {"title": "Using classifier features for studying the effect of native language on the choice of written second language words", "author": ["Tsur", "Rappoport2007] Oren Tsur", "Ari Rappoport"], "venue": "In Proceedings of the Workshop on Cognitive Aspects of Computational Language Ac-", "citeRegEx": "Tsur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tsur et al\\.", "year": 2007}, {"title": "Contrastive analysis and native language identification", "author": ["Wong", "Dras2009] Sze-Meng Jojo Wong", "Mark Dras"], "venue": "In Australasian Language Technology Association Workshop", "citeRegEx": "Wong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2009}, {"title": "Parser features for sentence grammaticality classification", "author": ["Wong", "Dras2010] Sze-Mong Jojo Wong", "Mark Dras"], "venue": "In Proc. Australasian Language Technology Association Workshop", "citeRegEx": "Wong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2010}, {"title": "Exploiting parse structures for native language identification", "author": ["Wong", "Dras2011] Sze-Meng Jojo Wong", "Mark Dras"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Wong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2011}, {"title": "Authorship analysis in cybercrime investigation", "author": ["Zheng et al.2003] Rong Zheng", "Yi Qin", "Zan Huang", "Hsinchun Chen"], "venue": "Intelligence and Security Informatics,", "citeRegEx": "Zheng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "(Argamon et al., 2009) concluded some more important features in the task of profiling authors of an anonymous text.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "(Estival et al., 2007) studied a wide range of lexical and document structure features in their native languages classification task.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "And (Zheng et al., 2003), though they did not directly conduct related experiments on nationality detection, they provide some features of style markes that could be used in the task of judging one\u2019s native languages.", "startOffset": 4, "endOffset": 24}, {"referenceID": 3, "context": "Besides, (Gamon, 2004) analysized the power of some general features under different frequency cutoffs.", "startOffset": 9, "endOffset": 22}, {"referenceID": 0, "context": "We use the same definition of \u201dsimilarity\u201d as mentioned in the previous section and (Alpaydin, 2004) and build a 20-classifier based on the whole training data (524 MB).", "startOffset": 84, "endOffset": 100}], "year": 2017, "abstractText": "This paper presents the first attempt, up to our knowledge, to classify English writing styles on this scale with the challenge of classifying day to day language written by writers with different backgrounds covering various areas of topics.The paper proposes simple machine learning algorithms and simple to generate features to solve hard problems. Relying on the scale of the data available from large sources of knowledge like Wikipedia. We believe such sources of data are crucial to generate robust solutions for the web with high accuracy and easy to deploy in practice. The paper achieves 74% accuracy classifying native versus non native speakers writing styles. Moreover, the paper shows some interesting observations on the similarity between different languages measured by the similarity of their users English writing styles. This technique could be used to show some well known facts about languages as in grouping them into families, which our experiments support.", "creator": "LaTeX with hyperref package"}}}