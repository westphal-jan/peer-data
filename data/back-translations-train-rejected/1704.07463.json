{"id": "1704.07463", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Streaming Word Embeddings with the Space-Saving Algorithm", "abstract": "We develop a streaming (one-pass, bounded-memory) word embedding algorithm based on the canonical skip-gram with negative sampling algorithm implemented in word2vec. We compare our streaming algorithm to word2vec empirically by measuring the cosine similarity between word pairs under each algorithm and by applying each algorithm in the downstream task of hashtag prediction on a two-month interval of the Twitter sample stream. We then discuss the results of these experiments, concluding they provide partial validation of our approach as a streaming replacement for word2vec. Finally, we discuss potential failure modes and suggest directions for future work.", "histories": [["v1", "Mon, 24 Apr 2017 20:55:33 GMT  (3077kb,D)", "http://arxiv.org/abs/1704.07463v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chandler may", "kevin duh", "benjamin van durme", "ashwin lall"], "accepted": false, "id": "1704.07463"}, "pdf": {"name": "1704.07463.pdf", "metadata": {"source": "CRF", "title": "Streaming Word Embeddings with the Space-Saving Algorithm", "authors": ["Chandler May", "Kevin Duh", "Benjamin Van Durme", "Ashwin Lall"], "emails": ["cjmay@jhu.edu", "kevinduh@cs.jhu.edu", "vandurme@cs.jhu.edu", "lalla@denison.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before we are once again able to find a solution with which we can identify."}, {"heading": "1.1 Incremental SGNS", "text": "The method presented in this paper is a modified reservoir that leads to an estimate. \"We,\" the communication states, \"have a similar concept to the recently proposed incremental SGNS (Kaji and Kobayashi, 2017), developed independently of each other.\" There are three major differences. \"\" We, \"it says in the description of the space-saving algorithms that surpass a modern implementation of the Misra Gries algorithms (Demaine et al., 2002) and many other streaming algorithms in both error and runtime ranges (Cormode and Hadjieleftheriou, 2008).\" Second, \"whereas the space rescue word2vec uses standard reservoir sampling to estimate an unfavorable distribution.\""}, {"heading": "2.2 Space-Saving Algorithm", "text": "The space-saving algorithm is a one-pass algorithm that estimates the most common elements in a stream in limited memory (Metwally et al., 2005). It does this by maintaining an array of K elements (for a given number K) and a corresponding array of K elements; if K-unique elements are seen and a new element is found, the element is replaced with the smallest number while the corresponding number of elements is incremented. Thus, the space-saving algorithm overestimates the number of elements, but enjoys a limit of n / K with respect to the error in each count, where n is the total number of elements (including repetitions) seen so far. Consider a stream of elements (xi) i. To initialize the space-saving algorithm, an array of size K samples (z1,... zK) is initialized with empty values, and a corresponding array of the respective count (conding, zonding, zonding) is not complete (the array)."}, {"heading": "2.3 Reservoir Sampling", "text": "To initialize the sampler, an array of K elements (r1,..., rK) is initialized with empty values and a counter n is initialized to zero. Then, when scatter element xi is seen, n is incremented by one and one of two actions. 1. If i \u2264 K, set ri to xi.2. Otherwise, draw an integer k uniformly from {1, 2,..., n} and if k is smaller or equal to K, replace rk with xi.At any time, the reservoir contains a uniform sample of the items seen so far (Vitter, 1985)."}, {"heading": "3 spacesaving-word2vec", "text": "To learn word embedding in one pass, we use the space-saving algorithm to create and update a vocabulary of K common words, and we use a hodgepodge of data to maintain a negative sampling distribution across that vocabulary. Specifically, we start the learning process by randomly indexing all word embedding and initializing the space-saving data structures and negative sampling data structures as empty. For each sampling data structure (si), vocabulary size K, negative sampling size N. Result: Each word word is indexed against space-saving data structure. we initialize empty data structures and Size N negative sampling data structures; initialize input, output word embedding; for one sentence s = (w1, wJ)."}, {"heading": "4 Intrinsic Evaluation", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "5 Extrinsic Evaluation", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they are able to move, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they, in which they live, in which they live, in which they live in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live."}, {"heading": "6 Discussion", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "7 Conclusion", "text": "Our approach, called Spacesaving-word2vec after the word2vec implementation of SGNS on which it is based, uses the space-saving algorithm and sampling method to maintain an approximate dynamic vocabulary and negative sampling distribution. Although preliminary experiments provide some evidence of the reliability of Spacesaving-word2vec to word2vec, there are still many unanswered questions to be answered. Although we cannot yet fully endorse the use of Spacesaving-word2vec in real world applications, we hope that thoughtful future research based on insights from Spacesaving-word2vec and gradual SGNS (Kaji and Kobayashi, 2017) will fill this gap."}, {"heading": "A Appendix", "text": "The complete space storage word2vec learning algorithm is in Algorithm 2.Data: Stream of Sentence (si) i, Subsampling Threshold \u03b4, Vocabulary Size K, Negative Sampling Memory Size N, Negative Sampling Size S, Embedding Dimension D, Context Radius C, Offset Rate Values, Exponent Rate Values. Result: Input, Output Word Embedding (vk), Indexed against Space Saving Data Structure N (0, 1) D, v Value K, Space Saving Data Structure and Size N Negative Sampling Reservoir; Input, Output Word Embedding (vk Value N (0, 1) D, v Value N (0, 1) D) for all k Value (K); Initialize tk Value 1 for all k Value."}], "references": [{"title": "Factored neural language models", "author": ["Andrei Alexandrescu", "Katrin Kirchhoff."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Alexandrescu and Kirchhoff.,? 2006", "shortCiteRegEx": "Alexandrescu and Kirchhoff.", "year": 2006}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "Proceedings of the 31st International Conference on Machine Learning.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u2013 2537, Aug.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Methods for finding frequent items in data streams", "author": ["Graham Cormode", "Marios Hadjieleftheriou."], "venue": "Proceedings of the 34th International Conference on Very Large Data Bases (VLDB), pages 1530\u20131541.", "citeRegEx": "Cormode and Hadjieleftheriou.,? 2008", "shortCiteRegEx": "Cormode and Hadjieleftheriou.", "year": 2008}, {"title": "Frequency estimation of internet packet streams with limited space", "author": ["Erik D. Demaine", "Alejandro L\u00f3pez-Ortiz", "J. Ian Munro."], "venue": "Proceedings of the European Symposium on Algorithms (ESA), pages 348\u2013 360.", "citeRegEx": "Demaine et al\\.,? 2002", "shortCiteRegEx": "Demaine et al\\.", "year": 2002}, {"title": "Automatic hashtag recommendation for microblogs using topic-specific translation model", "author": ["Zhuoye Ding", "Qi Zhang", "Xuanjing Huang."], "venue": "Proceedings of the 24th International Conference on Computational Linguistics (COLING).", "citeRegEx": "Ding et al\\.,? 2012", "shortCiteRegEx": "Ding et al\\.", "year": 2012}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero Dos Santos", "Bianca Zadrozny."], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Using topic models for Twitter hashtag recommendation", "author": ["Fr\u00e9deric Godin", "Viktor Slavkovikj", "Wesley De Neve", "Benjamin Schrauwen", "Rik Van de Walle."], "venue": "Proceedings of the 22nd International Conference on World Wide Web, pages 593\u2013596.", "citeRegEx": "Godin et al\\.,? 2013", "shortCiteRegEx": "Godin et al\\.", "year": 2013}, {"title": "Distributional structure", "author": ["Zellig Harris."], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Incremental skip-gram model with negative sampling", "author": ["Nobuhiro Kaji", "Hayato Kobayashi."], "venue": "Preprint, arXiv:1704.03956, April.", "citeRegEx": "Kaji and Kobayashi.,? 2017", "shortCiteRegEx": "Kaji and Kobayashi.", "year": 2017}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Improved neural network-based multi-label classification with better initialization leveraging label cooccurrence", "author": ["Gakuto Kurata", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "citeRegEx": "Kurata et al\\.,? 2016", "shortCiteRegEx": "Kurata et al\\.", "year": 2016}, {"title": "Compositional-ly derived representations of morphologically complex words in distributional semantics", "author": ["Angeliki Lazaridou", "Marco Marelli", "Roberto Zamparelli", "Marco Baroni."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning, pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient computation of frequent and topk elements in data streams", "author": ["Ahmed Metwally", "Divyakant Agrawal", "Amr El Abbadi."], "venue": "Proceedings of the 10th International Conference on Database Theory, pages 398\u2013412.", "citeRegEx": "Metwally et al\\.,? 2005", "shortCiteRegEx": "Metwally et al\\.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Finding repeated elements", "author": ["Jayadev Misra", "David Gries."], "venue": "Science of Computer Programming, 2(2):143\u2013152.", "citeRegEx": "Misra and Gries.,? 1982", "shortCiteRegEx": "Misra and Gries.", "year": 1982}, {"title": "Largescale multi-label text classification \u2014 revisiting neural networks", "author": ["Jinseok Nam", "Jungi Kim", "Eneldo Loza Menc\u0131\u0301a", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "venue": null, "citeRegEx": "Nam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2014}, {"title": "The distributional hypothesis", "author": ["Magnus Sahlgren."], "venue": "Rivista di Linguistica, 20(1):33\u201353.", "citeRegEx": "Sahlgren.,? 2008", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465. Asso-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1627\u2013", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384\u2013394, Uppsala,", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Random sampling with a reservoir", "author": ["Jeffrey S Vitter."], "venue": "ACM Transactions on Mathematical Software (TOMS), 11(1):37\u201357.", "citeRegEx": "Vitter.,? 1985", "shortCiteRegEx": "Vitter.", "year": 1985}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Mengqiu Wang", "Christopher D Manning."], "venue": "Proceedings of the 4th International Joint Conference on Natural Language Processing, pages 1285\u2013 1291.", "citeRegEx": "Wang and Manning.,? 2013", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "TagSpace: Semantic embeddings from hashtags", "author": ["Jason Weston", "Sumit Chopra", "Keith Adams."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822\u20131827, Doha, Qatar, October. Association", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "Preprint, arXiv:1212.5701, December.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "Word embedding algorithms such as the skip-gram with negative sampling (SGNS) (Mikolov et al., 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 23, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 25, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 21, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 2, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 15, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 22, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 0, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 1, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 13, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 16, "context": "In the present study, we augment word2vec to handle a potentially unbounded vocabulary in bounded memory using the space-saving algorithm (Metwally et al., 2005), updating an un-", "startOffset": 138, "endOffset": 161}, {"referenceID": 24, "context": "smoothed negative sampling distribution online using reservoir sampling (Vitter, 1985).", "startOffset": 72, "endOffset": 86}, {"referenceID": 10, "context": "recently proposed incremental SGNS (Kaji and Kobayashi, 2017), developed independently.", "startOffset": 35, "endOffset": 61}, {"referenceID": 18, "context": "First, whereas spacesaving-word2vec uses the space-saving algorithm to maintain an approximate vocabulary, incremental SGNS uses the MisraGries algorithm (Misra and Gries, 1982).", "startOffset": 154, "endOffset": 177}, {"referenceID": 4, "context": "algorithm (Demaine et al., 2002) and many other streaming frequent-item algorithms in both error and runtime (Cormode and Hadjieleftheriou, 2008).", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": ", 2002) and many other streaming frequent-item algorithms in both error and runtime (Cormode and Hadjieleftheriou, 2008).", "startOffset": 84, "endOffset": 120}, {"referenceID": 10, "context": "Second, whereas spacesaving-word2vec uses standard reservoir sampling to estimate an unsmoothed negative sampling distribution, Kaji and Kobayashi (2017) develop a modified reservoir sampling algorithm to estimate a smoothed negative", "startOffset": 128, "endOffset": 154}, {"referenceID": 14, "context": "In prior work, smoothing the negative sampling distribution was shown to increase word embedding quality consistently (Levy et al., 2015).", "startOffset": 118, "endOffset": 137}, {"referenceID": 7, "context": "Third, whereas spacesaving-word2vec employs a separate, thresholded, linearly decaying learning rate for each embedding, resetting the learning rate for an embedding each time the corresponding word in the space-saving data structure is replaced, incremental SGNS employs AdaGrad to adaptively set per-word learning rates (Duchi et al., 2011).", "startOffset": 322, "endOffset": 342}, {"referenceID": 27, "context": "The vanishing learning rates estimated by AdaGrad (Zeiler, 2012) may be inappropriate in the streaming setting if the data is not i.", "startOffset": 50, "endOffset": 64}, {"referenceID": 7, "context": "Third, whereas spacesaving-word2vec employs a separate, thresholded, linearly decaying learning rate for each embedding, resetting the learning rate for an embedding each time the corresponding word in the space-saving data structure is replaced, incremental SGNS employs AdaGrad to adaptively set per-word learning rates (Duchi et al., 2011). The vanishing learning rates estimated by AdaGrad (Zeiler, 2012) may be inappropriate in the streaming setting if the data is not i.i.d. and we desire a model that gives similar weight to data from the beginning, middle, and end of the stream. In light of the previous discussion, we cast our contribution as a complementary implementation and analysis to that of Kaji and Kobayashi (2017). We also release an open-source C++ implementation", "startOffset": 323, "endOffset": 734}, {"referenceID": 9, "context": "The intuition behind word2vec is to build on the distributional hypothesis (Harris, 1954; Sahlgren, 2008) and map words used in similar contexts to nearby vectors in Euclidean space.", "startOffset": 75, "endOffset": 105}, {"referenceID": 20, "context": "The intuition behind word2vec is to build on the distributional hypothesis (Harris, 1954; Sahlgren, 2008) and map words used in similar contexts to nearby vectors in Euclidean space.", "startOffset": 75, "endOffset": 105}, {"referenceID": 16, "context": "that estimates the most frequent items in a stream in bounded memory (Metwally et al., 2005).", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": "At any time i in the stream the space-saving data structure contains all items with true count greater than i/K seen so far and over-estimates all counts by at most i/K (Metwally et al., 2005).", "startOffset": 169, "endOffset": 192}, {"referenceID": 24, "context": "Reservoir sampling is a one-pass algorithm that computes a uniform subsample of a stream in bounded memory (Vitter, 1985).", "startOffset": 107, "endOffset": 121}, {"referenceID": 24, "context": "At any time i in the stream the reservoir contains a uniform sample of the items seen so far (Vitter, 1985).", "startOffset": 93, "endOffset": 107}, {"referenceID": 5, "context": "In particular, we apply the learned embeddings in the downstream task of hashtag prediction (Ding et al., 2012; Godin et al., 2013; Weston et al., 2014).", "startOffset": 92, "endOffset": 152}, {"referenceID": 8, "context": "In particular, we apply the learned embeddings in the downstream task of hashtag prediction (Ding et al., 2012; Godin et al., 2013; Weston et al., 2014).", "startOffset": 92, "endOffset": 152}, {"referenceID": 26, "context": "In particular, we apply the learned embeddings in the downstream task of hashtag prediction (Ding et al., 2012; Godin et al., 2013; Weston et al., 2014).", "startOffset": 92, "endOffset": 152}, {"referenceID": 14, "context": "and re-train\u2014we used the same hyperparameters for both methods, choosing the particular values according to recommendations for word2vec from prior work (Levy et al., 2015).", "startOffset": 153, "endOffset": 172}, {"referenceID": 12, "context": "We train the model using binary cross-entropy loss (Kurata et al., 2016; Nam et al., 2014).", "startOffset": 51, "endOffset": 90}, {"referenceID": 19, "context": "We train the model using binary cross-entropy loss (Kurata et al., 2016; Nam et al., 2014).", "startOffset": 51, "endOffset": 90}, {"referenceID": 14, "context": "Though prior work found context distribution smoothing consistently beneficial (Levy et al., 2015), in the preliminary experiments reported in this study we have not seen such a strong preference.", "startOffset": 79, "endOffset": 98}, {"referenceID": 10, "context": "data (Kaji and Kobayashi, 2017).", "startOffset": 5, "endOffset": 31}, {"referenceID": 10, "context": "iments comparing incremental SGNS to its batch version and word2vec in prior work (Kaji and Kobayashi, 2017).", "startOffset": 82, "endOffset": 108}, {"referenceID": 10, "context": "While we cannot yet wholeheartedly endorse the use of spacesaving-word2vec in real-world applications, we hope that thoughtful future research drawing on insights gleaned from spacesaving-word2vec and incremental SGNS (Kaji and Kobayashi, 2017) will close this gap.", "startOffset": 218, "endOffset": 244}], "year": 2017, "abstractText": "We develop a streaming (one-pass, boundedmemory) word embedding algorithm based on the canonical skip-gram with negative sampling algorithm implemented in word2vec. We compare our streaming algorithm to word2vec empirically by measuring the cosine similarity between word pairs under each algorithm and by applying each algorithm in the downstream task of hashtag prediction on a two-month interval of the Twitter sample stream. We then discuss the results of these experiments, concluding they provide partial validation of our approach as a streaming replacement for word2vec. Finally, we discuss potential failure modes and suggest directions for future work.", "creator": "LaTeX with hyperref package"}}}