{"id": "1704.07506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Some Like it Hoax: Automated Fake News Detection in Social Networks", "abstract": "In recent years, the reliability of information on the Internet has emerged as a crucial issue of modern society. Social network sites (SNSs) have revolutionized the way in which information is spread by allowing users to freely share content. As a consequence, SNSs are also increasingly used as vectors for the diffusion of misinformation and hoaxes. The amount of disseminated information and the rapidity of its diffusion make it practically impossible to assess reliability in a timely manner, highlighting the need for automatic hoax detection systems.", "histories": [["v1", "Tue, 25 Apr 2017 01:20:40 GMT  (1869kb,D)", "http://arxiv.org/abs/1704.07506v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.HC cs.SI", "authors": ["eugenio tacchini", "gabriele ballarin", "marco l della vedova", "stefano moret", "luca de alfaro"], "accepted": false, "id": "1704.07506"}, "pdf": {"name": "1704.07506.pdf", "metadata": {"source": "CRF", "title": "Some Like it Hoax: Automated Fake News Detection in Social Networks", "authors": ["Eugenio Tacchini", "Gabriele Ballarin", "Marco L. Della Vedova", "Stefano Moret", "Luca de Alfaro"], "emails": ["eugenio.tacchini@unicatt.it", "gabriele.ballarin@gmail.com", "marco.dellavedova@unicatt.it", "moret.stefano@gmail.com", "luca@ucsc.edu"], "sections": [{"heading": null, "text": "To contribute to this goal, we show that Facebook posts can be classified as hoaxes or non-hoaxes with high accuracy, based on the users who \"liked\" them. We present two classification techniques, one based on logistical regression, the other based on a novel adaptation of Boolean crowdsourcing algorithms. On a data set consisting of 15,500 Facebook posts and 909,236 users, we obtain classification accuracies of over 99%, even if the training set contains less than 1% of the posts. We also show that our techniques are robust: they work even if we limit our attention to users who like both false and non-false messages. These results suggest that mapping the distribution pattern of information can be a useful part of automatic falsehood detection systems."}, {"heading": "1 Introduction", "text": "The World Wide Web (WWW) has revolutionized the way information is first disseminated by email. In particular, social network sites (SNSs) are platforms on which Contentar Xiv: 170 4.07 506v 1 can be freely shared, allowing users to actively participate in novel techniques - and, possibly, influence - information diffusion processes. As a consequence, SNSs are also increasingly being used as vectors for the dissemination of spam [10], conspiracy theories and misinformation, i.e. intentionally designed fake pages in our time as the age of misinformation. A significant share of misinformation to SNSs spreads rapidly, with a peak in the first 2 hours. This finding, along with the large amount of shared content, underscores the need for automatic online hoax detection systems [9].In the literature, various approaches to automatic misinformation covering fairly heterogeneous applications have been suggested."}, {"heading": "2 Dataset", "text": "In fact, the number of users has increased in recent years in the United States, Europe, Europe, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa"}, {"heading": "3 Algorithmic Classification of Posts", "text": "Our goal is to divide posts into hoaxes and non-hoaxes. According to the analysis of social media sharing in [6], \"users tend to coalesce into communities of interest, which amplifies and promotes confirmation bias, segregation, and polarization,\" and \"users tend to select and share content according to a particular narrative and ignore the rest.\" This suggests that the number of users who like a post should be highly indicative of the nature of the post. We present two approaches, one based on logistical regression, the other on Boolean crowdsourcing algorithms."}, {"heading": "3.1 Classification via logistic regression", "text": "We formulate the post-classification problem as an overarching learning, binary classification problem. We look at a set of posts I and a set of users U, that is, on the basis on which the users liked them. To perform the classification, we use a logistic regression model. The logistic regression model learns a weight wu for each user u, the probability that a post i is not hoax is then given by pi = (1 + e \u2212 yi), where yi = u xiuwu. Intuitively, wu > 0, the probability that a post i is not hoax is given by pi."}, {"heading": "3.2 Classification via harmonic boolean label crowdsourcing", "text": "The weak aspect of logistical regression is that it does not pass on information about users who liked some of the same posts. In particular, if the training platform does not contain posts requested by a user, then logistical regression will not be able to learn anything about u, and posts that are only liked by users not in the training platform cannot be classified. As an alternative approach, we propose to derive the hoax / non-hoax classification using algorithms from crowdsourcing, from the Boolean label crowdsourcing (BLC) problem. in the BLC problem, users provide true / false labels for posts indicating whether a post is vandalism or whether it violates community policies. The BLC problem consists in calculating the consensus labels from the user input [13, 15, 5]."}, {"heading": "4 Results", "text": "We characterize the performance of logistical regression and the harmonious BLC algorithm using two sets of experiments: the first series measures the accuracy of the algorithms based on the number of posts available as a training set. Since the training set can generally only be produced through a laborious process of manual follow-up, these results tell us how much we need to invest in manual labeling to take advantage of automated classification; the second series measures how much information our learning can transfer from one page to another. As the community of Facebook users is organized across pages, these experiments shed light on how much we can transfer from one community to another through shared users among communities."}, {"heading": "4.1 Accuracy of classification vs. training set size", "text": "This year, it has come to the point where it can only take one year to reach a solution in which an agreement can be reached."}, {"heading": "4.2 Cross-page learning", "text": "Since the community of Facebook users naturally revolves around common interests and pages, an interesting question arises as to whether what we learn from a community of users on one page is transferred to other pages. To answer this question, we test our classifiers on posts related to pages they have not seen during the training phase. To this end, we conduct two experiments in which the pages from which we learn and those on which we test fall apart. In the first experiment, we take turns selecting one page and placing all posts belonging to those pages in the test set; the posts belonging to all other pages are in the training set; in the second experiment, we perform 50 runs on half pages. In each run, we randomly select a group consisting of half the pages in the data set, and we place the posts belonging to those pages in the test set, and all others in the training set."}, {"heading": "5 Conclusions", "text": "The high accuracy achieved by both logistical regression and the harmonious BLC algorithm confirms our basic hypothesis: the number of users who interact with messages on social networks can be used to predict whether posts are hoaxes. We presented two techniques for using this information: one is based on logistic regression, the other on Boolean label crowdsourcing (BLC). Both algorithms perform well, with the harmonious BLC algorithm offering an accuracy of over 99%, even when trained by groups of posts consisting of 0.5% of the entire data set (or about 80 posts), suggesting that the algorithms can scale up to the size of entire social networks, while requiring only a modest level of manual classification. We also analyzed the extent to which our performance depends on the community of users being naturally aggregated around pages of similar content."}], "references": [{"title": "Wikipedia vandalism detection: Combining natural language, metadata, and reputation features", "author": ["B Adler", "Luca De Alfaro", "Santiago Mola-Velasco", "Paolo Rosso", "Andrew West"], "venue": "Computational linguistics and intelligent text processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A Content-driven Reputation System for the Wikipedia", "author": ["B. Thomas Adler", "Luca de Alfaro"], "venue": "In Proceedings of the 16th International Conference on World Wide Web, WWW", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Science vs Conspiracy: Collective Narratives in the Age of Misinformation", "author": ["Alessandro Bessi", "Mauro Coletto", "George Alexandru Davidescu", "Antonio Scala", "Guido Caldarelli", "Walter Quattrociocchi"], "venue": "PLOS ONE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Scam Detection in Twitter. In Katsutoshi Yada, editor, Data Mining for Service, number 3 in Studies in Big Data, pages 133\u2013150", "author": ["Xiaoling Chen", "Rajarathnam Chandramouli", "Koduvayur P. Subbalakshmi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Reliable aggregation of boolean crowdsourced tasks", "author": ["Luca de Alfaro", "Vassilis Polychronopoulos", "Michael Shavlovsky"], "venue": "In Third AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "The spreading of misinformation online", "author": ["Michela Del Vicario", "Alessandro Bessi", "Fabiana Zollo", "Fabio Petroni", "Antonio Scala", "Guido Caldarelli", "H. Eugene Stanley", "Walter Quattrociocchi"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The digitization of word of mouth: Promise and challenges of online feedback mechanisms", "author": ["Chrysanthos Dellarocas"], "venue": "Management science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Accuracy of Metrics for Inferring Trust and Reputation in Semantic Web-Based Social Networks. In Engineering Knowledge in the Age of the Semantic Web, pages 116\u2013131", "author": ["Jennifer Golbeck", "James Hendler"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "A first step towards automatic hoax detection", "author": ["J.C. Hernandez", "C.J. Hernandez", "J.M. Sierra", "A. Ribagorda"], "venue": "In Proceedings. 36th Annual 2002 International Carnahan Conference on Security Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Fighting Spam on Social Web Sites: A Survey of Approaches and Future Challenges", "author": ["P. Heymann", "G. Koutrika", "H. Garcia-Molina"], "venue": "IEEE Internet Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Distance-based hoax detection system", "author": ["A. Ishak", "Y.Y. Chen", "Suet-Peng Yong"], "venue": "In 2012 International Conference on Computer Information Science (ICCIS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Assessment of Tweet Credibility with LDA Features", "author": ["Jun Ito", "Jing Song", "Hiroyuki Toda", "Yoshimasa Koike", "Satoshi Oyama"], "venue": "In Proceedings of the 24th International Conference on World Wide Web, WWW \u201915 Companion,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["David R. Karger", "Sewoong Oh", "Devavrat Shah"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Disinformation on the Web: Impact, Characteristics, and Detection of Wikipedia Hoaxes", "author": ["Srijan Kumar", "Robert West", "Jure Leskovec"], "venue": "In Proceedings of the 25th International Conference on World Wide Web, WWW", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Variational inference for crowdsourcing", "author": ["Qiang Liu", "Jian Peng", "Alexander T. Ihler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Filtering spam with spamassassin", "author": ["Justin Mason"], "venue": "In HEANet Annual Conference,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "A Computational Model of Trust and Reputation for E-businesses", "author": ["L. Mui", "M. Mohtashemi", "A. Halberstadt"], "venue": "In Proceedings of the 35th Annual Hawaii International Conference on System Sciences", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "E-Mail System for Automatic Hoax Recognition", "author": ["Tomislav Petkovi\u0107", "Zvonko Kostanj\u010dar", "Predrag Pale"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Automatic vandalism detection in wikipedia", "author": ["Martin Potthast", "Benno Stein", "Robert Gerling"], "venue": "In European Conference on Information Retrieval,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Detection of Internet scam using logistic regression", "author": ["M. Sharifi", "E. Fink", "J.G. Carbonell"], "venue": "In 2011 IEEE International Conference on Systems, Man, and Cybernetics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "An Intelligent Automatic Hoax Detection System. In Knowledge-Based and Intelligent Information and Engineering Systems, pages 318\u2013325", "author": ["Marin Vukovi\u0107", "Kre\u0161imir Pripu\u017ei\u0107", "Hrvoje Belani"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Optimising anti-spam filters with evolutionary algorithms", "author": ["Iryna Yevseyeva", "Vitor Basto-Fernandes", "David Ruano-Ord\u00e1s", "Jos\u00e9 R. M\u00e9ndez"], "venue": "Expert Systems with Applications,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "As a consequence, SNSs are also increasingly used as vectors for the dissemination of spam [10], conspiracy theories and hoaxes, i.", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "This recently led to the emphatic definition of our current times as the age of misinformation [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "A significant share of hoaxes on SNSs diffuses rapidly, with a peak in the first 2 hours [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "This finding, together with the high amount of shared content, highlights the need of automatic online hoax detection systems [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "In the context of scam e-mail detection, spamassassin uses keyword-based methods with logistic regression [16]; Petkovi\u0107 et al.", "startOffset": 106, "endOffset": 110}, {"referenceID": 17, "context": "[18] and Ishak et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] proposed the use of distance-based methods; Vukovi\u0107 et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] applied neural network and advanced text processing; Yevseyeva et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] used evolutionary algorithms for the development of anti-spam filters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] applied logistic regression to automatically detect scam on webpages, reaching an accuracy of 98%.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The concepts of trust and reputation [17, 7] can be adopted for hoax detection in applications with a dominant social component.", "startOffset": 37, "endOffset": 44}, {"referenceID": 6, "context": "The concepts of trust and reputation [17, 7] can be adopted for hoax detection in applications with a dominant social component.", "startOffset": 37, "endOffset": 44}, {"referenceID": 7, "context": "Metrics and algorithms for this purpose have been proposed by Golbeck and Hendler [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "Adler and de Alfaro [2] developed a content-driven user reputation system for Wikipedia, allowing to predict the quality of new contributions.", "startOffset": 20, "endOffset": 23}, {"referenceID": 18, "context": "in [19, 1, 14].", "startOffset": 3, "endOffset": 14}, {"referenceID": 0, "context": "in [19, 1, 14].", "startOffset": 3, "endOffset": 14}, {"referenceID": 13, "context": "in [19, 1, 14].", "startOffset": 3, "endOffset": 14}, {"referenceID": 3, "context": "[4] developed a semi-supervised scam detector for Twitter based on self-learning and clustering analysis, while Ito et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] proposed the use of Latent Dirichlet Allocation (LDA) to assess the credibility of tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "In particular, focusing on Facebook, we answer the following research question: Can a hoax be identified based on the users who \u201cliked\u201d it? We consider a dataset consisting of 15,500 posts and 909,236 users; the posts originate from pages that deal with either scientific topics or with conspiracies and fake scientific news [3].", "startOffset": 325, "endOffset": 328}, {"referenceID": 2, "context": "We based our selection of pages on [3].", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "Among the 73 pages listed in [3], we limited our analysis to the top 20 pages of both categories.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "We note also that the actual posts comprising our dataset are distinct from those originally included in the dataset of [3], as we performed our data collection in a different, and more recent, period.", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "According to the analysis of social media sharing by [6], \u201cusers tend to aggregate in communities of interest, which causes reinforcement and fosters confirmation bias, segregation, and polarization\u201d, and \u201cusers mostly tend to select and share content according to a specific narrative and to ignore the rest.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "The BLC problem consists in computing the consensus labels from the user input [13, 15, 5].", "startOffset": 79, "endOffset": 90}, {"referenceID": 14, "context": "The BLC problem consists in computing the consensus labels from the user input [13, 15, 5].", "startOffset": 79, "endOffset": 90}, {"referenceID": 4, "context": "The BLC problem consists in computing the consensus labels from the user input [13, 15, 5].", "startOffset": 79, "endOffset": 90}, {"referenceID": 12, "context": "The algorithms compare what people say, correct for the effect of the liars, and reconstruct a consensus truth [13, 5].", "startOffset": 111, "endOffset": 118}, {"referenceID": 4, "context": "The algorithms compare what people say, correct for the effect of the liars, and reconstruct a consensus truth [13, 5].", "startOffset": 111, "endOffset": 118}, {"referenceID": 4, "context": "We present here an adaptation of the harmonic algorithm of [5] to a setting with a learning set of posts.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "We chose the harmonic algorithm because it is computationally efficient, can cope with large datasets, and it offers good accuracy in practice, as evidenced in [5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": "Furthermore, while the harmonic algorithm can be adapted to the presence of a learning set, it is less obvious how to do so for some of the other algorithms, such as those of [13].", "startOffset": 175, "endOffset": 179}], "year": 2017, "abstractText": "In recent years, the reliability of information on the Internet has emerged as a crucial issue of modern society. Social network sites (SNSs) have revolutionized the way in which information is spread by allowing users to freely share content. As a consequence, SNSs are also increasingly used as vectors for the diffusion of misinformation and hoaxes. The amount of disseminated information and the rapidity of its diffusion make it practically impossible to assess reliability in a timely manner, highlighting the need for automatic hoax detection systems. As a contribution towards this objective, we show that Facebook posts can be classified with high accuracy as hoaxes or non-hoaxes on the basis of the users who \u201cliked\u201d them. We present two classification techniques, one based on logistic regression, the other on a novel adaptation of boolean crowdsourcing algorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users, we obtain classification accuracies exceeding 99% even when the training set contains less than 1% of the posts. We further show that our techniques are robust: they work even when we restrict our attention to the users who like both hoax and non-hoax posts. These results suggest that mapping the diffusion pattern of information can be a useful component of automatic hoax detection systems.", "creator": "LaTeX with hyperref package"}}}