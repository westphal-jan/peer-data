{"id": "1704.07555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Molecular De Novo Design through Deep Reinforcement Learning", "abstract": "This work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties. We demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target. As a proof of principle, the model is first trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique that could be used for scaffold hopping or library expansion starting from a single molecule. Finally, when tuning the model towards generating compounds predicted to be active against the dopamine receptor D2, the model generates structures of which more than 95% are predicted to be active, including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model.", "histories": [["v1", "Tue, 25 Apr 2017 06:41:21 GMT  (876kb,D)", "http://arxiv.org/abs/1704.07555v1", null], ["v2", "Tue, 29 Aug 2017 12:31:19 GMT  (984kb,D)", "http://arxiv.org/abs/1704.07555v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marcus olivecrona", "thomas blaschke", "ola engkvist", "hongming chen"], "accepted": false, "id": "1704.07555"}, "pdf": {"name": "1704.07555.pdf", "metadata": {"source": "META", "title": "Molecular De-Novo Design through Deep Reinforcement Learning", "authors": ["Marcus Olivecrona", "Thomas Blaschke", "Ola Engkvist", "Hongming Chen"], "emails": ["m.olivecrona@gmail.com"], "sections": [{"heading": null, "text": "This paper presents a method for adapting a sequence-based generative model for molecular de novo design, which can learn to create structures with certain desirable properties by increasing episodic probability. We show how this model can perform a number of tasks, such as generating analogies to a query structure and generating compounds that are predicted to be active against a biological target. To prove this, the model is first trained to produce molecules that do not contain sulfur. Secondly, the model is trained to generate analogies to the drug celecoxib, a technique that could be used to jump from scaffolds or expand libraries from a single molecule."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Recurrent Neural Networks", "text": "A recursive neural network is an architecture of neural networks designed to take advantage of the symmetry between steps in sequential data, while at the same time tracking at each step the most important information of the steps seen before, which can influence the interpretation of the current step. [22] This is done by introducing the concept of a cell (Figure 1). For each step t, the cell is a result of the previous cell \u2212 1 and the current input xt \u2212 1. The cell content determines both the output in the current step and the next cell state. The cell allows the network to have a memory of past events that can be used in deciding on the interpretation of new data. These properties make an RNN particularly suitable for problems in the area of natural language processing. In this setting, a sequence of words can be encoded into uniform vectors of the length of our vocabulary X. Two additional tokens, GO and EOS, can be added to indicate the beginning and end of the sequence, respectively."}, {"heading": "2.1.1 Learning the data", "text": "In each step, the model generates a probability distribution of what the next character is likely to be, and the goal is to maximize the probability assigned to the correct symbol: L (1) = \u2212 T = 1logP (xt | xt \u2212 1,... x1) The cost function L (1), which is often applied to a subset of all training examples, is minimized in terms of network parameters. Considering a predicted log probability logP of the target in step t, the cost function L (1), which is often applied to a subset of all training examples, the number of outliers is reduced."}, {"heading": "2.1.2 Generating new samples", "text": "Once an RNN has been trained on target sequences, it can be used to generate new sequences that follow the conditional probability distributions learned from the training set. The first input - the GO token - is given, and in each step of time, after we have queried an output token xt from the predicted probability distribution P (Xt) above our vocabulary X, we use xt as our next input. Once the EOS token is scanned, the sequence is considered finished (Figure 2)."}, {"heading": "2.1.3 Tokenizing and one-hot encoding SMILES", "text": "A SMILES [25] string represents a molecule as a sequence of characters corresponding to the atoms, as well as special characters indicating the opening and closing of rings and branches. In most cases, the SMILES string is based on a single character, with the exception of atomic types comprising two characters, such as \"Cl\" Cellt = 1x1GOCellt = 2x2Cellt = 3x3Cellt = 4EOS Figure 2 Generating sequences. Sequence generation by a trained RNN. Each time we try the next character of the sequence xt from the probability distribution given by the RNN, which is then used in the next input.and \"Br\" and special environments indicated by square brackets (e.g. [nH]), all of which are considered a symbol for sequence 0."}, {"heading": "2.2 Reinforcement Learning", "text": "Consider an agent who, given a certain state of what an agent has to do, must choose what action an agent has to take, with S being the set of possible states, and A (s) the set of possible actions for that state. An agent's policy \u03c0 (a | s) represents a state of probability of any action taken therein. Many problems of the fortification process are conceived as Markov decision-making processes, meaning that the current state contains all the information necessary to guide our actions, and that nothing is gained by knowing the history of past states. For most real problems, this is an approximation rather than a truth; however, we can generalize this concept to that of a partially observable Markov decision-making process, in which the agent can interact with an incomplete representation of the environment. Let r (a | s) be the reward that functions as a measure of how good it was to grasp an action in a particular state, and the long-term return to T = 1."}, {"heading": "2.3 The Prior network", "text": "The RNN was trained with the TDKit [26] canonical SMILES of 1.5 million structures of ChEMBL [20], where the molecules were limited to containing between 10 and 50 heavy atoms and elements, such as H, B, C, N, O, F, Si, P, S, Cl, Br, I}. The model was trained with stochastic gradient in 50,000 steps using the Adam optimizer [27] (\u03b21 = 0.9, \u03b22 = 0.999 and = 10 \u2212 8) with an initial learning rate of 0.001 and a learning rate of 0.02 learning drop every 100 steps. Slopes were truncated to [\u2212 3, 3]. Tensorflow [28] was used to implement the prior and the reinforcing agent."}, {"heading": "2.4 The Agent network", "text": "We now frame the problem of generating a SMILES representation of a molecule with specified desirable properties via an RNN as a partially observable Markov decision process, in which the agent must make a decision about which character to select next given the current cell state. We use the probability distributions learned through the aforementioned previous model as our original previous policy. We refer to the network that simply uses the previous policy as the prior policy, and to the network whose policy has since been modified as an agent. The task is episodic, starting with the first step of the RNN and ending when the EOS symbol is evaluated. The sequence of measures A = a1, a2,..., aT during this episode represents the SMILES generated, and the product of the probabilities of action P (A) = optimized T = 1 \u03c0 t = 1 \u03c0 (at | st) represents the model probability of the policy-formed function, LASS-1, which evaluates the actions generated."}, {"heading": "2.5 The DRD2 activity model", "text": "In one of our studies, the target of the drug was used to produce molecules that are predicted to be active against a biological target, selecting the type 2 dopamine receptor DRD2 as the target and extracting corresponding bioactivity data from ExCAPE-DB [29]. In this dataset, there are 7218 active (pIC50 > 5) and 343204 inactive (pIC50 < 5). A subset of 100,000 inactive compounds was randomly selected. In order to reduce the closest proximity between the training and test structures, the active ones were grouped into clusters based on their molecular similarity. The Jaccard [30] index, for binary vectors, also known as Tanimoto similarity, was based on the RDKit implementation of binary extended connectivity molecular fingerprints with a diameter of 6 (ECFP6 [31] and the activity divided into the three molecules based on it."}, {"heading": "3 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Structure generation by the Prior", "text": "After the first training, 94% of the sequences generated by the previous 2.1.2 parse corresponded to valid molecular structures according to RDKit [26], of which 90% are new structures outside the training set. A set of random structures generated by the previous step is shown in the appendix. The process of generating a SMILES by the prior is shown in Figure 5. For each token in the generated SMILES sequence, the conditional probability distribution in this step is shown after the prior."}, {"heading": "3.2 Learning to avoid sulphur", "text": "As proof of principle, the active ingredient was first trained to produce sulphur-free molecules by defining the scoring function as follows: S (A) = 1 if valid and no S 0 if not valid \u2212 1 if SThe agent is contained, was trained in 1000 steps using \u03c3 = 2, and 12,800 SMILES sequences were evaluated from both the Prior network and the Agent network for comparison. After training, only 2% of the structures generated by the agent contained sulphur, compared to 32% for the structures generated by the Prior. To investigate the effect of the training on the structures generated, relevant properties for sulphur-free structures generated by both the Prior and the Agent were compared. Molecular weight, cLogP, the number of rotating bonds and the number of aromatic rings were all calculated using RDKit. The experiment was repeated three times with different random seeds, there was no significant difference in the active ingredient values, however, while the two higher averages were sufficiently different for the active ingredient values."}, {"heading": "3.3 Similarity guided structure generation", "text": "The second task was to generate structures that resemble a query structure, and we first investigated whether this could be generated by using high values. Ji, j of the RDKit implementation of FCFP4 [31] fingerprints was used as a measure of similarity between molecules i and j. The scoring function was defined as: S (A) = 1 + 2 \u00b7 min {Ji, j, k} kThis definition means invariant version of the fingerprints and the small diameter 4 were used to get a fuzzy similarity measure. The scoring function was defined as: S (A) + 2 \u00b7 min {Ji, j} kThis definition means that an increase in similarity is only rewarded up to the point of k [0, 1] as scaling the reward from \u2212 1 (no overlap in the fingerprints between query and generated structure) to 1 (at least k degree of overlap)."}, {"heading": "3.4 Target activity guided structure generation", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4 Conclusion", "text": "In summary, we believe that an RNN based on the SMILES representation of molecules is not yet a promising method for molecular design. It is a data-driven generative model that is not based on predefined building blocks and rules that clearly distinguish it from traditional methods. In this study, we are extending on previous work [13-15, 17] by introducing an enhanced learning method that can be used to generate the RNN structures with certain desirable properties. The model was tested to the task of generating free molecules as proof of principle, suggesting that the agent can find solutions within the space of the primary while first exploring the underlying probability distribution. To evaluate whether the model could be used to generate analogs, the agent was used to generate free molecular molecular molecular molecules as evidence of principles."}, {"heading": "Acknowledgements", "text": "The authors thank Thierry Kogej and Christian Tyrchan for the general support and discussion as well as Dominik Peters for his expertise in LATEX."}], "references": [{"title": "Computer-based de novo design of drug-like molecules", "author": ["G. Schneider", "U. Fechner"], "venue": "Nat Rev Drug Discov 4(8), 649\u2013663", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "The computer program ludi: A new method for the de novo design of enzyme inhibitors", "author": ["B\u00f6hm", "H.-J."], "venue": "Journal of Computer-Aided Molecular Design 6(1), 61\u201378", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Sprout: Recent developments in the de novo design of molecules", "author": ["V.J. Gillet", "W. Newell", "P. Mata", "G. Myatt", "S. Sike", "Z. Zsoldos", "A.P. Johnson"], "venue": "Journal of Chemical Information and Computer Sciences 34(1), 207\u2013217", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Visualization and virtual screening of the chemical universe database gdb-17", "author": ["L. Ruddigkeit", "L.C. Blum", "Reymond", "J.-L."], "venue": "Journal of Chemical Information and Modeling 53(1), 56\u201365", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Dogs: Reaction-driven de novo design of bioactive compounds", "author": ["M. Hartenfeller", "H. Zettl", "M. Walter", "M. Rupp", "F. Reisen", "E. Proschak", "S. Weggen", "H. Stark", "G. Schneider"], "venue": "PLOS Computational Biology 8, 1\u201312", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Reaction-driven de novo design, synthesis and testing of potential type ii kinase inhibitors", "author": ["G. Schneider", "T. Geppert", "M. Hartenfeller", "F. Reisen", "A. Klenner", "M. Reutlinger", "V. H\u00e4hnke", "J.A. Hiss", "H. Zettl", "S. Keppner", "B. Sp\u00e4nkuch", "P. Schneider"], "venue": "Future Medicinal Chemistry 3(4), 415\u2013424", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Automated design of ligands to polypharmacological profiles", "author": ["J. Besnard", "G.F. Ruda", "V. Setola", "K. Abecassis", "R.M. Rodriguiz", "Huang", "X.-P.", "S. Norval", "M.F. Sassano", "A.I. Shin", "L.A. Webster", "F.R.C. Simeons", "L. Stojanovski", "A. Prat", "N.G. Seidah", "D.B. Constam", "G.R. Bickerton", "K.D. Read", "W.C. Wetsel", "I.H. Gilbert", "B.L. Roth", "A.L. Hopkins"], "venue": "Nature 492(7428), 215\u2013220", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Inverse qspr/qsar analysis for chemical structure generation (from y to x)", "author": ["T. Miyao", "H. Kaneko", "K. Funatsu"], "venue": "Journal of Chemical Information and Modeling 56(2), 286\u2013299", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "The signature molecular descriptor: 3", "author": ["C.J. Churchwell", "M.D. Rintoul", "S. Martin", "D.P.V. Jr.", "A. Kotu", "R.S. Larson", "L.O. Sillerud", "D.C. Brown", "Faulon", "J.-L."], "venue": "inverse-quantitative structure\u2013activity relationship of icam-1 inhibitory peptides. Journal of Molecular Graphics and Modelling 22(4), 263\u2013273", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "A constructive approach for discovering new drug leads: Using a kernel methodology for the inverse-qsar problem", "author": ["W.W. Wong", "F.J. Burkowski"], "venue": "J Cheminform 1, 4\u20134", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Interspeech, vol. 2, p. 3", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A first look at music composition using lstm recurrent neural networks", "author": ["D. Eck", "J. Schmidhuber"], "venue": "Technical report", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks", "author": ["M.H.S. Segler", "T. Kogej", "C. Tyrchan", "M.P. Waller"], "venue": "ArXiv e-prints", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Automatic chemical design using a data-driven continuous representation of molecules", "author": ["R. G\u00f3mez-Bombarelli", "D.K. Duvenaud", "J.M. Hern\u00e1ndez-Lobato", "J. Aguilera-Iparraguirre", "T.D. Hirzel", "R.P. Adams", "A. Aspuru-Guzik"], "venue": "CoRR abs/1610.02415", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "CoRR abs/1609.05473", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement Learning: An Introduction, 1st edn", "author": ["R. Sutton", "A. Barton"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Tuning recurrent neural networks with reinforcement learning", "author": ["N. Jaques", "S. Gu", "R.E. Turner", "D. Eck"], "venue": "CoRR abs/1611.02796", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Partition coefficients and their uses", "author": ["A. Leo", "C. Hansch", "D. Elkins"], "venue": "Chemical Reviews 71(6), 525\u2013616", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1971}, {"title": "Quantifying the chemical beauty of drugs", "author": ["G.R. Bickerton", "G.V. Paolini", "J. Besnard", "S. Muresan", "A.L. Hopkins"], "venue": "Nature Chemistry 4, 90\u201398", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Chembl: a large-scale bioactivity database for drug discovery", "author": ["A. Gaulton", "L.J. Bellis", "A.P. Bento", "J. Chambers", "M. Davies", "A. Hersey", "Y. Light", "S. McGlinchey", "D. Michalovich", "B. Al-Lazikani", "J.P. Overington"], "venue": "Nucleic Acids Res 40, 1100\u20131107", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks", "author": ["I.J. Goodfellow", "M. Mirza", "D. Xiao", "A. Courville", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural  Olivecrona et al.  Page 12 of 16 Comput. 9(8), 1735\u20131780", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR abs/1412.6980", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Excape-db: an integrated large scale dataset facilitating big data analysis in chemogenomics", "author": ["J. Sun", "N. Jeliazkova", "V. Chupakin", "Golib-Dzib", "J.-F.", "O. Engkvist", "L. Carlsson", "J. Wegner", "H. Ceulemans", "I. Georgiev", "V. Jeliazkov", "N. Kochev", "T.J. Ashby", "H. Chen"], "venue": "Journal of Cheminformatics 9(1), 17", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2017}, {"title": "\u00c9tude comparative de la distribution florale dans une portion des Alpes et des Jura", "author": ["P. Jaccard"], "venue": "Bulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles 37, 547\u2013579", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1901}, {"title": "Extended-connectivity fingerprints", "author": ["D. Rogers", "M. Hahn"], "venue": "Journal of Chemical Information and Modeling 50(5), 742\u2013754", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised data base clustering based on daylight\u2019s fingerprint and tanimoto similarity: A fast and automated way to cluster small and large data sets", "author": ["D. Butina"], "venue": "Journal of Chemical Information and Computer Sciences 39(4), 747\u2013750", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research 12, 2825\u20132830", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Chemically Advanced Template Search (CATS) for Scaffold-Hopping and Prospective Target Prediction for \u2019Orphan\u2019 Molecules", "author": ["M. Reutlinger", "C.P. Koch", "D. Reker", "N. Todoroff", "P. Schneider", "T. Rodrigues", "G. Schneider"], "venue": "Mol Inform 32(2), 133\u2013138", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Using Tversky similarity searches for core hopping: finding the needles in the haystack", "author": ["S. Senger"], "venue": "J Chem Inf Model 49(6), 1514\u20131524", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR abs/1409.2329", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28. ICML\u201913, pp. 1058\u20131066", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res. 3, 1137\u20131155", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "In this case, the haystack comprises the on the order of 10\u221210 synthetically feasible molecules [1], out of which we need to find an compound which satisfies the plethora of criteria such as bioactivity, drug metabolism and pharmacokinetic (DMPK) profile, synthetic accessibility, etc.", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "Early de novo design algorithms [1] used structure based approaches to grow ligands to sterically and electronically fit the binding pocket of the target of interest [2, 3].", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "Early de novo design algorithms [1] used structure based approaches to grow ligands to sterically and electronically fit the binding pocket of the target of interest [2, 3].", "startOffset": 166, "endOffset": 172}, {"referenceID": 2, "context": "Early de novo design algorithms [1] used structure based approaches to grow ligands to sterically and electronically fit the binding pocket of the target of interest [2, 3].", "startOffset": 166, "endOffset": 172}, {"referenceID": 3, "context": "Full list of author information is available at the end of the article and query structure similarity [4, 5].", "startOffset": 102, "endOffset": 108}, {"referenceID": 4, "context": "Full list of author information is available at the end of the article and query structure similarity [4, 5].", "startOffset": 102, "endOffset": 108}, {"referenceID": 5, "context": "One way to create such a virtual library is to use known chemical reactions alongside a set of available chemical building blocks, resulting in a large number of synthetically accessible structures [6]; another possibility is to use transformational rules based on the expertise of medicinal chemists to design analogues to a query structure.", "startOffset": 198, "endOffset": 201}, {"referenceID": 6, "context": "[7] applied a transformation rule approach to the design of novel dopamine receptor D2 (DRD2) receptor active compounds with specific polypharmacological profiles and appropriate DMPK profiles for a CNS indication.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "A third approach, known as inverse Quantitative Structure Activity Relationship (inverse QSAR), tackles the problem from a different angle: rather than first generating a virtual library and then using a QSAR model to score and search this library, inverse QSAR aims to map a favourable region in terms of predicted activity to the corresponding molecular structures [8\u2013 10].", "startOffset": 367, "endOffset": 374}, {"referenceID": 10, "context": "The Recurrent Neural Network (RNN) is commonly used as a generative model for data of sequential nature, and have been used successfully for tasks such as natural language processing [11] and music generation [12].", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "The Recurrent Neural Network (RNN) is commonly used as a generative model for data of sequential nature, and have been used successfully for tasks such as natural language processing [11] and music generation [12].", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "Recently, there has been an increasing interest in using this type of generative model for the de novo design of molecules [13\u201315].", "startOffset": 123, "endOffset": 130}, {"referenceID": 13, "context": "Recently, there has been an increasing interest in using this type of generative model for the de novo design of molecules [13\u201315].", "startOffset": 123, "endOffset": 130}, {"referenceID": 14, "context": "Recently, there has been an increasing interest in using this type of generative model for the de novo design of molecules [13\u201315].", "startOffset": 123, "endOffset": 130}, {"referenceID": 12, "context": "demonstrated that an RNN trained on the canonicalized SMILES representation of molecules can learn both the syntax of the language as well as the distribution in chemical space [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 15, "context": "In two recent studies, reinforcement learning [16] was used to fine tune pre-trained RNNs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "[15] use an adversarial network to estimate the expected return for state-action pairs sampled from the RNN, and by increasing the likelihood of highly rated pairs improves the generative network for tasks such as poem generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] use Deep Q-learning to improve a pre-trained generative RNN by introducing two ways to score the sequences generated: one is a measure of how well the sequences adhere to music theory, and one is the likelihood of sequences according to the initial pre-trained RNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "They later extend this method to several other tasks including the generation of SMILES, and optimize toward molecular properties such as cLogP [18] and QED drug-likeness [19].", "startOffset": 144, "endOffset": 148}, {"referenceID": 18, "context": "They later extend this method to several other tasks including the generation of SMILES, and optimize toward molecular properties such as cLogP [18] and QED drug-likeness [19].", "startOffset": 171, "endOffset": 175}, {"referenceID": 16, "context": "However, they report that the method can lead to exploitation of the reward resulting in unrealistically simple molecules that are more likely to satisfy the optimization requirements than more complex structures [17].", "startOffset": 213, "endOffset": 217}, {"referenceID": 15, "context": "In this study we propose a, to our knowledge, novel approach using reinforcement learning to tune RNNs for episodic tasks [16], in this case the task of generating molecules with given desirable properties.", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "Through learning an augmented episodic likelihood which is a composite of prior likelihood [17] and a user defined scoring function, the method aims to fine-tune an RNN pre-trained on the ChEMBL database [20] towards generating desirable compounds.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "Through learning an augmented episodic likelihood which is a composite of prior likelihood [17] and a user defined scoring function, the method aims to fine-tune an RNN pre-trained on the ChEMBL database [20] towards generating desirable compounds.", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "Compared to maximum likelihood estimation finetuning [13], this method can make use of negative as well as continuous scores, and may reduce the risk of catastrophic forgetting [21].", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "Compared to maximum likelihood estimation finetuning [13], this method can make use of negative as well as continuous scores, and may reduce the risk of catastrophic forgetting [21].", "startOffset": 177, "endOffset": 181}, {"referenceID": 21, "context": "1 Recurrent Neural Networks A recurrent neural network is an architecture of neural networks designed to make use of the symmetry across steps in sequential data while simultaneously at every step keep track of the most salient information of previously seen steps, which may affect the interpretation of the current one [22].", "startOffset": 321, "endOffset": 325}, {"referenceID": 22, "context": "introduced the Long-Short-Term Memory cell [23], which through a more controlled flow of information can decide what information to keep and what to discard.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "The Gated Recurrent Unit is a simplified implementation of the Long-Short-Term Memory architecture that achieves much of the same effect at a reduced computational cost [24].", "startOffset": 169, "endOffset": 173}, {"referenceID": 15, "context": "A task which has a clear endpoint at step T is referred to as an episodic task [16], where T corresponds to the length of the episode.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "If they are generated by the agent itself the learning is referred to as on-policy, and if they are generated by some other means the learning is referred to as off-policy [16].", "startOffset": 172, "endOffset": 176}, {"referenceID": 19, "context": "5 million structures from ChEMBL [20] where the molecules were restrained to containing between 10 and 50 heavy atoms and elements \u2208 {H,B,C,N,O, F, Si, P, S, Cl, Br, I}.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "The model was trained with stochastic gradient descent for 50 000 steps using a batch size of 128, utilizing the Adam optimizer [27] (\u03b21 = 0.", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "The dopamine type 2 receptor DRD2 was chosen as the target, and corresponding bioactivity data was extracted from ExCAPE-DB [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "The Jaccard [30] index, for binary vectors also known as the Tanimoto similarity, based on the RDKit implementation of binary Extended Connectivity Molecular Fingerprints with a diameter of 6 (ECFP6 [31]) was used as a similarity measure and the actives were clustered using the Butina clustering algorithm [32] in RDKit with a clustering cutoff of 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "The Jaccard [30] index, for binary vectors also known as the Tanimoto similarity, based on the RDKit implementation of binary Extended Connectivity Molecular Fingerprints with a diameter of 6 (ECFP6 [31]) was used as a similarity measure and the actives were clustered using the Butina clustering algorithm [32] in RDKit with a clustering cutoff of 0.", "startOffset": 199, "endOffset": 203}, {"referenceID": 28, "context": "The Jaccard [30] index, for binary vectors also known as the Tanimoto similarity, based on the RDKit implementation of binary Extended Connectivity Molecular Fingerprints with a diameter of 6 (ECFP6 [31]) was used as a similarity measure and the actives were clustered using the Butina clustering algorithm [32] in RDKit with a clustering cutoff of 0.", "startOffset": 307, "endOffset": 311}, {"referenceID": 29, "context": "A support vector machine (SVM) classifier with a Gaussian kernel was built in Sci-Kit Learn [33] on the training set as a predictive model for DRD2 activity.", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "index [30] Ji,j of the RDKit implementation of FCFP4 [31] fingerprints was used as a similarity measure between molecules i and j.", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "index [30] Ji,j of the RDKit implementation of FCFP4 [31] fingerprints was used as a similarity measure between molecules i and j.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "This definition means that an increase in similarity is only rewarded up to the point of k \u2208 [0, 1], as well as scaling the reward from \u22121 (no overlap in the fingerprints between query and generated structure) to 1 (at least k degree of overlap).", "startOffset": 93, "endOffset": 99}, {"referenceID": 30, "context": "Other types of similarity measures such as pharmacophoric fingerprints [34], Tversky substructure similarity [35], or presence/absence of certain pharmacophores could also be explored.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "Other types of similarity measures such as pharmacophoric fingerprints [34], Tversky substructure similarity [35], or presence/absence of certain pharmacophores could also be explored.", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "[13]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Ideally, any predictive model to be used in conjunction with the generative model should cover a broad chemical space within its domain of applicability, since it initially has to assess representative structures of the dataset used to build the Prior [13].", "startOffset": 252, "endOffset": 256}, {"referenceID": 12, "context": "In this study we extend upon previous work [13\u201315, 17] by introducing a reinforcement learning method which can be used to tune the RNN to generate structures with certain desirable properties through augmented episodic likelihood.", "startOffset": 43, "endOffset": 54}, {"referenceID": 13, "context": "In this study we extend upon previous work [13\u201315, 17] by introducing a reinforcement learning method which can be used to tune the RNN to generate structures with certain desirable properties through augmented episodic likelihood.", "startOffset": 43, "endOffset": 54}, {"referenceID": 14, "context": "In this study we extend upon previous work [13\u201315, 17] by introducing a reinforcement learning method which can be used to tune the RNN to generate structures with certain desirable properties through augmented episodic likelihood.", "startOffset": 43, "endOffset": 54}, {"referenceID": 16, "context": "In this study we extend upon previous work [13\u201315, 17] by introducing a reinforcement learning method which can be used to tune the RNN to generate structures with certain desirable properties through augmented episodic likelihood.", "startOffset": 43, "endOffset": 54}, {"referenceID": 32, "context": "study which explores how parameters such as training set size, model size, regularization [36, 37], and training time would influence the quality and variety of structures generated by the Prior would be interesting.", "startOffset": 90, "endOffset": 98}, {"referenceID": 33, "context": "study which explores how parameters such as training set size, model size, regularization [36, 37], and training time would influence the quality and variety of structures generated by the Prior would be interesting.", "startOffset": 90, "endOffset": 98}, {"referenceID": 34, "context": "Another interesting avenue for future research might be that of token embeddings [38].", "startOffset": 81, "endOffset": 85}], "year": 2017, "abstractText": "This work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties. We demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target. As a proof of principle, the model is first trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique that could be used for scaffold hopping or library expansion starting from a single molecule. Finally, when tuning the model towards generating compounds predicted to be active against the dopamine receptor D2, the model generates structures of which more than 95% are predicted to be active, including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model.", "creator": "LaTeX with hyperref package"}}}