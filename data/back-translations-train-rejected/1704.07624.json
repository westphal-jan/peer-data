{"id": "1704.07624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia using Character-level Classification", "abstract": "We propose a simple, yet effective, approach towards inducing multilingual taxonomies from Wikipedia. Given an English taxonomy, our approach leverages the interlanguage links of Wikipedia followed by character-level classifiers to induce high-precision, high-coverage taxonomies in other languages. Through experiments, we demonstrate that our approach significantly outperforms the state-of-the-art, heuristics-heavy approaches for six languages. As a consequence of our work, we release presumably the largest and the most accurate multilingual taxonomic resource spanning over 280 languages.", "histories": [["v1", "Tue, 25 Apr 2017 10:45:43 GMT  (586kb,D)", "https://arxiv.org/abs/1704.07624v1", null], ["v2", "Tue, 12 Sep 2017 09:23:40 GMT  (1013kb,D)", "http://arxiv.org/abs/1704.07624v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["amit gupta", "r\\'emi lebret", "hamza harkous", "karl aberer"], "accepted": false, "id": "1704.07624"}, "pdf": {"name": "1704.07624.pdf", "metadata": {"source": "META", "title": "280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification", "authors": ["Amit Gupta", "Hamza Harkous", "Karl Aberer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The idea behind it is that it is a matter of a way in which people place themselves and the world in the center, that they place in the center. (...) The idea behind it is that people who are able to change the world, to change the world, to change the world, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change. (...), to change, to change, to change, to change, to change, to change, to change, to change, to change, to change. \""}, {"heading": "2 Taxonomy Induction", "text": "Background: We start with a description of the various components of Wikipedia that will help us present the rest of this work: \u2022 A Wikipedia page describes a single entity or concept. Examples of pages are Johnny Depp, Person or Country. Currently, Wikipedia contains more than 44 million pages spanning more than 280 different languages (Wikipedia 2017). \u2022 A Wikipedia category groups related pages and other categories into broader categories. For example, the category American actors groups pages for American actors, such as Johnny Depp, as well as other categories, such as American child actors. The directed graph, which is formed by pages and categories as nodes, and which groupings are linked as edges, is known as the Wikpedia Category Network (WCN). Another WCN exists for each of Wikipedia's 280 languages. WCN edges tend to be loud, and are usually a mixture of different languages (e.g. Johnny Depp \u2192 American actors) and non-is edges."}, {"heading": "2.1 Projection Phase", "text": "Let Te be the specified English taxonomy. Let Gf be the WCN and Tf be the source taxonomy (initially blank) for the target language f (e.g. French). For a node nf-Gf with the English equivalent ne, for which there is no hypernym in Tf yet, we perform the following steps: i) Enter the set Ae of all ancestral nodes from ne in Te up to a fixed height k14. ii) Enter the set Af of equivalents for nodes in Ae in the target language f.iii) Find the shortest path between nf and each node in Af up to a fixed height k25; iv) Add all edges on the shortest path to the source taxonomy Tf. If no English equivalent exists, the node nf in the target language f will be ignored. Figure 1 shows an example of the projection phase with French as the target language \u2192 For the August node will be the English equivalent in the target language."}, {"heading": "2.2 Training Phase", "text": "Until now, we constructed an initial taxonomy for the target language simply by projecting the English taxonomy using the interlingual links. However, the resulting taxonomy suffers from low coverage as nodes that do not have an English equivalent are ignored. For example, only 44.8% of entities and 40.5% of categories from the French WCN have a hypernym in the projected taxonomy. To increase the range, we train two different binary classifiers to classify the remaining target WCN4In our experiments, k1 = 14 Sufficed as Heads Taxonomy had a maximum height of 14 and no Cycle.5k2 is set to 3 to maintain high precision. Edges in is-a (positive) or not-is-a (negative). The first classification is for Entity \u2192 Category Edges and the other category."}, {"heading": "2.3 Induction Phase", "text": "In the last step of our approach, we discover taxonomic edges for nodes that are not yet covered in the projected taxonomy (Tf). To this end, we first set the weights of Entity \u2192 Category and Category \u2192 Category edges in the target WCN as the probabilities6Entity \u2192 Entity and Category \u2192 Entity edges are not present in the WCN.7n-values = {2,3,4,5,6} worked best in our experiments. 8k is set to 1 if specified otherwise.origing from nf to any node in Tf, where the probability of a path is \u2192 Entity edges is not present in the WCN.7n-values = {2,3,4,5,6} were best in our experiments."}, {"heading": "3.1 Edge-level Evaluation", "text": "In fact, most of them are able to move to another world, where they can move to another world, where they can find their way to another world."}, {"heading": "3.2 Path-level Evaluation", "text": "In this section, we compare Char TFIDF versus MultiWiBi with a variety of PP quality measures. The path-based assessment of taxonomies was proposed by Gupta et al. (2016), who showed that good precision at the edge level does not directly lead to good precision at the path level for taxonomies. They suggested the average length of the correct path prefix (CPP), i.e., the maximum correct prefix of a generalization path, as an alternative measure of the quality of a taxonomy. Intuitively, it aims to capture the average number of emerging generalizations that can be taken until the first false hypernym is encountered. Following this metric, we randomly run sample paths starting from 25 entities and 25 categories from the taxonomies, and annotate the first false hypernym in the upward direction."}, {"heading": "4.1 Word vs. Character Models", "text": "To compare word and character models, we first report on the validation accuracy of Word-TFIDF and Char-TFIDF models in Figure 2, which was determined during the training phase 13 (see 13 validation phase). Char-TFIDF models significantly outperform Word-TFIDF models and achieve higher validation accuracies in six different languages. The improvements are usually higher for languages with non-Latin fonts. This is partly due to the error-prone nature of the white-packebased tokenization for such languages. For example, the word kenizer for Hindi splits words on many accented characters in addition to word boundaries, resulting in erroneous features and poor performance. In contrast, character models are better able to handle languages with arbitrary fonts as they do not need to perform text kenization."}, {"heading": "4.2 False Positives vs. False Negatives", "text": "In Figure 3, we show the confusion matrices of the Word TFIDF and the Char TFIDF model calculated using the validation set for French categories. While both models generally perform well, Char TFIDF outperforms the Word TFIDF and produces fewer false positives than false negatives. We noticed similar patterns in most languages for both units and categories. To demonstrate this, we hypothesize in Tables 5 and 6 that the superior performance of Char TFIDF is due to the fact that the characteristics of the n-gram contain the morphological properties calculated at the subordinate level, as well as word boundaries ignored by the word-based characteristics. To demonstrate this, in Tables 5 and 6, we show the uppermost Word TFIDF and Char TFIDF characteristics of a non-is-one and one-is-one edge. These edges are misclassified by Word TFIDF models, but may be limited to FIT.DF characteristics by FIT.DF characteristics, while FITDF characteristics may be FIT.FIT.DF characteristics are FIT.FIT.FIDF characteristics correctly limited by FIT.FIT.DF."}, {"heading": "4.3 Precision vs. Branching Factor", "text": "One of the most important structural properties of a taxonomy is the branching factor, which is defined as the average output of the nodes in the taxonomy. Taxonomies with higher branching factors are desirable because they are better able to take into account multiple facets of a concept or entity (e.g. Bill Gates is both a philanthropist and an entrepreneur). However, there is usually a trade-off between branching factor and precision in automatically induced taxonomies (Velardi, Faralli and Navigli 2013). Higher branching factors typically lead to a reduction in precision due to erroneous edges, with lower values added to the taxonomy. Prioritization of precision by branching factor or vice versa is usually determined by the specific application at hand."}, {"heading": "5 Related Work and Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z.G. Ives"], "venue": "The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007.,", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Ontology learning from text: A survey of methods", "author": ["C. Biemann"], "venue": "LDV Forum 20(2):75\u201393.", "citeRegEx": "Biemann,? 2005", "shortCiteRegEx": "Biemann", "year": 2005}, {"title": "MENTA: inducing multilingual taxonomies from wikipedia", "author": ["G. de Melo", "G. Weikum"], "venue": "In Proceedings of the 19th ACM Conference on Information and Knowledge Management,", "citeRegEx": "Melo and Weikum,? \\Q2010\\E", "shortCiteRegEx": "Melo and Weikum", "year": 2010}, {"title": "Multiwibi: The multilingual wikipedia bitaxonomy project", "author": ["T. Flati", "D. Vannella", "T. Pasini", "R. Navigli"], "venue": "Artif. Intell. 241:66\u2013102.", "citeRegEx": "Flati et al\\.,? 2016", "shortCiteRegEx": "Flati et al\\.", "year": 2016}, {"title": "Bag of tricks for efficient text classification", "author": ["E. Grave", "T. Mikolov", "A. Joulin", "P. Bojanowski"], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short Papers, 427\u2013431.", "citeRegEx": "Grave et al\\.,? 2017", "shortCiteRegEx": "Grave et al\\.", "year": 2017}, {"title": "Revisiting taxonomy induction over wikipedia", "author": ["A. Gupta", "F. Piccinno", "M. Kozhevnikov", "M. Pasca", "D. Pighin"], "venue": "COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka,", "citeRegEx": "Gupta et al\\.,? 2016", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "YAGO2: A spatially and temporally enhanced knowledge base from wikipedia", "author": ["J. Hoffart", "F.M. Suchanek", "K. Berberich", "G. Weikum"], "venue": "Artif. Intell. 194:28\u201361.", "citeRegEx": "Hoffart et al\\.,? 2013", "shortCiteRegEx": "Hoffart et al\\.", "year": 2013}, {"title": "Collaboratively built semi-structured content and artificial intelligence: The story so far", "author": ["E.H. Hovy", "R. Navigli", "S.P. Ponzetto"], "venue": "Artif. Intell. 194:2\u201327.", "citeRegEx": "Hovy et al\\.,? 2013", "shortCiteRegEx": "Hovy et al\\.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, 1746\u20131751.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Dbpedia - A large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web 6(2):167\u2013195", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. van Kleef", "S. Auer", "C. Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2015}, {"title": "YAGO3: A knowledge base from multilingual wikipedias", "author": ["F. Mahdisoltani", "J. Biega", "F.M. Suchanek"], "venue": "CIDR 2015, Seventh Biennial Conference on Innovative Data Systems Research, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings.", "citeRegEx": "Mahdisoltani et al\\.,? 2015", "shortCiteRegEx": "Mahdisoltani et al\\.", "year": 2015}, {"title": "WORDNET: A lexical database for english", "author": ["G.A. Miller"], "venue": "Human Language Technology, Proceedings of a Workshop held at Plainsboro, New Jerey, USA, March 8-11, 1994.", "citeRegEx": "Miller,? 1994", "shortCiteRegEx": "Miller", "year": 1994}, {"title": "Wikinet: A very large scale multi-lingual concept network", "author": ["V. Nastase", "M. Strube", "B. Boerschinger", "C. Zirn", "A. Elghafari"], "venue": "Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May 2010, Valletta, Malta.", "citeRegEx": "Nastase et al\\.,? 2010", "shortCiteRegEx": "Nastase et al\\.", "year": 2010}, {"title": "Wikitaxonomy: A large scale knowledge resource", "author": ["S.P. Ponzetto", "M. Strube"], "venue": "ECAI 2008 - 18th European Conference on Artificial Intelligence, Patras, Greece, July 21-25, 2008, Proceedings, 751\u2013752.", "citeRegEx": "Ponzetto and Strube,? 2008", "shortCiteRegEx": "Ponzetto and Strube", "year": 2008}, {"title": "Wikipedia: The free encyclopedia", "author": ["M. Remy"], "venue": "Online Information Review 26(6):434.", "citeRegEx": "Remy,? 2002", "shortCiteRegEx": "Remy", "year": 2002}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007, 697\u2013706.", "citeRegEx": "Suchanek et al\\.,? 2007", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Ontolearn reloaded: A graph-based algorithm for taxonomy induction", "author": ["P. Velardi", "S. Faralli", "R. Navigli"], "venue": "Computational Linguistics 39(3):665\u2013707.", "citeRegEx": "Velardi et al\\.,? 2013", "shortCiteRegEx": "Velardi et al\\.", "year": 2013}, {"title": "List of wikipedias \u2014 wikipedia, the free encyclopedia", "author": ["Wikipedia."], "venue": "https://en.wikipedia.org/ w/index.php?title=List\\_of\\_Wikipedias\\ &oldid773693902. [Online; accessed 9-April-2017].", "citeRegEx": "Wikipedia.,? 2017", "shortCiteRegEx": "Wikipedia.", "year": 2017}, {"title": "Characterlevel convolutional networks for text classification", "author": ["X. Zhang", "J.J. Zhao", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": ", a collection of is-a1 edges) has proved to be beneficial in an array of Natural Language Processing (NLP) tasks, including inference, textual entailment, question answering, and information extraction (Biemann 2005).", "startOffset": 203, "endOffset": 217}, {"referenceID": 13, "context": "This has led to multiple large-scale manual efforts towards taxonomy induction such as WordNet (Miller 1994).", "startOffset": 95, "endOffset": 108}, {"referenceID": 16, "context": "Wikipedia, the largest publicly-available source of multilingual, semi-structured content (Remy 2002), has served as a key resource for automated knowledge acquisition.", "startOffset": 90, "endOffset": 101}, {"referenceID": 15, "context": "This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al.", "startOffset": 27, "endOffset": 53}, {"referenceID": 14, "context": "This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al. 2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 7, "context": "2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013), DBPedia (Auer et al.", "startOffset": 12, "endOffset": 69}, {"referenceID": 0, "context": "2013), DBPedia (Auer et al. 2007), and Heads Taxonomy (Gupta et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 5, "context": "2007), and Heads Taxonomy (Gupta et al. 2016).", "startOffset": 26, "endOffset": 45}, {"referenceID": 3, "context": "The most recent approach to multilingual taxonomy induction from Wikipedia is the Multilingual Wikipedia Bitaxonomy Project or MultiWiBi (Flati et al. 2016).", "startOffset": 137, "endOffset": 156}, {"referenceID": 5, "context": "Given (1) a unified taxonomy of pages and categories in English (we use Heads Taxonomy publicly released by Gupta et al. (2016)3), (2) the interlanguage links, and (3) a target language, our approach aims to induce a unified taxonomy of pages and categories for the target language.", "startOffset": 108, "endOffset": 128}, {"referenceID": 4, "context": "Experiments show that fastText typically produces results on par with sophisticated deep learning classifiers (Grave et al. 2017).", "startOffset": 110, "endOffset": 129}, {"referenceID": 9, "context": "iv) Convolutional Neural Network (CNN): We use a single-layer CNN model trained on top of word vectors as proposed by Kim (2014). We also experiment with a character version of this model, in which instead of words, vectors are computed using characters and fed into the CNN.", "startOffset": 118, "endOffset": 129}, {"referenceID": 6, "context": "v) Long Short Term Memory Network (LSTM): We experiment with both word-level and character-level versions of LSTM (Hochreiter and Schmidhuber 1997).", "startOffset": 114, "endOffset": 147}, {"referenceID": 3, "context": "\u2022 MultiWiBi has been shown to outperform all other previous approaches including YAGO3 and MENTA (Flati et al. 2016).", "startOffset": 97, "endOffset": 116}, {"referenceID": 5, "context": "We faced a tough choice of selecting a Wikipedia snapshot since MultiWiBi, to which we compare, is constructed using a 2012 snapshot whereas Gupta et al. (2016), on which we build, uses a 2015 snapshot.", "startOffset": 141, "endOffset": 161}, {"referenceID": 5, "context": "We faced a tough choice of selecting a Wikipedia snapshot since MultiWiBi, to which we compare, is constructed using a 2012 snapshot whereas Gupta et al. (2016), on which we build, uses a 2015 snapshot. Additionally, the code, executable, and gold standards used by MultiWiBi were not available upon request. Therefore, to advance the field and produce a more recent resource, we decided to use a 2015 snapshot of Wikipedia, especially given that Gupta et al. (2016) point out that there is no evidence that taxonomy induction is easier on recent editions of Wikipedia.", "startOffset": 141, "endOffset": 467}, {"referenceID": 3, "context": "MENTA and MultiWiBi results as reported by Flati et al. (2016). The top 3 results are shown in bold, and the best is also underlined.", "startOffset": 43, "endOffset": 63}, {"referenceID": 5, "context": "Path-based evaluation of taxonomies was proposed by Gupta et al. (2016), who demonstrated that good edge-level precision may not directly translate to good path-level precision for taxonomies.", "startOffset": 52, "endOffset": 72}, {"referenceID": 5, "context": "This is also evidenced by previous approaches that utilize multiple hand-crafted features based on such morphological information (Suchanek, Kasneci, and Weikum 2007; Gupta et al. 2016).", "startOffset": 130, "endOffset": 185}, {"referenceID": 15, "context": "WikiTaxonomy, one of the first attempts to taxonomize Wikipedia, labels English WCN edges as isa or not-is-a using a cascade of heuristics based on handcrafted features (Ponzetto and Strube 2008).", "startOffset": 169, "endOffset": 195}, {"referenceID": 7, "context": "YAGO induces a taxonomy by linking Wikipedia categories to WordNet synsets using a set of simple heuristics (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013).", "startOffset": 108, "endOffset": 165}, {"referenceID": 0, "context": "DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015).", "startOffset": 184, "endOffset": 223}, {"referenceID": 11, "context": "DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015).", "startOffset": 184, "endOffset": 223}, {"referenceID": 0, "context": "DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015). More recently, Gupta et al. (2016) induce a unified taxonomy of entities and categories from English WCN using a novel set of high-precision heuristics that classify WCN edges into is-a and not-is-a.", "startOffset": 185, "endOffset": 260}, {"referenceID": 3, "context": "The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016).", "startOffset": 80, "endOffset": 99}, {"referenceID": 2, "context": "MENTA, a large-scale multilingual knowledge base, is induced by linking WordNet with WCN of different languages into a unified taxonomy (de Melo and Weikum 2010). The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016). MultiWiBi first simultaneously induces two separate taxonomies for English, one for pages and one for categories. To this end, it exploits the idea that information contained in pages are useful for taxonomy induction over categories and vice-versa. To induce taxonomies for other languages, MultiWiBi employs a set of complex heuristics, which utilize hand-crafted features (such as textual and network topology features) and a probabilistic translation table constructed using the interlanguage links. Our approach borrows inspiration from many of the aforementioned approaches. First, similar to WikiTaxonomy and Gupta et al. (2016), our approach also classifies WCN edges into is-a or not-is-a.", "startOffset": 140, "endOffset": 900}, {"referenceID": 2, "context": "MENTA, a large-scale multilingual knowledge base, is induced by linking WordNet with WCN of different languages into a unified taxonomy (de Melo and Weikum 2010). The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016). MultiWiBi first simultaneously induces two separate taxonomies for English, one for pages and one for categories. To this end, it exploits the idea that information contained in pages are useful for taxonomy induction over categories and vice-versa. To induce taxonomies for other languages, MultiWiBi employs a set of complex heuristics, which utilize hand-crafted features (such as textual and network topology features) and a probabilistic translation table constructed using the interlanguage links. Our approach borrows inspiration from many of the aforementioned approaches. First, similar to WikiTaxonomy and Gupta et al. (2016), our approach also classifies WCN edges into is-a or not-is-a. Second, similar to MultiWiBi, our approach also projects an English taxonomy into other languages using the interlanguage links. However, unlike these approaches, our approach does not employ any heuristics or hand-crafted features. Instead, it uses text classifiers trained on an automatically constructed dataset to assign edge weights to WCN edges. Taxonomic edges are discovered by running optimal path search over the WCN in a fully-automated and language-independent fashion. Our experiments show that taxonomies derived using our approach significantly outperform the state-of-the-art taxonomies, derived by MultiWiBi using more complex heuristics. We hypothesize that it is because our model primarily uses categories as hypernyms, whereas MultiWiBi first discovers hypernym lemmas for entities using potentially noisy textual features derived from unstructured text. Categories have redundant patterns, which can be effectively exploited using simpler models. This has also been shown by Gupta et al. (2016), who use simple high-precision heuristics based on the lexical head of categories to achieve significant improvements over MultiWiBi for English.", "startOffset": 140, "endOffset": 1980}, {"referenceID": 2, "context": "MENTA, a large-scale multilingual knowledge base, is induced by linking WordNet with WCN of different languages into a unified taxonomy (de Melo and Weikum 2010). The most recent and the most notable effort towards this direction is MultiWiBi (Flati et al. 2016). MultiWiBi first simultaneously induces two separate taxonomies for English, one for pages and one for categories. To this end, it exploits the idea that information contained in pages are useful for taxonomy induction over categories and vice-versa. To induce taxonomies for other languages, MultiWiBi employs a set of complex heuristics, which utilize hand-crafted features (such as textual and network topology features) and a probabilistic translation table constructed using the interlanguage links. Our approach borrows inspiration from many of the aforementioned approaches. First, similar to WikiTaxonomy and Gupta et al. (2016), our approach also classifies WCN edges into is-a or not-is-a. Second, similar to MultiWiBi, our approach also projects an English taxonomy into other languages using the interlanguage links. However, unlike these approaches, our approach does not employ any heuristics or hand-crafted features. Instead, it uses text classifiers trained on an automatically constructed dataset to assign edge weights to WCN edges. Taxonomic edges are discovered by running optimal path search over the WCN in a fully-automated and language-independent fashion. Our experiments show that taxonomies derived using our approach significantly outperform the state-of-the-art taxonomies, derived by MultiWiBi using more complex heuristics. We hypothesize that it is because our model primarily uses categories as hypernyms, whereas MultiWiBi first discovers hypernym lemmas for entities using potentially noisy textual features derived from unstructured text. Categories have redundant patterns, which can be effectively exploited using simpler models. This has also been shown by Gupta et al. (2016), who use simple high-precision heuristics based on the lexical head of categories to achieve significant improvements over MultiWiBi for English. Additionally, for taxonomy induction in other languages, MultiWiBi uses a probabilistic translation table, which is likely to introduce further noise. The high-precision heuristics of Gupta et al. (2016) are not easily extensible to languages other than English, due to the requirement of a syntactic parser for lexical head detection.", "startOffset": 140, "endOffset": 2330}], "year": 2017, "abstractText": "We propose a novel fully-automated approach towards inducing multilingual taxonomies from Wikipedia. Given an English taxonomy, our approach first leverages the interlanguage links of Wikipedia to automatically construct training datasets for the is-a relation in the target language. Character-level classifiers are trained on the constructed datasets, and used in an optimal path discovery framework to induce high-precision, high-coverage taxonomies in other languages. Through experiments, we demonstrate that our approach significantly outperforms the state-of-the-art, heuristics-heavy approaches for six languages. As a consequence of our work, we release presumably the largest and the most accurate multilingual taxonomic resource spanning over 280 languages.", "creator": "TeX"}}}