{"id": "1704.07657", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Decision Stream: Cultivating Deep Decision Trees", "abstract": "Various modifications of decision trees have been extensively used during the past years due to their high efficiency and interpretability. Selection of relevant features for spitting the tree nodes is a key property of their architecture, at the same time being their major shortcoming: the recursive nodes partitioning leads to geometric reduction of data quantity in the leaf nodes, which causes an excessive model complexity and data overfitting. In this paper, we present a novel architecture - a Decision Stream, - aimed to overcome this problem. Instead of building an acyclic tree structure during the training process, we propose merging nodes from different branches based on their similarity that is estimated with two-sample test statistics. To evaluate the proposed solution, we test it on several common machine learning problems~--- credit scoring, twitter sentiment analysis, aircraft flight control, MNIST and CIFAR image classification, synthetic data classification and regression. Our experimental results reveal that the proposed approach significantly outperforms the standard decision tree method on both regression and classification tasks, yielding a prediction error decrease up to 35%.", "histories": [["v1", "Tue, 25 Apr 2017 12:20:33 GMT  (659kb)", "http://arxiv.org/abs/1704.07657v1", null], ["v2", "Wed, 26 Apr 2017 10:22:29 GMT  (659kb)", "http://arxiv.org/abs/1704.07657v2", null], ["v3", "Sun, 3 Sep 2017 18:01:09 GMT  (332kb,D)", "http://arxiv.org/abs/1704.07657v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dmitry ignatov", "andrey ignatov"], "accepted": false, "id": "1704.07657"}, "pdf": {"name": "1704.07657.pdf", "metadata": {"source": "CRF", "title": "Decision Stream: Cultivating Deep Decision Trees", "authors": ["Dmitry Ignatov", "Andrey Ignatov"], "emails": ["ignatov.dmitry@huawei.com,", "andrey.ignatoff@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.07 657v 1 [cs.L G] 25 Apr 201 7Keywords: decision tree \u00b7 data fusion \u00b7 test statistics with two samples \u00b7 distributed machine learning"}, {"heading": "1 Introduction", "text": "With the recent growth in the amount of data available for analysis and exploration, there is an inevitable need for comprehensive and automated methods for intellectual data processing. Decision Tree (DT) is one of the most popular techniques in this field, and due to its robustness and efficiency, this predictive model has become a standard tool for many machine learning processes and big data problems. The idea behind this method is to separate a complex decision rule into a combination of several primitive rules that leads to another crucial characteristic - DT can be easily interpreted by humans compared to many other machine learning techniques. DT construction is done by recursive data partitioning. At each stage, the best slit criterion is determined, and the data from the current node is divided according to selected criteria."}, {"heading": "2 Related Work", "text": "The proposed methods include the Iterative Dichotomiser 3 and its successor - C4.5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18.19.23.28,31]. Despite the significant difference in training procedures, they tend to show similar performance in many real regression and classification tasks [5,8,11,16,24,25,30]. The majority of these algorithms consider only node partitioning for the construction of decision trees or the use of node merging as an auxiliary method that has no significant impact on the tree structure."}, {"heading": "3 Decision Stream", "text": "In this section we describe the proposed Decision Stream Algorithm. The main concept of this method is to merge similar nodes after each splitting iteration. The similarity is estimated using test statistics with two samples, which are used to compare the distribution of labels in each pair of nodes. Nodes are merged when the difference is statistically insignificant. This method eliminates the classic problem of decision trees - extreme reduction of the amount of data in the leaf nodes and creates a more general structure - a directed acyclic graph (Fig. 1). A more detailed explanation of the algorithm is given below."}, {"heading": "3.1 Node Merging with Two-Sample Test Statistics", "text": "The overview of the merge operation is illustrated in Fig. 2. After the classical branching of the decision tree, the merger algorithm takes as input all the leaf nodes presented in the tree at this stage (Fig. 2 (a)), and fuses statistically similar nodes (Fig. 2 (b-c)) using an input parameter - significance threshold Plim. Since the nodes are merged based on the similarity of their label distributions, the merger process is similar to the statistically based label clustering.The merging algorithm 1 consists of an outer and inner loop. In the outer loop, the leaves are sorted in ascending order according to the number of associated samples. The inner loop consists of the following three steps: 1. Leaf node is fetched from the head of the sorted collection. 2. For each (node, node) pair, we calculate the similarity of these two parent nodes, and then associate them with the highest signatures."}, {"heading": "3.2 Decision Stream Training", "text": "The entire DS training procedure is described in Algorithm 2, where each learning iteration consists of two steps. In the first step, DS grows using the classic branching method of the decision tree - new knots are created by splitting all current non-terminal leaves [3,11,19]. In the second step, the leaves are merged using the procedure described in Algorithm 1. A leaf is marked as a terminal if it cannot be split into statistically different child knots. To estimate the accuracy of the prediction, we use a cross-knot-Gini impurity measure calculated for K-leaf knots and J-classes: IG (knot) = K-knot-knot-knot of the generated DS. To estimate the accuracy of the prediction, we use a cross-knot-Gini impurity measure calculated for K-leaf knots and J-classes: IG (knot) = K-knot-knot = 1J-knot-knot."}, {"heading": "13 while A 6= \u2205", "text": "14 A \u2190 A \u2032 15 while the size of A is decreased 16 Return AAlgorithm 2: Decision flow training data: A set of gin = {s0, s1,..., sn} of marked training data samples Input: Significance threshold Plim Result: Decision flow 1 root node \u2190 Node (gin) 2 A \u2190 {root node} 3 do 4 A \"\u2190 5 for each node. A do 6 if node terminal then 7 A.\" add (node) 8 else9 Children \u2190 bestSplit (node, plim) algorithm 3 or 4 10 if # children \u2264 1 then 11 nodes as terminal 12 A. \"add (node) 13 otherwise 14 A \u2032.addAll (children) 15 A \u2190 merge (A \u2032, plim) algorithm 1 16, while non-terminal node \u0432A AND IG (A) are decreased 17 return rootNodewo N and nk the number of leaves in all samples."}, {"heading": "3.3 Splitting/Merging Criteria", "text": "The null hypothesis is rejected at the significance level Plim, and in case of rejection we consider that the nodes are statistically different, the similarity is estimated by test statistics with two samples. We use Z-Test / Student's t-Test for labels with presumably normal distribution. The choice between the tests is made according to rule [29]: Z-Test is used if the size of the two samples is greater than 30, Student's t-Test - otherwise. For labels with nonnormal distribution we use Kolmogorov-Smirnov / Mann-Whitney U-Tests: The first test is used if the size of the samples is greater than 2, the second - otherwise. We prefer Kolmogorov-Smirnov over Mann-Whitney U-Test because it is more sensitive to variations in the position and shape of the empirical sample."}, {"heading": "3.4 Node Splitting for Non-Distributed Data", "text": "For non-distributed datasets, the division is done by algorithm 3, which takes the significance threshold Plim and a specific node as input. First, for each unique value of each attribute, the binary division of the data within the node is generated. Then, for each split, the similarity function Sst is calculated and the one with the least significance of the similarity is selected. If this significance is lower than the input threshold Plim, the selected best split is returned, otherwise - splitting is rejected and the node becomes terminal. Although this method is relatively expensive in terms of computation, it offers the best split quality and is more reasonable for compact datases.Algorithm 3: Spitting the Non-Distributed DataData: A Node of Decision Stream Input: Signification threshold Plim Result: Child Nodes1 Splits - all binary splits of data within the Kchildnotes 2 (0, 0 childchild) childchild (1 childj) child (1 child) \u2205 1, child 1"}, {"heading": "3.5 Node Splitting for Distributed Data", "text": "Using the aforementioned large-format dataset algorithm, it is unfeasible in most cases, so in this thesis we propose a different type of splitting selection for big data solutions. Instead of a greedy search, we perform data splitting based on the characteristic that is most correlated within a particular node. (Another difference of the proposed method is that it attempts to produce multiple sheets for each node, as shown in Figure 3, as far as the large number of samples presupposes the robustness of such a splitting. Algorithm 4 demonstrates the body of the method. The procedure begins with a functional corridor that selects the characteristic that is most correlated to the label. The attribute obtained is then used to split the samples in the current node. Samples are split by Algorithm 4: Splitting the distributed datasets: A node of Decision Stream Input: Significance Threshold Plid Feature Child Feature 1 is unenforceable."}, {"heading": "4 Experiments", "text": "In this section, we describe the experiments performed to evaluate the performance of the proposed Decision Stream Algorithm. The solution was tested on five common machine learning problems and on large-scale synthetic classification / regression data. To compare the results obtained, we applied the Decision Tree algorithm to the same training and test data to obtain baseline accuracy. We use Decision Trees3 \"s Scikit learning implementation, which is based on the CART algorithm. Detailed classification and regression results for these methods are listed below."}, {"heading": "4.1 Datasets", "text": "The forward-looking methods were tested using the following data sets: \u2022 Credit rating 4 - Classification problem, 2 classes, 10 features, 100K training and 20K test samples. \u2022 Twitter sentiment analysis5 - Classification problem, 3 classes (positive, negative, neutral), 500 features, 6500 training and 824 test samples. \u2022 Twitter sentiment analysis5 - Classification problem, 3 classes (positive, negative, neutral), 500 features, 6500 training and 824 test samples. \u2022 Twitter sentiment analysis6 - Regression problem, 40 features, 7154 training and 6596 test samples. \u2022 http: / / scikit-learn.org / 4 https: / www.kaggle.com / c / GiveMeSomeCredit / data 5 http: / / alt.qcri.org / semeval2015 / task10 / 6 http: / / www.delectionelectionscale.fc.up.pt / ~ lgo / regression / DataSets.html / data categories: / SelectionSelectionSelectionSelection / Resourcategories / 2015 / SelectionSelectionSelectionSelection.me."}, {"heading": "4.2 Tuning the Significance Threshold", "text": "The significance threshold is the key parameter of our algorithm, and in the first experiment our goal was to estimate the optimal value for each problem. Plim level was set as follows: for each data set we varied it from 10 \u2212 4 to 0.5, and for each value we estimated the accuracy of DS on the validation set. For synthetic data, the similarity of labels was estimated by unpaired Z tests with two samples and student t tests, for all other data sets - by Kolmogorov-Smirnov and Mann-Whitney U non-parametric tests. For classification problems we use the standard accuracy metric, for regression tasks - the weighted absolute percentage error: LM (X, Y) = 100%."}, {"heading": "4.3 Classification and Regression Results for Non-Distributed Data", "text": "This year, it has never been as good as it has been this year."}, {"heading": "4.4 Classification and Regression Results for Large-Scale Data", "text": "The next set of experiments is performed using Apache Spark-based 10 distributed implementation of Decision Stream and Decision Tree algorithms. For the last set, an open source implementation of MLlib Machine Learning Library is used. To perform the distributed calculations, the models were executed on a computer cluster with 4 nodes, 50 GB of RAM and 12 cores per node. The algorithms were trained on synthetic data, the depth of which is automatically regulated by Spark Performance Testing Suite for classification and regression problems.Fig.7 shows the classification accuracy, the weighted absolute percentage error (equivalent. 4) and the training time for DT with a depth of 3 to 15 levels, and DS, the depth of which is automatically regulated. According to the results, decision trees trained with variance reduction metric and depth limitation of 5 levels show the best accuracy both in the classification and in the regression task, so the difference in the stretching algorithm and in the stretch set is used at the most significant point of the stretching algorithms at the preceding 48 times."}, {"heading": "5 Conclusion", "text": "In this paper, we introduced a novel decision tree-based algorithm - a decision stream that avoids the problems of data exhaustion and the formation of unrepresentative data samples in decision tree nodes by merging the leaves of the same and / or different layers of the model structure. By increasing the number of samples in each node and reducing the tree width, the proposed algorithm preserves statistically representative data and allows for an extremely deep graph architecture that can consist of hundreds of layers. The main parameter of the algorithm - significance threshold - determines the results of each split / merge operation and automatically defines the depth of the decision stream model. Experiments showed that the decision stream algorithm has a strong advantage over the standard method in both regression and classification tasks: not distributed for relatively small data sets, where a precise selection of the best data splits is crucial, and distributed where an equilibrium between accuracy and compression performance should be maintained."}], "references": [{"title": "Random forests and gradient boosting for wind energy prediction", "author": ["\u00c1. Alonso", "A. Torres", "J.R. Dorronsoro"], "venue": "HAIS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn. 24, 123\u2013140", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Wadsworth, Belmont", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1984}, {"title": "Investigating the use of gradient boosting machine, random forest and their ensemble to predict skin flavonoid content from berry physical-mechanical characteristics in wine grapes", "author": ["Brillante"], "venue": "Comput. Electron. Agric. 117, 186\u2013193", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning approach for distinction of ADHD and OSA", "author": ["K.C. Chu", "H.J. Huang", "Y.S. Huang"], "venue": "ASONAM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "RECPAM: a computer program for recursive partition amalgamation for censored survival data and other situations frequently occurring in biostatistics", "author": ["A. Ciampi", "J. Thiffault", "U. Sagman"], "venue": "Comput. Meth. Prog. Bio. 30, 283\u2013296", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Nonparametric statistics: a step-by-step approach", "author": ["G.W. Corder", "D.I. Foreman"], "venue": "Wiley, New York", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring firm performance using financial ratios: a decision tree approach", "author": ["D. Delen", "C. Kuzey", "A. Uyar"], "venue": "Expert Syst. Appl. 40, 3970\u20133983", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Random forests and stochastic gradient boosting for predicting tree canopy cover: comparing tuning processes and model performance", "author": ["E.A. Freeman", "G.G. Moisen", "J.W. Coulston", "B.T. Wilson"], "venue": "Can. J. For. Res. 46, 323\u2013339", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Ann. Stat. 29, 1189\u2013 1232", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "CART versus CHAID behavioral biometric parameter segmentation analysis", "author": ["I.R. Gl\u0103van", "D. Petcu", "E. Simion"], "venue": "SECITC", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Mach. Learn. 63, 3\u201342", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to rank with extremely randomized trees", "author": ["P. Geurts", "G. Louppe"], "venue": "JMLR 14, 49\u201361", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Random decision forests", "author": ["T.K. Ho"], "venue": "ICDAR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "A cross-platform evaluation of various decision tree algorithms for prognostic analysis of breast cancer data", "author": ["S. Jhajharia", "S. Verma", "R. Kumar"], "venue": "ICICT", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "An exploratory technique for investigating large quantities of categorical data", "author": ["G.V. Kass"], "venue": "Appl. Stat. 29, 119\u2013127", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1980}, {"title": "A hybrid classification algorithm by subspace partitioning through semi-supervised decision tree", "author": ["K. Kyoungok"], "venue": "Pattern Recogn. 60, 157\u2013163", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Fifty years of classification and regression trees", "author": ["Loh", "W.-Y."], "venue": "Intern. Stat. Review 82, 329\u2013348", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Split selection methods for classification trees", "author": ["Loh", "W.-Y.", "Shih", "Y.-S."], "venue": "Stat. Sin. 7, 815\u2013840", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Detection of independent associations in a large epidemiologic dataset: a comparison of random forests, boosted regression trees, conventional and penalized logistic regression for identifying factors associated with H1N1pdm influenza infections", "author": ["Y. Mansiaux", "F. Carrat"], "venue": "BMC Med. Res. Methodol. 14, 99", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Handbook of statistical analysis and data mining applications", "author": ["R. Nisbet", "J. Elder", "G. Miner"], "venue": "Academic Press, Canada", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "An outlook in some aspects of hybrid decision tree classification approach: a survey", "author": ["A. Panhalkar", "D. Doye"], "venue": "ICDECT", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine learning techniques for predicting hospital length of stay in Pennsylvania Federal and Specialty hospitals", "author": ["P.C. Pendharkar", "H. Khurana"], "venue": "IJACSA 11, 45\u201356", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparing decision tree algorithms to estimate intercity trip distribution", "author": ["C.S. Pitombo", "A.D. Souza", "A. Lindner"], "venue": "Transport. Res. C-EMER 77, 16\u201332", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Programs for machine learning", "author": ["Quinlan", "J.R.: C"], "venue": "Mach. Learn. 16,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Correlation based splitting criterion in multi-branch decision tree", "author": ["N. Salehi", "H. Yazdi", "H. Poostchi"], "venue": "Cent. Eur. J. Comp. Sci. 2, 205\u2013220", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "A decision tree based approach with sampling techniques to predict the survival status of poly-trauma patients", "author": ["J. Sanz", "J. Fernandez", "H. Bustince", "C. Gradin", "M. Fortun", "T. Belzunegui"], "venue": "IJCIS 10, 440\u2013455", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Basic statistical analysis", "author": ["R.C. Sprinthall"], "venue": "Pearson Education, Boston", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Using Kaplan-Meier analysis together with decision tree methods (C&RT, CHAID, QUEST, C4.5 and ID3) in determining recurrence-free survival of breast cancer patients", "author": ["M. Ture", "F. Tokatli", "I. Kurt"], "venue": "Expert Syst. Appl", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "A cost sensitive decision tree algorithm based on weighted class distribution with batch deleting attribute mechanism", "author": ["H. Zhao", "X. Li"], "venue": "Inform. Sciences 378, 303\u2013316", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 25, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 50, "endOffset": 53}, {"referenceID": 16, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 216, "endOffset": 232}, {"referenceID": 18, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 216, "endOffset": 232}, {"referenceID": 22, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 216, "endOffset": 232}, {"referenceID": 27, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 216, "endOffset": 232}, {"referenceID": 30, "context": "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].", "startOffset": 216, "endOffset": 232}, {"referenceID": 4, "context": "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].", "startOffset": 161, "endOffset": 181}, {"referenceID": 7, "context": "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].", "startOffset": 161, "endOffset": 181}, {"referenceID": 10, "context": "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].", "startOffset": 161, "endOffset": 181}, {"referenceID": 15, "context": "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].", "startOffset": 161, "endOffset": 181}, {"referenceID": 23, "context": "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].", "startOffset": 161, "endOffset": 181}, {"referenceID": 24, "context": "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].", "startOffset": 161, "endOffset": 181}, {"referenceID": 29, "context": "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].", "startOffset": 161, "endOffset": 181}, {"referenceID": 17, "context": "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.", "startOffset": 53, "endOffset": 69}, {"referenceID": 18, "context": "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.", "startOffset": 53, "endOffset": 69}, {"referenceID": 22, "context": "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.", "startOffset": 53, "endOffset": 69}, {"referenceID": 27, "context": "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.", "startOffset": 53, "endOffset": 69}, {"referenceID": 30, "context": "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.", "startOffset": 53, "endOffset": 69}, {"referenceID": 19, "context": "QUEST algorithm merges several classes into two superclasses to produce one binary split [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "In [6], the number of terminal nodes is reduced by fusing the leaves with similar predictions after the training is finished.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "Data samples are fused based on the significance of their similarity estimated by test statistics: Chi-squared [17] for categorical label and F-test [22] for continuous.", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "Data samples are fused based on the significance of their similarity estimated by test statistics: Chi-squared [17] for categorical label and F-test [22] for continuous.", "startOffset": 149, "endOffset": 153}, {"referenceID": 2, "context": "At the first step, DS grows using the classical decision tree branching operation \u2014 new nodes are created by splitting all current non-terminal leaves [3,11,19].", "startOffset": 151, "endOffset": 160}, {"referenceID": 10, "context": "At the first step, DS grows using the classical decision tree branching operation \u2014 new nodes are created by splitting all current non-terminal leaves [3,11,19].", "startOffset": 151, "endOffset": 160}, {"referenceID": 18, "context": "At the first step, DS grows using the classical decision tree branching operation \u2014 new nodes are created by splitting all current non-terminal leaves [3,11,19].", "startOffset": 151, "endOffset": 160}, {"referenceID": 28, "context": "The choice between the tests is determined according to rule [29]: Z-test is applied if the size of both data samples is greater than 30, Student\u2019s t-test \u2014 otherwise.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "We prefer Kolmogorov-Smirnov over Mann-Whitney U test since it is more sensitive to variations of both location and shape of the empirical cumulative distribution function [7], and provides better prediction accuracy in our experiments.", "startOffset": 172, "endOffset": 175}, {"referenceID": 26, "context": "Instead of the greedy search, we perform data splitting based on the feature that is most correlated with label within a particular node [27].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "Features were extracted from the last convolutional layer of the pre-trained ResNet-18 [14] CNN.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Each generated sample consisted of 500 features (125 binary, 125 categorical with 20 categories and 250 continuous within interval [0, 1]) and represented binary classification and regression problems.", "startOffset": 131, "endOffset": 137}, {"referenceID": 14, "context": "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].", "startOffset": 166, "endOffset": 170}, {"referenceID": 1, "context": "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].", "startOffset": 183, "endOffset": 186}], "year": 2017, "abstractText": "Various modifications of decision trees have been extensively used during the past years due to their high efficiency and interpretability. Selection of relevant features for spitting the tree nodes is a key property of their architecture, at the same time being their major shortcoming: the recursive nodes partitioning leads to geometric reduction of data quantity in the leaf nodes, which causes an excessive model complexity and data overfitting. In this paper, we present a novel architecture \u2014 a Decision Stream, \u2014 aimed to overcome this problem. Instead of building an acyclic tree structure during the training process, we propose merging nodes from different branches based on their similarity that is estimated with two-sample test statistics. To evaluate the proposed solution, we test it on several common machine learning problems \u2014 credit scoring, twitter sentiment analysis, aircraft flight control, MNIST and CIFAR image classification, synthetic data classification and regression. Our experimental results reveal that the proposed approach significantly outperforms the standard decision tree method on both regression and classification tasks, yielding a prediction error decrease up to 35%.", "creator": "LaTeX with hyperref package"}}}