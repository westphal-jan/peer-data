{"id": "1704.07790", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "FWDA: a Fast Wishart Discriminant Analysis with its Application to Electronic Health Records Data Classification", "abstract": "Linear Discriminant Analysis (LDA) on Electronic Health Records (EHR) data is widely-used for early detection of diseases. Classical LDA for EHR data classification, however, suffers from two handicaps: the ill-posed estimation of LDA parameters (e.g., covariance matrix), and the \"linear inseparability\" of EHR data. To handle these two issues, in this paper, we propose a novel classifier FWDA -- Fast Wishart Discriminant Analysis, that makes predictions in an ensemble way. Specifically, FWDA first surrogates the distribution of inverse covariance matrices using a Wishart distribution estimated from the training data, then \"weighted-averages\" the classification results of multiple LDA classifiers parameterized by the sampled inverse covariance matrices via a Bayesian Voting scheme. The weights for voting are optimally updated to adapt each new input data, so as to enable the nonlinear classification. Theoretical analysis indicates that FWDA possesses a fast convergence rate and a robust performance on high dimensional data. Extensive experiments on large-scale EHR dataset show that our approach outperforms state-of-the-art algorithms by a large margin.", "histories": [["v1", "Tue, 25 Apr 2017 17:11:57 GMT  (260kb,D)", "http://arxiv.org/abs/1704.07790v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haoyi xiong", "wei cheng", "wenqing hu", "jiang bian", "zhishan guo"], "accepted": false, "id": "1704.07790"}, "pdf": {"name": "1704.07790.pdf", "metadata": {"source": "CRF", "title": "FWDA: a Fast Wishart Discriminant Analysis with its Application to Electronic Health Records Data Classification", "authors": ["Haoyi Xiong", "Wei Cheng", "Wenqing Hu", "Jiang Bian", "Zhishan Guo"], "emails": [], "sections": [{"heading": null, "text": "However, the classical LDA for the classification of EHR data suffers from two disadvantages: the insufficient estimation of LDA parameters (e.g. covariance matrix) and the \"linear inseparability\" of EHR data. To solve these two problems, we propose in this paper a new classifier: FWDA - Fast Wishart Discriminant Analysis, which makes predictions in an ensemble way. Specifically, FWDA first surrounds the distribution of inverse covariance matrices using a Wishart distribution estimated from the training data, and then \"weighs\" the classification results of several LDA classifiers, which are parameterized by the sampled inverse covariance matrices using a Bayesian selection scheme. The weights for tuning are optimally updated to adapt each new input to large LDA classifiers that have a high linear rate as an indicator for the rapid convergence of data."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Background and Problem Formulation", "text": "In this section we first present the preparatory work of our research and then formulate the research problem of this work."}, {"heading": "2.1 Binary Classification for Early Detection of Diseases using EHR data", "text": "First, we introduce the EHR data representation based on diagnostic frequency vectors and present the settings of disease detection through binary classification of diagnostic frequency vectors. Later, we briefly discuss the solution based on the typical LDA classification. EHR data representation using diagnostic frequency vectors - There are many existing approaches to presenting EHR data, including the use of diagnostic frequencies [1, 3, 2], paired diagnostic transition [4, 5] and diagrams of diagnostic sequences [6]. Among these approaches, diagnostic frequency is a common method of presenting EHR data. Given each patient's EHR data, this method first calls up the diagnostic codes [26] that are recorded on each visit. Next, the frequency of each diagnosis that occurs on each past visit is counted, followed by a further transformation of the frequency of each diagnosis into a vector."}, {"heading": "2.2 Linear Discriminant Analysis", "text": "To solve the above problem of binary classification, we consider a simple LDA classifier f (x) based on the given p-dimensional data vector x and labeled samples x1, x2,... xnf (x, \u03a3) = character (x \u2212 x) T \u03a3 \u2212 1 (x + 1 \u2212 x \u2212 1) (1), where x refers receptively to the mean vectors of all samples x1, x2,... xn; x + 1, x \u2212 1 refers to the mean values of positive and negative samples."}, {"heading": "2.3 Bayesian Voting Scheme", "text": "In view of a binary classifier h\u03c9 (x), which is parameterized by \u03c9, the Bavarian election classification [24] of the classifier is: characters, where the signal function character (\u00b7) maps the non-negative input to + 1 and the negative input to \u2212 1, and p (\u03c9) is the previous probability of the parameter \u03c9. As a binary classifier, the above classifier in Eq.3 displays the label with the highest vote weighting. [24] addresses the theoretical advantages of the Bavarian election scheme."}, {"heading": "2.4 Problem Formulation", "text": "In order to overcome the uncertainty of the (inverse) covariance matrix for the LDA, we need to address the question of how far we see ourselves able to establish a new ratio. (4) In our study, we called this pattern as Input Adaptive Bayesian Voting. (5) We are considering the new input vector x to account for the generationality of the \"hypothesis\" of Bayesian Inference.With all of the above backgrounds and attitudes, the problem of this research is at least two major technical challenges. (5) We are considering the new input vector x. (5) We have the \"hypothesis\" of Bayesian Inference.n \"We have all of the above factors and attitudes in mind, the problem of this research consists of at least two major technical challenges. (5) We can consider the new input vector x around which we consider the\" hypothesis \"of Bayesian Inference.n"}, {"heading": "3.1 Problem Reformulation", "text": "We first define P (x | \u03a3) as the probability of the input vector x taking into account the covariance matrix \u03a3, and P (\u03a3 | x1, x2... xn) as the probability of the covariance matrix \u03a3 taking into account the training samples x1, x2... xn. Then we define a function: g (x) = result of the classification of signs (g (x).Proof. If we take all x1, x2,... xn, x, we are drawn from an unknown distribution according to the Bayesian theorem. (5) Theorem 1. Equation 4 is equivalent to the classification result of signs (g (x).Proof. If we take all x1, x2,... xn, x, we are drawn from an unknown distribution according to the Bayesian theorem, we decompose P (\u03a3 | x1,... xn, x), then the variance of P (\u04451, xn, xn, x) becomes."}, {"heading": "3.2 Wishart Distribution Model based on De-sparsified Graphical Lasso", "text": "To sample the (inverse) covariance matrices according to P (\u03a3 | x1, x2... xn). FWDA uses a wishart distribution [30] namelyW (T, v), where T refers to the \"mean\" positive definite matrix for the wishart distribution and v to the degree of freedom.Given each p \u00b7 p positive definitive matrix (as a reversal of the potential covariance matrix), we estimate the probability density of p (v \u2212 p \u2212 1) / 2 e \u2212 (1 / 2) tr (T, v), as: Pw (T, v) = 1 2vp / 2, and refer to the determinant and the multivariate gamma function as: p (v \u2212 p \u2212 1) / 2 e \u2212 (1 / 2) tr (T-1) tr (T-1), where the matrix refers to the determinant and the multivariate function."}, {"heading": "3.3 Binary Classification as Bayesian Inference via Regularized Wishart Prior", "text": "Using the typical inverse wishart sampling algorithm [32], the FWDA first generated randomly generated m inverse covariance matrices [1], [2]... derived from the Wishart distribution W (T], [3], [4 as: g [3] (x) = 1m \u00b2 1 \u2264 i \u2264 m (f (x), [4] P (x |] \u2212 1i)), (10), where P (x \u2212 1i) refers to the probability of the input vector x in view of the inverse covariance matrix \u0433i. In this paper, we describe the probability as: P (x \u2212 1i) = 1 \u2212 2 \u2212 1i | e \u2212 1 (x \u2212 2 (x \u2212 x) T \u044bi (x \u2212 x), (11), where x = n \u2212 1 \u2212 nj refers to the mean value of the problem expressed in the following EQ4 analysis."}, {"heading": "3.4 Approximation Analysis", "text": "& & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & & & # 160; & & & & # 160; & & # 160; & & & # 160; & & & # 160; & & # 160; & # 160; & & # 160; & # 160; & # 160; & # 160; & & & # 160; & # 160; & & & # 160; & & # 160; & & # 160; & & # 160; & & # 160; & & # 160; & & & # 160; & & & # 160; & & # 160; & & # 160; & & # 160; & # 160; & # 160; & # 160; & # 160; & & # 160; & & # 160; & # 160; & & & # 160; & & & & # 160; & & & & & # 160; & & # 160; & & # 160; & # 160 & & & & & # 160; & & # 160; & & # 160; & & & # 160; & # 160; & & # 160; & & # 160; & & # 160; & # 160; & # 160 & # 160; & # 160 & & & # 160 & # 160; & # 160; & & & # 160; & # 160 & # 160; & & & # 160 & # 160; & & # 160 & & # 160 & # 160; & # 160; & # 160 & # 160 & & # 160; & & & # 160; & # 160 & # 160; & # 160 & & & & # 160; & & & # 160; & # 160; & & & # 160 & & # 160 & & # 160; & & # 160; & & # 160; & & & & # 160; & & & & # 160 & # 160; & & & # 160; & & & & # 160"}, {"heading": "4 Evaluation", "text": "In this section, we will first present the experimental design of our evaluation, then present the experimental results, including performance comparison between the proposed FWDA algorithm, existing LDA baselines and other predictive models, and a comparison between FWDA and the method that uses a simple discretization strategy to support our theoretical analysis of the FWDA."}, {"heading": "4.1 Experimental Setups", "text": "In fact, it is so that we are able to assert ourselves, that we are able to assert ourselves, that we are able to assert ourselves, and that we are able to assert ourselves in the world, that we are able to stay in the world, \"he said."}, {"heading": "4.2 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Overall Comparison", "text": "Figure 1 shows the performance of our approach together with classical LDA, linear SVM, kernel SVM and decision trees on 200 test samples and different sample sizes. FWDA (200, 1.0) refers to the FWDA classifier based on 200 sampled inverse covariance matrices using De-Sparsified Graphical Lasso \u03bb = 1.0 for estimating the mean values of the desired type matrix. As can be seen from the results, FWDA exceeds the basic algorithms in terms of overall accuracy and F1 score.Due to the spatial limitation, we do not present the comparison results based on two-stage LDA, logistic regression and LDA with shrink estimates in Figure 1. 1. FWDA (200, 1.0) clearly exceeds these two-stage LDA values by achieving an average of 4.3% higher accuracy and 3.5% higher F1 values."}, {"heading": "4.2.2 Comparison to Ensemble learners", "text": "Since FWDA compiles the classification results from multiple classifiers, we also compared FWDA with existing ensemble learning algorithms such as Random Forest and AdaBoost. To compare ensemble learners with 100 and 200 base classifiers, we use FWDA with 100 and 200 sampled inverse covariance matrices (i.e. ensembles with 100 and 200 LDA classifiers), with the performance of Random Forest not quite stable. In addition, Figure 2 also shows the performance of FWDA classifiers with 100 and 200 sampled inverse covariance matrices, which are very similar. In fact, we tested FWDA with 50 to 2000 inverse covariance matrices, with predictive accuracy or F1 values of FWDA even showing a nearly consistent number of WDA variations."}, {"heading": "4.2.3 Comparison on Discretization and Regularization", "text": "We compare FWDA with a classifier, namely \"Discrete-FWDA,\" based on the simple discretization strategy: Signs: 1 \u2264 i \u2264 m f (x, \u0432 \u2212 1i) P (x | \u044b \u2212 1i) Pw (\u0441i | T, v). Both algorithms use a de-sparsified Graphical Lasso with \u03bb = 1.0, 10.0, 100.0 for the Wishart mean estimate. Figure 3 shows the performance comparison. It shows that FWDA exceeds the simple discretization strategy with the same significance. In Figure 3 we also show the performance improvement achieved by regulation (De-sparsified Graphical Lasso) for Wishart mean matrix estimation. The lines titled \"Sample-FWDA\" refer to a derived method using the instability variance (De-sparsified Graphical Lasso) for Wishart mean matrix estimation."}, {"heading": "4.3 Comparison on Time Consumption", "text": "In addition to the accuracy comparison, we also compare the time consumption of FWDA (based on \"lazy sampling\") with an alternative method based on the same regulated wishart distribution; for each new data classification, a group of inverse covariance matrices for LDA classification and tuning is randomly assigned; for 400 x 2 training samples, FWDA (200.1.0) takes an average of 149.5 seconds to train the model (including Desparsified Graphical Lasso with \u03bb = 0.1 and sampling 200 inverse covariance matrices), and 0.037 seconds to classify 200 x 2 samples for the test. We evaluate the alternative algorithm in the same settings - the alternative algorithm does not need to be trained, and it takes an average of 16.7 hours to classify the 200 x 2 test samples."}, {"heading": "5 Conclusion and Discussion", "text": "In this paper, we have proposed FWDA - a novel disease early detection algorithm based on EHR data and diagnostic frequency vector data representation. FWDA reduces uncertainty in estimating LDA parameters and also enables non-linear classification through the Input Adaptive Bayesian Election Scheme. Theoretical analysis shows that FWDA is rapidly approaching optimal Bayesian vote delivery, and the experimental results of the real EHR data set CHSN show that FWDA exceeds all basic algorithms. In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6] to enable superior prediction."}], "references": [{"title": "Supervised patient similarity measure of heterogeneous patient", "author": ["Jimeng Sun", "Fei Wang", "Jianying Hu", "Shahram Edabollahi"], "venue": "records. ACM SIGKDD Explorations Newsletter,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Personalized predictive modeling and risk factor identification using patient similarity", "author": ["Kenney Ng", "Jimeng Sun", "Jianying Hu", "Fei Wang"], "venue": "AMIA Summit on Clinical Research Informatics (CRI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Psf: A unified patient similarity evaluation framework through metric learning with weak supervision", "author": ["Fei Wang", "Jimeng Sun"], "venue": "Biomedical and Health Informatics, IEEE Journal of,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "MSEQ: Early detection of anxiety and depression via temporal orders of diagnoses in electronic health data", "author": ["Jinghe Zhang", "Haoyi Xiong", "Yu Huang", "Hao Wu", "Kevin Leach", "Laura E. Barnes"], "venue": "In Big Data (Workshop),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Mining medical data for predictive and sequential patterns: Pkdd", "author": ["Susan Jensen", "UK SPSS"], "venue": "In Proceedings of the 5th European Conference on Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Temporal Phenotyping from Longitudinal Electronic Health Records: A Graph Based Framework", "author": ["Chuanren Liu", "Fei Wang", "Jianying Hu", "Hui Xiong"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Clinical risk prediction by exploring high-order feature correlations", "author": ["Fei Wang", "Ping Zhang", "Xiang Wang", "Jianying Hu"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Bayes optimality in linear discriminant analysis", "author": ["Onur C Hamsici", "Aleix M Martinez"], "venue": "TPAMI, 30(4):647\u2013657,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Random matrix theory in pattern classification: An application to error estimation", "author": ["Amin Zollanvari", "Edward R Dougherty"], "venue": "In 2013 Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Marble: high-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization", "author": ["Joyce C Ho", "Joydeep Ghosh", "Jimeng Sun"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Estimating structured highdimensional covariance and precision matrices: Optimal rates and adaptive estimation", "author": ["T Tony Cai", "Zhao Ren", "Harrison H Zhou"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "The use of shrinkage estimators in linear discriminant analysis", "author": ["Roger Peck", "John Van Ness"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1982}, {"title": "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition", "author": ["Juwei Lu", "Konstantinos N Plataniotis", "Anastasios N Venetsanopoulos"], "venue": "Pattern Recognition Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Covariance-regularized regression and classification for high dimensional problems", "author": ["Daniela M Witten", "Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sparse linear discriminant analysis by thresholding for high dimensional data", "author": ["Jun Shao", "Yazhen Wang", "Xinwei Deng", "Sijian Wang"], "venue": "The Annals of statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Statistics for high-dimensional data: methods, theory and applications", "author": ["Peter Buhlmann", "Sara Van De Geer"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["Neil D Lawrence", "Bernhard Sch\u00f6lkopf"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Learning metrics via discriminant kernels and multidimensional scaling: Toward expected euclidean representation", "author": ["Zhihua Zhang"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Optimal kernel selection in kernel fisher discriminant analysis", "author": ["Seung-Jean Kim", "Alessandro Magnani", "Stephen Boyd"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Regularized discriminant analysis, ridge regression and beyond", "author": ["Zhihua Zhang", "Guang Dai", "Congfu Xu", "Michael I Jordan"], "venue": "JMLR, 11(Aug):2199\u20132228,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Model selection and multimodel inference: a practical information-theoretic approach", "author": ["Kenneth P Burnham", "David R Anderson"], "venue": "Springer Science & Business Media,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Bayesian model averaging: a tutorial", "author": ["Jennifer A Hoeting", "David Madigan", "Adrian E Raftery", "Chris T Volinsky"], "venue": "Statistical science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Bayesian voting schemes and large margin classifiers", "author": ["Nello Cristianini", "John Shawe-Taylor"], "venue": "Advances in Kernel MethodsSupport Vector Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm", "author": ["Pascal Germain", "Alexandre Lacasse", "Francois Laviolette", "Mario Marchand", "Jean-Francis Roy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Icd-9 codes and surveillance for clostridium difficile\u2013associated disease", "author": ["Erik R Dubberke", "Kimberly A Reske", "L Clifford McDonald", "Victoria J Fraser"], "venue": "Emerging infectious diseases,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Monte carlo methods", "author": ["John Hammersley"], "venue": "Springer Science & Business Media,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Methods of numerical integration", "author": ["Philip J Davis", "Philip Rabinowitz"], "venue": "Courier Corporation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Estimation of the inverse covariance matrix: Random mixtures of the inverse wishart matrix and the identity", "author": ["LR Haff"], "venue": "The Annals of Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1979}, {"title": "Confidence intervals for high-dimensional inverse covariance estimation", "author": ["Jana Jankova", "Sara van de Geer"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Wishart distributions and inverse-wishart sampling", "author": ["S Sawyer"], "venue": "URL: www. math. wustl. edu/sawyer/hmhandouts/Whishart. pdf,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Bayesian inference for a covariance matrix", "author": ["Tom Leonard", "John SJ Hsu"], "venue": "The Annals of Statistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1992}, {"title": "Bayesian quadratic discriminant analysis", "author": ["Santosh Srivastava", "Maya R Gupta", "B\u00e9la A Frigyik"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Bayesian inference for a covariance matrix", "author": ["Ignacio Alvarez", "Jarad Niemi", "Matt Simpson"], "venue": "arXiv preprint arXiv:1408.4050,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "College Health Surveillance Network: Epidemiology and Health Care Utilization of College Students at U.S. 4-Year Universities", "author": ["James C. Turner", "Adrienne Keller"], "venue": "Journal of American college health: J of ACH,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "An optimization criterion for generalized discriminant analysis on undersampled problems", "author": ["Jieping Ye", "Ravi Janardan", "Cheong Hee Park", "Haesun Park"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "Two-dimensional linear discriminant analysis", "author": ["Jieping Ye", "Ravi Janardan", "Qi Li"], "venue": "In NIPS,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "A two-stage linear discriminant analysis via qrdecomposition", "author": ["Jieping Ye", "Qi Li"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Efficient l \u0303 1 regularized logistic regression", "author": ["Su-In Lee", "Honglak Lee", "Pieter Abbeel", "Andrew Y Ng"], "venue": "In AAAI,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The ubiquity of Electronic Health Records (EHR) [1, 2] in healthcare systems provides an unique opportunity for early detection of patients\u2019 potential diseases using their historical health records.", "startOffset": 48, "endOffset": 54}, {"referenceID": 1, "context": "The ubiquity of Electronic Health Records (EHR) [1, 2] in healthcare systems provides an unique opportunity for early detection of patients\u2019 potential diseases using their historical health records.", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 87, "endOffset": 96}, {"referenceID": 2, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 87, "endOffset": 96}, {"referenceID": 1, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 87, "endOffset": 96}, {"referenceID": 3, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 128, "endOffset": 134}, {"referenceID": 4, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 128, "endOffset": 134}, {"referenceID": 5, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 170, "endOffset": 173}, {"referenceID": 0, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 2, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 1, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 3, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 6, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 3, "context": "Among these methods, LDA is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 6, "context": "Among these methods, LDA is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": "Among these methods, LDA is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "However, recent studies demonstrate the limitation of LDA under high dimension low sample size (HDLSS) settings [9], such as the EHR records [10].", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "However, recent studies demonstrate the limitation of LDA under high dimension low sample size (HDLSS) settings [9], such as the EHR records [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "Even when the sample size is larger than the number of dimensions, the sample (inverse) covariance estimation could be quite different with the \u201ctrue\u201d (inverse) covariance matrix, with an inconsistent estimate of the largest eigenvalues and almost-orthogonal eigenvectors to the truth [11].", "startOffset": 285, "endOffset": 289}, {"referenceID": 0, "context": "Moreover, EHR data is usually not linearly separable [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 1, "context": "Moreover, EHR data is usually not linearly separable [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 11, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 188, "endOffset": 200}, {"referenceID": 12, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 188, "endOffset": 200}, {"referenceID": 13, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 188, "endOffset": 200}, {"referenceID": 14, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 224, "endOffset": 232}, {"referenceID": 15, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 283, "endOffset": 287}, {"referenceID": 16, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 17, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 18, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 19, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 20, "context": "In summary, these methods intend to improve LDA classification through optimizing the parameters of LDA, such as (inverse) covariance matrices, linear projection metrics, or kernel settings, in a so-called optimal model selection manner [22].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "Instead of \u201cbidding\u201d the optimal parameter in the full and usually unknown parameter space, in this work, we intend to improve LDA in an ensemble way [23], while adapting to the new input data.", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "Specifically, we first sample a set of (inverse) covariance matrices from the both training data and the new input data, then \u201cweightedaverages\u201d the classification results of multiple LDA classifiers parameterized by the sampled inverse covariance matrices via a Bayesian Voting Scheme [24].", "startOffset": 286, "endOffset": 290}, {"referenceID": 22, "context": "Theoretical studies show that such Bayesian voting scheme can secure a wider margin and guarantee a good classification performance with a lower generalization error bound [24].", "startOffset": 172, "endOffset": 176}, {"referenceID": 23, "context": "This theoretically guarantees that the proposed framework can \u201con average\u201d outperform those regularization-based LDA classifiers using only single (inverse) covariance matrix estimator [25].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 160, "endOffset": 169}, {"referenceID": 2, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 160, "endOffset": 169}, {"referenceID": 1, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 160, "endOffset": 169}, {"referenceID": 3, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 201, "endOffset": 207}, {"referenceID": 4, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 201, "endOffset": 207}, {"referenceID": 5, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 258, "endOffset": 261}, {"referenceID": 24, "context": "Given each patient\u2019s EHR data, this method first retrieves the diagnosis codes [26] recorded during each visit.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "3 Bayesian Voting Scheme Given a binary classifier h\u03c9(x) \u2208 {\u00b11}, which is parameterized by \u03c9, the Bayesian Voting Classification [24] of the classifier is:", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "The theoretical advantages of Bayesian voting scheme are addressed in [24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "4, a common solution is to leverage a Monte-Carlo Integration algorithm [28] that", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "For the high-dimensional numeric integration [29], the approximation is usually bottle-necked by the number of dimensions (e.", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "xn), FWDA leverages a Wishart Distribution [30] namelyW(T\u0302 , v), where T\u0302 refers to the \u201cmean\u201d positivedefinite matrix for the Wishart distribution and v is the degree of freedom.", "startOffset": 43, "endOffset": 47}, {"referenceID": 28, "context": "Specifically, in our research, we set the degree of feedom v as v = n \u2212 1, and further estimate T\u0302 using De-sparsified Graphical Lasso [31]: T\u0302 = 2\u0398\u0302\u2212 \u0398\u0302\u03a3\u0304\u0398\u0302.", "startOffset": 135, "endOffset": 139}, {"referenceID": 29, "context": "3 Binary Classification as Bayesian Inference via Regularized Wishart Prior Using the typical inverse-wishart sampling algorithm [32], FWDA first randomly generated m inverse-covariance matrices \u03981,\u03982.", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "First of all, considering the fast convergence rate of De-Sparsified Graphical Lasso [31] i.", "startOffset": 85, "endOffset": 89}, {"referenceID": 30, "context": ", ||T\u0302 \u2212 \u0398\u2217||\u221e = Op( \u221a log p /n), with a fixed number of dimensions p and an increasing number of samples n, we are more confident to follow an assumption frequently made in many of previous Bayesian inference studies [33, 34, 35]: Assumption 1.", "startOffset": 218, "endOffset": 230}, {"referenceID": 31, "context": ", ||T\u0302 \u2212 \u0398\u2217||\u221e = Op( \u221a log p /n), with a fixed number of dimensions p and an increasing number of samples n, we are more confident to follow an assumption frequently made in many of previous Bayesian inference studies [33, 34, 35]: Assumption 1.", "startOffset": 218, "endOffset": 230}, {"referenceID": 32, "context": ", ||T\u0302 \u2212 \u0398\u2217||\u221e = Op( \u221a log p /n), with a fixed number of dimensions p and an increasing number of samples n, we are more confident to follow an assumption frequently made in many of previous Bayesian inference studies [33, 34, 35]: Assumption 1.", "startOffset": 218, "endOffset": 230}, {"referenceID": 33, "context": "1 Experimental Setups We use the de-identified EHR data from the College Health Surveillance Network (CHSN), which contains over 1 million patients and 6 million visits from 31 student health centers across the United States [36].", "startOffset": 225, "endOffset": 229}, {"referenceID": 34, "context": "Specifically, LDA uses the sample covariance estimation, and inverts the covariance matrix using pseudoinverse [37] when the matrix inverse is not available.", "startOffset": 111, "endOffset": 115}, {"referenceID": 35, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 140, "endOffset": 148}, {"referenceID": 36, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 140, "endOffset": 148}, {"referenceID": 37, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 210, "endOffset": 214}, {"referenceID": 0, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 2, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 1, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 3, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 5, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}], "year": 2017, "abstractText": "Linear Discriminant Analysis (LDA) on Electronic Health Records (EHR) data is widely-used for early detection of diseases. Classical LDA for EHR data classification, however, suffers from two handicaps: the ill-posed estimation of LDA parameters (e.g., covariance matrix), and the \u201clinear inseparability\u201d of EHR data. To handle these two issues, in this paper, we propose a novel classifier FWDA \u2014 Fast Wishart Discriminant Analysis, that makes predictions in an ensemble way. Specifically, FWDA first surrogates the distribution of inverse covariance matrices using a Wishart distribution estimated from the training data, then \u201cweightedaverages\u201d the classification results of multiple LDA classifiers parameterized by the sampled inverse covariance matrices via a Bayesian Voting scheme. The weights for voting are optimally updated to adapt each new input data, so as to enable the nonlinear classification. Theoretical analysis indicates that FWDA possesses a fast convergence rate and a robust performance on high dimensional data. Extensive experiments on large-scale EHR dataset show that our approach outperforms stateof-the-art algorithms by a large margin.", "creator": "LaTeX with hyperref package"}}}