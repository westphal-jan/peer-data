{"id": "1704.07816", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Introspective Classifier Learning: Empower Generatively", "abstract": "In this paper we propose introspective classifier learning (ICL) that emphasizes the importance of having a discriminative classifier empowered with generative capabilities. We develop a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our classifier is able to iteratively: (1) synthesize pseudo-negative samples in the synthesis step; and (2) enhance itself by improving the classification in the reclassification step. The single classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on standard benchmark datasets including MNIST, CIFAR, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.", "histories": [["v1", "Tue, 25 Apr 2017 17:49:03 GMT  (1105kb,D)", "http://arxiv.org/abs/1704.07816v1", "11 pages, 6 figure"]], "COMMENTS": "11 pages, 6 figure", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["long jin", "justin lazarow", "zhuowen tu"], "accepted": false, "id": "1704.07816"}, "pdf": {"name": "1704.07816.pdf", "metadata": {"source": "META", "title": "Introspective Classifier Learning: Empower Generatively", "authors": ["Long Jin", "Justin Lazarow", "Zhuowen Tu"], "emails": ["ztu}@ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "Other learning principles that are not fully monitored include unmonitored (Duda et al., 2000), semi-discriminated (Zhu, 2005), discriminated (Dietterich et al., 1997), and reinforced (Sutton & Barto, 1998), which also point to promising approaches when commenting on the data. Existing classification processes, such as decision trees (Quinlan, 1996), supporting vector machines (Vapnik, 1995), reinforcing neural networks (LeCun et al., 1989) (Freund & Schapire, 1997), lead to training processes that implicitly or explicitly minimize an objective function that balance training error and each other."}, {"heading": "2. Significance and related work", "text": "In fact, it is a way in which people discriminate against themselves and against themselves. (2) We have developed an efficient method of synthesizing new data. (3) A reclassification according to the motto: \"There is only one way.\" We show a consistent improvement over a state of art that negatively affects people. (4) We propose a formulation that is obvious to get involved in an obvious subject. (3) We show that a consistent improvement is taking place in relation to art. (Er et al., 2016a))) CIFAR and SVHN datasets are able to compare themselves."}, {"heading": "2.1. Relationship with GDL (Tu, 2007)", "text": "The generative about discriminative learning framework (GDL) (Tu, 2007) learns a generative model through a sequence of discriminatory classifiers (Boosting (Freund & Schapire, 1997) using repeatedly self-generated samples, so-called pseudo-negatives. Figure 1 shows the basic pipeline of GDL in (Tu, 2007).The far left panel shows a set of points (in red) as input data, and the task is to learn a generative model to characterize the distribution of these points, hence an unattended learning task; the GDL algorithm assumes a uniform distribution as reference distribution to generate the first group of pseudo-negatives (shown on the upper left panel), and a discriminating classifier is then trained to separate the given input data from the pseudo-negatives; new samples passing the learned classifiers are viewed as a new group of pseudo-negatives in the upper panel (shown in the middle panel)."}, {"heading": "3. Method", "text": "The ICL pipeline is shown in Figure 2, which shows an immediate improvement over GDL in several aspects described in the previous section. A particular advantage of the ICL is its representational power and efficient sampling process through back propagation as a variable sampling strategy."}, {"heading": "3.1. Formulation", "text": "We begin the discussion by introducing the basic formulation and borrowing the notations from (Tu, 2007) (x = 1). Let x be a data sample (vector) and y (x) indicating either a negative or a positive sample. In the multi-class classification y = 1,..., K}. Let's examine the binary classification first. A generative model instead is models p (y) = p (x | y) p (y) p (y) limiting the underlying generation processes of x for class y. In the binary classification, positive samples are of primary interest. Under the Bayes rule: p (x) = p (y = 1 | x) p (y = \u2212 x) p (y) p (y) p (y = 1) p (y = 1) p (y = 1) p (y)."}, {"heading": "3.2. Reclassification-by-synthesis", "text": "The key to the algorithm consists of two steps: (1) reclassification step and (2) synthesis step, which are discussed in detail below. (1) The key to the algorithm consists of two steps: (1) reclassification step and (2) synthesis step."}, {"heading": "3.2.1. RECLASSIFICATION-STEP", "text": "The reclassification step can be regarded as training a normal classifier on the training set Ste = S-Stpn = Update (S-Stpn) (S = {(xi, yi), i = 1.. n} and S0pn = \u2205. Stpn = {(xi, \u2212 1), i = n,..., n + tl} for t \u2265 1. We use CNN as our basic classifier. When training a classifier Ct to Ste, we refer to the parameters that in Ct are combined by a high-dimensional alvector Wt = (w (0) t, w (1) t), w (1) t), which can consist of millions of parameters. w (1) t refers to the weights on the top layer combining the properties (x; w (0) t) and w (0) t: the internal representations."}, {"heading": "3.2.2. SYNTHESIS-STEP", "text": "In the reclassification step, we get qt (y; x; Wt), which is then used to update p \u2212 t (x) according to eqn. (5): p \u2212 t (x) = 1Zt qt (y = + 1 | x; Wt) qt (y = \u2212 1 | x; Wt) p \u2212 r (x). (8) In the synthesis step, our goal is to draw fair samples from p \u2212 t (x). In (Tu, 2007), various Markov chain Monte Carlo techniques (Liu, 2008) including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow. Motivated by the DeepDream Code (Mordvintsev et al., 2015) and Neural Artistic Style work (Gatys et al., 2015), we update a random sample x (r) by increasing qt (x)."}, {"heading": "3.3. Multi-class classification", "text": "In the above section we discussed the case of binary classification. In dealing with multi-class classification problems, such as MNIST and CIFAR-10, we need to adapt our proposed reclassification-by-synthesis scheme to the case of multi-class classification, which can be done directly with a one-class strategy by performing {w} w a binary classification by using the i \u2212 th class as a positive class and then combining the remaining classes into the negative class, resulting in a total of K binary classifiers. The training procedure then becomes identical to the case of binary classification. If we have K classes, then the algorithm will use the individual binary classifiers with < (w (0) 1 t, w (1) 1 t),..., (w (0) K t, w (1) K t) >.The prediction function is simple f (x) = argmax kexp."}, {"heading": "3.4. Analysis", "text": "We show the convergence of p \u2212 t (x) = p + (x) = p (x) = p (x) = p (x) = p (x) = p (x) = p (x), p (x) = p (x) = p (x) = p (x) = p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), x (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x, p (x), p (x), p (x), p (x, p (x), p (x), p (x, p (x), p (x), p (x, p (x), p (x), p (x, p (x), p (x), p (x, p, p (x), p (x, p (x), p (x, p (x, p, p, p, p, x, x), p (x, p, p (x, p, x), p (x), p (x, x, p (x), p (x), p (x, x, p (x, p, x), p (x, x, x, p, x, x, p, p, x, p, x, x, p, p, x, x, p (x), x, p (x), p (x"}, {"heading": "4. Experiments", "text": "We conduct experiments with three standard benchmark datasets, including MNIST, CIFAR-10 and SVHN. We use MNIST as a running example to illustrate our proposed framework using a shallow CNN and to show competitive results using a state-of-the-art CNN classifier, ResNet (He et al., 2016a) on CIFAR-10 and SVHN. In our experiments we use the SGD optimizer for the reclassification step with a mini batch size of 64 (MNIST) or 128 (CIFAR-10 and SVHN) and a dynamic of 0.9; for the synthesis step we use the Adam optimizer (Kingma & Ba, 2014) with a dynamic of \u03b21 equal to 0.5."}, {"heading": "4.1. MNIST", "text": "The MNIST (LeCun & Cortes, 1998) dataset consists of 28 \u00d7 28 grayscale images from 10 different classes (0 to 9) with 60, 000 training and 10, 000 test samples. We use a simple network that contains 4 different layers, each of which has a 5 \u00d7 5 filter size of 64, 128, 256 and 512 channels. The last revolutionary layer is flat and fed into a significant layer (in one-on-one cases), which is the discriminator in DCGAN (Radford et al., 2015), but we do not use normalization layers. In the synthesis step, we use the backpropagation process as discussed in Section 3.2.2."}, {"heading": "4.2. CIFAR-10", "text": "The CIFAR-10 dataset (Krizhevsky, 2009) consists of 32 x 32 color images. A collection of 60,000 images is divided into 50,000 training images and 10,000 test images. We use ResNet (He et al., 2016b) as our widely used base model. For data expansion, we follow the standard procedure in (Lee et al., 2015; 2016; He et al., 2016b) by enlarging the dataset by 4 pixels on each side by zero padding; we also perform cutouts and random flipping. In both single and softmax cases, ICL exceeds the base method. Our proposed ICL method is orthogonal to many existing approaches that use various improvements in network structures to improve CNN's performance."}, {"heading": "4.3. SVHN", "text": "The SVHN dataset (Netzer et al., 2011) consists of color images of house numbers collected by Google Street View. We use a training set that combines its training and additional data in our experiments and leaves the test data as our 1https: / / github.com / ppwwyxx / tensorpack / tree / master / examples / ResNettest datasets."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed Introspective Classification Learning (ICL), which leads to a CNN classifier equipped with generative capabilities. We developed a training algorithm for reclassification by synthesis. In addition, a new loss function was developed to seamlessly integrate ICL into a single CNN classifier with multiple classes. We tested the algorithm against standard benchmarks and reported encouraging results."}, {"heading": "6. Acknowledgement", "text": "This work is supported by NSF IIS-1618477 and a grant from Northrop Grumman Contextual Robotics. We thank Saining Xie, Shuai Tang and Sanjoy Dasgupta for helpful discussions."}], "references": [{"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["Baldi", "Pierre"], "venue": "ICML unsupervised and transfer learning,", "citeRegEx": "Baldi and Pierre.,? \\Q2012\\E", "shortCiteRegEx": "Baldi and Pierre.", "year": 2012}, {"title": "Random Forests", "author": ["Breiman", "Leo"], "venue": "Machine Learning,", "citeRegEx": "Breiman and Leo.,? \\Q2001\\E", "shortCiteRegEx": "Breiman and Leo.", "year": 2001}, {"title": "Neural photo editing with introspective adversarial networks", "author": ["Brock", "Andrew", "Lim", "Theodore", "JM Ritchie", "Weston", "Nick"], "venue": "arXiv preprint arXiv:1609.07093,", "citeRegEx": "Brock et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brock et al\\.", "year": 2016}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["Chen", "Tianqi", "Fox", "Emily B", "Guestrin", "Carlos"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Inducing features of random fields", "author": ["Della Pietra", "Stephen", "Vincent", "Lafferty", "John"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "P.T. Lozano"], "venue": "Artificial intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. of Comp. and Sys. Sci.,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "A neural algorithm of artistic style", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Representations of knowledge in complex systems", "author": ["Grenander", "Ulf", "Miller", "Michael I"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Grenander et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Grenander et al\\.", "year": 1994}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyv\u00e4rinen", "Aapo"], "venue": "In AISTATS,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M. The"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Machine learning: discriminative and generative", "author": ["Jebara", "Tony"], "venue": null, "citeRegEx": "Jebara and Tony.,? \\Q2012\\E", "shortCiteRegEx": "Jebara and Tony.", "year": 2012}, {"title": "DCGAN-tensorflow. https://github.com/ carpedm20/DCGAN-tensorflow, 2016", "author": ["Kim", "Taehoon"], "venue": null, "citeRegEx": "Kim and Taehoon.,? \\Q2016\\E", "shortCiteRegEx": "Kim and Taehoon.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Krizhevsky", "Alex"], "venue": "CS Dept., U Toronto, Tech. Rep.,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Introspective learning and reasoning", "author": ["Leake", "David B"], "venue": "In Encyclopedia of the Sciences of Learning,", "citeRegEx": "Leake and B.,? \\Q2012\\E", "shortCiteRegEx": "Leake and B.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree", "author": ["Lee", "Chen-Yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["Liang", "Percy", "Jordan", "Michael I"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Liang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Monte Carlo strategies in scientific computing", "author": ["Liu", "Jun S"], "venue": "Springer Science & Business Media,", "citeRegEx": "Liu and S.,? \\Q2008\\E", "shortCiteRegEx": "Liu and S.", "year": 2008}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Stochastic gradient descent as approximate bayesian inference", "author": ["Mandt", "Stephan", "Hoffman", "Matthew D", "Blei", "David M"], "venue": "arXiv preprint arXiv:1704.04289,", "citeRegEx": "Mandt et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mandt et al\\.", "year": 2017}, {"title": "Bootstrapping: A nonparametric approach to statistical inference", "author": ["Mooney", "Christopher Z", "Duval", "Robert D", "Duvall", "Robert"], "venue": "Number 94-95", "citeRegEx": "Mooney et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mooney et al\\.", "year": 1993}, {"title": "Deepdream - a code example for visualizing neural networks", "author": ["Mordvintsev", "Alexander", "Olah", "Christopher", "Tyka", "Mike"], "venue": "Google Research,", "citeRegEx": "Mordvintsev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordvintsev et al\\.", "year": 2015}, {"title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Improved use of continuous attributes in c4.5", "author": ["J.R. Quinlan"], "venue": "J. of Art. Intell. Res.,", "citeRegEx": "Quinlan,? \\Q1996\\E", "shortCiteRegEx": "Quinlan", "year": 1996}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Active learning literature survey", "author": ["Settles", "Burr"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "Settles and Burr.,? \\Q2010\\E", "shortCiteRegEx": "Settles and Burr.", "year": 2010}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Adagan: Boosting generative models", "author": ["Tolstikhin", "Ilya", "Gelly", "Sylvain", "Bousquet", "Olivier", "SimonGabriel", "Carl-Johann", "Sch\u00f6lkopf", "Bernhard"], "venue": "arXiv preprint arXiv:1701.02386,", "citeRegEx": "Tolstikhin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tolstikhin et al\\.", "year": 2017}, {"title": "Learning generative models via discriminative approaches", "author": ["Tu", "Zhuowen"], "venue": "In CVPR,", "citeRegEx": "Tu and Zhuowen.,? \\Q2007\\E", "shortCiteRegEx": "Tu and Zhuowen.", "year": 2007}, {"title": "Brain anatomical structure segmentation by hybrid discriminative/generative models", "author": ["Tu", "Zhuowen", "Narr", "Katherine L", "Doll\u00e1r", "Piotr", "Dinov", "Ivo", "Thompson", "Paul M", "Toga", "Arthur W"], "venue": "Medical Imaging, IEEE Transactions on,", "citeRegEx": "Tu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2008}, {"title": "The nature of statistical learning theory", "author": ["Vapnik", "Vladimir N"], "venue": null, "citeRegEx": "Vapnik and N.,? \\Q1995\\E", "shortCiteRegEx": "Vapnik and N.", "year": 1995}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee W"], "venue": "In ICML,", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Self supervised boosting", "author": ["Welling", "Max", "Zemel", "Richard S", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Welling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2002}, {"title": "Equivalence of julesz ensembles and frame models", "author": ["Wu", "Ying Nian", "Zhu", "Song Chun", "Liu", "Xiuwen"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Wu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2000}, {"title": "Cooperative training of descriptor and generator networks", "author": ["Xie", "Jianwen", "Lu", "Yang", "Zhu", "Song-Chun", "Wu", "Ying Nian"], "venue": "arXiv preprint arXiv:1609.09408,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "A theory of generative convnet", "author": ["Xie", "Jianwen", "Lu", "Yang", "Zhu", "Song-Chun", "Wu", "Ying Nian"], "venue": "arXiv preprint arXiv:1602.03264,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Energybased generative adversarial network", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Minimax entropy principle and its application to texture modeling", "author": ["Zhu", "Song Chun", "Wu", "Ying Nian", "Mumford", "David"], "venue": "Neural Computation,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}, {"title": "Semi-supervised learning literature survey", "author": ["Zhu", "Xiaojin"], "venue": "Technical Report 1530,", "citeRegEx": "Zhu and Xiaojin.,? \\Q2005\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2005}], "referenceMentions": [{"referenceID": 5, "context": ", 2000), semi-supervised (Zhu, 2005), weakly-supervised (Dietterich et al., 1997), and reinforcement (Sutton & Barto, 1998) which also point to promising directions when annotated data is limited.", "startOffset": 56, "endOffset": 81}, {"referenceID": 32, "context": "Existing classifiers, such as decision trees (Quinlan, 1996), support vector machines (Vapnik, 1995), neural networks (LeCun et al.", "startOffset": 45, "endOffset": 60}, {"referenceID": 23, "context": "Existing classifiers, such as decision trees (Quinlan, 1996), support vector machines (Vapnik, 1995), neural networks (LeCun et al., 1989), boosting (Freund & Schapire, 1997), and random forests (Breiman, 2001), carry out training processes to either implicitly or explicitly minimize an objective function that balances the training error and regularization.", "startOffset": 118, "endOffset": 138}, {"referenceID": 21, "context": "Recent studies reveal that even modern classifiers like deep convolutional neural networks (Krizhevsky et al., 2012) still make mistakes that look absurd to humans (Goodfellow et al.", "startOffset": 91, "endOffset": 116}, {"referenceID": 29, "context": "Different types of approaches have been proposed in the past including bootstrapping (Mooney et al., 1993), active learning (Settles, 2010), semi-supervised learning (Zhu, 2005), and data augmentation (Krizhevsky et al.", "startOffset": 85, "endOffset": 106}, {"referenceID": 21, "context": ", 1993), active learning (Settles, 2010), semi-supervised learning (Zhu, 2005), and data augmentation (Krizhevsky et al., 2012).", "startOffset": 102, "endOffset": 127}, {"referenceID": 7, "context": "In the past, attempts have been made to build connections between generative models and discriminative classifiers (Friedman et al., 2001; Liang & Jordan, 2008; Jebara, 2012).", "startOffset": 115, "endOffset": 174}, {"referenceID": 39, "context": "Many existing approaches however combine discriminative classifiers with generative models to form hybrid models (Tu et al., 2008).", "startOffset": 113, "endOffset": 130}, {"referenceID": 42, "context": "In (Welling et al., 2002) a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additional self-generated negative samples; the generative discriminative modeling work in (Tu, 2007) generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.", "startOffset": 3, "endOffset": 25}, {"referenceID": 42, "context": "Inspired by the work that learns density functions (Welling et al., 2002) and generative models (Tu, 2007) using sequentially self-generated pseudo-negative samples, as well as recent success in deep learning (LeCun et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 23, "context": ", 2002) and generative models (Tu, 2007) using sequentially self-generated pseudo-negative samples, as well as recent success in deep learning (LeCun et al., 1989; Hinton et al., 2006; Krizhevsky et al., 2012; Gatys et al., 2015), we propose here an algorithm that capitalizes on the endto-end learning of deep convolutional nets while making it internally generative.", "startOffset": 143, "endOffset": 229}, {"referenceID": 15, "context": ", 2002) and generative models (Tu, 2007) using sequentially self-generated pseudo-negative samples, as well as recent success in deep learning (LeCun et al., 1989; Hinton et al., 2006; Krizhevsky et al., 2012; Gatys et al., 2015), we propose here an algorithm that capitalizes on the endto-end learning of deep convolutional nets while making it internally generative.", "startOffset": 143, "endOffset": 229}, {"referenceID": 21, "context": ", 2002) and generative models (Tu, 2007) using sequentially self-generated pseudo-negative samples, as well as recent success in deep learning (LeCun et al., 1989; Hinton et al., 2006; Krizhevsky et al., 2012; Gatys et al., 2015), we propose here an algorithm that capitalizes on the endto-end learning of deep convolutional nets while making it internally generative.", "startOffset": 143, "endOffset": 229}, {"referenceID": 8, "context": ", 2002) and generative models (Tu, 2007) using sequentially self-generated pseudo-negative samples, as well as recent success in deep learning (LeCun et al., 1989; Hinton et al., 2006; Krizhevsky et al., 2012; Gatys et al., 2015), we propose here an algorithm that capitalizes on the endto-end learning of deep convolutional nets while making it internally generative.", "startOffset": 143, "endOffset": 229}, {"referenceID": 42, "context": "To compare with the self-supervised boosting work (Welling et al., 2002), ICL has its advantage in: a).", "startOffset": 50, "endOffset": 72}, {"referenceID": 37, "context": "Other methods (Gutmann & Hyv\u00e4rinen, 2010; Tolstikhin et al., 2017) in which generative models are learned using discriminative classifiers share some common disadvantages as in (Welling et al.", "startOffset": 14, "endOffset": 66}, {"referenceID": 42, "context": ", 2017) in which generative models are learned using discriminative classifiers share some common disadvantages as in (Welling et al., 2002; Tu, 2007).", "startOffset": 118, "endOffset": 150}, {"referenceID": 7, "context": "Previous algorithms connecting generative modeling with discriminative classification (Friedman et al., 2001; Liang & Jordan, 2008; Tu et al., 2008; Jebara, 2012) fall in the category of hybrid models that are combinations of the two and mostly limited to specific tasks.", "startOffset": 86, "endOffset": 162}, {"referenceID": 39, "context": "Previous algorithms connecting generative modeling with discriminative classification (Friedman et al., 2001; Liang & Jordan, 2008; Tu et al., 2008; Jebara, 2012) fall in the category of hybrid models that are combinations of the two and mostly limited to specific tasks.", "startOffset": 86, "endOffset": 162}, {"referenceID": 29, "context": "Other directions in machine learning such as bootstrapping (Mooney et al., 1993), active learning (Settles, 2010; Beygelzimer et al.", "startOffset": 59, "endOffset": 80}, {"referenceID": 21, "context": ", 2009), semi-supervised learning (Zhu, 2005), and data augmentation (Krizhevsky et al., 2012), try to effectively utilize the existing data whereas ICL is able to self-generate new data.", "startOffset": 69, "endOffset": 94}, {"referenceID": 33, "context": ", 2014a) engages two separate models, a generator and a discriminator, with the objective of making use of the discriminator to help the generator generate faithful samples; the discriminator in GAN is not meant to perform the generic two-class/multi-class classification task; thus, some special settings for semi-supervised learning (Goodfellow et al., 2014a; Radford et al., 2015; Zhao et al., 2016; Brock et al., 2016; Salimans et al., 2016) were created since the discriminators in GAN were trained to classify between \u201creal\u201d and \u201cfake\u201d samples.", "startOffset": 335, "endOffset": 445}, {"referenceID": 46, "context": ", 2014a) engages two separate models, a generator and a discriminator, with the objective of making use of the discriminator to help the generator generate faithful samples; the discriminator in GAN is not meant to perform the generic two-class/multi-class classification task; thus, some special settings for semi-supervised learning (Goodfellow et al., 2014a; Radford et al., 2015; Zhao et al., 2016; Brock et al., 2016; Salimans et al., 2016) were created since the discriminators in GAN were trained to classify between \u201creal\u201d and \u201cfake\u201d samples.", "startOffset": 335, "endOffset": 445}, {"referenceID": 2, "context": ", 2014a) engages two separate models, a generator and a discriminator, with the objective of making use of the discriminator to help the generator generate faithful samples; the discriminator in GAN is not meant to perform the generic two-class/multi-class classification task; thus, some special settings for semi-supervised learning (Goodfellow et al., 2014a; Radford et al., 2015; Zhao et al., 2016; Brock et al., 2016; Salimans et al., 2016) were created since the discriminators in GAN were trained to classify between \u201creal\u201d and \u201cfake\u201d samples.", "startOffset": 335, "endOffset": 445}, {"referenceID": 34, "context": ", 2014a) engages two separate models, a generator and a discriminator, with the objective of making use of the discriminator to help the generator generate faithful samples; the discriminator in GAN is not meant to perform the generic two-class/multi-class classification task; thus, some special settings for semi-supervised learning (Goodfellow et al., 2014a; Radford et al., 2015; Zhao et al., 2016; Brock et al., 2016; Salimans et al., 2016) were created since the discriminators in GAN were trained to classify between \u201creal\u201d and \u201cfake\u201d samples.", "startOffset": 335, "endOffset": 445}, {"referenceID": 33, "context": "Later development alongside GAN (Radford et al., 2015; Salimans et al., 2016; Zhao et al., 2016; Brock et al., 2016) share some similar aspects to GAN, which also do not achieve the same goal as ICL does.", "startOffset": 32, "endOffset": 116}, {"referenceID": 34, "context": "Later development alongside GAN (Radford et al., 2015; Salimans et al., 2016; Zhao et al., 2016; Brock et al., 2016) share some similar aspects to GAN, which also do not achieve the same goal as ICL does.", "startOffset": 32, "endOffset": 116}, {"referenceID": 46, "context": "Later development alongside GAN (Radford et al., 2015; Salimans et al., 2016; Zhao et al., 2016; Brock et al., 2016) share some similar aspects to GAN, which also do not achieve the same goal as ICL does.", "startOffset": 32, "endOffset": 116}, {"referenceID": 2, "context": "Later development alongside GAN (Radford et al., 2015; Salimans et al., 2016; Zhao et al., 2016; Brock et al., 2016) share some similar aspects to GAN, which also do not achieve the same goal as ICL does.", "startOffset": 32, "endOffset": 116}, {"referenceID": 34, "context": "For example, an additional \u201cnot-real\u201d class was generated in addition to the standard k classes in multi-class classification for a semisupervised learning setting in (Salimans et al., 2016).", "startOffset": 167, "endOffset": 190}, {"referenceID": 47, "context": "Other generative modeling schemes such as the MiniMax entropy theory (Zhu et al., 1997), inducing features (Della Pietra et al.", "startOffset": 69, "endOffset": 87}, {"referenceID": 16, "context": ", 1997), auto-encoder (Baldi, 2012), Wake-sleep (Hinton et al., 1995), and recent CNN based generative modeling approaches (Xie et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 23, "context": "a boosting algorithm (Freund & Schapire, 1997) or convolutional neural networks (LeCun et al., 1989) to learn p(y = +1|x), which is always an approximation due to various reasons including insufficient training samples, generalization error, and classifier limitations.", "startOffset": 80, "endOffset": 100}, {"referenceID": 21, "context": "Previous attempts to improve classification by data augmentation were mostly done to increase the positive samples (Krizhevsky et al., 2012; Goodfellow et al., 2014b); we instead argue the importance of increasing negative samples to improve the classification performance.", "startOffset": 115, "endOffset": 166}, {"referenceID": 30, "context": "Motivated by the DeepDream code (Mordvintsev et al., 2015) and Neural Artistic Style work (Gatys et al.", "startOffset": 32, "endOffset": 58}, {"referenceID": 8, "context": ", 2015) and Neural Artistic Style work (Gatys et al., 2015), we update a random sample x drawn from pr (x) by increasing qt(y=+1|x;Wt) qt(y=\u22121|x;Wt) using backpropagation.", "startOffset": 39, "endOffset": 59}, {"referenceID": 3, "context": "Building the connection between SGD and MCMC is an active area in machine learning (Welling & Teh, 2011; Chen et al., 2014; Mandt et al., 2017).", "startOffset": 83, "endOffset": 143}, {"referenceID": 28, "context": "Building the connection between SGD and MCMC is an active area in machine learning (Welling & Teh, 2011; Chen et al., 2014; Mandt et al., 2017).", "startOffset": 83, "endOffset": 143}, {"referenceID": 28, "context": "A recent work (Mandt et al., 2017) further shows the similarity between constant SGD and MCMC, along with analysis of SGD using momentum updates.", "startOffset": 14, "endOffset": 34}, {"referenceID": 43, "context": "Our progressively learned discriminative classifier can be viewed as carving out the feature space on \u03c6(x), which essentially becomes a equivalent class for the positives; the volume of the equivalent class that satisfies the condition is exponentially large, as analyzed in (Wu et al., 2000).", "startOffset": 275, "endOffset": 292}, {"referenceID": 34, "context": "We perform direct multi-class classification where the parameter setting is identical to a standard multi-class classification in CNN whereas an additional \u201cnot-real\u201d class is created in (Salimans et al., 2016).", "startOffset": 187, "endOffset": 210}, {"referenceID": 27, "context": "LeakyReLU activations (Maas et al., 2013) are used after each convolutional layer.", "startOffset": 22, "endOffset": 41}, {"referenceID": 33, "context": ", 2014a) and DCGAN (Radford et al., 2015) show results for unsupervised learning and semi-supervised classification.", "startOffset": 19, "endOffset": 41}, {"referenceID": 33, "context": "To compare with DCGAN (Radford et al., 2015), we follow the same procedure: each generator trained by DCGAN (Radford et al.", "startOffset": 22, "endOffset": 44}, {"referenceID": 33, "context": ", 2015), we follow the same procedure: each generator trained by DCGAN (Radford et al., 2015) using tensorflow implementation (Kim, 2016) was used to generate positive samples, which are then augmented to the negative set to train the individual one-vs-all CNN classifiers (also using an identical CNN architecture to ICL), which are combined to create the overall multi-class classifier.", "startOffset": 71, "endOffset": 93}, {"referenceID": 33, "context": "As the supervised learning task was not directly specified in DCGAN (Radford et al., 2015), some care is needed to design the optimal setting to utilize the generated samples from DCGAN in the two-step approach (we may not get the best setting in the experiments).", "startOffset": 68, "endOffset": 90}, {"referenceID": 31, "context": "The SVHN (Netzer et al., 2011) dataset consists of color images of house numbers collected by Google Street View.", "startOffset": 9, "endOffset": 30}], "year": 2017, "abstractText": "In this paper we propose introspective classifier learning (ICL) that emphasizes the importance of having a discriminative classifier empowered with generative capabilities. We develop a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our classifier is able to iteratively: (1) synthesize pseudo-negative samples in the synthesis step; and (2) enhance itself by improving the classification in the reclassification step. The single classifier learned is at the same time generative \u2014 being able to directly synthesize new samples within its own discriminative model. We conduct experiments on standard benchmark datasets including MNIST, CIFAR, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.", "creator": "LaTeX with hyperref package"}}}