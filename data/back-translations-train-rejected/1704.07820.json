{"id": "1704.07820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Introspective Generative Modeling: Decide Discriminatively", "abstract": "We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. IGM learns a cascade of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and semi-supervised learning.", "histories": [["v1", "Tue, 25 Apr 2017 17:57:33 GMT  (3380kb,D)", "http://arxiv.org/abs/1704.07820v1", "10 pages, 9 figures"]], "COMMENTS": "10 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["justin lazarow", "long jin", "zhuowen tu"], "accepted": false, "id": "1704.07820"}, "pdf": {"name": "1704.07820.pdf", "metadata": {"source": "CRF", "title": "Introspective Generative Modeling: Decide Discriminatively", "authors": ["Justin Lazarow", "Long Jin", "Zhuowen Tu"], "emails": ["jlazarow@ucsd.edu", "lojin@ucsd.edu", "ztu@ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "There are some well-known typologies that include SVM [42], where enhancing learning is still one of the most difficult problems in machine learning, but has a bright future as a large number of tasks have little to no supervision. Popular uncontrolled learning methods include blending models [10], principle component analysis (PCA) [22], spectral clustering [37], subject modeling [4], and autocoders [3, 2]. In a nutshell, uncontrolled learning techniques are largely guided through the minimum description length (MDL) to reconstruct the data, while supervised learning methods are prioritized error metrics to minimize the best adaptation to adaptation."}, {"heading": "2. Significance and related work", "text": "In fact, most of them are able to trump themselves, both in terms of themselves and in terms of themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves. (...) Most of them have trumped themselves."}, {"heading": "3. Method", "text": "In the following, we describe the introspective generative modelling algorithm (IGM). First, we discuss the main formulation, which has some similarity to GDL [39]. However, with the replacement of the boosting algorithm [12] by revolutionary neural networks [25], IGM shows significant improvements over GDL in terms of both modelling and computing power. The increased modelling power comes mainly from CNNs due to their end-to-end learning by automatic feature learning and tuning in the back propagation to the network parameters; the increased computing power also largely comes from CNNs due to a natural implementation of sampling by back propagation to the input image. The GDL is similar to the IGM (see fig. 3), but IGM-single (see fig. 4) maintains a single CNN as opposed to a sequence of classifiers in both the GDL and the IGM. We motivate the formulation of the IGM, similar to the IGM theory."}, {"heading": "3.1. Formulation", "text": "We start the discussion by borrowing the notation from [39]. Suppose we get a set of training images (patches = = 1 = = = = = = Sample (patches): S = {xi, i = 1.. n}. We apply the pseudo-negative concept defined in [39] and define the class names y \u2212 \u2212 1, + 1 \u2212 \u2212 \u2212 p, indicating that x is a negative or positive sample. Here, we assume that the positive samples y = 1 are the patterns / targets we want to examine. A generative model calculates for p (y, x) = p (x) = p (y) p (y) p (y), which captures the underlying generation process of class y. A discriminatory classifier calculates p (y) instead. Under the Bayes rule, we calculate similarly to the motivation in [39]: p (x | y) = p (y = 1) p (y = p (y) p (y) p (p =) p (y))."}, {"heading": "3.2.1 Classification-step", "text": "The classification step can be regarded as training a normal classifier on the training set S + VP St \u2212 where S + = {(xi, yi = + 1), i = 1.. n}. St \u2212 = {(xi, \u2212 1), i = 1,..., l} for t \u2265 1. We use a CNN as the basic classifier. When training a classifier Ct to S + VP St \u2212 we designate the parameters to be learned in Ct by a high-dimensional vector Wt = (w (0) t, w (1) t), which could consist of millions of parameters. w (1) t denotes the weights on the top layer that combine the features."}, {"heading": "3.2.2 Synthesis-step", "text": "In the classification we get qt (y; Wt) qt (y; Wt), which is then used to update p \u2212 t (x) according to Eq. (6): p \u2212 t (x) = t \u00b2 a = 1 Za qa (y = 1 | x). The sampling process is done by back propagation, but now we have to go through a sequence classification by taking 1Za qa (y = 1 | x; Wa) qa = \u2212 x; Wa), a = 1.. This can be time consuming. In practice, we can simply reverse propagate to the previous classification St \u2212 1 \u2212 by taking 1 Zt qt (y = 1; Wt) qt (y = 1 x) qt."}, {"heading": "3.3. An alternative: IGM-single", "text": "We briefly introduce the IGM individual algorithm, which is similar to the introspective classifier learning algorithm [21], with the difference without the presence of negative input samples. IGM-single's pipeline is in Fig. 4. An essential aspect of this is that we maintain a single CNN classifier throughout the learning process. In the classification step, we obtain qt (y | x; Wt) (similar to Eq. 8), which is then used to update p \u2212 t (x) according to Eq. (13): p \u2212 t (x) = 1Zt qt (y = + 1 | x; Wt) qt (y = \u2212 1 | x; Wt) p \u2212 0 (x). (13) In the synthesis step, we extract samples from p \u2212 t (x): p \u2212 t (x). Overall model The entire IGM individual model according to T training stages becomes: p \u2212 T (0) > Zw (2) (T & T) (1)."}, {"heading": "3.4. Model-based anysize-image-generation", "text": "In practice, we add stochasticity and efficiency to the synthesis process by randomly sampling these patches, where gt (x (i, j) (15), where gt (x (i, j))) (see Equation 10) indicates the score of the patch's size, e.g. 64 \u00d7 64 for x (i, j) under the discriminator in round. Fig. 5 shows an illustration for a round of sampling. This allows us to synthesize much larger images by being able to enhance coherence and interactions around a particular pixel."}, {"heading": "4. Experiments", "text": "In each method, we use the discriminator architecture of [33], which includes an input size of 64x64x3 in the RGB color space, four revolutionary layers with 5 x 5 core sizes, using layers 64, 128, 256 and 512 channels. We include batch normalization after each revolutionary layer (except the first) and use leaky ReLU activations with a leak tendency of 0.2. The classification layer flattens out the input and finally feeds it into a sigmoid activation, which serves as a distinguishing criterion for the 64 x 64 fields we extract from the training images. Note that this is a general purpose architecture with no changes for a particular task in the head. In texture synthesis and in artistic style, we use the \"arbitrary-sized image generation\" by adding a \"header\" to the network, which makes random changes to a particular task in the head each time the network is passed."}, {"heading": "4.1. Texture synthesis", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he played the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\""}, {"heading": "4.2. Artistic style transfer", "text": "We also try to transfer the artistic style shown in [14]. However, our architecture does not use any additional networks for the transfer of content and textures. The task uses a loss function during synthesis to minimize \u2212 ln p (Istyle | I) \u03b1 \u00b7 | | Istyle \u2212 I | | 2 \u2212 (1 \u2212 \u03b1) \u00b7 ln p \u2212 style (Istyle), where I am an input image and Istyle is its stylized version, and p \u2212 style (I) denotes the model learned from the training style image. During synthesis, we use an L2 loyalty term weighted by a parameter \u03b1, which does not make Istyle too far away from the input image I. We choose \u03b1 = 0.3 and calculate the L2 difference between the original content image and the current stylized image at each step of synthesis. Two examples of artistic style transfer are shown in Figure 7."}, {"heading": "4.3. Face modeling", "text": "The CelebA dataset [28] is used in our face modeling experiment, which consists of 202,599 face images. As positive examples, we cut the mean 64 x 64 fields in these images. For the classification step, we use the stochastic gradient descent with learning rate 0.01 and a stack size of 100 images containing 50 random positives and 50 random negatives. For the synthesis step, we use the Adam optimizer with learning rate 0.01 and \u03b2 = 0.5 and stop early when the pseudo-negatives exceed the decision limit. In Fig. 8, we show some face examples generated by our model and the DCGAN model."}, {"heading": "4.4. SVHN unsupervised learning", "text": "The SVHN [31] dataset consists of color images of house numbers collected by Google Street View. The training set consists of 73 257 images, the additional set consists of 531 131 images and the test set of 26 032 images. The images are 32 x 32 in size. We combine the training and the additional set as our positive examples of unattended learning. Following the same settings in the face modeling experiments, our IGM model can generate examples as shown in Fig. 9."}, {"heading": "4.5. SVHN semi-supervised classification", "text": "We perform the semi-supervised classification experiment according to the procedure outlined in [33]. First, we train a model on the SVHN training and the additional set in an unsupervised manner as described in Section 4.4. Subsequently, we train an L2-SVM on the learned representations of this model. The characteristics of the last three revolutionary layers are linked to a 14336-dimensional trait vector. From the training set, a 10,000-fold validation set is extracted, which is used for model selection. The SVM classifier is trained on 1000 randomly selected examples from the rest of the training set. The test error rate is averaged over 100 different SVMs trained on random 1000 examples. In the same setting, our IGM model achieves the test error rate of 36.44 \u00b1 0.72% and the DCGAN model reaches 33.13 \u00b1 0.83% (we conducted the DCGAN code [23] based on an identical setting for an IGM training])."}, {"heading": "5. Conclusion", "text": "Introspective generative modeling points in an encouraging direction for unattended image modeling, harnessing the power of discriminatory deep-revolutionary neural networks, and can be used to address a wide range of problems in computer vision and machine learning."}, {"heading": "6. Acknowledgement", "text": "This work is supported by NSF IIS-1618477 and a grant from Northrop Grumman Contextual Robotics. We thank Saining Xie, Jun-Yan Zhu, Jiajun Wu, Stella Yu and Alexei Efros for helpful discussions."}], "references": [{"title": "Wasserstein gan", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["P. Baldi"], "venue": "ICML unsupervised and transfer learning, 27,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Scaling learning algorithms towards ai", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Large-scale kernel machines, 34(5),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, 3(Jan):993\u2013 1022,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural photo editing with introspective adversarial networks", "author": ["A. Brock", "T. Lim", "J. Ritchie", "N. Weston"], "venue": "arXiv preprint arXiv:1609.07093,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G. Hinton"], "venue": "AISTATS, volume 10, pages 33\u201340. Citeseer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 19(4):380\u2013393,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "2nd edition,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Texture synthesis by nonparametric sampling", "author": ["A.A. Efros", "T.K. Leung"], "venue": "ICCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. of Comp. and Sys. Sci., 55(1),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "The elements of statistical learning, volume 1", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Springer series in statistics,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "NIPS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "General pattern theory-A mathematical study of regular structures", "author": ["U. Grenander"], "venue": "Clarendon Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Pyramid-based texture analysis/synthesis", "author": ["D.J. Heeger", "J.R. Bergen"], "venue": "SIGGRAPH, pages 229\u2013238,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "The\u201d wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": "Science, 268(5214):1158,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation, 18:1527\u2013 1554,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning: discriminative and generative", "author": ["T. Jebara"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Introspective classifier learning: Empower generatively", "author": ["L. Jin", "J. Lazarow", "Z. Tu"], "venue": "arXiv preprint arXiv:,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "DCGAN-tensorflow", "author": ["T. Kim"], "venue": "https://github.com/ carpedm20/DCGAN-tensorflow,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1989}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Monte Carlo strategies in scientific computing", "author": ["J.S. Liu"], "venue": "Springer Science & Business Media,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "ICCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient descent as approximate bayesian inference", "author": ["S. Mandt", "M.D. Hoffman", "D.M. Blei"], "venue": "arXiv preprint arXiv:1704.04289,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2017}, {"title": "Deepdream - a code example for visualizing neural networks", "author": ["A. Mordvintsev", "C. Olah", "M. Tyka"], "venue": "Google Research,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["J. Portilla", "E.P. Simoncelli"], "venue": "Int\u2019l j. of computer vision, 40(1):49\u201370,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv:1511.06434,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling by shortest data description", "author": ["J. Rissanen"], "venue": "Automatica, 14(5):465\u2013471,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1978}, {"title": "Fields of experts: A framework for learning image priors", "author": ["S. Roth", "M.J. Black"], "venue": "CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "PAMI, 22(8):888\u2013905,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Adagan: Boosting generative models", "author": ["I. Tolstikhin", "S. Gelly", "O. Bousquet", "C.-J. Simon-Gabriel", "B. Sch\u00f6lkopf"], "venue": "arXiv:1701.02386,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning generative models via discriminative approaches", "author": ["Z. Tu"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Brain anatomical structure segmentation by hybrid discriminative/generative models", "author": ["Z. Tu", "K.L. Narr", "P. Doll\u00e1r", "I. Dinov", "P.M. Thompson", "A.W. Toga"], "venue": "IEEE Tran. on Medical Imag.,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Texture networks: Feed-forward synthesis of textures and stylized images", "author": ["D. Ulyanov", "V. Lebedev", "A. Vedaldi", "V. Lempitsky"], "venue": "ICML,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "The nature of statistical learning theory", "author": ["V.N. Vapnik"], "venue": "Springer-Verlag New York, Inc.,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1995}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "ICML,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Self supervised boosting", "author": ["M. Welling", "R.S. Zemel", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Equivalence of julesz ensembles and frame models", "author": ["Y.N. Wu", "S.C. Zhu", "X. Liu"], "venue": "International Journal of Computer Vision, 38(3),", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2000}, {"title": "Cooperative training of descriptor and generator networks", "author": ["J. Xie", "Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "arXiv:1609.09408,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "A theory of generative convnet", "author": ["J. Xie", "Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "ICML,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature extraction from faces using deformable templates", "author": ["A.L. Yuille", "P.W. Hallinan", "D.S. Cohen"], "venue": "International journal of computer vision, 8(2):99\u2013111,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1992}, {"title": "Vision as bayesian inference: analysis by synthesis", "author": ["A.L. Yuille", "D. Kersten"], "venue": "Trends in cognitive sciences,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "Energy-based generative adversarial network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "arXiv:1609.03126,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimax entropy principle and its application to texture modeling", "author": ["S.C. Zhu", "Y.N. Wu", "D. Mumford"], "venue": "Neural Computation, 9(8):1627\u20131660,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 41, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 187, "endOffset": 191}, {"referenceID": 4, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 208, "endOffset": 211}, {"referenceID": 24, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 247, "endOffset": 251}, {"referenceID": 9, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 21, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 102, "endOffset": 106}, {"referenceID": 36, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 1, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 33, "context": "In a nutshell, unsupervised learning techniques are mostly guided by the minimum description length principle (MDL) [34] to best reconstruct the data whereas supervised learning methods are primarily driven by minimizing error metrics to best fit the input labeling.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "a much harder task than discriminative learning [13], due to its intrinsic learning complexity, as well many assumptions and simplifications made about the underlying models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 25, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 39, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 19, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 23, "context": "In the presence of supervised information with a large amount of data, a discriminative classifier [24] exhibits superior capability in making robust classification by learning rich and informative representations; unsupervised generative models do not require supervision but at a price of relying on assumptions that are often too ideal in dealing with problems of real-world complexity.", "startOffset": 99, "endOffset": 103}, {"referenceID": 43, "context": "Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation [44] and image modeling [39].", "startOffset": 127, "endOffset": 131}, {"referenceID": 38, "context": "Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation [44] and image modeling [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 32, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 35, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 0, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 43, "context": "In [44], a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples; the generative discriminative modeling work (GDL) in [39] generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "In [44], a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples; the generative discriminative modeling work (GDL) in [39] generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.", "startOffset": 243, "endOffset": 247}, {"referenceID": 50, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 50, "endOffset": 62}, {"referenceID": 43, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 50, "endOffset": 62}, {"referenceID": 38, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 50, "endOffset": 62}, {"referenceID": 24, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 112, "endOffset": 124}, {"referenceID": 23, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 112, "endOffset": 124}, {"referenceID": 13, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 112, "endOffset": 124}, {"referenceID": 20, "context": "Supervised classification cases of an algorithm in the same introspective learning family can been seen in [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 50, "context": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44].", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44].", "startOffset": 184, "endOffset": 188}, {"referenceID": 43, "context": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44].", "startOffset": 233, "endOffset": 237}, {"referenceID": 24, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 38, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 187, "endOffset": 191}, {"referenceID": 29, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 256, "endOffset": 260}, {"referenceID": 38, "context": "The general pipeline of IGM is similar to that of GDL [39], with the boosting algorithm used in [39] is replaced by a CNN in IGM.", "startOffset": 54, "endOffset": 58}, {"referenceID": 38, "context": "The general pipeline of IGM is similar to that of GDL [39], with the boosting algorithm used in [39] is replaced by a CNN in IGM.", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d).", "startOffset": 30, "endOffset": 38}, {"referenceID": 13, "context": "More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d).", "startOffset": 30, "endOffset": 38}, {"referenceID": 38, "context": "More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d).", "startOffset": 114, "endOffset": 118}, {"referenceID": 38, "context": "Next, we review some existing generative image modeling work, followed by detailed discussions about the two most related algorithms: GDL [39] and the recent development of generative adversarial networks (GAN) [15].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "Next, we review some existing generative image modeling work, followed by detailed discussions about the two most related algorithms: GDL [39] and the recent development of generative adversarial networks (GAN) [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 15, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 47, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 8, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 174, "endOffset": 177}, {"referenceID": 17, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 190, "endOffset": 194}, {"referenceID": 50, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 223, "endOffset": 227}, {"referenceID": 34, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 250, "endOffset": 254}, {"referenceID": 48, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 272, "endOffset": 276}, {"referenceID": 18, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 299, "endOffset": 303}, {"referenceID": 46, "context": "Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41].", "startOffset": 78, "endOffset": 82}, {"referenceID": 45, "context": "Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41].", "startOffset": 147, "endOffset": 155}, {"referenceID": 40, "context": "Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41].", "startOffset": 147, "endOffset": 155}, {"referenceID": 13, "context": "The neural artistic transferring work [14] has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused [14] on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "The neural artistic transferring work [14] has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused [14] on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework.", "startOffset": 151, "endOffset": 155}, {"referenceID": 43, "context": "The self-supervised boosting work [44] sequentially learns weak classifiers under boosting [12] for density estimation, but its modeling power was not adequately demonstrated.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "The self-supervised boosting work [44] sequentially learns weak classifiers under boosting [12] for density estimation, but its modeling power was not adequately demonstrated.", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "Relationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution.", "startOffset": 22, "endOffset": 26}, {"referenceID": 38, "context": "Relationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Relationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution.", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "Comparison with GAN [15] The recent development of generative adversarial neural networks [15] is very interesting and also highly related to IGM.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "Comparison with GAN [15] The recent development of generative adversarial neural networks [15] is very interesting and also highly related to IGM.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 32, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 49, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 37, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 0, "context": "Due to the internal competition between the generator and the discriminator, GAN is known to be hard to train [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 20, "context": "Relationship with ICL [21] The Introspective Classifier Learning work (ICL) [21] is a sister paper to IGM, with ICL focusing on the discriminator side emphasizing its classification power.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "Relationship with ICL [21] The Introspective Classifier Learning work (ICL) [21] is a sister paper to IGM, with ICL focusing on the discriminator side emphasizing its classification power.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "A number of important image modeling tasks, including texture modeling, style transferring, face modeling, and semi-supervised learning are demonstrated here, which are not covered in ICL [21].", "startOffset": 188, "endOffset": 192}, {"referenceID": 38, "context": "We discuss the main formulation first, which bears some level of similarity to GDL [39].", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "However, with the replacement of the boosting algorithm [12] by convolutional neural networks [25], IGM demonstrates significant improvement over GDL in terms of both modeling and computational power.", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "However, with the replacement of the boosting algorithm [12] by convolutional neural networks [25], IGM demonstrates significant improvement over GDL in terms of both modeling and computational power.", "startOffset": 94, "endOffset": 98}, {"referenceID": 38, "context": "Formulation We start the discussion by borrowing notation from [39].", "startOffset": 63, "endOffset": 67}, {"referenceID": 38, "context": "We adopt the pseudo-negative concept defined in [39] and define class labels y \u2208 {\u22121,+1}, indicating x being a negative or a positive sample.", "startOffset": 48, "endOffset": 52}, {"referenceID": 38, "context": "Under the Bayes rule, similar to the motivation in [39]:", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 63, "endOffset": 67}, {"referenceID": 40, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 85, "endOffset": 89}, {"referenceID": 38, "context": "In the GDL algorithm [39], a solution was given to learning p(x|y = +1) by using an iterative process starting from an initial reference distribution of the negatives p0 (x), e.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "The samples drawn from x \u223c pt (x) are called pseudo-negatives, following a definition in [39].", "startOffset": 89, "endOffset": 93}, {"referenceID": 42, "context": "Additional Gaussian noise can be added to the stochastic gradient as in [43] but we did not observe a big difference in the quality of samples in practice.", "startOffset": 72, "endOffset": 76}, {"referenceID": 44, "context": "This is probably due to the equivalent class [45] where the probability mass is widely distributed over an extremely large image space.", "startOffset": 45, "endOffset": 49}, {"referenceID": 38, "context": "Sampling strategies In [39], various Markov chain Monte Carlo techniques [27] including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "Sampling strategies In [39], various Markov chain Monte Carlo techniques [27] including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow.", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "Motivated by the DeepDream code [30] and Neural Artistic Style work [14], we perform stochastic gradient descent via backpropagation in synthesis.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Motivated by the DeepDream code [30] and Neural Artistic Style work [14], we perform stochastic gradient descent via backpropagation in synthesis.", "startOffset": 68, "endOffset": 72}, {"referenceID": 42, "context": "Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 7, "context": "Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 28, "context": "Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 6, "context": "We found early-stopping effective and efficient, which can be viewed as contrastive divergence [7] where a short Markov chain is simulated.", "startOffset": 95, "endOffset": 98}, {"referenceID": 42, "context": "A noise can be injected as in [43] when performing SGD sampling.", "startOffset": 30, "endOffset": 34}, {"referenceID": 38, "context": "IGM shares a similar cascade aspect with GDL [39] where the convergence of this iterative learning process to the target distribution was shown by the following theorem in [39].", "startOffset": 45, "endOffset": 49}, {"referenceID": 38, "context": "IGM shares a similar cascade aspect with GDL [39] where the convergence of this iterative learning process to the target distribution was shown by the following theorem in [39].", "startOffset": 172, "endOffset": 176}, {"referenceID": 20, "context": "We briefly present the IGM-single algorithm, which is similar to the introspective classifier learning algorithm [21] with the difference without the presence of input negative samples.", "startOffset": 113, "endOffset": 117}, {"referenceID": 32, "context": "In each method, we adopt the discriminator architecture of [33] which involves an input size of 64x64x3 in the RGB colorspace, four convolutional layers using 5 \u00d7 5 kernel sizes with the layers using 64, 128, 256 and 512 channels, respectively.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "[14] and Texture Nets [41] results are from [41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[14] and Texture Nets [41] results are from [41].", "startOffset": 22, "endOffset": 26}, {"referenceID": 40, "context": "[14] and Texture Nets [41] results are from [41].", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 50, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 10, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 31, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 50, "context": "Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11].", "startOffset": 55, "endOffset": 63}, {"referenceID": 45, "context": "Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11].", "startOffset": 55, "endOffset": 63}, {"referenceID": 10, "context": "Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "We train similar textures to [41].", "startOffset": 29, "endOffset": 33}, {"referenceID": 40, "context": "2 and 6, we see that IGM generates images of similar quality to [41], however, it is usually more faithful to the structure of the input images.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Artistic style transfer We also attempt to transfer artistic style as shown in [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "The CelebA dataset [28] is used in our face modeling experiment, which consists of 202, 599 face images.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 115, "endOffset": 119}, {"referenceID": 30, "context": "The SVHN [31] dataset consists of color images of house numbers collected by Google Street View.", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "We perform the semi-supervised classification experiment by following the procedure outlined in [33].", "startOffset": 96, "endOffset": 100}, {"referenceID": 22, "context": "83% (we ran the DCGAN code [23] in an identical setting as IGM for a fair comparison since the result reported in [33] was achieved by training on the ImageNet dataset).", "startOffset": 27, "endOffset": 31}, {"referenceID": 32, "context": "83% (we ran the DCGAN code [23] in an identical setting as IGM for a fair comparison since the result reported in [33] was achieved by training on the ImageNet dataset).", "startOffset": 114, "endOffset": 118}], "year": 2017, "abstractText": "We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. IGM learns a cascade of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and semisupervised learning. 1", "creator": "LaTeX with hyperref package"}}}