{"id": "1704.07911", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car", "abstract": "As part of a complete software stack for autonomous driving, NVIDIA has created a neural-network-based system, known as PilotNet, which outputs steering angles given images of the road ahead. PilotNet is trained using road images paired with the steering angles generated by a human driving a data-collection car. It derives the necessary domain knowledge by observing human drivers. This eliminates the need for human engineers to anticipate what is important in an image and foresee all the necessary rules for safe driving. Road tests demonstrated that PilotNet can successfully perform lane keeping in a wide variety of driving conditions, regardless of whether lane markings are present or not.", "histories": [["v1", "Tue, 25 Apr 2017 21:25:41 GMT  (3943kb,D)", "http://arxiv.org/abs/1704.07911v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE cs.RO", "authors": ["mariusz bojarski", "philip yeres", "anna choromanska", "krzysztof choromanski", "bernhard firner", "lawrence jackel", "urs muller"], "accepted": false, "id": "1704.07911"}, "pdf": {"name": "1704.07911.pdf", "metadata": {"source": "CRF", "title": "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car", "authors": ["Mariusz Bojarski"], "emails": ["64@1x18", "64@3x20", "48@5x22", "36@14x47", "24@31x98", "3@66x200"], "sections": [{"heading": "1 Introduction", "text": "An earlier report [1] described an end-to-end learning system for self-driving cars in which a Convolutionary Neural Network (CNN) [2] was trained to output steering angles based on input images of the road in front of us. This system is now called PilotNet. The training data was images of a front-facing camera in a data-gathering car in conjunction with the time-synchronized steering angle recorded by a human driver. PilotNet's motivation was to eliminate the need for hand-coding rules and instead create a system that learns through observation. Initial results were encouraging, although major improvements are needed before such a system can drive without human intervention. To gain insight into how the learned system decides what to do, and thus allow for further system improvements, as well as to build confidence that the system will heed the essential guidance for safe steering, we developed a simple method to highlight those parts of an image that we have described most conspicuously [c07: 94,1111]."}, {"heading": "1.1 Training the PilotNet Self-Driving System", "text": "PilotNet training data contains frames from the video of a front-facing camera in the car, paired with the corresponding steering command (1 / r), where r is the turning circle of the vehicle. It is supplemented by additional image / steering command pairs that simulate the vehicle in various positions outside of the center and outside of the orientation. In the advanced images, the destination command is adjusted accordingly to a command that steers the vehicle back into the middle of the lane. Once the network is trained, it can be used to give the steering command a new image."}, {"heading": "2 PilotNet Network Architecture", "text": "The PilotNet architecture is illustrated in Figure 1: The network consists of 9 layers, including a normalization layer, 5 wave layers and 3 fully connected layers; the input image is split into YUV layers and passed on to the network; the first layer of the network performs image normalization; the normalizer is hard coded and is not adapted in the learning process.The wave layers were designed to extract characteristics and were empirically selected through a series of experiments that had different layer configurations.Striped wave layers were used in the first three wave layers with a step of 2 x 2 and a core of 5 x 5 and a non-striped convolution with a size of 3 x 3 cores in the last two shaft layers; the five shaft layers are followed by three fully connected layers that result in an output control value that represents the reverse turning radius. The fully connected layers are designed to serve as a training device, but not as a control device at the end of the system."}, {"heading": "3 Finding the Salient Objects", "text": "The central idea in distinguishing the prominent objects is to find parts of the mask corresponding to the places where the function boards described above have the greatest activations; the activations of the parent cards become masks for activating the lower levels, using the following algorithm: 1. In each layer, the activations of the function boards are averaged; 2. The top layer of the averaged map is scaled up to the size of the layer below it. Scaling is done by means of deconvolution; the parameters (filter size and stripes) used for deconvolution are the same as in the conventional layer used to generate the map; the weights for deconvolution are set to 1.0 and the distortions are set to 0.3; the upscaled average map is then multiplied by the upper layer."}, {"heading": "4 Analysis", "text": "While the prominent objects found by our method clearly appear to be those that should affect the steering, we conducted a series of experiments to confirm that these objects actually control the steering. To perform these tests, we segmented the input image that represents PilotNet into two classes. Class 1 is intended to include all regions that have a significant effect on the steering angle output of PilotNet. These regions include all pixels that correspond to locations where the visualization mask exceeds a threshold. These regions are then expanded by 30 pixels to counteract the increasing extent of the superordinate map layers in relation to the input image. The exact amount of the steering is determined empirically. Class 2 includes all pixels in the original image minus the pixels in class 1. If the objects found by our method dominate control over the steering output angle, we expect the following: when we create an image in which we uniformly place the pixels in class 1."}, {"heading": "5 Conclusions", "text": "The results contribute significantly to our understanding of what PilotNet learns. Studying the prominent objects shows that PilotNet learns features that are \"meaningful\" to a human, while ignoring structures in the camera images that are not relevant to driving. This ability is derived from data, without the need for craft rules. In fact, PilotNet learns to recognize subtle features that would be difficult for human engineers to predict and program, such as roadside bushes and atypical vehicle classes."}], "references": [{"title": "End to end learning for self-driving", "author": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D. Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang", "Xin Zhang", "Jake Zhao", "Karol Zieba"], "venue": "cars, April", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1989}, {"title": "VisualBackProp: visualizing CNNs for autonomous driving, November 16 2016", "author": ["Mariusz Bojarski", "Anna Choromanska", "Krzysztof Choromanski", "Bernhard Firner", "Larry Jackel", "Urs Muller", "Karol Zieba"], "venue": "URL: https://arxiv.org/abs/1611.05418,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "How to explain individual classification decisions", "author": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M.i Kawanabe", "K. Hansen", "K.-R. M\u00fcller"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In Workshop Proc. ICLR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Visualization of nonlinear classification models in neuroimaging - signed sensitivity", "author": ["P.M. Rasmussen", "T. Schmah", "K.H. Madsen", "T.E. Lund", "S.C. Strother", "L.K. Hansen"], "venue": "maps. BIOSIGNALS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "In ICCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "author": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W Samek"], "venue": "PLOS ONE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A previous report [1] described an end-to-end learning system for self-driving cars in which a convolutional neural network (CNN) [2] was trained to output steering angles given input images of the road ahead.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "A previous report [1] described an end-to-end learning system for self-driving cars in which a convolutional neural network (CNN) [2] was trained to output steering angles given input images of the road ahead.", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "A detailed report describing our saliency detecting method can be found in [3] Several methods for finding saliency have been described by other authors.", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 44, "endOffset": 53}, {"referenceID": 4, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 44, "endOffset": 53}, {"referenceID": 5, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 80, "endOffset": 86}, {"referenceID": 7, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 80, "endOffset": 86}, {"referenceID": 8, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 153, "endOffset": 156}], "year": 2017, "abstractText": "As part of a complete software stack for autonomous driving, NVIDIA has created a neural-network-based system, known as PilotNet, which outputs steering angles given images of the road ahead. PilotNet is trained using road images paired with the steering angles generated by a human driving a data-collection car. It derives the necessary domain knowledge by observing human drivers. This eliminates the need for human engineers to anticipate what is important in an image and foresee all the necessary rules for safe driving. Road tests demonstrated that PilotNet can successfully perform lane keeping in a wide variety of driving conditions, regardless of whether lane markings are present or not. The goal of the work described here is to explain what PilotNet learns and how it makes its decisions. To this end we developed a method for determining which elements in the road image most influence PilotNet\u2019s steering decision. Results show that PilotNet indeed learns to recognize relevant objects on the road. In addition to learning the obvious features such as lane markings, edges of roads, and other cars, PilotNet learns more subtle features that would be hard to anticipate and program by engineers, for example, bushes lining the edge of the road and atypical vehicle classes.", "creator": "LaTeX with hyperref package"}}}