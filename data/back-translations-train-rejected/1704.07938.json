{"id": "1704.07938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "An ensemble-based online learning algorithm for streaming data", "abstract": "In this study, we introduce an ensemble-based approach for online machine learning. The ensemble of base classifiers in our approach is obtained by learning Naive Bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space. We propose a mechanism to learn sequences of data using data chunks paradigm. The experiments conducted on a number of UCI datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms.", "histories": [["v1", "Wed, 26 Apr 2017 00:33:36 GMT  (193kb)", "http://arxiv.org/abs/1704.07938v1", "19 pages, 3 figures"]], "COMMENTS": "19 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tien thanh nguyen", "thi thu thuy nguyen", "xuan cuong pham", "alan wee-chung liew", "james c bezdek"], "accepted": false, "id": "1704.07938"}, "pdf": {"name": "1704.07938.pdf", "metadata": {"source": "CRF", "title": "An ensemble-based online learning algorithm for streaming data", "authors": ["Tien Thanh Nguyen", "Thi Thu Thuy Nguyen", "Xuan Cuong Pham", "Alan Wee-Chung Liew", "James C. Bezdek"], "emails": [], "sections": [{"heading": null, "text": "In this study, we present an ensemble-based approach to machine learning online. The overall set of basic classifiers of our approach is achieved by learning Na\u00efve Bayes classifiers on different training sets generated by projecting the original training set onto a lower dimensional space. We propose a mechanism for learning data sequences using the paradigm of data blocks. Experiments conducted with a number of UCI data sets and a synthetic data set show that the proposed approach works significantly better than some well-known online learning algorithms. Keywords: online learning, ensemble method, multi-classification system, random projection, Na\u00efve Bayes classifier"}, {"heading": "1. Introduction", "text": "With rapid advances in storage and sensor technologies, large amounts of data are being collected in the form of data streams in many applications such as network traffic and stock market analysis. Streaming data have created problems for traditional offline machine learning systems. Firstly, learning the entire volume of data at once is often not possible. Furthermore, offline algorithms require retraining when new data is available, so they are not applicable to the situation where data continuously arrives and the predictive model must be obtained before all data is available. Therefore, the online learning system dealing with data streams has become increasingly popular [Nguyen et al, 2016a]. In this paper, we focus on supervised online learning, where the training data arrives sequentially, i.e. the predictive model is updated when a label is made available for observation."}, {"heading": "2. Proposed method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Random projection", "text": "In 1984, Johnson and Lindenstrauss (JL) published a paper on the extension of Lipschitz's continuous maps of metric spaces to Euclidean spaces and introduced the JL Lemma [Johnson and Lindenstrauss, 1984], which describes a linear transformation from a -dimensional space to a -dimensional space (called up-space). Specifically, in a finite set of -dimensional data vectors, there is a linear transformation. * The linear transformation T can be represented by a matrix%, so that # $=% $$$. If each element of the matrix is generated according to a certain random distribution, T $= T $$$, which has a different distribution. * The linear transformation T can be represented by a matrix%, so that # $= T $=% $. If each element of the matrix is generated according to a certain random distribution, T is known as a random projection."}, {"heading": "2.2. Class label prediction", "text": "The Na\u00efve Bayes Classifier is a well-known learning algorithm based on Bayes theorem with assumptions about conditional independence between features of observation. Despite the simplistic assumptions, Naive Bayes Classifiers are fast and efficient in training, which is important for streaming data. In detail, given a -dimensional vector = AB, B..., B C, the posterior probability belonging to class D is calculated by: P D | = PA D, B, B, B, B, P, D, D, probability P B | D, based on the assumption about the distribution of each feature B givenD, such as B | D ~ @ AID, JD, JD C, in which the parameters ID and JD are calculated."}, {"heading": "2.3. Parameters update", "text": "Theorem 1: Suppose that at the \u2212 1 KL step \u2212 1 minibatch M | j = 1,..., \u2212 1, where the minibatch M k has 1-dimensional observations, i.e. M =) N,, N,,..., N, l +. If we name I and J as the mean and variance of the model at the KL step, the actualization equations are: I = 1 l [lmn) A, k K\\ H CI \u2212 1 + 1 NK, [H +] NK."}, {"heading": "J = \u2211 l[lmn oA\u2211 k K\\ H C 5J \u2212 1 + AI \u2212 1 \u2212 I C 6 + \u2211 5NK, \u2212 I 6 [ H p", "text": "Episode 1: If a single observation NK arrives in the sequence, the update equations for I and J in the KL step result as follows: I = 1 \u2212 1 I \u2212 1 + NK"}, {"heading": "J = K 0 \u2212 1 5J \u2212 1 + AI \u2212 1 \u2212 I C 6 + ANK \u2212 I C 3", "text": "Episode 2: When the model is updated with each incoming observation, when \u2192 q, I \u2192 I \u00b2 and J \u2192 J, in which I \u00b2 and J are the mean and variance calculated on the entire dataset, the proof for theorem1 is provided in the appendix, while the evidence for Corollary 1 and 2 is straightforward. In this study, we use the misclassified observations in MK to update the classification model. They will first be updated in s mini-batches MKD t = 1,... in which MKD contains all the misclassified observations belonging to class D, i.e., the mini-batch MKD is used to update the mean and variance of the probability distribution related to the OKL function in the down spaces under all \"training areas.\" (DD2 * = \"DDDDAT2,\" DATZ 0A = KID2 \u2212 m. \""}, {"heading": "1. Parameter initialization", "text": "Input: ID2 * 0 and JD2 * 0, ensemble size K, dimension q For, = 1... 'For t = 1... s For O = 1... Set ID2 * = ID2 * 0, JD2 * = JD2 * 0 EndEnd"}, {"heading": "2. Random matrix generation", "text": "For, = 1... 'Generate% * =) 1 2 * +, 1 2 * ~ @ 0,1 End 3rd Class label prediction Input: MK =) A K, K C + For, = 1,...,' K * = \u221a MK% * Compute log 0P 5 D | # K * 63 using (5) End Predict label y K using Sum rule (6)"}, {"heading": "4. Parameter update", "text": "Division of incorrectly classified observations from MK to MKD, so that MKD if = D For, = 1... 'For t = 1... s For O = 1... Update ID2 * using (7) Update JD2 * using (8) EndEnd"}, {"heading": "3. Empirical Studies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Setup", "text": "In order to evaluate the performance of the proposed method, we conduct experiments on thirty-two UCI-marked datasets [UCI] and a labeled synthetic dataset called GM. The GM dataset consists of 1000 observations generated from a Gaussian mixture of 3 components in equal ratios. Means of the components are 1 \u2044,..., 1 2 \u2044 zzz, 2 \u2044 zzz, 0,..., 0 zzz, and \u2212 1 \u2044,..., \u2212 1 2 \u2044 zzz or the corresponding standard deviations are diag 1, 1 zzz, diag 2,..., 2 zzz, and diag 3, 3 zzz. Information on the datasets is provided in Table 1. We conduct extensive comparative studies using a number of state-of-the-art algorithms as benchmarks: PA [Crammer et al., 2006], SCW [Wang et al., 2012], OGD [Zinkevich et al., 2003], AROOW Crammer al [Crammer et al., 2009; we use Crammer et al.]."}, {"heading": "3.2. Results and Discussions", "text": "In fact, the number of people able to lead the EU is higher than ever before in the history of the European Union: the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU, the EU Commission, the EU, the EU, the EU Commission, the EU, the EU, the EU Commission, the EU, the EU Commission, the EU Commission, the EU, the Commission, the EU, the Commission, the EU, the Commission, the Commission, the EU, the Commission, the Commission, the EU, the Commission, the Commission, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the Commission, the EU, the EU, the EU, the EU, the EU, the EU,"}, {"heading": "4. Conclusions", "text": "In this paper, we have introduced an ensemble-based online learning algorithm that uses random projections and Na\u00efve Bayes classifiers. In our approach, the parameters of the Na\u00efve Bayes classifiers are simply initialized at the beginning and then updated when the observations received are incorrectly classified. We proposed the update equations for the parameters of the Na\u00efve Bayes classifiers by mini-batch learning and 1-to-1 learning. Extensive experimental results for the individual case showed the benefit of our approach compared to several known benchmark algorithms in terms of classification error rate, F1 score and number of updates."}, {"heading": "A. Appendix: Proof of Theorem 1", "text": "I = 1, 2, 3, 4, 5, 5, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,"}], "references": [{"title": "A novel online Bayes classifier", "author": ["T.T.T. Nguyen", "T.T. Nguyen", "X.C. Pham", "A.W.-C. Liew", "Y. Hu", "T. Liang", "C.-T. Li"], "venue": "Proceedings of the International Conference on Digital Image Computing: Techniques and Applications (DICTA), Gold Coast, Australia", "citeRegEx": "Nguyen et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6): 386\u2013408", "citeRegEx": "Rosenblatt. 1958", "shortCiteRegEx": null, "year": 1958}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 928\u2013936", "citeRegEx": "Zinkevich. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Online passive aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research, 7: 551\u2013585", "citeRegEx": "Crammer et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Exact soft confidence-weighted learning", "author": ["J. Wang", "P. Zhao", "S.C.H Hoi"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML), Edinburgh, Scotland, UK", "citeRegEx": "Wang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive regularization of weight vectors", "author": ["K. Crammer", "A. Kulesza", "M. Dredze"], "venue": "Proceedings of the 22th Advances in Neural Information Processing Systems (NIPS), pages 414\u2013422", "citeRegEx": "Crammer et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive regularization of weight vectors", "author": ["K. Crammer", "A. Kulesza", "M. Dredze"], "venue": "Machine Learning, 91(2): 155\u2013187", "citeRegEx": "Crammer et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Online bagging and boosting", "author": ["N. Oza", "S. Russell"], "venue": "Proceedings of the International Conference on Systems, Man and Cybernetics, pages 2340-2345", "citeRegEx": "Oza and Russell. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Online learning by ellipsoid method", "author": ["Yang et al", "2009] L. Yang", "R. Jin", "J Ye"], "venue": "Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Extensions of Lipshitz mapping into Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Proceeding of the Conference in modern analysis and probability. 26, pages 189-206, American Mathematical Society", "citeRegEx": "Johnson and Lindenstrauss. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "Proceeding of the 7th International Conferene on Knowledge Discovery and Data Mining (ACM SIGKDD), pages 245-250", "citeRegEx": "Bingham and Mannila. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach", "author": ["X.Z. Fern", "C.E. Brodley"], "venue": "Proceedings of the 20th International Conference on Machine Learning (ICML), pages 186-193", "citeRegEx": "Fern and Brodley. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Fuzzy ensemble clustering based on random projections for DNA microarray data analysis", "author": ["R. Avogadri", "G. Valentini"], "venue": "Artificial Intelligence in Medicine, 45: 173-183", "citeRegEx": "Avogadri and Valentini. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond independence: Conditions for the optimality of the simple Bayesian classifier", "author": ["P. Domingos", "M. Pazzani"], "venue": "Proceedings of the Thirteenth International Conference on Machine Learning, pp. 105\u2013112", "citeRegEx": "Domingo. Pazzani. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "A novel genetic algorithm approach for simultaneous feature and classifier selection in multi classifier system", "author": ["T.T. Nguyen", "A.W.-C. Liew", "M.T. Tran", "X.C. Pham", "M.P. Nguyen"], "venue": "Proceeding of the IEEE Congress on Evolutionary Computation (CEC), pages 1698-1705", "citeRegEx": "Nguyen et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble methods in machine learning", "author": ["T. Dietterich"], "venue": "the first International Workshop on Multiple Classifier Systems, Springer, pp. 1-15", "citeRegEx": "Dietterich. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "A Novel Combining Classifier Method based on Variational Inference", "author": ["T.T. Nguyen", "T.T.T. Nguyen", "X.C. Pham", "A.W-C. Liew"], "venue": "Pattern Recognition, 49: 198-212", "citeRegEx": "Nguyen et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "Nevzorov, A Primer on Statistical Distributions", "author": ["Balakrishnan", "Nevzorov", "V.B. 2003] N. Balakrishnan"], "venue": null, "citeRegEx": "Balakrishnan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2003}, {"title": "On Combining Classifiers", "author": ["J. Kittler", "M. Hatef", "R.P.W. Duin", "J. Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(3): 226-239", "citeRegEx": "Kittler et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "LIBOL: A Library for Online Learning Algorithms", "author": ["S.C.H. Hoi", "J. Wang", "P. Zhao"], "venue": "Journal of Machine Learning Research. 15: 495-499", "citeRegEx": "Hoi et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "MOA: Massive Online Analysis", "author": ["A. Bifet", "G. Holmes", "B. Pfahringer", "P. Kranen", "H. Kremer", "T. Jansen", "T. Seidl"], "venue": "a Framework for Stream Classification and Clustering. in Journal of Machine Learning Research Workshop and Conference Proceedings, Vol. 11: Workshop on Applications of Pattern Analysis", "citeRegEx": "Bifet et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing and Management.45: 427-437", "citeRegEx": "Sokolova and Lapalme. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Not So Na\u00efve Bayes: Aggregating One-Dependence Estimators", "author": ["G.I. Webb", "J.R. Boughton", "Z. Wang"], "venue": "Machine Learning. 58: 5-24", "citeRegEx": "Webb et al.. 2005", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, the online learning framework that deals with data streams has become increasing popular [Nguyen et al., 2016a].", "startOffset": 100, "endOffset": 122}, {"referenceID": 1, "context": "Well-known additive models include the Perceptron [Rosenblatt, 1958], Online Gradient Descent (OGD) [Zinkevich, 2003], Passive Aggressive learning (PA) [Crammer et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 2, "context": "Well-known additive models include the Perceptron [Rosenblatt, 1958], Online Gradient Descent (OGD) [Zinkevich, 2003], Passive Aggressive learning (PA) [Crammer et al.", "startOffset": 100, "endOffset": 117}, {"referenceID": 3, "context": "Well-known additive models include the Perceptron [Rosenblatt, 1958], Online Gradient Descent (OGD) [Zinkevich, 2003], Passive Aggressive learning (PA) [Crammer et al., 2006], Soft Confident Weighted (SCW) [Wang et al.", "startOffset": 152, "endOffset": 174}, {"referenceID": 4, "context": ", 2006], Soft Confident Weighted (SCW) [Wang et al., 2012], and Adaptive Regularization of Weights (AROW) [Crammer et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 5, "context": ", 2012], and Adaptive Regularization of Weights (AROW) [Crammer et al., 2009; Crammer et al., 2013].", "startOffset": 55, "endOffset": 99}, {"referenceID": 6, "context": ", 2012], and Adaptive Regularization of Weights (AROW) [Crammer et al., 2009; Crammer et al., 2013].", "startOffset": 55, "endOffset": 99}, {"referenceID": 7, "context": "Online Bagging and Online Boosting [Oza and Russell, 2005] are two well-known online ensemble algorithms.", "startOffset": 35, "endOffset": 58}, {"referenceID": 0, "context": "Other algorithms such as the Bayesian-based method [Nguyen et al., 2016a] and the Ellipsoid method [Yang et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 0, "context": "Finally, approaches like the Bayesian-based method [Nguyen et al., 2016a] estimate the distribution of each class and have problems dealing with very high dimensional datasets.", "startOffset": 51, "endOffset": 73}, {"referenceID": 9, "context": "To deal with the high dimensional data, our algorithm uses the theory of random projections [Johnson and Lindenstrauss, 1984] to project new observations to low dimension subspaces, thereby obtaining different data schemes for the ensemble of homogenuous base classifiers.", "startOffset": 92, "endOffset": 125}, {"referenceID": 15, "context": "The ensemble of Na\u00efve Bayes classifiers is expected to obtain better result than a single classifier due to the characteristic of ensemble system [Dietterich, 2000].", "startOffset": 146, "endOffset": 164}, {"referenceID": 9, "context": "In 1984, Johnson and Lindenstrauss (JL) published a paper about extending Lipschitz continuous maps from metric spaces to Euclidean spaces and introduced the JL Lemma [Johnson and Lindenstrauss, 1984].", "startOffset": 167, "endOffset": 200}, {"referenceID": 10, "context": "Moreover, generating the principle components is computationally expensive compare to generating the random matrix in random projection [Bingham and Mannila, 2001].", "startOffset": 136, "endOffset": 163}, {"referenceID": 12, "context": "follow the construction of random matrix in [Avogadri and Valentini, 2009] in which the projections are simply obtained by using a \u00d7 random matrix % * = 1 .", "startOffset": 44, "endOffset": 74}, {"referenceID": 14, "context": "Several popular fixed combining methods, namely Sum, Product, Majority Vote, Max, Min, and Median can be used as the combiner [Nguyen et al., 2014; Nguyen et al., 2016b; Kittler et al., 1998].", "startOffset": 126, "endOffset": 191}, {"referenceID": 16, "context": "Several popular fixed combining methods, namely Sum, Product, Majority Vote, Max, Min, and Median can be used as the combiner [Nguyen et al., 2014; Nguyen et al., 2016b; Kittler et al., 1998].", "startOffset": 126, "endOffset": 191}, {"referenceID": 18, "context": "Several popular fixed combining methods, namely Sum, Product, Majority Vote, Max, Min, and Median can be used as the combiner [Nguyen et al., 2014; Nguyen et al., 2016b; Kittler et al., 1998].", "startOffset": 126, "endOffset": 191}, {"referenceID": 3, "context": "We perform extensive comparative studies with a number of state-of-the-art algorithms as benchmarks: PA [Crammer et al., 2006], SCW [Wang et al.", "startOffset": 104, "endOffset": 126}, {"referenceID": 4, "context": ", 2006], SCW [Wang et al., 2012], OGD [Zinkevich, 2003], AROW [Crammer et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": ", 2012], OGD [Zinkevich, 2003], AROW [Crammer et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 5, "context": ", 2012], OGD [Zinkevich, 2003], AROW [Crammer et al., 2009; Crammer et al., 2013] (we use the implementation in LIBOL library [Hoi et al.", "startOffset": 37, "endOffset": 81}, {"referenceID": 6, "context": ", 2012], OGD [Zinkevich, 2003], AROW [Crammer et al., 2009; Crammer et al., 2013] (we use the implementation in LIBOL library [Hoi et al.", "startOffset": 37, "endOffset": 81}, {"referenceID": 19, "context": ", 2013] (we use the implementation in LIBOL library [Hoi et al., 2014] for these algorithms,", "startOffset": 52, "endOffset": 70}, {"referenceID": 7, "context": "10 default value for parameters are used if available), and Online Bagging [Oza and Russell, 2005] (we use the implementation in MOA library [Bifet et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 20, "context": "10 default value for parameters are used if available), and Online Bagging [Oza and Russell, 2005] (we use the implementation in MOA library [Bifet et al., 2010]).", "startOffset": 141, "endOffset": 161}, {"referenceID": 16, "context": "The number of learners in Online Bagging and K in the proposed method are set to 200 as in [Nguyen et al., 2016b],", "startOffset": 91, "endOffset": 113}, {"referenceID": 21, "context": "The proposed method is compared to the benchmark algorithms with respect to the error rate and F1 score (which is the harmonic mean of Precision and Recall) [Sokolova and Lapalme, 2009].", "startOffset": 157, "endOffset": 185}, {"referenceID": 19, "context": "Here we followed the performance measurements from LIBOL library [Hoi et al., 2014] where the authors used criteria such as mistake rate (classification error rate), and the number of updates (to measure the model stability) to evaluate the performance.", "startOffset": 65, "endOffset": 83}, {"referenceID": 22, "context": "In addition, we used Na\u00efve Bayes, a simple but efficacious learning algorithm [Webb et al., 2005] to generate the base classifiers where the parameter updates are simple and fast to compute.", "startOffset": 78, "endOffset": 97}], "year": 2017, "abstractText": "In this study, we introduce an ensemble-based approach for online machine learning. The ensemble of base classifiers in our approach is obtained by learning Na\u00efve Bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space. We propose a mechanism to learn sequences of data using data chunks paradigm. The experiments conducted on a number of UCI datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms.", "creator": "PDFCreator 2.5.1.5"}}}