{"id": "1704.07978", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "On Improving Deep Reinforcement Learning for POMDPs", "abstract": "Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.", "histories": [["v1", "Wed, 26 Apr 2017 05:55:07 GMT  (3189kb,D)", "https://arxiv.org/abs/1704.07978v1", "7 pages, 6 figures, 3 tables"], ["v2", "Mon, 8 May 2017 11:03:37 GMT  (2687kb,D)", "http://arxiv.org/abs/1704.07978v2", "7 pages, 6 figures, 3 tables"]], "COMMENTS": "7 pages, 6 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pengfei zhu", "xin li", "pascal poupart"], "accepted": false, "id": "1704.07978"}, "pdf": {"name": "1704.07978.pdf", "metadata": {"source": "CRF", "title": "On Improving Deep Reinforcement Learning for POMDPs", "authors": ["Pengfei Zhu", "Xin Li", "Pascal Poupart"], "emails": ["pengfei0408@163.com,", "xinli@bit.edu.cn,", "ppoupart@uwaterloo.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to break the rules that they have given themselves. (...) In fact, it is the case that they are able to break the rules. \"(...) In fact, it is the case that they are able to determine for themselves what they want. (...) It is as if they want people to be able to break the rules.\" (...). (...)"}, {"heading": "2 Background", "text": "In this section we give a brief overview of Deep Q-Networks (DQNs), Partially Observable Markov Decision Processes (POMDPs) and Deep Recurrent Q-networks (DRQNs)."}, {"heading": "2.1 Deep Q-Networks", "text": "A sequential decision problem with known environmental dynamics is usually defined as a Markov decision process (Q = Q value) characterized by a quadruple < S, A, P, R >. At each step, an agent selects an action that he performs in relation to his fully observable current state. He receives an immediate reward rt rt rt (st, at) and transitions to a new state st + 1. The goal of enhanced learning is to find the policy that maximizes the expected discounted rewards. RtRt = rt + \u03b3rt + 1 + \u03b3 2rt + 2 + \u00b7 (1), where a discount factor [0, 1] is the discount factor. In MDPs, optimal policy can be calculated by value teration. [Sutton and Barto, 1998].Q-Learning [Watkins and Dayan, 1992] is considered a model-free technique for amplifying learning problems with unknown dynamics."}, {"heading": "2.2 Partially Observable Markov Decision Processes (POMDPs)", "text": "POMDPs generalize MDPs for planning under partial observability. A POMDP is mathematically defined as tuples < S, A, Z, T, O, R >, consisting of a finite set of states S, a finite set of actions A, a transitional function T: S \u00b7 A \u2192 (S), a reward function depending on the state and the action just performed R: S \u00b7 A \u2192 R, a finite set of observations Z and an observation function O: S \u00b7 A (Z). Since the true states are no longer fully observable, a belief is used to estimate the current state, defined as the probability mass function over the states and designated as b = (s1), b (s2),... b (s | S |)))), where the true states are no longer fully observable (si), a belief is used to estimate the current state, defined as the probability mass function over the states and designated as b = (s2), S (si), and S (si), where (si), (si), and (si), where (b), S (b), and S ()."}, {"heading": "2.3 DRQN", "text": "DQN assumes that the agent has complete knowledge of the state of the environment, i.e., the observation of the agent corresponds to the state of the environment. In practice, however, this is rarely the case, even in some Atari games. For example, a frame of Pong does not reveal the speed of the ball and its direction of motion, as shown in Fig. 1 (a). In the game of asteroids, parts of the image are concealed for several consecutive frames in order to challenge the player. Therefore, these games are of course POMDP problems. While DQN's strategy affects several successive frames in which it is hoped to derive the full state, this only works if a short, limited history is sufficient to characterize the state of the game. For more complex games, the performance of DQN increases."}, {"heading": "3 ADRQN - Action-specific Deep Recurrent Q-Network", "text": "Inspired by the aforementioned work, our goal is to propose a model-free, deep Q > Q concept that incorporates the influence of the action performed over time. Specifically, we couple the actions performed and the observations obtained as input to the Q network. The architecture of our model is shown in Fig. 2. Our first attempt to couple the action and observation was to directly link a fixed representation of the action to the observation vector, i.e., we used one-hot vectors to represent each action. However, such a concatenation did not perform well, because the lengths of the action and observation vectors differ greatly from each other, leading to numerical instability. In tari games, as well as many real POMDPs, the number of actions is far less than the dimensionality of state representation. In conventional DQNsand DRQNs, the proposed dimensionality of the current state is coded by several features (the 136 or 3observation)."}, {"heading": "4 Experiments", "text": "In this section we evaluate the training and test performance of ADRQN on the basis of several Atari games and their flickering version."}, {"heading": "4.1 Experiments setup", "text": "In their frame, before a frame is sent to the neural network as input, each raw screen is either fully observable or completely blacked out with black pixels. \u2022 Frame skip Scheme. We have adopted the frame before a frame is sent to the neural network, 2013]. This mechanism is commonly used in most previous works of deep gain, namely in most previous works of deep gain - algorithm 1 ADRQN 1: initialization of repeat memory D, # of iterations M 2: exploration of the Q network and target network with the targets - each 3: for episode 1 to M do 4: Initialize the first action a0 = no operation, 5: h0 = 6: Initialization of the first observation with 7: Q network and target network with the targets we select: each 3: for episode 1 to M do 4: Initialize the first action a0 = no operation, 5: h0 6: Initialize the first observation with 7: Q: we select"}, {"heading": "4.2 Training", "text": "Compared to the other approaches, the key idea of our framework is to construct action observation pairs as input to the LSTM layer to obtain more representative features for the Qnetwork to regain it. Actions are first encoded with one-hot vectors, then processed by a fully connected layer to construct a higher-dimensional vector associated with the output of the third Convolutionary Layer for better numerical stability. Since the LSTM layer is rolled out for 10 time steps, we need to ensure that there are enough transitions to be stored in the D replay memory so that we can try a minibatch sequence of length 10 each time we update the entire network until the replay memory is full."}, {"heading": "4.3 Testing Evaluation", "text": "We play through each game on the basis of the learned model ten times to get the averages as the final results. Epsilon, which is used for \u2212 greed in the test evaluation, is 0.05, which is less than the value used in the training process because we consider the model to be well trained. Table 2 summarizes the results obtained from complete observations. ADRQN generally outperforms DRQN. While Table 3 shows that ADRQN significantly outperforms DRQN when exercising with half of the observed frames, it is interesting to observe that the test results are generally better than the results obtained during the training process for both DRQN and ADRQN. This can be explained by the fact that a different distribution of covert frames makes the problem more or less observable, insofar as better results can be obtained in some cases."}, {"heading": "4.4 Generalization Performance", "text": "To further demonstrate the advantages of ADRQN in dealing with environmental change, we evaluate the generalization performance of DRQN and ADRQN using patch versions of the games. POMDP to MDP Generalization: After we have been trained with an observation probability of 0.5, we test the learned guidelines in environments with observation probabilities (0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0). The results are shown in Figure 5. We observe that both DRQN and ADRQN consistently perform better with the increase in observation probability. If the observable probability reaches 1.0, the results obtained are close to the performance achieved by training with complete observations, further demonstrating that both models can respond effectively to environmental change. ADRQN consistently exceeds DRQN. MDP to POMDP Generalization: After we have been trained as MDP, we significantly increase the probability of observation values by testing the probability of the observations on the QN with differing observational probability of 1."}, {"heading": "5 Conclusion", "text": "In this paper, we propose an action-specific, deeply recurrent Q-Network (ADRQN) to improve learning performance in partially observable areas. Actions are encoded via a fully interconnected layer and coupled with wavy observations to form action observation pairs. Action observation pairs time series are then integrated through an LSTM layer to derive the latent states, on the basis of which Q values are calculated by a fully interconnected layer similar to conventional DQNs. We have shown the effectiveness of the approach to several POMDP problems, including flickering Atari games."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "J. Artif. Intell. Res. (JAIR), 47:253\u2013279,", "citeRegEx": "Bellemare et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A decision-theoretic approach to task assistance for persons with dementia", "author": ["Boger et al", "2005] Jennifer Boger", "Pascal Poupart", "Jesse Hoey", "Craig Boutilier", "Geoff Fernie", "Alex Mihailidis"], "venue": "Proceedings of the Nineteenth International Joint Conference on Artificial Intelli-", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}, {"title": "CoRR", "author": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks"], "venue": "abs/1602.02672,", "citeRegEx": "Foerster et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Matthew J. Hausknecht", "Peter Stone. Deep recurrent q-learning for partially observable mdps"], "venue": "abs/1507.06527,", "citeRegEx": "Hausknecht and Stone. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "SARSOP: efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["Kurniawati et al", "2008] Hanna Kurniawati", "David Hsu", "Wee Sun Lee"], "venue": "In Robotics: Science and Systems IV,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "CoRR", "author": ["Guillaume Lample", "Devendra Singh Chaplot. Playing FPS games with deep reinforcement learning"], "venue": "abs/1609.05521,", "citeRegEx": "Lample and Chaplot. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech and Audio Processing", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Trans"], "venue": "8(1):11\u201323,", "citeRegEx": "Levin et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "A novel orthogonal nmf-based belief compression for pomdps", "author": ["Li et al", "2007] Xin Li", "William Kwok-Wai Cheung", "Jiming Liu", "Zhili Wu"], "venue": "In Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "CoRR", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra. Continuous control with deep reinforcement learning"], "venue": "abs/1509.02971,", "citeRegEx": "Lillicrap et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Technical report", "author": ["Long-Ji Lin. Reinforcement learning for robots using neural networks"], "venue": "DTIC Document,", "citeRegEx": "Lin. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "CoRR", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller. Playing atari with deep reinforcement learning"], "venue": "abs/1312.5602,", "citeRegEx": "Mnih et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih et al", "2015] Volodymyr Mnih", "Kavukcuoglu Koray", "Silver David", "Rusu Andrei A", "Veness Joel", "Bellemare Marc G", "Graves Alex", "Riedmiller Martin", "Fidjeland Andreas K", "Ostrovski Georg"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Autonomous Agents and Multi-Agent Systems", "author": ["Guy Shani", "Joelle Pineau", "Robert Kaplow. A survey of point-based pomdp solvers"], "venue": "pages 1\u201351,", "citeRegEx": "Shani et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Silver et al", "2016] David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Probabilistic robot navigation in partially observable environments", "author": ["Simmons", "Koenig", "1995] Reid G. Simmons", "Sven Koenig"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Simmons et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Simmons et al\\.", "year": 1995}, {"title": "CoRR", "author": ["Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models"], "venue": "abs/1507.00814,", "citeRegEx": "Stadie et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": "IEEE Trans. Neural Networks, 9(5):1054\u20131054,", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Machine Learning", "author": ["Christopher J.C.H. Watkins", "Peter Dayan. Technical note q-learning"], "venue": "8:279\u2013292,", "citeRegEx": "Watkins and Dayan. 1992", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 7, "context": "In the past 3 decades, many reinforcement learning techniques based on fully and partially observable Markov decision processes (MDPs) have been successfully applied to a variety of problems including spoken dialog management [Levin et al., 2000], robotic control [Simmons and Koenig, 1995] and automated assistance in health care [Boger et al.", "startOffset": 226, "endOffset": 246}, {"referenceID": 9, "context": ", continuous control [Lillicrap et al., 2015], high-dimensional robot control [Stadie et al.", "startOffset": 21, "endOffset": 45}, {"referenceID": 16, "context": ", 2015], high-dimensional robot control [Stadie et al., 2015], and Atari Learning Environment benchmarks (ALE) [Bellemare et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 0, "context": ", 2015], and Atari Learning Environment benchmarks (ALE) [Bellemare et al., 2013].", "startOffset": 57, "endOffset": 81}, {"referenceID": 3, "context": "To address this, [Hausknecht and Stone, 2015] adapted the fully connected structure of DQN with a recurrent network [Hochreiter and Schmidhuber, 1997], and called the new architecture Deep Recurrent Q-Network (DRQN).", "startOffset": 17, "endOffset": 45}, {"referenceID": 4, "context": "To address this, [Hausknecht and Stone, 2015] adapted the fully connected structure of DQN with a recurrent network [Hochreiter and Schmidhuber, 1997], and called the new architecture Deep Recurrent Q-Network (DRQN).", "startOffset": 116, "endOffset": 150}, {"referenceID": 6, "context": "[Lample and Chaplot, 2016] combined DRQN with handcrafted features to jointly supervise the learning process of 3D games in partially observable environments, however the approach suffers from the same problem as DRQN since it overlooks action histories.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "[Foerster et al., 2016] extended DRQN to handle partially observable multi-agent reinforcement learning problems by proposing a deep distributed recurrent Q-networks (DDRQN).", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "In MDPs, an optimal policy can be computed by value iteration [Sutton and Barto, 1998].", "startOffset": 62, "endOffset": 86}, {"referenceID": 18, "context": "Q-Learning [Watkins and Dayan, 1992] was proposed as a model-free technique for reinforcement learning problems with unknown dynamics.", "startOffset": 11, "endOffset": 36}, {"referenceID": 10, "context": "DQN uses experience replay [Lin, 1993] to store previous samples et = (st, at, rt, st+1) up to a fixed size memory Dt.", "startOffset": 27, "endOffset": 38}, {"referenceID": 13, "context": "In practice, it is common to have the optimal policy represented by a set of linear functions (so called \u03b1-vectors) over the belief space, with the maximum \u201cenvelop\u201d of the \u03b1-vectors forming the value function [Shani et al., 2013].", "startOffset": 210, "endOffset": 230}, {"referenceID": 3, "context": "To tackle problems with partially observable environments by deep reinforcement learning, [Hausknecht and Stone, 2015] proposed a framework called Deep Recurrent QLearning (DRQN) in which an LSTM layer was used to replace the first post-convolutional fully-connected layer of the conventional DQN.", "startOffset": 90, "endOffset": 118}, {"referenceID": 3, "context": "\u2022 Flickering Atari game: [Hausknecht and Stone, 2015] introduced a flickering version of the Atari games, which modified the Atari games by obscuring the entire screen with a certain probability at each time step, which introduces partial observability and therefore yields a POMDP.", "startOffset": 25, "endOffset": 53}, {"referenceID": 0, "context": "We adopted the frame skip technique [Bellemare et al., 2013].", "startOffset": 36, "endOffset": 60}, {"referenceID": 3, "context": "As suggested in [Hausknecht and Stone, 2015], random updates can achieve the similar performance as conventional sequential updates of the entire episode, but with much lower training cost.", "startOffset": 16, "endOffset": 44}, {"referenceID": 11, "context": "Moreover, the scores obtained by playing the games are not always stable since small changes of the weights may have a significant impact on the outcome [Mnih et al., 2013].", "startOffset": 153, "endOffset": 172}], "year": 2017, "abstractText": "Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep QNetworks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.", "creator": "LaTeX with hyperref package"}}}