{"id": "1704.08006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Deep Text Classification Can be Fooled", "abstract": "Deep neural networks (DNNs) play a key role in many applications. Current studies focus on crafting adversarial samples against DNN-based image classifiers by introducing some imperceptible perturbations to the input. However, DNNs for natural language processing have not got the attention they deserve. In fact, the existing perturbation algorithms for images cannot be directly applied to text. This paper presents a simple but effective method to attack DNN-based text classifiers. Three perturbation strategies, namely insertion, modification, and removal, are designed to generate an adversarial sample for a given text. By computing the cost gradients, what should be inserted, modified or removed, where to insert and how to modify are determined effectively. The experimental results show that the adversarial samples generated by our method can successfully fool a state-of-the-art model to misclassify them as any desirable classes without compromising their utilities. At the same time, the introduced perturbations are difficult to be perceived. Our study demonstrates that DNN-based text classifiers are also prone to the adversarial sample attack.", "histories": [["v1", "Wed, 26 Apr 2017 08:17:34 GMT  (639kb)", "http://arxiv.org/abs/1704.08006v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["bin liang", "hongcheng li", "miaoqiang su", "pan bian", "xirong li", "wenchang shi"], "accepted": false, "id": "1704.08006"}, "pdf": {"name": "1704.08006.pdf", "metadata": {"source": "CRF", "title": "Deep Text Classification Can be Fooled", "authors": ["Bin Liang", "Hongcheng Li", "Xirong Li", "Wenchang Shi"], "emails": ["wenchang}@ruc.edu.cn"], "sections": [{"heading": "Introduction", "text": "It is not the first time that this type of information has come into focus in other areas of public life in recent years. (Nor is it the first time that this type of information has been influenced primarily by the way in which its robustness has been raised to misclassify a trained model. As shown in Figure 1, an image can be generated by adding some imperceptible perturbations in a given image (Goodfellow et al.). In 2015, the famous GoogLeNet (Szegedy et al.) will miscsify the result, while a human observer can correctly classify it and reveal the existence of the image."}, {"heading": "Target Model and Dataset", "text": "A recent paper (Zhang et al. 2015) presents a revolutionary network model for text classification at character level. For model training and evaluation, the authors use an ontology dataset from DBpedia (Lehmann et al. 2014), which contains 560,000 training samples and 70,000 test samples from 14 high-level classes such as company, construction and film. Before an input text is fed into the network, each character of the text is quantified as a vector, using a uniform representation per alphabet for the encoding. For example, the character \"c\" is encoded as a vector (0, 0, 1, 0,..., 0), with only the third dimension placed on one. Finally, through six revolutionary layers and three fully networked layers, the input is projected into a vector indicating the classification reliability of the 14 classes. We choose this state-of-the-art model and the DBpedia dataset as experimental targets to evaluate the effectiveness of the proposed methodology."}, {"heading": "Overview", "text": "In fact, most of them are able to outdo themselves."}, {"heading": "Insertion Strategy", "text": "For a given text t, if F (t) = c, the goal of the strategy is to enter some new text telemente (attributes) in t, which effectively classifies the class of t for class c \"c\" of interest (c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \""}, {"heading": "Modification Strategy", "text": "The modification strategy is designed to influence the output of the model by slightly manipulating some phrases in the input. Here, too, we use the cost differential to choose what should be changed. For a particular example, t, which is classified as class c, we calculate the cost differential t J (F, t, c). Similar to identifying the HTPs, we identify phrases with a significant contribution to the current classification by identifying hot characters with the highest gradient magnitude. We call this type of phrase the Hot Sample Phrase (HSP). Changing the HSPs of an input is likely to change the current classification by considering how we modify the HSPs to let the output classification be a certain class c. \"Theoretically, the modification should increase the loss function J (F, t, c) and at the same time reduce J (F, c), c.\" In other words, the modification should follow the direction of the cost gradient J (F)."}, {"heading": "Removal Strategy", "text": "Because the arbitrary elimination of HSPs from the input often impairs their meaning, only words from the HSPs that play a complementary role, such as an insignificant adjective or adverb, can be removed. For example, in Figure 8, the sample is classified as a film class with 95.5% certainty. By calculating the cost differential, we identify only a seven-part British television series. Removing \"British\" does not have much impact on the meaning, but leads to a loss of confidence in the original class of 35.0% (from 95.5% to 60.5%)."}, {"heading": "Combination of Three Strategies", "text": "In practice, we need to combine the above three strategies to produce opposing samples. For example, changing the output classification by the removal strategy alone is difficult. However, by combining them with the other strategies, we can avoid introducing excessive modification or insertion into the original text. As shown in Figure 9, by selecting a fake sample from Edward Sample, removing an HTP and modifying an HSP, the output classification can be successfully changed (from 83.7% building to 88.7% means of transport), but not if we only evaluate each strategy alone.EvaluationWe evaluate the effectiveness of our method by answering the following questions: Can the classification of a given sample be changed to select any target class from the test set that was originally classified as means of transport (class 6 #)."}, {"heading": "Related Work", "text": "This year, it has come to the point where it is only a matter of time before a result is achieved."}], "references": [{"title": "Can machine learning be secure", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A. Joseph", "J. Tygar"], "venue": "ASIACCS", "citeRegEx": "Barreno et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2006}, {"title": "Towards evaluating the robustness of neural networks", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv: 1608.04644.", "citeRegEx": "Carlini and Wagner,? 2016", "shortCiteRegEx": "Carlini and Wagner", "year": 2016}, {"title": "A unified architecture for natural language processing: Deep neural networks with task learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML 2008.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Contextdependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE TASLP, 20(1): 30\u201342.", "citeRegEx": "Dahl et al\\.,? 2012", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "ICLR 2015.", "citeRegEx": "Goodfellow et al\\.,? 2015", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Deep learning and music adversaries", "author": ["C. Kereliuk", "B. Sturm", "J. Larsen"], "venue": "IEEE TMM, 17(11): 2059-2071.", "citeRegEx": "Kereliuk et al\\.,? 2015", "shortCiteRegEx": "Kereliuk et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS 2012.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. Kleef", "S. Auer", "C. Bizer"], "venue": "Semantic Web Journal.", "citeRegEx": "Lehmann et al\\.,? 2014", "shortCiteRegEx": "Lehmann et al\\.", "year": 2014}, {"title": "Cracking classifiers for evasion: a case study on the Google\u2019s phishing pages filter", "author": ["B. Liang", "M. Su", "W. You", "W. Shi", "G. Yang"], "venue": "WWW 2016.", "citeRegEx": "Liang et al\\.,? 2016", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS 2013.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "DeepFool: a simple and accurate method to fool deep neural networks", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "venue": null, "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "CVPR 2015.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697.", "citeRegEx": "Papernot et al\\.,? 2016a", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "IEEE EuroS&P 2016.", "citeRegEx": "Papernot et al\\.,? 2016b", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "IEEE S&P 2016.", "citeRegEx": "Papernot et al\\.,? 2016c", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Understanding adversarial training: increasing local stability of neural nets through robust optimization", "author": ["U. Shaham", "Y. Yamada", "S. Negahban"], "venue": "arXiv preprint arXiv:1511.05432.", "citeRegEx": "Shaham et al\\.,? 2015", "shortCiteRegEx": "Shaham et al\\.", "year": 2015}, {"title": "Practical evasion of a learningbased classifier: A case study", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "IEEE S&P 2014.", "citeRegEx": "\u0160rndi\u0107 and Laskov,? 2014", "shortCiteRegEx": "\u0160rndi\u0107 and Laskov", "year": 2014}, {"title": "YAGO: a core of semantic knowledge unifying wordnet and Wikipedia", "author": ["F. Suchanek", "G Kasneci", "G. Weikum"], "venue": null, "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "ICLR 2014.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Large-scale automatic classification of phishing pages", "author": ["C. Whittaker", "B. Ryner", "M. Nazif"], "venue": "NDSS 2010. Zhang, X.; Zhao, J.; and LeCun, Y. 2015. Character-level convolutional networks for text classification. In NIPS 2015.", "citeRegEx": "Whittaker et al\\.,? 2010", "shortCiteRegEx": "Whittaker et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision (Krizhevsky et al. 2012), speech recognition (Dahl et al.", "startOffset": 98, "endOffset": 122}, {"referenceID": 3, "context": "2012), speech recognition (Dahl et al. 2012), and natural language processing (Collobert and Weston 2008).", "startOffset": 26, "endOffset": 44}, {"referenceID": 2, "context": "2012), and natural language processing (Collobert and Weston 2008).", "startOffset": 39, "endOffset": 66}, {"referenceID": 4, "context": "Some recent studies (Goodfellow et al. 2015; Kereliuk et al. 2015) demonstrate that DNN-based image or audio classifiers can be fooled by adversarial samples, which are well-crafted to cause a trained model to misclassify.", "startOffset": 20, "endOffset": 66}, {"referenceID": 5, "context": "Some recent studies (Goodfellow et al. 2015; Kereliuk et al. 2015) demonstrate that DNN-based image or audio classifiers can be fooled by adversarial samples, which are well-crafted to cause a trained model to misclassify.", "startOffset": 20, "endOffset": 66}, {"referenceID": 4, "context": "by adding some imperceptible perturbations in a given image (Goodfellow et al. 2015).", "startOffset": 60, "endOffset": 84}, {"referenceID": 18, "context": "Consequently, the famous GoogLeNet (Szegedy et al. 2015) will misclassify the resultant image, while a human observer can still correctly classify it and without noticing the existence of the introduced perturbations.", "startOffset": 35, "endOffset": 56}, {"referenceID": 12, "context": "As shown in (Papernot et al. 2016a), a stop sign, after being crafted, will be incorrectly classified as a yield sign.", "startOffset": 12, "endOffset": 35}, {"referenceID": 20, "context": "many spam or phishing webpage detection systems are primarily based on text classification (Whittaker et al. 2010).", "startOffset": 91, "endOffset": 114}, {"referenceID": 4, "context": "presented in (Goodfellow et al. 2015).", "startOffset": 13, "endOffset": 37}, {"referenceID": 7, "context": "For model training and evaluation, the authors employ a DBpedia ontology dataset (Lehmann et al. 2014),", "startOffset": 81, "endOffset": 102}, {"referenceID": 4, "context": "(Goodfellow et al. 2015), has proven to be effective (Kereliuk et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "2015), has proven to be effective (Kereliuk et al. 2015; Moosavi-Dezfooli et al. 2016; Papernot et al. 2016b).", "startOffset": 34, "endOffset": 109}, {"referenceID": 10, "context": "2015), has proven to be effective (Kereliuk et al. 2015; Moosavi-Dezfooli et al. 2016; Papernot et al. 2016b).", "startOffset": 34, "endOffset": 109}, {"referenceID": 13, "context": "2015), has proven to be effective (Kereliuk et al. 2015; Moosavi-Dezfooli et al. 2016; Papernot et al. 2016b).", "startOffset": 34, "endOffset": 109}, {"referenceID": 4, "context": "The gradient-based adversarial sample generation technique is proposed by Goodfellow et al (Goodfellow et al. 2015).", "startOffset": 91, "endOffset": 115}, {"referenceID": 4, "context": "We are inspired by (Goodfellow et al. 2015), but leverage the cost gradient in a completely different way due to", "startOffset": 19, "endOffset": 43}, {"referenceID": 17, "context": ", (Suchanek et al. 2007), we can get some facts that are closely related to the insertion point and embody some desirable HTPs of the target class as well.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": "Many studies have investigated the security of traditional machine learning methods (Barreno et al. 2006) and proposed some attack methods, including the black-box attack (\u0160rndi\u0107 and Laskov 2014) and the white-box attack (Liang et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 16, "context": "2006) and proposed some attack methods, including the black-box attack (\u0160rndi\u0107 and Laskov 2014) and the white-box attack (Liang et al.", "startOffset": 71, "endOffset": 95}, {"referenceID": 8, "context": "2006) and proposed some attack methods, including the black-box attack (\u0160rndi\u0107 and Laskov 2014) and the white-box attack (Liang et al. 2016).", "startOffset": 121, "endOffset": 140}, {"referenceID": 4, "context": "various methods to generate adversarial samples against DNNs, including gradient-based (Goodfellow et al. 2015; Kereliuk et al. 2015; Nguyen et al. 2015), decision function-based (Papernot et al.", "startOffset": 87, "endOffset": 153}, {"referenceID": 5, "context": "various methods to generate adversarial samples against DNNs, including gradient-based (Goodfellow et al. 2015; Kereliuk et al. 2015; Nguyen et al. 2015), decision function-based (Papernot et al.", "startOffset": 87, "endOffset": 153}, {"referenceID": 11, "context": "various methods to generate adversarial samples against DNNs, including gradient-based (Goodfellow et al. 2015; Kereliuk et al. 2015; Nguyen et al. 2015), decision function-based (Papernot et al.", "startOffset": 87, "endOffset": 153}, {"referenceID": 13, "context": "2015), decision function-based (Papernot et al. 2016b; Moosavi-Dezfooli et al. 2016), and evolution-based (Nguyen et al.", "startOffset": 31, "endOffset": 84}, {"referenceID": 10, "context": "2015), decision function-based (Papernot et al. 2016b; Moosavi-Dezfooli et al. 2016), and evolution-based (Nguyen et al.", "startOffset": 31, "endOffset": 84}, {"referenceID": 11, "context": "2016), and evolution-based (Nguyen et al. 2015).", "startOffset": 27, "endOffset": 47}, {"referenceID": 14, "context": "introduced a defense named defensive distillation to adversarial samples (Papernot et al. 2016c).", "startOffset": 73, "endOffset": 96}, {"referenceID": 1, "context": "However, as pointed in (Carlini and Wagner 2016), how to construct defenses that are robust to adversarial examples remains open.", "startOffset": 23, "endOffset": 48}, {"referenceID": 4, "context": "The adversarial training is a straightforward defense technique which uses as many as possible adversarial samples during training process as a kind of regularization (Goodfellow et al. 2015; Kereliuk et al. 2015; Papernot et al. 2016b).", "startOffset": 167, "endOffset": 236}, {"referenceID": 5, "context": "The adversarial training is a straightforward defense technique which uses as many as possible adversarial samples during training process as a kind of regularization (Goodfellow et al. 2015; Kereliuk et al. 2015; Papernot et al. 2016b).", "startOffset": 167, "endOffset": 236}, {"referenceID": 13, "context": "The adversarial training is a straightforward defense technique which uses as many as possible adversarial samples during training process as a kind of regularization (Goodfellow et al. 2015; Kereliuk et al. 2015; Papernot et al. 2016b).", "startOffset": 167, "endOffset": 236}, {"referenceID": 15, "context": "A study (Shaham et al. 2015) also shows that", "startOffset": 8, "endOffset": 28}, {"referenceID": 12, "context": "(Papernot et al. 2016a) proposed a Jacobian-based dataset augmentation technique to train a substitute model for a target DNN-based on limited pairs of inputs-outputs, without accessing its model, parameters, or training data.", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Besides, according to some studies (Szegedy et al. 2014; Goodfellow et al. 2015), adversarial samples can transfer from one model to another, even if the second model has a different architecture or was trained on a different set.", "startOffset": 35, "endOffset": 80}, {"referenceID": 4, "context": "Besides, according to some studies (Szegedy et al. 2014; Goodfellow et al. 2015), adversarial samples can transfer from one model to another, even if the second model has a different architecture or was trained on a different set.", "startOffset": 35, "endOffset": 80}, {"referenceID": 9, "context": ", word2vec (Mikolov et al. 2013).", "startOffset": 11, "endOffset": 32}], "year": 2017, "abstractText": "Deep neural networks (DNNs) play a key role in many applications. Current studies focus on crafting adversarial samples against DNN-based image classifiers by introducing some imperceptible perturbations to the input. However, DNNs for natural language processing have not got the attention they deserve. In fact, the existing perturbation algorithms for images cannot be directly applied to text. This paper presents a simple but effective method to attack DNN-based text classifiers. Three perturbation strategies, namely insertion, modification, and removal, are designed to generate an adversarial sample for a given text. By computing the cost gradients, what should be inserted, modified or removed, where to insert and how to modify are determined effectively. The experimental results show that the adversarial samples generated by our method can successfully fool a state-of-the-art model to misclassify them as any desirable classes without compromising their utilities. At the same time, the introduced perturbations are difficult to be perceived. Our study demonstrates that DNN-based text classifiers are also prone to the adversarial sample attack.", "creator": "Microsoft\u00ae Word 2016"}}}