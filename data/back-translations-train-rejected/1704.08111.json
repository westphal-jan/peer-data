{"id": "1704.08111", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "A Popperian Falsification of AI - Lighthill's Argument Defended", "abstract": "The area of computation called artificial intelligence (AI) is falsified by describing a previous 1972 falsification of AI by British applied mathematician James Lighthill. It is explained how Lighthill's arguments continue to apply to current AI. It is argued that AI should use the Popperian scientific method in which it is the duty of every scientist to attempt to falsify theories and if theories are falsified to replace or modify them. The paper describes the Popperian method in detail and discusses Paul Nurse's application of the method to cell biology that also involves questions of mechanism and behavior. Arguments used by Lighthill in his original 1972 report that falsifed AI are discussed. The Lighthill arguments are then shown to apply to current AI. The argument uses recent scholarship to explain Lighthill's assumptions and to show how the arguments based on those assumptions continue to falsify modern AI. An iimportant focus of the argument involves Hilbert's philosophical programme that defined knowledge and truth as provable formal sentences. Current AI takes the Hilbert programme as dogma beyond criticism while Lighthill as a mid 20th century applied mathematician had abandoned it. The paper uses recent scholarship to explain John von Neumann's criticism of AI that I claim was assumed by Lighthill. The paper discusses computer chess programs to show Lighthill's combinatorial explosion still applies to AI but not humans. An argument showing that Turing Machines (TM) are not the correct description of computation is given. The paper concludes by advocating studying computation as Peter Naur's Dataology.", "histories": [["v1", "Sun, 23 Apr 2017 21:16:40 GMT  (34kb)", "http://arxiv.org/abs/1704.08111v1", "8 Pages"]], "COMMENTS": "8 Pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["steven meyer"], "accepted": false, "id": "1704.08111"}, "pdf": {"name": "1704.08111.pdf", "metadata": {"source": "CRF", "title": "A Popperian Falsification of AI - Lighthill\u2019s Argument Defended", "authors": ["Steven Meyer"], "emails": ["smeyer@tdl.com"], "sections": [{"heading": "1. Introduction", "text": "This paper uses the method of falsification discovered by Karl Popper to show that artificial intelligence (AI) programs are not intelligent and are in fact normal computer programs in which programmers express their ideas by writing computer code. AI is meaningless metaphysics in the Popperian sense of metaphysics, based on a series of false assumptions and dogmas falsified by James Lighthill in his assessment of AI for the British Science Financing Agency (Lighthill [1972]). This paper defends Lighthill's falsification of AI in the 20th century and explains how it relates to current AI.This paper presents material developed by the author after he was encouraged to criticize AI as a student at Stanford University in the 1960s, and from a lecture given by Paul Feyerabend in the seminar on philosophy of science in the evening while the author was a computer science student at UC Berkeley. To understand why Lighthill's criticisms of 21st century physics are still falsified after AI's second decade, and why his arguments are still falsified in the 21st century."}, {"heading": "2. What is Popperian falsification", "text": "There are only a few statements that have any meaning. Such statements can be refuted either by scientific experimentation or by logic. Popper's most important contribution to the philosophy of science is to insist that it is the duty of every scientist to fully criticize his own theories, so that false theories can be modified or replaced. Popper's most important contribution consists of numerous bold assumptions, which are then falsified. Popper's method gives rise to bold assumptions followed by severe criticism. Popper's original falsification theory was called naive falsification in the late 1920s and early 1930s."}, {"heading": "3. Lighthill\u2019s falsification of AI", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "3.1 Understanding Lighthill\u2019s falsification in modern terms", "text": "In 1972, Lighthill falsified artificial intelligence by showing that his individual claims were false, and by arguing that there was no unified theme, but only normal problems in calculating computer applications and studying data. I believe AI researchers were not convinced at the time because Lighthill had not clearly explained his Popperian view of science. The rest of this paper discusses how scientific background knowledge of the 1970s, especially in physics and applied mathematics, distorts current AI methods. Discussion is possible because of recent scholarship, particularly in the areas of Hilbert's philosophical program and in examining the thinking of John von Neumann during the development of digital computers."}, {"heading": "4. Skepticism toward Hilbert\u2019s programme of truth as formal proof", "text": "In the 1920s, the mathematician David Hilbert surmised that knowledge and truth consist exclusively of all propositions that can be proved by axioms. Hilbert's initial conjecture was a mathematical problem, but it was interpreted as a philosophical theory in which truth became formal proof by axioms. Hilbert's program as the basic assumption of AI is that knowledge of the world can be expressed as formal propositions. Knowledge is then expressed as formulas that can be derived by logic (usually dictated by calculus). Hilbert's program as the basic assumption of AI is that knowledge of the world can be expressed as formal propositions."}, {"heading": "4.1 Von Neumann\u2019s argument automata and neural networks useless at high levels of complexity", "text": "In the second half of the 20th century, the work of John von Neumann on computers and calculations was widely accepted, but the publication of Von Neumann's work on computer technology came years after Lighthill's forgery had been written (in particular Aspray [1990], Neumann [2005], and Kohler [2001]). However, as an applied mathematician, Lighthill was certainly familiar with Von Neumann's work. John Von Neumann investigated automatons and neural networks when he developed his Von Neumann computer architecture. Von Neumann combined all his skepticism about linguistics and automatons as sources of AI algorithms in discussing formal neural networks when he wrote: The insight that a formal neural network can do anything that can be described in words is a very important insight and greatly simplifies things at low complication levels. It is by no means certain that it is a simplification at high complication levels. It is entirely possible that the value of the theorem on high complication level 94 is not true, namely, that Neumann can be expressed in the opposite direction."}, {"heading": "4.2 Skepticism toward linguistics and formal languages in computing", "text": "Starting with Ludwig Wittgenstein in the late 1930s, skepticism about linguistics, and especially formal languages, prevailed. Wittgenstein's claim was that mathematical (and other) language was nothing more than showing (Wittgenstein [1930]). Popperians and English science in general were receptive to Wittgenstein and his \"pointing philosophy\" of mathematics. Popperians avoided linguistic philosophy because they saw it as creating more problems than solving them."}, {"heading": "5. Physicist skepticism towards mathematics as axiomatized logic", "text": "s program. Physicists have always been skeptical of axiomatized mathematics. Albert Einstein expressed this skepticism in his lecture on geometry in 1921. Einstein believed that formal mathematics was incomplete and detached from physical reality. Einstein stated: This view of axioms held by modern axiomatics removes mathematics from all foreign elements.... Such an explicit representation of mathematics also makes it clear that mathematics as such cannot say anything about objects of our intuition or real objects (Einstein [1921]). Niels Bohr argued that conceptual theory comes first and then calculation."}, {"heading": "6. Finsler\u2019s rejection of axiomatics and general 1926 inconsistency result", "text": "In addition to skepticism about axiomatics, there was also skepticism about set theory and its core assertion that only propositions can exist that can be derived from axioms (probably Zermelo Fraenkel). Swiss mathematician Paul Finsler believed that mathematics exists outside language (formal propositions). Finsler claimed to have shown incompleteness in formal systems before Goedel in 1925, and that his proof was superior because he was not bound by Russell's logic like Goethe. See \"A Restoration of Failure: Paul Finsler's set theory\" in Breger [1996], p. 257 for discussion of Finsler's result on indecisiveness and formal proofs and his history (also Finsler [1996] and Finsler [1969]."}, {"heading": "7. Chess - elite human players response to chess programs", "text": "The response of the world's best chess players shows that Lighthill's claims that even in a formally condemned toy world combinatorial explosions limit the ability to solve problems with algorithms, the study of chess programs and the evaluation of their effectiveness show the problems with the recent claims of AI successes in general. In 1997, the Deep Blue chess program defeated the then world champion Gary Kasperow. Since then, the world's best chess players have adapted to computer chess programs. Leonard Barton wrote in the Financial Times newspaper column of 31 December in reference to the US champion Fabiano Caruana: the US champion and number 2 in the world unleashed a brilliant opening novelty, which by the way showed the limits of the most powerful computers (Barton [2016])."}, {"heading": "8. Turing Machine incorrect model for computation", "text": "The central argument for artificial intelligence is based on the Church Turing thesis: Turing machines (TM) are universally applicable and anything that includes intelligence can be calculated by TMs. Based on Lighthill's combinatorial explosion arguments, it seems to me that TMs are the wrong computing model. Instead, another computing model called MRAMS (random access machines with unit multiple and a limited number of unbounded size memory cells) is a better computing model (Meyer [2016]). Von Neumann understood the need for random access memory in his design of the von Neumann architecture (ibid. pp. 5-6). For MRAM machines, deterministic and non-deterministic computations are solvable both in polynomially bound time, so that at least for some problems in class NP the combinatorial explosion is mitigated. This suggests that algorithms should be investigated as normal data processing, because the assumption of AI that hayristics and the rate somehow improve algorithms is problematic."}, {"heading": "9. Conclusion - suggestion to replace AI with Naur\u2019s Dataology", "text": "One problem with this work is that people who were trained primarily by physicists before the 1970s cannot imagine that AI has any content, but that people who were trained as object-oriented programming after the formalization of CS, computer programs that are verified by proofs of correctness and axiomatized proofs of the efficiency of algorithms, cannot imagine anything other than computation as formalized logic. Computer researchers trained after the 1970s are not able to imagine alternatives to the AI dogmas. My suggestion is to adopt the ideas of the Danish computer scientist who was trained as an astronomer, Peter Naur. Naur argued that computation should be studied as dataology. Dataology is a theoretically neutral term for the study of data. Naur wrote \"Spiritual life during the twentieth century is completely misguided into an ideological position in which only discussions that the computer-inspired form CNaur recognized in 2007 [87] that Naur's ideas were separated from those of the 1990s."}, {"heading": "10. References", "text": "[1990] Aspray, W. John von Neumann and The Origins of Modern Computing. MIT Press, 1990. Barton [2016] Barton, L. Chess column, Financial Times. Games page weekend life and style section, December 30 and January 1 edition, 2016. Birkhoff [1936] Birkhoff, G. and Von Neumann, J. The Logic of Quantum Mechanics. Annals of Math. 27, no. 4 (1936), 721-734. Breger, H. A Restoration that failed: Paul Finsler's theory of sets. In Gillies, D. Revolutions in Mathematics. Oxford, 1992, 264. Budiansky, S. Blackett's War: The Men Who Defeated the Nazi U-Boats and Brought Sciene to the Art of Warfare. Knopf, 2013. Copeland 2015."}], "references": [{"title": "The Logic of Quantum Mechanics", "author": ["G. Birkhoff", "J. Von Neumann"], "venue": "Annals of Math. 27, no", "citeRegEx": "Birkhoff and Neumann,? \\Q1936\\E", "shortCiteRegEx": "Birkhoff and Neumann", "year": 1936}, {"title": "A Restoration that failed: Paul Finsler\u2019s theory of sets", "author": ["H. Breger"], "venue": "In Gillies, D. ed. Revolutions in Mathematics. Oxford,", "citeRegEx": "Breger,? \\Q1992\\E", "shortCiteRegEx": "Breger", "year": 1992}, {"title": "Blackett\u2019s War: The Men Who Defeated the Nazi U-Boats and Brought Sciene to the Art of Warfare", "author": ["S. Budiansky"], "venue": null, "citeRegEx": "Budiansky,? \\Q2013\\E", "shortCiteRegEx": "Budiansky", "year": 2013}, {"title": "The Church-turing thesis", "author": ["J. Copeland"], "venue": "The Stanford Encyclopedia of Philosophy (Summer 2015 Edition),", "citeRegEx": "Copeland,? \\Q2015\\E", "shortCiteRegEx": "Copeland", "year": 2015}, {"title": "Hilbert\u2019s programme and formalism", "author": ["M. Detlefsen"], "venue": "Routledge Encyclopedia of Philosophy. Feb", "citeRegEx": "Detlefsen,? \\Q2017\\E", "shortCiteRegEx": "Detlefsen", "year": 2017}, {"title": "Geometry and Experience", "author": ["A. Einstein"], "venue": "Lecture before Prussian Academy of Sciences. Berlin, January", "citeRegEx": "Einstein,? \\Q1921\\E", "shortCiteRegEx": "Einstein", "year": 1921}, {"title": "Finsler set theory: Platonism and circularity", "author": ["P. Finsler"], "venue": "D. Booth and R. Ziegler eds. Birkhauser,", "citeRegEx": "Finsler,? \\Q1996\\E", "shortCiteRegEx": "Finsler", "year": 1996}, {"title": "Why Von Neumann Rejected Carnap\u2019s Duality of Information Concepts", "author": ["E. Kohler"], "venue": "John von Neumann and the Foundations of Quantum Physics. Vienna Circle Institute Yearbook", "citeRegEx": "Kohler,? \\Q2001\\E", "shortCiteRegEx": "Kohler", "year": 2001}, {"title": "Falsification and the methodology of scientific research programmes", "author": ["I. Lakatos"], "venue": "Criticism and the growth of knowledge.,", "citeRegEx": "Lakatos,? \\Q1970\\E", "shortCiteRegEx": "Lakatos", "year": 1970}, {"title": "Artificial Intelligence: A General Survey\" in Artificial Intelligence: a paper symposium", "author": ["J. Lighthill"], "venue": "UK Science Research Council,", "citeRegEx": "Lighthill,? \\Q1973\\E", "shortCiteRegEx": "Lighthill", "year": 1973}, {"title": "Adding Methodological Testing to Naur\u2019s Anti-formalism", "author": ["S. Meyer"], "venue": null, "citeRegEx": "Meyer,? \\Q2013\\E", "shortCiteRegEx": "Meyer", "year": 2013}, {"title": "Philosophical Solution to P=?NP: P is equal to NP", "author": ["S. Meyer"], "venue": "IACAP", "citeRegEx": "Meyer,? \\Q2013\\E", "shortCiteRegEx": "Meyer", "year": 2013}, {"title": "Knowing and the mystique of logic and rules", "author": ["P. Naur"], "venue": "Kluwer Academic,", "citeRegEx": "Naur,? \\Q1995\\E", "shortCiteRegEx": "Naur", "year": 1995}, {"title": "Computing as science\", in An anatomy of human mental life", "author": ["P. Naur"], "venue": "naur.com Publishing,", "citeRegEx": "Naur,? \\Q2005\\E", "shortCiteRegEx": "Naur", "year": 2005}, {"title": "Computing versus human thinking", "author": ["P. Naur"], "venue": "Comm. ACM 50(1),", "citeRegEx": "Naur,? \\Q2007\\E", "shortCiteRegEx": "Naur", "year": 2007}, {"title": "Conversations - pluralism in software engineering", "author": ["P. Naur"], "venue": "E. Daylight ed. Belgium:Lonely Scholar Publishing,", "citeRegEx": "Naur,? \\Q2011\\E", "shortCiteRegEx": "Naur", "year": 2011}, {"title": "How Philosophy Drives Discovery: A scientists view of Popper\" 2016 Popper Memorial Lecture", "author": ["P. Nurse"], "venue": "London School of Economics Podcast,", "citeRegEx": "Nurse,? \\Q2016\\E", "shortCiteRegEx": "Nurse", "year": 2016}, {"title": "The Logic of Scientific Discovery", "author": ["K. Popper"], "venue": "(original in german 1934)", "citeRegEx": "Popper,? \\Q1968\\E", "shortCiteRegEx": "Popper", "year": 1968}, {"title": "Birkhoff and Von Neumann\u2019s Interpretation of Quantum Mechanics", "author": ["K. Popper"], "venue": "Nature", "citeRegEx": "Popper,? \\Q1968\\E", "shortCiteRegEx": "Popper", "year": 1968}, {"title": "Hilberts Program. The Stanford Encyclopedia of Philosophy (Spring 2016 Edition), 2016", "author": ["R. Zach"], "venue": "URL Feb", "citeRegEx": "Zach,? \\Q2017\\E", "shortCiteRegEx": "Zach", "year": 2017}], "referenceMentions": [{"referenceID": 9, "context": "AI is meaningless metaphysics in the Popperian sense of metaphysics based on a number of incorrect assumptions and dogmas that was falsified by James Lighthill in his evaluation of AI for the British science funding agency (Lighthill[1972]).", "startOffset": 150, "endOffset": 240}, {"referenceID": 17, "context": "Only singular statements Popper calls basic statements that have simple structure have meaning. Such statements can be disproven either by scientific experiments or by logic (Popper[1968], p.", "startOffset": 25, "endOffset": 188}, {"referenceID": 8, "context": "Popper\u2019s original falsification theory developed in the late 1920s and early 1930s is called naive falsification (Lakatos[1999], pp.", "startOffset": 114, "endOffset": 128}, {"referenceID": 8, "context": "Popper\u2019s original falsification theory developed in the late 1920s and early 1930s is called naive falsification (Lakatos[1999], pp. 64-85). The theory was improved and generalized by Popper and his colleagues during most of the 20th century. I am using the term Popperian philosophy in a sense that includes the modifications and improvement to Popper\u2019s theory mostly carried out at the London School of Economics not just by Popper but also by: Imre Lakatos, Paul Feyerabend and Thomas Kuhn. The other aspects of Popperian methodology is most clearly expressed by Imre Lakatos as the Methodology of Scientific Research Pro grammes (MSRP) (Lakatos[1970]).", "startOffset": 114, "endOffset": 655}, {"referenceID": 16, "context": "Paul Nurse in his 2016 Popper Memorial Lecture discusses the importance of bold conjectures and diligent attempts to eliminate incorrect theory by falsification (Nurse[2016]).", "startOffset": 5, "endOffset": 174}, {"referenceID": 3, "context": "In addition to the belief that knowledge is formal sentences, the foundation of AI is the belief that the Church\u2019-Turing Thesis (Copeland[2015]) is true.", "startOffset": 129, "endOffset": 144}, {"referenceID": 18, "context": "Zach[2015] Stanford Encyclopedia of Philosophy article discusses some attempts to mitigate Goedel\u2019s results.", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "See Detlefsen[2017]) for a more skeptical view of Hilbert\u2019s programme.", "startOffset": 4, "endOffset": 20}, {"referenceID": 8, "context": "Publication of Von Neumann\u2019s work on computing did not occur until years after Lighthill\u2019s falsification was written (in particular Aspray[1990], Neumann[2005] and Kohler[2001]).", "startOffset": 79, "endOffset": 145}, {"referenceID": 8, "context": "Publication of Von Neumann\u2019s work on computing did not occur until years after Lighthill\u2019s falsification was written (in particular Aspray[1990], Neumann[2005] and Kohler[2001]).", "startOffset": 79, "endOffset": 160}, {"referenceID": 7, "context": "Publication of Von Neumann\u2019s work on computing did not occur until years after Lighthill\u2019s falsification was written (in particular Aspray[1990], Neumann[2005] and Kohler[2001]).", "startOffset": 164, "endOffset": 177}, {"referenceID": 7, "context": "Edward Kohler (Kohler[2000]), p.", "startOffset": 7, "endOffset": 28}, {"referenceID": 2, "context": "The problem context in the area of operations research solution space searching that influenced both von Neumann and Lighthill was pre computer algorithmic operations research experience (see Budiansky[2013] for the detailed story).", "startOffset": 192, "endOffset": 208}, {"referenceID": 5, "context": "such an expurgated exposition of mathematics makes it also evident that mathematics as such cannot predicate anything about objects of our intuition or real objects (Einstein[1921]).", "startOffset": 166, "endOffset": 181}, {"referenceID": 1, "context": "See \"A Restoration the failed: Paul Finsler\u2019s theory of sets\" in Breger[1996], p.", "startOffset": 65, "endOffset": 78}, {"referenceID": 1, "context": "See \"A Restoration the failed: Paul Finsler\u2019s theory of sets\" in Breger[1996], p. 257 for discussion of Finsler\u2019s result on undecidability and formal proofs and its history (also Finsler[1996] and Finsler[1969].", "startOffset": 65, "endOffset": 193}, {"referenceID": 1, "context": "See \"A Restoration the failed: Paul Finsler\u2019s theory of sets\" in Breger[1996], p. 257 for discussion of Finsler\u2019s result on undecidability and formal proofs and its history (also Finsler[1996] and Finsler[1969].", "startOffset": 65, "endOffset": 211}, {"referenceID": 9, "context": "Applying Lighthill\u2019s combinatorial explosion arguments, it seems to me that TMs are the wrong model of computation. Instead a different computational model called MRAMS (random access machines with unit multiply and a bounded number of unbounded size memory cells) is a better model of computation (Meyer[2016]).", "startOffset": 9, "endOffset": 311}, {"referenceID": 12, "context": "My suggestion is to adopt the ideas of Danish computer scientist, who was trained as an astronomer, Peter Naur. Naur argued that computation should be studied as Dataology. Dataology is a theory neutral term for studying data. Naur wrote \"mental life during the twentieth century has become entirely misguided into an ideological position such that only discussions that adopt the computer inspired form\" are accepted. (Naur[2007], 87).", "startOffset": 106, "endOffset": 431}, {"referenceID": 12, "context": "in Software Engineering (Naur[2011]).", "startOffset": 25, "endOffset": 36}, {"referenceID": 12, "context": "in Software Engineering (Naur[2011]). This books amplifies the program development method Naur described in his 2005 Turing Award lecture (Naur[2007]).", "startOffset": 25, "endOffset": 150}, {"referenceID": 12, "context": "in Software Engineering (Naur[2011]). This books amplifies the program development method Naur described in his 2005 Turing Award lecture (Naur[2007]). In Naur[2011] page 30, the interviewer asks \".", "startOffset": 25, "endOffset": 166}], "year": 2017, "abstractText": "The area of computation called artificial intelligence (AI) is falsified by describing a previous 1972 falsification of AI by British applied mathematician James Lighthill. It is explained how Lighthill\u2019s arguments continue to apply to current AI. It is argued that AI should use the Popperian scientific method in which it is the duty of every scientist to attempt to falsify theories and if theories are falsified to replace or modify them. The paper describes the Popperian method in detail and discusses Paul Nurse\u2019s application of the method to cell biology that also involves questions of mechanism and behavior. Arguments used by Lighthill in his original 1972 report that falsifed AI are discussed. The Lighthill arguments are then shown to apply to current AI. The argument uses recent scholarship to explain Lighthill\u2019s assumptions and to show how the arguments based on those assumptions continue to falsify modern AI. An iimportant focus of the argument involves Hilbert\u2019s philosophical programme that defined knowledge and truth as provable formal sentences. Current AI takes the Hilbert programme as dogma beyond criticism while Lighthill as a mid 20th century applied mathematician had abandoned it. The paper uses recent scholarship to explain John von Neumann\u2019s criticism of AI that I claim was assumed by Lighthill. The paper discusses computer chess programs to show Lighthill\u2019s combinatorial explosion still applies to AI but not humans. An argument showing that Turing Machines (TM) are not the correct description of computation is given. The paper concludes by advocating studying computation as Peter Naur\u2019s Dataology.", "creator": "groff version 1.18.1.4"}}}