{"id": "1704.08243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset", "abstract": "Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.", "histories": [["v1", "Wed, 26 Apr 2017 17:57:59 GMT  (3667kb,D)", "http://arxiv.org/abs/1704.08243v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["aishwarya agrawal", "aniruddha kembhavi", "dhruv batra", "devi parikh"], "accepted": false, "id": "1704.08243"}, "pdf": {"name": "1704.08243.pdf", "metadata": {"source": "CRF", "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset", "authors": ["Aishwarya Agrawal", "Aniruddha Kembhavi", "Dhruv Batra", "Devi Parikh"], "emails": ["aish@vt.edu,", "anik@allenai.org,", "parikh}@gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that the two are two persons who have been in Turkey in the past."}, {"heading": "2 Related Work", "text": "This year, it has reached the stage where it will be able to take the lead, in the same way as it has done in recent years."}, {"heading": "3 Compositional Visual Question Answering (C-VQA)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 C-VQA Creation", "text": "In fact, it is a way of playing by the rules that you have imposed on yourself, \"he told the Deutsche Presse-Agentur.\" It's like playing by the rules, \"he said.\" But it's not like playing by the rules. \"He added,\" It's like playing by the rules. \""}, {"heading": "3.2 C-VQA Analysis", "text": "In this section we analyze how the distribution of questions and answers in the question C-VQA train and test splits differ from those in the VQA v1.0 train and val splits. question distribution. Fig. 2 shows the distribution of questions based on the first four words of the questions for the train (left) and test (right) splits of the C-VQA data set. We can see that the distribution of the data set compositively (as in the VQA v1.0 val split) does not result in significant differences in the distribution of questions about splits, where the distributions are qualitatively similar to VQA v1.0 splits [5]. Quantitatively 46.06% of the question strings in the VQA v1.0 val split also in the VQA v1.0 train split, whereas this percentage is 37.76 for the C-VQA splits."}, {"heading": "4 Baselines", "text": "In fact, most of them will be able to move into a different world, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "5 Conclusion", "text": "Finally, we present a new setting for Visual Question Answering - Compositional Visual Question Answering. Under this setting, the question-answer pairs in the test set are compositional innovations compared to the question-answer pairs in the training set. We create a compositional split of the VQA (v1.0) dataset [5], called C-VQA, which facilitates the training of compositional VQA models. We show the similarities and differences between the VQA v1.0 and C-VQA splits. Finally, we report on the performance of several existing VQA models in the C-VQA splits and show that the performance of all models decreases significantly compared to the original VQA v1.0 setting. This suggests that today's VQA models do not master compositivity well and that C-VQA splits can be used as a benchmark for building and evaluating compositional VQA models."}], "references": [{"title": "Analyzing the behavior of visual question answering models", "author": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh"], "venue": "In EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Balancing and answering binary visual questions", "author": ["Peng Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh. Yin", "Yang"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1612.00837,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick"], "venue": "arXiv preprint arXiv:1612.06890,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Visual Dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 M.F. Moura", "Stefan Lee", "Dhruv Batra"], "venue": "arXiv preprint arXiv:1703.06585,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Guesswhat?! visual object discovery through multi-modal dialogue", "author": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1611.08481,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Image-grounded conversations: Multimodal context for natural question and response", "author": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P. Spithourakis", "Lucy Vanderwende"], "venue": "generation. CoRR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Visual turing test for computer vision systems", "author": ["Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "ABC-CNN: an attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "CoRR, abs/1511.05960,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Compositional memory for visual question answering", "author": ["Aiwen Jiang", "Fang Wang", "Fatih Porikli", "Yi Li"], "venue": "CoRR, abs/1511.05676,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Deep compositional question answering with neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Explicit knowledgebased reasoning for visual question answering", "author": ["Peng Wang", "Qi Wu", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": "CoRR, abs/1511.02570,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Answer-type prediction for visual question answering", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In NAACL,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J. Shih", "Saurabh Singh", "Derek Hoiem"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Multimodal residual learning for visual QA", "author": ["Jin-Hwa Kim", "Sang-Woo Lee", "Dong-Hyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Training recurrent answering units with joint loss minimization for vqa", "author": ["Hyeonwoo Noh", "Bohyung Han"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Relevance feedback in image retrieval: A comprehensive review", "author": ["Xiang Sean Zhou", "Thomas S. Huang"], "venue": "Proceedings of ACM Multimedia Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Kuniaki Saito", "Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "venue": "CoRR, abs/1606.06108,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": "https://github.com/VT-vision-lab/VQA_LSTM_CNN,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Decorrelating semantic visual attributes by resisting the urge to share", "author": ["Dinesh Jayaraman", "Fei Sha", "Kristen Grauman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Learning to generalize to new compositions in image understanding", "author": ["Yuval Atzmon", "Jonathan Berant", "Vahid Kezami", "Amir Globerson", "Gal Chechik"], "venue": "arXiv preprint arXiv:1608.07639,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Zero-shot visual question answering", "author": ["Damien Teney", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1611.05546,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "An empirical evaluation of visual question answering for novel objects", "author": ["Santhosh K Ramakrishnan", "Ambar Pal", "Gaurav Sharma", "Anurag Mittal"], "venue": "arXiv preprint arXiv:1704.02516,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 1, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 2, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 3, "context": "However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 27, "endOffset": 32}, {"referenceID": 4, "context": "0 [5] dataset, which we call Compositional VQA (C-VQA).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 6, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 7, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 8, "context": "VQA is a stepping stone to visually grounded dialog and intelligent agents [6\u20139].", "startOffset": 75, "endOffset": 80}, {"referenceID": 1, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 2, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 4, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 9, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 10, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 11, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 12, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 13, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 14, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 60, "endOffset": 76}, {"referenceID": 4, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 15, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 16, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 17, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 18, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 19, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 20, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 21, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 22, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 23, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 24, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 25, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 26, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 27, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 28, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 29, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 30, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 31, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 32, "context": "Various VQA datasets have been proposed by different groups [2, 3, 5, 10\u201315] and a number of deep-learning models have been developed [5, 16\u201333].", "startOffset": 134, "endOffset": 144}, {"referenceID": 0, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 1, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 2, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 3, "context": "However, it has been shown that despite recent progress, today\u2019s VQA models are heavily driven by superficial correlations in the training data and lack compositionality [1\u20134] \u2013 the ability to answer questions about unseen compositions of seen concepts.", "startOffset": 170, "endOffset": 175}, {"referenceID": 4, "context": "0 dataset [5], called Compositional VQA (C-VQA).", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 22, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 26, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 33, "context": "To demonstrate the difficulty of our C-VQA splits, we report the performance of several existing VQA models [17, 20, 23, 27, 34] on our C-VQA splits.", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "Our experiments show that the performance of the VQA models drops significantly (with performance drop being smaller for models which are compositional by design such as the Neural Module Networks [20]) when trained and evaluated on train and test splits (respectively) of C-VQA, compared to when these models are trained and evaluated on train and val splits (respectively) of the original VQA v1.", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 2, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 4, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 9, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 10, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 11, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 12, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 13, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 14, "context": "Several papers have proposed visual question answering datasets to train and test machines for the task of visual understanding [2, 3, 5, 10\u201315].", "startOffset": 128, "endOffset": 144}, {"referenceID": 11, "context": "For instance, one of the earliest VQA datasets [12] considers questions generated using templates and consists of fixed vocabulary of objects, attributes, etc.", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "[13] also consider questions whose answers come from a closed world.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] generate questions automatically using image captions and their", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 9, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 10, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 13, "context": "[5, 10, 11, 14] consist of free form open-ended questions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "0 dataset [5] has been used widely to train deep models.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "However, careful examination of the behaviors of such models reveals that these models are heavily driven by superficial correlations in the training data and lack compositionality [1].", "startOffset": 181, "endOffset": 184}, {"referenceID": 2, "context": "0 contains strong language priors which data-driven models can learn easily and can perform well on the test set which consists of similar priors as the training set, without truly understanding the visual content in images [3], because it is easier to learn the biases of the data (or even our world) than to truly understand images.", "startOffset": 224, "endOffset": 227}, {"referenceID": 2, "context": "[3] balance every question in the VQA v1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "Zero-shot object recognition using attributes is based on the idea of composing attributes to detect novel object categories [35, 36].", "startOffset": 125, "endOffset": 133}, {"referenceID": 35, "context": "Zero-shot object recognition using attributes is based on the idea of composing attributes to detect novel object categories [35, 36].", "startOffset": 125, "endOffset": 133}, {"referenceID": 36, "context": "More recently, [37] have studied compositionality in the domain of image captioning by focusing on structured representations (subject-relation-object triplets).", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "The work closest to us is [4] where they study compositionality in the domain of VQA.", "startOffset": 26, "endOffset": 29}, {"referenceID": 19, "context": "[20, 24] have developed compositional models for VQA that consist of different modules each specialized for a particular task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 23, "context": "[20, 24] have developed compositional models for VQA that consist of different modules each specialized for a particular task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 37, "context": "[38] propose a setting for VQA where the test questions (the question string itself or the multiple choices) contain atleast one unseen word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] propose answering questions about unknown objects (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "0 dataset [5]2.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "0) [5] Val 121,512 40,504 1,215,120", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "We then trained and evaluated the deeper LSTM Q + norm I model from [5] on these new splits.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "0 splits [5].", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "0 dataset, the distribution for a given question type is similar across train and val splits [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 33, "context": "Deeper LSTM Question + normalized Image (deeper LSTM Q + norm I) [34]: This model was proposed in [5].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "Deeper LSTM Question + normalized Image (deeper LSTM Q + norm I) [34]: This model was proposed in [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 39, "context": "For each image, the image channel extracts the activations (4096-dim) of the last hidden layer of the VGGNet [40] and normalizes them.", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "Neural Module Networks (NMN) [20]: This model is designed to be compositional in nature.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "Stacked Attention Networks (SAN) [17]: This is one of the widely used models for VQA.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "5 Hierarchical Question-Image Co-attention Networks (HieCoAtt) [23]: This is one of the top performing models for VQA.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "Multimodal Compact Bilinear Pooling (MCB) [27]: This model won the real image track of the VQA Challenge 2016.", "startOffset": 42, "endOffset": 46}, {"referenceID": 33, "context": "23 deeper LSTM Q + norm I [34] C-VQA test 70.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "83 NMN [20] C-VQA test 72.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "86 SAN [17] C-VQA test 66.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "09 HieCoAtt [23] C-VQA test 71.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "97 MCB [27] C-VQA test 71.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "0) dataset [5], called C-VQA, which facilitates training compositional VQA models.", "startOffset": 11, "endOffset": 14}], "year": 2017, "abstractText": "Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 [5] dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.", "creator": "LaTeX with hyperref package"}}}