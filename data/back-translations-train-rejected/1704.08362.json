{"id": "1704.08362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "A New Type of Neurons for Machine Learning", "abstract": "In machine learning, the use of an artificial neural network is the mainstream approach. Such a network consists of layers of neurons. These neurons are of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the 1st order neuron to the 2nd order neuron, empowering individual neurons, and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the 2nd order neurons. Finally, further topics are discussed.", "histories": [["v1", "Wed, 26 Apr 2017 22:02:25 GMT  (890kb)", "http://arxiv.org/abs/1704.08362v1", "5 pages, 8 figures, 11 references"]], "COMMENTS": "5 pages, 8 figures, 11 references", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["fenglei fan", "wenxiang cong", "ge wang"], "accepted": false, "id": "1704.08362"}, "pdf": {"name": "1704.08362.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "This year, the time has come for it to be able to get to grips with the problems that have been mentioned."}, {"heading": "II. SECOND ORDER MODEL AND OPTIMIZATION", "text": "The structure of a current neuron is shown in the form of Fig. 1, where we have 0w: b and 0x = 1. The linear functions of the input vector will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be."}, {"heading": "III. NUMERICAL RESULTS", "text": "In our pilot study, the proposed 2nd order neurons of two input variables were applied individually in fuzzy logic experiments = 2. Extension of classical Boolean logic, fuzzy logic has been extensively studied > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 3 and applied in recent decades, which manipulates vague or inaccurate logic statements. Compared to Boolean logic, the value of a fuzzy logic variable is at the interval [0, 1] because a practical judgment cannot be pure black (false or 0) or white (true or 1). Compared to Boolean logic, the value of a fuzzy logic variable can easily be used in a continuous variable due to its proximity to 0 or 1, For visualizing the color chart \"cool\" in MATLAB to represent the functional value at any point."}, {"heading": "B. NAND and NOR Gates", "text": "The XOR gate is exemplary, but not a universal logic gate. To further demonstrate the performance of the proposed 2nd order neuron, additional data sets were generated, each requiring blurred NAN and NOR operations for correct classification. Subsequently, training steps similar to those in Section A were repeated to simulate NAN and NOR operations. In this pilot study, the initial parameters were set to rw = [0.4, -0.1], gw = [0.3, 0.1], bw = [0.0], 1b = 0, 2b = 0, c = 1.3 and rw = [-1.1], gw = [1, -2], bw = [0.0], 1b = -0.5, 2b = 1, c = 0 for NAN and NOR tasks, respectively. Results are shown in Fig. 6 and 7."}, {"heading": "C. Concentric Rings", "text": "Another type of natural pattern of linearly inseparable isconcentric rings. For example, we created two concentric rings, each assigned to two classes. By using the initial parameters rw = [0.04, 01], gw = [0.03, -01], bw = [0.0.4], 1b = 0.1 and 2b = 0.2, c = 1.3, our second order neuron was trained, achieving an ideal result, as shown in Fig. 8, which completely separates the inner ring from the outer ring."}, {"heading": "IV. DISCUSSIONS AND CONCLUSION", "text": "This year, it is so far that it will be able to retaliate, \"he said.\" We have never hesitated so long, \"he said.\" We are not as far as it will be, \"he said."}], "references": [{"title": "Brain Tumor Segmentation using Convolutional Neural Networks in MRI Images", "author": ["S. Pereira", "A. Pinto", "V. Alves", "C.A. Silva"], "venue": "IEEE Trans Med Imaging, Mar 04, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Convolutional Neural  Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning", "author": ["H.C. Shin", "H.R. Roth", "M. Gao", "L. Lu", "Z. Xu", "I. Nogues", "J. Yao", "D. Mollura", "R.M. Summers"], "venue": "IEEE Trans Med Imaging, vol. 35, no. 5, pp. 1285-98, May, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network", "author": ["M. Anthimopoulos", "S. Christodoulidis", "L. Ebner", "A. Christe", "S. Mougiakakou"], "venue": "IEEE Trans Med Imaging, vol. 35, no. 5, pp. 1207-1216, May, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "A logical calculus of the ideas immanent in nervous activity. 1943", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "Bull Math Biol, vol. 52, no. 1-2, pp. 99-115; discussion 73-97, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Shape Classification Using Wasserstein Distance for Brain Morphometry Analysis", "author": ["Z. Su", "W. Zeng", "Y. Wang", "Z.L. Lu", "X. Gu"], "venue": "Inf Process Med Imaging, vol. 24, pp. 411-23, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv: 1701.07875, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep learning in neural networks: an overview", "author": ["J. Schmidhuber"], "venue": "Neural Netw, vol. 61, pp. 85-117, Jan, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning in medical imaging", "author": ["D. Shen", "G. Wu", "D. Zhang", "K. Suzuki", "F. Wang", "P. Yan"], "venue": "Comput Med Imaging Graph, vol. 41, pp. 1-2, Apr, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Training redundant artificial neural networks: imposing biology on technology", "author": ["D.A. Medler", "M.R. Dawson"], "venue": "Psychol Res, vol. 57, no. 1, pp. 54-62, 1994.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Artificial neural networks in biology and chemistry: the evolution of a new analytical tool", "author": ["H.M. Cartwright"], "venue": "Methods Mol Biol, vol. 458, pp. 1-13, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION N the field of machine learning, artificial neural networks (ANNs) especially deep neural networks (CNNs) have recently achieved remarkable successes in various types of applications such as classification, unsupervised learning, prediction, image processing and analysis [1-3].", "startOffset": 285, "endOffset": 290}, {"referenceID": 1, "context": "INTRODUCTION N the field of machine learning, artificial neural networks (ANNs) especially deep neural networks (CNNs) have recently achieved remarkable successes in various types of applications such as classification, unsupervised learning, prediction, image processing and analysis [1-3].", "startOffset": 285, "endOffset": 290}, {"referenceID": 2, "context": "INTRODUCTION N the field of machine learning, artificial neural networks (ANNs) especially deep neural networks (CNNs) have recently achieved remarkable successes in various types of applications such as classification, unsupervised learning, prediction, image processing and analysis [1-3].", "startOffset": 285, "endOffset": 290}, {"referenceID": 3, "context": "Excited by the tremendous potential of machine learning, major efforts are being made to improve machine learning methods [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "In particular, the Wasserstein distance was introduced for effective, efficient and stable performance of GAN [5, 6].", "startOffset": 110, "endOffset": 116}, {"referenceID": 5, "context": "In particular, the Wasserstein distance was introduced for effective, efficient and stable performance of GAN [5, 6].", "startOffset": 110, "endOffset": 116}, {"referenceID": 6, "context": "To our best knowledge, all ANNs/CNNs are currently constructed with neurons of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function [7-9].", "startOffset": 264, "endOffset": 269}, {"referenceID": 7, "context": "To our best knowledge, all ANNs/CNNs are currently constructed with neurons of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function [7-9].", "startOffset": 264, "endOffset": 269}, {"referenceID": 8, "context": "In biology, multiple types of cells are available to make various organisms [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "components contribute complementary strengths and compensate for others\u2019 weaknesses [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "The goal of machine learning is to find optimal parameters to minimize the objective function [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Compared with Boolean logic, the value of a fuzzy logic variable is on the interval [0, 1], since a practical judgement may not be purely black (false or 0) or white (true or 1).", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 42, "endOffset": 47}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 49, "endOffset": 54}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 59, "endOffset": 64}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 59, "endOffset": 64}, {"referenceID": 0, "context": "3 and r w =[-1,1], g w =[1,-2], b w =[0,0],", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "3 and r w =[-1,1], g w =[1,-2], b w =[0,0],", "startOffset": 24, "endOffset": 30}], "year": 2017, "abstractText": "\uf020 Abstract \u2014 In machine learning, the use of an artificial neural network is the mainstream approach. Such a network consists of layers of neurons. These neurons are of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the 1 order neuron to the 2 order neuron, empowering individual neurons, and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the 2 order neurons. Finally, further topics are discussed.", "creator": "Microsoft\u00ae Word 2016"}}}