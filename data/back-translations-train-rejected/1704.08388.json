{"id": "1704.08388", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "Duluth at Semeval-2017 Task 7 : Puns upon a midnight dreary, Lexical Semantics for the weak and weary", "abstract": "This paper describes the Duluth systems that participated in SemEval-2017 Task 7 : Detection and Interpretation of English Puns. The Duluth systems participated in all three subtasks, and relied on methods that included word sense disambiguation and measures of semantic relatedness.", "histories": [["v1", "Thu, 27 Apr 2017 00:29:17 GMT  (15kb)", "http://arxiv.org/abs/1704.08388v1", "5 pages, to Appear in the Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC"], ["v2", "Fri, 28 Apr 2017 01:16:07 GMT  (15kb)", "http://arxiv.org/abs/1704.08388v2", "5 pages, to Appear in the Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC"]], "COMMENTS": "5 pages, to Appear in the Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ted pedersen"], "accepted": false, "id": "1704.08388"}, "pdf": {"name": "1704.08388.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tpederse@d.umn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.08 388v 1 [cs.C L] 27 Apr 201 7, which participated in SemEval-2017 Task 7: Detection and Interpretation of English Pun. Duluth systems participated in all three subtasks and relied on methods that included word confusion and semantic kinship."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Systems", "text": "The evaluation data for each subtask consisted of individual, independent sentences. All sentences were tokenized so that each alphanumeric string was separated from an adjacent punctuation and the entire text converted to lowercase letters. Multiword expressions (connections) found in WordNet were identified. SemEval-2017 Task 7 (Miller et al., 2017) focused on word play recognition and was divided into three subtasks."}, {"heading": "2.1 Subtask 1", "text": "The problem in Subtask 1 was to determine whether a sentence contains (or does not contain) a pun. We relied on the premise that a sentence has a unique assignment of senses, and that this should apply even if the parameters of a literal disambiguation algorithm are varied. So, if a sentence has several possible assignments of senses based on http: / / wn-similarity.sourceforge.neton, the results of different runs of a literal disambiguation algorithm, then there is a possibility that a pun exists. To test this hypothesis, we ran theWordNet:: SenseRelate:: AllWords algorithm, which uses four different configurations, and then compared the four meaningful sentences with each other. If there were more than two differences in the assignments of meaning resulting from these different runs, then the sentence will probably contain a punch."}, {"heading": "2.2 Subtask 2", "text": "In Subtask 2, the evaluation data consists of the instances of Subtask 1 that contain puns. The task is to identify the pun. We chose two approaches to this subtask, both of which were influenced by our observation that puns often occur later in sentences. The first (run 1) was to rely on our word sense results from Subtask 1 and to identify the last word that changed the senses between different runs of WordNet:: SenseRelate:: AllWords clarification algorithm. We relied on two of the four configurations used in Subtask 1. We used the narrow and broad context of Subtask 1 without finding connections. We realized that this might cause us to overlook some cases where a pun was created with a connection, but our intuition was that the more common cases (especially for homographic puns) would be those without connections. Our second approach (run 2) was a simple starting point, where the last word was assumed to be the last word."}, {"heading": "2.3 Subtask 3", "text": "The question that arises is whether the solution to the problems is actually a solution or not, whether the solution is a solution, or whether the solution is a solution that is only a solution, or whether it is a solution that is only a solution that is capable of finding a solution."}, {"heading": "3 Results", "text": "We review our results in the three subtasks in this section. Table 1 refers to homographic results as hom and heterographic as het. Thus, the first run of Duluth systems on homographic data is referred to as Duluth-hom1, and the first run on heterographic data is Duluth-het1. The highest ranking system is indicated by high-hom and high-het. P and R stand for precision and memory, A for accuracy, and C for coverage. Rank x / y indicates that this system was x of the y participating systems."}, {"heading": "3.1 Subtask 1", "text": "Puns were found in 71% (1,271) of the heterographic and 71% of the homographic instances (1,607), suggesting that this sub-task would have a relatively high output, for example, if a system simply predicted that each sentence contained a pun. In light of this, we do not want to make too strong a claim about our approach, but it seems that focusing on sentences with multiple possible (and valid) meanings is promising for pun identification. Our method tended to over-predict puns and reported that a pun occurred in 84% (1,489 out of 1,780 instances) of heterographic data and 80% (1,791 out of 2,250 instances) of homographic data."}, {"heading": "3.2 Subtask 2", "text": "Subtask 2 consists of all instances of Subtask 1 that contained a pun, resulting in 1,489 heterographic puns and 1,791 homographic words.We see that our simple basic method of selecting the last word in content rather than the hallmarked word (run 2) significantly exceeded our more elaborate method (run 1) of determining which word has undergone more sensory changes in several variations of the disambiguation algorithm. We also see that run 1 has not performed particularly well with heterographic puns. In general, we believe that the difficulty experienced by run 1 is due to the general noise characteristic of literal disambiguation algorithms."}, {"heading": "3.3 Subtask 3", "text": "Subtask 3 consists of 1,298 homographic instances and 1,098 heterographic instances. We see that our method for homographs performed very well and was at the forefront of the participating systems. On the other hand, our heterographic approach was not very successful. We believe that the idea of generating alternative target words for heterographic word games is necessary because without it it it would be impossible to identify any of the senses of the dotted word. However, our Run-1 approach of simply using target word variations with an edit spacing of one did not capture the variations that are present in heterographic word games (e.g. Opening and Office have an edit spacing of 2).Our Run-2 approach of finding many different target words via the Datamuse API resulted in an overwhelming number of possibilities where the intended target word was very difficult to identify."}, {"heading": "4 Discussion and Future Work", "text": "One limitation of our approach is the uncertain degree of accuracy of word sense decoding algorithms, which vary from word to word and from domain to domain. Finding several possible senses for a single word can signal a pun or show the limits of a particular WSD algorithm. In addition, the contexts used in this evaluation were all individual sentences and relatively short. Whether or not more context is available would help or hinder these approaches is an interesting question.Heterographic puns presented a variety of challenges, especially mapping clever near spellings and near pronunciations into their intended form (e.g., angles as a triangle). Simply trying to map senses to try angles will obviously miss the pun, and so the ability to match similar-sounding phrases to the intended word is an ability that our systems have not been terribly successful with."}], "references": [{"title": "Extended gloss overlaps as a measure of semantic relatedness", "author": ["Satanjeev Banerjee", "Ted Pedersen."], "venue": "Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence. Acapulco, pages 805\u2013810.", "citeRegEx": "Banerjee and Pedersen.,? 2003", "shortCiteRegEx": "Banerjee and Pedersen.", "year": 2003}, {"title": "Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone", "author": ["M.E. Lesk."], "venue": "Proceedings of the 5th Annual International Conference on Systems Documentation. ACM Press, pages 24\u201326.", "citeRegEx": "Lesk.,? 1986", "shortCiteRegEx": "Lesk.", "year": 1986}, {"title": "SemEval-2017 Task 7: Detection and interpretation of English puns", "author": ["Tristan Miller", "Christian F. Hempelmann", "Iryna Gurevych."], "venue": "Proceedings of the 11th InternationalWorkshop on Semantic Evaluation (SemEval-2017). Vancouver, BC.", "citeRegEx": "Miller et al\\.,? 2017", "shortCiteRegEx": "Miller et al\\.", "year": 2017}, {"title": "Using measures of semantic relatedness for word sense disambiguation", "author": ["S. Patwardhan", "S. Banerjee", "T. Pedersen."], "venue": "Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics. Mexico City, pages", "citeRegEx": "Patwardhan et al\\.,? 2003", "shortCiteRegEx": "Patwardhan et al\\.", "year": 2003}, {"title": "SenseRelate::TargetWord - a generalized framework for word sense disambiguation", "author": ["S. Patwardhan", "S. Banerjee", "T Pedersen."], "venue": "Proceedings of the Demonstration and Interactive Poster Session of the 43rd Annual Meeting of the Association for", "citeRegEx": "Patwardhan et al\\.,? 2005", "shortCiteRegEx": "Patwardhan et al\\.", "year": 2005}, {"title": "UsingWordNetbased Context Vectors to Estimate the Semantic Relatedness of Concepts", "author": ["S. Patwardhan", "T. Pedersen."], "venue": "Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguis-", "citeRegEx": "Patwardhan and Pedersen.,? 2006", "shortCiteRegEx": "Patwardhan and Pedersen.", "year": 2006}, {"title": "WordNet::SenseRelate::AllWords - a broad coverage word sense tagger that maximizes semantic relatedness", "author": ["T. Pedersen", "V. Kolhatkar."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Pedersen and Kolhatkar.,? 2009", "shortCiteRegEx": "Pedersen and Kolhatkar.", "year": 2009}, {"title": "Wordnet::Similarity - Measuring the relatedness of concepts", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi."], "venue": "Proceedings of Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics. Boston, MA, pages", "citeRegEx": "Pedersen et al\\.,? 2004", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "getWord (Patwardhan et al., 2005) and AllWords (Pedersen and Kolhatkar, 2009).", "startOffset": 8, "endOffset": 33}, {"referenceID": 6, "context": ", 2005) and AllWords (Pedersen and Kolhatkar, 2009).", "startOffset": 21, "endOffset": 51}, {"referenceID": 3, "context": "Both have the goal of finding the assignment of senses in a context that maximizes their overall semantic relatedness (Patwardhan et al., 2003) according to measures in WordNet::Similarity (Pedersen et al.", "startOffset": 118, "endOffset": 143}, {"referenceID": 7, "context": ", 2003) according to measures in WordNet::Similarity (Pedersen et al., 2004).", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "We relied on the Extended Gloss Overlaps measure (lesk) (Banerjee and Pedersen, 2003) and the Gloss vector measure (vector) (Patwardhan and Pedersen, 2006).", "startOffset": 56, "endOffset": 85}, {"referenceID": 5, "context": "We relied on the Extended Gloss Overlaps measure (lesk) (Banerjee and Pedersen, 2003) and the Gloss vector measure (vector) (Patwardhan and Pedersen, 2006).", "startOffset": 124, "endOffset": 155}, {"referenceID": 1, "context": "The intuition behind a Lesk measure is that related words will be defined using some of the same words, and that recognizing these overlaps can serve as a means of identifying relationships between words (Lesk, 1986).", "startOffset": 204, "endOffset": 216}, {"referenceID": 2, "context": "SemEval\u20132017 Task 7 (Miller et al., 2017) focused on pun identification, and was divided into three subtasks.", "startOffset": 20, "endOffset": 41}], "year": 2017, "abstractText": "This paper describes the Duluth systems that participated in SemEval-2017 Task 7 : Detection and Interpretation of English Puns. The Duluth systems participated in all three subtasks, and relied on methods that included word sense disambiguation and measures of semantic relatedness.", "creator": "LaTeX with hyperref package"}}}