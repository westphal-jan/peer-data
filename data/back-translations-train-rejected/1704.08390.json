{"id": "1704.08390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "abstract": "This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.", "histories": [["v1", "Thu, 27 Apr 2017 00:40:33 GMT  (14kb)", "http://arxiv.org/abs/1704.08390v1", "5 pages, to Appear in the Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC"]], "COMMENTS": "5 pages, to Appear in the Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinru yan", "ted pedersen"], "accepted": false, "id": "1704.08390"}, "pdf": {"name": "1704.08390.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yanxx418@d.umn.edu", "tpederse@d.umn.edu", "@midnight"], "sections": [{"heading": null, "text": "ar Xiv: 170 4.08 390v 1 [cs.C L] 27 Apr 201 7This paper describes the Duluth system that participated in SemEval-2017 Task 6 # HashtagWars: Learning a Sense of Humor. The system participated in Tasks A and B and used N-gram language models, placing a high value on task evaluation. In this paper, the results of our system are discussed during the development and evaluation phase and from two re-evaluation runs."}, {"heading": "1 Introduction", "text": "Humour is an expression of human uniqueness and intelligence and has attracted attention in various fields, such as linguistics, psychology, philosophy and computer science. Computer humor draws from all of these areas and is a relatively new field of study. There are some historical problems that are able to generate humor (e.g. (Stock and Strapparava, 2003), (O \u00a4zbal and Strapparava, 2012), but humor recognition remains a less explored and challenging problem (e.g. (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015). SemEval 2017 task 6 (Potash et al., 2017) also focuses on humor recognition, asking participants to develop systems that learn a sense of humor from the Comedy Central TV show @ midnight with Chris Hardwick. Our system tweets in the way that they are funny, by linking messages to each other as gram models that are available."}, {"heading": "2 Background", "text": "A statistical language model estimates the probability of a word sequence or an upcoming word. An N-gram is a contiguous sequence of N-words: a unique piece is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. An N-gram is a contiguous sequence of N-words: a unique piece is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweets in ramen # SingleLifeIn3Words \"tears,\" \"in\" ramen \"and\" # SingleLifeIn3Words \"are unigrams;\" tears in \"we,\" in ramen # SingleLifeIn3Words \"are bigrams and\" tears in ramen # SingleLifeInpredictions. \""}, {"heading": "3 Method", "text": "Our system1 estimated the likelihood of tweets using Ngram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) in four steps: 1. Corpus preparation and pre-processing: All training data in a single file. Pre-processing included filtering and tokenization. 2. Language model training: Created N-gram language models using KenLM.3. Tweet rating: Calculated log probability for each tweet based on a trained N-gram language model. 4. Tweet prediction: Based on log probability values. \u2022 Subtask A - Compare and predict which funnier.1https: / / xinru1414.github.io / HumorDetectionSemEval2017-Task6 / \u2022 Subtask B - Given a number of tweets associated with a hashtag, tweets rank from the funniest to the least funny."}, {"heading": "3.1 Corpus Preparation and Pre-processing", "text": "The tweet data was provided by the task organizers and consists of 106 hashtag files consisting of approximately 21,000 tokens. The hashtag files were further divided into a test directory of 6 hashtags and a training set of 100 hashtags. In addition, we received 6.2 GB of English news data with approximately two million tokens from the News Commentary Corpus and News Crawl Corpus from 2008, 2010 and 20112. Each tweet and sentence from the news data is found in a single line in their respective files."}, {"heading": "3.1.1 Preparation", "text": "During the development of our system, we trained our language models exclusively on the 100 hashtag files of train dir and then rated our performance on the 6 hashtag files found in trial dir, which were formatted so that each tweet was found in a single line."}, {"heading": "3.1.2 Pre-processing", "text": "Pre-processing consists of two steps: filtering and tokenization. The filter step was only for the Tweet training corpus. During the development phase, we experimented with various filter and tokenization combinations to determine the best setting. \u2022 Filtering removes the following elements from tweets: URLs, tokens starting with the \"@\" symbol (Twitter username), and tokens starting with the \"#\" symbol (hashtags). \u2022 Tokenization: Text in all training data has been split into spaces and punctuation."}, {"heading": "3.2 Language Model Training", "text": "Once we had the corpora ready, we used the KenLMToolkit to train the N-gram language models on each corpus. We trained with bigrams and trigrams on the tweet and message data. Our language models contained unknown words and were created with or without sentence or tweet boundaries. 2http: / / www.statmt.org / wmt11 / featured-translationtask.html"}, {"heading": "3.3 Tweet Scoring", "text": "After training the N-gram language models, the next step was scoring. For each hashtag file to be evaluated, each tweet in the hashtag file was assigned the logarithm of probability based on the trained language model. The greater the probability, the more likely it was that the tweet corresponded to the language model. Table 1 shows an example of two rated tweets from the hashtag file Bad Job In 5 Words.tsv based on the Tweet-Data-Trigram language model. Note that KenLM indicates the log of the probability of the N-gram instead of the actual probabilities, so that the value closer to 0 (-19) has the higher probability and is associated with the tweet rated as funnier."}, {"heading": "3.4 Tweet Prediction", "text": "The system sorts all tweets for each hashtag and arranges them based on their log probability value, with the funniest tweet listed first. If the values for the log probability are based on the tweet language model, they are sorted in ascending order, because the log probability value closest to 0 indicates the tweet that is most similar to the (funniest) tweet model. However, if the log probability values are based on the message data, they are sorted in descending order, as the highest value has the least probability associated with it and is therefore the most similar to the (funniest) message model. In Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each tweet pair. If the first tweet was funnier than the second, the system returns the tweet IDs for the pair followed by a \"1\" tweet from the second tweet file, if the second one is followed by a funnier IDs."}, {"heading": "4 Experiments and Results", "text": "In this section, we present the results from our development phase (Table 2), the evaluation phase (Table 3) and two results after the evaluation (Table 3). Since we implemented both Bigram and Trigam language models during the development phase, but only results from trigram language models of the task were presented, we evaluated Bigram language models during the post-evaluation phase. Note that the accuracy and distance measurements in Table 2 and Table 3 are defined by task organizers (Potash et al., 2017).Table 2 shows results from the development phase. These results show that the best setting for the tweet data is to keep the # and @, ignore sentence limits, consider upper and lower case and ignore tokenization. While using these settings, the Trigram language model in Subtask B (.887) performed better and the Bigram language model in Subtask A better than the results in Subtask 48 (we rated 5.48)."}, {"heading": "5 Discussion and Future Work", "text": "We relied on Bigram and Trigram language models because tweets are short and concise and often consist of only a few words.Our system's performance was not consistent when comparing development with assessment outcomes. During development, language models trained on the Tweet data performed better, but during evaluation and post-evaluation, language models trained on the message data were significantly more effective. We also observed that Bigram language models performed slightly better than trigram models on the assessment data, suggesting that we should also consider the use of Unigram and sign language models in the future. These results suggest that there are only slight differences between Bigram and Trigram models, and that the type and amount of corpora used to train the models really determine the re-evaluation. The task description paper (Potash et al, 2017) reported the difference of system results for each hash tag, and we were more surprised to see the subhash tag on our Breakout 5 file."}], "references": [{"title": "You had me at hello: How phrasing affects memorability", "author": ["Cristian Danescu-Niculescu-Mizil", "Justin Cheng", "Jon Kleinberg", "Lillian Lee."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers", "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? 2012", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2012}, {"title": "A synopsis of linguistic theory 19301955", "author": ["J. Firth."], "venue": "F. Palmer, editor, Selected Papers of J. R. Firth, Longman.", "citeRegEx": "Firth.,? 1968", "shortCiteRegEx": "Firth.", "year": 1968}, {"title": "Scalable modified Kneser-Ney languagemodel estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria,", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Computational Analysis of Present-day American English", "author": ["Henry Kucera", "W. Nelson Francis."], "venue": "Brown University Press, Providence, RI, USA.", "citeRegEx": "Kucera and Francis.,? 1967", "shortCiteRegEx": "Kucera and Francis.", "year": 1967}, {"title": "An example of statistical investigation of the text Eugene Onegin concerning the connection of samples in chains", "author": ["A.A. Markov."], "venue": "Science in Context 19(4):591\u2013600.", "citeRegEx": "Markov.,? 2006", "shortCiteRegEx": "Markov.", "year": 2006}, {"title": "Learning to laugh (automatically): Computational models for humor recognition", "author": ["Rada Mihalcea", "Carlo Strapparava."], "venue": "Computational Intelligence 22(2):126\u2013142.", "citeRegEx": "Mihalcea and Strapparava.,? 2006", "shortCiteRegEx": "Mihalcea and Strapparava.", "year": 2006}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Automatic disambiguation of English puns", "author": ["Tristan Miller", "Iryna Gurevych."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "Miller and Gurevych.,? 2015", "shortCiteRegEx": "Miller and Gurevych.", "year": 2015}, {"title": "A computational approach to the automation of creative naming", "author": ["G\u00f6zde \u00d6zbal", "Carlo Strapparava."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Com-", "citeRegEx": "\u00d6zbal and Strapparava.,? 2012", "shortCiteRegEx": "\u00d6zbal and Strapparava.", "year": 2012}, {"title": "SemEval-2017 Task 6: #HashtagWars: learning a sense of humor", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Vancouver, BC.", "citeRegEx": "Potash et al\\.,? 2017", "shortCiteRegEx": "Potash et al\\.", "year": 2017}, {"title": "Inside jokes: Identifying humorous cartoon", "author": ["Dafna Shahaf", "Eric Horvitz", "Robert Mankoff"], "venue": null, "citeRegEx": "Shahaf et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahaf et al\\.", "year": 2015}, {"title": "Getting serious about the development of computational humor", "author": ["Oliviero Stock", "Carlo Strapparava."], "venue": "Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence. Acapulco, pages 59\u201364.", "citeRegEx": "Stock and Strapparava.,? 2003", "shortCiteRegEx": "Stock and Strapparava.", "year": 2003}, {"title": "Recognizing humor on Twitter", "author": ["Renxian Zhang", "Naishi Liu."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and KnowledgeManagement. ACM, New York, NY, USA, CIKM \u201914, pages 889\u2013898.", "citeRegEx": "Zhang and Liu.,? 2014", "shortCiteRegEx": "Zhang and Liu.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": ", (Stock and Strapparava, 2003), (\u00d6zbal and Strapparava, 2012)).", "startOffset": 2, "endOffset": 31}, {"referenceID": 8, "context": ", (Stock and Strapparava, 2003), (\u00d6zbal and Strapparava, 2012)).", "startOffset": 33, "endOffset": 62}, {"referenceID": 5, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.", "startOffset": 2, "endOffset": 34}, {"referenceID": 12, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 10, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)).", "startOffset": 59, "endOffset": 80}, {"referenceID": 7, "context": ", 2015), (Miller and Gurevych, 2015)).", "startOffset": 9, "endOffset": 36}, {"referenceID": 9, "context": "SemEval-2017 Task 6 (Potash et al., 2017) also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps (Firth, 1968).", "startOffset": 236, "endOffset": 249}, {"referenceID": 4, "context": "The assumption that the probability of a word depends only on a small number of previous words is called a Markov assumption (Markov, 2006).", "startOffset": 125, "endOffset": 139}, {"referenceID": 0, "context": "In a study on how phrasing affects memorability, (Danescu-Niculescu-Mizil et al., 2012) take a language model approach to measure the distinctiveness of memorable movie quotes.", "startOffset": 49, "endOffset": 87}, {"referenceID": 3, "context": "by evaluating a quote with respect to a \u201ccommon language\u201d model built from the newswire sections of the Brown corpus (Kucera and Francis, 1967).", "startOffset": 117, "endOffset": 143}, {"referenceID": 2, "context": "We use KenLM (Heafield et al., 2013) as our language modeling tool.", "startOffset": 13, "endOffset": 36}, {"referenceID": 9, "context": "Note that the accuracy and distance measurements listed in Table 2 and Table 3 are defined by the task organizers (Potash et al., 2017).", "startOffset": 114, "endOffset": 135}, {"referenceID": 9, "context": "The task description paper (Potash et al., 2017) reported system by system results for each hashtag.", "startOffset": 27, "endOffset": 48}, {"referenceID": 6, "context": "While our language models performed well, there is some evidence that neural network models can outperform standard back-off N-gram models (Mikolov et al., 2011).", "startOffset": 139, "endOffset": 161}], "year": 2017, "abstractText": "This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.", "creator": "LaTeX with hyperref package"}}}