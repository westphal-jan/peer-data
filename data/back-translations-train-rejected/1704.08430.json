{"id": "1704.08430", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "A GRU-Gated Attention Model for Neural Machine Translation", "abstract": "Neural machine translation (NMT) heavily relies on an attention network to produce a context vector for each target word prediction. In practice, we find that context vectors for different target words are quite similar to one another and therefore are insufficient in discriminatively predicting target words. The reason for this might be that context vectors produced by the vanilla attention network are just a weighted sum of source representations that are invariant to decoder states. In this paper, we propose a novel GRU-gated attention model (GAtt) for NMT which enhances the degree of discrimination of context vectors by enabling source representations to be sensitive to the partial translation generated by the decoder. GAtt uses a gated recurrent unit (GRU) to combine two types of information: treating a source annotation vector originally produced by the bidirectional encoder as the history state while the corresponding previous decoder state as the input to the GRU. The GRU-combined information forms a new source annotation vector. In this way, we can obtain translation-sensitive source representations which are then feed into the attention network to generate discriminative context vectors. We further propose a variant that regards a source annotation vector as the current input while the previous decoder state as the history. Experiments on NIST Chinese-English translation tasks show that both GAtt-based models achieve significant improvements over the vanilla attentionbased NMT. Further analyses on attention weights and context vectors demonstrate the effectiveness of GAtt in improving the discrimination power of representations and handling the challenging issue of over-translation.", "histories": [["v1", "Thu, 27 Apr 2017 04:25:41 GMT  (374kb,D)", "http://arxiv.org/abs/1704.08430v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su"], "accepted": false, "id": "1704.08430"}, "pdf": {"name": "1704.08430.pdf", "metadata": {"source": "CRF", "title": "A GRU-Gated Attention Model for Neural Machine Translation", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "jssu@xmu.edu.cn", "dyxiong@suda.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to survive themselves are able to survive themselves, \"he said in an interview with the German Press Agency.\" I don't think we will be able to save the world, \"he said.\" But I think we will be able to save the world. \"He added,\" I don't think the world is in order. \""}, {"heading": "2 Related Work", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "3 Background", "text": "Unlike traditional SMT, NMT directly maps a source sentence x = {x1,.., xn} to its target translation y = {y1,.., ym} using an encoder frame. The encoder reads the source sentence x \u2212 and encodes the representation of each word hi by summarizing the information of neighboring words. As the blue color in Figure 1 shows, this is achieved by a bidirectional RNN, especially the bidirectional GRU model. The encoder is a conditional language model that generates the target sentence word by word using the following conditional probability (see the yellow lines in Figure 1 (a)): p (yj, y < j) = softmax (g, sj \u2212 1, cj)."}, {"heading": "4 GRU-Gated Attention for NMT", "text": "s look again at the generation of cj in Eq. (4) Because different target terms may be aligned to different source words, the attention weights of the source words are different in different decoding steps. (4) However, no matter how the attention weights of the source words vary, the source representations H remain the same, i.e. they are decodable, and this inventory would vector the discriminatory power of the generated context. Accordingly, we try to break this inventory down by redefining the source representations before they enter the vanilla attention network at each decoding step. To achieve this, we suggest GRU-gated attention (GAtt), which can be summarized similarly to vanilla attention in the following form: cj = GAtt (H, sj \u2212 1) (5) The difference between the color of the GAtt (1) and the color of the Gray underlines (b)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "Our training data consists of 1.25M pairs of sentences with 27.9M Chinese words and 34.5m English words, respectively. We chose the NIST 2005 data set as a development set to perform the model selection, and the NIST 2002, 2003, 2004, 2006 and 2008 data set as a test set. There are 878, 919, 1788, 1082 and 1664 sentences in the NIST 2002, 2003, 2004, 2005, 2006, 2008 data sets. We evaluated translation quality using the casein-sensitive metric BLEU-4 [Papineni et al., 2002] 2 and TER metric [Snover et al., 2006] 3. We performed paired bootstrap samples [Koehn, 2004] to verify statistical significance using the script in Moses4."}, {"heading": "5.2 Baselines", "text": "We compared our proposed model with the following two state-of-the-art SMT and NMT systems: \u2022 Moses [Koehn et al., 2007]: an open source state-of-the-art phrase-based SMT system. \u2022 RNNSearch [Bahdanau et al., 2014]: a state-of-the-artattention-based NMT system using the vanilla attention mechanism. We further feed the information of yj \u2212 1 to the attention, and implement the decoder with two GRU layers, following the proposals in dl4mt5.1This data is a combination of LDC2002E18, LDC2003E07, LDC2003E07 to the article of LDC2004T07, LDC2005T08 and LDC2005T06.2https: / / github.com / moses-smt / moses-smt / master / scripts / multi-bleu.perlhttp / we were."}, {"heading": "5.3 Translation Results", "text": "GAtt and GAtt-Inv exceed both Moses and RNNSearch. Specifically GAtt delivers 35.70 BLEU and 56.06 TER points to 6http: / / www.speech.sri.com / projects / srilm / download.htmlaverage, with improvements of 4.59 BLEU and 1.61 TER points to Moses and 1.66 BLEU and 2.12 TER points to RNNSearch; GAtt-Inv averages 35.70 BLEU and 55.99 TER points, with gains of 4.59 BLEU and 1.68 TER points to Moses and 1.66 BLEU and 2.19 TER points to RNNSearch. All improvements are statistically significant. It seems that GAtt-Inv performs very slightly better than GAtt in relation to TER. However, these improvements are neither significant nor consistent. In other words, GAtt is as efficient as GAtt-Inv. This is GUTs reasonably, as the input between GAtt-Inv and GAU input occurs."}, {"heading": "5.4 Effects of Model Ensemble", "text": "We show the results in Table 2. Unsurprisingly, all ensemble systems achieve significant improvements over the best single system, and the overall ensemble of \"RNNSearch + GAtt + GAtt-Inv\" delivers the best results, averaging 38.64 BLEU and 54.15 TER, demonstrating that these neural models complement each other and are beneficial."}, {"heading": "5.5 Translation Analysis", "text": "In the last few years, it has been shown that the number of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "5.6 Over-Translation Evaluation", "text": "The translation or repeated prediction of the same target words [Tu et al., 2016] is therefore a difficult problem for the NMT. We suspect that part of the reason for the translation problem lies in the small differences in context vectors learned through the vanilla attention mechanism. Since the proposed GAtt can improve the discriminatory power of context vectors, we assume that our model is better able to deal with the translation problem than the vanilla attention network. To prove this hypothesis, we introduce a metric called N-Gram Repetition Rate (N-GRR), which calculates the proportion of repeated n-grams in a sentence: N-GRR = 1CRC \u2211 c = 1R \u0445 r = 1 | N-gramsc (N-Gram Repetition Rate, r) | N-gramsc (10), whereby Ngramsc, r | the total number of n-GAT translations can be compared in the translation."}, {"heading": "6 Conclusion", "text": "In this article, we have introduced a novel GRU-gated attention model (GAtt) for NMT. Instead of using decoding invariant source representations, GAtt produces new source representations that vary according to the partial translation through different decoding steps to improve the discrimination of context vectors for translation, achieved by a gating layer that considers the source representations and the previous decoder state as history and input to a gated recurrent unit. Experiments with Chinese-English translation tasks show the effectiveness of our model. An in-depth analysis also shows that our model is capable of significantly reducing repeated redundant translations (translations). In the future, we would like to apply our model to other sequence learning tasks, as our model can easily be adapted to other sequence-to-sequence tasks (e.g. document summary, neural conversion model, language recognition, etc.). In addition, we will present GAtt as an architectural unit with an exception to the GRU structure in our tree."}], "references": [{"title": "In Proc", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio. Neural machine translation by jointly learning to align", "translate"], "venue": "of ICLR,", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE Transactions on Neural Networks", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult"], "venue": "5:157\u2013166,", "citeRegEx": "Bengio et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al", "2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proc. of EMNLP,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR,", "citeRegEx": "Chung et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "CoRR", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari. Incorporating structural alignment biases into an attentional neural translation model"], "venue": "abs/1601.01085,", "citeRegEx": "Cohn et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput., 9:1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "pages 1\u201310", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio. On using very large target vocabulary for neural machine translation. In Proc. of ACL-IJCNLP"], "venue": "July", "citeRegEx": "Jean et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "NAACL \u201903", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu. Statistical phrase-based translation. In Proc. of NAACL"], "venue": "pages 48\u201354,", "citeRegEx": "Koehn et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Open source toolkit", "author": ["Koehn et al", "2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "In Proc", "author": ["Philipp Koehn. Statistical significance tests for machine translation evaluation"], "venue": "of EMNLP,", "citeRegEx": "Koehn. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "pages 1412\u20131421", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning. Effective approaches to attention-based neural machine translation. In Proc. of EMNLP"], "venue": "September", "citeRegEx": "Luong et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 11\u201319", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba. Addressing the rare word problem in neural machine translation. In Proc. of ACL-IJCNLP"], "venue": "July", "citeRegEx": "Luong et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive attention for neural machine", "author": ["Meng et al", "2016] Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "pages 2283\u20132288", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah. Supervised attentions for neural machine translation. In Proc. of EMNLP"], "venue": "Austin, Texas, November", "citeRegEx": "Mi et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "29(1):19\u201351", "author": ["Franz Josef Och", "Hermann Ney. A systematic comparison of various statistical alignment models. Comput. Linguist."], "venue": "March", "citeRegEx": "Och and Ney. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Proc. of ACL, pages 311\u2013318,", "citeRegEx": "Papineni et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "CoRR", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu. Minimum risk training for neural machine translation"], "venue": "abs/1512.02433,", "citeRegEx": "Shen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In In Proceedings of Association for Machine Translation in the Americas", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul. A study of translation edit rate with targeted human annotation"], "venue": "pages 223\u2013231,", "citeRegEx": "Snover et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "CoRR", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le. Sequence to sequence learning with neural networks"], "venue": "abs/1409.3215,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 1556\u20131566", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proc. of ACL"], "venue": "Beijing, China, July", "citeRegEx": "Tai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li. Coverage-based neural machine translation"], "venue": "abs/1601.04811,", "citeRegEx": "Tu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang. Neural machine translation advised by statistical machine translation"], "venue": "abs/1610.05150,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ArXiv e-prints", "author": ["Z. Yang", "Z. Hu", "Y. Deng", "C. Dyer", "A. Smola. Neural Machine Translation with Recurrent Attention Modeling"], "venue": "July", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 0, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 16, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 6, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 11, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 21, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 0, "context": "What makes NMT outperform conventional statistical machine translation (SMT) is the attention mechanism [Bahdanau et al., 2014], an information bridge between the encoder and the decoder that produces context vectors by dynamically detecting relevant source words for predicting the next target word.", "startOffset": 104, "endOffset": 127}, {"referenceID": 3, "context": "Specifically, we model this gating layer with a GRU unit [Chung et al., 2014], which takes the original source representations as its history and the corresponding previous decoder state as its current input.", "startOffset": 57, "endOffset": 77}, {"referenceID": 18, "context": "Originally, NMT does not have the attention mechanism and mainly relies on the encoder to summarize all source-side semantic details into a fixed-length vector [Sutskever et al., 2014; Cho et al., 2014].", "startOffset": 160, "endOffset": 202}, {"referenceID": 3, "context": "GRU usually acts as a recurrent unit that leverages a reset gate and an update gate to control how much information flow from the history state and the current input respectively [Chung et al., 2014].", "startOffset": 179, "endOffset": 199}, {"referenceID": 1, "context": "It is an extension of the vanilla recurrent neural network (RNN) unit with the advantage of alleviating the vanishing and exploding gradient problems during training [Bengio et al., 1994], and also a simplification of the LSTM model [Hochreiter and Schmidhuber, 1997] with the advantage of efficient computation.", "startOffset": 166, "endOffset": 187}, {"referenceID": 5, "context": ", 1994], and also a simplification of the LSTM model [Hochreiter and Schmidhuber, 1997] with the advantage of efficient computation.", "startOffset": 53, "endOffset": 87}, {"referenceID": 19, "context": "Additionally, our model is also related with the treestructured LSTM [Tai et al., 2015], where LSTM is adapted to compose vary-sized children nodes and current input node in a dependency tree into the current hidden state.", "startOffset": 69, "endOffset": 87}, {"referenceID": 0, "context": "In this section, we briefly review the vanilla attention-based NMT [Bahdanau et al., 2014].", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "Please refer to [Bahdanau et al., 2014] for more details.", "startOffset": 16, "endOffset": 39}, {"referenceID": 0, "context": "The relevance score eji is estimated via an alignment model as in [Bahdanau et al., 2014]: eji = v a tanh(Wasj\u22121 + Uahi).", "startOffset": 66, "endOffset": 89}, {"referenceID": 3, "context": "Instead of using conventional gating mechanism [Chung et al., 2014], we directly choose the whole GRU unit to perform this task.", "startOffset": 47, "endOffset": 67}, {"referenceID": 15, "context": "We evaluated the translation quality using the caseinsensitive BLEU-4 metric [Papineni et al., 2002]2 and TER metric [Snover et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 17, "context": ", 2002]2 and TER metric [Snover et al., 2006]3.", "startOffset": 24, "endOffset": 45}, {"referenceID": 9, "context": "We performed paired bootstrap sampling [Koehn, 2004] for statistical significance test using the script in Moses4.", "startOffset": 39, "endOffset": 52}, {"referenceID": 0, "context": "\u2022 RNNSearch [Bahdanau et al., 2014]: a state-of-the-art attention-based NMT system using the vanilla attention mechanism.", "startOffset": 12, "endOffset": 35}, {"referenceID": 14, "context": "The word alignments were obtained with GIZA++ [Och and Ney, 2003] on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d strategy [Koehn et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 7, "context": "The word alignments were obtained with GIZA++ [Och and Ney, 2003] on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d strategy [Koehn et al., 2003].", "startOffset": 151, "endOffset": 171}, {"referenceID": 0, "context": "Following the settings in [Bahdanau et al., 2014], we set dw = 620, dh = 1000.", "startOffset": 26, "endOffset": 49}, {"referenceID": 23, "context": "We used the Adadelta algorithm [Zeiler, 2012] for optimization, with a batch size of 80 and gradient norm as 5.", "startOffset": 31, "endOffset": 45}, {"referenceID": 11, "context": "We ensemble different systems by simply averaging their predicted target word probabilities at each decoding step, as suggested in [Luong et al., 2015b].", "startOffset": 131, "endOffset": 152}, {"referenceID": 14, "context": "To verify this point, we evaluated the quality of word alignments induced from different neural systems in terms of alignment error rate (AER) [Och and Ney, 2003] and the soft version (SAER) of AER, following Tu et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 20, "context": "[Tu et al., 2016].", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "[Tu et al., 2016].", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "We refer the readers to [Tu et al., 2016] for more details.", "startOffset": 24, "endOffset": 41}, {"referenceID": 20, "context": "Over-translation or repeatedly predicting the same target words [Tu et al., 2016] is a challenging problem for NMT.", "startOffset": 64, "endOffset": 81}], "year": 2017, "abstractText": "Neural machine translation (NMT) heavily relies on an attention network to produce a context vector for each target word prediction. In practice, we find that context vectors for different target words are quite similar to one another and therefore are insufficient in discriminatively predicting target words. The reason for this might be that context vectors produced by the vanilla attention network are just a weighted sum of source representations that are invariant to decoder states. In this paper, we propose a novel GRU-gated attention model (GAtt) for NMT which enhances the degree of discrimination of context vectors by enabling source representations to be sensitive to the partial translation generated by the decoder. GAtt uses a gated recurrent unit (GRU) to combine two types of information: treating a source annotation vector originally produced by the bidirectional encoder as the history state while the corresponding previous decoder state as the input to the GRU. The GRU-combined information forms a new source annotation vector. In this way, we can obtain translation-sensitive source representations which are then feed into the attention network to generate discriminative context vectors. We further propose a variant that regards a source annotation vector as the current input while the previous decoder state as the history. Experiments on NIST Chinese-English translation tasks show that both GAtt-based models achieve significant improvements over the vanilla attentionbased NMT. Further analyses on attention weights and context vectors demonstrate the effectiveness of GAtt in improving the discrimination power of representations and handling the challenging issue of over-translation.", "creator": "LaTeX with hyperref package"}}}