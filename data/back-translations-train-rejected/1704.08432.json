{"id": "1704.08432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "DeepCCI: End-to-end Deep Learning for Chemical-Chemical Interaction Prediction", "abstract": "Chemical-chemical interaction (CCI) plays a key role in predicting candidate drugs, toxicity, therapeutic effects, and biological functions. CCI was created from text mining, experiments, similarities, and databases; to date, no learning-based CCI prediction method exist. In chemical analyses, computational approaches are required. The recent remarkable growth and outstanding performance of deep learning have attracted considerable esearch attention. However,even in state-of-the-art drug analyses, deep learning continues to be used only as a classifier. Nevertheless, its purpose includes not only simple classification, but also automated feature extraction. In this paper, we propose the first end-to-end learning method for CCI, named DeepCCI. Hidden features are derived from a simplified molecular input line entry system (SMILES), which is a string notation representing the chemical structure, instead of learning from crafted features. To discover hidden representations for the SMILES strings, we use convolutional neural networks (CNNs). To guarantee the commutative property for homogeneous interaction, we apply model sharing and hidden representation merging techniques. The performance of DeepCCI was compared with a plain deep classifier and conventional machine learning methods. The proposed DeepCCI showed the best performance in all seven evaluation metrics used. In addition, the commutative property was experimentally validated. The automatically extracted features through end-to-end SMILES learning alleviates the significant efforts required for manual feature engineering. It is expected to improve prediction performance, in drug analyses.", "histories": [["v1", "Thu, 27 Apr 2017 05:03:08 GMT  (1106kb,D)", "http://arxiv.org/abs/1704.08432v1", null], ["v2", "Tue, 27 Jun 2017 04:28:17 GMT  (1275kb,D)", "http://arxiv.org/abs/1704.08432v2", "ACM-BCB 2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sunyoung kwon", "sungroh yoon"], "accepted": false, "id": "1704.08432"}, "pdf": {"name": "1704.08432.pdf", "metadata": {"source": "CRF", "title": "DeepCCI: End-to-end Deep Learning for Chemical-Chemical Interaction Prediction", "authors": ["Sunyoung Kwon", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr"], "sections": [{"heading": null, "text": "Key concepts chemical-chemical interaction, deep learning, neural networks, Convolutionary neural networks, commutative property"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to flourish."}, {"heading": "2 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Representation of chemical compound", "text": "For end-to-end learning, it is important that the input contains all the latent characteristic information of the chemical structure. There is a considerable amount of chemical information such as weight, molecular formula, rings, atoms, SMILES [66] and InChI [19]. Among the much information, SMILES represents the chemical structure as a line of ASCII characters. As shown in Figure 2, cyclohexane and acetaminophene are expressed as C1CCCC1 and CC (= O) NC1 = CC = C (O) C = C1 and (C for carbon, O for oxygen, = for double bonds and 1 for the first ring) respectively. Atoms (e.g. carbon, nitrogen and oxygen), bonds (e.g. single, double and triple bonds), rings (e.g. open ring, tight ring and ring number), aromaticity as a metastature. As a one-dimensional representation of the chemical structure MILES."}, {"heading": "2.2 Convolutional neural networks", "text": "The Convolutionary Neural Network (CNN) [31], one of the most widely used deep learning architectures, has excelled in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27] as well as in two-dimensional image processing [31, 33, 44]. Designed to analyze shi-invariant spatial information, CNNs consist of folding layers and pooling layers. On each folding layer, feature maps called Lters scan the sub-regions throughout the sequence. Filters enable CNNs to detect locally correlated motifs regardless of location through local connectivity and parameter sharing. Each pooling layer brings together non-overlapping sub-regions and aggregates local features into more global features."}, {"heading": "2.3 Commutative property", "text": "In mathematics, a commutative property means that a change in order does not affect the result (e.g. 2 + 5 = 5 + 2). As with indirect symmetric problems, such as distance or similarity between objects, the interaction between A and B should be the same regardless of the order I (A, B) = I (B, A). However, if the values xA and xB are simply linked for objects A and B and then used for learning, the result can be according to the input orderI (xA, xB) = w1xA1 + \u00b7 + wnxA + wn + 1xB1 + \u00b7 + w2nxBn, w1xB1 + \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 + wnxB + wn + 1xA1 + \u00b7 \u00b7 + w2nxAn = I (xB, xA)."}, {"heading": "3 PROPOSED METHODOLOGY", "text": "Figure 3 shows the overview of the proposed DeepCCI. Our method presents end-to-end SMILES learning for the CCI. It takes SMILES strings as inputs xA and xB for objects A and B and then generates an interaction probability y. e structure is divided into three stages. e first stage is for pre-processing SMILES inputs, the second stage is for learning latent hidden representations by 1D CNNs, and the third stage is for interaction learning through merging and fully connected layers. Algorithm 1 shows the entire process of the DeepCCI."}, {"heading": "3.1 Notations", "text": "\u2022 I (A, B): Interaction between objects A and B \u2022 X: SMILES character set, X = {C, =, (,), O, F, 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 x: SMILES string, xA and xB for objects A and B \u2022 xi: i-th character of x, \u00b7 \u00b7 \u00b7 \u00b7 n), xi \u00b2 X \u2022 h: hidden representations, hA and hB for objects A and B \u2022 i-th element of h = (h1, h2, \u00b7, hn) \u2022 \u03bb: maximum length of SMILES \u2022 F: number of Lters \u2022 k: length of Lters (core size) \u2022 W: learning weightC (= O) N C 1 = C Cconvolutionmax-pooling | F: number of filters."}, {"heading": "3.2 Input Preprocessing", "text": "We used the input x = (x1, x2, \u00b7 \u00b7, xn), which is represented by \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 characters xi-X, where X = {C, =, (,), O, F, 1, 2, \u00b7}, | X | = 65 in SMILES format. The input x should be converted into a numerical expression for learning. For the numerical expression, we use a commonly used one-dimensional encoding scheme that sets the corresponding single character to \"1\" and all others to \"0,\" whereby each character xi is converted to a | X | -dimensional binary vector. An example of the cyclop length (C1CCC1), which encodes x, is as follows: {C, =,), (O, N, 1, 2 \u00b7 \u00b7 \u00b7} x, x = x1, x2 x4 x5 x7 x8 = C C C C C C C C C C C C C C 1 C C C = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,,, 0, 0."}, {"heading": "3.3 Modeling of Hidden Representations", "text": "To learn the hidden representations h from the pre-processed input x of SMILES strings, we used CNNs. e CNN architecture used in our method consists of a folding layer and a pooling layer. h = Pool (\u03c3r (Conv (x))))) (2) e folding layer plays a \"motif detector role\" within the sequence. e pre-processed input layer x is interwoven with a series of learnable feature cards called lters (or cores). e exhibitors can detect motifs of positive chemical properties in the SMILES string. Scanning the local region via the input line with a parameter-divided lter can harness the recognition of the shi -invariant local pa. The output form of discarded input becomes an F \u00d7 (\u03bb \u2212 k + 1) dimensional matrix, where F represents the number of lters and the length is greater."}, {"heading": "3.4 Modeling of Interaction Between Chemicals", "text": "In contrast to heterogeneous interactions, such as protein chemistry and miRNA-mRNA interaction, CCI is a homogeneous interaction between two chemicals. Homogeneous interaction between objects A and B should be the same regardless of their input order. (I (A, B) = I (B, A), which guarantees the commutative property described in Section 2.3. (However, the simple concatenation of the values of objects A and B is no guarantee of the property according to Equation 1. To recognize I (A, B) and I (B, A) equally, we strive to give the same weights W to both hidden representations hA and hB for objects A and BI (A, B). (WhA + WhB = w1h A + \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 rophAn + w1hB1 + \u00b7 \u00b7 mini-hBn = w1h B 1 + \u00b7 \u00b7 \u00b7 \u00b7 hBwnn)."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setup", "text": "In our experiment, we used the CCI data (version 5.0) downloaded from the STITCH database. Each interaction pair had a con score from 0 to 999, derived from text mining, and more than 900, 800 and 700 interaction probabilities were generated from the samples."}, {"heading": "91% of SMILES strings is less than 100 characters in length.", "text": "We retrieved the SMILES string and the PubChem ngerprint using PubChemPy [57]. A representative example of Cyclohexane is as follows: \u2022 PubChem chemical ID: 8078 \u2022 SMILES string: C1CCCCC1 \u2022 Fingerprint: 110000000000000000000000000000000000 \u00b7 \u00b7 \u00b7 (the 9th bit indicates \u2265 C and the 10th bit indicates \u2265 C) 4.1.3 Experimental configuration. Our data set consisted of training, validation and test samples, as in the general learning experiments [2]. Figure 4 describes the data sets. The complete data set was split into two parts, one for model selection and the other for independent testing. The aim of the model selection was to nd find the optimal hyperparameters. Models with different sets of hyperparameters were trained and subsequently iteratively validated."}, {"heading": "4.2 E ects of Hyperparameter Variation", "text": "As shown in Figure 5, the lengths of the SMILES strings vary from 1 to over 200. The mean length is 45 and the average length is 53.5. e 84%, 91%, and 94% of the SMILES strings are less than 80, 100, and 120 characters in length. e proposed method DeepCCI limits the SMILES strings to a certain maximum length. e Loss of information and dummy zeros after the maximum length could be an effect on overall performance. Figure 6A shows the performance of the DeepCCI by varying the maximum SMILES strings to 50, 80, and 120. Among them, a maximum length of 100 shows the best performance in terms of AUC."}, {"heading": "4.3 Performance Comparison with Other Methods", "text": "We compared our proposed method, DeepCCI, with a feedback neural network (FFNN), which is a simple deep class that is described in Figure 1A, and the conventional machine learning methods of supporting vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive enhancement (AdaBoost) [16]. SVM, RF, and AdaBoost were tested using standard parameters. FFNN, which is a deep neural network of three fully connected layers (with 1,024 and 128 hidden units) was tested using ReLU for the hidden activation function and signature for the output activation function."}, {"heading": "5 DISCUSSION", "text": "In this paper, we have proposed the first end-to-end deep learning method for CCI. It showed the best performance compared to simple deep classics and conventional machine learning methods. Deep learning has shown remarkable successes in chemical analyses and has produced considerable research work. Despite the success and interest, the protein traits designed by domain experts are still used, even in state-of-the-art deep learning approaches. The goal of deep learning is not only classisi - cation or regression from the kra ed features, but the automatic extraction of significant traits from the original input and their analysis. For this reason, we have endeavoured to perform end-to-deep learning based on SMILES. Our approach showed that better results are obtained than the use of the cra ed property PubChem ngerprint. There are a variety of well-tested characteristics from the original input and its analysis."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partially funded by the National Research Foundation of Korea (NRF), which is funded by the Korean Government (Ministry of Science, ICT and Future Planning) [No. 2012M3A9D1054622 and No. 2014M3C9A3063541], partially funded by the Korea Health Technology R & D Project through the Korea Health Industry Development Institute (KHIDI), financed by the Ministry of Health and Welfare [HI14C3405030014] and partially funded by the Brain Korea 21 Plus Project in 2017."}], "references": [{"title": "Predicting the sequence speci\u0080cities of DNA-and RNA-binding proteins by deep learning", "author": ["Babak Alipanahi", "Andrew Delong", "Ma\u008ahew T Weirauch", "Brendan J Frey"], "venue": "Nature biotechnology 33,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Deep learning for computational biology", "author": ["Christof Angermueller", "Tanel P\u00e4rnamaa", "Leopold Parts", "Oliver Stegle"], "venue": "Molecular systems biology 12,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Molecular function prediction using neighborhood features", "author": ["Petko Bogdanov", "Ambuj K Singh"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB) 7,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Predicting drugs side e\u0082ects based on chemicalchemical interactions and protein-chemical interactions", "author": ["Lei Chen", "Tao Huang", "Jian Zhang", "Ming-Yue Zheng", "Kai-Yan Feng", "Yu-Dong Cai", "Kuo-Chen Chou"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Predicting chemical toxicity e\u0082ects based on chemical-chemical interactions", "author": ["Lei Chen", "Jing Lu", "Jian Zhang", "Kai-Rui Feng", "Ming-Yue Zheng", "Yu-Dong Cai"], "venue": "PLoS One 8,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "\u008ce use of chemical-chemical interaction and chemical structure to identify new candidate chemicals related to lung cancer", "author": ["Lei Chen", "Jing Yang", "Mingyue Zheng", "Xiangyin Kong", "Tao Huang", "Yu-Dong Cai"], "venue": "PloS one 10,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Keras: Deep Learning library for \u008ceano and TensorFlow. h\u008aps://github.com/fchollet/keras", "author": ["Fran\u00e7ois Chollet"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Use of support vector machine in pa\u008aern classi\u0080cation: Application to QSAR studies", "author": ["Ryszard Czermi\u0144ski", "Abdelaziz Yasri", "David Hartsough"], "venue": "Molecular  Informatics 20,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Multi-task neural networks for QSAR predictions", "author": ["George E Dahl", "Navdeep Jaitly", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1406.1231", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Deep dynamic models for learning hidden representations of speech features", "author": ["Li Deng", "Roberto Togneri"], "venue": "In Speech and Audio Processing for Coding, Enhancement and Recognition", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Nonlinear SVM approaches to QSPR/QSAR studies and drug design", "author": ["Jean-Pierre Doucet", "Florent Barbault", "Hairong Xia", "Annick Panaye", "Botao Fan"], "venue": "Current Computer-Aided Drug Design 3,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "DNdisorder: predicting protein disorder using boosting and deep networks", "author": ["Jesse Eickholt", "Jianlin Cheng"], "venue": "BMC bioinformatics 14,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In European conference on computational learning theory", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Automatic chemical design using a data-driven continuous representation of molecules", "author": ["Rafael G\u00f3mez-Bombarelli", "David Duvenaud", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Jorge Aguilera-Iparraguirre", "Timothy D Hirzel", "Ryan P Adams", "Al\u00e1n Aspuru- Guzik"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geo\u0082rey Hinton"], "venue": "(icassp), 2013 ieee international conference on", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "InChI-the worldwide chemical structure identi\u0080er standard", "author": ["Stephen Heller", "Alan McNaught", "Stephen Stein", "Dmitrii Tchekhovskoi", "Igor Pletnev"], "venue": "Journal of cheminformatics", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Predicting biological functions of compounds based on chemical-chemical interactions", "author": ["Le-Le Hu", "Chen Chen", "Tao Huang", "Yu-Dong Cai", "Kuo-Chen Chou"], "venue": "PLoS One 6,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shi\u0089", "author": ["Sergey Io\u0082e", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Lethality and centrality in protein networks", "author": ["Hawoong Jeong", "Sean P Mason", "A-L Barab\u00e1si", "Zoltan N Oltvai"], "venue": "Nature 411,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenste\u008ae", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "A robust peak detection method for RNA structure inference by high-throughput contact mapping", "author": ["Jinkyu Kim", "Seunghak Yu", "Byonghyo Shim", "Hanjoo Kim", "Hyeyoung Min", "Eui- Young Chung", "Rhiju Das", "Sungroh Yoon"], "venue": "Bioinformatics 25,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Convolutional neural networks for sentence classi\u0080cation", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Structure-based strategies for drug design and discovery", "author": ["T Kindt", "S Morse", "E Gotschlich", "K Lyons"], "venue": "Nature", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1991}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Imagenet classi\u0080cation with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geo\u0082rey E Hinton"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "STITCH: interaction networks of chemicals and proteins", "author": ["Michael Kuhn", "Christian von Mering", "Monica Campillos", "Lars Juhl Jensen", "Peer Bork"], "venue": "Nucleic acids research 36,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["Steve Lawrence", "C Lee Giles", "Ah Chung Tsoi", "Andrew D Back"], "venue": "IEEE transactions on neural networks", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "deepTarget: end-to-end learning framework for microRNA target prediction using deep recurrent neural networks", "author": ["Byunghan Lee", "Junghwan Baek", "Seunghyun Park", "Sungroh Yoon"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "DNA-level splice junction prediction using deep recurrent neural networks", "author": ["Byunghan Lee", "Taehoon Lee", "Byunggook Na", "Sungroh Yoon"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "FingerNet: Deep learning-based robust \u0080nger joint detection from radiographs", "author": ["Sungmin Lee", "Minsuk Choi", "Hyun-soo Choi", "Moon Seok Park", "Sungroh Yoon"], "venue": "In Biomedical Circuits and Systems Conference (BioCAS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions", "author": ["Taehoon Lee", "Sungroh Yoon"], "venue": "In ICML", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Deep learning of the tissue-regulated splicing code", "author": ["Michael KK Leung", "Hui Yuan Xiong", "Leo J Lee", "Brendan J Frey"], "venue": "Bioinformatics 30,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Zachary C Lipton", "John Berkowitz", "Charles Elkan"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules", "author": ["Alessandro Lusci", "Gianluca Pollastri", "Pierre Baldi"], "venue": "Journal of chemical information and modeling 53,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Deep learning in bioinformatics", "author": ["Seonwoo Min", "Byunghan Lee", "Sungroh Yoon"], "venue": "Brie\u0080ngs in Bioinformatics (2016),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Recti\u0080ed linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geo\u0082rey E Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Prediction of protein functions based on function\u2013function correlation relations", "author": ["Ka-Lok Ng", "Jin-Shuei Ciou", "Chien-Hung Huang"], "venue": "Computers in Biology and Medicine 40,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Maxime Oquab", "Leon Bo\u008aou", "Ivan Laptev", "Josef Sivic"], "venue": "In Proceedings of the IEEE conference on computer vision and pa\u0088ern recognition", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A comparative study of SMILES-based compound similarity functions for drug-target interaction prediction", "author": ["Hakime \u00d6zt\u00fcrk", "Elif Ozkirimli", "Arzucan \u00d6zg\u00fcr"], "venue": "BMC bioinformatics 17,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "deep- MiRGene: deep neural network based precursor microRNA prediction", "author": ["Seunghyun Park", "Seonwoo Min", "Hyunsoo Choi", "Sungroh Yoon"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Massively multitask networks for drug discovery", "author": ["Bharath Ramsundar", "Steven Kearnes", "Patrick Riley", "Dale Webster", "David Konerding", "Vijay Pande"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "I-TASSER: a uni\u0080ed platform for automated protein structure and function prediction", "author": ["Ambrish Roy", "Alper Kucukural", "Yang Zhang"], "venue": "Nature protocols 5,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Comparing chemical \u0080ngerprints for ecotoxicology", "author": ["Leander Schietgat", "Bertrand Cuissart", "Alban Lepailleur", "Kurt De Grave", "Bruno Cr\u00e9milleux", "Ronan Bureau", "Jan Ramon"], "venue": "In 6e\u0300mes journe\u0301es de la Socie\u0301te\u0301 Franc\u0327aise de Che\u0301moinformatique", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks", "author": ["Marwin HS Segler", "\u008cierry Kogej", "Christian Tyrchan", "Mark P Waller"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2017}, {"title": "Network-based prediction of protein function", "author": ["Roded Sharan", "Igor Ulitsky", "Ron Shamir"], "venue": "Molecular systems biology 3,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Dropout: a simple way to prevent neural networks from  over\u0080\u008aing", "author": ["Nitish Srivastava", "Geo\u0082rey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Computer assisted studies of chemical structure and biological function", "author": ["Andrew J Stuper", "William E Br\u00fcgger", "Peter C Jurs"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1979}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "\u008boc V Le"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "Random forest: a classi\u0080cation and regression tool for compound classi\u0080cation and QSAR modeling", "author": ["Vladimir Svetnik", "Andy Liaw", "Christopher Tong", "J Christopher Culberson", "Robert P Sheridan", "Bradley P Feuston"], "venue": "Journal of chemical information and computer sciences 43,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "PubChemPy: a way to interact with PubChem in Python. h\u008ap://pubchempy.readthedocs.io", "author": ["Ma\u008a Swain"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Predictions of the ADMET properties of candidate drug molecules utilizing di\u0082erent QSAR/QSPR modelling approaches", "author": ["Mahmud Tareq Hassan Khan"], "venue": "Current Drug Metabolism 11,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2010}, {"title": "Boosting compound-protein interaction prediction by deep learning", "author": ["Kai Tian", "Mingyu Shao", "Yang Wang", "Jihong Guan", "Shuigeng Zhou"], "venue": "Methods", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2016}, {"title": "Molecular descriptors for chemoinformatics, volume 41 (2 volume set)", "author": ["Roberto Todeschini", "Viviana Consonni"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2009}, {"title": "ADMET in silico modelling: towards prediction paradise? Nature reviews Drug discovery", "author": ["Han Van De Waterbeemd", "Eric Gi\u0082ord"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2003}, {"title": "\u008ae nature of statistical learning theory. Springer science & business media", "author": ["Vladimir Vapnik"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "In silico prediction of serious eye irritation or corrosion potential of chemicals", "author": ["Qin Wang", "Xiao Li", "Hongbin Yang", "Yingchun Cai", "Yinyin Wang", "Zhuang Wang", "Weihua Li", "Yun Tang", "Guixia Liu"], "venue": "RSC Advances 7,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2017}, {"title": "PubChem: a public information system for analyzing bioactivities of small molecules", "author": ["Yanli Wang", "Jewen Xiao", "Tugba O Suzek", "Jian Zhang", "Jiyao Wang", "Stephen H Bryant"], "venue": "Nucleic acids research 37,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2009}, {"title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules", "author": ["David Weininger"], "venue": "In Proc. Edinburgh Math. SOC,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 1970}, {"title": "Systematic chemical-genetic and chemical-chemical interaction datasets for prediction of compound synergism", "author": ["Jan Wildenhain", "Michaela Spitzer", "Sonam Dolma", "Nick Jarvik", "Rachel White", "Marcia Roy", "Emma Gri\u0081ths", "David S Bellows", "Gerard D Wright", "Mike Tyers"], "venue": "Scienti\u0080c Data", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2016}, {"title": "Prediction of regulatory modules comprising microRNAs and target genes", "author": ["Sungroh Yoon", "Giovanni De Micheli"], "venue": "Bioinformatics 21,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2005}, {"title": "Ensemble learning can signi\u0080cantly improve human microRNA target prediction", "author": ["Seunghak Yu", "Juho Kim", "Hyeyoung Min", "Sungroh Yoon"], "venue": "Methods 69,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Ma\u008ahew D Zeiler", "Rob Fergus"], "venue": "In European conference on computer vision", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2014}, {"title": "Convolutional neural network architectures for predicting DNA\u2013protein binding", "author": ["Haoyang Zeng", "Ma\u008ahew D Edwards", "Ge Liu", "David K Gi\u0082ord"], "venue": "Bioinformatics 32,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Interaction is an action that occurs between two entities that may share similar functions or metabolic pathways [4, 43, 52].", "startOffset": 113, "endOffset": 124}, {"referenceID": 39, "context": "Interaction is an action that occurs between two entities that may share similar functions or metabolic pathways [4, 43, 52].", "startOffset": 113, "endOffset": 124}, {"referenceID": 47, "context": "Interaction is an action that occurs between two entities that may share similar functions or metabolic pathways [4, 43, 52].", "startOffset": 113, "endOffset": 124}, {"referenceID": 20, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 52, "endOffset": 64}, {"referenceID": 54, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 30, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 62, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 63, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 28, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 176, "endOffset": 180}, {"referenceID": 61, "context": "\u008ce following CCI-related studies have been conducted: a study on compound synergism that predicts the therapeutic e\u0081cacy of molecule combinations, based on interactive chemicals [67]; toxicity related studies", "startOffset": 178, "endOffset": 182}, {"referenceID": 4, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 145, "endOffset": 151}, {"referenceID": 6, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 352, "endOffset": 355}, {"referenceID": 18, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 554, "endOffset": 558}, {"referenceID": 55, "context": ", atom pairs, shapes, and connectivities) [60].", "startOffset": 42, "endOffset": 46}, {"referenceID": 55, "context": "Various works have been proposed to capture the essential properties of chemicals, and more than 5,000 diverse features have been identi\u0080ed [60] (e.", "startOffset": 140, "endOffset": 144}, {"referenceID": 53, "context": "\u008ce predicting activity stage uncovers the relationship between the extracted features and biological activities, such as absorption, distribution, metabolism, excretion, and toxicity (ADMET) [58, 61].", "startOffset": 191, "endOffset": 199}, {"referenceID": 56, "context": "\u008ce predicting activity stage uncovers the relationship between the extracted features and biological activities, such as absorption, distribution, metabolism, excretion, and toxicity (ADMET) [58, 61].", "startOffset": 191, "endOffset": 199}, {"referenceID": 8, "context": "To robustly learn relationships between chemicals and activities, and to handle a large amount of data, machine learning approaches (such as support vector machine (SVM) [10, 13] and random forest (RF) [56]) have been successfully applied.", "startOffset": 170, "endOffset": 178}, {"referenceID": 11, "context": "To robustly learn relationships between chemicals and activities, and to handle a large amount of data, machine learning approaches (such as support vector machine (SVM) [10, 13] and random forest (RF) [56]) have been successfully applied.", "startOffset": 170, "endOffset": 178}, {"referenceID": 51, "context": "To robustly learn relationships between chemicals and activities, and to handle a large amount of data, machine learning approaches (such as support vector machine (SVM) [10, 13] and random forest (RF) [56]) have been successfully applied.", "startOffset": 202, "endOffset": 206}, {"referenceID": 2, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 125, "endOffset": 136}, {"referenceID": 35, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 125, "endOffset": 136}, {"referenceID": 50, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 125, "endOffset": 136}, {"referenceID": 10, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 15, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 27, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 64, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 12, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 31, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 32, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 33, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 34, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 42, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 9, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 36, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 43, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 54, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 9, "context": "\u008ce deep learning based classi\u0080er has a\u008aracted considerable a\u008aention through the Kaggle \u2018Merck Molecular Activity Challenge\u2019 and its subsequent quantitative structure activity relationship (QSAR) predictions [11, 47].", "startOffset": 207, "endOffset": 215}, {"referenceID": 43, "context": "\u008ce deep learning based classi\u0080er has a\u008aracted considerable a\u008aention through the Kaggle \u2018Merck Molecular Activity Challenge\u2019 and its subsequent quantitative structure activity relationship (QSAR) predictions [11, 47].", "startOffset": 207, "endOffset": 215}, {"referenceID": 54, "context": "In addition, prediction of chemical-protein interaction [59] (for drug discovery, network pharmacology, and drug/protein target identi\u0080cation) showed boosted results by using deep learning from manually cra\u0089ed chemical features.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 135, "endOffset": 143}, {"referenceID": 46, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 135, "endOffset": 143}, {"referenceID": 53, "context": "Both models use the simpli\u0080ed molecular input line entry system (SMILES) [58] as input to extract the features and regenerate the SMILES string (or functionally similar SMILES string).", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": "Deep learning in drug (chemical) analyses is mainly used as a classi\u0080er; however, the original purpose of deep learning is not only for classi\u0080cation, but also for extracting hidden representations [41].", "startOffset": 198, "endOffset": 202}, {"referenceID": 60, "context": "A considerable amount of chemical information exists, such as weight, molecular formula, rings, atoms, SMILES [66], and InChI [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "A considerable amount of chemical information exists, such as weight, molecular formula, rings, atoms, SMILES [66], and InChI [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "SMILES is also used for drug generation [17, 51] and for compound similarity [45].", "startOffset": 40, "endOffset": 48}, {"referenceID": 46, "context": "SMILES is also used for drug generation [17, 51] and for compound similarity [45].", "startOffset": 40, "endOffset": 48}, {"referenceID": 41, "context": "SMILES is also used for drug generation [17, 51] and for compound similarity [45].", "startOffset": 77, "endOffset": 81}, {"referenceID": 44, "context": "It is generally known that structure is closely related to function [48, 54].", "startOffset": 68, "endOffset": 76}, {"referenceID": 49, "context": "It is generally known that structure is closely related to function [48, 54].", "startOffset": 68, "endOffset": 76}, {"referenceID": 24, "context": "QSAR prediction, which exploits the relationship between the chemical structure and the biological activity, for identifying \u2018druglikeness\u2019 1 and establishing metabolic pathways [28, 56].", "startOffset": 178, "endOffset": 186}, {"referenceID": 51, "context": "QSAR prediction, which exploits the relationship between the chemical structure and the biological activity, for identifying \u2018druglikeness\u2019 1 and establishing metabolic pathways [28, 56].", "startOffset": 178, "endOffset": 186}, {"referenceID": 27, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 174, "endOffset": 181}, {"referenceID": 65, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 174, "endOffset": 181}, {"referenceID": 21, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 207, "endOffset": 215}, {"referenceID": 23, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 207, "endOffset": 215}, {"referenceID": 27, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 264, "endOffset": 276}, {"referenceID": 29, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 264, "endOffset": 276}, {"referenceID": 40, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 264, "endOffset": 276}, {"referenceID": 38, "context": "A\u0089er the convolution layer, each convolued element is processed with the recti\u0080ed linear unit (ReLU) [42]", "startOffset": 101, "endOffset": 105}, {"referenceID": 48, "context": "\u008ce maxpooling length of 6 showed the best performance in our experiments (see Figure 6C) To prevent over\u0080\u008aing and to produce generalization e\u0082ects, we apply dropout [53] for the last CNN units.", "startOffset": 165, "endOffset": 169}, {"referenceID": 7, "context": "For a symmetric problem, such as a tweets comparison, Keras use shared layers, which reuse the weights [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 19, "context": "In the fully-connected layers, we use 128 hidden nodes, batch normalization [23], and a dropout [53] rate of 0.", "startOffset": 76, "endOffset": 80}, {"referenceID": 48, "context": "In the fully-connected layers, we use 128 hidden nodes, batch normalization [23], and a dropout [53] rate of 0.", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "\u008ce entire learning is conducted with optimization algorithm Adam [29] with an epoch of 100 and mini-batch size of 256.", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "0) downloaded from the STITCH [32] database.", "startOffset": 30, "endOffset": 34}, {"referenceID": 59, "context": "\u008ce representations of chemical compounds were prepared as two types: the SMILES string, and the other was PubChem \u0080ngerprint [65].", "startOffset": 125, "endOffset": 129}, {"referenceID": 45, "context": "It has showed the best performance for certain tasks [50, 64], although the performance of the molecular descriptor depends on the target problem and classi\u0080er.", "startOffset": 53, "endOffset": 61}, {"referenceID": 58, "context": "It has showed the best performance for certain tasks [50, 64], although the performance of the molecular descriptor depends on the target problem and classi\u0080er.", "startOffset": 53, "endOffset": 61}, {"referenceID": 52, "context": "we retrieved the SMILES string and PubChem \u0080ngerprint by using PubChemPy [57].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Our dataset consisted of training, validation, and test samples, as in the general learning experiments [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "1) [9].", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "\u008ce pooling layer summarized the adjacent features resulting abstract representations, and inputs were down-sampled, resulting in a smaller number of model parameters to learn [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 57, "context": "We compared our proposed method, DeepCCI, with a feedforward neural network (FFNN), which is a plain deep classi\u0080er described in Figure 1A, and the conventional machine learning methods of support vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive boosting (AdaBoost) [16].", "startOffset": 218, "endOffset": 226}, {"referenceID": 51, "context": "We compared our proposed method, DeepCCI, with a feedforward neural network (FFNN), which is a plain deep classi\u0080er described in Figure 1A, and the conventional machine learning methods of support vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive boosting (AdaBoost) [16].", "startOffset": 247, "endOffset": 254}, {"referenceID": 13, "context": "We compared our proposed method, DeepCCI, with a feedforward neural network (FFNN), which is a plain deep classi\u0080er described in Figure 1A, and the conventional machine learning methods of support vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive boosting (AdaBoost) [16].", "startOffset": 289, "endOffset": 293}], "year": 2017, "abstractText": "Chemical-chemical interaction (CCI) plays a key role in predicting candidate drugs, toxicity, therapeutic e\u0082ects, and biological functions. CCI was created from text mining, experiments, similarities, and databases; to date, no learning-based CCI prediction method exist. In chemical analyses, computational approaches are required. \u008ce recent remarkable growth and outstanding performance of deep learning have a\u008aracted considerable research a\u008aention. However, even in state-of-the-art drug analyses, deep learning continues to be used only as a classi\u0080er. Nevertheless, its purpose includes not only simple classi\u0080cation, but also automated feature extraction. In this paper, we propose the \u0080rst end-to-end learning method for CCI, named DeepCCI. Hidden features are derived from a simpli\u0080ed molecular input line entry system (SMILES), which is a string notation representing the chemical structure, instead of learning from cra\u0089ed features. To discover hidden representations for the SMILES strings, we use convolutional neural networks (CNNs). To guarantee the commutative property for homogeneous interaction, we apply model sharing and hidden representation merging techniques. \u008ce performance of DeepCCI was compared with a plain deep classi\u0080er and conventional machine learning methods. \u008ce proposed DeepCCI showed the best performance in all seven evaluation metrics used. In addition, the commutative property was experimentally validated. \u008ce automatically extracted features through end-to-end SMILES learning alleviates the signi\u0080cant efforts required for manual feature engineering. It is expected to improve prediction performance, in drug analyses.", "creator": "LaTeX with hyperref package"}}}