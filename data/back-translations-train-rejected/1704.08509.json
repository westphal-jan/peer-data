{"id": "1704.08509", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "No More Discrimination: Cross City Adaptation of Road Scene Segmenters", "abstract": "Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its time-machine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data.", "histories": [["v1", "Thu, 27 Apr 2017 11:14:21 GMT  (5266kb,D)", "http://arxiv.org/abs/1704.08509v1", "13 pages, 10 figures"]], "COMMENTS": "13 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yi-hsin chen", "wei-yu chen", "yu-ting chen", "bo-cheng tsai", "yu-chiang frank wang", "min sun"], "accepted": false, "id": "1704.08509"}, "pdf": {"name": "1704.08509.pdf", "metadata": {"source": "CRF", "title": "No More Discrimination: Cross City Adaptation of Road Scene Segmenters", "authors": ["Yi-Hsin Chen", "Wei-Yu Chen", "Yu-Ting Chen", "Bo-Cheng Tsai", "Yu-Chiang Frank Wang", "Min Sun"], "emails": ["vigorous0503}@gmail.com", "ycwang@citi.sinica.edu.tw", "sunmin@ee.nthu.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. CNN-based Semantic Segmentation", "text": "Semantic segmentation is one of the recent breakthroughs in computer vision due to the development and proliferation of Convolutional Neural Networks (CNN), which has been successfully used to predict dense pixel-by-pixel semantic labels [6, 18, 22, 2, 4]. For example, Long1https: / / maps.googleblog.com / 2014 / 04 / go-back-in-time-with-streetview.htmlet al. [18] use CNN for pixel-level classification, which is capable of generating pixel-by-pixel outputs of any size. To achieve high-resolution prediction, [22, 2] deformation layers continue to match CNN with promising performance. Chen et al. [4], on the other hand, opt to add a fully networked CRF layer to their CNN output, which refines the pixel labels with context information."}, {"heading": "2.2. Segmentation of Road Scene Images", "text": "In order to apply CNN-based segmentators to images of street scenes, there are several attempts to train segmentators on large-format image datasets [5, 37, 30, 31]. Thus, Cordts et al. [5] publish a natural segmentation dataset for street scenes consisting of over 5000 annotated images. Xie et al. [37] comment on semantic 3D labels in a scene, followed by the transfer of 3D labels to the accompanying 2D video images. [30, 31] collect large-scale semantic labels from Computer Graphic (CG) images; however, the construction of CG worlds for practical purposes can still be computer-aided. [3] decide to loosen oversight during the data acquisition process and simply require a number of dot labels per image. In addition [24, 26, 27] image-level labels require only 38 labels to be used for image-level image capture, and 34 data-point labels to be used for data-limited objects [14], in addition to data-level data collection and data-limited objects."}, {"heading": "2.3. DNN-based Domain Adaptation", "text": "Since the goal of our work is to adapt CNN-based segmentation across datasets (or cities, to be more precise), we are now reviewing newer approaches to domain adaptation based on Deep Neural Networks (DNN) [23]. Based on Maximum Mean Discrepancy (MMD), Long et al. [19] minimize the mean distance between data domains and later incorporate the concept of residual learning [21] for further improvement. Zellinger et al. [40] consider Central Moment Discrepancy (CMD) instead of MMD, while Sener et al. [33] enforce cyclical consistency in adaptation and structured consistency in transduction within their framework. Recently, Generative Adversarial Network (GAN) [9] has attracted much attention in the fields of computer vision and machine learning. While most existing architectures for synthesizing images with specific styles [9, 29, 41]."}, {"heading": "3. Dataset", "text": "With Google Street View, street images can be retrieved globally from a large number of cities around the world. To address the issue of geo-location discrimination in a street scenario, we download street images from four cities in different locations, Rome, Rio, Tokyo and Taipei, which are expected to have significant differences in appearance. To ensure that we cover sufficient variations in the visual appearance of each city, we randomly test the locations in each city for image collection. Temporary information. Google Street View's time machine functions allow pairs of images of the same location to be preserved over different periods of time. As detailed later in Sec. 4.2, this feature in particular allows us to observe prior information from static objects so that improved adaptation can be achieved without annotation."}, {"heading": "4. Our Method", "text": "In this section, we present the details of our proposed unattended domain adaptation framework, which is capable of adapting pre-trained segmentators in different cities without using any user-commented data. In other words, while both images IS and labels YS are available from the source domain S, only images IT can be observed for the target domain T. When adapting image segments in different cities, two different types of domain shifts (or dataset distortions) can be expected: global and class-by-class domain shifts. The former comes from the general differences in appearance between cities, while the latter is due to different compositions of street components in each city. To minimize global domain shifts, we follow the technique of contradictory learning, which introduces a domain discriminator at a loss LG. This is to distinguish the difference between source and target domain class F."}, {"heading": "4.1. Global Domain Alignment", "text": "For cross-domain image segmentation, however, each image would consist of several pixels that can be viewed as multiple instances per observation. So how to expand the idea of cross-domain learning to fit image segments beyond image domains would be our focus. \u2212 Inspire by [11], we take every grid in the fc7 function board of the FCN-based segment as an instance. Let's leave the function cards of source and target domain images as MF (IS, \u03b8LF) and MF (IT, \u03b8LF), each map consisting of N grids. Let pn (x) = oller (MG (MF) n, \u0445F) n: the function cards of source and target domain images as source domains. \u2212 DIS: The grid n of image x belongs to the source domain where it is the sigmoid function."}, {"heading": "4.2. Class-wise Domain Alignment", "text": "In addition to suppressing global misalignment between image areas, we propose to advance the same adversarial learning architecture to make class-by-class domain adjustments. Whereas the idea of regulating class-by-segment information during segment adjustment was already seen in [11], its class-by-class orientation is based on the composition of class components in cross-city street scene images. To be more precise, it assumes that the composition / proportion of object classes in cities would be similar. Therefore, such regulation essentially performs global rather than class-by-class adjustments. Recall this when we adjust our segment images in cities, we only observe the images of the target city of interest without any labeling. We expand the idea in [20] and assign pseudo-labels to the images of the target domain, which is after the global adjustment in Fig. 3, the predicted probability distribution maps."}, {"heading": "4.3. Harvesting Static-Object Prior", "text": "Within the scope of unattended domain adjustment, however, we are able to use the temporal information to extract the priorities of static objects from images in the target domain. As shown in Fig. 4, fine-tuning of the segment by such information is not possible. As shown in Fig. 4, we first apply DeepMatching [36] to map pixels within each image pair. For regions with matching pixels across images, it implies that such regions are associated with static objects in the target domain (e.g. buildings, streets, etc.) that have static boundaries c."}, {"heading": "5. Experiments", "text": "First, we conduct experiments to prove the problem of discrimination between cities even with the help of a state-of-the-art semantic segment. Then, we review the effectiveness of our proposed unsupervised learning method at the cityscapes to our Dataset Domain Adaptation task. By comparing it to a fully monitored baseline (i.e. fine-tuning it with fully commented training data), we show that in most cases, our unsupervised method would achieve performance comparable to the fully monitored methods. Finally, we conduct an additional experiment, SYNTHIA to Cityscapes, to prove that our method is generally applicable to different data sets."}, {"heading": "5.1. Implementation Details", "text": "In this thesis, all implementations are created using the open source TensorFlow [1] framework, and the codes are released after acceptance. In the following experiments, we use mini-batch size 16 and the Adam Optimizer [13] with a learning rate of 5 \u00d7 10 \u2212 6, beta1 = 0.9 and beta2 = 0.999 to optimize the network. In addition, we use the hyperparameters in (1): \u03bbG and \u03bbclass, with the numbers gradually changing from 0 to 0.1 and 0 to 0.5, respectively. In addition, we use for experiments with static object priors {street, sidewalk, building, wall, fence, mast, traffic lights, traffic signs, vegetation, terrain, sky} as a set of static object classes defined in Section 4.3."}, {"heading": "5.2. Cross-City Discrimination", "text": "Interestingly, we observe a trend: the farther the geographic distance between the target city and the trained city (Frankfurt), the more severe the performance decline. This means that different visual appearances would dramatically impair the accuracy of the segment due to cultural differences in the cities. For example, in Taipei, as shown in Fig. 2, many signs and shop signs are attached to the buildings and many scooters on the street, which is unusual in Frankfurt. It also justifies the need for an effective adaptation method for the segmentator of the street scene in order to alleviate discrimination."}, {"heading": "5.3. Cross-City Adaptation", "text": "We divide our 100 images with fine annotations into 10 subsets for each city. Each time we select one subset as a test set, and the other 90 images as a training set and fine-tune the segment for 2000 steps. We repeat the procedure for 10 times and calculate the test results as a baseline performance. Our method. Now, we apply our adversarial learning method in the domain to adjust the pre-segmentor in an unsupervised manner. In the meantime, we conduct the ablation study to demonstrate the contribution of each component: global alignment, class-based alignment and static object. We summarize the experimental results in Table 2, where \"Pre-Trained\" denotes the pre-trained model, \"UB\" the fully supervised upper limit, \"GA\" the global alignment, \"GA + CA\" the combination of object and a global U method, \"\" the average U method, \"\" the average U class, \"\" the average U gain, \"\" \"the average U gain.\""}, {"heading": "5.4. Synthetic to Real Adaptation", "text": "In this experiment, we use SYNTHIARAND-CITYSCAPES [31] as the source domain containing 9400 synthetic street images with Cityscapes-compatible annotations. For the unlabeled target area, we use the Cityscapes training set. During the evaluation, we test our customized segmentator on the Cityscapes validation set. We note that since Cityscapes (as in our dataset) does not have paired images with time information, we cannot extract static object priors in this Ex periment. Nevertheless, these results from the results shown in Table 3 show that the global and class-by-class alignment with our proposed method still yields 3.1% and 1.9% mIOU gain."}, {"heading": "6. Conclusion", "text": "In this paper, we present a method of unattended domain adaptation for semantic segmentation that reduces cross-sectoral discrimination in images of street scenes in different cities. We propose a unified framework that leverages domain-hostile learning that adopts a common global and class-by-class alignment using soft labels from source and target domain data. In addition, our method identifies and implements static object priorities of our method that are retrieved from images over time via natural synchronization of static objects. Finally, we provide a new dataset that includes street scenes from four cities in different countries, including high-quality annotations and paired images with time information. We demonstrate the effectiveness of each component of our method in tasks with different levels of domain shift."}, {"heading": "B. Dataset", "text": "To demonstrate the uniqueness of our dataset for the semantic adaptation of street scenes, we show here further examples of this. Unlabeled Image Pair In Fig. 8, further examples are collected from different cities with different manifestations. Valuable temporal information that facilitates unattended adaptation is included in these image pairs. Labeled Image In Fig. 9, we also show more commented images to demonstrate the label quality of our dataset."}, {"heading": "C. Synthetic to Real Adaptation", "text": "In paragraph 5.4 of the main paper, we have presented the quantitative results of this adjustment task in Table 3 and come to the conclusion that our method could work well even in this challenging environment. To better support our conclusion, we show here some typical examples of this task in Fig. 10."}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Whats the point: Semantic segmentation with point supervision", "author": ["A. Bearman", "O. Russakovsky", "V. Ferrari", "L. Fei-Fei"], "venue": "ECCV. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1915\u20131929", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "ICML", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Domainadversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "Journal of Machine Learning Research, 17(59):1\u201335", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet auto-annotation with segmentation propagation", "author": ["M. Guillaumin", "D. K\u00fcttel", "V. Ferrari"], "venue": "IJCV. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "FCNs in the wild: Pixel-level adversarial and constraint-based adaptation", "author": ["J. Hoffman", "D. Wang", "F. Yu", "T. Darrell"], "venue": "arXiv preprint arXiv:1612.02649", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Undoing the damage of dataset bias", "author": ["A. Khosla", "T. Zhou", "T. Malisiewicz", "A.A. Efros", "A. Torralba"], "venue": "ECCV. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Seed", "author": ["A. Kolesnikov", "C.H. Lampert"], "venue": "expand and constrain: Three principles for weakly-supervised image segmentation. In ECCV. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Scribblesup: Scribble-supervised convolutional networks for semantic segmentation", "author": ["D. Lin", "J. Dai", "J. Jia", "K. He", "J. Sun"], "venue": "CVPR. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Coupled generative adversarial networks", "author": ["M.-Y. Liu", "O. Tuzel"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Entropy rate superpixel segmentation", "author": ["M.-Y. Liu", "O. Tuzel", "S. Ramalingam", "R. Chellappa"], "venue": "CVPR. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["M. Long", "J. Wang", "G. Ding", "J. Sun", "P.S. Yu"], "venue": "ICCV", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised domain adaptation with residual transfer networks", "author": ["M. Long", "H. Zhu", "J. Wang", "M.I. Jordan"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation", "author": ["G. Papandreou", "L.-C. Chen", "K.P. Murphy", "A.L. Yuille"], "venue": "ICCV. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Constrained convolutional neural networks for weakly supervised segmentation", "author": ["D. Pathak", "P. Krahenbuhl", "T. Darrell"], "venue": "ICCV. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional multi-class multiple instance learning", "author": ["D. Pathak", "E. Shelhamer", "J. Long", "T. Darrell"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "From image-level to pixellevel labeling with convolutional networks", "author": ["P.O. Pinheiro", "R. Collobert"], "venue": "CVPR. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Variational recurrent adversarial deep domain adaptation", "author": ["S. Purushotham", "W. Carvalho", "T. Nilanon", "Y. Liu"], "venue": "ICLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "ICLR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Playing for data: Ground truth from computer games", "author": ["S.R. Richter", "V. Vineet", "S. Roth", "V. Koltun"], "venue": "ECCV. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", "author": ["G. Ros", "L. Sellart", "J. Materzynska", "D. Vazquez", "A.M. Lopez"], "venue": "CVPR. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Built-in foreground/background prior for weakly-supervised semantic segmentation", "author": ["F. Saleh", "M.S.A. Akbarian", "M. Salzmann", "L. Petersson", "S. Gould", "J.M. Alvarez"], "venue": "ECCV. Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning transferrable representations for unsupervised domain adaptation", "author": ["O. Sener", "H.O. Song", "A. Saxena", "S. Savarese"], "venue": "NIPS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Distinct class-specific saliency maps for weakly supervised semantic segmentation", "author": ["W. Shimoda", "K. Yanai"], "venue": "ECCV. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros"], "venue": "CVPR. IEEE", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "ICCV. IEEE", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic instance annotation of street scenes by 3d to 2d label transfer", "author": ["J. Xie", "M. Kiefel", "M.-T. Sun", "A. Geiger"], "venue": "CVPR. IEEE", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to segment under various forms of weak supervision", "author": ["J. Xu", "A.G. Schwing", "R. Urtasun"], "venue": "CVPR. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Central moment discrepancy (CMD) for domain-invariant representation learning", "author": ["W. Zellinger", "T. Grubinger", "E. Lughofer", "T. Natschl\u00e4ger", "S. Saminger-Platz"], "venue": "ICLR", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A.A. Efros"], "venue": "ECCV. Springer", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "We consider the segmenter of [2] which is trained on Cityscapes [5], and apply for segmenting about 400 annotated road scene images of different cities across countries: Rome, Rio, Taipei, and Tokyo.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "We consider the segmenter of [2] which is trained on Cityscapes [5], and apply for segmenting about 400 annotated road scene images of different cities across countries: Rome, Rio, Taipei, and Tokyo.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "For instance, pixel labeling of one Cityscapes image takes 90 minutes on average [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 36, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 80, "endOffset": 88}, {"referenceID": 30, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 80, "endOffset": 88}, {"referenceID": 31, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 118, "endOffset": 129}, {"referenceID": 33, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 118, "endOffset": 129}, {"referenceID": 2, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 118, "endOffset": 129}, {"referenceID": 22, "context": "Inspired by the recent advances in domain adaptation [23, 35, 12], we propose an unsupervised learning framework for performing cross-city semantic segmentation.", "startOffset": 53, "endOffset": 65}, {"referenceID": 34, "context": "Inspired by the recent advances in domain adaptation [23, 35, 12], we propose an unsupervised learning framework for performing cross-city semantic segmentation.", "startOffset": 53, "endOffset": 65}, {"referenceID": 11, "context": "Inspired by the recent advances in domain adaptation [23, 35, 12], we propose an unsupervised learning framework for performing cross-city semantic segmentation.", "startOffset": 53, "endOffset": 65}, {"referenceID": 5, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 17, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 21, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 1, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 3, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 17, "context": "[18] utilize CNN for performing pixel-level classification, which is able to produce pixel-wise outputs of arbitrary sizes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In order to achieve high resolution prediction, [22, 2] further adapt deconvolution layers into CNN with promising performances.", "startOffset": 48, "endOffset": 55}, {"referenceID": 1, "context": "In order to achieve high resolution prediction, [22, 2] further adapt deconvolution layers into CNN with promising performances.", "startOffset": 48, "endOffset": 55}, {"referenceID": 3, "context": "[4] choose to add a fully-connected CRF layer at their CNN output, which refines the pixel labels with context information properly preserved.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 36, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 29, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 30, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 4, "context": "[5] release a natural road scene segmentation dataset, which consists of over 5000 annotated images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[37] annotate 3D semantic labels in a scene, followed by transferring the 3D labels into the associated 2D video frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30, 31] collect semantic labels from Computer Graphic (CG) images at a large scale; however, building CG worlds for practical uses might still be computationally expensive.", "startOffset": 0, "endOffset": 8}, {"referenceID": 30, "context": "[30, 31] collect semantic labels from Computer Graphic (CG) images at a large scale; however, building CG worlds for practical uses might still be computationally expensive.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "On the other hand, [3] choose to relax the supervision during the data collection process, and simply require a number of point-labels per image.", "startOffset": 19, "endOffset": 22}, {"referenceID": 23, "context": "Moreover, [24, 26, 27] only require image-level labels during data collection and training.", "startOffset": 10, "endOffset": 22}, {"referenceID": 25, "context": "Moreover, [24, 26, 27] only require image-level labels during data collection and training.", "startOffset": 10, "endOffset": 22}, {"referenceID": 26, "context": "Moreover, [24, 26, 27] only require image-level labels during data collection and training.", "startOffset": 10, "endOffset": 22}, {"referenceID": 24, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 46, "endOffset": 58}, {"referenceID": 33, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 46, "endOffset": 58}, {"referenceID": 31, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 46, "endOffset": 58}, {"referenceID": 13, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Alternatively, [15, 38] apply free-form squiggles to provide partial pixel labels for data collection.", "startOffset": 15, "endOffset": 23}, {"referenceID": 37, "context": "Alternatively, [15, 38] apply free-form squiggles to provide partial pixel labels for data collection.", "startOffset": 15, "endOffset": 23}, {"referenceID": 9, "context": "Finally, [10] utilize image-level labels with cosegmentation techniques to infer semantic segmentation of foreground objects in the images of ImageNet.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "Since the goal of our work is to adapt CNN-based segmenters across datasets (or cities to be more precise), we now review recent deep neural networks (DNN) based approaches for domain adaptation [23].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "[19] minimize the mean distance between data domains, and later they incorporate the concept of residual learning [21] for further improvements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[19] minimize the mean distance between data domains, and later they incorporate the concept of residual learning [21] for further improvements.", "startOffset": 114, "endOffset": 118}, {"referenceID": 39, "context": "[40] consider Central Moment Discrepancy (CMD) instead of MMD, while Sener et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] enforce cyclic consistency on adaptation and structured consistency on transduction in their framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Recently, Generative Adversarial Network (GAN) [9] has raised great attention in the fields of computer vision and machine learning.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "styles [9, 29, 41].", "startOffset": 7, "endOffset": 18}, {"referenceID": 28, "context": "styles [9, 29, 41].", "startOffset": 7, "endOffset": 18}, {"referenceID": 40, "context": "styles [9, 29, 41].", "startOffset": 7, "endOffset": 18}, {"referenceID": 15, "context": "In Coupled GAN [16], domain adaptation is achieved by first generating corresponded instances across domains, followed by performing classification.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "In parallel with the appearance of GAN [9], Ganin et al.", "startOffset": 39, "endOffset": 42}, {"referenceID": 6, "context": "propose Domain Adversarial Neural Networks (DANN) [7, 8], which consider adversarial training for suppressing domain biases.", "startOffset": 50, "endOffset": 56}, {"referenceID": 7, "context": "propose Domain Adversarial Neural Networks (DANN) [7, 8], which consider adversarial training for suppressing domain biases.", "startOffset": 50, "endOffset": 56}, {"referenceID": 27, "context": "For further extension, Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) [28] utilizes Variational Auto Encoder (VAE) and RNN for timeseries adaptation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "[11] extend such frameworks for semantic segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "We define 13 major classes for annotation: road, sidewalk, building, traffic light, traffic sign, vegetation, sky, person, rider, car, bus, motorcycle, and bicycle, as defined in Cityscapes [5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 10, "context": "To minimize the global domain shift, we follow [11] and apply the technique of adversarial learning, which introduces a domain discriminator with a loss LG.", "startOffset": 47, "endOffset": 51}, {"referenceID": 38, "context": "While we utilize the front-end dilated-FCN [39] as the pre-trained segmenter in our work, it is worth noting that our framework can be generally applied to other semantic segmenters.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Previously, domain adversarial learning frameworks have been applied for solving cross-domain image classification tasks [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "Inspire by [11], we take each grid in the fc7 feature map of the FCN-based segmenter as an instance.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "[7] use the same loss function plus a gradient reversal layer to update the feature extractor and domain discriminator simultaneously.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "To alleviate the above issue, we follow [9] and decompose the above problem into two subtasks.", "startOffset": 40, "endOffset": 43}, {"referenceID": 10, "context": "While the idea of regularizing class-wise information during segmenter adaptation has been seen in [11], its classwise alignment is performed based on the composition of the class components in cross-city road scene images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "Under such unsupervised settings, we extend the idea in [20] and assign pseudo labels to pixels/grids in the images of the target domain.", "startOffset": 56, "endOffset": 60}, {"referenceID": 35, "context": "4, given an image pair of the same location but across different times, we first apply DeepMatching [36] to relate pixels within each image pair.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Then, we additionally perform superpixel segmentation on the image pair using Entropy Rate Superpixel [17], which would group the nearby pixels into regions while the boundaries of the objects can be properly preserved.", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "In this work, all the implementations are produced utilizing the open source TensorFlow [1] framework, and the codes will be released upon acceptance.", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "In the following experiments, we use mini-batch size 16 and the Adam optimizer [13] with learning rate of 5\u00d7 10\u22126, beta1 = 0.", "startOffset": 79, "endOffset": 83}, {"referenceID": 30, "context": "In this experiment, we take SYNTHIARAND-CITYSCAPES [31] as the source domain, which contains 9400 synthetic road scene images with Cityscapescompatible annotations.", "startOffset": 51, "endOffset": 55}], "year": 2017, "abstractText": "Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its timemachine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data.", "creator": "LaTeX with hyperref package"}}}