{"id": "1704.08531", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "A Survey of Neural Network Techniques for Feature Extraction from Text", "abstract": "This paper aims to catalyze the discussions about text feature extraction techniques using neural network architectures. The research questions discussed in the paper focus on the state-of-the-art neural network techniques that have proven to be useful tools for language processing, language generation, text classification and other computational linguistics tasks.", "histories": [["v1", "Thu, 27 Apr 2017 12:27:25 GMT  (118kb,D)", "http://arxiv.org/abs/1704.08531v1", "9 pages, 2 figures"]], "COMMENTS": "9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vineet john"], "accepted": false, "id": "1704.08531"}, "pdf": {"name": "1704.08531.pdf", "metadata": {"source": "CRF", "title": "A Survey of Neural Network Techniques for Feature Extraction from Text", "authors": ["Vineet John"], "emails": ["v2john@uwaterloo.ca"], "sections": [{"heading": "1 Motivation", "text": "Much of the methods currently used for text-based feature extraction are based on relatively simple statistical techniques, such as a word coincidence model such as n-grams or a sack-of-words model such as TF-IDF.The motivation of this research project is to identify and examine the techniques used by neural networks and compare them with traditional text composition models to demonstrate their differences in procedural models.Featurextraction of text can be used for a variety of applications, including - but not limited to - unattended semantic similarity detection, article classification, and mood analysis.The aim of this project is to document the differences, pros, and cons of feature extraction from text data using neural networks."}, {"heading": "2 Research Questions", "text": "RQ1 What are the relatively simple statistical techniques for extracting attributes from text? RQ2 Is there an inherent advantage when using neural networks as opposed to simple methods? RQ3 What are the trade-offs that neural networks enter into as opposed to simple methods? RQ4 How can the different techniques be compared in terms of performance and accuracy? RQ5 In which use cases do the trade-offs outweigh the benefits of neural networks?"}, {"heading": "3 Methodology", "text": "The research issues listed in Section 2 will be addressed by examining some of the important reviews in this area (Goldberg et al., 2016) (Bengio et al., 2003) (Morin et al., 2005), and some of the ground-breaking research in this area, including Word Embeddings (Mikolov et al., 2013a) (Mikolov et al., 2013b) (Mikolov et al., 2013c), as well as other less obvious methods of feature extraction, including tasks such as part-of-speech tagging, chunking, designated entity recognition and semantic role marking (Socher et al., 2011) (Luong et al., 2013) (Maas et al., 2015) (Collobert et al., 2011) (Pennington et al., 2014)."}, {"heading": "4 Background", "text": "This section provides background knowledge on the tasks within computational linguistics."}, {"heading": "4.1 Part-of-Speech Tagging", "text": "\u2022 POS marking aims to mark each word with a unique tag indicating its syntactical role, ar Xiv: 170 4.08 531v 1 [cs.C L] 27 April 201 7such as noun, verb, adjective, etc. \u2022 The best POS markers are based on classifiers trained on text windows which are then fed into a bidirectional decoding algorithm during inference. \u2022 Generally, models resemble a bidirectional dependency network and can be trained with a variety of methods, including support vector machines and bidirectional Viterbi decoders."}, {"heading": "4.2 Chunking", "text": "\u2022 Chunking aims to mark segments of a sentence with syntactical components such as noun or verb phrases. It is also known as shallow parsing and can be seen as generalizing the marking of a portion of the language to phrases rather than words. \u2022 Implementations of chunking usually require an underlying POS implementation whereby the words are concatenated or dissected."}, {"heading": "4.3 Named Entity Recognition", "text": "\u2022 NER labels atomic elements in one sentence in categories such as PERSON or LOCATION. \u2022 Training for NER classifiers includes POS tags, CHUNK tags, prefixes and suffixes, as well as large lexicographs of the labeled units."}, {"heading": "4.4 Semantic Role Labeling", "text": "\u2022 SRL aims to assign a semantic role to a syntactic component of a sentence. \u2022 State-of-the-art SRL systems consist of several stages: generating a parse tree, determining which parse tree nodes represent the arguments of a given verb, and finally classifying these nodes to calculate the corresponding SRL tags. \u2022 SRL systems usually contain numerous features such as the parts of the language and syntactic names of words and nodes in the tree, the syntactic path to the verb in the parse tree, whether a node in the parse tree is part of a noun or verb phrase, etc."}, {"heading": "5 Document Vectorization", "text": "Document vectorization is required to convert text content into a numerical vector representation that can be used as traits to train a machine learning model. This section discusses a few different statistical methods for calculating this trait vector (John and Vechtomova, 2017)."}, {"heading": "5.1 N-gram Model", "text": "N-grams are contiguous sequences of \"n\" items from a given text or speech sequence. In a complete corpus of documents, each tuple of \"n\" grams, characters or words is represented by a unique bit in a bit vector that, when combined into a text body, forms a sparse vectorized representation of the text in the form of n-gram occurrences."}, {"heading": "5.2 TF-IDF Model", "text": "Term Frequency - Inverse Document Frequency (TF-IDF) is a numerical statistic intended to reflect how important a word is to a document in a collection or corpus (Sparck Jones, 1972). The value of the TF-IDF increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adapt to the fact that some words generally occur more frequently."}, {"heading": "5.3 Paragraph Vector Model", "text": "A paragraph vector model consists of an unattended learning algorithm that learns fixed vector representations for texts with variable length such as sentences and documents (Le and Mikolov, 2014); vector representations learn to predict surrounding words in contexts taken from the paragraph; two different implementations have gained importance in the community. \u2022 Doc2Vec: An implementation of the Python library in Gensim. 1. \u2022 FastText: A standalone implementation in C + +. (Bojanowski et al., 2016) (Joulin et al., 2016).1https: / / radimrehurek.com / gensim / models / doc2vec.html"}, {"heading": "6 A Primer of Neural Net Models for", "text": "NLP (Goldberg, 2016) \u2022 Fully networked feed-forward neural networks are non-linear learners that can be used as drop-in replacements wherever a linear learner is used. \u2022 The high accuracy observed in experimental results is a consequence of this non-linearity along with the availability of pre-trained word embedding. \u2022 Multi-layered feed-forward networks can provide competitive results in terms of sense classification and response to factual questions. \u2022 Convolutions and pooling architecture show promising results in many tasks, including document classification, short text categorization, sentiment classification, relationship typing between units, event recognition, paraphrase identification, semantic role labeling, question response, prediction of kinokasse turnover based on critical considerations, modeling of textual interactions and modeling sequences and the relationship between language and parts of structure sequences, as well as the characterization of language and parts of sequences."}, {"heading": "7 A Neural Probabilistic Language Model", "text": "Objective: Knowing the basic structure of a sentence, one should be able to create a new sentence by replacing parts of the old sentence with interchangeable units (Bengio et al., 2003). Challenge: The most important bottleneck is the calculation of the activations of the output layer, since it is a fully connected Softmax activation layer. \u2022 The authors propose to combat the curse of dimensionality by learning a distributed representation of the words, which allows each training set to form the model over an exponential number of semantically adjacent sentences. \u2022 A fundamental problem that complicates speech modelling and other learning problems is the curse of dimensionality, by giving a distributed representation of the words that form the sequences."}, {"heading": "8 Hierarchical Probabilistic Neural Network Language Model", "text": "Objective: Implementation of a hierarchical decomposition of conditional probabilities that allows an acceleration of about 200 both during training and during detection. Hierarchical decomposition is a binary hierarchical clustering that is limited by the prior knowledge gained from the semantic hierarchy of WordNet2 (Morin and Bengio, 2005). Description: \u2022 Similar to the previous essay, attempts are made to overcome the \"curse of dimensionality\" (Section 7) and create a much faster variant. \u2022 Past n grammars are used to learn a re-evaluated vector representation of each word. \u2022 The learned word embeddings are shared across all involved nodes in the distributed architecture. \u2022 A very important component of the entire model is the choice of the words binary encoding, i.e. the hierarchical word Cluster-2https: / / wordnet.princeton.edu / Combine the precursor of this essay with empirical knowledge of the authors."}, {"heading": "9 A Hierarchical Neural Autoencoder for Paragraphs and Documents", "text": "Aim: Attempts to create an embedded paragraph from the underlying word and sentence embedding and then encode it in an attempt to reconstruct the original paragraph (Li et al., 2015). Description: \u2022 The implementation uses an LSTM layer to convert words into a vector representation of a sentence. \u2022 A subsequent LSTM layer converts several sentences into a paragraph. \u2022 To achieve this, we need to maintain syntactic, semantic and discursion-related properties while we create the embedded representation. \u2022 Hierarchical LSTM used to maintain the sentence structure. \u2022 Parameters are estimated by maximizing the likelihood of input, similar to standard sequence-to-to-sequence models. \u2022 Estimations are calculated using Softmax functions to maximize the likelihood of constituent words. \u2022 Attention models using the hierarchical autoencoder could be used for dialog systems, as they are used explicitly for disc courses."}, {"heading": "10 Linguistic Regularities in Continuous Space Word Representations", "text": "Aim: In this paper, the authors examine the vector space word representations, which are implicitly learned by the input layer weights. (These representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relationship-specific vector offset. (Mikolov et al., 2013c) This is one of the pioneering papers that led to the creation of Word2Vec, which is a state-of-the-art word embedding tool (Mikolov et al., 2013a). (Description: \u2022 A defining feature of neural network systems is their representation of words as high-dimensional real vectors. \u2022 In this model, words are transformed via a learned search table into real, evaluated vector macros used as inputs for a neural network."}, {"heading": "11 Better Word Representations with Recursive Neural Networks for Morphology", "text": "Aim: The work aims to address the imprecision in the vector representation of complex and rare words allegedly caused by the lack of relationship between morphologically related words (Luong et al., 2013). Description: \u2022 The authors treat each morpheme as a base unit in the RNNs and construct representations for morphologically complex words on the fly from their morphemes. \u2022 Discuss a problem that resembles the syntactical relations between Word2Vec and Word2Vec - xcars \u2212 xcarmight not apply if the vector representation of a rare word is inaccurate to begin with morphologic semantics and its compositional properties. \u2022 morphoRNN operates at the morpheme level and not at the word level. An example of this is the representation of Nekars \u2212 xcars \u2212 xcarmight not be true if the vector representation of a rare word is inaccurate at the beginning."}, {"heading": "12 Efficient Estimation of Word Representations in Vector Space", "text": "Objective: The main objective of this paper is to introduce techniques that can be used to learn high-quality word vectors from huge datasets containing billions of words and with millions of words in vocabulary (Mikolov et al., 2013a). Challenge: The complexity that results from the complete normalization of the neural network is the dominant part of the calculation. \u2022 The ideas presented in this paper are based on the hierarchical versions of softmax output activation units proposed by (Bengio et al., 2003), or the refrain of per-forming normalization altogether.Description: \u2022 The ideas build on the receptive ideas proposed by (Bengio et al., 2003). The aim was to obtain high-quality word embedding that capture the syntactic and semantic properties of words that are altogethermic."}, {"heading": "13 Distributed Representations of Words and Phrases and their Compositionality", "text": "Objective: This paper builds on the idea of the Word2Vec Skip-gram model and presents optimizations in terms of the quality of word embedding as well as speed-ups during training. It also proposes an alternative to the hierarchical Softmax end layer, known as negative sampling (Mikolov et al., 2013b). Description: \u2022 One of the proposed optimizations is to substantially count the words used in training in order to achieve an acceleration in model training. \u2022 Given a sequence of training words [w1, w2, w3,..., wT] the goal of the Skip-gram model is to maximize the average log probability shown in the equation 31T T T T. \u2212 c \u2264 j \u2264 j c; j 6 = 0 logP (wt + j, wt) (3), where c is the window or context in which the current word is trained. \u2022 As introduced by 2005, Bengio is hierarchical, a benchmark."}, {"heading": "14 Glove: Global Vectors for Word Representation", "text": "Aim: This paper proposes a global log-bilinear regression model that combines the advantages of the two most important model families in literature: global matrix factorization and local context window methods (Pennington et al., 2014). Description: \u2022 Methods such as LSA use leverage statistical information efficiently, but perform relatively poorly on word analog tasks, indicating a suboptimal vector spatial structure. Methods such as Skip-gram may perform better on analog tasks, but they make poor use of corpus statistics by training on separate local context windows rather than global co-event values. \u2022 The relationship between arbitrary words can be investigated by examining the relationship of their co-event probabilities with different sample words. \u2022 The authors suggest that the appropriate starting point for word vector learning should be coevent probability ratios rather than probabilities themselves."}, {"heading": "15 Discussion", "text": "Following the literature survey, this section returns to the original research questions and provides a concise summary that can be derived from the experimental results and conclusions from the original papers. RQ1 What are the relatively simple statistical techniques to extract characteristics from text? Frequency models of word counting such as n-gram and simple terminology bag models such as TFIDF are still the simplest tools to obtain a numerical vector representation of text. RQ2 Is there an inherent advantage in using neural networks as opposed to simple methods? The advantage of using neural networks primarily consists in their ability to identify arcane patterns and remain flexible enough for a range of applications from topic classification to syntax parameter tree generation. RQ3 What are the target conflicts that neural networks cause as opposed to simple methods? Target conflicts are typically trained in terms of computing costs and network memory factor, as the complexity of neural networks can be expressed."}, {"heading": "16 Conclusion", "text": "This paper summarizes the important aspects of modern neural networking techniques that have emerged in recent years. Machine translation, understanding natural language, and producing natural language are important areas of research when it comes to developing a range of applications, from a simple chatbot to the conceptualization of a general AI unit. The discussion part aggregates the results of the work studied and provides a ready reference for newcomers to the field. For future work, it is intended to experimentally compare different word-embedding approaches to act as a boot-strapping method to create iteratively high-quality datasets for the future use of machine learning."}, {"heading": "17 Acknowledgments", "text": "The author thanks Dr. Pascal Poupart for his constructive feedback on the survey proposal."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of machine learning research 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606 .", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg."], "venue": "Journal of Artificial Intelligence Research 57:345\u2013420.", "citeRegEx": "Goldberg.,? 2016", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Uw-finsent at semeval-2017 task 5: Fine-grained sentiment analysis on financial news headlines", "author": ["Vineet John", "Olga Vechtomova."], "venue": "Proceedings of the 11th international workshop on semantic evaluation .", "citeRegEx": "John and Vechtomova.,? 2017", "shortCiteRegEx": "John and Vechtomova.", "year": 2017}, {"title": "Bag of tricks for efficient text classification", "author": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.01759 .", "citeRegEx": "Joulin et al\\.,? 2016", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML. volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL. pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Andrew L Maas", "Ziang Xie", "Dan Jurafsky", "Andrew Y Ng."], "venue": "HLT-NAACL. pages 345\u2013354.", "citeRegEx": "Maas et al\\.,? 2015", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Hlt-naacl. volume 13, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Aistats. Citeseer, volume 5, pages 246\u2013252.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng."], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11). pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["Karen Sparck Jones."], "venue": "Journal of documentation 28(1):11\u201321.", "citeRegEx": "Jones.,? 1972", "shortCiteRegEx": "Jones.", "year": 1972}], "referenceMentions": [{"referenceID": 3, "context": "The research questions listed in Section 2 will be tackled by surveying a few of the important overview papers on the topic(Goldberg, 2016)(Bengio et al.", "startOffset": 123, "endOffset": 139}, {"referenceID": 0, "context": "The research questions listed in Section 2 will be tackled by surveying a few of the important overview papers on the topic(Goldberg, 2016)(Bengio et al., 2003)(Morin and Bengio, 2005).", "startOffset": 139, "endOffset": 160}, {"referenceID": 12, "context": ", 2003)(Morin and Bengio, 2005).", "startOffset": 7, "endOffset": 31}, {"referenceID": 9, "context": "A few of the groundbreaking research papers in this area will also be studied, including word embeddings(Mikolov et al., 2013a)(Mikolov et al.", "startOffset": 104, "endOffset": 127}, {"referenceID": 10, "context": ", 2013a)(Mikolov et al., 2013b)(Mikolov et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 11, "context": ", 2013b)(Mikolov et al., 2013c).", "startOffset": 8, "endOffset": 31}, {"referenceID": 14, "context": "In addition to this, other less-obvious methods of features extraction will be surveyed, including tasks like part-of-speech tagging, chunking, named entity recognition, and semantic role labeling(Socher et al., 2011)(Luong et al.", "startOffset": 196, "endOffset": 217}, {"referenceID": 7, "context": ", 2011)(Luong et al., 2013)(Maas et al.", "startOffset": 7, "endOffset": 27}, {"referenceID": 8, "context": ", 2013)(Maas et al., 2015)(Li et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": ", 2015)(Collobert et al., 2011)(Pennington et al.", "startOffset": 7, "endOffset": 31}, {"referenceID": 13, "context": ", 2011)(Pennington et al., 2014).", "startOffset": 7, "endOffset": 32}, {"referenceID": 4, "context": "This section talks about a few different statistical methods for computing this feature vector(John and Vechtomova, 2017).", "startOffset": 94, "endOffset": 121}, {"referenceID": 6, "context": "A Paragraph Vector model is comprised of an unsupervised learning algorithm that learns fixedsize vector representations for variable-length pieces of texts such as sentences and documents (Le and Mikolov, 2014).", "startOffset": 189, "endOffset": 211}, {"referenceID": 1, "context": "(Bojanowski et al., 2016) (Joulin et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": ", 2016) (Joulin et al., 2016).", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "6 A Primer of Neural Net Models for NLP(Goldberg, 2016)", "startOffset": 39, "endOffset": 55}, {"referenceID": 0, "context": "Goal: Knowing the basic structure of a sentence, one should be able to create a new sentence by replacing parts of the old sentence with interchangeable entities(Bengio et al., 2003).", "startOffset": 161, "endOffset": 182}, {"referenceID": 12, "context": "The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet2 semantic hierarchy(Morin and Bengio, 2005).", "startOffset": 148, "endOffset": 172}, {"referenceID": 11, "context": "This allows vector-oriented reasoning based on the offsets between words(Mikolov et al., 2013c).", "startOffset": 72, "endOffset": 95}, {"referenceID": 9, "context": "This is one of the seminal papers that led to the creation of Word2Vec, which is a state-of-the-art word embedding tool(Mikolov et al., 2013a).", "startOffset": 119, "endOffset": 142}, {"referenceID": 7, "context": "Goal: The paper aims to address the inaccuracy in vector representations of complex and rare words, supposedly caused by the lack of relation between morphologically related words(Luong et al., 2013).", "startOffset": 179, "endOffset": 199}, {"referenceID": 9, "context": "Goal: The main goal of this paper is to introduce techniques that can be used for learning highquality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary(Mikolov et al., 2013a).", "startOffset": 204, "endOffset": 227}, {"referenceID": 0, "context": "Description: \u2022 The ideas presented in this paper build on the previous ideas presented by (Bengio et al., 2003).", "startOffset": 90, "endOffset": 111}, {"referenceID": 10, "context": "It also proposes an alternative to the hierarchical softmax final layer, called negative sampling(Mikolov et al., 2013b).", "startOffset": 97, "endOffset": 120}, {"referenceID": 12, "context": "\u2022 As introduced by (Morin and Bengio, 2005), a computationally efficient approximation of the full softmax is the hierarchical softmax.", "startOffset": 19, "endOffset": 43}, {"referenceID": 13, "context": "context window methods(Pennington et al., 2014).", "startOffset": 22, "endOffset": 47}, {"referenceID": 9, "context": "\u2022 The model obtained in the paper could be compared to a global skip-gram model as opposed to a fixed window-size skipgram model as proposed by (Mikolov et al., 2013a).", "startOffset": 144, "endOffset": 167}], "year": 2017, "abstractText": "This paper aims to catalyze research discussions about text feature extraction techniques using neural network architectures. The research questions discussed here focus on the state-of-the-art neural network techniques that have proven to be useful tools for language processing, language generation, text classification and other computational linguistics tasks.", "creator": "LaTeX with hyperref package"}}}