{"id": "1704.08619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "abstract": "Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to extract features from the speech, while for the visual modality a deep residual network (ResNet) of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-Term Memory (LSTM) networks are utilized. The system is then trained in an end-to-end fashion where - by also taking advantage of the correlations of the each of the streams - we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.", "histories": [["v1", "Thu, 27 Apr 2017 15:14:33 GMT  (1324kb)", "http://arxiv.org/abs/1704.08619v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["panagiotis tzirakis", "george trigeorgis", "mihalis a nicolaou", "bj\\\"orn schuller", "stefanos zafeiriou"], "accepted": false, "id": "1704.08619"}, "pdf": {"name": "1704.08619.pdf", "metadata": {"source": "CRF", "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "authors": ["Panagiotis Tzirakis", "George Trigeorgis", "Mihalis A. Nicolaou", "Bj\u00f6rn Schuller", "Stefanos Zafeiriou"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "II. RELATED WORK", "text": "The performance of pattern recognition models has been greatly improved with DNNs. Recently, a number of new neural network architectures have been revitalized with multimodal models of multimodal architecture, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13], or memory enhanced neural network models such as Long Short-Term Memory (LSTM) networks, which are used in various ways for multimodal recognition tasks such as speech recognition. [15] A multimodal Deep Autoencoder (MDAE) network has been proposed to extract functions from audio and video modalities. Initially, a bimodal DBN recognition task such as speech recognition has been trained."}, {"heading": "III. PROPOSED METHOD", "text": "One of the first steps in a traditional machine is to extract functions in audio technology."}, {"heading": "B. Speech Network", "text": "Unlike previous work in the field of paralinguistics, where acoustic features are first extracted and then transferred to a machine learning algorithm, we aim to learn the feature extraction and regression steps in a jointly trained model to predict emotion input. We segment the raw waveform into 6-second sequences after pre-processing the time sequences to have zero mean and unit variance to account for variations in different speaker levels between speakers. At 16 kHz sampling rate, this is equivalent to a 96000-dimensional input vector. Temporal contraction. We use F = 20 spatiotemporal pulse filters with a 5ms window to extract finescale spectral information from the high sampling rate. Pooling over time. The impulse response of each filter is guided through a half-wave rectifier (analogous to the cochlear transduction step in the human ear)."}, {"heading": "C. Objective function", "text": "In order to evaluate the level of consistency between the network's predictions and the gold standard derived from the remarks, the concordance correlation coefficient (\u03c1c) [9] was recently proposed [37], [7]. Nevertheless, the MSE was minimized during the network training, but the models were evaluated with respect to \u03c1c [37], [7]. Instead, we propose to include the metric used to evaluate performance in the objective function (Lc) used for the formation of networks. Since the objective function is a cost function, we define Lc as follows: Lc = 1 \u2212 \u03c1c = 1 \u2212 2xy\u03c32x + \u03c3 2 y + (\u00b5x \u2212 \u00b5y) 2 = 1 \u2212 2\u04452\u04452xy\u03c32, where the development is a cost function, we define Lc = 2 + ctu2 y + (\u00b5x \u2212 \u00b5c) 2 and the final paint operations (x), a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a -"}, {"heading": "D. Network Training", "text": "Before the training of the multimodal network, each modality-specific network is trained separately to speed up the training process.Visual network. For the visual network, we decided to fine-tune the pre-formed ResNet-50 on the database used in this thesis. This model was trained on the ImageNet 2012 [38] classification dataset, which consists of 1000 classs.The pre-formed model was brought forward from the ground up to the training of the network in order to benefit from the characteristics already learned by the model. To train the network, we use two LSTM layers, each with 256 cells, stacked on top of it to gather temporal information.Speech Network. The CNN network works on raw signal to extract characteristics. To take into account the temporal structure of the language, we use two LSTM layers, each with 256 cells on top of the CNN.Multimodal Network. After the training of the visual and linguistic networks, the STM layers are extracted and only the STM layers are taken into account."}, {"heading": "IV. DATASET", "text": "The time-continuous prediction of spontaneous and natural emotions (arousal and valence) will be investigated by means of speech and image data using the REmote COLlaborative and Affective (RECOLA) database introduced by Ringeval et al. [39]; the complete data set for which participants have given their consent to the transmission of their data will be used for the purpose of this study. Four modalities are included in the corpus: audio, video, electrocardiogram (ECG) and electrodermal activity (EDA). A total of 9.5 hours of multimodal recordings were recorded from 46 French-speaking participants and commented on for 5 minutes each, with a collaboration task carried out in twins during a video conference. Among the participants were 17 Frenchmen, three Germans and three Italians. The data set is divided into three sections - train (16 subjects), validation (15 subjects) and test (15 subjects) - by stratification (i.e. balancing the gender and gender of the speaker)."}, {"heading": "V. EXPERIMENTS & RESULTS", "text": "For the training of the models, we used the Adam optimization method [40] and a fixed learning rate of 10 \u2212 4 for all experiments. For the audio model, we used a mini-batch with 25 samples. In addition, we used dropouts [41] to regulate the network with p = 0.5 for all layers except the recurring ones. This step is important because our models have a large amount of parameters (\u2248 1.5 million) and the network is not regulated, which makes it prone to adapt too much to the training data. For the video model, the image size used was 96 \u00d7 96 with mini-batch size 2. Small mini-batch is selected due to hardware constraints, the data was enlarged to 110 \u00d7 110 and randomly cut to its original size, resulting in a scaling invariant model. Furthermore, the color agmentation is applied to the forecast values by introducing random brightness and saturation. Finally, all of the preceding values [4] are applied to the chain being searched for."}, {"heading": "A. Ablation study", "text": "Due to concerns about memory and training instability [44], it is not always optimal to use very large sequences in recurrent networks. In order to select the best sequence length for our LSTM layers, we conducted experiments with sequence lengths 75, 150 and 300 for both speech and visual models. Table II shows the results on the development set. A total of 60 epochs were performed for our models for all experiments. For the visual network, we expect the highest value in the valence dimension, while for the language model in the excitation dimension. Results show that the best value for the language model is 150, while for the visual model, 300. Due to the fact that the performance difference for the visual network is small when sequence length of 150 or 300 is used, we have decided to train the multimodal network with 150 sequence lengths."}, {"heading": "B. Speech Modality", "text": "In all experiments, our model exceeds the designed characteristics in relation to \u03c1c. However, it can be noted that the eGEMAPS feature set provides a narrow performance on value, which is much more difficult to predict from language than from arousal. Furthermore, we show that by including species directly in the optimization function of all networks it is possible for us to optimize the models on the metric (\u03c1c) on which we evaluate the models. Thisprovides us with i) a more elegant way to optimize models, and ii) consistently gives better results in all tests as shown in Table III.I. In addition, we compare the performance on the methods that exist in the literature, most of which have been submitted to the challenge of AVEC 2016, with 27 participants using Table IV. In the case of performance on the test or validation was not reported."}, {"heading": "D. Multimodal Analysis", "text": "Only two other models found in the literature to use both linguistic and visual modalities in the RECOLA database are the Output-Associative Relevance Vector Machine Staircase Regression (OA RVM-SR) [30] and the Force Modeling System proposed by Han et al. [28]. The results are presented in Table VI. Our model outperforms the other two models in the large-scale valence dimension. For the excitation dimension, the OA RVM-SR provides the best results. However, we would like to note at this point that there are two main differences between our system and the system in [30] (a) that the system was trained in [30] with both the training and validation set, while our model was trained only with the training set and (b) our system works directly on the raw pixel domain, while the system in [30] uses a number of geometric features (e.g. the anticipation 2D / 3D visual folding, which we expect to be precise)."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we propose a multimodal system based on the raw signal to perform an end-to-end task for the spontaneous prediction of emotions from speech and visual data. In order to take into account the contextual information in the LSTM network of data, the training of the model was accelerated by practicing speech and visual networks separately. Furthermore, we study the gate activation of the recurring layers in the speech modality and find cells that are strongly correlated with prosodic features that were always thought to cause arousal. Our experiments on the unimmodal modality show that our models perform significantly better than other models that use the RECOLA database, including those that are subjected to the AVEC2016 challenge, demonstrating the effectiveness of learning functions that are better suited to the respective task."}, {"heading": "ACKNOWLEDGMENT", "text": "The support of the EPSRC Centre for Doctoral Training in High Performance Embedded and Distributed Systems (HiPEDS, Grant Reference EP / L016796 / 1) is appreciated."}], "references": [{"title": "Real-time nonintrusive monitoring and prediction of driver fatigue", "author": ["Q. Ji", "Z. Zhu", "P. Lan"], "venue": "IEEE Transactions on Vehicular Technology, vol. 53, no. 4, pp. 1052\u20131068, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Detecting anger in automated voice portal dialogs", "author": ["F. Burkhardt", "J. Ajmera", "R. Englert", "J. Stegmann", "W. Burleson"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, Pittsburgh, United States, September 2006, pp. 1053\u20131056.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011", "author": ["C.-N. Anagnostopoulos", "T. Iliou", "I. Giannoukos"], "venue": "Artificial Intelligence Review, vol. 43, no. 2, pp. 155\u2013177, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "Y. Dong", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proceedings of International Conference on Machine Learning, Beijing, China, June 2014, pp. 1764\u20131772.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The INTER- SPEECH 2013 Computational Paralinguistics Challenge: Social signals, conflict, emotion, autism", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "A. Vinciarelli", "K. Scherer", "F. Ringeval", "M. Chetouani", "F. Weninger", "F. Eyben", "E. Marchi", "M. Mortillaro", "H. Salamin", "A. Polychroniou", "F. Valente", "S. Kim"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, Lyon, France, August 2013, pp. 148\u2013152.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "AV+EC 2015 \u2013 The First Affect Recognition Challenge Bridging Across Audio, Video, and Physiological Data", "author": ["F. Ringeval", "B. Schuller", "M. Valstar", "S. Jaiswal", "E. Marchi", "D. Lalanne", "R. Cowie", "M. Pantic"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, Brisbane, Australia, October 2015, pp. 3\u20138.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition, Las Vegas, United States, June-July 2016, pp. 770\u2013 778.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A concordance correlation coefficient to evaluate reproducibility", "author": ["L.I.-K. Lin"], "venue": "Biometrics, vol. 45, no. 1, pp. 255\u2013268, March 1989.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "B. Schuller", "S. Zafeiriou"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Shanghai, China, March 2016, pp. 5200\u20135204.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["R.S. Zemel"], "venue": "Proceedings of the Neural Information Processing Systems, 1994.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Generalization and network design strategies", "author": ["Y. LeCun"], "venue": "Connectionism in Perspective, pp. 143\u2013155, 1989.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1989}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the International Conference on Machine Learning, Washington, United States, June-July 2011, pp. 689\u2013696.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporal multimodal learning in audiovisual speech recognition", "author": ["D. Hu", "X. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, United States, June-July 2016, pp. 3574\u20133582.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep dynamic neural networks for multimodal gesture segmentation and recognition", "author": ["D. Wu", "L. Pigou", "P.-J. Kindermans", "N.D.-H. Le", "L. Shao", "J. Dambre", "J.-M. Odobez"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 8, pp. 1583\u20131597, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech emotion recognition using deep neural network and extreme learning machine.", "author": ["K. Han", "D. Yu", "I. Tashev"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Speech emotion recognition using convolutional and recurrent neural networks", "author": ["W. Lim", "D. Jang", "T. Lee"], "venue": "Proceedings of the Signal and Information Processing Association Annual Summit and Conference, Jeju, Korea, December 2016, pp. 1\u20134.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning driven hypergraph representation for image-based emotion recognition", "author": ["Y. Huang", "H. Lu"], "venue": "Proceedings of the International Conference on Multimodal Interaction, Tokyo, Japan, November 2016, pp. 243\u2013247.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural networks for emotion recognition in video", "author": ["S. Ebrahimi Kahou", "V. Michalski", "K. Konda", "R. Memisevic", "C. Pal"], "venue": "Proceedings of the International Conference on Multimodal Interaction, Seattle, United States, November 2015, pp. 467\u2013474.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Emotion recognition using multimodal deep learning", "author": ["W. Liu", "W.-L. Zheng", "B.-L. Lu"], "venue": "International Conference on Neural Information Processing, Kyoto, Japan, October 2016, pp. 521\u2013529.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning for robust feature generation in audiovisual emotion recognition", "author": ["Y. Kim", "H. Lee", "E.M. Provost"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing, Vancouver, Canada, May 2013, pp. 3687\u20133691.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Emonets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulcehre", "V. Michalski", "K. Konda", "S. Jean", "P. Froumenty", "Y. Dauphin", "N. Boulanger- Lewandowski"], "venue": "Journal on Multimodal User Interfaces, vol. 10, no. 2, pp. 99\u2013111, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal deep convolutional neural network for audio-visual emotion recognition", "author": ["S. Zhang", "S. Zhang", "T. Huang", "W. Gao"], "venue": "Proceedings of the International Conference on Multimedia Retrieval, New York, United States, June 2016, pp. 281\u2013284.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data", "author": ["F. Ringeval", "F. Eyben", "E. Kroupi", "A. Yuce", "J.-P. Thiran", "T. Ebrahimi", "D. Lalanne", "B. Schuller"], "venue": "Pattern Recognition Letters, vol. 66, pp. 22\u201330, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Strength modelling for real-worldautomatic continuous affect recognition from audiovisual signals", "author": ["J. Han", "Z. Zhang", "N. Cummins", "F. Ringeval", "B. Schuller"], "venue": "Image and Vision Computing, pp. \u2013, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge", "author": ["M. Valstar", "J. Gratch", "B. Schuller", "F. Ringeval", "D. Lalanne", "M. Torres Torres", "S. Scherer", "G. Stratou", "R. Cowie", "M. Pantic"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 3\u201310.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Staircase regression in oa rvm, data selection and gender dependency in avec 2016", "author": ["Z. Huang", "B. Stasak", "T. Dang", "K. Wataraka Gamage", "P. Le", "V. Sethu", "J. Epps"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 19\u201326.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "High-level geometrybased features of video modality for emotion prediction", "author": ["R. Weber", "V. Barrielle", "C. Soladi\u00e9", "R. S\u00e9guier"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 51\u201358.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-modal audio, video and physiological sensor learning for continuous emotion prediction", "author": ["K. Brady", "Y. Gwon", "P. Khorrami", "E. Godoy", "W. Campbell", "C. Dagli", "T.S. Huang"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 97\u2013104.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal emotion recognition for avec 2016 challenge", "author": ["F. Povolny", "P. Matejka", "M. Hradis", "A. Popkov\u00e1", "L. Otrusina", "P. Smrz", "I. Wood", "C. Robin", "L. Lamel"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 75\u201382.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Online affect tracking with multimodal kalman filters", "author": ["K. Somandepalli", "R. Gupta", "M. Nasir", "B.M. Booth", "S. Lee", "S.S. Narayanan"], "venue": "Proceedings of the International Workshop on Audio/Visual Emotion Challenge, 2016, pp. 59\u201366.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved speech recognition using high-pass filtering of subband envelopes", "author": ["H. Hirsch", "P. Meyer", "H. Ruehl"], "venue": "Proceedings of the European Conference on Speech Technology, Genoa, Italy, September 1991, pp. 413\u2013416.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1991}, {"title": "Gammatone features and feature combination for large vocabulary speech recognition", "author": ["R. Schl\u00fcter", "L. Bezrukov", "H. Wagner", "H. Ney"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Honolulu, United States, April 2007, pp. 649\u2013 652.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data", "author": ["F. Ringeval", "F. Eyben", "E. Kroupi", "A. Yuce", "J.-P. Thiran", "T. Ebrahimi", "D. Lalanne", "B. Schuller"], "venue": "Pattern Recognition Letters, vol. 66, pp. 22\u201330, November 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions", "author": ["S.-A.S.J. Ringeval", "Fabien", "D. Lalanne"], "venue": "Proceedings of the IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, Shanghai, China, April 2013, pp. 1\u20138.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u2013 1958, January 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1929}, {"title": "Ensemble methods for continuous affect recognition: Multimodality, temporality, and challenges", "author": ["M. K\u00e4chele", "P. Thiam", "G. Palm", "F. Schwenker", "M. Schels"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, Brisbane, Australia, October 2015, pp. 9\u201316.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Correcting time-continuous emotional labels by modeling the reaction lag of evaluators", "author": ["S. Mariooryad", "C. Busso"], "venue": "IEEE Transactions on Affective Computing, vol. 6, no. 2, pp. 97\u2013108, April-June 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Vocal communication of emotion: A review of research paradigms", "author": ["K. Scherer"], "venue": "Speech Communication, vol. 40, no. 1-2, pp. 227\u2013256, April 2003.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2003}, {"title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  9 affective computing", "author": ["F. Eyben", "K.R. Scherer", "B.W. Schuller", "J. Sundberg", "E. Andr\u00e9", "C. Busso", "L.Y. Devillers", "J. Epps", "P. Laukka", "S.S. Narayanan"], "venue": "IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For instance, emotion states can be used to monitor and predict fatigue state [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "In speech recognition, emotion recognition can be used in call centres, where the goal is to detect the emotional state of the caller and provide feedback for the quality of the service [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 2, "context": "The task of recognising emotions is challenging because human emotions lack of temporal boundaries and different individuals express emotions in different ways [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "Numerous studies have shown the favourable property of these network variants to model inherent structure contained in the speech signal [4], with more recent research attempting end-to-end optimisation utilising as little human a-priori knowledge as possible [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "Numerous studies have shown the favourable property of these network variants to model inherent structure contained in the speech signal [4], with more recent research attempting end-to-end optimisation utilising as little human a-priori knowledge as possible [5].", "startOffset": 260, "endOffset": 263}, {"referenceID": 5, "context": "Nevertheless, the majority of these works make use of commonly hand-engineered features have been used as input features, such as Mel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP) coefficients, and supra-segmental features such as those used in the series of ComParE [6] and AVEC challenges [7], which build upon knowledge gained in decades of auditory research and have shown to be robust for many speech domains.", "startOffset": 296, "endOffset": 299}, {"referenceID": 6, "context": "Nevertheless, the majority of these works make use of commonly hand-engineered features have been used as input features, such as Mel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP) coefficients, and supra-segmental features such as those used in the series of ComParE [6] and AVEC challenges [7], which build upon knowledge gained in decades of auditory research and have shown to be robust for many speech domains.", "startOffset": 320, "endOffset": 323}, {"referenceID": 7, "context": "Features are extracted from the speech signal using a CNN architecture designed for the audio channel and from the visual information using a ResNet-50 network architecture [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "correlation coefficient (\u03c1c) [9] in our model and show that this improves performance in terms of emotion prediction compared to optimising the mean square error objective, which is traditionally used.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "A preliminary version of this work was presented in [10], where only the raw speech waveform was used.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 268, "endOffset": 272}, {"referenceID": 14, "context": "[15] proposed a Multimodal Deep Autoencoder (MDAE) network to extract features from audio and video modalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] proposed a temporal multimodal network named Recurrent Temporal Multimodal Restricted Boltzmann Machine (RTMRBM) to model audiovisual sequence of data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17] the authors use skeletal information and RGB-D images to recognize gestures.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "[18] uses hand-crafted features to feed a DNN that produces a probability distribution over categorical emotion states.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Lim et al [19] after transforming the data using short time fourier transform, they used CNNs to extract high-level features.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "[10] proposed an end-to-end model that uses a CNN to extract features from the raw signal and then an LSTM network to capture the contextual information in the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] proposed a transductive learning framework for image-based emotion recognition by combining DNNs and hypergraphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] combined CNNs and RNNs to recognise categorical emotions in videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Some studies exploited the beneficial features DNNs can extract [22], [23], [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "Some studies exploited the beneficial features DNNs can extract [22], [23], [24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Some studies exploited the beneficial features DNNs can extract [22], [23], [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "[23] proposed four different DBN architectures with one of them being a basic 2-layer DBN, and the others variation of it.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] proposed to combine modality-specific DNNs to recognize categorical emotions in video.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] used a multimodal CNN for classifying emotions with audio and visual modalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] uses an BLSTM-RNN to capture the contextual information that exists in the multimodal features (audio, video, physiological) extracted from the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] proposes the strength modeling framework which can be implemented as feature-level and decision-level fusion strategy and comprises of two regression models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The importance of recognizing emotions motivated the creation of the Audio/Visual Emotion Challenge and Workshop (AVEC) [29].", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "[30] proposed to use variants of Relevance Vector Machine (RVM) for modeling audio, video and audiovisual data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] used high-level geometry features for predicting dimensional features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] also used low- and high-level features for modeling emotions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] complemented original baseline features for both audio and video to perform emotion recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] also used additional features but only for the audio modality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "To extract features in audio, finite impulse response filters can be used which perform time-frequency decomposition to reduce the influence of background noise [35].", "startOffset": 161, "endOffset": 165}, {"referenceID": 34, "context": "More complicated handengineered kernels, such as gammatone filters [36], which were formulated by studying the frequency responses of the receptive fields of auditory neurons of grassfrogs, can be used as well.", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "Recently, deep convolutional networks have been used to extract features from faces [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "In this study we use a deep residual network (ResNet) of 50 layers [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "To evaluate the agreement level between the predictions of the network and the gold-standard derived from the annotations, the concordance correlation coefficient (\u03c1c) [9] has recently been proposed [37], [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 35, "context": "To evaluate the agreement level between the predictions of the network and the gold-standard derived from the annotations, the concordance correlation coefficient (\u03c1c) [9] has recently been proposed [37], [7].", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "To evaluate the agreement level between the predictions of the network and the gold-standard derived from the annotations, the concordance correlation coefficient (\u03c1c) [9] has recently been proposed [37], [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 35, "context": "Nonetheless, previous work minimized the MSE during the training of the networks, but evaluated the models with respect to \u03c1c [37], [7].", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Nonetheless, previous work minimized the MSE during the training of the networks, but evaluated the models with respect to \u03c1c [37], [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 36, "context": "This model was trained on the ImageNet 2012 [38] classification dataset that consists of 1000 classes.", "startOffset": 44, "endOffset": 48}, {"referenceID": 37, "context": "[39]; the full dataset for which participants gave their consent to share their data is used for the purpose of this study.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "For training the models we utilised the Adam optimisation method [40], and a fixed learning rate of 10 throughout all experiments.", "startOffset": 65, "endOffset": 69}, {"referenceID": 39, "context": "Also, for regularisation of the network, we used dropout [41] with p = 0.", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "4 s to 20 s) [7], (ii) centring (by computing the bias between gold-standard and prediction) [42], (iii) scaling (using the ratio of standard-deviation of gold-standard and prediction as scaling factor) [42] and (iv) time-shifting (by shifting the prediction forward in time with values ranging from 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 40, "context": "4 s to 20 s) [7], (ii) centring (by computing the bias between gold-standard and prediction) [42], (iii) scaling (using the ratio of standard-deviation of gold-standard and prediction as scaling factor) [42] and (iv) time-shifting (by shifting the prediction forward in time with values ranging from 0.", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "4 s to 20 s) [7], (ii) centring (by computing the bias between gold-standard and prediction) [42], (iii) scaling (using the ratio of standard-deviation of gold-standard and prediction as scaling factor) [42] and (iv) time-shifting (by shifting the prediction forward in time with values ranging from 0.", "startOffset": 203, "endOffset": 207}, {"referenceID": 41, "context": "04 s to 10 s), to compensate for delays in the ratings [43].", "startOffset": 55, "endOffset": 59}, {"referenceID": 42, "context": "Due to memory and training instability concerns [44] its not always optimal to use very large sequences in recurrent networks.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "Baseline [29] eGeMAPS .", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "455) RVM [30] eGeMAPS - (.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "396) Audio BN-Multi [33] Mixed - (.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "503) Brady et al [32] MFCC - (.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "[31] eGeMAPS - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Mixed - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] 13 LLDs .", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "It is well accepted amongst the research community that certain acoustic and prosodic features play an important role in recognising the affective state [45].", "startOffset": 153, "endOffset": 157}, {"referenceID": 44, "context": "Some of these features, such as the mean of the fundamental frequency (F0), mean speech intensity, loudness, as well as pitch range [46], should thus be captured by our model.", "startOffset": 132, "endOffset": 136}, {"referenceID": 26, "context": "[28] was not submitted to the AVEC 2016 challenge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Baseline [29] Geometric .", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "612) RVM [30] Geometric - (.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "571) Video CNN-L4 [33] Mixed - (.", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "497) Brady et al [32] Appearance - (.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "[31] Geometric - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Geometric - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Geometric .", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "These are the Output-Associative Relevance Vector Machine Staircase Regression (OA RVM-SR) [30] and the strength modeling system proposed by Han et al.", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "However, we would like to note here that there are two main differences between our system and the system in [30] (a) the system in [30] was trained using both training and validation set, whereas our model was trained using only the training set and (b) our system operates directly on the raw pixel domain, while the system in [30] made use of a number of geometric features (e.", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "However, we would like to note here that there are two main differences between our system and the system in [30] (a) the system in [30] was trained using both training and validation set, whereas our model was trained using only the training set and (b) our system operates directly on the raw pixel domain, while the system in [30] made use of a number of geometric features (e.", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "However, we would like to note here that there are two main differences between our system and the system in [30] (a) the system in [30] was trained using both training and validation set, whereas our model was trained using only the training set and (b) our system operates directly on the raw pixel domain, while the system in [30] made use of a number of geometric features (e.", "startOffset": 329, "endOffset": 333}, {"referenceID": 28, "context": "OA RVMSR [30] eGeMAPS ComParE Geometric Appearance .", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "[28] 13 LLDs Geometric .", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to extract features from the speech, while for the visual modality a deep residual network (ResNet) of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-TermMemory (LSTM) networks are utilized. The system is then trained in an end-to-end fashion where \u2013 by also taking advantage of the correlations of the each of the streams \u2013 we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.", "creator": "LaTeX with hyperref package"}}}