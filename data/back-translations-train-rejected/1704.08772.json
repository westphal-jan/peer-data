{"id": "1704.08772", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "Deep Face Deblurring", "abstract": "Even though the problem of blind deblurring is a long studied vision task, the outcomes of generic methods are not effective in real world blurred images. Exploiting domain knowledge for specific object deblurring, e.g. text or faces, has attracted recently an increasing amount of attention. Faces are among the most well studied objects in computer vision and despite the significant progress made, the deblurring of faces does not yield yet satisfying results in an arbitrary image. We study the problem of face deblurring by inserting weak supervision in the form of alignment in a deep network. We introduce an efficient framework, used to generate a dataset of over two million frames. The dataset, which we call 2MF2, was used during the network's training process. We conduct experiments with real world blurred facial images and report that our method returns a result close to the sharp natural latent image.", "histories": [["v1", "Thu, 27 Apr 2017 23:01:45 GMT  (9763kb,D)", "http://arxiv.org/abs/1704.08772v1", null], ["v2", "Thu, 25 May 2017 07:45:36 GMT  (6693kb,D)", "http://arxiv.org/abs/1704.08772v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["grigorios g chrysos", "stefanos zafeiriou"], "accepted": false, "id": "1704.08772"}, "pdf": {"name": "1704.08772.pdf", "metadata": {"source": "CRF", "title": "Deep Face Deblurring", "authors": ["Grigorios Chrysos", "Stefanos Zafeiriou"], "emails": ["g.chrysos@imperial.ac.uk", "s.zafeiriou@imperial.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "There is not a single generic algorithm that allows all objects to be debluralized. Although the human face is one of the most studied objects in computer vision, which has not attracted much attention due to its significant applications in various areas such as face recognition, computer graphics, facial weathering, it is true that the use of domain-specific knowledge can lead to superior results. It has been debated for a long time (42, 8, 34), but the results are far from satisfactory (e.g. state-of-the-art results in the real world are blurred). 1. The difficulty in the real world of weathered images can be attributed to the non-linear functions involved in the imaging process, such as lens saturated, depth, variation, weathering varies, blurred."}, {"heading": "2. Related Work", "text": "It is a question of whether it is a \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" \"new,\" new, \"\" new, \"new,\" \"new,\" new, \"new,\" new, \"\" new, \"new,\" new, \"new,\" new, \"new,\" \"new,\" new, \"new,\" \"new,\" new, \"new,\" new, \""}, {"heading": "3. Method", "text": "In this section, we will develop our method for defusing an image, including the deep network we use, which is a modified version of the powerful ResNet architecture. As a pre-processing step, we will include weak monitoring by localizing landmarks. We will explain how to create the image pairs to train the network, architecture, and then the follow-up steps for an invisible image."}, {"heading": "3.1. Notation", "text": "A sparse form of n loyalty points (landmarks) is designated as l for image I with l = [['1] T, [' 2] T,..., ['n] T, with \"j = [xj, yj] T, j [1, n], xj, yj\" R for the cartesian coordinates of the jth point. If we refer to a random image I, we will implicitly assume that I contain a human face, of which the sparse face form l is available."}, {"heading": "3.2. Training pair creation", "text": "Unfortunately, obtaining blurred images with a dense correspondence to a similarly sharp image is not a trivial task, especially when thousands of such images are needed to train a deep network. Therefore, we use similar methods ([40, 5]) to simulate blur from sharp images. A synthetically blurred image Ibl is created by merging the original sharp image I with a blur core (simulation of Equivalent 1). For each input image, a unique blur core is created to allow the maximum variation in the number of blur cores during training. At each step, the blur core is randomly selected between a Gaussian blur core and a motion blur core, both with varying deviation and spatial support."}, {"heading": "3.3. Network", "text": "We use the residual network (ResNet) architecture of [20] as a learning component in our method. ResNet consists of a number of \"blocks\"; each \"block\" is a sequence of revolutionary layers followed by linear units, with identity connections connecting the blocks. This simple architecture has proven state-of-the-art performance in several task areas. We modify the original ResNet by disabling the entire maximum pooling operation, while in the second and third ResNet block skip connections ([19]) are added. In each skip connection, a batch normalization is required to ensure a common scale; linear mapping is learned from the high-dimensional space of the connections to the low dimensional space of the output image form. [22] is used for our loss function, which is a continuous and differentiated function with Lh (x) = (x) | 1 \u2212 > | 1 | 0.5 | x | 1 | 1 | 1 (otherwise)."}, {"heading": "3.4. Inference", "text": "In the conclusion, we do not need a latent image, only a blurred Ibl image is sufficient. A face detector is used off the bar to obtain the boundary frame; the boundary stones are localized by a localization technique; the image is re-scaled according to the size of the boundary stones, while a rectangular area around the face (boundary stones) is truncated, this rktangulated area is fed into the above network (only the forward-facing part is required). One of the most successful face detectors is the Deformable Part Detector (DPM) [16, 30]. DPM learn a mix of models aimed at recognizing faces in different poses. Each model implicitly takes into account some parts that may deform at square cost. DPM's cost function includes an appearance (uniform) and a pair-by-word (deformation) plus a distortion, all of which are learned with a discriminatory training method."}, {"heading": "4. Data mining", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5. Experiments", "text": "In this section, the technical implementation details will be developed together with the experiments carried out. In the following paragraphs, we will explain only a few implementation details, summarize a validation experiment for our method with a simple Gaussian blur, compare current defuse methods in two different scenarios, which include motion blur and blurred images from the real world."}, {"heading": "5.1. Implementation details", "text": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were determined from the original ResNet paper [20], while most of the remaining functionality was provided by the Menpo project [1].The forms of the 68-point public data sets were used for additional input into the network for training (e.g. IBUG [38], HELEN [27], LFPW [3], and the 300W [37].Input: Video frames V = [I (1), I (2), I (2), b) additional input into the network (e.g. IBUG [38], HELEN [3], and the 2MF 2 data.Input: Video frame V = [I (2), I (2), I (2), x x x (M)] Output: Accepted frames F = [Landmarks L Initialize: F = [], Idle V = [I (2), I (2), I (2), I (2), x (x) Output: Accepted frames F = [Landmarks L Initialize: F = [], Idle F = [], Idle V = (x), Faces (x), > (x)."}, {"heading": "17 end", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "18 end", "text": "19 if cnt over < M / 2 then 20 return [], [] 21 end 22 return F, cluded from the training set; the frames of 2MF 2 were subsampled and every second frame was used for the training.The training steps of the classifier were the following: (a) Positive training samples were extracted from the 300W workout; disturbed versions of the annotations of these images along with selected images of the Pascal dataset [14] were used to obtain the negative samples. (b) A fixed-size patch was extracted from each positive sample around each of the n boundaries; SIFT [29] were calculated per patch. For each negative sample, a random disturbance of the ground truth points was performed to produce a faulty fit before extracting the patches. (c) A linear SVM was trained, with the hyperparameters in the withheld validation measurement being cross-validated."}, {"heading": "5.2. Self evaluation", "text": "Seventy images of AFLW [25] were used to validate the results of the network. Images were synthetically blurred with Gaussian noise, while the standard visual quality metrics of PSNR and SSIM [43] were used to compare the blurred images with the results of our network. Quality metrics are given in Table 1 and few indicative images are visualized in Figure 3. Both the qualitative and quantitative metrics indicate that the method actually works well under Gaussian blur."}, {"heading": "5.3. Simulated motion blur", "text": "In order to extend the simple Gaussian blur, an experiment was carried out that simulates the real world, blurred images. A series of sequential images of a high frame rate video are averaged and simulate the movement of the person. Averaging produces the effect of a dynamic motion, while the mean image of the mean can be considered the ground truth frame. The peripheral cases of this simulation consist of (a) no case of movement, (b) extreme case of motion. The first case was avoided by considering the optical flow of both images as too noisy and ensuring that there is at least some movement in the scene from picture to picture. In the second case, the PSNR of the averaged image was compared with the mean image (ground truth) and the images below a threshold were discarded as too noisy. In our experiment, four videos of the 300VW dataset [39] were used. All videos of the 300VW video contain one person per video, while all of them are over 25 ps."}, {"heading": "5.4. Real world blurred images", "text": "Providing a method that works for blurred images from the real world is a strong motivation for our work. Unfortunately, the comparison with blurred images from the real world comes with the price that there is no basic image. Therefore, we have decided to report the visual comparisons here. In Fig. 5, the comparisons between the different methods for the facial images provided by Lai et al. [26] are provided. To further emphasize the merits of the proposed method, we have collected some images from Internet sources, both indoor and outdoor. The faces in these images are of fairly low resolution, while there are rapid movements in the scene. Qualitative results are provided in Fig. 6.1Capturing a blurred image from the real world and a sharp image with exact match requires an elaborate hardware / software setup that makes the creation of such a dataset challenging."}, {"heading": "6. Discussion and conclusions", "text": "The architecture we have implemented is a modified version of the powerful ResNet. In addition, we have developed a robust framework that enables the creation of large data sets by using commercially available tools from literature. Using this framework, we have created 2MF 2, a data set containing more than two million frames of faces, which has been used to train our network. A series of experiments are being conducted to verify the performance of our method and compare it with the state-of-the-art blurring methods."}], "references": [{"title": "Menpo: A comprehensive platform for parametric image alignment and visual deformable mod-  Figure 6: (Preferably viewed in colour) Qualitative results in real world blurred images from arbitrary videos", "author": ["J. Alabort-i-Medina", "E. Antonakos", "J. Booth", "P. Snape", "S. Zafeiriou"], "venue": "On the top row, the original frame (there is no ground-truth available); on the second row the output of our method. els. In Proceedings of ACM International Conference on Multimedia (ACM\u2019MM), pages 679\u2013682. ACM,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian blind deconvolution with general sparse image priors", "author": ["S.D. Babacan", "R. Molina", "M.N. Do", "A.K. Katsaggelos"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 341\u2013355. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Localizing parts of faces using a consensus of exemplars", "author": ["P.N. Belhumeur", "D.W. Jacobs", "D.J. Kriegman", "N. Kumar"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 35(12):2930\u20132940,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks", "author": ["S. Bell", "C.L. Zitnick", "K. Bala", "R. Girshick"], "venue": "arXiv preprint arXiv:1512.04143,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural approach to blind motion deblurring", "author": ["A. Chakrabarti"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 221\u2013235. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs", "author": ["S. Chandra", "I. Kokkinos"], "venue": "Proceedings of European Conference on Computer Vision (ECCV). Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic and efficient human pose estimation for sign language videos", "author": ["J. Charles", "T. Pfister", "M. Everingham", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV), 110(1):70\u201390,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Blur identification and image restoration using a multilayer neural network", "author": ["C.-M. Cho", "H.-S. Don"], "venue": "Neural Networks, 1991. 1991 IEEE International Joint Conference on, pages 2558\u20132563. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Video deblurring for handheld cameras using patch-based synthesis", "author": ["S. Cho", "J. Wang", "S. Lee"], "venue": "ACM Trans. Gr., 31(4):64,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A comprehensive performance evaluation of deformable face tracking \u201cin-the-wild", "author": ["G.G. Chrysos", "E. Antonakos", "P. Snape", "A. Asthana", "S. Zafeiriou"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning spatially regularized correlation filters for visual tracking", "author": ["M. Danelljan", "G. H\u00e4ger", "F. Shahbaz Khan", "M. Felsberg"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV), pages 4310\u20134318,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. [Code: http: //tensorflow.org/, Status: Online; accessed 9- November-2016", "author": ["A M"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV), 88(2):303\u2013338,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Two-frame motion estimation based on polynomial expansion", "author": ["G. Farneb\u00e4ck"], "venue": "Image analysis, pages 363\u2013370,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 32(9):1627\u20131645,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV), pages 1440\u2013 1448,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deblurring by example using dense correspondence", "author": ["Y. Hacohen", "E. Shechtman", "D. Lischinski"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV), pages 2384\u20132391,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 447\u2013456,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural networks for direct text deblurring", "author": ["M. Hradi\u0161", "J. Kotera", "P. Zemc\u0131\u0301k", "F. \u0160roubek"], "venue": "In Proceedings of British Machine Vision Conference (BMVC),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Robust regression: asymptotics, conjectures and monte carlo", "author": ["P.J. Huber"], "venue": "The Annals of Statistics, pages 799\u2013821,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1973}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1867\u20131874,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Dlib-ml: A machine learning toolkit", "author": ["D.E. King"], "venue": "The Journal of Machine Learning Research, 10:1755\u20131758,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization", "author": ["M. K\u00f6stinger", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 2144\u20132151. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparative study for single image blind deblurring", "author": ["W.-S. Lai", "J.-B. Huang", "Z. Hu", "N. Ahuja", "M.-H. Yang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Interactive facial feature localization", "author": ["V. Le", "J. Brandt", "Z. Lin", "L. Bourdev", "T.S. Huang"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 679\u2013 692. Springer,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding and evaluating blind deconvolution algorithms", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1964\u20131971. IEEE,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision (IJCV), 60(2):91\u2013110,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Face detection without bells and whistles", "author": ["M. Mathias", "R. Benenson", "M. Pedersoli", "L. Van Gool"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 720\u2013735. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "IEEE Proceedings  of International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Deblurring face images with exemplars", "author": ["J. Pan", "Z. Hu", "Z. Su", "M.-H. Yang"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 47\u201362. Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Deblurring text images via l0-regularized intensity and gradient prior", "author": ["J. Pan", "Z. Hu", "Z. Su", "M.-H. Yang"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2901\u20132908,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Blind image deblurring using dark channel", "author": ["J. Pan", "D. Sun", "H. Pfister", "M.-H. Yang"], "venue": "prior. 2016", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Total variation blind deconvolution: The devil is in the details", "author": ["D. Perrone", "P. Favaro"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2909\u20132916. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "300 faces in-the-wild challenge: Database and results", "author": ["C. Sagonas", "E. Antonakos", "G. Tzimiropoulos", "S. Zafeiriou", "M. Pantic"], "venue": "Image and Vision Computing,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "300 faces in-the-wild challenge: The first facial landmark localization challenge", "author": ["C. Sagonas", "G. Tzimiropoulos", "S. Zafeiriou", "M. Pantic"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV-W), 300 Faces Inthe-Wild Challenge (300-W), pages 397\u2013403,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "The first facial landmark tracking in-the-wild challenge: Benchmark and results", "author": ["J. Shen", "S. Zafeiriou", "G. Chrysos", "J. Kossaifi", "G. Tzimiropoulos", "M. Pantic"], "venue": "IEEE Proceedings of International Conference on Computer Vision, 300 Videos in the Wild (300-VW): Facial Landmark Tracking in-the-Wild Challenge & Workshop (ICCV- W), December", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a convolutional neural network for non-uniform motion blur removal", "author": ["J. Sun", "W. Cao", "Z. Xu", "J. Ponce"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 769\u2013 777. IEEE,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel-free video deblurring via synthesis", "author": ["F. Tan", "S. Liu", "L. Zeng", "B. Zeng"], "venue": "IEEE Proceedings of International Conference on Image Processing (ICIP), pages 2683\u20132687. IEEE,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Identification of image and blur parameters for the restoration of noncausal blurs", "author": ["A. Tekalp", "H. Kaufman", "J. Woods"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, 34(4):963\u2013972,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1986}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions in Image Processing (TIP), 13(4):600\u2013612,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-image blind deblurring using a coupled adaptive sparse prior", "author": ["H. Zhang", "D. Wipf", "Y. Zhang"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1051\u20131058. IEEE,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "From actemes to action: A strongly-supervised representation for detailed action understanding", "author": ["W. Zhang", "M. Zhu", "K. Derpanis"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2248\u20132255,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Face alignment by coarse-to-fine shape searching", "author": ["S. Zhu", "C. Li", "C. Change Loy", "X. Tang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4998\u20135006,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 41, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 7, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 27, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 31, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 33, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 25, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 27, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 17, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 33, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 27, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 218, "endOffset": 226}, {"referenceID": 34, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 218, "endOffset": 226}, {"referenceID": 27, "context": "Apart from the generic deblurring method which are applied to all objects ([28, 18, 35]), there are also methods that utilise domain-specific knowledge, e.", "startOffset": 75, "endOffset": 87}, {"referenceID": 17, "context": "Apart from the generic deblurring method which are applied to all objects ([28, 18, 35]), there are also methods that utilise domain-specific knowledge, e.", "startOffset": 75, "endOffset": 87}, {"referenceID": 34, "context": "Apart from the generic deblurring method which are applied to all objects ([28, 18, 35]), there are also methods that utilise domain-specific knowledge, e.", "startOffset": 75, "endOffset": 87}, {"referenceID": 32, "context": "text or face priors ([33, 32]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 31, "context": "text or face priors ([33, 32]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 31, "context": "The method of [32] is currently the method that explicitly models the blurring for the human face.", "startOffset": 14, "endOffset": 18}, {"referenceID": 35, "context": "The last few years the introduction of elaborate benchmarks [36] allowed CNN methods to surpass the performance of the hand-crafted linear optimisation techniques, e.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 3, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 30, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 41, "endOffset": 45}, {"referenceID": 5, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 60, "endOffset": 63}, {"referenceID": 19, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "Similarly in object alignment the introduction of elaborate benchmarks [37] led to significant progress in landmark localisation [23, 46].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Similarly in object alignment the introduction of elaborate benchmarks [37] led to significant progress in landmark localisation [23, 46].", "startOffset": 129, "endOffset": 137}, {"referenceID": 45, "context": "Similarly in object alignment the introduction of elaborate benchmarks [37] led to significant progress in landmark localisation [23, 46].", "startOffset": 129, "endOffset": 137}, {"referenceID": 1, "context": "[2] (c) Zhang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "[44] (d) Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] (e) Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] (f) Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] (g) Original image", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The difference between deblurring results depending on the type of blur as emphasized in [26] can be visually confirmed.", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "For a more thorough study, the interested reader is redirected to [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "The majority of those works ([9, 41]) implicitly assume that there are multiple frames with approximately the same content and that there exists a sharp patch that matches in content the respective blurry one.", "startOffset": 29, "endOffset": 36}, {"referenceID": 40, "context": "The majority of those works ([9, 41]) implicitly assume that there are multiple frames with approximately the same content and that there exists a sharp patch that matches in content the respective blurry one.", "startOffset": 29, "endOffset": 36}, {"referenceID": 27, "context": "2, they frequently minimise with respect to the blur kernel Kg , which might lead to a blurry \u0128 if a joint MAP (Maximum a posteriori) optimisation is followed ([28]).", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "in [33] apply an `0 norm as a sparse prior on both the intensity values and the image gradient for deblurring text.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "[18] support that the gradient prior alone is not sufficient, and introduce a prior that locates dense correspondences of the blurry image with a similar sharp image, while they iteratively optimise over the correspondence, the kernel and the sharp image estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "A generalisation of [18] is the work of [32], which also requires an exemplar dataset to locate an image with a similar contour.", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "A generalisation of [18] is the work of [32], which also requires an exemplar dataset to locate an image with a similar contour.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "Even though those methods have proven to work well with synthetic blurs, they do not generalise well in realworld blurred images ([26]) due to the strong assumptions of invariance and the simplified format of \u03c8.", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "Another common attribute of these methods is the iterative optimisation procedure; they are executed in a loop hundreds or even thousands of times to return a deblurred image, which classifies these methods as computationally intensive; some of them require hours for deblurring a single image ([5]).", "startOffset": 295, "endOffset": 298}, {"referenceID": 20, "context": "Some methods ([21]) learn straight away the function \u03c6 from the data, while others ([40, 5]) learn an estimate and perform non-blind de-blurring/refinement of the sharp image.", "startOffset": 14, "endOffset": 18}, {"referenceID": 39, "context": "Some methods ([21]) learn straight away the function \u03c6 from the data, while others ([40, 5]) learn an estimate and perform non-blind de-blurring/refinement of the sharp image.", "startOffset": 84, "endOffset": 91}, {"referenceID": 4, "context": "Some methods ([21]) learn straight away the function \u03c6 from the data, while others ([40, 5]) learn an estimate and perform non-blind de-blurring/refinement of the sharp image.", "startOffset": 84, "endOffset": 91}, {"referenceID": 20, "context": "In [21], the regularised `2 loss of an up to 15-layer CNN is minimised for text deblurring.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "in [40], learn a CNN to recognise few discretised motion kernels and then perform a non-blind deconvolution in a dense motion field estimate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "Hence, following similar methods ([40, 5]) we resort to simulating the blur from sharp images.", "startOffset": 34, "endOffset": 41}, {"referenceID": 4, "context": "Hence, following similar methods ([40, 5]) we resort to simulating the blur from sharp images.", "startOffset": 34, "endOffset": 41}, {"referenceID": 19, "context": "We employ the residual network (ResNet) architecture of [20] as the learning component in our method.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "We modify the original ResNet by disabling all the max pooling operation, while skip connections ([19]) are added in the 2 and 3 ResNet blocks.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "The huber loss of [22] is utilised for our loss function.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "Among the most successful face detectors is the deformable part models (DPM) detector [16, 30].", "startOffset": 86, "endOffset": 94}, {"referenceID": 29, "context": "Among the most successful face detectors is the deformable part models (DPM) detector [16, 30].", "startOffset": 86, "endOffset": 94}, {"referenceID": 45, "context": "The crude bounding box of the DPM consists the initialisation of a landmark localisation technique [46, 23, 10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 22, "context": "The crude bounding box of the DPM consists the initialisation of a landmark localisation technique [46, 23, 10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 9, "context": "The crude bounding box of the DPM consists the initialisation of a landmark localisation technique [46, 23, 10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 45, "context": "Both techniques [46, 23] belong to the regression based discriminative methods for landmark localisation.", "startOffset": 16, "endOffset": 24}, {"referenceID": 22, "context": "Both techniques [46, 23] belong to the regression based discriminative methods for landmark localisation.", "startOffset": 16, "endOffset": 24}, {"referenceID": 36, "context": "Both methods have proven very accurate in a number of benchmarks [37, 10], hence we adopt the method of Kazemi et al.", "startOffset": 65, "endOffset": 73}, {"referenceID": 9, "context": "Both methods have proven very accurate in a number of benchmarks [37, 10], hence we adopt the method of Kazemi et al.", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "due to a publicly available fast implementation [24].", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 6, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 44, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 38, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 38, "context": "Collecting such datasets is an expensive and laborious process, hence there is an increasing effort to create datasets semi-automatically [39, 37] or almost in an unsupervised manner [7].", "startOffset": 138, "endOffset": 146}, {"referenceID": 36, "context": "Collecting such datasets is an expensive and laborious process, hence there is an increasing effort to create datasets semi-automatically [39, 37] or almost in an unsupervised manner [7].", "startOffset": 138, "endOffset": 146}, {"referenceID": 6, "context": "Collecting such datasets is an expensive and laborious process, hence there is an increasing effort to create datasets semi-automatically [39, 37] or almost in an unsupervised manner [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 15, "context": "The face detector of [16, 30] is applied to the first frame of the video.", "startOffset": 21, "endOffset": 29}, {"referenceID": 29, "context": "The face detector of [16, 30] is applied to the first frame of the video.", "startOffset": 21, "endOffset": 29}, {"referenceID": 11, "context": "In our work, we utilise the SRDCF [12], which provides a decent trade-off of accurate deformable tracking quality and computation complexity [10].", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "In our work, we utilise the SRDCF [12], which provides a decent trade-off of accurate deformable tracking quality and computation complexity [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 45, "context": "Subsequently, the landmark localisation technique of [46] is employed to obtain the sparse shapes for each face.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "We utilise a linear patchbased SVM [11] as the classifier fcl(I, l) which accepts a frame I along with the respective fitting l and returns a binary decision on whether this is an acceptable fitting.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "The requirement of non-static faces is fulfilled by computing the optical flow [15] in the accepted frames and requiring that there is at least a pixel movement from frame to frame.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "The modifications are: (i) the face detection module, which can be trivially replaced by a generic detector like [17], (ii) the classifier module for the removal of erroneous fittings, which should be trained for the task, e.", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were obtained from the original ResNet paper [20], while the majority of the rest functionality was provided by the Menpo project [1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were obtained from the original ResNet paper [20], while the majority of the rest functionality was provided by the Menpo project [1].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were obtained from the original ResNet paper [20], while the majority of the rest functionality was provided by the Menpo project [1].", "startOffset": 238, "endOffset": 241}, {"referenceID": 37, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 28, "endOffset": 31}, {"referenceID": 36, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "The training steps of the classifier were the following: (a) The positive training samples were extracted from the 300W trainset; perturbed versions of the annotations of those images along with selected images of Pascal dataset [14] were used for mining the negative samples.", "startOffset": 229, "endOffset": 233}, {"referenceID": 28, "context": "(b) A fixed size patch was extracted from each positive sample around each of the n landmark points; SIFT [29] were computed per patch.", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "Seventy images of AFLW [25] were used to validate the outcome of the network.", "startOffset": 23, "endOffset": 27}, {"referenceID": 42, "context": "The images were synthetically blurred with Gaussian noise, while the standard visual quality metrics of PSNR and SSIM [43] were employed to compare the blurred images with the outputs of our network.", "startOffset": 118, "endOffset": 122}, {"referenceID": 1, "context": "[2] 25.", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "[44] 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] 21.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] 22.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "512 Chakrabarti [5] 23.", "startOffset": 16, "endOffset": 19}, {"referenceID": 38, "context": "In our experiment, four videos of the 300VW dataset [39] were utilised.", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "[2], Zhang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "[44], Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32], Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33], Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] and Chakrabarti [5] were also included in the experiment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[34] and Chakrabarti [5] were also included in the experiment.", "startOffset": 21, "endOffset": 24}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 23, "endOffset": 26}, {"referenceID": 43, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 40, "endOffset": 44}, {"referenceID": 32, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 58, "endOffset": 62}, {"referenceID": 4, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 22, "endOffset": 25}, {"referenceID": 43, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 48, "endOffset": 52}, {"referenceID": 33, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 66, "endOffset": 69}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Even though the problem of blind deblurring is a long studied vision task, the outcomes of generic methods are not effective in real world blurred images. Exploiting domain knowledge for specific object deblurring, e.g. text or faces, has attracted recently an increasing amount of attention. Faces are among the most well studied objects in computer vision and despite the significant progress made, the deblurring of faces does not yield yet satisfying results in an arbitrary image. We study the problem of face deblurring by inserting weak supervision in the form of alignment in a deep network. We introduce an efficient framework, used to generate a dataset of over two million frames. The dataset, which we call 2MF , was used during the networks training process. We conduct experiments with real world blurred facial images and report that our method returns a result close to the sharp natural latent image.", "creator": "LaTeX with hyperref package"}}}