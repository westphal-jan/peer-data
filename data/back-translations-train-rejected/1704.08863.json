{"id": "1704.08863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "On weight initialization in deep neural networks", "abstract": "A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations. First, I derive a general weight initialization strategy for any neural network using activation functions differentiable at 0. Next, I derive the weight initialization strategy for the Rectified Linear Unit (RELU), and provide theoretical insights into why the Xavier initialization is a poor choice with RELU activations. My analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations.", "histories": [["v1", "Fri, 28 Apr 2017 09:57:52 GMT  (248kb,D)", "http://arxiv.org/abs/1704.08863v1", "9 pages, 4 figures"], ["v2", "Tue, 2 May 2017 22:43:10 GMT  (248kb,D)", "http://arxiv.org/abs/1704.08863v2", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siddharth krishna kumar"], "accepted": false, "id": "1704.08863"}, "pdf": {"name": "1704.08863.pdf", "metadata": {"source": "CRF", "title": "On weight initialization in deep neural networks", "authors": ["Siddharth Krishna Kumar"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recent years have seen rapid advances in our understanding of deep neural networks, which have led to breakthroughs in several areas, from image recovery (13) to speech recognition (5) to activation (8), despite the notoriously difficult training of these models."}, {"heading": "2 The setup", "text": "Consider a deep neural network with M layers. The relationship between the inputs to the mth layer (xm) and m + 1. Layer (xm + 1) is determined by the recursion sym (i) = j = N \u2211 j = 1 Wm (i, j) xm (j = N \u2211 j (1 pij (1) and xm + 1 (i) = g (m). (2) Here pj = Wm (i, j) xm (j), Wm is a matrix of weights for the 1st layer, g is the non-linear activation function, and N is the number of nodes in the hidden layers respectively. The weights Wm (i, j) are assumed to be independently identically distributed normal variables with the mean value 0 and the variance v2. Consistent with the assumptions in [3] and [6], I assume that the inputs to the first layer are independent and identically distributed random variables with the mean value of 2 and v2."}, {"heading": "3 Activation functions differentiable at 0", "text": "If g (x) is differentiable at 0, we can perform a Taylor expansion in (2) about E (ym (i)) = 0. Assuming that the terms of higher order can be ignored, xm + 1 (i) \u2248 g (0) + (ym \u2212 0) g \u2032 (0) results. (8) Taking the expectation in (8) results in a small dependence on the moments of the inputs to the mth layer. Based on this result, it can be determined recursively for all layers (except the first) that \u00b5j = g (0) for all j \u2265 1. (10) Taking (6) and (10), the variance of xm + 1 (i) results in the difference of (8) ass2m + 1 = V ar (xm + 1 (i) a differentiation (0) of the bit rate (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2)."}, {"heading": "3.1 Hyperbolic tangent activation", "text": "For the hyperbolic tangentg (x) = tanh (x), (13) we have g (0) = 0 and g \"(0) = 1. Using these results, we obtain (12) results in the order of magnitude v \u2248 1 \u221a N, (14) which represent exactly the Xavier initialization. Sequential saturation with the hyperbolic tangent In their analysis of a neural network with hyperbolic tangential activation, [3] we find that the deeper layers in the neural network have a greater proportion of unsaturated nodes than the shallower layers. As explained in their paper,\" why this happens remains to be understood. \"To explain their results, I will start with the authors initializing weights on the basis of a uniform distribution (U [\u2212 1 / \u221a N, 1 / \u0430 N]) and the variance of 1 / 3N."}, {"heading": "3.2 Sigmoid activation", "text": "For the sigmoid activation defined asg (x) = 11 + e \u2212 x (18), we have g (0) = 0.5, g \u2032 (0) = 1 / 4. Putting these values in (12), we get v \u2248 3.6 \u221a N, (19) To compare the initialization described in (19) with the Xavier initialization, I use a simple neural network with 10 layers, the architecture of which is described in Figure 2. For my experiments, I use the CIFAR-10 dataset [9] with 60,000 32 \u00d7 32 color images, evenly divided into 10 classes. The dataset includes 50,000 training examples (which form the training dataset in my analyses) and 10,000 test examples (which form the validation dataset in my analyses). First, I train the neural network with the Xavier initialization for 10 iterations and calculate the top 5 accuracies on the validation dataset specified next for use in the Item 19."}, {"heading": "4 Activation functions not differentiable at 0", "text": "If g (x) is not differentiable at 0, the analysis appears more difficult than in the previous section. Instead of trying to find a general solution, I will focus on the most important non-differentiable activation function used in the analysis of neural networks - the Rectified Linear Unit (RELU)."}, {"heading": "4.1 RELU activation", "text": "Since the RELU activation is indistinguishable from 0, the results of (8 - 11) cannot be used to calculate the optimum value of v2. To continue, I use Proposal 2 and (3), which indicate that the 2Python code (with the Keras package [1]) can be downloaded to replicate Figure 3: / / github.com / sidk86 / weight _ initializationfor the first iteration, ym (i) \u0445 N (0, u2m). We are interested in the mean value and the variance of xm + 1 (i) = max (0, ym (i)). The mean value is \u00b5m + 1 = Nym (i) > 0))))) = 1um \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "To converge or not to converge, that is the question.", "text": "To understand why this happens, I calculate the central moments of xm + 1 in relation to the central moments of xm when v2 = 1 / N. From (6) we have 2 m + \u00b5 2 m + 2 m. (26) Plugging results from (26) in (20) gives the recursion\u00b52m + 1 = 12\u03c0 (s2m + \u00b5 2 m) \u2248 0.16 (s2m + \u00b52m). (27) Similarly, plugging results from (26) and (27) in (22) gives a result of 0.34 (s2m + \u00b52m). (28) Simple manipulations of the equations (26 - 28) show that the neural deviation of (26) and (27) in (22) interdependent deviations of 0.34 (s2m + \u00b52m) yields the smaller deviation from the deviation (26 - 28)."}, {"heading": "5 Conclusion", "text": "In this paper, I have provided a general framework for weight initialization with nonlinear activation functions. First, I provide a general formula for the ideal weight initialization for all activation functions that are differentiable at 0. I will show how weight initializations for hyperbolic tangents and sigmoid activation functions change. Second, I will focus only on the Equilibrium Linear Unit (RELU) from the class of functions that are non-differentiable at 0, and I will provide rigorous proof for the He3It is surprising that the neural network converges with 22 layers! Initialization. Finally, I will show why the Xavier initialization does not work with the RELU activation function. Given the sharp increase in nonlinear activation functions over the years, a more general version of my (largely incomplete) analysis of nonlinear functions is warranted. My analysis repeatedly demonstrates the drastic difference in the dynamics of the system that can result from the introduction of nonlinear activation functions."}], "references": [{"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "The tanh transformation", "author": ["Michael D Godfrey"], "venue": "Information Systems Laboratory, Stanford University,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Cs 231n: Convolutional neural networks for visual recognition, lecture 5, slide", "author": ["Andrej Karpathy", "Justin Johnson", "Fei Fei Li"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Elements of large-sample theory", "author": ["Erich Leo Lehmann"], "venue": "Springer Science & Business Media,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": "In Proc. ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "All you need is a good init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": "arXiv preprint arXiv:1511.06422,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Random walk initialization for training very deep feedforward networks", "author": ["David Sussillo", "LF Abbott"], "venue": "arXiv preprint arXiv:1412.6558,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["Christian Szegedy", "Alexander Toshev", "Dumitru Erhan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["Alexander Toshev", "Christian Szegedy"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 187, "endOffset": 190}, {"referenceID": 13, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 192, "endOffset": 196}, {"referenceID": 10, "context": "It is well known [12] that arbitrary initializations can slow down or even completely stall the convergence process.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Weight initialization is an area of active research, and numerous methods ([12], [14], [16] to state a few) have been proposed to deal with the problem of the shrinking variance in the deeper layers.", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Weight initialization is an area of active research, and numerous methods ([12], [14], [16] to state a few) have been proposed to deal with the problem of the shrinking variance in the deeper layers.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "Weight initialization is an area of active research, and numerous methods ([12], [14], [16] to state a few) have been proposed to deal with the problem of the shrinking variance in the deeper layers.", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "The problem can be stated as follows: If the weights in a neural network are initialized using samples from a normal distribution, N (0, v), how should v be chosen to ensure that the variance of the outputs from the different layers are approximately the same? The first systematic analysis of this problem was conducted by Glorot and Bengio [3] who showed that for a linear activation function, the optimal value of v = 1/N , where N is the number of nodes feeding into that layer.", "startOffset": 342, "endOffset": 345}, {"referenceID": 4, "context": "In an important follow up paper, He and colleagues [6] argue that the Xavier initialization does not work well with the RELU activation function, and instead propose an initialization of v = 2/N (commonly referred to as the He initialization).", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "My main contributions in this paper are to (a) generalize the results of [3] to the case of non-linear activation functions and (b) to provide a continuum between the results of [3] and [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "My main contributions in this paper are to (a) generalize the results of [3] to the case of non-linear activation functions and (b) to provide a continuum between the results of [3] and [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "My main contributions in this paper are to (a) generalize the results of [3] to the case of non-linear activation functions and (b) to provide a continuum between the results of [3] and [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "As a small bonus, I resolve an unanswered question posed in [3] regarding the distributions of activations under the hyperbolic tangent activation.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Consistent with the assumptions in [3] and [6], I assume that the inputs to the first layer are independent and identically distributed random variables with mean 0 and variance 1.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Consistent with the assumptions in [3] and [6], I assume that the inputs to the first layer are independent and identically distributed random variables with mean 0 and variance 1.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "j=1 pij to converge to a normal distribution when N is large [10].", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "2 in [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "Sequential saturation with the hyperbolic tangent In their analysis of a neural network with the hyperbolic tangent activations, [3] find that the deeper layers in the neural network have a greater proportion of unsaturated nodes than the shallower layers.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "To explain their finding, I begin by noting that in [3], the authors initialize the weights using samples from a uniform distribution ( U [\u22121/ \u221a N, 1/ \u221a N ] ) having a variance of 1/3N .", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "Therefore, from (10) and (11), with g(0) = 0 and g\u2032(0) = 1, we have \u03bcm = 0 and 1In their calculations, [3] and [6] impose an additional set of constraints to ensure that the variance is maintained even during the backward pass.", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "Therefore, from (10) and (11), with g(0) = 0 and g\u2032(0) = 1, we have \u03bcm = 0 and 1In their calculations, [3] and [6] impose an additional set of constraints to ensure that the variance is maintained even during the backward pass.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Using results from [4], xm+1(i) = tanh(ym(i)) will have a probability density function (pdf) given by", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Plots of this pdf for different values of um (provided in Figure 1) produce trends similar to those observed by the simulation studies of [3] (figure 4 in their paper).", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "A comparison of Figure 1 and figure 4 of [3] suggests that um is a decreasing function of m, as is expected.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "From the results in [4], we expect the activations to be (a) approximately normally distributed when um is close to 0 and (b) bimodally distributed with local maximas near -1 and +1 when um is close to 1.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "For my experiments, I use the CIFAR 10 dataset [9] comprising 60,000 32 \u00d7 32 color images evenly split over 10 classes.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "or v \u2248 2/N, (25) which is consistent with that obtained by [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "In [6] paper, the authors provide an example of a 22 layer neural network using RELU activations which converges with the Xavier Initialization, and a 30 layer neural network which does not converge with the same initializations and activation functions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Figure 4: A comparison of the predicted means and standard errors obtained from (29 \u2013 30) with the simulated values reported in slide 61 of [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "51) = 3 \u00d7 10\u22123 times smaller than the variance to the 22 layer, and explains the possible reason why the 30 layer neural network described in [6] converges, but the 22 layer neural network does not.", "startOffset": 142, "endOffset": 145}], "year": 2017, "abstractText": "A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations. First, I derive a general weight initialization strategy for any neural network using activation functions differentiable at 0. Next, I derive the weight initialization strategy for the Rectified Linear Unit (RELU), and provide theoretical insights into why the Xavier initialization is a poor choice with RELU activations. My analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations.", "creator": "LaTeX with hyperref package"}}}