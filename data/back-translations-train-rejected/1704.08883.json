{"id": "1704.08883", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Traffic Light Control Using Deep Policy-Gradient and Value-Function Based Reinforcement Learning", "abstract": "Recent advances in combining deep neural network architectures with reinforcement learning techniques have shown promising potential results in solving complex control problems with high dimensional state and action spaces. Inspired by these successes, in this paper, we build two kinds of reinforcement learning algorithms: deep policy-gradient and value-function based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The policy-gradient based agent maps its observation directly to the control signal, however the value-function based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Our methods show promising results in a traffic network simulated in the SUMO traffic simulator, without suffering from instability issues during the training process.", "histories": [["v1", "Fri, 28 Apr 2017 11:44:42 GMT  (128kb,D)", "https://arxiv.org/abs/1704.08883v1", null], ["v2", "Sat, 27 May 2017 14:45:56 GMT  (128kb,D)", "http://arxiv.org/abs/1704.08883v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["seyed sajad mousavi", "michael schukat", "enda howley"], "accepted": false, "id": "1704.08883"}, "pdf": {"name": "1704.08883.pdf", "metadata": {"source": "CRF", "title": "Traffic Light Control Using Deep Policy-Gradient and Value-Function Based Reinforcement Learning", "authors": ["Seyed Sajad Mousavi", "Michael Schukat", "Enda Howley"], "emails": ["s.mousavi1@nuiglaway.ie", "michael.schukat@nuigalway.ie", "enda.howley@nuigalway.ie"], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Calculation Theory \u2192 Sequential Decision Making; Tags Traffic Control, Reinforcement learning, Deep learning, Policy Gradient Method, Value-Function Method, Artificial Neural Networks"}, {"heading": "1. INTRODUCTION", "text": "Dre eeisrVnlrsrteeeeeeteeteeteeteerrrrrlrlrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreteeteeteerrrrrrrrrrreteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrreteeteeteeteeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrreeeteeteeteeteeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeeeeteeeeeeeteeteeteeteeteeteeteeteeteeeteeeteeteeeeeeteeteeeteeteeteeeeteeeteeteeteeteeteeteeteeteeteeteeteeeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeee"}, {"heading": "2. RELATED WORK", "text": "Much research has been done in academic and industrial communities to build adaptive traffic light control systems, but significant research has also been conducted using enhanced traffic light learning methods, which have yielded promising results, but their simulation tests have not been mature enough to be comparable to more realistic situations, and the development of advanced traffic light simulation tools has led researchers to develop novel governmental representation and reward functions for enhanced learning algorithms that could address other aspects of the complexity and reality of real traffic problems [13, 1, 8, 3]. All of these experiments considered traffic light control as a fully observable Markov decision-making process (MDP) and explored whether the Q-Learning algorithm could be applied to it. Richter's study, however, formulated the traffic problem as a partially observable MDP (POMDP) and applied strategies to ensure local convergence in a partially observable environment."}, {"heading": "3. BACKGROUND", "text": "In this section, we will review the approaches of Reinforcement Learning (RL) and briefly describe how RL is applied to real-world problems where the number of states and policies is extremely high, so that regular Reinforcement Learning techniques cannot cope with them."}, {"heading": "3.1 Reinforcement Learning", "text": "As a rule, the problems to which RL techniques are applied are treated as Markov decision-making processes (MDPs). An MDP is defined as a quintuple function < S, A, T, R, \u03b3 >, where S is the number of states in the environmental state space, A is the amount of actions in the action space that the actor can use to interact with the environment, T is the transition function, which is the probability of movement between environmental states, R is the reward function, and Q [0, 1] is known as a discount factor that models the meaning of future and immediate rewards. At each time step, the actor perceives the state S and selects an action based on his observation."}, {"heading": "3.2 Deep learning and Deep Q-learning", "text": "Deep learning techniques are one of the best solutions to address high dimensional data and extract discriminative information from the data. Deep learning algorithms have the capability of automating feature extraction (the extraction of representations) from the data. Representation is learned through the data fed directly into deep networks without using human knowledge (i.e. automated feature extraction). Deep learning models has multiple layers of representations. In fact, it is a stack of building blocks such as auto-encoders, Restricted Boltzmann Machines (RBMs) and revolutionary layers. During the training, the raw data is fed into a network that consists of multiple layers, the results of each layer, which are nonlinear feature transformations, are used as inputs to the next layers of the deep neural network. The output representation of the last layer can be used to narrow down classifiers or those applications that have better efficiency and performance with an abstract representation of the data."}, {"heading": "3.3 Policy Gradient Methods", "text": "A method of policy gradient (PG) attempts to optimize a parameterized political function through the method of gradient gradient gradient. In fact, policy gradient methods are interested in searching the political space to learn policy actions directly, rather than estimating the functions of state value or action value. In contrast to traditional learning algorithms for amplification, PG methods do not suffer from convergence problems in estimating value functions within the framework of non-linear functional approximations or in environments that could be partially observable MDPs. They are also better able to deal with the complexity of continuous state and action spaces than purely value-oriented methods [35]. Policy gradient methods estimate policy gradients using Monte Carlo estimates of policy gradients [6]. These methods guarantee convergence to a local optimum of their parameterized policy functions. Typically, however, PG methods lead to a high variance in their gradient estimates of policy gradients, i.e. to reduce some of the gradient function."}, {"heading": "4. SYSTEM DESCRIPTION", "text": "In this section, we will formulate the problem of traffic light control as an enhanced learning task by describing the states, actions and reward functions. Then, we will introduce politics as a deep neural network and how the network can be trained."}, {"heading": "4.1 State Representation", "text": "We present the state of the system as an st-Rd image or as a snapshot of the current state of a graphical simulator (e.g. SUMO-GUI [18]), which is a vector of row pixel values of the current view of the intersection at each step of the simulation (as shown in Figure 1).This type of representation is like placing a camera on an intersection, which allows it to see the entire intersection. State representation in the traffic light control literature usually uses a vector, which represents the presence of a vehicle at the intersection, a Boolean vector, where a value of 1 indicates the presence of a vehicle and a value of 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector, which indicates the speed of the vehicle at the given intersection [14]. Regardless of these state representations, which use prior knowledge, they form assumptions that are not available to the real world."}, {"heading": "4.2 Action Set", "text": "In order to control traffic light phases, we define a number of possible measures A = {North / South-Green (NSG), East / West-Green (EEC)}. NSG allows vehicles to drive from north to south and vice versa, and also indicates that vehicles should stop on the east / west route and not drive through the intersection. EEC allows vehicles to drive from east to west and vice versa, and implies that vehicles should stop on the north / south route and not drive through the intersection. t, an agent for his strategy selects an action on the A at any time. Depending on the action selected, vehicles may cross the intersection in any lane."}, {"heading": "4.3 Reward Function", "text": "Typically, an immediate reward rt-R is a scalar value obtained by the agent after he has performed the chosen action in the vicinity at each time step. We set the reward as the difference between the total cumulative delays of two consecutive actions, i.e. rt = Dt-1 \u2212 Dt, (2) where Dt and Dt-1 are the total cumulative delays in the current and previous time steps. The total cumulative delay at the time t is the sum of the cumulative delay of all vehicles that appeared in the system from t = 0 to the current time step t. The positive reward values imply the actions taken that have led to a reduction in the cumulative delay, and the negative rewards imply an increase in the delay. With respect to the reward values, the agent may decide to change its policy in certain states of the system in the future."}, {"heading": "4.4 Agent\u2019s Policy", "text": "In the policy-based algorithm, politics is defined as a mapping of the input state to a probability distribution via Actions A. We use the deep neural network as a functional approximation and refer to its parameters \u03b8 as political parameters. The policy distribution \u03c0 (at | st; \u03b8) is learned by performing a gradient descent based on the political parameters. In the value-based algorithm, the deep neural network is used to estimate the Action Value Function. The Action Value Function maps the input state to action values, each representing the future reward that can be achieved for the given state and action. The optimal policy can then be extracted by implementing a greedy approach to selecting the best possible action."}, {"heading": "4.5 Objective Function and System Training", "text": "There are many measures such as maximizing data throughput, minimizing and balancing the queue, minimizing the algorithm Q = 17 (Deep Value-Function based reinforcement agent of traffic signal control with experience replay1: Initialize parameters, \u03b8 with random values 2: Initialize replay memory M with capacity L 3: for each simulation do 4: initialize s with current view of the intersection 5: repeat # each step in the simulation 6: choose action a correspondingly -giedy policy 7: take action a, observe reward r and next state s \u2032 8: store transition (s, r, s \u2032) in M 9: s \u2032 10: b random minbatch of transitions from the replay memory, M 11: for each transition (sj, aj, rj, s \u2032 j) in b do 12: if s \u2032 j is terminal 13: yi."}, {"heading": "5. EXPERIMENT AND RESULTS", "text": "In this section, we present the simulation environment in which our experiments were conducted, and then describe the details of the deep neural network used, including hyperparameters that represent the politics of the drug."}, {"heading": "5.1 Experiment Setup", "text": "We used the Simulation of Urban MObility (SUMO) tool [18] to simulate traffic in all experiments. SUMO is a well-known open source traffic simulator that provides useful application programming interfaces (APIs) and a graphical user interface (GUI) to model large road networks and some ways to handle them. In particular, we used SUMO-GUI v0.28.0. as it provides snapshots of each step of the simulation. The intersection geometry used in this study is shown in Figure 2. There are four incoming lanes to the intersection and four outgoing lanes from the intersection. To generate traffic requirements to the road network from different directions (i.e. from north to south and from west to east and vice versa), a uniform probability distribution with a probability of 0.1 was used."}, {"heading": "5.2 System Architecture and Hyperparameters", "text": "We took the snapshots from the SUMO GUI and did some basic pre-processing. The snapshots are converted from red-green-blue (RGB) representation to grayscale and enlarged to 128 \u00d7 128 frames. To enable our system to store a history of past observations, we stacked the last four frames of history and provided them to the system as input. So, the input to the network was a 128 \u00d7 128 \u00d7 4 image. We applied roughly the same architecture of the Deep Q Network (DQN) algorithm introduced by MnihTable 1: Comparison of the performance of the proposed methods against the SNN method Metric (\u00b5, \u03c3), based on the value function SNN Queue (Vehicles) (1.79, 0.073) (1.74, 0.10) Cumulative Delay (s) (11.25, 0.39)."}, {"heading": "5.3 Results and Discussion", "text": "In order to evaluate the performance of the proposed methods, we compared them with a base controller, a controller who provides an equal fixed time for each phase of the intersection. We performed the SUMO GUI simulator for the proposed model using the configuration method explained in Section 5.2 and compared the average reward, the average cumulative delay and the average queue reached to the baseline. Figure 3 shows the average reward received while the agent follows a particular policy. As shown in Figure 3, the proposed method performs significantly better than the baseline and performs more reward magnitudes by making more eras. This gradual increase in reward reflects the agent's ability to learn an optimal control policy in a stable manner. In contrast to the use of deep reinforcement of learning for estimating the Q values in the traffic light optimization problem [38], the proposed agent suffers from a cumulative of traffic delay problems implied by the length of the two most common traffic problems:"}, {"heading": "6. CONCLUSION", "text": "In this paper, we applied depth enhancement learning algorithms, focusing on both policy and value-based traffic signal control methods, to find optimal signaling control strategies by using raw visual data from the traffic simulator snapshots. Our approaches have yielded promising results and demonstrated that they could find more stable signaling strategies compared to previous work on using depth enhancement in traffic light optimization. In our work, we developed and tested the proposed methods in a small application, extending the work to more complex traffic simulations, for example by considering many intersections and multiple agents to control each intersection."}, {"heading": "Acknowledgments", "text": "We would like to thank FotoNation for enabling us to work with their GPU cluster."}], "references": [{"title": "Holonic multi-agent system for traffic signals control", "author": ["M. Abdoos", "N. Mozayani", "A.L. Bazzan"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Reinforcement learning for true adaptive traffic signal control", "author": ["B. Abdulhai", "R. Pringle", "G.J. Karakoulas"], "venue": "Journal of Transportation Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Reinforcement learning-based multi-agent system for network traffic signal control", "author": ["I. Arel", "C. Liu", "T. Urbanik", "A. Kohls"], "venue": "IET Intelligent Transport Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Urban traffic signal control using reinforcement learning agents", "author": ["P. Balaji", "X. German", "D. Srinivasan"], "venue": "IET Intelligent Transport Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Experiments with infinite-horizon, policy-gradient estimation", "author": ["J. Baxter", "P.L. Bartlett", "L. Weaver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Optimizing traffic lights in a cellular automaton model for city traffic", "author": ["E. Brockfeld", "R. Barlovic", "A. Schadschneider", "M. Schreckenberg"], "venue": "Physical Review E,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Q-learning based traffic optimization in management of signal timing plan", "author": ["Y.K. Chin", "N. Bolong", "A. Kiring", "S.S. Yang", "K.T.K. Teo"], "venue": "International Journal of Simulation, Systems, Science & Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Model-free reinforcement learning with continuous action in practice", "author": ["T. Degris", "P.M. Pilarski", "R.S. Sutton"], "venue": "In American Control Conference (ACC),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "author": ["L. Deng"], "venue": "APSIPA Transactions on Signal and Information Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "An autonomous network aware vm migration strategy in cloud data centres", "author": ["M. Duggan", "J. Duggan", "E. Howley", "E. Barrett"], "venue": "In Cloud and Autonomic Computing (ICCAC),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A reinforcement learning approach for dynamic selection of virtual machines in cloud data centres", "author": ["M. Duggan", "K. Flesk", "J. Duggan", "E. Howley", "E. Barrett"], "venue": "In Sixth International Conference on Innovating Computing Technology", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Multiagent reinforcement learning for integrated  network of adaptive traffic signal controllers (marlin-atsc): methodology and large-scale application on downtown toronto", "author": ["S. El-Tantawy", "B. Abdulhai", "H. Abdelgawad"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Using a deep reinforcement learning agent for traffic signal control", "author": ["W. Genders", "S. Razavi"], "venue": "arXiv preprint arXiv:1611.01142,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Enhancing nash q-learning and team q-learning mechanisms by using bottlenecks", "author": ["B. Ghazanfari", "N. Mozayani"], "venue": "Journal of Intelligent & Fuzzy Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Extracting bottlenecks for reinforcement learning agent by holonic concept clustering and attentional functions", "author": ["B. Ghazanfari", "N. Mozayani"], "venue": "Expert Systems with Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Recent development and applications of sumo-simulation of urban mobility", "author": ["D. Krajzewicz", "J. Erdmann", "M. Behrisch", "L. Bieker"], "venue": "International Journal On Advances in Systems and Measurements,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep auto-encoder neural networks in reinforcement learning", "author": ["S. Lange", "M. Riedmiller"], "venue": "In Neural Networks (IJCNN), The 2010 International Joint Conference on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Traffic signal timing via deep reinforcement learning", "author": ["L. Li", "Y. Lv", "F.-Y. Wang"], "venue": "IEEE/CAA Journal of Automatica Sinica,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Parallel systems for traffic control: A rethinking", "author": ["L. Li", "D. Wen"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "A survey of traffic control with vehicular communications", "author": ["L. Li", "D. Wen", "D. Yao"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L.-J. Lin"], "venue": "PhD thesis, Fujitsu Laboratories Ltd,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1993}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Automatic abstraction controller in reinforcement learning agent via automata", "author": ["S.S. Mousavi", "B. Ghazanfari", "N. Mozayani", "M.R. Jahed-Motlagh"], "venue": "Applied Soft Computing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Deep reinforcement learning: An overview", "author": ["S.S. Mousavi", "M. Schukat", "E. Howley"], "venue": "Intelligent Systems Conference,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Reinforcement learning with function approximation for traffic signal control", "author": ["L. Prashanth", "S. Bhatnagar"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Traffic light scheduling using policy-gradient reinforcement learning", "author": ["S. Ritcher"], "venue": "In The International Conference on Automated Planning and Scheduling.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["J. Schulman", "N. Heess", "T. Weber", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Traffic light control using sarsa with three state representations", "author": ["T.L. Thorpe", "C.W. Anderson"], "venue": "Technical report, Citeseer,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1996}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Coordinated deep reinforcement learners for traffic light control", "author": ["E. Van der Pol", "F.A. Oliehoek"], "venue": "In NIPS\u201916 Workshop on Learning, Inference and Control of Multi-Agent Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Multi-agent reinforcement learning for traffic light control", "author": ["M. Wiering"], "venue": "In ICML,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2000}, {"title": "Recurrent policy gradients", "author": ["D. Wierstra", "A. F\u00f6rster", "J. Peters", "J. Schmidhuber"], "venue": "Logic Journal of IGPL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1992}, {"title": "Reinforcement learning algorithms with function approximation: Recent advances and applications", "author": ["X. Xu", "L. Zuo", "Z. Huang"], "venue": "Information Sciences,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 3, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 0, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 21, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 12, "context": "traffic demand, queue length of vehicles in each lane of the intersection and traffic flow fluctuation) [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "In this study, we are interested in the third approach and aim to propose two novel methods for traffic signal control by leveraging recent advances in machine learning and artificial intelligence fields [26, 27].", "startOffset": 204, "endOffset": 212}, {"referenceID": 26, "context": "In this study, we are interested in the third approach and aim to propose two novel methods for traffic signal control by leveraging recent advances in machine learning and artificial intelligence fields [26, 27].", "startOffset": 204, "endOffset": 212}, {"referenceID": 33, "context": "Reinforcement learning [34] as a machine learning technique for traffic signal control problem has led to impressive results [4, 30] and has shown a promising potential solver.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Reinforcement learning [34] as a machine learning technique for traffic signal control problem has led to impressive results [4, 30] and has shown a promising potential solver.", "startOffset": 125, "endOffset": 132}, {"referenceID": 29, "context": "Reinforcement learning [34] as a machine learning technique for traffic signal control problem has led to impressive results [4, 30] and has shown a promising potential solver.", "startOffset": 125, "endOffset": 132}, {"referenceID": 11, "context": "Aside from traffic control, reinforcement learning has been applied to a number of real world problems such as cloud computing [12, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 10, "context": "Aside from traffic control, reinforcement learning has been applied to a number of real world problems such as cloud computing [12, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 26, "context": "Recently, deep learning has gained huge attraction and has been successfully combined with reinforcement learning techniques to deal with complex optimization problems such as playing Atari 2600 games [27], Computer Go program [33], etc.", "startOffset": 201, "endOffset": 205}, {"referenceID": 32, "context": "Recently, deep learning has gained huge attraction and has been successfully combined with reinforcement learning techniques to deal with complex optimization problems such as playing Atari 2600 games [27], Computer Go program [33], etc.", "startOffset": 227, "endOffset": 231}, {"referenceID": 19, "context": "a convolutional neural network [20]) trained by reinforcement learning techniques to predict the next possible optimal action(s).", "startOffset": 31, "endOffset": 35}, {"referenceID": 37, "context": "learning in the traffic signal control problem [38, 14], in this research the state representation is different.", "startOffset": 47, "endOffset": 55}, {"referenceID": 13, "context": "learning in the traffic signal control problem [38, 14], in this research the state representation is different.", "startOffset": 47, "endOffset": 55}, {"referenceID": 38, "context": "In particular, significant research has been conducted employing reinforcement learning methods in the area of traffic light signal control [39, 2, 7].", "startOffset": 140, "endOffset": 150}, {"referenceID": 1, "context": "In particular, significant research has been conducted employing reinforcement learning methods in the area of traffic light signal control [39, 2, 7].", "startOffset": 140, "endOffset": 150}, {"referenceID": 6, "context": "In particular, significant research has been conducted employing reinforcement learning methods in the area of traffic light signal control [39, 2, 7].", "startOffset": 140, "endOffset": 150}, {"referenceID": 12, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 0, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 7, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 2, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 30, "context": "However, Richter\u2019s study formulated the traffic problem as a partially observable MDP (POMDP) and applied policy gradient methods to guarantee local convergence under a partial observable environment [31].", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "By utilizing advances in deep learning and its application to different domains [10, 11], deep learning has gained attention in the area of traffic management systems.", "startOffset": 80, "endOffset": 88}, {"referenceID": 10, "context": "By utilizing advances in deep learning and its application to different domains [10, 11], deep learning has gained attention in the area of traffic management systems.", "startOffset": 80, "endOffset": 88}, {"referenceID": 20, "context": "Previous research has used deep stacked autoencoders (SAE) neural networks to estimate Q-values, where each Q-value is corresponding to each available signal phase [21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 37, "context": "Two recent studies by [38, 14] provided deep reinforcement learning agents that used deep Q-netwok [27] to map from given states to Q-values.", "startOffset": 22, "endOffset": 30}, {"referenceID": 13, "context": "Two recent studies by [38, 14] provided deep reinforcement learning agents that used deep Q-netwok [27] to map from given states to Q-values.", "startOffset": 22, "endOffset": 30}, {"referenceID": 26, "context": "Two recent studies by [38, 14] provided deep reinforcement learning agents that used deep Q-netwok [27] to map from given states to Q-values.", "startOffset": 99, "endOffset": 103}, {"referenceID": 33, "context": "A common reinforcement learning [34] setting is shown in Figure 1 where an RL agent interacts with an environment.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "A MDP is defined as a five-tuple < S,A, T,R, \u03b3 > , where S is the set of states in the state space of the environment, A is the set of actions in the action space that the agent can use in order to interact with the environment, T is the transition function, which is the probability of moving between the environment states, R is the reward function and \u03b3 \u2208 [0, 1] is known as the discount factor, which models the importance of the future and immediate rewards.", "startOffset": 359, "endOffset": 365}, {"referenceID": 0, "context": "The goal of the learning agent in reinforcement learning framework is to learn an optimal policy \u03c0 : S \u00d7 A \u2192 [0, 1] which defines the probability of selecting action at in state st, so that with following the underlying policy the expected cumulative discounted reward over time is maximized.", "startOffset": 109, "endOffset": 115}, {"referenceID": 41, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 157, "endOffset": 168}, {"referenceID": 14, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 157, "endOffset": 168}, {"referenceID": 27, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 157, "endOffset": 168}, {"referenceID": 15, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 185, "endOffset": 189}, {"referenceID": 18, "context": "have also been commonly used as function approximators for large reinforcement learning tasks [19, 26].", "startOffset": 94, "endOffset": 102}, {"referenceID": 25, "context": "have also been commonly used as function approximators for large reinforcement learning tasks [19, 26].", "startOffset": 94, "endOffset": 102}, {"referenceID": 28, "context": "The interested readers are referred to [29] for a review of using deep neural networks", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "A deep Q-learning Network (DQN) [26] uses this benefit of deep learning in order to represent the agent\u2019s observation as an abstract representation in learning an optimal control policy.", "startOffset": 32, "endOffset": 36}, {"referenceID": 36, "context": "Applying non-linear function approximators such as neural networks with modelfree reinforcement learning algorithms in high-dimensional continuous state and action spaces, has some convergence problems [37].", "startOffset": 202, "endOffset": 206}, {"referenceID": 23, "context": "For the problem of correlated states, DQN uses the previously proposed experience replay approach [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 34, "context": "This approach has demonstrated better convergence properties in some RL problems [35].", "startOffset": 81, "endOffset": 85}, {"referenceID": 34, "context": "They can also deal with the complexity of continuous state and action spaces better than purely value-based methods [35].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Policy gradient methods estimate policy gradients using Monte Carlo estimates of the policy gradients [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 31, "context": "The baseline function can be calculated in different manners [32, 40].", "startOffset": 61, "endOffset": 69}, {"referenceID": 39, "context": "The baseline function can be calculated in different manners [32, 40].", "startOffset": 61, "endOffset": 69}, {"referenceID": 17, "context": "SUMO-GUI [18]) which is a vector of row pixel values of current view of the intersection at each step of simulation (as shown in Figure 1).", "startOffset": 9, "endOffset": 13}, {"referenceID": 37, "context": "The state representation in the traffic light control literature usually uses a vector representing the presence of a vehicle at the intersection, a Boolean-valued vector where a value 1 indicates the presence of a vehicle and a value 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector indicating the vehicle\u2019s speed at the given intersection [14].", "startOffset": 272, "endOffset": 280}, {"referenceID": 35, "context": "The state representation in the traffic light control literature usually uses a vector representing the presence of a vehicle at the intersection, a Boolean-valued vector where a value 1 indicates the presence of a vehicle and a value 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector indicating the vehicle\u2019s speed at the given intersection [14].", "startOffset": 272, "endOffset": 280}, {"referenceID": 13, "context": "The state representation in the traffic light control literature usually uses a vector representing the presence of a vehicle at the intersection, a Boolean-valued vector where a value 1 indicates the presence of a vehicle and a value 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector indicating the vehicle\u2019s speed at the given intersection [14].", "startOffset": 399, "endOffset": 403}, {"referenceID": 40, "context": "This equation (8) is standard learning rule of the REINFORCE algorithm [41].", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "With regard to the advantage actor-critic method [25], computing a single update is done by selecting actions using the underlying policy for up to M steps or till a terminal state is met.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "this process is an actor-critic algorithm, the policy \u03c0(at|st; \u03b8) refers to the actor and the estimate of the state value function V \u03c0\u03b8v (st) implies to the critic [9, 25].", "startOffset": 164, "endOffset": 171}, {"referenceID": 24, "context": "this process is an actor-critic algorithm, the policy \u03c0(at|st; \u03b8) refers to the actor and the estimate of the state value function V \u03c0\u03b8v (st) implies to the critic [9, 25].", "startOffset": 164, "endOffset": 171}, {"referenceID": 17, "context": "We have used the Simulation of Urban MObility (SUMO) [18] tool to simulate traffic in all experiments.", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "[26, 27].", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[26, 27].", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "99 and all weights of the network were updated by the Adam optimizer [17] with a learning rate \u03b1 = 0.", "startOffset": 69, "endOffset": 73}, {"referenceID": 37, "context": "Unlike using deep reinforcement learning for estimating the Q-values in traffic light optimisation problem [38], the proposed agent doesn\u2019t suffer stability issues.", "startOffset": 107, "endOffset": 111}], "year": 2017, "abstractText": "Recent advances in combining deep neural network architectures with reinforcement learning techniques have shown promising potential results in solving complex control problems with high dimensional state and action spaces. Inspired by these successes, in this paper, we build two kinds of reinforcement learning algorithms: deep policy-gradient and value-function based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The policy-gradient based agent maps its observation directly to the control signal, however the value-function based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Our methods show promising results in a traffic network simulated in the SUMO traffic simulator, without suffering from instability issues during the training process.", "creator": "LaTeX with hyperref package"}}}