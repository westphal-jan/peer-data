{"id": "1704.08966", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Not All Dialogues are Created Equal: Instance Weighting for Neural Conversational Models", "abstract": "Neural conversational models require substantial amounts of dialogue data for their parameter estimation and are therefore usually learned on large corpora such as chat forums or movie subtitles. These corpora are, however, often challenging to work with, notably due to their frequent lack of turn segmentation and the presence of multiple references external to the dialogue itself. This paper shows that these challenges can be mitigated by adding a weighting model into the architecture. The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling. At training time, these sample weights are included into the empirical loss to be minimised. Evaluation results on retrieval-based models trained on movie and TV subtitles demonstrate that the inclusion of such a weighting model improves the model performance on unsupervised metrics.", "histories": [["v1", "Fri, 28 Apr 2017 14:57:29 GMT  (201kb,D)", "https://arxiv.org/abs/1704.08966v1", null], ["v2", "Sat, 15 Jul 2017 17:27:13 GMT  (198kb,D)", "http://arxiv.org/abs/1704.08966v2", "Accepted to SIGDIAL 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["pierre lison", "serge bibauw"], "accepted": false, "id": "1704.08966"}, "pdf": {"name": "1704.08966.pdf", "metadata": {"source": "CRF", "title": "Not All Dialogues are Created Equal: Instance Weighting for Neural Conversational Models", "authors": ["Pierre Lison", "Serge Bibauw"], "emails": ["plison@nr.no", "serge.bibauw@kuleuven.be"], "sections": [{"heading": null, "text": "Neural conversation models require considerable amounts of dialogue data to estimate their parameters, and are therefore typically learned in large corpora such as chat forums, Twitter discussions, or movie subtitles. However, these corpora are often difficult to work on, especially due to their frequent lack of rotation segmentation and the presence of multiple references outside the dialogue itself. This paper shows that these challenges can be mitigated by adding a weighting model to the neural architecture. The weighting model, which is itself estimated from dialogue data, associates each training example with a numerical weight that reflects its intrinsic quality for dialogue modeling. At training time, these sample weights are included in the empirical loss to be minimized."}, {"heading": "1 Introduction", "text": "The development of interlocutors (such as mobile assistants, chatbots or interactive robots) is increasingly based on data-driven methods aimed at deriving conversation patterns from dialogue data. A major trend in recent years has been the emergence of neural conversation models (Vinyals and Le, 2015; Shang et al., 2016; Lowe et al., 2017; Li et al., 2017), which can be directly associated with Universidad Central del Ecuador (Quito, Ecuador)."}, {"heading": "2 Related Work", "text": "In recent years, it has become clear that these two models are not only a purely counter-model, but also a model in which the different approaches to dialogue can be compared with each other. (Levin et al., 2000; Rieser and Lemon, 2011; Young et al., 2013) One advantage of these neural models is that they can be estimated from a raw dialogue corporatism without having to rely on additional layers of annotation for intermedia representations, such as state variables or dialogue activities. Rather, they automatically result in latent representations of the dialogue state based on the observed statements. Neural conversation models can be divided into two main categories, retrieval models and generative models. Retrieval models are used to select the relevant answers for a particular context (possibly large)."}, {"heading": "3 Approach", "text": "As mentioned in the introduction, the interactions extracted from large dialog corpora do not all have the same intrinsic quality, due, for example, to the frequent absence of spin segmentation or the presence of external, insoluble references to person names. In other words, there is a discrepancy between the actual < context, response pairs in these corpora, and the conversation patterns that should be taken into account in the neural model. In this case, one way to address this discrepancy is to define the problem as domain matching, where the initial domain is the original dialog corpus and the target domain that represents the dialogues we want to produce. In this case, the target domain is not necessarily another dialog domain, but merely reflects the fact that the distribution of responses in the raw corpus does not necessarily reflect the distribution of responses that we ultimately want to code in the conversation model."}, {"heading": "3.1 Weighting model", "text": "The quality of a particular < context, response > pair is difficult to determine using handcrafted rules - for example, the likelihood of a tie may depend on several factors, such as the presence of pull curves or the time span between utterances (Lison and Meena, 2016). To overcome these limitations, we take a data-driven approach and automatically learn a weight model from examples of \"high-quality\" responses. What constitutes a high-quality response in practice depends on the specific criteria we want to maintain in the conversation model - for example, by favoring responses that are likely to form a new dialogue curve (rather than a continuation of the current curve) by avoiding the use of boring, everyday responses or selecting answers that contain unresolved references to person names. The weighting model can be expressed as a neural model linking each < context, reaction > example curve to a numerical weight."}, {"heading": "3.2 Instance weighting", "text": "Once the weighting model is estimated, the next step is to run it on the entire dialog corpus to calculate the expected weight of each < context, response > pair. These sample weights are then included in the empirical loss minimized during training. Formally, the estimate of model parameters is expressed as a minimization problem: {(c1, r1), (c2, r2),... (cn, rn)} with associated weights {w1,... wn}. On-call models, this minimization is expressed as a minimization problem, where L is a loss function (for example, transverse entropy loss), and yi is either set to 1 if ri is the answer to ci, and 0 otherwise (if ri is a negative example). On-call models, the minimization is similarly expressed as a greater loss (for example, transverse entropy loss), and yi is set to 1 if ri is an answer and ri is an answer (for example)."}, {"heading": "4 Evaluation", "text": "The approach will be evaluated on the basis of on-demand neural models trained using English subtitles by Lison and Tiedemann (2016). Three alternative models will be evaluated: 1. A traditional TF-IDF model, 2. A dual encoder model trained directly on the corpus examples, 3. A dual encoder model in combination with the weighting model from Section 3.1."}, {"heading": "4.1 Models", "text": "Dre rf\u00fc eeisrcnlhsrteeaeVnlhsrtee\u00fccnh hacu ide eeisrcnlhsrtee\u00fccnlhsrtee\u00fccsrf\u00fc ide eeirlrrcehnlrcehnc\u00fcehncS ni rde eeirlrcehncnlhSe ni ende eeirlrf\u00fc for the eeirlrlrteeVnlrlrrtee\u00fccehnc\u00fc nlrrrf\u00fc eeirlrrteeeeVnlrf\u00fc ide eeirf\u00fc ide eeirlrlrlrlrteeVnlrlrlrtee\u00fccehcehncehnc\u00fc nlrrrrrrlf\u00fc eeeeVnlrlrlrlrlrteeeVnlf\u00fc the eeirf\u00fc ide eeirlrlrlrlrlrlrlrlrlrteeVnlrteeVnlrteeD"}, {"heading": "4.2 Datasets", "text": "< < < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "4.2.1 Experimental design Preprocessing", "text": "The statements from all records were accompanied by the indication that the names of people mentioned in films and plays typically refer to fictional characters, so that they are limited to one unique day per unit. The couple: Dana: Frank, do you think you could give me a hand with those bags? Frank: I'm not a bouncer, I'm a builder. I'm a builder."}, {"heading": "4.3 Results", "text": "The three models (the TF-IDF model, the Baseline Dual Encoder, and the Dual Encoder combined with the weighting model) are evaluated using the Recallm @ i metric, which is the most common metric for evaluating retrieval models. Let's list the m context response pairs from the test set. For each context, we create a series of alternative responses, one of which is the actual answer ri, and they \u2212 1 other responses are randomly sampled from the same corpus.The m alternative responses are then evaluated using the results from the conversation model, and the Recallm @ imeasures determines how often the correct answer appears in the top i results of this ranking. The Recallm @ i metric is often used to evaluate retrieval models, as multiple responses can be equally \"correct.\" Experimental results are presented in Table 1."}, {"heading": "4.4 Human evaluation", "text": "To further investigate the potential of this weighting strategy for neural conversation models, we performed a human evaluation of the responses generated by the two neural models included in the evaluation = 97. We collected human judgments on < context, response > pairs using a crowdsourced platform. We extracted 115 random contexts from the Corpus Cornell Movie Dialog and used four different strategies to generate dialogue responses: a random predictor (used to identify the lower limit), the two dual encoder models (both without and with instance weights) and expert responses (used to identify the upper limit).The expert responses were manually written by two human commentators, resulting in 460 < context, response > pairs were each rated by 8 different human judges (920 ratings per model).The human judges were asked to rate the consistency between context and overall response on a crowd-consistent scale of 5-118."}, {"heading": "5 Discussion", "text": "The limitations of neural conversation models, which are based on large, noisy corporations of dialogue such as film and TV subtitles, have been discussed in several papers; some of the issues raised in previous papers are the absence of segmentation in subtitling (Vinyals and Le, 2015; Serban and Pineau, 2015), the lack of long-term consistency and \"personality\" in the responses generated (Li et al., 2016b), and the ubiquity of generative models in education (Li et al., 2016a). To gain the best knowledge, this paper is the first to undertake instance weighting to address some of these limitations; a similar approach is described in relation to slow, common responses to generative models."}, {"heading": "6 Conclusion", "text": "Dialogue corpora, such as chat logs or movie subtitles, are very useful resources for developing conversation models in the open end domain, but they also pose a number of challenges to conversation modeling. Two notable challenges are the lack of segmentation in dialogue turns (at least for movie subtitles) and the presence of an external context that is not captured in the dialogue transcripts themselves (which leads to the mention of personal names and insoluble named units).This paper showed how these challenges can be mitigated by using a weighting model applied to the training examples. This weighting model can be estimated in a data-driven way by providing examples of \"high-quality\" training pairs together with random pairs from the same corpus.The criteria that determine how these training pairs are selected in practice depend on the type of conversation model that one wants to learn, this input weight set can be considered in this domain's original form."}], "references": [{"title": "Discriminative learning for differing training and test distributions", "author": ["Steffen Bickel", "Michael Br\u00fcckner", "Tobias Scheffer."], "venue": "Proceedings of the 24th International Conference on Machine Learning. ACM, New York, NY, USA, ICML \u201907, pages", "citeRegEx": "Bickel et al\\.,? 2007", "shortCiteRegEx": "Bickel et al\\.", "year": 2007}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "SimpleDS: A simple deep reinforcement learning dialogue system", "author": ["Heriberto Cuay\u00e1huitl."], "venue": "Kristiina Jokinen and Graham Wilcock, editors, Dialogues with Social Robots: Enablements, Analyses, and Evaluation, Springer, Singapore, Lecture Notes", "citeRegEx": "Cuay\u00e1huitl.,? 2017", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2017}, {"title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs", "author": ["Cristian Danescu-Niculescu-Mizil", "Lillian Lee."], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational", "citeRegEx": "Danescu.Niculescu.Mizil and Lee.,? 2011", "shortCiteRegEx": "Danescu.Niculescu.Mizil and Lee.", "year": 2011}, {"title": "Discriminative instance weighting for domain adaptation in statistical machine translation", "author": ["George Foster", "Cyril Goutte", "Roland Kuhn."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Foster et al\\.,? 2010", "shortCiteRegEx": "Foster et al\\.", "year": 2010}, {"title": "A knowledge-grounded neural conversation model", "author": ["Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley."], "venue": "CoRR abs/1702.01932.", "citeRegEx": "Ghazvininejad et al\\.,? 2017", "shortCiteRegEx": "Ghazvininejad et al\\.", "year": 2017}, {"title": "Instance weighting for domain adaptation in NLP", "author": ["Jing Jiang", "Chengxiang Zhai."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics, Prague, Czech Republic,", "citeRegEx": "Jiang and Zhai.,? 2007", "shortCiteRegEx": "Jiang and Zhai.", "year": 2007}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["E. Levin", "R. Pieraccini", "W. Eckert."], "venue": "IEEE Transactions on Speech and Audio Processing 8(1):11\u201323.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Compu-", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios Spithourakis", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky."], "venue": "CoRR abs/1606.01541.", "citeRegEx": "Li et al\\.,? 2016c", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky."], "venue": "CoRR abs/1701.06547.", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Automatic turn segmentation of movie & TV subtitles", "author": ["Pierre Lison", "Raveesh Meena."], "venue": "Proceedings of the 2016 Spoken Language Technology Workshop. IEEE, San Diego, CA, USA, pages 245\u2013 252.", "citeRegEx": "Lison and Meena.,? 2016", "shortCiteRegEx": "Lison and Meena.", "year": 2016}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and TV subtitles", "author": ["Pierre Lison", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016).", "citeRegEx": "Lison and Tiedemann.,? 2016", "shortCiteRegEx": "Lison and Tiedemann.", "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "Proceedings of the 16th Annual Meeting on Discourse and Dialogue (SIGDIAL", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Training end-to-end dialogue systems with the Ubuntu Dialogue Corpus", "author": ["Ryan Lowe", "Nissan Pow", "Iulian V. Serban", "Laurent Charlin", "Chia-Wei Liu", "Joelle Pineau."], "venue": "Dialogue & Discourse 8(1):31\u201365.", "citeRegEx": "Lowe et al\\.,? 2017", "shortCiteRegEx": "Lowe et al\\.", "year": 2017}, {"title": "Emulating human conversations using convolutional neural network-based IR", "author": ["Abhay Prakash", "Chris Brockett", "Puneet Agrawal."], "venue": "CoRR abs/1606.07056.", "citeRegEx": "Prakash et al\\.,? 2016", "shortCiteRegEx": "Prakash et al\\.", "year": 2016}, {"title": "Using TF-IDF to Determine Word Relevance in Document Queries", "author": ["Juan Ramos."], "venue": "Proceedings of the First Instructional Conference on Machine Learning. Rutgers University, New Brunswick, NJ, USA.", "citeRegEx": "Ramos.,? 2003", "shortCiteRegEx": "Ramos.", "year": 2003}, {"title": "Reinforcement Learning for Adaptive Dialogue Systems", "author": ["V. Rieser", "O. Lemon."], "venue": "Springer, Berlin, Heidelberg.", "citeRegEx": "Rieser and Lemon.,? 2011", "shortCiteRegEx": "Rieser and Lemon.", "year": 2011}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL. Association for Computational Linguistics,", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Text-based speaker identification for multi-participant opendomain dialogue systems", "author": ["Iulian V Serban", "Joelle Pineau."], "venue": "NIPS Workshop on Machine Learning for Spoken Language Understanding. Montreal, Canada.", "citeRegEx": "Serban and Pineau.,? 2015", "shortCiteRegEx": "Serban and Pineau.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelli-", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceed-", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "A Neural Conversational Model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "CoRR abs/1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "author": ["Jason D. Williams", "Kavosh Asadi", "Geoffrey Zweig."], "venue": "CoRR abs/1702.03274.", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "Instance selection and instance weighting for cross-domain sentiment classification via pu learning", "author": ["Rui Xia", "Xuelei Hu", "Jianfeng Lu", "Jian Yang", "Chengqing Zong."], "venue": "Proceedings of the Twenty-Third International Joint Conference on Ar-", "citeRegEx": "Xia et al\\.,? 2013", "shortCiteRegEx": "Xia et al\\.", "year": 2013}, {"title": "An attentional neural conversation model with improved specificity", "author": ["Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Kam-Fai Wong."], "venue": "CoRR abs/1606.01292.", "citeRegEx": "Yao et al\\.,? 2016", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gai", "B. Thomson", "J.D. Williams."], "venue": "Proceedings of the IEEE 101(5):1160\u20131179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Neural personalized response generation as domain adaptation", "author": ["Weinan Zhang", "Ting Liu", "Yifa Wang", "Qingfu Zhu."], "venue": "CoRR abs/1701.02073.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 24, "context": "One major trend in the last recent years is the emergence of neural conversation models (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2016; Lowe et al., 2017; Li et al., 2017).", "startOffset": 88, "endOffset": 209}, {"referenceID": 23, "context": "One major trend in the last recent years is the emergence of neural conversation models (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2016; Lowe et al., 2017; Li et al., 2017).", "startOffset": 88, "endOffset": 209}, {"referenceID": 22, "context": "One major trend in the last recent years is the emergence of neural conversation models (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2016; Lowe et al., 2017; Li et al., 2017).", "startOffset": 88, "endOffset": 209}, {"referenceID": 21, "context": "One major trend in the last recent years is the emergence of neural conversation models (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2016; Lowe et al., 2017; Li et al., 2017).", "startOffset": 88, "endOffset": 209}, {"referenceID": 15, "context": "One major trend in the last recent years is the emergence of neural conversation models (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2016; Lowe et al., 2017; Li et al., 2017).", "startOffset": 88, "endOffset": 209}, {"referenceID": 11, "context": "One major trend in the last recent years is the emergence of neural conversation models (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2016; Lowe et al., 2017; Li et al., 2017).", "startOffset": 88, "endOffset": 209}, {"referenceID": 19, "context": "They are therefore often trained on conversations collected from various online resources, such as Twitter discussions (Ritter et al., 2010) online chat logs (Lowe et al.", "startOffset": 119, "endOffset": 140}, {"referenceID": 15, "context": ", 2010) online chat logs (Lowe et al., 2017), movie scripts (DanescuNiculescu-Mizil and Lee, 2011) and movie and TV subtitles (Lison and Tiedemann, 2016).", "startOffset": 25, "endOffset": 44}, {"referenceID": 13, "context": ", 2017), movie scripts (DanescuNiculescu-Mizil and Lee, 2011) and movie and TV subtitles (Lison and Tiedemann, 2016).", "startOffset": 89, "endOffset": 116}, {"referenceID": 20, "context": "First of all, several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification (Serban and Pineau, 2015; Lison and Meena, 2016).", "startOffset": 157, "endOffset": 205}, {"referenceID": 12, "context": "First of all, several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification (Serban and Pineau, 2015; Lison and Meena, 2016).", "startOffset": 157, "endOffset": 205}, {"referenceID": 8, "context": ") that can be used in most conversational situations but fall short of creating meaningful and engaging conversations with human users (Li et al., 2016a).", "startOffset": 135, "endOffset": 153}, {"referenceID": 7, "context": "Compared to previous statistical approaches to dialogue modelling based on Markov processes (Levin et al., 2000; Rieser and Lemon, 2011; Young et al., 2013), one benefit of these neural models is their ability to be estimated from raw dialogue corpora, without having to rely on additional annotation layers for intermediate representations such as state variables or dialogue acts.", "startOffset": 92, "endOffset": 156}, {"referenceID": 18, "context": "Compared to previous statistical approaches to dialogue modelling based on Markov processes (Levin et al., 2000; Rieser and Lemon, 2011; Young et al., 2013), one benefit of these neural models is their ability to be estimated from raw dialogue corpora, without having to rely on additional annotation layers for intermediate representations such as state variables or dialogue acts.", "startOffset": 92, "endOffset": 156}, {"referenceID": 28, "context": "Compared to previous statistical approaches to dialogue modelling based on Markov processes (Levin et al., 2000; Rieser and Lemon, 2011; Young et al., 2013), one benefit of these neural models is their ability to be estimated from raw dialogue corpora, without having to rely on additional annotation layers for intermediate representations such as state variables or dialogue acts.", "startOffset": 92, "endOffset": 156}, {"referenceID": 14, "context": "Retrieval models are used to select the most relevant response for a given context amongst a (possibly large) set of predefined responses, such as the set of utterances extracted from a corpus (Lowe et al., 2015; Prakash et al., 2016).", "startOffset": 193, "endOffset": 234}, {"referenceID": 16, "context": "Retrieval models are used to select the most relevant response for a given context amongst a (possibly large) set of predefined responses, such as the set of utterances extracted from a corpus (Lowe et al., 2015; Prakash et al., 2016).", "startOffset": 193, "endOffset": 234}, {"referenceID": 23, "context": "Generative models, on the other hand, rely on sequence-to-sequence models (Sordoni et al., 2015) to generate new, possibly unseen responses given the provided context.", "startOffset": 74, "endOffset": 96}, {"referenceID": 24, "context": "These models are built by linking together two recurrent architectures: one encoder which maps the sequence of input tokens in the context utterance(s) to a fixedsized vector, and one decoder that generates the response token by token given the context vector (Vinyals and Le, 2015; Sordoni et al., 2015).", "startOffset": 260, "endOffset": 304}, {"referenceID": 23, "context": "These models are built by linking together two recurrent architectures: one encoder which maps the sequence of input tokens in the context utterance(s) to a fixedsized vector, and one decoder that generates the response token by token given the context vector (Vinyals and Le, 2015; Sordoni et al., 2015).", "startOffset": 260, "endOffset": 304}, {"referenceID": 27, "context": "Recent papers have shown that the performance of these generative models can be improved by incorporating attentional mechanisms (Yao et al., 2016) and accounting for the structure of conversations through hierarchical networks (Serban et al.", "startOffset": 129, "endOffset": 147}, {"referenceID": 21, "context": ", 2016) and accounting for the structure of conversations through hierarchical networks (Serban et al., 2016).", "startOffset": 88, "endOffset": 109}, {"referenceID": 11, "context": "Neural conversation models can also be learned using adversarial learning (Li et al., 2017).", "startOffset": 74, "endOffset": 91}, {"referenceID": 9, "context": "The linguistic coherence and diversity of the models can be enhanced by including speakeraddressee information (Li et al., 2016b) and by expressing the objective function in terms of Maximum Mutual Information to enhance the diversity of the generated responses (Li et al.", "startOffset": 111, "endOffset": 129}, {"referenceID": 8, "context": ", 2016b) and by expressing the objective function in terms of Maximum Mutual Information to enhance the diversity of the generated responses (Li et al., 2016a).", "startOffset": 141, "endOffset": 159}, {"referenceID": 5, "context": "As demonstrated by (Ghazvininejad et al., 2017), neural conversation models can also be combined with external knowledge sources in the form of factual information or entity-grounded opinions, which is an important requirement for developing task-oriented dialogue systems that must ground their action in an external context.", "startOffset": 19, "endOffset": 47}, {"referenceID": 7, "context": "Dialogue is a sequential decision-making process where the conversational actions of each participant influence not only the current turn but the long-term evolution of the dialogue (Levin et al., 2000).", "startOffset": 182, "endOffset": 202}, {"referenceID": 10, "context": "To incorporate the prediction of future outcomes in the generation process, several papers have explored the use of reinforcement learning techniques, using deep neural networks to model the expected future reward (Li et al., 2016c; Cuay\u00e1huitl, 2017).", "startOffset": 214, "endOffset": 250}, {"referenceID": 2, "context": "To incorporate the prediction of future outcomes in the generation process, several papers have explored the use of reinforcement learning techniques, using deep neural networks to model the expected future reward (Li et al., 2016c; Cuay\u00e1huitl, 2017).", "startOffset": 214, "endOffset": 250}, {"referenceID": 25, "context": "In particular, the Hybrid Code Networks model of (Williams et al., 2017) demonstrate how a mixture of supervised learning, reinforcement learning and domain-specific knowl-", "startOffset": 49, "endOffset": 72}, {"referenceID": 0, "context": "A popular strategy for domain adaptation in natural language processing, which has notably been used in POS-tagging, sentiment analysis, spam filtering and machine translation (Bickel et al., 2007; Jiang and Zhai, 2007; Foster et al., 2010; Xia et al., 2013), is to assign a higher weight to training instances whose properties are similar to the target domain.", "startOffset": 176, "endOffset": 258}, {"referenceID": 6, "context": "A popular strategy for domain adaptation in natural language processing, which has notably been used in POS-tagging, sentiment analysis, spam filtering and machine translation (Bickel et al., 2007; Jiang and Zhai, 2007; Foster et al., 2010; Xia et al., 2013), is to assign a higher weight to training instances whose properties are similar to the target domain.", "startOffset": 176, "endOffset": 258}, {"referenceID": 4, "context": "A popular strategy for domain adaptation in natural language processing, which has notably been used in POS-tagging, sentiment analysis, spam filtering and machine translation (Bickel et al., 2007; Jiang and Zhai, 2007; Foster et al., 2010; Xia et al., 2013), is to assign a higher weight to training instances whose properties are similar to the target domain.", "startOffset": 176, "endOffset": 258}, {"referenceID": 26, "context": "A popular strategy for domain adaptation in natural language processing, which has notably been used in POS-tagging, sentiment analysis, spam filtering and machine translation (Bickel et al., 2007; Jiang and Zhai, 2007; Foster et al., 2010; Xia et al., 2013), is to assign a higher weight to training instances whose properties are similar to the target domain.", "startOffset": 176, "endOffset": 258}, {"referenceID": 12, "context": "The quality of a particular \u3008context, response\u3009 pair is difficult to determine using handcrafted rules \u2013 for instance, the probability of a turn boundary may depend on multiple factors such as the presence of turn-yielding cues or the time gap between the utterances (Lison and Meena, 2016).", "startOffset": 267, "endOffset": 290}, {"referenceID": 13, "context": "The approach is evaluated on the basis of retrievalbased neural models trained on English-language subtitles from (Lison and Tiedemann, 2016).", "startOffset": 114, "endOffset": 141}, {"referenceID": 17, "context": "The TF-IDF (Term Frequency - Inverse Document Frequency) model computes the similarity between the context and its response using methods from information retrieval (Ramos, 2003).", "startOffset": 165, "endOffset": 178}, {"referenceID": 15, "context": "The Dual Encoder model (Lowe et al., 2017) consists of two recurrent networks, one for the context and one for the response.", "startOffset": 23, "endOffset": 42}, {"referenceID": 14, "context": "In the original formalisation of this model (Lowe et al., 2015), the context vector is transformed through a dense layer of same dimension, representing the \u201cpredicted\u201d response.", "startOffset": 44, "endOffset": 63}, {"referenceID": 13, "context": "The dataset used for training the three retrieval models is the English-language portion of the OpenSubtitles corpus of movie and TV subtitles (Lison and Tiedemann, 2016).", "startOffset": 143, "endOffset": 170}, {"referenceID": 12, "context": "The first step was to align at the sentence level the subtitles with an online collection of movie and TV scripts (1 069 movies and 6 398 TV episodes), following the approach described in (Lison and Meena, 2016).", "startOffset": 188, "endOffset": 211}, {"referenceID": 3, "context": "The first corpus, whose genre is relatively close to the training set, is the Cornell Movie Dialog Corpus (Danescu-Niculescu-Mizil and Lee, 2011), which is a collection of fictional conversations extracted from movie scripts (unrelated to the ones used for training the weighting model).", "startOffset": 106, "endOffset": 145}, {"referenceID": 1, "context": "For the recurrent layers, we tested the use of both GRU and LSTM cells, along with their bidirectional equivalents (Chung et al., 2014), without noticeable differences in accuracy.", "startOffset": 115, "endOffset": 135}, {"referenceID": 24, "context": "Some of the issues raised in previous papers are the absence of turn segmentation in subtitling corpus (Vinyals and Le, 2015; Serban and Pineau, 2015; Lison and Meena, 2016), the lack of long-term consistency and \u201cpersonality\u201d in the generated responses (Li et al.", "startOffset": 103, "endOffset": 173}, {"referenceID": 20, "context": "Some of the issues raised in previous papers are the absence of turn segmentation in subtitling corpus (Vinyals and Le, 2015; Serban and Pineau, 2015; Lison and Meena, 2016), the lack of long-term consistency and \u201cpersonality\u201d in the generated responses (Li et al.", "startOffset": 103, "endOffset": 173}, {"referenceID": 12, "context": "Some of the issues raised in previous papers are the absence of turn segmentation in subtitling corpus (Vinyals and Le, 2015; Serban and Pineau, 2015; Lison and Meena, 2016), the lack of long-term consistency and \u201cpersonality\u201d in the generated responses (Li et al.", "startOffset": 103, "endOffset": 173}, {"referenceID": 9, "context": "Some of the issues raised in previous papers are the absence of turn segmentation in subtitling corpus (Vinyals and Le, 2015; Serban and Pineau, 2015; Lison and Meena, 2016), the lack of long-term consistency and \u201cpersonality\u201d in the generated responses (Li et al., 2016b), and the ubiquity of dull, commonplace responses when training generative models (Li et al.", "startOffset": 254, "endOffset": 272}, {"referenceID": 8, "context": ", 2016b), and the ubiquity of dull, commonplace responses when training generative models (Li et al., 2016a).", "startOffset": 90, "endOffset": 108}, {"referenceID": 29, "context": "One related approach is described in (Zhang et al., 2017) which also relies on domain adaptation for neural response generation, using a combination of online and offline human judgement.", "startOffset": 37, "endOffset": 57}], "year": 2017, "abstractText": "Neural conversational models require substantial amounts of dialogue data to estimate their parameters and are therefore usually learned on large corpora such as chat forums, Twitter discussions or movie subtitles. These corpora are, however, often challenging to work with, notably due to their frequent lack of turn segmentation and the presence of multiple references external to the dialogue itself. This paper shows that these challenges can be mitigated by adding a weighting model into the neural architecture. The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling. At training time, these sample weights are included into the empirical loss to be minimised. Evaluation results on retrieval-based models trained on movie and TV subtitles demonstrate that the inclusion of such a weighting model improves the model performance on unsupervised metrics.", "creator": "LaTeX with hyperref package"}}}