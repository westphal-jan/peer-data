{"id": "1704.09028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Time-Sensitive Bandit Learning and Satisficing Thompson Sampling", "abstract": "The literature on bandit learning and regret analysis has focused on contexts where the goal is to converge on an optimal action in a manner that limits exploration costs. One shortcoming imposed by this orientation is that it does not treat time preference in a coherent manner. Time preference plays an important role when the optimal action is costly to learn relative to near-optimal actions. This limitation has not only restricted the relevance of theoretical results but has also influenced the design of algorithms. Indeed, popular approaches such as Thompson sampling and UCB can fare poorly in such situations. In this paper, we consider discounted rather than cumulative regret, where a discount factor encodes time preference. We propose satisficing Thompson sampling -- a variation of Thompson sampling -- and establish a strong discounted regret bound for this new algorithm.", "histories": [["v1", "Fri, 28 Apr 2017 17:54:59 GMT  (189kb,D)", "http://arxiv.org/abs/1704.09028v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel russo", "david tse", "benjamin van roy"], "accepted": false, "id": "1704.09028"}, "pdf": {"name": "1704.09028.pdf", "metadata": {"source": "CRF", "title": "Time-Sensitive Bandit Learning and Satisficing Thompson Sampling", "authors": ["Daniel Russo", "David Tse", "Benjamin Van Roy"], "emails": ["daniel.russo@kellogg.northwestern.edu", "bvr@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The system is facing the problem that it is a system in which most people are able to decide whether they want it or not. It is a system in which most people are able to decide whether they want it or not. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it. It is a system in which they do not want it."}, {"heading": "2 Problem Formulation", "text": "An agent selects actions sequentially (At) t. \"N0\" from the sentence A and observes the corresponding results (Yt, At) t. \"N0.\" There is a random result Yt, a. \"Y\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N..N.\" N. \"N.N.\" N. N. N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N..N.N. N.N. N.N. N. N. N. N. N. N. N. N. N. N. N. N. N. \"N. N.\" N. \"N.\" N. \"N.\" N. \"N.\" N.. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N.\" N. \"N."}, {"heading": "3 Algorithms", "text": "Thompson sampling (TS) is a popular algorithm that implements a useful decision-making policy. During each tth period, TS selects an action as follows: 1. Sampling. [Sampling.] The action is broken by selecting the action with the smallest index. [Sampling.] Note: As should be the case for any decision-making policy, we can write TSas. [Sampling.] For a properly defined action, we will assume that the action is broken by selecting the action with the smallest index. [Sampling.] Where the action is independent of the action. [Sampling.] As a key contribution to this work, we present a modification of the TS that we will call satisfactory Thompson sampling. [Sampling.] While TS aims to identify an optimal action, STS is designed to identify an action that is sufficiently satisfactory or close enough to optimal."}, {"heading": "4 Example: Infinitely-Armed Deterministic Bandit", "text": "To illustrate our motivation, we now offer a simple analytical representation of the advantages that STS enjoys. Let's consider a problem with a countable action space A = {1, 2,....}, in which each action we call the endlessly armed deterministic bandit problem produces a reward factor. Our supervisor over each action is independent and uniform at the interval [0, 1]. The optimal reward is almost certainly R * = 1. For this problem, which we call the endlessly armed deterministic bandit problem, TS never selects the same action twice. This is because there is a high probability that no action selected within a finite time horizon results in reward 1, and as such, at any given time, there are better actions than those previously selected by TS. STS, on the other hand, stops the search for an action that produces reward that goes beyond 1. After such an action has been identified, this STS repeatedly chooses this action ratio from the expected STS to last."}, {"heading": "5 Computational Examples", "text": "In this section we present results from experiments with four bandit problems. Each case is designed in such a way that near-optimal actions can be identified much earlier than the optimal action. In any case, the regret of the STS decreases faster per period than that of the TS over early periods. Our first case is a deterministic bandit problem with 250 actions. The mean reward associated with each action becomes independent of unif ([0, 1]). When an action is sampled, the realized reward of the mean reward is equal to the mean reward; in other words, there is no observation noise. Figure 1 (a) plots per period regret of the TS and STS over 500 periods, averaged over 5000 simulations, each with an independently sampled problem example. For STS we have a tolerance parameter of 05.5 We consider next a problem that is the same as our previous one with observation noise."}, {"heading": "6 A General Regret Bound", "text": "This section provides a general discounted remorse limit and a new information theory analysis technique. We will use this general remorse limit in the analysis of STS in the next section. Let's start with the information theory analysis of the Thompson samples from Russo and Van Roy [14] on which our analysis is based."}, {"heading": "6.1 Notation", "text": "We use Et [\u00b7] = E [\u00b7 | Ft] to denote the expectation operator dependent on the history to the time t and similarly define Pt (\u00b7) = P (\u00b7 | Ft). We denote the entropy of a discrete random variable X by H (X), the mutual information between two random variables X and Y by I (X; Y), and the Kullback-Leibler divergence between the probability distributions P and Q by D (P | | Q). The definitions of entropy and mutual information depend on a basic measure. We use Ht (\u00b7) and It (\u00b7, \u00b7) to denote entropy and mutual information if the basic measure is the posterior distribution Pt. For example, if X is a discrete random variable that takes values in a set of X, Ht (X) = \u2212 and it (X = x) log Pt (X = x).Due to its dependence on the Ft, it is a random variable over a given history (Ft)."}, {"heading": "6.2 Information Theoretic Analysis of Thompson Sampling", "text": "The regret analysis in [14] relates the regret an algorithm suffers to the information it acquires about the identity of the optimal action. It defines the information relationship within a given period of time as the relationship between the square of the expected regret within a period of time and the information it obtains about the optimal action: Et [R * \u2212 Rt, At] 2It (A *; Yt, At | \u044b). (1) It has been shown that each algorithm fulfills a limit of the undiscounted expectation regret period T with respect to its average information ratio over the first T periods and the entropy of the optimal action H (A *). Here, the information ratio roughly captures the cost per bit of information the algorithm acquires about the optimum, and entropy H (A *) measures the magnitude of the initial uncertainty of the decision maker about the optimal identity of the action to be derived from the number of optimizing classes being searched for."}, {"heading": "6.3 A Modified Information Ratio", "text": "This section introduces a modified information relationship that is more suitable for time-dependent online learning problems. As motivation, we consider the infinitely armed deterministic bandit of Section 4. Although no algorithm can identify an optimal action in this example, STS is able to efficiently converge to a satisfactory level of performance. In this sense, the algorithm cannot identify the true optimum, but it seems to obtain enough information for such an action. Building on this intuition, our information theory analysis will aim to formally respond to the information that the algorithm acquires about this action. To justify this discussion, we will consider two examples of such action arising from different issues. Example 1. Consider the infinitely armed deterministic bandit of Section 4. Over time, STS will become examples of a sequence of actions (A0, A1, A2,...)."}, {"heading": "6.4 General Regret Bound", "text": "The following theorem limits the expected discounted regret of each algorithm or action process in terms of the information relationship (2). Theorem 3. For each action process (At: t-N0) and each action process (A: t-N0) H (A: t-N0) 1-2 (3), for the R = R (Y), A (A). (3), for the R = R (A). This limit splits regret into the sum of two terms; one that captures the discounted performance deficit of the benchmark action A (A) relative to A (A), and one that limits the additional regret as it learns to identify A (Y)."}, {"heading": "6.5 Connections to Rate Distortion Theory", "text": "In information theory, the entropy of a source characterizes the length of an optimal lossless encoding. Acclaimed distortion theory [7, Chapter 10] characterizes the number of bits required for lossless encoding, which resolves when it is possible to derive a satisfactory lossy compression scheme while transmitting much less information than is required for lossless compression. At a high level, the developments in this paper represent a shift from entropy to the application of the distortion function. While previous results depend on the entropy of A, Theorem 3 depends on a naturally defined distortion function to compress the optimal decision: R (D): = min E (R): = min E (R). DI (A). If A (A) depends deterministically on A (A), I (A) = H (A)."}, {"heading": "7 Information Ratio Analysis of the Infinitely-Armed Bandit", "text": "The general regret of the previous section can be illustrated by two variants of the boundless bandit problem: the next subsection revisits the deterministic boundless bandit of section 4 and shows how to derive from theorem 3 a boundless regret for STS. Section 7.2 examines an extension of the boundless armed bandit problem, in which reward observations are loud. Again, Theorem 3 can be specialized to derive a boundless regret for STS."}, {"heading": "7.1 Infinitely\u2013Armed Bandit with Deterministic Observations", "text": "We now come to the endlessly armed deterministic bandit problem of Section 4. By specializing in our general regret for this setting, we will effectively restore the boundary of Theorem 2 derived from the direct analysis. As there is no observation noise in this example, as soon as STS samples an action with a reward greater than 1 \u2212 sample in all subsequent periods, the algorithm will know for certain that no previously sampled action generates a reward greater than 1 \u2212 and thus a new action is selected in each period of time. Let it happen that the first time a -optimal action is sampled. The next result applies the general regret limit of Theorem 3 to this problem with A = AH, so the benchmark action is the first -optimal action sampled by STS.Theorem 4."}, {"heading": "7.2 Infinitely\u2013Armed Bandit with Noisy Observations", "text": "Let us now consider a generalization of the problem discussed in the previous paragraph, which allows for noisy observations and non-uniform priorities. Again, let us assume that there is a countable action space A = {1, 2,..}. Let us assume that the rewards in [0, 1] are almost certainly limited. Let us examine the discounted regret generated by STS, regardless of a distribution whose support is the unit interval [0, 1]. Any action sampled by STS is -optimal with the probability. Let us suppose that the rewards in [0, 1] are almost certainly limited, but because the observations are loud, the algorithm is uncertain about the quality of the actions it has sampled. The next result is a regret that is limited for STS in this more complicated environment. The evidence reverts to Theorem 3 with the benchmark action A."}, {"heading": "8 Conclusion", "text": "In this paper, we introduce satisfactory Thompson sampling - a variant of Thompson sampling that can offer vastly superior performance when optimal action is costly relative to powerful suboptimal action. We have also developed a general information theory framework for analyzing discounted regret, which provides a novel link between optimal decision-making with time preferences and research into lossy data compression. Important questions remain unanswered, but we hope that this relationship will open many avenues for future research."}, {"heading": "A Proof of Theorem 1: Regret of TS on the Infinitely-Armed Deterministic Bandit", "text": "Proof. In each period t sampled a previously unsampled action At / \u0439 {A1,..., At \u2212 1}, which is the expected reward E [\u03b8At] = E [\u03b81] = 1 / 2. The optimal expected reward is 1, and therefore the expected discount remorse of the TS \u221e \u2211 t = 0 \u03b1t (1 \u2212 1 / 2) = 12 (1 \u2212 \u03b1)."}, {"heading": "B Proof of Theorem 2: Direct Analysis of the Infinitely-Armed Deterministic Bandit", "text": "Prove that the choice of E [1 \u2212 1 \u2212 \u2212 \u2212 \u2212 \u2212 1 \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2) is a decision (1 \u2212 2) The evidence shows that the choice of E [1 \u2212 2 \u2212 1 \u2212 \u2212 2 \u2212 2) + (1 \u2212 2) 2 (1 \u2212 3) (1 \u2212 3) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (5) (5) (5 5) (5 5) (5 \u2212 5) (5 (5) (5 \u2212 4) (5) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) \u2212 4) (1 \u2212 4) (1 \u2212 4) (1 \u2212 4) \u2212 4) (1 \u2212 4) (1 \u2212 4) \u2212 4 (1 \u2212 4) (1 \u2212 4) \u2212 4) (1 \u2212 4) (1 \u2212 4) \u2212 4) (1 \u2212 4 (1 \u2212 4) \u2212 4) (1 \u2212 4 (1 \u2212 4) \u2212 4) (1 \u2212 5 \u2212 4) (1"}, {"heading": "C Proof of Theorem 3", "text": "We first show that entropy limits the expected accumulation of mutual information, according to the chain rule for mutual information, for each T, E [T \u2212 1 \u2211 t = 0. \u2212 Es (A \u00b2; Yt, At \u00b2) = T \u2212 1 \u2211 t = 0 I (A \u00b2; Yt, At \u00b2, Ht = 0 I (A \u00b2; Yt, At \u00b2, A0, Y0, A0,..., At \u2212 1, At \u2212 1, At \u2212 1) = T \u2212 1 \u2211 t = 0 I (A \u00b2; Yt, At \u00b2, A0, Yt, A0,..)., At \u2212 1, Yt \u2212 1, At) = I (A \u00b2; (A0, Y0, A0, A0.)."}, {"heading": "D Proof of Theorem 4: Information-Ratio Analysis of Infinitely\u2013 Armed Deterministic Bandit", "text": "Lemma 6 (STS with tolerance) is the order of the new actions (STS with tolerance) is the order of the new actions (STS with tolerance) is the order of the order of the order of the order of the order. (STS with tolerance) is the order of the order of the order of the order of the order. (STS with tolerance) is the order of the order of the order of the order of the order of the order. (STS = Order of the order of the order of the order of the order. (STS = Order of the order, order of the order of the order, order of the order of the order, order of the order of the order, order of the order of the order, order of the order, order of the order, order of the order, order of the order, order of the order, order of the order, order of the order, order of the order, order of the order, order of the order of the order, the order of the order of the order, the order of the order of the order, the order of the order of the order, the order of the order of the order, the order of the order of the order"}, {"heading": "E Proof of Theorem 5: Information Ratio Analysis of the InfinitelyArmed Bandit with Noisy Observations", "text": "The evidence for Theorem 5 is based on the attribute of probability (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (P) (A) (P) (P) (P) (P) (P) (P) (P) (A) (A) (P) (A) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) () ()) () () () () () ()) () () () () () ()) () () () ()) () () () ()) () () () () ()) () () () () () ()) () () () () () ()) () () () () ()) () () () ()) () () () () () ()) () () ()) () () () () () ()) () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () () (() () () () ((() () () () () () () () () () () () () (() () () () (()"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>The literature on bandit learning and regret analysis has focused on contexts where the goal<lb>is to converge on an optimal action in a manner that limits exploration costs. One shortcoming<lb>imposed by this orientation is that it does not treat time preference in a coherent manner.<lb>Time preference plays an important role when the optimal action is costly to learn relative to<lb>near-optimal actions. This limitation has not only restricted the relevance of theoretical results<lb>but has also influenced the design of algorithms. Indeed, popular approaches such as Thompson<lb>sampling and UCB can fare poorly in such situations. In this paper, we consider discounted<lb>rather than cumulative regret, where a discount factor encodes time preference. We propose<lb>satisficing Thompson sampling \u2013 a variation of Thompson sampling \u2013 and establish a strong<lb>discounted regret bound for this new algorithm.", "creator": "LaTeX with hyperref package"}}}