{"id": "1705.00034", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Deep Multi-view Models for Glitch Classification", "abstract": "Non-cosmic, non-Gaussian disturbances known as \"glitches\", show up in gravitational-wave data of the Advanced Laser Interferometer Gravitational-wave Observatory, or aLIGO. In this paper, we propose a deep multi-view convolutional neural network to classify glitches automatically. The primary purpose of classifying glitches is to understand their characteristics and origin, which facilitates their removal from the data or from the detector entirely. We visualize glitches as spectrograms and leverage the state-of-the-art image classification techniques in our model. The suggested classifier is a multi-view deep neural network that exploits four different views for classification. The experimental results demonstrate that the proposed model improves the overall accuracy of the classification compared to traditional single view algorithms.", "histories": [["v1", "Fri, 28 Apr 2017 18:45:57 GMT  (1707kb,D)", "http://arxiv.org/abs/1705.00034v1", "Accepted to the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'17)"]], "COMMENTS": "Accepted to the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'17)", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["sara bahaadini", "neda rohani", "scott coughlin", "michael zevin", "vicky kalogera", "aggelos k katsaggelos"], "accepted": false, "id": "1705.00034"}, "pdf": {"name": "1705.00034.pdf", "metadata": {"source": "CRF", "title": "DEEP MULTI-VIEW MODELS FOR GLITCH CLASSIFICATION", "authors": ["Sara Bahaadini", "Neda Rohani", "Scott Coughlin", "Michael Zevin", "Vicky Kalogera", "Aggelos K Katsaggelos"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2.1. Parallel view model", "text": "The idea behind the parallel view model is to project each view into a feature space based more on the statistical properties of the specimens than on their visual properties. Through this projection, the views interact with each other and are presented more efficiently in a common feature space. We illustrate the construction of the parallel view model using the four durations as views in Fig. 2. On the very first level, each view goes through a sinuous layer, followed by maxpooling and ReLU activation, and then we introduce a common layer (merger layer) to map all views into a common feature space. Another group of folding, maximum pooling and activation layers is used after the merge layer to model the common features obtained from the previous layers. At the end, a fully bonded layer and a softmax layer are used (see Fig. 2)."}, {"heading": "2.2. Merged view model", "text": "In this model, we place the network layers above the merged views. We merge the views by forming a 2m x 2k matrix by placing four m x k images side by side. After merging the views, a series of revolutionary layers is used, followed by a max pooling and activation layer, and then a fully connected layer and a softmax layer are utilized (see Fig. 3). This approach clearly models the distribution of the different views in their original feature space. This seems to be a reasonable approach, as the correlations between the views in our problem are not particularly nonlinear compared to other tasks where the views are very different, such as image and text [8] or audio and video [9]. Therefore, it is possible for the model to learn such correlations even in the original data space."}, {"heading": "2.3. Training", "text": "The models presented above optimize a loss function defined on the training data. To train the model, we can use either the mean square error or the average cross entropy error as a loss function. Due to the advantages of the average cross entropy error over the mean square error, e.g. the derivation of a \"better\" gradient for reproduction, for multi-stage classification problems [14], we use in our model a cross entropy-based loss function defined as follows: E = \u2212 N \u2211 n = 1 C \u2211 i = 1 yni log o n i (1), where oni is the output of the model for class i when the n-th training sample is given to the network, yni is one when the n-th sample is from class i, otherwise it is zero, and N and C are the total number of training samples or classes. There are many optimization techniques [15, 16, 17, 18] which we can use to optimize the objective function."}, {"heading": "3.1. Dataset", "text": "The data set consists of glitch samples from 20 classes. The glitches are represented as a spectrogram, a time-frequency representation in which the x-axis represents the duration of the glitch and the y-axis shows the frequency content of the glitch. The colors show the \"loudness\" of the glitch in the aLIGO detector. The classes arise from different environmental and instrumental mechanisms and are based on the shape and intensity of the glitch in the time-frequency space. The primary sampling duration is 0.5 seconds. In this study, however, we are using three additional durations, i.e. 1.0, 2.0 and 4 seconds, per glitch, to train our multiview models.An example of the four-duration data set is shown in Fig. 1. In total, there are 7730 glitches in our data set. We are using 75%, 12.5% of the samples for training, validation and test sets, respectively."}, {"heading": "3.2. Experiment", "text": "Baseline A simple approach is to use only one glitch duration, as is the case with a traditional single-view approach. We use this as a baseline to compare the performance of our multi-view deep models. For single-view models, we use CNNs with the structure shown in Table 1 (left column). Batch size is optimized for best classification accuracy. We use two 128 cores of size 5 x 5, max pooling of 2 x 2 and the ReLU activation function. Batch size is set to 30, and the number of iterations is 130. Table 2 shows the classification accuracy of single-view CNNs. Deep multi-view models In the parallel view model, we first use four separate volume layers (see Figure 2). Each layer has 128 cores of size 5 x 5 and 2 max-pooling and Lu activation."}, {"heading": "3.3. Analysis", "text": "In fact, most of them are able to survive themselves if they don't see themselves able to survive themselves; most of them are able to survive themselves, and most of them are able to survive themselves; most of them are able to survive themselves; and most of them are able to survive themselves. \"Most of them are able to survive themselves; and most of them are able to survive themselves.\" Most of them are able to survive themselves, and most of them are able to survive themselves. \"Most of them are able to survive themselves, and most of them are able to survive themselves.\" Most of them are able to survive themselves. \"Most of them are able to survive themselves,\" to survive themselves, and to survive themselves. \""}], "references": [{"title": "A survey on multi-view learning", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "arXiv preprint arXiv:1304.5634, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-modal emotion analysis from facial expressions and electroencephalogram", "author": ["X. Huang", "J. Kortelainen", "G. Zhao", "X. Li", "A. Moilanen", "T. Sepp\u00e4nen", "M. Pietik\u00e4inen"], "venue": "Computer Vision and Image Understanding, vol. 147, pp. 114\u2013124, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint deep modeling of users and items using reviews for recommendation", "author": ["L. Zheng", "V. Noroozi", "Ph. S. Yu"], "venue": "ACM International Conference on Web Search and Data Mining (WSDM), 2017.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "Audiovisual fusion: Challenges and new approaches", "author": ["A.K. Katsaggelos", "S. Bahaadini", "R. Molina"], "venue": "Proceedings of the IEEE, vol. 103, no. 9, pp. 1635\u20131653, Sept 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Posterior-based sparse representation for automatic speech recognition", "author": ["S. Bahaadini", "A. Asaei", "D. Imseng", "H. Bourlard"], "venue": "Proceeding of Interspeech, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "An audio-visual person identification and verification system using faps as visual features", "author": ["P.S. Aleksic", "A.K. Katsaggelos"], "venue": "ACM Workshop on Multimodal User Authentication. Citeseer, 2003, pp. 80\u201384.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Variational gaussian process for sensor fusion", "author": ["N. Rohani", "P. Ruiz", "E. Besler", "R. Molina", "A.K. Katsaggelos"], "venue": "Signal Processing Conference (EUSIPCO), 2015 23rd European, Aug 2015, pp. 170\u2013174.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012, pp. 2222\u20132230.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical feature representation and multimodal fusion with deep learning for ad/mci diagnosis", "author": ["H. Suk", "S. Lee", "D. Shen"], "venue": "NeuroImage, vol. 101, pp. 569\u2013 582, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Gravity spy: Integrating advanced ligo detector characterization, machine learning, and citizen science", "author": ["M. Zevin", "S. Coughlin", "S. Bahaadini", "E. Besler", "N. Rohani", "S. Allen", "M. Cabero", "K. Crowston", "A. Katsaggelos", "Sh. Larson"], "venue": "arXiv preprint arXiv:1611.04596, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Observation of Gravitational Waves from a Binary Black Hole Merger", "author": ["B.P. Abbott", "R. Abbott", "T.D. Abbott", "M.R. Abernathy", "F. Acernese", "K. Ackley", "C. Adams", "T. Adams", "P. Addesso", "R.X. Adhikari"], "venue": "Physical Review Letters, vol. 116, no. 6, pp. 061102, Feb. 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "GW151226: Observation of Gravitational Waves from a 22-Solar-Mass Binary Black Hole Coalescence", "author": ["B.P. Abbott", "R. Abbott", "T.D. Abbott", "M.R. Abernathy", "F. Acernese", "K. Ackley", "C. Adams", "T. Adams", "P. Addesso", "R.X. Adhikari"], "venue": "Physical Review Letters, vol. 116, no. 24, pp. 241103, June 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-entropy vs. squared error training: a theoretical and experimental comparison", "author": ["P. Golik", "P. Doetsch", "H. Ney"], "venue": "INTERSPEECH, 2013, pp. 1756\u2013 1760.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Alpinist cellularde: a cellular based optimization algorithm for dynamic environments", "author": ["V. Noroozi", "A. Hashemi", "M.R Meybodi"], "venue": "Proceedings of the 14th annual conference companion on Genetic and evolutionary computation. ACM, 2012, pp. 1519\u20131520.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Two phased cellular pso: A new collaborative cellular algorithm for optimization in dynamic environments", "author": ["A. Sharifi", "V. Noroozi", "M. Bashiri", "A. Hashemi", "M.R. Meybodi"], "venue": "2012 IEEE Congress on Evolutionary Computation. IEEE, 2012, pp. 1\u20138.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "Fr. Bastien", "P. Lamblin", "Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010, Oral Presentation.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Using multiple views can improve performance as they may provide complementary or redundant information [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 157, "endOffset": 163}, {"referenceID": 4, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 157, "endOffset": 163}, {"referenceID": 5, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "Integrating multiple sources of data is a challenging task, and various approaches have been proposed in the literature [4][7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "Integrating multiple sources of data is a challenging task, and various approaches have been proposed in the literature [4][7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "More recently, deep learning techniques have shown promising performance for multimodal fusion [8, 9, 10].", "startOffset": 95, "endOffset": 105}, {"referenceID": 8, "context": "More recently, deep learning techniques have shown promising performance for multimodal fusion [8, 9, 10].", "startOffset": 95, "endOffset": 105}, {"referenceID": 9, "context": "More recently, deep learning techniques have shown promising performance for multimodal fusion [8, 9, 10].", "startOffset": 95, "endOffset": 105}, {"referenceID": 10, "context": "In this paper, we propose deep multi-view models for a particular classification problem from the aLIGO project [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Advanced LIGO (Advanced Laser Interferometer Gravitational -wave Observatory, or aLIGO) has recently made the first direct observations of gravitational waves [12] [13], Fig.", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "Advanced LIGO (Advanced Laser Interferometer Gravitational -wave Observatory, or aLIGO) has recently made the first direct observations of gravitational waves [12] [13], Fig.", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "This seems a reasonable approach since the correlations among the views in our problem are not highly non-linear compared to other tasks where the views are very different, such as image and text [8], or audio and video [9].", "startOffset": 196, "endOffset": 199}, {"referenceID": 8, "context": "This seems a reasonable approach since the correlations among the views in our problem are not highly non-linear compared to other tasks where the views are very different, such as image and text [8], or audio and video [9].", "startOffset": 220, "endOffset": 223}, {"referenceID": 13, "context": ", the derivation of a \u201cbetter\u201d gradient for back propagation, for multi-class classification problems [14], in our model we use a cross-entropy based loss function defined as follows:", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 15, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 16, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 17, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 14, "context": "We use the Adadelta [15] optimizer.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "We use Keras [19] with Theano [20] back-end for all implementations.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "We use Keras [19] with Theano [20] back-end for all implementations.", "startOffset": 30, "endOffset": 34}], "year": 2017, "abstractText": "Non-cosmic, non-Gaussian disturbances known as \u201cglitches\u201d, show up in gravitational-wave data of the Advanced Laser Interferometer Gravitational-wave Observatory, or aLIGO. In this paper, we propose a deep multi-view convolutional neural network to classify glitches automatically. The primary purpose of classifying glitches is to understand their characteristics and origin, which facilitates their removal from the data or from the detector entirely. We visualize glitches as spectrograms and leverage the state-of-the-art image classification techniques in our model. The suggested classifier is a multi-view deep neural network that exploits four different views for classification. The experimental results demonstrate that the proposed model improves the overall accuracy of the classification compared to traditional single view algorithms.", "creator": "LaTeX with hyperref package"}}}