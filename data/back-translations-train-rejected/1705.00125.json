{"id": "1705.00125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2017", "title": "Cnvlutin2: Ineffectual-Activation-and-Weight-Free Deep Neural Network Computing", "abstract": "We discuss several modifications and extensions over the previous proposed Cnvlutin (CNV) accelerator for convolutional and fully-connected layers of Deep Learning Network. We first describe different encodings of the activations that are deemed ineffectual. The encodings have different memory overhead and energy characteristics. We propose using a level of indirection when accessing activations from memory to reduce their memory footprint by storing only the effectual activations. We also present a modified organization that detects the activations that are deemed as ineffectual while fetching them from memory. This is different than the original design that instead detected them at the output of the preceding layer. Finally, we present an extended CNV that can also skip ineffectual weights.", "histories": [["v1", "Sat, 29 Apr 2017 03:49:34 GMT  (791kb,D)", "http://arxiv.org/abs/1705.00125v1", "6 pages, 5 figures"]], "COMMENTS": "6 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["patrick judd", "alberto delmas", "sayeh sharify", "andreas moshovos"], "accepted": false, "id": "1705.00125"}, "pdf": {"name": "1705.00125.pdf", "metadata": {"source": "CRF", "title": "Cnvlutin2: Ineffectual-Activation-and-Weight-Free Deep Neural Network Computing", "authors": ["Patrick Judd", "Alberto Delmas", "Sayeh Sharify"], "emails": ["moshovos}@ece.utoronto.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTIONAlbericio et al. proposed the Cnvlutinn (CNV) accelerator [1], which exploits ineffective activation values to improve performance and energy efficiency compared to conventional hardware, which processes all activations regardless of their value content. CNV benefits from fully connected and revolutionary layers and has been evaluated for the image classification of Convolutionary Neural Networks (CNNs). CNV detects ineffective activations at runtime and therefore does not require purpose-trained neural networks (NNs). This work extends the work of Albericio et al. [1] by discussing design alternatives and extensions beyond the originally proposed designs. Specifically, this work presents the following coding options, which are ineffective. The proposed encoding reduces storage space and energy consumption compared to the original proposal. 2) An extension whereby the detection of ineffective activations occurs while they are fetched from memory."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. DaDianNao", "text": "This year is the highest in the history of the country."}, {"heading": "B. Cnvlutin", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "III. ALTERNATE ENCODING OF EFFECTUAL ACTIVATIONS", "text": "Previously, we described the ZFNAf format, which encodes the effective neuron values by packaging them at the beginning of the brick container. Their offsets were encoded separately, using 4 bits per value for a 16-value brick, corresponding to a 25% overhead for 16-bit values and 16-element bricks. In this section, alternative activation array formats are presented that reduce memory consumption."}, {"heading": "A. RAW or Encoded Format (RoE)", "text": "Another encoding uses only one additional bit per brick container at the expense of the inability to encode all possible combinations of ineffective values. Specifically, the first bit of the brick determines whether the brick is encoded or not. If the brick is encoded, the remaining bits are used to store the neuron values and their offsets. As long as the number of effective activations is such that they fit into the brick container, the brick can be encoded. Otherwise, all the activation values are stored as they are and we lose the ability to skip the ineffective activations for the specific brick. For example, suppose that we have bricks of size 4 and 16 bit values. In total, each such brick requires 4 x 16 = 64 bits. A brick containing the values (1,2,0,0) can be encoded with 65 bits as follows: (1, (0,1), (1,2). The first 1 means that the brick is encoded."}, {"heading": "C. Storing Only the Effectual Activations", "text": "Another format builds on VIAI by storing only the RMS values. For example, a 4-element activation block of (1,0,0,4) would be stored as (1001,1,0,4) in VIAI. CompressedVIAI would store it as (1001,1,4) instead. Here, the two ineffective zero activations were not stored in memory. As the blocks are no longer of fixed size, a degree of redirection is necessary to support the retrieval of arbitrary blocks. If the original dimensions of the activation field (X, Y, I) were stored, this indirection array would have IR (X, Y, dI / 16e) pointers, which can be generated at the output of the previous layer. A further decrease in memory can be possible by storing activations with reduced precision. For example, with the method of Judd et al. [3] it is possible to calculate precision per layer in the pre-profile based on profiles."}, {"heading": "IV. ZERO-MEMORY-OVERHEAD INEFFECTUAL ACTIVATION SKIPPING", "text": "In the original CNV implementation, the ineffective activations at the output of the previous layer were \"removed.\" ZFNAf suffers memory expenditure and writes and reads the activation offset values, which require additional energy. This section describes an alternative activation design that retrieves \"ineffective activations\" while it is retrieving from the NM and transmits these activation values to the tiles prior to communication. Specifically, processing for one shift begins by the dispatcher retrieving 16 activation bricks, one brick per neuron track, as previously described, and the dispatcher then calculates the I (as previously described in the VIAI format) of vectors dealing with 16 comparators per brick, one per activation value. The dispatcher then proceeds to communicate the effective activation bricks, one brick per neuron lane."}, {"heading": "V. SKIPPING INEFFECTUAL SYNAPSES (WEIGHTS)", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the ru the ru the rfu the ru the ru the rfu the rfu the ru the ru the ru the ru the ru the rfu the rfu the rfu the rfu the ru the fu the ru the rfu the ru the ru the ru the ru the ru the rfu the ru the ru the ru the rfu the ru the ru the ru the fu the ru the rfu the fu the ru the ru the ru the fu the fu the fu the fu the ru the ru the ru the fu the fu the ru the fu the ru the ru the ru the ru the ru the fu the fu the fu the ru the ru the ru the fu the ru the fu the fu the fu the ru the fu the ru the ru the ru the ru the ru the fu the ru the ru the ru the r"}, {"heading": "VI. CONCLUSION", "text": "We introduced a series of modifications and enhancements to the CNV-CNN Accelerator. These modifications change the memory and energy compromise by encoding ineffective activations differently from the original CNV proposal. We also introduced a technique that identifies the ineffective activations as they are read out of the memory, without disk space and access costs. Finally, we proposed an extension of the CNV that can also benefit from ineffective weights. ACKNOWLEDGMENTSThis work was supported by an NSERC Discovery Grant."}], "references": [{"title": "and A", "author": ["J. Albericio", "P. Judd", "T. Hetherington", "T. Aamodt", "N.E. Jerger"], "venue": "Moshovos, \u201cCnvlutin: Ineffectual-neuron-free deep neural network computing,\u201d in Proceedings of the 43rd International Symposium on Computer Architecture, ISCA \u201916, pp. 1\u201313", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets", "author": ["P. Judd", "J. Albericio", "T. Hetherington", "T. Aamodt", "N. Enright Jerger", "R. Urtasun", "A. Moshovos"], "venue": "arXiv:1511.05236v4 [cs.LG] ,\u201d arXiv.org", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "and M", "author": ["K. Ullrich", "E. Meeds"], "venue": "Welling, \u201cSoft weight-sharing for neural network compression,\u201d in Proceedings of the 5th International Conference on Learning Representations", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "proposed the Cnvlutinn (CNV) accelerator [1] which exploits ineffectual activation values to improve performance and energy efficiency over conventional hardware that processes all activations regardless of their value content.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "[1] by discussing design alternatives and extensions over the originally proposed design.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1]", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "3: Dispatcher [1]", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "[3] it is possible to determine precisions per layer in advance based on profiling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] a large fraction of weights becomes zero.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Other work has shown that networks can be also be trained to increase the fraction of weights that are ineffectual [4].", "startOffset": 115, "endOffset": 118}], "year": 2017, "abstractText": "We discuss several modifications and extensions over the previous proposed Cnvlutin (CNV) accelerator for convolutional and fully-connected layers of Deep Learning Network. We first describe different encodings of the activations that are deemed ineffectual. The encodings have different memory overhead and energy characteristics. We propose using a level of indirection when accessing activations from memory to reduce their memory footprint by storing only the effectual activations. We also present a modified organization that detects the activations that are deemed as ineffectual while fetching them from memory. This is different than the original design that instead detected them at the output of the preceding layer. Finally, we present an extended CNV that can also skip ineffectual weights.", "creator": "LaTeX with hyperref package"}}}