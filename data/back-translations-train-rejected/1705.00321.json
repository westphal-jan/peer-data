{"id": "1705.00321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2017", "title": "Generative Neural Machine for Tree Structures", "abstract": "Tree structures are commonly used in the tasks of semantic analysis and understanding over the data of different modalities, such as natural language, 2D or 3D graphics and images, or Web pages. Previous studies model the tree structures in a bottom-up manner, where the leaf nodes (given in advance) are merged into internal nodes until they reach the root node. However, these models are not applicable when the leaf nodes are not explicitly specified ahead of prediction. Here, we introduce a neural machine for top-down generation of tree structures that aims to infer such tree structures without the specified leaf nodes. In this model, the history memories from ancestors are fed to a node to generate its (ordered) children in a recursive manner. This model can be utilized as a tree-structured decoder in the framework of \"X to tree\" learning, where X stands for any structure (e.g. chain, tree etc.) that can be represented as a latent vector. By transforming the dialogue generation problem into a sequence-to-tree task, we demonstrate the proposed X2Tree framework achieves a 11.15% increase of response acceptance ratio over the baseline methods.", "histories": [["v1", "Sun, 30 Apr 2017 15:09:10 GMT  (2873kb,D)", "https://arxiv.org/abs/1705.00321v1", null], ["v2", "Sat, 6 May 2017 14:38:10 GMT  (3033kb,D)", "http://arxiv.org/abs/1705.00321v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["ganbin zhou", "ping luo", "rongyu cao", "yijun xiao", "fen lin", "bo chen", "qing he"], "accepted": false, "id": "1705.00321"}, "pdf": {"name": "1705.00321.pdf", "metadata": {"source": "CRF", "title": "Generative Neural Machine for Tree Structures", "authors": ["Ganbin Zhou", "Ping Luo", "Rongyu Cao", "Yijun Xiao", "Fen Lin", "Bo Chen", "Qing He"], "emails": ["caory}@ics.ict.ac.cn,", "heqing}@ict.ac.cn"], "sections": [{"heading": null, "text": "Previous studies have modelled the tree structures in a bottom-up manner, merging the (pre-specified) leaf nodes into internal nodes until they reach the root node. However, these models are not applicable unless the leaf nodes are explicitly specified prior to the prediction. At this point, we present a neural machine for generating tree structures from top to bottom that aims to infer such tree structures without the specified leaf nodes. In this model, the history memories of ancestors are fed to a node in order to generate its (ordered) children in a recursive manner. This model can be used as a tree-structured decoder in the context of \"X to tree\" learning, where X stands for any structure (e.g. chain, tree, etc.) that can be represented as a latent vector. By transferring the problem into a dialogue for accepting a task in the ratio of 15% to the proposed tree structure, we show that the proposed X15% ratio of the tree structure to the proposed task is achieved."}, {"heading": "1 Introduction", "text": "In fact, most people who stand up for people's rights are not aware of themselves and their rights."}, {"heading": "2 X2Tree Neural Network", "text": "In fact, it is such that most of us will be able to move into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able to change the world, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "2.2 Tree Generation", "text": "In this section, we develop a greedy search algorithm for this sequence task. Previous studies have focused on the sequence task of sequence structures in which the bar search is often performed. At each step, this process repeats recursively until all the candidates end with the highest probabilities. Since the sequence is a specific case of trees, the greedy search for tree generation has more challenges to overcome. First, an arbitrary tree at the current end of the sequence has a new node that could potentially produce new children. Second, when we grow new children for a leaf node, we must produce all children as a whole because they correlate with each other (as mentioned in Section 2.1). Multiple groups of such K children must be created as the best generation."}, {"heading": "2.3 Tree Canonicalization", "text": "As mentioned above, the proposed X2Tree model requires that the tree be K-ary full tree. This section discusses how to transform a tree into K-ary full tree, and selecting K.Basically, the transformed K-ary full tree should correspond to the original one. In other words, an algorithm must exist to support the bidirectional transformation between a tree and its Kary full counterpart. Considering the number of K-ary full tree is linear to the number of model parameters, in order to reduce model complexity, we usually hope that K will be as small as possible. For a given tree, an easy way to transform it into a full tree is to fill all empty positions with eob nodes."}, {"heading": "3 Experimental Validation: Ap-", "text": "Transfer to Dialog Generation"}, {"heading": "3.1 Dialog Generation as a Seq2Tree Task", "text": "To demonstrate the effectiveness of the X2Tree model, we apply it to the generation of dialogs. This task is conventionally modeled as a Seq2Seq learning problem, which maps an input post x to its response y, where x and y are both sequences of words. Recently, neural models in Seq2Seq Learning [3, 13, 21, 18, 12] have provided the most advanced performance. These neural models essentially use a chained decoder to sequentially generate tokens that have encoded a context vector from an input sequence. Basically, it considers the dependence between each word and all of its preceding ones. LSTM or GRU units are commonly used to memorize all preceding word information. However, it is difficult to maintain the information perfectly over long distances, making some subsets of generated answers irrelevant or unrammatic."}, {"heading": "3.2 From Generated Tree to Response Sentence", "text": "Note that the X2Tree model outputs a tree structure. To perform the task of generating dialogs, we must flatten the predicted tree into a sequence. Therefore, we must store position information in the dependency tree. To do this, we first specify the following definition of the sequence-preserving tree (SP tree for short). Definition 1. An SP tree is an ordered tree in which each node t is labeled as an integer I (t). The left part contains the first I (t) nodes (child nodes are ordered from left to right), while the right part contains the remaining nodes. In the ordered traverse of an SP tree, the children of the node t are divided into two parts. The left part contains the first I (t) nodes (child nodes are ordered from left to right), while the right part contains the remaining nodes. In the ordered traverse of an SP tree, we first visit the current part of the tree formation with the left part, then the corresponding three nodes."}, {"heading": "3.3 Canonicalization of SP Trees", "text": "As already discussed, a tree canonization step is necessary to turn the original dependency tree into a K-ary full tree. To get the sequence, we transform the dependence into a ternary tree. We will now introduce the algorithm and discuss why ternary tree is the \"best\" choice. Alg. 2 Details of this canonization process and a figure is shown in Fig.6.In a ternary tree, each child is set to c1, the first child in the original tree; and its middle child is set to cI (t) + 1. Every other child cj (j 6 = 1 and j 6 = I (t) + 1) is set as the right child of cj \u2212 1."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset Details", "text": "We apply the X2Tree model to the task of dialog generation and demonstrate its effectiveness. To construct the dialog corpus, 14 million post-response pairs were obtained from Tencent Weibo3. 815, 852 were left after spam and advertising were removed, including 775, 852 for training, 40,000 for model validation. Note that in the dataset, every post and response is a sequence. To enable X2Tree modeling, the LTP (Language Technology Platform4) dependency saver is used to convert the responses into dependency trees, which are fed to the X2Tree model for training after canonization."}, {"heading": "4.2 Benchmark Methods", "text": "We have implemented the following four popular neural-based talking models for comparison: 1) Seq2Seq [19]: An RNN model that uses the last hidden state of the encoder as the initial hidden state of the decoder. 2) EncDec [3]: An RNN model that feeds the last hidden state of the encoder to each cell and Softmax unit of the decoder. 3) ATT [1]: An RNN model that is based on EncDec with attention signal. 4) NRM [13]: Neural Responding Machine with global and local schemes.All of these models build sequences directly on sequences and differ only in how to combine the hidden states of the encoder into a latent vector. Therefore, the 3http: / / t.qq.com /? lang = en _ US 4http: / / www.ltp-cloud.com / decoded tree can be applied to each of these models from this perspective to improve the coding quality of the other Tree (one of these models may be applied here)."}, {"heading": "4.3 Implementation Details", "text": "Since segmentation granularity and vocabulary size have an impact on model performance, all sentences in the experiments are segmented by LTP for fair comparison. Our implementations are based on the Theano library [2]. We used single-layer GRU [3] with 1,024-dimensional hidden states on {fk} Kk = 1 and all baseline models. As proposed in [13], word embeddings for the encoders and decoders are learned separately, the dimensions of which are set at 128 for all models. All parameters were initialized using a uniform distribution between -0.01 and 0.01. In training, we divided the corpus into mini-batches, each containing 128 pairs of items and answers, and used ADADELTA [24] for optimization."}, {"heading": "4.4 Human Judgment", "text": "Due to the high diversity of dialogs, it is practically impossible to create a data set that adequately covers all the answers for each post. Therefore, match-based metrics, such as BLEU [11], are not appropriate. While helplessness is common in SMT, lower values of this metric do not lead to better answers [9]. Therefore, in our experiments, we only use human judgment. In detail, 3 markup specialists were invited to evaluate the quality of responses to 300 randomly selected posts. For each post, each model generated 5 responses (a total of 25). To make a fair comparison, we create a single file in which each post is followed by 25 responses, which are shuffled to avoid markup specialists knowing from which model each answer is generated. For each answer, the markup specialists determine the quality as one of the following three levels: \u2022 Level 1: The answer is ungrammatic. \u2022 Level 2: The answer is essentially grammatical, but irrelevant to the input."}, {"heading": "4.5 Experimental Results and Analysis", "text": "The experimental results are summarized in Table 1. For Seq2Seq, NRM, and X2Tree, the match value is in the range of 0.6 to 0.8, which is interpreted as a \"substantial match.\" Meanwhile, EncDec and ATT achieve a relatively higher kappa value between 0.8 and 1.0, which is \"near perfect match.\" Therefore, the labeling standard is considered clear, leading to a high match among labelers.For level 3 (acceptable ratio), X2Tree performs visibly better than other models. The best base method NRM achieves a level-3 ratio of 54.38%, while X2Tree achieves 65.53% with an increase of 11.15%. This improvement is mainly due to the fact that less irrelevant (level-2) responses are generated (34.02% vs. 44.98%), indicating that X2Tree generates more acceptable responses."}, {"heading": "5 Related Work", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves."}, {"heading": "6 Conclusion and Future Work", "text": "In this study, we proposed a tree-structured neural network that aims to predict tree structures and apply them to the generation of conversations for evaluation. The modules of parent-child dependence, tree canonization, and generalized beam search are developed to guarantee the success of this generative process. Finally, we argue that the proposed X2Tree model has broad applications. It can be used, for example, for the text-to-picture task, as a 2d image can be represented as a quadtree [17]. The only difference is that it is better to model the tree nodes for images as continuous random variables."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Theano: New Features and Speed Improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua  Bengio"], "venue": "In NIPS Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning Phrase Representations using RNN Encoderdecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Attention-Based Models for Speech Recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Introduction to algorithms", "author": ["Thomas H Cormen"], "venue": "MIT press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "The Average Height of Binary Trees and Other Simple Trees", "author": ["Philippe Flajolet", "Andrew Odlyzko"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Measuring Nominal Scale Agreement Among Many Raters", "author": ["Joseph L Fleiss"], "venue": "Psychological Bulletin,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1971}, {"title": "Unsupervised concept-to-text generation with hypergraphs", "author": ["Ioannis Konstas", "Mirella Lapata"], "venue": "In NAACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V. Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Geometric Modeling Using Octree Encoding", "author": ["Donald Meagher"], "venue": "Computer Graphics & Image Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1982}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wj Zhu"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Neural Responding Machine for Short-Text Conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y.Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Image Processing, Analysis, and Machine Vision", "author": ["Milan Sonka", "Vaclav Hlavac", "Roger Boyle"], "venue": "Cengage Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A Hierarchical Recurrent Encoder-Decoder For Generative Context- Aware Query Suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob G. Simonsen", "Jian-Yun Nie"], "venue": "In CIKM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In ACL,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking", "author": ["Tsung Hsien Wen", "Milica Gasic", "Dongho Kim", "Nikola Mrksic", "Pei Hao Su", "David Vandyke", "Steve Young"], "venue": "DIAL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning", "author": ["Matthew D. Zeiler"], "venue": "Rate Method. arXiv,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Top-down Tree Long Short-Term Memory Networks", "author": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata"], "venue": "In NAACL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Long Short-Term Memory Over Tree Structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 138, "endOffset": 145}, {"referenceID": 18, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 138, "endOffset": 145}, {"referenceID": 12, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 170, "endOffset": 174}, {"referenceID": 3, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 195, "endOffset": 198}, {"referenceID": 7, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 217, "endOffset": 224}, {"referenceID": 21, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 217, "endOffset": 224}, {"referenceID": 18, "context": "Since sequence is a basic structure for data representation, most of these studies focus on the Seq2Seq models [19, 13], which achieve the state-of-the-art performance for many tasks.", "startOffset": 111, "endOffset": 119}, {"referenceID": 12, "context": "Since sequence is a basic structure for data representation, most of these studies focus on the Seq2Seq models [19, 13], which achieve the state-of-the-art performance for many tasks.", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "For example, in semantic analysis of various digital data, sentences are usually parsed into trees [16], 2d and 3d photograph can be represented as a Quadtree [17] or a Octree [10] respectively, and Web pages are naturally coded as HTML tree.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "For example, in semantic analysis of various digital data, sentences are usually parsed into trees [16], 2d and 3d photograph can be represented as a Quadtree [17] or a Octree [10] respectively, and Web pages are naturally coded as HTML tree.", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "For example, in semantic analysis of various digital data, sentences are usually parsed into trees [16], 2d and 3d photograph can be represented as a Quadtree [17] or a Octree [10] respectively, and Web pages are naturally coded as HTML tree.", "startOffset": 176, "endOffset": 180}, {"referenceID": 23, "context": "[25] proposed treelstm and ldtreelstm via Tree LSTM activation functions in top-down fashion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a max-margin structure prediction architecture based on recursive neural networks, and demonstrated that it successfully parses sentences and understands scene images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] and Zhu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] extended the chainstructured LSTM to tree-structured LSTM, which is shown to be more effective in representing a tree structure as a latent vector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Similarly, to parse natural scene images [15], an image is first divided into segments, each of which corresponds to one leaf node in output tree.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "[19, 26] ) , and mostly focus on the tree-structured decoder for the generation of Tx.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[19, 26] ) , and mostly focus on the tree-structured decoder for the generation of Tx.", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "For an ordered tree, where ordering is specified for the children of each node, we transform it to a left-child right-sibling (LCRS) binary tree [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 12, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 17, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 11, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 5, "context": "In Ty, the average depth of nodes is O( \u221a |y|) [6], which is much smaller than the sentence length |y|.", "startOffset": 47, "endOffset": 50}, {"referenceID": 24, "context": "It is also easily developed by switching the chain-structured encoder with a tree-structured encoder [26].", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "Since the ordered trees and LCRS trees obtain one-to-one correspondence [5], it can be inferred that the element number |On| = |Ln|.", "startOffset": 72, "endOffset": 75}, {"referenceID": 18, "context": "We implemented the following four popular neuralbased conversation models for comparison: 1) Seq2Seq[19]: A RNN model that utilizes the last hidden state of the encoder as the initial hidden state of the decoder.", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "2) EncDec[3]: A RNN model that feeds the last hidden state of the encoder to every cell and softmax unit of the decoder.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "3) ATT[1]: A RNN model based on EncDec with attention signal.", "startOffset": 6, "endOffset": 9}, {"referenceID": 12, "context": "4) NRM[13]: Neural Responding Machine with both global and local schemes.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "In this study, a tree decoder grafted on EncDec [3] is implemented for evaluation (denoted as X2Tree).", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": "Here, we stress that this tree-decoder can be easily applied to the model [12], which summarizes multiple rounds of dialogues into a latent vector.", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "Our implementations are based on the Theano library [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "We applied one-layer GRU [3] with 1,024dimensional hidden states to {fk}k=1 and all baseline models.", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "As suggested in [13], the word embeddings for the encoders and decoders are learned separately, whose dimensions are set to 128 for all models.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "In training, we divided the corpus into mini-batches, each of which contains 128 pairs of posts and responses, and used ADADELTA [24] for optimization.", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "Thus, match-based metrics, for example the BLEU [11], are not appropriate.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "While perplexity is widely used in SMT, lower values of this measure do not lead to better responses [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Additionally, labeling agreement is evaluated by Fleiss\u2019 kappa [7] which is a measure of inter-rater consistency.", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "structured data) and output text [22, 8].", "startOffset": 33, "endOffset": 40}, {"referenceID": 7, "context": "structured data) and output text [22, 8].", "startOffset": 33, "endOffset": 40}, {"referenceID": 7, "context": "2) Image captioning models generate text descriptions of a given image [8, 23].", "startOffset": 71, "endOffset": 78}, {"referenceID": 21, "context": "2) Image captioning models generate text descriptions of a given image [8, 23].", "startOffset": 71, "endOffset": 78}, {"referenceID": 2, "context": "3) Sequence-to-sequence models are popular for chatbot and translation, which encode a source sequence to a context vector and decode the vector to a target sequence [3, 19, 13].", "startOffset": 166, "endOffset": 177}, {"referenceID": 18, "context": "3) Sequence-to-sequence models are popular for chatbot and translation, which encode a source sequence to a context vector and decode the vector to a target sequence [3, 19, 13].", "startOffset": 166, "endOffset": 177}, {"referenceID": 12, "context": "3) Sequence-to-sequence models are popular for chatbot and translation, which encode a source sequence to a context vector and decode the vector to a target sequence [3, 19, 13].", "startOffset": 166, "endOffset": 177}, {"referenceID": 15, "context": "4) Recursive neural networks which encode tree structures in bottom-up fashion have been developed for sentence classification [16, 20, 26].", "startOffset": 127, "endOffset": 139}, {"referenceID": 19, "context": "4) Recursive neural networks which encode tree structures in bottom-up fashion have been developed for sentence classification [16, 20, 26].", "startOffset": 127, "endOffset": 139}, {"referenceID": 24, "context": "4) Recursive neural networks which encode tree structures in bottom-up fashion have been developed for sentence classification [16, 20, 26].", "startOffset": 127, "endOffset": 139}, {"referenceID": 14, "context": "Note that some models generate tree in the bottom-up fashion, for example constituency parser [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "[19] used multi-layered LSTM as the encoder and the decoder for machine translation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed the encoder-decoder framework, where the context vector is fed to every unit in the decoder.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] proposed the Neural Responding Machine which fur-", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] built an end-to-end dialogue system using hierarchical neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed the Recursive Neural Tensor Network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] and Zhu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] extended the chain-structured LSTM to tree structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] proposed treelstm and ldtreelstm via Tree LSTM activation functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "For example, it can be applied for the text-to-image task, since a 2d picture can be represented as a Quadtree [17].", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Tree structures are commonly used in the tasks of semantic analysis and understanding over the data of different modalities, such as natural language, 2d or 3d graphics and images, or Web pages. Previous studies model the tree structures in a bottomup manner, where the leaf nodes (given in advance) are merged into internal nodes until they reach the root node. However, these models are not applicable when the leaf nodes are not explicitly specified ahead of prediction. Here, we introduce a neural machine for top-down generation of tree structures that aims to infer such tree structures without the specified leaf nodes. In this model, the history memories from ancestors are fed to a node to generate its (ordered) children in a recursive manner. This model can be utilized as a tree-structured decoder in the framework of \u201cX to tree\u201d learning, where X stands for any structure (e.g. chain, tree etc.) that can be represented as a latent vector. By transforming the dialogue generation problem into a sequence-to-tree task, we demonstrate the proposed X2Tree framework achieves a 11.15% increase of response acceptance ratio over the baseline methods.", "creator": "LaTeX with hyperref package"}}}