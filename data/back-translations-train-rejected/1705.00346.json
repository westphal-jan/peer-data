{"id": "1705.00346", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2017", "title": "Deep Learning in the Automotive Industry: Applications and Tools", "abstract": "Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-the-art in libraries, tools and infrastructures (e.\\,g.\\ GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.", "histories": [["v1", "Sun, 30 Apr 2017 17:17:44 GMT  (96kb,D)", "http://arxiv.org/abs/1705.00346v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.DC", "authors": ["andre luckow", "matthew cook", "nathan ashcraft", "edwin weill", "emil djerekarov", "bennie vorster"], "accepted": false, "id": "1705.00346"}, "pdf": {"name": "1705.00346.pdf", "metadata": {"source": "META", "title": "Automotive Deep Learning", "authors": ["Andre Luckow", "Matthew Cook", "Nathan Ashcraft", "Edwin Weill", "Emil Djerekarov", "Bennie Vorster"], "emails": [], "sections": [{"heading": null, "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "II. AUTOMOTIVE USE CASES", "text": "Deep learning techniques can be applied to many applications in the automotive industry. Computer vision, for example, is an area where deep learning systems have dramatically improved recently. Ng et al. [4] used vehicle and trace detection convolutionary neural networks that allow expensive sensors (e.g. LIDAR) to be replaced by cameras. Pomerleau [5] used neural networks to automatically control a vehicle by observing the input of a camera, a laser rangefinder and a real driver. In this section, we describe a number of automotive use cases for deep learning. Visual inspection in manufacturing: The increasing use of mobile devices and IoT sensors has led to a flood of image and video data, often maintained manually using tables and folders. Deep learning can help organize this data and improve data collection.Social media analytics: applications of computer vision can extend to social media analytics."}, {"heading": "III. BACKGROUND, TOOLS AND INFRASTRUCTURE", "text": "In this section, we provide some background information on deep learning and an overview of the tool landscape for the formation of neural networks."}, {"heading": "A. Background", "text": "These deep neural networks (DNNs) are particularly beneficial for unstructured data (which is the majority of the data) and complex, non-linear separable attribute spaces. Schmidhuber [6] provides a comprehensive overview of deep neural networks. DNNs have shown better results compared to existing techniques for image classification [7], language understanding, translation, speech recognition [8] and autonomous robots. Specialized neural networks have emerged for various applications, such as Convolutionary Neural Networks (CNN), which use pre-processes and tile image regions for improved image recognition. [Conversely, recursive neural networks are a hidden layer that is linked to itself for improved speech recognition."}, {"heading": "B. Deep Learning Libraries", "text": "Neural networks - especially deep networks with many hidden layers - are difficult to scale. Currently, their application / evaluation is more computationally intensive than other models. Figure 1 illustrates the different layers of an experienced learning system. GPUs have been shown to scale neural networks particularly well, but have their limits for larger image sizes. Several libraries rely on GPUs to optimize the formation of neural networks [14]. Both NVIDIA's cuDNN [15] and Intel's MKL [16] optimize critical deep learning operations, such as rotations. In addition, several high-level frameworks have been created - some of which provide integrated support for distributed training, while others rely on other distributed runtime engines for this purpose. Several higher-level deep learning libraries for different languages have emerged: Python / scikit-learn [17], Python / Pylearn2 / Theano [19], Java / Padmidfied [22] CNflow [Tuni24] [S]."}, {"heading": "C. Distributed Deep Learning", "text": "This year is the highest in the history of the country."}, {"heading": "D. Cloud Services", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "A. Experiments and Evaluation", "text": "In the following, we evaluate different framework conditions for the formation of deep neural networks. For experiments, we use a machine with 2 CPUs, a total of 8 cores, 128 GB of memory and a TITAN X GPU. Furthermore, we use Amazon Web Services GPU nodes (g2,8xlarge), which provide 32 cores, 60 GB of memory and 4 K520 GPUs [48]. For the training of the Caffe and Torch models, we use DIGITS [29] and the models supplied with it. For Tensorflow, we have adapted the provided AlexNet implementation [60]."}, {"heading": "B. Datasets", "text": "This year it is more than ever before."}, {"heading": "D. Social Media Analytics", "text": "Below is a CNN for recognizing vehicle models in social media data collected by Twitter. A Python application was developed to display the current image with its top five classifications predicted by the neural network. Further experiments were conducted with focus regions within the image to improve the classification. Further details are discussed in the following sections. The car data sets published by Stanford AI Lab consist of 16,185 images grouped into 196 categories of shape: Make, Model, Year. We have reduced the granularity of classes across 49 different car brands as we were primarily concerned with recognizing different brands. We used an upstream ImageNet GoogLeNet model from the Berkeley Vision and Learning Center (BVLC)."}, {"heading": "V. CONCLUSION AND ON-GOING RESEARCH", "text": "Deep learning enables computers to learn objects and representations, but it poses several challenges: it requires enormous amounts of data, new tools and infrastructures for calculation and data. We demonstrated that existing model architectures and transfer learning can be applied to solve computer vision problems in the automotive sector. In this paper we demonstrated the successful use of deep learning for visual inspection and social media analysis. We successfully demonstrated the trade-offs in training and deployment of deep neural networks in a variety of environments (on-premise, cloud). We demonstrated the effectiveness of the training classifier, which achieves an accuracy of 85% in real life. Several challenges for a wider use of deep learning remain: the availability of marked data is critical for the development and refinement of deep learning systems. Unfortunately, the publicly available data sets (except ImageNet) are not sufficient for advanced systems, e.g. for autonomous driving."}], "references": [{"title": "Deep learning. Book in preparation for", "author": ["Yoshua Bengio", "Ian J. Goodfellow", "Aaron Courville"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "The elements of statistical learning: data mining, inference, and prediction. Springer series in statistics", "author": ["Trevor J. Hastie", "Robert John Tibshirani", "Jerome H. Friedman"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Automotive big data: Applications, workloads and infrastructures", "author": ["Andre Luckow", "Ken Kennedy", "Fabian Manhardt", "Emil Djerekarov", "Bennie Vorster", "Amy Apon"], "venue": "In Proceedings of IEEE Conference on Big Data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "An Empirical Evaluation of Deep Learning on Highway Driving", "author": ["B. Huval", "T. Wang", "S. Tandon", "J. Kiske", "W. Song", "J. Pazhayampallil", "M. Andriluka", "P. Rajpurkar", "T. Migimatsu", "R. Cheng-Yue", "F. Mujica", "A. Coates", "A.Y. Ng"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Rapidly adapting artificial neural networks for autonomous navigation", "author": ["Dean Pomerleau"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent"], "venue": "CoRR, abs/1206.5538,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey Hinton", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei-Fei Li"], "venue": "CoRR, abs/1409.0575,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "Nature, 529(7587):484\u2013489,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["Nicolas Vasilache", "Jeff Johnson", "Micha\u00ebl Mathieu", "Soumith Chintala", "Serkan Piantino", "Yann LeCun"], "venue": "CoRR, abs/1412.7580,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Deep Neural Network", "author": ["Gennady Fedorov", "Vadim Pirogov", "Nikita Shustrov"], "venue": "Technical Preview for Intel Math Kernel Library (Intel MKL)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Dato\u2019s Deep Learning Toolkit", "author": ["Danny Bickson"], "venue": "http://blog.dato.com/ deep-learning-blog-post,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Neuralnet: Training of neural networks", "author": ["Frauke G\u00fcnther", "Stefan Fritsch"], "venue": "The R Journal,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross B. Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "CoRR, abs/1408.5093,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["Dong Yu", "Adam Eversole", "Mike Seltzer", "Kaisheng Yao", "Oleksii Kuchaiev", "Yu Zhang", "Frank Seide", "Zhiheng Huang", "Brian Guenter", "Huaming Wang", "Jasha Droppo", "Geoffrey Zweig", "Chris Rossbach", "Jie Gao", "Andreas Stolcke", "Jon Currey", "Malcolm Slaney", "Guoguo Chen", "Amit Agarwal", "Chris Basoglu", "Marko Padmilac", "Alexey Kamenev", "Vladimir Ivanov", "Scott Cypher", "Hari Parthasarathi", "Bhaskar Mitra", "Baolin Peng", "Xuedong Huang"], "venue": "Technical report,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "CoRR, abs/1512.01274,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Keras: Deep Learning library for Theano and TensorFlow", "author": ["Fran\u00e7ois Chollet et. al"], "venue": "http://keras.io/,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Lasagne: Neural Network Tools for Theano", "author": ["Jan Schl\u00fcter et. al"], "venue": "https: //github.com/Lasagne/Lasagne,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Accelerating deep convolutional neural networks using specialized hardware", "author": ["Kalin Ovtcharov", "Olatunji Ruwase", "Joo-Young Kim", "Jeremy Fowers", "Karin Strauss", "Eric S. Chung"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Deep image: Scaling up image recognition", "author": ["Ren Wu", "Shengen Yan", "Yi Shan", "Qingqing Dang", "Gang Sun"], "venue": "CoRR, abs/1501.02876,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Strategies and principles of distributed machine learning on big data", "author": ["Eric P. Xing", "Qirong Ho", "Pengtao Xie", "Dai Wei"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Spark: Cluster computing with working sets", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Michael J. Franklin", "Scott Shenker", "Ion Stoica"], "venue": "In Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Spark Multilayer perceptron classifier", "author": ["Alexander Ulanov"], "venue": "https://spark.apache.org/docs/latest/ml-classification-regression.html# multilayer-perceptron-classifier,https://issues.apache.org/jira/browse/", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "SparkNet: Training Deep Networks in Spark", "author": ["P. Moritz", "R. Nishihara", "I. Stoica", "M.I. Jordan"], "venue": "ArXiv e-prints:", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Distributed TensorFlow: Scaling Google\u2019s Deep Learning", "author": ["Christopher Smith", "Ushnish De", "Christopher Nguyen"], "venue": "Library on Spark. https://arimo.com/machine-learning/deep-learning/2016/ arimo-distributed-tensorflow-on-spark/,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns", "author": ["Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Firecaffe: near-linear acceleration of deep neural network training on compute clusters", "author": ["Forrest N. Iandola", "Khalid Ashraf", "Matthew W. Moskewicz", "Kurt Keutzer"], "venue": "CoRR, abs/1511.00175,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Distributed tensorflow with MPI", "author": ["Abhinav Vishnu", "Charles Siegel", "Jeffrey Daily"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Distributed TensorFlow: Scaling Google\u2019s Deep Learning", "author": ["Christopher Smith", "Christopher Nguyen", "Ushnish De"], "venue": "Library on Spark. https://arimo.com/machine-learning/deep-learning/2016/ arimo-distributed-tensorflow-on-spark/,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "New g2 instance type with 4x more gpu power", "author": ["Jeff Barr"], "venue": "https://aws.amazon.com/blogs/aws/ new-g2-instance-type-with-4x-more-gpu-power/,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark", "author": ["Sebastian Houben", "Johannes Stallkamp", "Jan Salmen", "Marc Schlipsing", "Christian Igel"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["Andreas Geiger", "Philip Lenz", "Raquel Urtasun"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2012}, {"title": "3d object representations for fine-grained categorization", "author": ["Jonathan Krause", "Michael Stark", "Jia Deng", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2013}, {"title": "Automotive big data: Applications, workloads and infrastructures", "author": ["Andre Luckow", "Ken Kennedy", "Fabian Manhardt", "Emil Djerekarov", "Bennie Vorster", "Amy Apon"], "venue": "In Big Data (Big Data),", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "shift. CoRR,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2014}, {"title": "Rethinking the Inception Architecture for Computer Vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2015}, {"title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size", "author": ["Forrest N. Iandola", "Matthew W. Moskewicz", "Khalid Ashraf", "Song Han", "William J. Dally", "Kurt Keutzer"], "venue": null, "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2016}, {"title": "3d object representations for fine-grained categorization", "author": ["Jonathan Krause", "Michael Stark", "Jia Deng", "Li Fei-Fei"], "venue": "IEEE Workshop on 3D Representation and Recognition,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2013}, {"title": "Selective search for object recognition", "author": ["J.R.R. Uijlings", "K.E.A. van de Sande", "T. Gevers", "A.W.M. Smeulders"], "venue": "International Journal of Computer Vision,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning [1], [2] refers to a set of machine learning algorithms that utilize large neural networks with many hidden layers (also referred", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "Deep learning [1], [2] refers to a set of machine learning algorithms that utilize large neural networks with many hidden layers (also referred", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "KMeans, SVM and logistic regression) [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "[4] utilized convolutional neural networks for vehicle and lane detection enabling the replacement of expensive sensors (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Pomerleau [5] used neural networks to automatically train a vehicle to drive by observing the input from a camera, a laser rangefinder and a real driver.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Schmidhuber [6] provides an extensive survey of deep neural networks.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "DNNs have shown superior results when compared to existing techniques for image classifications [7], language understanding, translation, speech recognition [8], and autonomous robots.", "startOffset": 96, "endOffset": 99}, {"referenceID": 7, "context": "DNNs have shown superior results when compared to existing techniques for image classifications [7], language understanding, translation, speech recognition [8], and autonomous robots.", "startOffset": 157, "endOffset": 160}, {"referenceID": 8, "context": "Promising advances have been made in automatically learning features (also referred to as representation learning), through auto-encoders, sparse coders and other techniques (see [9], [10]).", "startOffset": 179, "endOffset": 182}, {"referenceID": 9, "context": "Promising advances have been made in automatically learning features (also referred to as representation learning), through auto-encoders, sparse coders and other techniques (see [9], [10]).", "startOffset": 184, "endOffset": 188}, {"referenceID": 10, "context": "There have been great advances in deep learning observable in the rapid improvements of image classification accuracy in the ImageNet competition [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "57 % for Microsoft\u2019s Residual Nets approach [12]) was better than that of a human (5.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "Another example is the recent success of AlphaGo [13] in mastering the Go Game.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "Several libraries rely on GPUs for optimizing the training of neural networks [14].", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "Both NVIDIA\u2019s cuDNN [15] and Intel\u2019s MKL [16] optimize critical deep learning operations, e.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 179, "endOffset": 183}, {"referenceID": 19, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 21, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 229, "endOffset": 233}, {"referenceID": 22, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 261, "endOffset": 265}, {"referenceID": 23, "context": "Several higher-level deep learning libraries for different languages emerged: Python/scikit-learn [17], Python/Pylearn2/Theano [18], Python/Dato [19], Java/DL4J [20], R/neuralnet [21], Caffe [22], Tensorflow [23], Microsoft CNTK [24], Amazon DSSTNE [25], MXNet [26], Lua/Torch [27] and Baidu\u2019s PaddlePaddle [28].", "startOffset": 277, "endOffset": 281}, {"referenceID": 24, "context": "Further, several high-level frameworks emerged: Keras [30] provides a unified abstraction for specifying deep learning networks agnostic of the backend.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "Lasagne [31] is another example for a Theano-based library.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "To scale neural networks, the usage of GPUs [15], FPGAs [32], multicore machines and distributed clusters (e.", "startOffset": 56, "endOffset": 60}, {"referenceID": 27, "context": "DistBelief [33], Baidu [34]) have been proposed.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "DistBelief [33], Baidu [34]) have been proposed.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "[35] for a overview).", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Model updates can be done synchronously or asynchronously (Hogwild [36]).", "startOffset": 67, "endOffset": 71}, {"referenceID": 31, "context": "Hadoop [37] and Spark [38] emerged as de-facto-standard for data-parallel applications [3].", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "Hadoop [37] and Spark [38] emerged as de-facto-standard for data-parallel applications [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 32, "context": "There is ongoing work to implement artificial neural networks in Spark [39] as part of its MLlib machine learning library [40].", "startOffset": 71, "endOffset": 75}, {"referenceID": 33, "context": "SparkNet [42] utilizes mini-batch parallelization to compute the gradient on RDD-local data on worker-level.", "startOffset": 9, "endOffset": 13}, {"referenceID": 34, "context": "Similarly, TensorSpark [43] utilizes a parameter server approach to implement a \u201cDownpourSGD\u201d (see DistBelief [33]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": "Similarly, TensorSpark [43] utilizes a parameter server approach to implement a \u201cDownpourSGD\u201d (see DistBelief [33]).", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "Both Tensorflow [23] and CNTK [12] provide different distributed optimizer implementations.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Both Tensorflow [23] and CNTK [12] provide different distributed optimizer implementations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 35, "context": "The 1-bit SGD [44] reduces the amount of data for model updates significantly by quantizing the gradients to 1-bit.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "In addition to the frameworks described above, several other systems exist: FireCaffe [45] is another framework built on", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "top of Caffe; [46] and [47] provide alternative distributed Tensorflow implementations.", "startOffset": 14, "endOffset": 18}, {"referenceID": 38, "context": "top of Caffe; [46] and [47] provide alternative distributed Tensorflow implementations.", "startOffset": 23, "endOffset": 27}, {"referenceID": 39, "context": "Amazon Web Services Elastic Compute Cloud (EC2) is a service that provides cloud computing with resizable compute capabilities including up to four K520 Grid GPUs [48].", "startOffset": 163, "endOffset": 167}, {"referenceID": 43, "context": "Cars [64] 196 16,185 1.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "87 GB (LMDB) ImageNet 2012 [11] 1000 1,281,167 130 GB (LMDB)", "startOffset": 27, "endOffset": 31}, {"referenceID": 40, "context": "Traffic Signs [61] 43 1,200 54.", "startOffset": 14, "endOffset": 18}, {"referenceID": 41, "context": "Places [62] 205 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 39, "context": "8xlarge), which provides 32 cores, 60 GB memory and 4 K520 GPUs [48].", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "In particular for advanced applications, such as autonomous driving, it is essential to create suitable datasets, as datasets like Traffic Signs [61], Places [62] and Kitti [63], are designed for benchmarking primarily.", "startOffset": 145, "endOffset": 149}, {"referenceID": 41, "context": "In particular for advanced applications, such as autonomous driving, it is essential to create suitable datasets, as datasets like Traffic Signs [61], Places [62] and Kitti [63], are designed for benchmarking primarily.", "startOffset": 158, "endOffset": 162}, {"referenceID": 42, "context": "In particular for advanced applications, such as autonomous driving, it is essential to create suitable datasets, as datasets like Traffic Signs [61], Places [62] and Kitti [63], are designed for benchmarking primarily.", "startOffset": 173, "endOffset": 177}, {"referenceID": 44, "context": "For data preprocessing and structured queries, we rely on Hadoop and Spark [65]; for deep learning we rely on some GPU nodes.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Network Number Parameters Number Layers ImageNet Top 5 Error AlexNet (2012) [7] 60 mio.", "startOffset": 76, "endOffset": 79}, {"referenceID": 45, "context": "GoogLeNet (2014) [66], [67] 5 mio.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": "VGG (2014) [68] \u223c140 mio.", "startOffset": 11, "endOffset": 15}, {"referenceID": 47, "context": "Inception v3 (2015) [69] 25 mio.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "58 % Deep Residual Learning (2015) [12] \u223c60 mio.", "startOffset": 35, "endOffset": 39}, {"referenceID": 48, "context": ", using compressing techniques [70].", "startOffset": 31, "endOffset": 35}, {"referenceID": 49, "context": "The Cars dataset released by the Stanford AI Lab [71] consists of 16,185 images grouped into 196 categories of the form: Make, Model, Year.", "startOffset": 49, "endOffset": 53}, {"referenceID": 50, "context": "To process social media data, we implemented a two version: (i) the standard version processes the image is processed in its original form, (ii) the region-search version adds an additional pre-processing step: First, we conduct a selective search [73] on the image to isolate object regions within the", "startOffset": 248, "endOffset": 252}], "year": 2017, "abstractText": "Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-theart in libraries, tools and infrastructures (e. g. GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.", "creator": "LaTeX with hyperref package"}}}