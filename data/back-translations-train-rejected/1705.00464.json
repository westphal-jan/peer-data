{"id": "1705.00464", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Speech-Based Visual Question Answering", "abstract": "This paper introduces the task of speech-based visual question answering (VQA), that is, to generate an answer given an image and an associated spoken question. Our work is the first study of speech-based VQA with the intention of providing insights for applications such as speech-based virtual assistants. Two methods are studied: an end to end, deep neural network that directly uses audio waveforms as input versus a pipelined approach that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Our main findings are 1) speech-based VQA achieves slightly worse results than the extensively-studied VQA with noise-free text and 2) the end-to-end model is competitive even though it has a simple architecture. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find speech-based VQA to be tolerant of noise at reasonable levels. The speech dataset, code, and supplementary material will be released to the public.", "histories": [["v1", "Mon, 1 May 2017 10:43:28 GMT  (2640kb,D)", "http://arxiv.org/abs/1705.00464v1", "In review"], ["v2", "Sat, 16 Sep 2017 03:43:20 GMT  (2690kb,D)", "http://arxiv.org/abs/1705.00464v2", null]], "COMMENTS": "In review", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["ted zhang", "dengxin dai", "tinne tuytelaars", "marie-francine moens", "luc van gool"], "accepted": false, "id": "1705.00464"}, "pdf": {"name": "1705.00464.pdf", "metadata": {"source": "META", "title": "Speech-Based Visual Question Answering", "authors": ["Ted Zhang", "Dengxin Dai", "Tinne Tuytelaars", "Luc Van Gool"], "emails": ["tedz.cs@gmail.com", "dai@vision.ee.ethz.ch", "tinne.tuytelaars@esat.kuleuven.be", "sien.moens@cs.kuleuven.be", "vangool@vision.ee.ethz.ch"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent years have shown that this system is a system in which the interests of the individual are paramount and in which the interests of the individual are paramount."}, {"heading": "2 RELATED WORKS", "text": "Our work is generally relevant for answering visual questions, integrating vision and speech, and providing consistent speech recognition."}, {"heading": "2.1 Visual Question Answering", "text": "The initial introduction of VQA to the AI community [6, 19] was motivated by the desire to develop intelligent systems that can understand the world more holistically. To complete the task of VQA, it was necessary to understand both a textual question and a visual scene. However, it was only with the introduction of VQA 1.0 [3] that the application became mainstream in the Computer Vision and Natural Language Processing (NLP) communities. Recently, popular exploration topics have focused on the development of attention models. Attention models have been popularized by their success with the NLP community in machine translation [5]."}, {"heading": "2.2 Integration of Speech and Vision", "text": "Pixeltones [16] and Image spirit [7] are examples that use voice commands to control image processing and semantic segmentation; there is also scientific work [13-15, 27] and an app [1] that uses language to provide image descriptions; their tasks and algorithms are different from ours. We are investigating the potential of integrating language and vision in the context of VQA and striving for a common understanding of language and vision; however, these approaches use speech recognition for data acquisition or result refinement; our work also shares similarities with visually grounded speech understanding or speech recognition; the next in this direction is [12], where a deep model is learned of speech via captions for speech-based image reproduction. In a broader context of integrating sound and vision, Soundnet [4] transfers visual information into acoustic representations, but this differs from our work as its ultimate goal is to label a sound."}, {"heading": "2.3 End-To-End Speech Recognition", "text": "Over the past decade, deep learning has enabled many areas of artificial intelligence to replace traditional handcrafted functions and pipeline systems with end-to-end models. As speech recognition is typically seen as a sequence to the sequence transmission problem, i.e. an input sequence that predicts an output sequence, the application of LSTM and the CTC [8, 11] immediately demonstrated the success needed to justify its superiority over conventional methods.The current state of the art in ASR systems such as DeepSpeech2 [2] uses stacked bi-directional recurring neural networks (RNNNs) in conjunction with Convolutionary Neural Networks (CNNs), as these deep neural architectures all share the ability to perform back propagation. Although our work does not attempt to predict sequences or use CTCs, our approach and integration of language and vision will nevertheless be enabled by the same basic principle of working mentioned above."}, {"heading": "3 MODEL", "text": "Two models are used in this work, they are henceforth referred to as TextMod and SpeechMod. TextMod and SpeechMod differ only in their language components, leaving the rest of the architecture the same. On the language side, TextMod takes a series of hot encodings as input, followed by an embedding layer that is learned from scratch, followed by an LSTM encoder, and ends with a dense layer. Architecturally, it is similar to [3] with some minor adjustments. The language side of SpeechMod takes the raw waveform as input and pushes it through a series of 1D convolutions. The main consideration when selecting the layer parameters is that the last evolution must be the dimensions of (x, 512), whenx must be a positive number, but not too large, because we treat it as a sequence fed into an LSTM. Sequences that generally make it difficult to extract the M pattern into the extraction pattern."}, {"heading": "4 DATA", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 EXPERIMENTS", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "6 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Results", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is not a country, but a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in which is a country,"}, {"heading": "6.2 Future Work", "text": "A simple approach to improving the end-to-end model is to enlarge the data. It is widely accepted that the effectiveness of neural architectures is data-driven, so training with noisy data and different speakers makes the model more robust to input during runtime. There are just as many options for improving the architecture. Feature extractors, attention mechanisms, or any combination of techniques can be incorporated into the deep learning mainstream."}, {"heading": "7 CONCLUSION", "text": "We propose a language-based visual answer to questions and present two approaches to solving this problem, one of which can be trained end-to-end on audio inputs. Despite its simple architecture, the end-to-end method works well if the test data has audio signatures comparable to the training data. Similarly, both methods suffer performance losses when introducing noise. An ASR pipeline method tolerates different inputs much better because it normalizes the input variance in text before executing the VQA module. We publish the speech data set and invite the multimedia research community to explore the intersection of speech and vision."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer Vision (ICCV)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "SoundNet: Learning Sound Representations from Unlabeled Video", "author": ["Yusuf Aytar", "Carl Vondrick", "Antonio Torralba"], "venue": "CoRR abs/1610.09001", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "ImageSpirit: Verbal Guided Image Parsing", "author": ["Ming-Ming Cheng", "Shuai Zheng", "Wen-Yan Lin", "Vibhav Vineet", "Paul Sturgess", "Nigel Crook", "Niloy J. Mitra", "Philip Torr"], "venue": "ACM Trans. Graph. 34,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "An Application of Recurrent Neural Networks to Discriminative Keyword Spotting", "author": ["Santiago Fern\u00e1ndez", "Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Connectionist temporal classification: labelling unsegmented sequence  data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Unsupervised Learning of Spoken Language with Visual Context", "author": ["David Harwath", "Antonio Torralba", "James Glass"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Speech-based annotation and retrieval of digital photographs", "author": ["Timothy J. Hazen", "Brennan Sherry", "Mark Adler"], "venue": "In INTERSPEECH", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Video content annotation using visual analysis and a large semantic knowledgebase", "author": ["A. Hoogs", "J. Rittscher", "G. Stein", "J. Schmiederer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "A Semantics-Based Approach for Speech Annotation of Images", "author": ["D.V. Kalashnikov", "S. Mehrotra", "Jie Xu", "N. Venkatasubramanian"], "venue": "IEEE Trans. Knowl. Data Eng. 23,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "PixelTone: a multimodal interface for image editing", "author": ["Gierad P Laput", "Mira Dontcheva", "Gregg Wilensky", "Walter Chang", "Aseem Agarwala", "Jason Linder", "Eytan Adar"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Dual Attention Networks for Multimodal Reasoning and Matching", "author": ["Hyeonseob Nam", "Jung-Woo Ha", "Jeonghee Kim"], "venue": "CoRR abs/1611.00471", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society. IEEE Catalog No.: CFP11SRW- USB", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Speech Is 3x Faster than Typing for English and Mandarin Text Entry on Mobile Devices", "author": ["Sherry Ruan", "Jacob O Wobbrock", "Kenny Liou", "Andrew Ng", "James Landay"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "A Dataset and Taxonomy for Urban Sound Research", "author": ["Justin Salamon", "Christopher Jacoby", "Juan Pablo Bello"], "venue": "In Proceedings of the ACM International Conference on Multimedia, MM \u201914,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Show&Tell: a semi-automated image annotation system", "author": ["R.K. Srihari", "Zhongfei Zhang"], "venue": "MultiMedia, IEEE 7,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Marijn F Stollenga", "Jonathan Masci", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In European Conference on Computer", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The recent years have witnessed great advances in computer vision, natural language processing, and speech recognition thanks to the advances in deep learning [17] and abundance of data [3, 24].", "startOffset": 186, "endOffset": 193}, {"referenceID": 22, "context": "Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning [29], movie-to-book alignment [35], and visual question answering (VQA) [3].", "startOffset": 147, "endOffset": 151}, {"referenceID": 28, "context": "Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning [29], movie-to-book alignment [35], and visual question answering (VQA) [3].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning [29], movie-to-book alignment [35], and visual question answering (VQA) [3].", "startOffset": 219, "endOffset": 222}, {"referenceID": 17, "context": "Pertaining to practical applications, voice-user interface (VUI) has become more commonplace, and people are increasingly taking advantage of its characteristics; it is natural, hands-free, eyes-free, far more mobile and even faster than typing on certain devices [23].", "startOffset": 264, "endOffset": 268}, {"referenceID": 14, "context": "The initial introduction of VQA into the AI community [6, 19] was motivated by a desire to build intelligent systems that can understand the world more holistically.", "startOffset": 54, "endOffset": 61}, {"referenceID": 0, "context": "0 [3] that the application took mainstream in the computer vision and natural language processing (NLP) communities.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "Attention models were popularized by its success with the NLP community in machine translation [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 21, "context": "[Elaborate if needed on attention] They eventually found a place in the computer vision community in works such as [20, 28, 32].", "startOffset": 115, "endOffset": 127}, {"referenceID": 25, "context": "[Elaborate if needed on attention] They eventually found a place in the computer vision community in works such as [20, 28, 32].", "startOffset": 115, "endOffset": 127}, {"referenceID": 26, "context": "Stacked Attention Network [33] learns an attention mechanism based on the of the question\u2019s encoding to determine the salient regions in an image.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "More sophisticated attention-centric models such as [18, 21, 30] were since then developed.", "startOffset": 52, "endOffset": 64}, {"referenceID": 15, "context": "More sophisticated attention-centric models such as [18, 21, 30] were since then developed.", "startOffset": 52, "endOffset": 64}, {"referenceID": 23, "context": "More sophisticated attention-centric models such as [18, 21, 30] were since then developed.", "startOffset": 52, "endOffset": 64}, {"referenceID": 24, "context": "Many models [31, 33, 34] use an element-wise multiplication to pool these modalities, but Lu et Al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 26, "context": "Many models [31, 33, 34] use an element-wise multiplication to pool these modalities, but Lu et Al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 27, "context": "Many models [31, 33, 34] use an element-wise multiplication to pool these modalities, but Lu et Al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 13, "context": "[18] and Fukui et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[9] have shown much success in using more complex methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Pixeltone [16] and Image spirit [7] are examples that use voice commands to guide image processing and semantic segmentation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "Pixeltone [16] and Image spirit [7] are examples that use voice commands to guide image processing and semantic segmentation.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 10, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 11, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 20, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 8, "context": "The closest one in this vein is [12], in .", "startOffset": 32, "endOffset": 36}, {"referenceID": 1, "context": "In a broader context of integration of sound and vision, Soundnet [4] transfers visual information into sound representations, but this differs from our work because their end goal is to label a sound.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "given an input sequence, predict an output sequence, the application of LSTM and the CTC [8, 11] promptly showed the sucess needed to justify its superiority over traditional methods.", "startOffset": 89, "endOffset": 96}, {"referenceID": 7, "context": "given an input sequence, predict an output sequence, the application of LSTM and the CTC [8, 11] promptly showed the sucess needed to justify its superiority over traditional methods.", "startOffset": 89, "endOffset": 96}, {"referenceID": 0, "context": "Architecturally, it is similar to [3] with some minor adjustments.", "startOffset": 34, "endOffset": 37}, {"referenceID": 19, "context": "On the visual side, both models start by taking as input the 4096 dimensional vector of the last layer of VGG19 [26] followed by a sin-", "startOffset": 112, "endOffset": 116}, {"referenceID": 18, "context": "The noise we mixed with the original speech files is selected randomly from the Urban8K dataset [25].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "For SpeechMod, the only preprocessing step is to scale each waveform to a range of [-256, 256] as done by [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "For TextMod, the standard preprocessing steps from [3] were followed.", "startOffset": 51, "endOffset": 54}, {"referenceID": 16, "context": "We use Kaldi [22] for ASR, due to its open-source codebase and popularity with the speech research community.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "I Only [3] 28.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "It is documented in [3, 10] the type of bias in VQA 1.", "startOffset": 20, "endOffset": 27}, {"referenceID": 6, "context": "It is documented in [3, 10] the type of bias in VQA 1.", "startOffset": 20, "endOffset": 27}], "year": 2017, "abstractText": "This paper introduces the task of speech-based visual question answering (VQA), that is, to generate an answer given an image and an associated spoken question. Our work is the first study of speechbased VQA with the intention of providing insights for applications such as speech-based virtual assistants. Two methods are studied: an end to end, deep neural network that directly uses audio waveforms as input versus a pipelined approach that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Our main findings are 1) speech-based VQA achieves slightly worse results than the extensively-studied VQA with noise-free text and 2) the end-to-end model is competitive even though it has a simple architecture. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find speech-based VQA to be tolerant of noise at reasonable levels. The speech dataset, code, and supplementary material will be released to the public. 1", "creator": "LaTeX with hyperref package"}}}