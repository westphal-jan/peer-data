{"id": "1705.00522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach", "abstract": "The Residual Quantization (RQ) framework is revisited where the quantization distortion is being successively reduced in multi-layers. Inspired by the reverse-water-filling paradigm in rate-distortion theory, an efficient regularization on the variances of the codewords is introduced which allows to extend the RQ for very large numbers of layers and also for high dimensional data, without getting over-trained. The proposed Regularized Residual Quantization (RRQ) results in multi-layer dictionaries which are additionally sparse, thanks to the soft-thresholding nature of the regularization when applied to variance-decaying data which can arise from de-correlating transformations applied to correlated data. Furthermore, we also propose a general-purpose pre-processing for natural images which makes them suitable for such quantization. The RRQ framework is first tested on synthetic variance-decaying data to show its efficiency in quantization of high-dimensional data. Next, we use the RRQ in super-resolution of a database of facial images where it is shown that low-resolution facial images from the test set quantized with codebooks trained on high-resolution images from the training set show relevant high-frequency content when reconstructed with those codebooks.", "histories": [["v1", "Mon, 1 May 2017 13:59:04 GMT  (656kb,D)", "http://arxiv.org/abs/1705.00522v1", "To be presented at SPARS 2017, Lisbon, Portugal"]], "COMMENTS": "To be presented at SPARS 2017, Lisbon, Portugal", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["sohrab ferdowsi", "slava voloshynovskiy", "dimche kostadinov"], "accepted": false, "id": "1705.00522"}, "pdf": {"name": "1705.00522.pdf", "metadata": {"source": "CRF", "title": "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach", "authors": ["Sohrab Ferdowsi", "Slava Voloshynovskiy", "Dimche Kostadinov"], "emails": ["Dimche.Kostadinov}@unige.ch"], "sections": [{"heading": null, "text": "Regulated residual quantification: a multi-layered, sparse dictionary for learning approaches Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov Department of Computer Science, University of Geneva, Switzerland {Sohrab.Ferdowsi, svolos, Dimche.Kostadinov} @ unige.chI. INTRODUCTIONThe quantification of residual errors from an earlier level of quantification was considered in signal processing for various applications, e.g. image coding. This problem was extensively investigated in the 1980s and 1990s (see [1] and [4]), but due to the severe overmatch, its efficiency was limited for more modern applications with larger scales. In practice, it was not possible to train more than a few layers. In particular, at high dimensions, the code books learned on a training set were unable to quantify a statistically similar test set."}, {"heading": "II. BACKGROUND: QUANTIZATION OF INDEPENDENT GAUSSIAN SOURCES", "text": "In view of the fact that n independent Gaussian sources Xj's are distributed with variance \u03c32j as Xj \u0445 N (0, \u03c32j), the optimal rate allocation is derived from the distortion theory for this constellation as (Ch. 10 of [2]): Dj = {\u03b3, if \u03c32j > \u03b3, \u03c32j, if \u03c3 2 j < \u03b3, (1) where \u03b3 should be chosen to guarantee that \u0445 n j = 1Dj = D. Therefore, the optimal code word variance \u03c32Cj is a soft threshold of \u03c3 2 j: \u03c32Cj = (\u03c32j \u2212 \u03b3) + = {\u03c32j \u2212 \u03b3, if \u03c32j > \u03b3, 0, if \u03c32j < \u03b3. (2) This means that sources with variances smaller than \u04212Cj should not be allocated any rate at all."}, {"heading": "III. THE PROPOSED APPROACH: RRQ", "text": "Instead of the standard K-mean used in RQ, we first propose its regularized version and then use it as a building block for the RRQ.A. VR-Kmeans algorithm. After we have decorrelated the data points, e.g. using the pre-processing suggested in Fig. 2, and have collected them in columns of X with \u03c32j in each dimension, we define S, diag ([\u03c32C1, \u00b7 \u00b7, 2 Cn]) from Equation 2. For codebook C, to regulate only the diagonal elements of CCT, we define Pj with all elements as zeros except for P (j, j) = 1. We formulate the variance-regulated K-mean algorithm with the parameter \u03bb, A1 2 | | X \u2212 CA | 2F + 1 2 \u00b2 CCT with all elements as nulos."}, {"heading": "B. Regularized Residual Quantization (RRQ) algorithm", "text": "For a fixed number of centroids K (l) on layer l and D (l \u2212 1) j, the RRQ first specifies the distortion of the previous stage of quantification for each dimension, followed by the calculation of an active set of dimensions A (l) \u03b3 = {j: 1 6 j 6 n | \u03c32j > \u03b3 \u043c}. The algorithm then proceeds to quantify the residual quantity of stage l \u2212 1 using the VR-Kmeans algorithm described above until a desired level L can be selected, which can be selected on the basis of distortion constraints or a permissible total rate budget."}, {"heading": "IV. EXPERIMENTS", "text": "Fig. 1 and Table I compare the performance of VR-Kmeans with K-means in the quantification of high-dimensional independent data decaying from variance. In fact, in many practical cases, the correlated data behave similarly after an energy-compressing and decorative transformation. As can be seen in this figure, the VR-Kmeans regulates the variance, resulting in a reduced distortion gap in tensile experiments.Fig. 3 demonstrates the performance of the RRQ at the high-resolution of similar images. It is clear from this figure that the radio frequency content lost during down sampling can be reconstructed from a multi-layered codebook learned from full-resolution facial imagers.ar Xiv: 170 5.00 522v 1 [cs.L G] 1M ay2 017"}], "references": [{"title": "Advances in residual vector quantization: a review", "author": ["C.F. Barnes", "S.A. Rizvi", "N.M. Nasrabadi"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Elements of Information Theory 2nd Edition", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley-Interscience, 2 edition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Image coding using vector quantization: a review", "author": ["N.M. Nasrabadi", "R.A. King"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}], "referenceMentions": [{"referenceID": 0, "context": ", see [1] and [4]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": ", see [1] and [4]).", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "10 of [2]):", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "3: Super-resolution using RRQ on the CroppedYale [3] set with L = 100 layers with K = 256 centroids each: After preprocessing as proposed in Fig.", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "Quantizing the residual errors from a previous level of quantization has been considered in signal processing for different applications, e.g., image coding. This problem was extensively studied in the 80\u2019s and 90\u2019s (e.g., see [1] and [4]). However, due to strong over-fitting, its efficiency was limited for more modern applications with larger scales. In practice, it was not feasible to train more than a couple of layers. Particularly at high dimensions, the codebooks learned on a training set were not able to quantize a statistically similar test set. Inspired by an insight from rate-distortion theory, we introduce an effective regularization for the framework of Residual Quantization (RQ), making it capable to learn multiple layers of codebooks with many stages. Moreover, the introduced framework effectively deals with high dimensions making it feasible to go beyond patch level processing and deals with entire images. The proposed regularization makes use of the problem of optimal rate allocation for asymptotic case of Gaussian independent sources, which is reviewed next.", "creator": "LaTeX with hyperref package"}}}