{"id": "1705.00571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines", "abstract": "This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.", "histories": [["v1", "Mon, 1 May 2017 15:57:41 GMT  (105kb,D)", "http://arxiv.org/abs/1705.00571v1", "5 pages, to Appear in the Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC"]], "COMMENTS": "5 pages, to Appear in the Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrew moore", "paul rayson"], "accepted": false, "id": "1705.00571"}, "pdf": {"name": "1705.00571.pdf", "metadata": {"source": "CRF", "title": "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines", "authors": ["Andrew Moore", "Paul Rayson"], "emails": ["initial.surname@lancaster.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Task 5 Track 2 of SemEval (2017) was aimed at predicting the mood of news headlines with respect to companies mentioned in the headlines, which can be considered a financial aspect-based sentiment task (Nasukawa and Yi, 2003). The main motivation of this task is to find specific features and learning algorithms that will work better in this area, since aspect-based sentiment analysis tasks have already been performed at SemEval (Pontiki et al., 2014). Domain-specific terminology is likely to play a key role in this, as reporters, investors and financial analysts will use specific terminology when it comes to financial performance. Potentially, this may also vary between different financial sectors and industry sectors. Therefore, we have taken an exploratory approach and investigated how different features and learning algorithms operate differently, particularly SVR and BLSTMs. We have found that BLSTMs exceed an SVR without having any knowledge of the company's mood related to them."}, {"heading": "2 Related Work", "text": "This work ranges from domain-specific lexicon (Loughran and McDonald, 2011) and lexicon generation (Moore et al., 2016) to equity market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016). Peng and Jiang (2016) used a multi-layered neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction. Kazemian et al. (2016) demonstrated the importance of sentiment analysis for the task of stock market prediction. However, much of the previous work was based on financial market numerical data rather than on textual data at the aspect level. In aspect-based sentiment analysis, many different techniques were used to predict the polarity of an aspect, as shown in SemEval-2016 task 5 (Pontiki et al., 2014)."}, {"heading": "3 Data", "text": "The training data released by the organizers of this track were a series of financial news headlines, in which each sentence was marked with the company name (which we treat as an aspect) and the polarity of the sentence with respect to the company. It is possible that the same sentence occurs more than once when more than one company is mentioned. Polarity was a real value between -1 (negative mood) and 1 (positive mood). Additionally, we trained a word2vec (Mikolov et al., 2013) word-embedding Model 3 on a set of 189,206 financial articles with 161,877,425 tokens downloaded manually from Factiva4. Articles come from a number of sources, including the Financial Times, and refer exclusively to companies from the United States. We trained the model on domain-specific data, as it has been widely shown that the financial domain can contain very different languages."}, {"heading": "4 System description", "text": "Although we have outlined this task as an aspect-based sentiment task, this is specified in only one of the features of the SVR. The following two subsections describe the two approaches, first SVR and then BLSTM. Important implementation details are set out here in the paper, but we have published the source code and the Word embed models to facilitate replicability and further experimentation."}, {"heading": "4.1 SVR", "text": "The system was developed with ScitKit learn (Pedregosa et al., 2011), a linear model of support vector regression (Drucker et al., 1997).For reproducibility, the model can be downloaded, but the articles cannot be downloaded due to copyright and license limitations.4https: / / global.factiva.com / factivalogin / login.asp? productname = globalimented with the following different features and parameter settings:"}, {"heading": "4.1.1 Tokenisation", "text": "For comparison purposes, we tested whether a simple whitespace tokeniser could work as well as a full tokeniser, and in this case we used Unitok5."}, {"heading": "4.1.2 N-grams", "text": "We compared unigrams and bigrams at word level individually and in combination."}, {"heading": "4.1.3 SVR parameters", "text": "We tested different penalty parameters C and different epsilon parameters of the SVR."}, {"heading": "4.1.4 Word Replacements", "text": "We tested whether generalizing words by inserting special characters would help reduce the problem of rarity. We divided the word replacement into three separate groups: 1. Enterprise - When a company was mentioned in the input heading from the list of companies in the training data marked as aspects, it was replaced by a special logo. 2. Positive - When a positive word was mentioned in the input heading from a list of positive words (created using the most N-like words based on cosinal distance) to \"excellent\" using the pre-trained word2vec model.3. Negative - The same as the positive group, however, was \"bad\" instead of \"excellent.\" In the positive and negative groups, according to Turney (2002), we chose the words \"excellent\" and \"bad\" to group the terms under non-domain-specific sentiment words."}, {"heading": "4.1.5 Target aspect", "text": "To integrate the company as an aspect, we used a Boolean vector to represent the mood of the sentence, to see if the system could better distinguish the mood if the sentence was the same but the company was different. 5http: / / corpus.tools / wiki / Unitok"}, {"heading": "4.2 BLSTM", "text": "We created two different bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al., 2016). We chose an LSTM model because it solves the problem of disappearing gradients of recursive neural networks. We used a bidirectional model because it allows us to capture information that emerges before and after each step, allowing us to capture a more relevant context within the model. Practically, a BLSTM consists of two LSTMs, each moving through the tokens in reverse order, and linking the resulting output vectors in our models. The BLSTM models take as input a L tokens6 headset, with L being the length of the longest sentence in the training texts. Each word is converted using the 2c word."}, {"heading": "4.2.1 Standard LSTM (SLSTM)", "text": "The BLSTMs contain dropouts both in the input and between the connections of 0.2 each. Finally, the epoch is set to 25."}, {"heading": "4.2.2 Early LSTM (ELSTM)", "text": "As can be seen in Figure 1, the 0.5 drop-off occurs only between shifts and not, as in the SLSTM, under the following link for detailed implementation details: https: / / github.com / apmoore1 / semeval # finance-word2vec-modellconnections. The epoch is also not fixed, it uses early stopping with a patience of 10. We expect that due to the higher drop-off, this model can be generalized better than the standard model and that the epoch is based on early stopping, which is based on a validation that determines when the training should be stopped."}, {"heading": "5 Results", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\" rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green"}, {"heading": "6 Conclusion and Future Work", "text": "In this short article, we described our implemented solutions for SemEval Task 5 Track 2, using both SVR and BLSTM approaches. Our results show an improvement of about 5% in the use of LSTM models compared to SVR. We have shown that this task can be presented in part as an aspect-based sentiment task on a domain-specific problem. Generally, our approaches functioned as set level classifiers, as they do not consider a target company. As our results show, the choice of evaluation size makes a big difference to system training and testing. Future work will consist of implementing aspect-specific information into an LSTM model, as has proven useful in other work (Wang et al., 2016)."}, {"heading": "Acknowledgements", "text": "We are grateful to Nikolaos Tsileponis (University of Manchester) and Mahmoud El-Haj (Lancaster University) for accessing headlines in the corpus of financial news collected by Factiva. This research was supported at Lancaster University by an EPSRC doctoral student."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Xrce at semeval-2016 task 5: Feedbacked ensemble modelling on syntactico-semantic knowledge for aspect based sentiment analysis", "author": ["Caroline Brun", "Julien Perez", "Claude Roux."], "venue": "Proceedings of SemEval pages 277\u2013281.", "citeRegEx": "Brun et al\\.,? 2016", "shortCiteRegEx": "Brun et al\\.", "year": 2016}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news", "author": ["Keith Cortis", "Andre Freitas", "Tobias Daudert", "Manuela Huerlimann", "Manel Zarrouk", "Brian Davis."], "venue": "Proceedings of SemEval .", "citeRegEx": "Cortis et al\\.,? 2017", "shortCiteRegEx": "Cortis et al\\.", "year": 2017}, {"title": "Support vector regression machines. Advances in neural information processing systems 9:155\u2013161", "author": ["Harris Drucker", "Christopher JC Burges", "Linda Kaufman", "Alex Smola", "Vladimir Vapnik"], "venue": null, "citeRegEx": "Drucker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Drucker et al\\.", "year": 1997}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Evaluating sentiment analysis in the context of securities trading", "author": ["Siavash Kazemian", "Shunan Zhao", "Gerald Penn."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association", "citeRegEx": "Kazemian et al\\.,? 2016", "shortCiteRegEx": "Kazemian et al\\.", "year": 2016}, {"title": "Iit-tuda at semeval-2016 task 5: Beyond sentiment lexicon: Combining domain dependency and distributional semantics features for aspect based sentiment analysis", "author": ["Ayush Kumar", "Sarah Kohail", "Amit Kumar", "Asif Ekbal", "Chris Biemann."], "venue": "Proceed-", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "When is a liability not a liability? textual analysis, dictionaries, and 10-ks", "author": ["Tim Loughran", "Bill McDonald."], "venue": "The Journal of Finance 66(1):35\u201365.", "citeRegEx": "Loughran and McDonald.,? 2011", "shortCiteRegEx": "Loughran and McDonald.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Domain adaptation using stock market prices to refine sentiment dictionaries", "author": ["Andrew Moore", "Paul Rayson", "Steven Young."], "venue": "Proceedings of the 10th edition of Language Resources and Evaluation", "citeRegEx": "Moore et al\\.,? 2016", "shortCiteRegEx": "Moore et al\\.", "year": 2016}, {"title": "Sentiment analysis: Capturing favorability using natural language processing", "author": ["Tetsuya Nasukawa", "Jeonghee Yi."], "venue": "Proceedings of the 2nd international conference on Knowledge capture. ACM, pages 70\u201377.", "citeRegEx": "Nasukawa and Yi.,? 2003", "shortCiteRegEx": "Nasukawa and Yi.", "year": 2003}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "Journal of Machine", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Leverage financial news to predict stock price movements using word embeddings and deep neural networks", "author": ["Yangtuo Peng", "Hui Jiang."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Peng and Jiang.,? 2016", "shortCiteRegEx": "Peng and Jiang.", "year": 2016}, {"title": "Semeval-2014 task 4: Aspect based sentiment analysis", "author": ["Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar."], "venue": "Proceedings of SemEval pages 27\u201335.", "citeRegEx": "Pontiki et al\\.,? 2014", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "A hierarchical model of reviews for aspect-based sentiment analysis", "author": ["Sebastian Ruder", "Parsa Ghaffari", "G. John Breslin."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Ruder et al\\.,? 2016", "shortCiteRegEx": "Ruder et al\\.", "year": 2016}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "author": ["Peter D Turney."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics,", "citeRegEx": "Turney.,? 2002", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "Attention-based lstm for aspectlevel sentiment classification", "author": ["Yequan Wang", "Minlie Huang", "xiaoyan zhu", "Li Zhao"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "This task can be seen as a financespecific aspect-based sentiment task (Nasukawa and Yi, 2003).", "startOffset": 71, "endOffset": 94}, {"referenceID": 15, "context": "The main motivations of this task is to find specific features and learning algorithms that will perform better for this domain as aspect based sentiment analysis tasks have been conducted before at SemEval (Pontiki et al., 2014).", "startOffset": 207, "endOffset": 229}, {"referenceID": 9, "context": "This work ranges from domainspecific lexicons (Loughran and McDonald, 2011) and lexicon creation (Moore et al.", "startOffset": 46, "endOffset": 75}, {"referenceID": 11, "context": "This work ranges from domainspecific lexicons (Loughran and McDonald, 2011) and lexicon creation (Moore et al., 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al.", "startOffset": 97, "endOffset": 117}, {"referenceID": 14, "context": ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016).", "startOffset": 42, "endOffset": 87}, {"referenceID": 7, "context": ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016).", "startOffset": 42, "endOffset": 87}, {"referenceID": 7, "context": ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016). Peng and Jiang (2016) used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction.", "startOffset": 65, "endOffset": 111}, {"referenceID": 7, "context": ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016). Peng and Jiang (2016) used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction. Kazemian et al. (2016) showed the importance of tuning sentiment analysis to the task of stock market prediction.", "startOffset": 65, "endOffset": 302}, {"referenceID": 15, "context": "In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 (Pontiki et al., 2014).", "startOffset": 152, "endOffset": 174}, {"referenceID": 1, "context": "The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 8, "context": ", 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM).", "startOffset": 89, "endOffset": 109}, {"referenceID": 1, "context": "The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. Ruder et al. (2016) created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context.", "startOffset": 20, "endOffset": 333}, {"referenceID": 15, "context": "on the SemEval-2016 task 5 dataset (Pontiki et al., 2014) and on other languages performed close to the best systems.", "startOffset": 35, "endOffset": 57}, {"referenceID": 15, "context": "on the SemEval-2016 task 5 dataset (Pontiki et al., 2014) and on other languages performed close to the best systems. Wang et al. (2016) also created an LSTM based model using word embeddings but instead of a hierarchical model it was a one layered LSTM with attention which puts more emphasis on learning the sentiment of words specific to a given aspect.", "startOffset": 36, "endOffset": 137}, {"referenceID": 10, "context": "We additionally trained a word2vec (Mikolov et al., 2013) word embedding model3 on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva4.", "startOffset": 35, "endOffset": 57}, {"referenceID": 13, "context": "The system was created using ScitKit learn (Pedregosa et al., 2011) linear Support Vector Regression model (Drucker et al.", "startOffset": 43, "endOffset": 67}, {"referenceID": 4, "context": ", 2011) linear Support Vector Regression model (Drucker et al., 1997).", "startOffset": 47, "endOffset": 69}, {"referenceID": 17, "context": "In the positive and negative groups, we chose the words \u2018excellent\u2019 and \u2018poor\u2019 following Turney (2002) to group the terms together under nondomain specific sentiment words.", "startOffset": 89, "endOffset": 103}, {"referenceID": 5, "context": "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al.", "startOffset": 39, "endOffset": 69}, {"referenceID": 6, "context": "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al.", "startOffset": 93, "endOffset": 127}, {"referenceID": 2, "context": "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al.", "startOffset": 159, "endOffset": 174}, {"referenceID": 0, "context": "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al., 2016).", "startOffset": 200, "endOffset": 220}, {"referenceID": 3, "context": "This was then changed after the evaluation deadline to equation 110 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in Cortis et al. (2017) (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th).", "startOffset": 261, "endOffset": 282}, {"referenceID": 18, "context": "Future work will be to implement aspect specific information into an LSTM model as it has been shown to be useful in other work (Wang et al., 2016).", "startOffset": 128, "endOffset": 147}], "year": 2017, "abstractText": "This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.", "creator": "LaTeX with hyperref package"}}}