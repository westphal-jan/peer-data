{"id": "1705.00574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "abstract": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for a clustering task and introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to an arbitrary model and cost function, and do not require a complicated training procedure. We evaluate them on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of Adjusted Mutual Information score and outperforms previously proposed methods.", "histories": [["v1", "Mon, 1 May 2017 16:03:25 GMT  (1690kb,D)", "http://arxiv.org/abs/1705.00574v1", "Abstract accepted at ICLR 2017 Workshop:this https URL"]], "COMMENTS": "Abstract accepted at ICLR 2017 Workshop:this https URL", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexey romanov", "anna rumshisky"], "accepted": false, "id": "1705.00574"}, "pdf": {"name": "1705.00574.pdf", "metadata": {"source": "META", "title": "Forced to Learn: Discovering Disentangled Representations Without Exhaustive Labels", "authors": ["Alexey Romanov", "Anna Rumshisky"], "emails": ["<aromanov@cs.uml.edu>."], "sections": [{"heading": "1. Introduction", "text": "In recent years, a significant amount of work has been devoted to learning a better representation of the input data, which can be used either in downstream tasks, such as clustering, or to improve the generalization or perfection of the model. Generally, this work can be divided into two categories: (1) approaches that suggest a new loss component that can easily be applied to any cost function. (2) approaches by Liao et al. (3) and Xie et al. (4) can be classified into the first category because they refine clustering."}, {"heading": "2. Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3. The proposed method", "text": "Inspired by the work of Cheung et al. (2014) and Cogswell et al. (2015), we propose two novel loss components that, despite their simplicity, significantly improve the quality of clustering compared to the representation generated by the model. Lsingle, the first loss component, operates on a single layer and does not affect the other layers in the network, which can be a desirable behavior in some cases. Lmulti, the second loss component, affects the entire network behind the target layer, forcing it to create untangled representations in more complex and deeper networks where the first loss may not bring the desired improvements."}, {"heading": "3.1. Single layer loss", "text": "Consider the model in Figure 1. The layer FC2 has the output size 1 and produces a binary classification decision. The output of the layer FC1 is used to perform KMeans clustering. Let us remember from the example in the introduction that we want to force the model to produce divergent representations for samples that belong to the same class but are actually fundamentally different from each other. One way to do this would be to force the lines of the weight matrix WFC1 of the FC1 layer to differ from each other, which leads to different activation patterns in the output of the FC1 layer. Formally, it can be expressed as follows: Lsingle = k \u00b2 i = 1 k \u00b2 j = i + 1 fl (di, dj) + fl (dj, di) (1), where dk is normalized weights of the line k of the weight matrix W of the given layer: dk = softmax (W [fk] + fl (Dj) is a loss of the component Dj (dj = Dj)."}, {"heading": "3.2. Multilayer loss", "text": "Note that the loss component Lsingle only affects the weights of the specific layer, since it does not affect the results of the layer, but directly affects its weights, similar to, for example, \"2 regularization. Therefore, this loss component can only help to learn a better representation if the input to the target layer still contains the information about latent properties of the input data. This may be the case in simple flat networks, but in the case of very deep complex networks, the input data is so often not transformed linearly that only the information required for binary classification remains, and any remaining latent properties of the input data are lost as not being important for binary classification (see Figure 3a). In fact, as we can see from the experiments in Section 4, the loss component is described in Section 4, which significantly improves the quality of cluster properties in a simple base case."}, {"heading": "3.3. Unsupervised learning", "text": "Although our main focus in the presented experiments is on a binary classification task, our two proposed loss components can also be used for unattended learning. Lsingle loss component does not require labels, so it can be used without modifications. Lmulti loss component can be applied to unlabeled data simply by taking the sums without considering the labels of the samples as follows: Lmulti2 = 1N2 N \u00b2 i = 1 N \u00b2 J = 1 fl (h s i, h s j) + fl (h s j, h s i) (6) For example, since automation models are a common choice to learn representations to be used in a downstream task, the proposed loss components can simply be applied to their cost function as follows: Lae = (1 \u2212 \u03b1), 1N N N \u00b2 i = 1 | Xi \u2212 X \u00b2 i | | | 2 + L2 \u03b1 multi (7), which is a standalone cost, the second part being a loss component and the second one being a major one."}, {"heading": "3.4. Hyperparameter m", "text": "An important choice to be made when using the proposed loss components is the value of the hyperparameter m. The smaller the value of m, the less influence the proposed loss components have. In our experiments, we found that the proposed loss component Lsingle is relatively stable with respect to the choice of m and generally performs better with larger values (in the range of 5-10). In the case of the loss component Lmulti, we found that even a small value of the loss component m (0.1-1) unravels the learned representations better and consequently leads to significant improvements in the AMIscore. In all reported experiments, we found that the proposed loss component with a reasonable change in the classification task does not affect the performance of the model."}, {"heading": "4. Experiments", "text": "To validate our hypotheses in the case of an RNN model, we conducted experiments with the MNIST sequence dataset (de Jong, 2016) 1, which contains pen strokes that are automatically generated from the original MNIST dataset (LeCun et al., 1998) Although the sequences generated do not always reflect a choice a human would make to write a number, the strokes within the dataset are consistent. To confirm our hypothesis on a more complex dataset, we also experimented with the CIFAR 10 dataset (Krizhevsky & Hinton, 2009) and a more complex CNN model based on the VGG-16 architecture (Simonyan & Zisserman, 2014). Therefore, our experiments cover two types of neural networks that are most commonly used in modern research: Recurrent Neural Networks and Simonural Nevolutional Networks (which are used as basic models in 2014 & Zman)."}, {"heading": "4.1. MNIST strokes sequences experiments", "text": "For this experiment, we divided the examples into two groups: samples belonging to classes 0 to 4 were assigned to the first group, and samples belonging to classes 1https: / / github.com / edwin-de-jong / mnist-digits-stroke-sequence-data5 to 9 were assigned to the second group. The model is trained to predict the group of a given sample and does not have access to the underlying classes. This experiment is a simplified version of the example we discussed in the introduction, where we wanted to group patients into meaningful groups while only having binary mortality results, rather than the finer-grained labels. We used the model shown in Figure 1 for this experiment. After the models were trained on the binary classification task, we used the results of the penultimate layer FC2 to perform the cluster results, and evaluated the quality of the cluster results produced using the original class markings as truths."}, {"heading": "4.2. CIFAR-10 experiments", "text": "As in the MNIST line sequence experiments, we split the examples into two groups: samples of classes \"Airplan,\" \"Auto,\" \"Bird,\" \"Cat,\" and \"Deer\" were assigned to the first group, and samples of classes \"Dog,\" \"Frog,\" \"Horse,\" \"Ship,\" \"Truck\" were assigned to the second group. Note that this assignment is quite arbitrary as it merely reflects the order of the labels of the classes in the dataset (namely, labels 0-4 for the first group and labels 4-9 for the second group).All groups contain quite different types of objects, both natural and human-made. For the experiments in the CIFAR-10 dataset, we used a CNN model based on the VGG-16 architecture, which is shown in Figure 4."}, {"heading": "5. Implementation details", "text": "Despite the fact that the proposed loss components can be implemented directly with two nested loops, such an implementation will not be mathematically efficient since it will result in a large arithmetic that works on separate vectors without taking full advantage of highly optimized parallel matrix calculations on the GPU. Therefore, it is desirable to have an efficient implementation that can take full advantage of modern GPUs. We have developed such an efficient implementation that significantly speeds up the calculation of the loss component in exchange for higher memory consumption by creating two matrices that contain all the combinations of di and dj from the sums in Equation 1 and perform the operations to calculate the loss on them. We have made our implementation for TensorFlow (Abadi et al., 2016) on GitHub2 publicly available, along with the aforementioned models of 2placement subsection: http / subgith.com / 4.12 / It is the component of the Loss section, which is directly related to the http: / 2.ubceholder subsection of the Loss section."}, {"heading": "6. Discussion", "text": "As we can see from Figure 2 and Figure 3, during the binary classification task on both datasets without the proposed loss component, the models tend to learn representations that are specific to the target binary label, even though the samples within a group come from different classes. The model learns to mostly use only two neurons to distinguish between the target groups, and hardly uses the rest of the neurons in the layer. We observe this behavior across different types of models and datasets: an RNN model that is applied to a span of time, and a CNN model that behaves in exactly the same way. Both proposed loss components, Lsingle and Lmulti, force the model to produce unleashed representations, and we can see how it changes the patterns of activation in the target layer. It is easy to see in Figure 2b that the patterns of activation are learned by the networks."}, {"heading": "7. Conclusion", "text": "In this paper, we proposed two novel loss components that greatly improve the quality of KMeans clustering using the representations of the input data learned from the model. We conducted a comprehensive series of experiments with two major model types (RNNs and CNNs) and different data sets, and demonstrated that the proposed loss components consistently increase the Adjusted Mutual Information Score by a significant advantage and exceed the previously proposed methods. Furthermore, we analyzed the representations learned through the network by visualizing the activation patterns and relative position of the samples in the learned space, and the visualizations show that the proposed loss components actually force the network to learn untangled representations."}], "references": [{"title": "Discovering hidden factors of variation in deep networks", "author": ["Cheung", "Brian", "Livezey", "Jesse A", "Bansal", "Arjun K", "Olshausen", "Bruno A"], "venue": "arXiv preprint arXiv:1412.6583,", "citeRegEx": "Cheung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2014}, {"title": "Reducing overfitting in deep networks by decorrelating representations", "author": ["Cogswell", "Michael", "Ahmed", "Faruk", "Girshick", "Ross", "Zitnick", "Larry", "Batra", "Dhruv"], "venue": "arXiv preprint arXiv:1511.06068,", "citeRegEx": "Cogswell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cogswell et al\\.", "year": 2015}, {"title": "Incremental sequence learning", "author": ["de Jong", "Edwin D"], "venue": "arXiv preprint arXiv:1611.03068,", "citeRegEx": "Jong and D.,? \\Q2016\\E", "shortCiteRegEx": "Jong and D.", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Li", "Jiwei", "Chen", "Xinlei", "Hovy", "Eduard", "Jurafsky", "Dan"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning deep parsimonious representations", "author": ["Liao", "Renjie", "Schwing", "Alex", "Zemel", "Richard", "Urtasun", "Raquel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Vinh", "Nguyen Xuan", "Epps", "Julien", "Bailey", "James"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vinh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vinh et al\\.", "year": 2010}, {"title": "Unsupervised deep embedding for clustering analysis", "author": ["Xie", "Junyuan", "Girshick", "Ross", "Farhadi", "Ali"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "For example, approaches by Liao et al. (2016) and Xie et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 7, "context": "For example, approaches by Liao et al. (2016) and Xie et al. (2016) can be assigned to the first category, as they propose to iteratively refine the clusters during the training.", "startOffset": 27, "endOffset": 68}, {"referenceID": 0, "context": "In contrast, approaches by Cogswell et al. (2015) and Cheung et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 0, "context": "(2015) and Cheung et al. Cheung et al. (2014) introduce new loss compo-", "startOffset": 11, "endOffset": 46}, {"referenceID": 6, "context": ", 2015), where a deep convolutional neural network model won the challenge with a shocking gap (Krizhevsky et al., 2012), neural networks has achieved remarkable success in nearly every classification task.", "startOffset": 95, "endOffset": 120}, {"referenceID": 4, "context": ", 2015), where a deep convolutional neural network model won the challenge with a shocking gap (Krizhevsky et al., 2012), neural networks has achieved remarkable success in nearly every classification task. For a long time, it was unclear why deep neural networks, and convolutional neural networks (CNN) in particular, work so well and what is happening inside this \u201cblack box\u201d, until the work of Zeiler & Fergus (2014), which showed that it is possible to visualize which features and representations specifically a network learns during the training, which answered this question for CNNs, but not for Recurrent Neural Networks (RNN).", "startOffset": 96, "endOffset": 421}, {"referenceID": 3, "context": "In particular, the work of Karpathy et al. (2015) showed what is happening inside an RNN cell during the inference and the areas of responsibility of neurons inside the cell.", "startOffset": 27, "endOffset": 50}, {"referenceID": 3, "context": "In particular, the work of Karpathy et al. (2015) showed what is happening inside an RNN cell during the inference and the areas of responsibility of neurons inside the cell. After that, Li et al. (2016) plotted the representations learned by an RNN trained on a task of sentiment classification and showed that the network is able to learn local compositionality, embedding the negated expressions (such as \u201cnot good\u201d, \u201cnot nice\u201d) into the space near words with a negative polarity (such as \u201cbad\u201d).", "startOffset": 27, "endOffset": 204}, {"referenceID": 0, "context": "More relevant to the goal of this paper, Cheung et al. (2014) proposed a cross-covariance penalty (XCov) to force the network to produce representations with disentangled factors.", "startOffset": 41, "endOffset": 62}, {"referenceID": 0, "context": "More relevant to the goal of this paper, Cheung et al. (2014) proposed a cross-covariance penalty (XCov) to force the network to produce representations with disentangled factors. The proposed penalty is, essentially, cross-covariance between the predicted labels and the activations of samples in a batch. Their experiments showed that the network can produce a representation, with components that are responsible to different characteristics of the input data. For example, in case of the MNIST dataset, there was a classinvariant factor that was responsible for the style of the digit, and in case of the Toronto Faces Dataset (Susskind et al., 2010), there was a factor responsible for the subject\u2019s identity. Similarly, but with a different goal in mind, Cogswell et al. (2015) proposed a new regularizer (DeCov) which minimizes cross-covariance of hidden activations, leading to non-redundant representations and, consequently, less overfitting and better generalization.", "startOffset": 41, "endOffset": 784}, {"referenceID": 9, "context": "Liao et al. (2016) proposed a method to learn parsimonious representations.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Liao et al. (2016) proposed a method to learn parsimonious representations. Essentially, the proposed algorithm iteratively calculates cluster centroids, which are updated every M iterations and used in the cost function. The authors\u2019 experiments showed that such algorithm leads to a better generalization and a higher test performance of the model in case of supervised learning, as well as unsupervised and even zero-shot learning. Similarly, Xie et al. (2016) proposed an iterative algorithm that first calculates soft cluster assignments, then updates the weights of the network and cluster centroids.", "startOffset": 0, "endOffset": 464}, {"referenceID": 0, "context": "Inspired by the work of Cheung et al. (2014) and Cogswell et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 0, "context": "Inspired by the work of Cheung et al. (2014) and Cogswell et al. (2015), we propose two novel loss components which despite their simplicity, significantly improve the quality of the clustering over the representation produced by the model.", "startOffset": 24, "endOffset": 72}, {"referenceID": 7, "context": "This dataset contains pen strokes, automatically generated from the original MNIST dataset (LeCun et al., 1998).", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "We compared our loss components Lsingle and Lmulti with the DeCov regularizer (Cogswell et al., 2015) and XCov penalty (Cheung et al.", "startOffset": 78, "endOffset": 101}, {"referenceID": 0, "context": ", 2015) and XCov penalty (Cheung et al., 2014) as these losses use similar ideas, even though they do not directly target the task of improving the quality of clustering.", "startOffset": 25, "endOffset": 46}, {"referenceID": 0, "context": ", 2015) and XCov penalty (Cheung et al., 2014) as these losses use similar ideas, even though they do not directly target the task of improving the quality of clustering. We did not do comparison with the work of Liao et al. (2016) and Xie et al.", "startOffset": 26, "endOffset": 232}, {"referenceID": 0, "context": ", 2015) and XCov penalty (Cheung et al., 2014) as these losses use similar ideas, even though they do not directly target the task of improving the quality of clustering. We did not do comparison with the work of Liao et al. (2016) and Xie et al. (2016) as they belong to the second group of methods which requires a more complicated training procedure, whereas the loss components proposed here are simple to apply to an arbitrary cost function and do not require any changes in the training procedure.", "startOffset": 26, "endOffset": 254}, {"referenceID": 11, "context": "We report the average of the Adjusted Mutual Information (AMI) and Normalized Mutual Information (NMI) scores (Vinh et al., 2010) across three runs in the Table 1.", "startOffset": 110, "endOffset": 129}], "year": 2017, "abstractText": "Learning a better representation with neural networks is a challenging problem, which was tackled extensively from different prospectives in the past few years. In this work, we focus on learning a representation that could be used for a clustering task and introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to an arbitrary model and cost function, and do not require a complicated training procedure. We evaluate them on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of Adjusted Mutual Information score and outperforms previously proposed methods.", "creator": "LaTeX with hyperref package"}}}