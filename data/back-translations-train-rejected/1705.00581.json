{"id": "1705.00581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Query-adaptive Video Summarization via Quality-aware Relevance Estimation", "abstract": "Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.", "histories": [["v1", "Mon, 1 May 2017 16:28:18 GMT  (8913kb,D)", "http://arxiv.org/abs/1705.00581v1", "Submitted to ACM Multimedia 2017"], ["v2", "Thu, 28 Sep 2017 13:18:56 GMT  (3909kb,D)", "http://arxiv.org/abs/1705.00581v2", "ACM Multimedia 2017"]], "COMMENTS": "Submitted to ACM Multimedia 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["arun balajee vasudevan", "michael gygli", "anna volokitin", "luc van gool"], "accepted": false, "id": "1705.00581"}, "pdf": {"name": "1705.00581.pdf", "metadata": {"source": "META", "title": "Query-adaptive Video Summarization via Quality-aware Relevance Estimation-1.5ex", "authors": ["Arun Balajee Vasudevan", "Michael Gygli", "Anna Volokitin", "Luc Van Gool"], "emails": ["arunv@vision.ee.ethz.ch", "gygli@vision.ee.ethz.ch", "anna.volokitin@vision.ee.ethz.ch", "vangool@vision.ee.ethz.ch"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, the fact is that you will be able to move to another world, where you can move to another world, where you have to move to another world, where you can move to another world."}, {"heading": "2 RELATED WORK", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "3 METHOD FOR RELEVANCE PREDICTION", "text": "The aim of this thesis is to introduce a method for automatically selecting a set of video thumbnails that are both relevant to a query and diverse enough to represent the video. In order to later jointly optimize relevance and diversity, we first need a way to assess the relevance of frames. Our relevance model learns a projection of video frames and text requests t into the same embedding space. We refer to the projection of t and v as t or v. Once trained, the relevance of a frame v given to a query t can be assessed by a measure of similarity. Like [12] we use the cosmic similarities (t, v) = t \u00b7 v-t-t-t-t-bay v. (1) While this allows us to judge the semantic relevance of a frame v by a query, it is also possible to predict suitability as thumbnails a priori, based on the image quality, composition, etc."}, {"heading": "3.1 Training objective", "text": "Intuitively, our model should be able to answer \"What is the best thumbnail for this query?\" Therefore, the problem of selecting the best thumbnail for a video is, of course, formulated as a ranking problem. We would like the embedding vectors of a query and a frame that match to be more similar than those of the same query and a non-relevant frame \u2020. Therefore, our model should learn to meet the ranking constraint that, in a query t, the relevance value of the relevant frame v + is higher than the relevance value of the irrelevant frame v \u2212: r (t, v +) > r (t, v \u2212). (3) Alternatively, we can train the model by requiring that both the similarity value and the quality value of the relevant frame v + be higher than the relevance value of the irrelevant frame, rather than explicitly imposing a constraint (t, v +) as above. (In this case, we would impose the following two constraints:"}, {"heading": "3.2 Text and Frame Representation", "text": "We use a convolutionary neural network to predict v and qv, while t is obtained via a recursive neural network. To learn the parameters of these networks together, we use a Siamese ranking network trained on triplets of (t, v +, v \u2212). We make the model architecture available in complementary material. We now describe the textual representation t and the representations v and qv in detail. \u2020 Liu et al. [31] does the opposite. It presents the problem that we learn to assign corresponding frames and queries to a greater similarity than to the same frame and a random query. Thus, the model learns to answer the question \"What is a good query for this image?.\" Textual representation. As a feature representation t of the textual query t, we first project each word of the query into a 300-dimensional semantic space using the word2vec M model [39], we then set the unique dataset length from the Vqv."}, {"heading": "4 SUMMARIZATION MODEL", "text": "In this context, the summary is raised as a problem for selecting a subset (in our case, frames) and for a linear combination of submodular objective functions f (xV, y)."}, {"heading": "5 RELEVANCE AND DIVERSITY DATASET (RAD)", "text": "We collected a dataset with query relevance and diversity annotation to train us and evaluate query-relevant summaries. Our dataset consists of 200 videos, each of which was retrieved based on a different query. Amazon Mechanical Turk (AMT) allows us to first comment on the video frames with query relevance labels, and then divide the frames into clusters by visual similarity. These types of labels were previously used in the MediaEval Various Social Images Challenge [19] and enabled the evaluation of automatic methods for creating relevant and diverse sums. To select a representative sample of queries and videos for the dataset, we used the following procedure: We take the top YouTube queries from 2008 to 2016 from 22 different categories as Seed Queries. These queries are typically short and generic, so they take longer, more realistic queries and videos for the dataset."}, {"heading": "5.1 Analysis", "text": "We analyze the two types of annotations that we get through this process, and describe how we distribute these annotations to a number of multiple truth markers per video.Label distributions. The distribution of relevance markers is \"Very Good\": 0.5%, \"Good\": 57.40%, \"Not good\": 12.31% and \"Trash\": 12.72%. The minimum, maximum and average number of annotations per video are 4.9, 25.2 and 13.4 respectively across all videos of RAD.Relevance Consistency of annotations Consistency. Given the inherent subjectivity of the task, we want to know if annotations agree with each other on the query relevance of frames. To do this, we follow previous work and calculate the spearman rank correlation (s) between the relevant results of different subjects that we divide five annotations of each video into two groups of two and three raters each."}, {"heading": "6 CONFIGURATION TESTING", "text": "Before comparing our proposed relevance model with the state of the art, in Section 7 we first analyze the performance of using different targets, cost functions and text representations. For evaluation, we use the query-dependent Thumbnail Selection Dataset (QTS), that of Liu et al. [31]. The dataset contains 20 candidate thumbnails for each video, each of which is labeled with one of five scores: Very Good (VG), Good (G), Fair (F), Bad (B) or Very Bad (VB). We evaluate the available 749 query video pairs. To transform the categorical labels into numerical values, we use the same mapping metrics as [31]. As evaluation metrics, we use HIT @ 1 and mean average precision (mAP) as reported and defined."}, {"heading": "6.1 Tested components", "text": "We test three key components of our model, which we discuss below: objective. we compare our proposed training goal with that of Liu et al. [31]. Your model is trained to rank a positive query higher than a negative query within a specified framework. In contrast, our method is trained to rank a positive frame higher than a negative frame within a specified query. Cost function. we also examine the importance of modeling frame quality. In particular, we compare different cost functions. (i) We impose two ranking constraints: one for the quality concept and one for embedding similarity, as in Equation. (4) (Qexpli), (ii) We add the quality and similarity concept in an output score, for which we enforce the ranking constraint, as in Equation. (3) (Qimpli) or (iii) we do not model quality at all. Text representation. As mentioned in paragraph 3.2, we represent the individual words we embed in a fraction (the)."}, {"heading": "6.2 Results", "text": "We present the results of our detailed experiments in Tab. 1. They provide insights into several important points.Text representation. modeling queries with an LSTM, rather than intersecting the individual word representations, significantly improves performance, which is not surprising since this model can learn to ignore stop words and words that are not visually informative (e.g. 2014).Objective and cost functionality. The analysis shows that training with our goal leads to better performance than using the goal of Liu et al. [31]. This can be explained by the properties of videos, which typically contain many frames that are not visually informative [51]."}, {"heading": "7 EXPERIMENTS", "text": "In the previous section, we found that our goal of embedding queries in an LSTM and explicitly modelling quality performs best. In the following, we call this model QAR (Quality-Aware Relevance) and compare it with modern models on the QTS and RAD records. We also evaluate the full summary model on RAD. For these experiments, we divide RAD into 100 videos for training, 50 for validation and 50 for testing. Evaluation metrics. For relevance, we use the same metrics as in paragraph 6. To evaluate video summaries on RAD, we also use F1 values. F1 value is the harmonious means for accuracy of relevance prediction and cluster reminder [19]. It is high when a method selects relevant frames from different clusters."}, {"heading": "7.1 Evaluating the Relevance Model", "text": "We evaluate our model (QAR) and compare it with Liu et al. [31] and Video2GIF [16]. Query-dependent Thumbnail Selection Dataset (QTS) [31] We compare it to the state of the art on the QTS evaluation dataset in Tab. 2. We report on the performance of Liu et al. [31] from their paper. Note, however, that the results are not directly comparable, as they use query-video pairs to predict relevance while only the titles are publicly shared. So we use the titles instead, which is an important difference. Relevance is commented on in relation to the queries, which are often different from the video titles."}, {"heading": "7.2 Evaluating the Summarization Model", "text": "As mentioned in Sec. 4, we use four objectives for our summary model. Referring to Tab. 4, we use QAR model to obtain similarity and quality of results, while diversity and representativeness results are achieved, as described in Sec. 4. We compare the performance of our complete model with each individual goal, a baseline based on a query (MMR) and Hecate [51]. MMR greedily builds a sentence that maximizes the weighted sum of two terms: (i) The similarity of the selected elements with a query and (ii) The dissimilarity with previously selected elements. To assess the similarity with the query, we use our own model (QAR without Qexpli) and for the dissimilarity of the selected elements."}, {"heading": "8 CONCLUSION", "text": "Unlike previous work such as [48, 63], this model allows us to handle unrestricted queries and even complete sentences. We have proposed various improvements over [31] and empirically evaluated them to learn a relevance model. Our empirical evaluation showed that a better training target, a more sophisticated text model, and explicit quality modeling lead to significant performance gains. In particular, we demonstrated that quality plays an important role in the absence of high-quality relevance information, such as queries, i.e. when only the title can be used. Finally, we introduced a new dataset for the selection of thumbnail images that comes with query relevance labels and a grouping of frames by visual and semantic similarity. On the basis of this data, we tested our complete summary framework and showed that it is favorable compared to strong M51 [our] measurements and such similarity."}, {"heading": "9 ACKNOWLEDGEMENTS", "text": "This work was supported by Toyota through the TRACEZurich project. We would also like to thank for the support of the CHIST-ERA SAMPLE project. MG was supported by the European Research Council within the VarCity project (# 273940)."}], "references": [{"title": "Automatic editing of footage from multiple social cameras", "author": ["I Arev", "HS Park", "Yaser Sheikh"], "venue": "ACM Transactions on Graphics (TOG)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A data-driven approach for tag refinement and localization in web videos", "author": ["Lamberto Ballan", "Marco Bertini", "Giuseppe Serra", "Alberto Del Bimbo"], "venue": "Computer Vision and Image Understanding", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein"], "venue": "In ACM SIGIR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Learning a Recurrent Visual Representation for Image Caption Generation", "author": ["Xinlei Chen", "C Lawrence Zitnick"], "venue": "Proceedings of CoRR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["Pradipto Das", "Chenliang Xu", "Richard F. Doell", "Jason J. Corso"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "VSUMM: A mechanism designed to produce static video summaries and a novel evaluation method", "author": ["Sandra Eliza Fontes de Avila", "Ana Paula Brand\u00e3o Lopes", "Antonio da Luz", "Arnaldo de Albuquerque Ara\u00fajo"], "venue": "Pattern Recognition Letters 32,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Robust Data Clustering", "author": ["Ana L.N. Fred", "Anil K. Jain"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["Andrea Frome", "Gs Corrado", "Jonathon Shlens"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Diverse sequential subset selection for supervised video summarization", "author": ["Boqing Gong", "Wei-Lun Chao", "Kristen Grauman", "Fei Sha"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Video Summarization by Learning Submodular Mixtures of Objectives", "author": ["Michael Gygli", "Helmut Grabner", "Luc Van Gool"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "The Interestingness of Images", "author": ["M Gygli", "H Grabner", "H Riemenschneider", "F Nater", "L Van Gool"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Video2GIF: Automatic Generation of Animated GIFs from Video", "author": ["Michael Gygli", "Yale Song", "Liangliang Cao"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Clickture: A large-scale real-world image dataset", "author": ["XS Hua", "L Yang", "M Ye", "K Wang", "Y Rui", "J Li"], "venue": "Technical Report. Microsoft Research Technical Report MSR-TR-2013-75", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Retrieving Diverse Social Images at MediaEval 2015:  Challenge, Dataset and Evaluation", "author": ["Bogdan Ionescu", "Adrian Popescu", "Mihai Lupu", "Alexandru Lucian Ginsca", "Henning M\u00fcller"], "venue": "MediaEval", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Understanding the intrinsic memorability of images", "author": ["Phillip Isola", "Devi Parikh", "Antonio Torralba", "Aude Oliva"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei Li"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei Fei Li"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Large-Scale Video Summarization Using Web-Image Priors", "author": ["Aditya Khosla", "Raffay Hamid", "CJ Lin", "Neel Sundaresan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Joint summarization of large-scale collections of web images and videos for storyline reconstruction", "author": ["Gunhee Kim", "Leonid Sigal", "Eric P Xing"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "First-person hyper-lapse videos", "author": ["Johannes Kopf", "Michael F Cohen", "Richard Szeliski"], "venue": "ACM Transactions on Graphics", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Submodular function maximization", "author": ["Andreas Krause", "Daniel Golovin"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Discovering important people and objects for egocentric video summarization", "author": ["Yong Jae Lee", "Joydeep Ghosh", "Kristen Grauman"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Hui Lin", "JA Bilmes"], "venue": "arXiv preprint arXiv:1210.4871", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Using Web Photos for Measuring Video Frame Interestingness", "author": ["Feng Liu", "Yuzhen Niu", "Michael Gleicher"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Multi-task deep visual-semantic embedding for video thumbnail selection", "author": ["Wu Liu", "Tao Mei", "Yongdong Zhang", "C Che", "Jiebo Luo"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Deep multi-patch aggregation network for image style, aesthetics, and quality estimation", "author": ["Xin Lu", "Zhe Lin", "Xiaohui Shen", "Radomir Mech", "James Z Wang"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Story-driven summarization for egocentric video", "author": ["Zheng Lu", "Kristen Grauman"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Composition-preserving deep photo aesthetics assessment", "author": ["Long Mai", "Hailin Jin", "Feng Liu"], "venue": "In CVPR", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "TagBook: A Semantic Video Representation without Supervision for Event Detection", "author": ["Masoud Mazloom", "Xirong Li", "Cees Snoek"], "venue": "IEEE Transactions on Multimedia", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Comparing clusterings by the variation of information. In Learning theory and kernel", "author": ["Marina Meil\u0103"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "J Dean"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["M Minoux"], "venue": "Optimization Techniques", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1978}, {"title": "Siamese Recurrent Architectures for Learning Sentence Similarity", "author": ["Jonas Mueller", "Aditya Thyagarajan"], "venue": "In AAAI", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "A submodular-supermodular procedure with applications to discriminative structure learning", "author": ["Mukund Narasimhan", "Jeff A Bilmes"], "venue": "arXiv preprint arXiv:1207.1404", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "An analysis of approximations for maximizing submodular set functions - I", "author": ["GL Nemhauser", "LA Wolsey", "ML Fisher"], "venue": "Mathematical Programming", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1978}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Category-specific video summarization", "author": ["Danila Potapov", "Matthijs Douze", "Zaid Harchaoui", "Cordelia Schmid"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Nonchronological video synopsis and indexing", "author": ["Yael Pritch", "Alex Rav-Acha", "Shmuel Peleg"], "venue": "IEEE transactions on pattern analysis and machine intelligence 30,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Query-Focused Extractive Video Summarization", "author": ["Aidean Sharghi", "Boqing Gong", "Mubarak Shah"], "venue": "CoRR abs/1607.05177", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "To Click or Not To Click: Automatic Selection of Beautiful Thumbnails from Videos", "author": ["Yale Song", "Miriam Redi", "Jordi Vallmitjana", "Alejandro Jaimes"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "TV- SUM: Summarizing web videos using titles", "author": ["Yale Song", "Jordi Vallmitjana", "Amanda Stent", "Alejandro Jaimes"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Ranking Domain-Specific Highlights by Analyzing Edited Videos", "author": ["Min Sun", "Ali Farhadi", "Steve Seitz"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Salient Montages from Unconstrained Videos", "author": ["Min Sun", "Ali Farhadi", "Ben Taskar", "Steve Seitz"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Semantic Highlight Retrieval and Term Prediction", "author": ["Min Sun", "Kuo-Hao Zeng", "Yenchen Lin", "Farhadi Ali"], "venue": "IEEE Transactions on Image Processing", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2017}, {"title": "Comparing Clusterings - An Overview", "author": ["Silke Wagner", "Dorothea Wagner"], "venue": "Graph-Theoretic Concepts in Computer Science", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2007}, {"title": "Event-Specific Image Importance", "author": ["Yufei Wang", "Zhe Lin", "Xiaohui Shen", "Radomir Mech", "Gavin Miller", "Garrison W Cottrell"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2016}, {"title": "Key frame selection by motion analysis", "author": ["Wayne Wolf"], "venue": "Acoustics, Speech, and Signal Processing", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1996}, {"title": "Detecting snap points in egocentric video with a web photo prior", "author": ["Bo Xiong", "Kristen Grauman"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Highlight detection with pairwise deep ranking for first-person video summarization", "author": ["Ting Yao", "Tao Mei", "Yong Rui"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2016}, {"title": "Mouse activity as an indicator of interestingness in video. In ICMR", "author": ["Gloria Zen", "Paloma de Juan", "Yale Song", "Alejandro Jaimes"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Semantic highlight retrieval", "author": ["Kuo-Hao Zeng", "Yen-Chen Lin", "Ali Farhadi", "Min Sun"], "venue": "In ICIP", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2016}, {"title": "Video Summarization with Long Short-term Memory", "author": ["Ke Zhang", "Wei-Lun Chao", "Fei Sha", "Kristen Grauman"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2016}, {"title": "Quasi real-time summarization for consumer videos", "author": ["Bin Zhao", "Eric P Xing"], "venue": "In CVPR", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 95, "endOffset": 106}, {"referenceID": 33, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 95, "endOffset": 106}, {"referenceID": 0, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 11, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 20, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 21, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 25, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 30, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 42, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 49, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 60, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 20, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 142, "endOffset": 150}, {"referenceID": 25, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 142, "endOffset": 150}, {"referenceID": 11, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 160, "endOffset": 168}, {"referenceID": 48, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 160, "endOffset": 168}, {"referenceID": 22, "context": "Our approach improves previous works in the area of textual-visual embeddings [25, 31] and proposes an extension of an existing video summarization method using submodular mixtures [14] for creating summaries that are query-adaptive.", "startOffset": 78, "endOffset": 86}, {"referenceID": 28, "context": "Our approach improves previous works in the area of textual-visual embeddings [25, 31] and proposes an extension of an existing video summarization method using submodular mixtures [14] for creating summaries that are query-adaptive.", "startOffset": 78, "endOffset": 86}, {"referenceID": 11, "context": "Our approach improves previous works in the area of textual-visual embeddings [25, 31] and proposes an extension of an existing video summarization method using submodular mixtures [14] for creating summaries that are query-adaptive.", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "We train this model on a large dataset of image search data [18] and our newly introduced Relevance and Diversity dataset (Section 5).", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "exist: (i) modelling generic frame interestingness [16, 28] or (ii) using additional information such as the video title or a text query to", "startOffset": 51, "endOffset": 59}, {"referenceID": 25, "context": "exist: (i) modelling generic frame interestingness [16, 28] or (ii) using additional information such as the video title or a text query to", "startOffset": 51, "endOffset": 59}, {"referenceID": 27, "context": "find relevant frames [30, 31, 52].", "startOffset": 21, "endOffset": 33}, {"referenceID": 28, "context": "find relevant frames [30, 31, 52].", "startOffset": 21, "endOffset": 33}, {"referenceID": 48, "context": "find relevant frames [30, 31, 52].", "startOffset": 21, "endOffset": 33}, {"referenceID": 1, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 5, "endOffset": 16}, {"referenceID": 33, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 5, "endOffset": 16}, {"referenceID": 9, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 44, "endOffset": 56}, {"referenceID": 28, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 44, "endOffset": 56}, {"referenceID": 46, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 44, "endOffset": 56}, {"referenceID": 3, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 4, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 6, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 18, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 19, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 32, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 23, "context": "hyperlapses [26], montages [54] or video synopses [46].", "startOffset": 12, "endOffset": 16}, {"referenceID": 50, "context": "hyperlapses [26], montages [54] or video synopses [46].", "startOffset": 27, "endOffset": 31}, {"referenceID": 43, "context": "hyperlapses [26], montages [54] or video synopses [46].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 21, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 25, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 54, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 11, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 122, "endOffset": 130}, {"referenceID": 30, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 122, "endOffset": 130}, {"referenceID": 10, "context": "Extractive methods need to optimise at least two properties of the summary: the quality of the selected frames and their diversity [13, 14, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 11, "context": "Extractive methods need to optimise at least two properties of the summary: the quality of the selected frames and their diversity [13, 14, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 44, "context": "Extractive methods need to optimise at least two properties of the summary: the quality of the selected frames and their diversity [13, 14, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 11, "context": "Sometimes, additional objectives such as temporal uniformity [14] and relevance [48] are also optimised.", "startOffset": 61, "endOffset": 65}, {"referenceID": 44, "context": "Sometimes, additional objectives such as temporal uniformity [14] and relevance [48] are also optimised.", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "The simplest approach to obtain a representative and diverse summary is to cluster videos into events and select the best frame per event [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 10, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 120, "endOffset": 132}, {"referenceID": 44, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 120, "endOffset": 132}, {"referenceID": 59, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 156, "endOffset": 164}, {"referenceID": 26, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 156, "endOffset": 164}, {"referenceID": 44, "context": "[48], who present an approach for query-adaptive video summarization using DPPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In this work, we formulate video summarization as a maximisation problem over a set of submodular functions, following [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 42, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 78, "endOffset": 86}, {"referenceID": 57, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 49, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 51, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 56, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 42, "context": "To simplify the task, some approaches assume the domain of the video given and train a model for each domain [45, 53, 61].", "startOffset": 109, "endOffset": 121}, {"referenceID": 49, "context": "To simplify the task, some approaches assume the domain of the video given and train a model for each domain [45, 53, 61].", "startOffset": 109, "endOffset": 121}, {"referenceID": 56, "context": "To simplify the task, some approaches assume the domain of the video given and train a model for each domain [45, 53, 61].", "startOffset": 109, "endOffset": 121}, {"referenceID": 55, "context": "[60], detects \u201csnap points\u201d by using a web image prior.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 9, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 18, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 19, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 32, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 9, "context": "These typically project both modalities into a joint embedding space [12], where semantic similarity can be compared using a measure like cosine similarity.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "Word2vec [38] and GloVe [44]", "startOffset": 9, "endOffset": 13}, {"referenceID": 41, "context": "Word2vec [38] and GloVe [44]", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "Once both modalities are in the same space, they may be easily compared [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "[31] applied this idea to video thumbnail", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31], but we provide several important improvements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31], we directly optimise the target objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As [12], we use the cosine similarity", "startOffset": 3, "endOffset": 7}, {"referenceID": 55, "context": "[60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We follow [16] and use a Huber loss for lp , i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "[31] does the inverse.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "As a feature representation t of the textual query t , we first project each word of the query into a 300dimensional semantic space using the word2vec model [39], which is fine-tuned on unique queries from the Bing Clickture dataset [18].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "As a feature representation t of the textual query t , we first project each word of the query into a 300dimensional semantic space using the word2vec model [39], which is fine-tuned on unique queries from the Bing Clickture dataset [18].", "startOffset": 233, "endOffset": 237}, {"referenceID": 14, "context": "Then, we encode the individual word representations into a single fixed-length embedding using an LSTM [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 45, "context": "To represent the image, we leverage the feature representations of a pre-trained VGG-19 network [49].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "We use the framework of submodular optimization to create summaries that take into account multiple objectives [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "(6) is submodular [27], meaning that it can be optimized near-optimally in an efficient way using a greedy algorithm with lazy evaluations [40, 43].", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "(6) is submodular [27], meaning that it can be optimized near-optimally in an efficient way using a greedy algorithm with lazy evaluations [40, 43].", "startOffset": 139, "endOffset": 147}, {"referenceID": 40, "context": "(6) is submodular [27], meaning that it can be optimized near-optimally in an efficient way using a greedy algorithm with lazy evaluations [40, 43].", "startOffset": 139, "endOffset": 147}, {"referenceID": 11, "context": "(4) Representativeness [14].", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "Previous methods typically only optimized for relevance [31] or used small datasets with limited vocabularies [48].", "startOffset": 56, "endOffset": 60}, {"referenceID": 44, "context": "Previous methods typically only optimized for relevance [31] or used small datasets with limited vocabularies [48].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "If relevance and diversity labels are known, we can estimate the optimal mixing weights of the submodular functions through subgradient descent [29].", "startOffset": 144, "endOffset": 148}, {"referenceID": 39, "context": "In order to directly optimize for the F1score used at test time, we use a locally modular approximation based on the procedure of [42] and optimize the weights using AdaGrad [9].", "startOffset": 130, "endOffset": 134}, {"referenceID": 7, "context": "In order to directly optimize for the F1score used at test time, we use a locally modular approximation based on the procedure of [42] and optimize the weights using AdaGrad [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 16, "context": "These kind of labels were used previously in the MediaEval diverse social images challenge [19] and enabled evaluation of the automatic methods for creating relevant and diverse summaries.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "To do this, we follow previous work [15, 20, 58] and compute the Spearman rank correlation (\u03c1) between the relevance scores of different subjects, splitting five annotations of each video into two groups of two and three raters each.", "startOffset": 36, "endOffset": 48}, {"referenceID": 17, "context": "To do this, we follow previous work [15, 20, 58] and compute the Spearman rank correlation (\u03c1) between the relevance scores of different subjects, splitting five annotations of each video into two groups of two and three raters each.", "startOffset": 36, "endOffset": 48}, {"referenceID": 53, "context": "To do this, we follow previous work [15, 20, 58] and compute the Spearman rank correlation (\u03c1) between the relevance scores of different subjects, splitting five annotations of each video into two groups of two and three raters each.", "startOffset": 36, "endOffset": 48}, {"referenceID": 53, "context": "4 [58].", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "MediaEval, for example, used multiple relevance labels but only one clustering [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "Variation of Information, Normalised Mutual Information or the Rand index (See Wagner and Wagner [57] for an excellent overview).", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "In the following we propose to use Normalised Mutual Information (NMI), an information theoretic measure [11] which is the ratio of the mutual information between two clusterings (I (C,C \u2032)) and the sum of entropies of the clusterings (H (C) + H (C \u2032)):", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "We chose NMI over the more recently proposed Variation of Information (VI) [37], as NMI has a fixed range ([0, 1]) while still being closely related to VI (see supplementary material).", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "We chose NMI over the more recently proposed Variation of Information (VI) [37], as NMI has a fixed range ([0, 1]) while still being closely related to VI (see supplementary material).", "startOffset": 107, "endOffset": 113}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "To transform the categorical labels to numerical values, we use the same mapping as [31].", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "[31], as well as the Spearman\u2019s Rank Correlation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For training, we use two datasets: (i) the Bing Clickture dataset [18] and (ii) the RAD dataset (Sec.", "startOffset": 66, "endOffset": 70}, {"referenceID": 38, "context": "We truncate the number of words in the query at 14, as a tradeoff between the mean and maximum query length in Clickture dataset(5 and 26 respectively) [41].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "5 as in [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "We train the parameters of the LSTM and projection layer M using stochastic gradient descent with adaptive weight updates (AdaGrad) [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] 40.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Note that [31] uses queries for their method which are not publicly available (see text).", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "This can be explained with the properties of videos, which typically contain many frames that are low-quality or not visually informative [51].", "startOffset": 138, "endOffset": 142}, {"referenceID": 28, "context": "[31] + LSTM: 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] in terms of mAP, despite not using any textual information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "674 Video2GIF [16] 67.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "[31] +LSTM 70.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] +LSTM 72.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] Video2GIF [16] Ours", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[31] Video2GIF [16] Ours", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "The F1 score is the harmonic mean of precision of relevance prediction and cluster recall [19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "[31] and Video2GIF [16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[31] and Video2GIF [16].", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "Query-dependent Thumbnail Selection Dataset (QTS) [31] We compare against the state of the art on the QTS evaluation dataset in Tab.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "[31] from their paper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "We compare the re-implementation of [31] using titles in detail in Tab.", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "22% over [31] and correlation by a margin of 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "As can be seen QAR outperforms [31] for all recall ratios.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "MMR [4] X (33%) X (66%) \u2212 \u2212 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 47, "context": "Hecate [51] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "QAR (ours) significantly outperforms the previous state of the art of [16, 31], even when augmenting Liu et al.", "startOffset": 70, "endOffset": 78}, {"referenceID": 28, "context": "QAR (ours) significantly outperforms the previous state of the art of [16, 31], even when augmenting Liu et al.", "startOffset": 70, "endOffset": 78}, {"referenceID": 28, "context": "[31] with an LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31]+LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "We compare the performance of our full model with each individual objective, a baseline based on Maximal Marginal Relevance (MMR) [4] and Hecate [51].", "startOffset": 130, "endOffset": 133}, {"referenceID": 47, "context": "We compare the performance of our full model with each individual objective, a baseline based on Maximal Marginal Relevance (MMR) [4] and Hecate [51].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "H ec at e [5 1]", "startOffset": 10, "endOffset": 15}, {"referenceID": 0, "context": "H ec at e [5 1]", "startOffset": 10, "endOffset": 15}, {"referenceID": 2, "context": "M M R [4 ]", "startOffset": 6, "endOffset": 10}, {"referenceID": 47, "context": "Figure 6: We show video summaries created by Hecate [51], MMR [4], our similarity model and our full summarization approach.", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "Figure 6: We show video summaries created by Hecate [51], MMR [4], our similarity model and our full summarization approach.", "startOffset": 62, "endOffset": 65}, {"referenceID": 47, "context": "Finally, we compare it to Hecate, a recent method introduced in [51].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "It outperforms all single objectives, as well as the MMR [4] baseline, even though MMR also uses our wellperforming similarity estimation.", "startOffset": 57, "endOffset": 60}, {"referenceID": 47, "context": "Somewhat surprisingly, Hecate [51] is a relatively strong baseline.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "[32, 34], and might be used to improve our method.", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[32, 34], and might be used to improve our method.", "startOffset": 0, "endOffset": 8}, {"referenceID": 44, "context": "In contrast to earlier works, such as [48, 63], this model allows us to handle unconstrained queries and even full sentences.", "startOffset": 38, "endOffset": 46}, {"referenceID": 58, "context": "In contrast to earlier works, such as [48, 63], this model allows us to handle unconstrained queries and even full sentences.", "startOffset": 38, "endOffset": 46}, {"referenceID": 28, "context": "different improvements over [31], for learning a relevance model.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "data, we tested our full summarization framework and showed that it compares favourably to strong baselines such as MMR [4] and [51].", "startOffset": 120, "endOffset": 123}, {"referenceID": 47, "context": "data, we tested our full summarization framework and showed that it compares favourably to strong baselines such as MMR [4] and [51].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture queryindependent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.", "creator": "LaTeX with hyperref package"}}}