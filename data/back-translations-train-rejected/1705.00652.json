{"id": "1705.00652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Efficient Natural Language Response Suggestion for Smart Reply", "abstract": "This paper presents a computationally efficient machine-learned method for natural language response suggestion. Feed-forward neural networks using n-gram embedding features encode messages into vectors which are optimized to give message-response pairs a high dot-product value. An optimized search finds response suggestions. The method is evaluated in a large-scale commercial e-mail application, Inbox by Gmail. Compared to a sequence-to-sequence approach, the new system achieves the same quality at a small fraction of the computational requirements and latency.", "histories": [["v1", "Mon, 1 May 2017 18:24:15 GMT  (431kb,D)", "http://arxiv.org/abs/1705.00652v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matthew henderson", "rami al-rfou", "brian strope", "yun-hsuan sung", "laszlo lukacs", "ruiqi guo", "sanjiv kumar", "balint miklos", "ray kurzweil"], "accepted": false, "id": "1705.00652"}, "pdf": {"name": "1705.00652.pdf", "metadata": {"source": "CRF", "title": "Efficient Natural Language Response Suggestion for Smart Reply", "authors": ["MATTHEW HENDERSON", "RAMI AL-RFOU", "BRIAN STROPE", "YUN-HSUAN SUNG", "L\u00c1SZL\u00d3 LUK\u00c1CS", "RUIQI GUO", "SANJIV KUMAR", "BALINT MIKLOS"], "emails": ["bps}@google.com."], "sections": [{"heading": null, "text": "Efficient Natural Language Response Suggestion for Smart ReplyMATTHEW HENDERSON, RAMI AL-RFOU, BRIAN STROPE, YUN-HSUAN SUNG, LA \"SZLO\" LUKA \"CS, RUIQI GUO, SANJIV KUMAR, BALINT MIKLOS, and RAY KURZWEIL, GoogleThis paper presents a computationally efficient machine-learned method for responding to natural language. Forward-facing neural networks using N-gram embedding encode messages in vectors optimized to give message-to-product pairs a high value. An optimized search finds response suggestions. The method is evaluated in a large-scale commercial email application, Inbox by Gmail. Compared to a sequence-to-sequence-to-product approach, the new approach achieves the same quality of click-based fraction with a small complementary and computerized encoding requirement."}, {"heading": "1 INTRODUCTION", "text": "In fact, most people are able to decide whether they will be able to play by the rules, or whether they will be able to break the rules."}, {"heading": "2 PROBLEM DEFINITION", "text": "The Smart Reply system provides short response suggestions to help users respond quickly to e-mails. E-mails are processed by the system according to the pipeline shown in Figure 2.The decision on whether to make suggestions is made by a deep neural network classifier, the so-called triggering model. If the output of the triggering model exceeds a threshold, Smart Reply m (typically 3) gives short response suggestions for the e-mail. Otherwise, no suggestions are made. As a result, suggestions are not shown for e-mails where a response is not likely (e.g. spam, newsletters, and promotional e-mails), thereby reducing clutter in the user interface and saving unnecessary arithmetic operations. The system is limited to a fixed set of response suggestions, R, selected from millions of common messages. The response selection step involves searching for the uppermost N (typically around 100) suggestions in the response selection (R) and the choice (x) of a variant."}, {"heading": "3 BASELINE SEQUENCE-TO-SEQUENCE SCORING", "text": "The response selection model presented in Kannan et al. [11] is a recursive neural network [8] - an application of the Sequence-to-Sequence Learning Framework (Seq2Seq) [23]. Input e-mail x is converted to a word sequence (x1,.., xm), and LSTM calculates the conditional probability of a response string y = (y1,.., yn) as: P (y | x) = P (y1,.,.., yn | x1,.., xm) = 1 PLSTM (yi | x1,.., yi \u2212 1), where PLSTM is the output of the word level LSTM. LSTM is trained to maximize the probability of loyalty according to P (y | x) of the training data (a large collection of e-mails and answers found in this section b)."}, {"heading": "4 FEEDFORWARD APPROACH", "text": "Instead of learning a generative model, we study learning a feedback network to evaluate potential responses. Let's remember the goal of response selection, which is used to classify possible responses y based on an input mail x. This probability distribution can be written as follows: P (y | x) = P (x, y) \u2211 k P (x, yk) (1) The common probability of P (x, y) is estimated based on a learned Neural Network Assessment function, S such that: P (x, y) \u0192eS (x, y) (2) Note that the computation of Equation 1 requires summing up the results of the neural network for all possible responses yk. (This is only a question of training, not a conclusion, as the denominator is a constant for each given x and therefore does not affect the arg max over y."}, {"heading": "4.1 N-gram Representation", "text": "In order to represent input e-mails x and answers y as fixed-dimensional input characteristics, we extract Ngram characteristics. During the training, we learn a d-dimensional embedding for each n-gram together with the other neural network parameters. To represent word sequences, we combine N-gram embedding by adding their values. We refer to this bag of N-gram representations as \u044b (x) and Rd. This representation is quick to calculate and captures basic semantic and word order information."}, {"heading": "4.2 Joint Scoring Model", "text": "Figure 3a shows the common scoring model of neural networks, which takes into account the n-gram representations of e-mail input x and response y and generates a scalar score S (x, y). This deep neural network can model complex common interactions between input and response when calculating the score."}, {"heading": "4.3 Dot-Product Scoring Model", "text": "Figure 3b shows the structure of the point product scoring model, in which S (x, y) is factored as a point product between a vector hx, which only depends on x, and a vector hy, which only depends on y. This is similar to Deep Structured Semantic Models, which use feedback networks to project queries and documents into a common space, in which the relevance of a document that has received a query is calculated as a cosinal distance between them [9]. While the interaction between features is not as direct as the common scoring model (see Section 4.2), this factoring allows us to calculate independently the representation of the input x and possible responses. In particular, the representations of the response set R can be pre-calculated. In this case, the search for suggested responses is reduced to the coding of a new e-mail x in a simple forward step on the vector followed by hx and the search for high scores (see section 4.7)."}, {"heading": "4.4 Multiple Negatives", "text": "Let us remember from Section 4 that a set of K possible answers is used to approximate P (y | x) - a correct answer and K \u2212 1 random negatives. For efficiency reasons, we use the answers of other examples in a training program for stochastic gradient descent as negative answers. For a group of size K, there are K input emails x = (x1,..., xK) and their corresponding responses y = (y1,., yK). Each answer yj is effectively treated as a negative candidate for xi if i 6 = j. For a single group, the K \u2212 1 negative examples for each x are different each time the data is passed due to the shuffling in stochastic gradient decrease. The goal of the training is to minimize the approximate negative probability of loyalty of the data. For a single group, this is: J (x, y, \u03b8) = \u2212 1 KK-i = 1logboard x (yi | xi) = 1 function xi, S (j-j), where this function is equal to S-1 (S-1), whereby this value is performed."}, {"heading": "4.5 Incorporating Multiple Features", "text": "There is a structure in e-mails that can be used to improve the accuracy of the scoring models. We follow the multi-loss architecture of Al-Rfou et al. [2] to integrate additional functions beyond the message body, for example the subject line. Figure 4 shows the multi-loss architecture, which is applied to both the common and the point-product scoring model. Multi-loss networks have a sub-network for each feature of the e-mail, which is trained to independently receive responses from candidates who use this feature alone. The highest level hidden layer of the sub-network is used in a final sub-network, which is trained to combine the information from all the features and give a final assessment. This hierarchical structure leads to models that learn how to use each feature faster than a network that sees all the features at once, and also allows deeper networks to learn than is otherwise possible to input e-mail [x-form] an e-mail."}, {"heading": "4.6 Response Biasing", "text": "The discriminatory objective function introduced in Section 4.4 leads to a distorted estimate of the denominator in Equation (1). Since our negative examples come from the distribution of training data, ordinary answers are more likely to appear as negative examples. In practice, we observed that this distortion leads to models that prefer specific and long answers rather than short and general answers. To encourage more generic answers, we distort the answers in R based on a score derived from the probability of logic of the answer as estimated by a language model. Our end result Sf (x, y) of any answer pair sent by email is calculated as follows: Sf (x, y) = Sm (x, y) + \u03b1 logPLM (y) (6), where Sm is the score calculated by our trained scoring model, PLM (y) is the probability of y according to the language model, and \u03b1 is matched with online experiments."}, {"heading": "4.7 Hierarchical Quantization for Efficient Search", "text": "In the conclusion of the time we need to maintain a very high retrieval memory, we use the Dot product scoring model in order to find answers to the question whether Q = Q = Q = Q = Q = Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q Q = Q Q = Q Q = Q Q = Q Q Q = Q Q Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q Q = Q = Q Q = Q"}, {"heading": "5 EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "Data. Email pairs and their responses are scanned from user data to create data sets for training and testing of the scoring models for feedback replies. In total, about 300 million pairs are collected, and the data is randomly divided into two disjointed sets of 95% and 5% that make up the training and test sets. All email data (raw data, pre-processed data, and training / evaluation data) is encrypted. Engineers can only view aggregated statistics on anonymized sentences that occurred with many users and do not identify user.Language identification is performed in the emails, and only emails in English are retained. Subject lines and message bodies are combined into word sequences from which N-gram features are extracted. Common words, URLs, email addresses, phone numbers, etc. are replaced by special tokens. Type text that is also redirected and removed from the forward."}, {"heading": "5.2 Offline Evaluation", "text": "Our models are evaluated offline for their ability to determine the true response to an email in the test data against a series of randomly selected competing responses. In this work, we evaluate a set of 100 responses that contain the correct response, and 99 randomly selected false competitors. We evaluate responses according to their results and give the accuracy 1 (P @ 1) as the benchmark for the rating. We found that P @ 1 correlates with the quality of our models as measured in online experiments with users (see Section 5.3). Batch Size Scoring Model P @ 1Table 2 presents the results of offline evaluation of scoring models for joint and dot products. The common scoring model does not scale well to larger stacks, as each possible pre-run requires the complete pairing of emails and the response to a full email."}, {"heading": "5.3 Online Evaluation", "text": "In fact, most of them will be able to play by the rules they have established in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "6 CONCLUSIONS", "text": "This paper presents a feed-forward approach to evaluating the consistency between input messages and potential responses. A hierarchy of deep networks using simple n-gram representations outperforms competitive sequence-to-sequence models in this context. The deep networks use different components to read input and pre-calculate the representation of possible responses. This architecture enables a highly efficient runtime search. We evaluate the models using the Smart Reply application. Live experiments with production traffic enabled a series of improvements that resulted in a system of higher quality than the original sequence-to-sequence system and at a small fraction of the computation and latency time. Without addressing the generation of novel responses, this paper proposes a minimal, efficient and scalable implementation that enables many rank-based applications."}, {"heading": "ACKNOWLEDGMENTS", "text": "Thanks to Fernando Pereira, Corinna Cortes, Anjuli Kannan, Dilek Hakkani-Tu \ufffd r and Larry Heck for their valuable contributions to this article. We would also like to thank the many engineers at Google whose work on the tools and infrastructure made these experiments possible. Special thanks to the users of Smart Reply."}], "references": [{"title": "et al", "author": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard"], "venue": "Tensorflow: A system for large-scale machine learning. In USENIX Symposium on Operating Systems Design and Implementation (OSDI)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Conversational contextual cues: The case of personalization and history for response ranking", "author": ["R. Al-Rfou", "M. Pickett", "J. Snaider", "Y. Sung", "B. Strope", "R. Kurzweil"], "venue": "arXiv preprint arXiv:1606.00372", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Clustering is efficient for approximate maximum inner product search", "author": ["A. Auvolat", "S. Chandar", "P. Vincent", "H. Larochelle", "Y. Bengio"], "venue": "arXiv preprint arXiv:1507.05910", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Vector quantization", "author": ["R.M. Gray"], "venue": "ASSP Magazine, IEEE, 1(2):4\u201329", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1984}, {"title": "Quantization based fast inner product search", "author": ["R. Guo", "S. Kumar", "K. Choromanski", "D. Simcha"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["H. He", "K. Gimpel", "J.J. Lin"], "venue": "Empirical Methods on Natural Language Processing (EMNLP)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "Special Interest Group on Discourse and Dialogue (SIGDIAL)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "Pattern Analysis and Machine Intelligence, 33(1)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Smart Reply: Automated response suggestion for email", "author": ["A. Kannan", "K. Kurach", "S. Ravi", "T. Kaufman", "B. Miklos", "G. Corrado", "A. Tomkins", "L. Luk\u00e1cs", "M. Ganea", "P. Young", "V. Ramavajjala"], "venue": "Conference on Knowledge Discovery and Data Mining (KDD). ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "How to Create a Mind: The Secret of Human Thought Revealed", "author": ["R. Kurzweil"], "venue": "Penguin Books, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "X. He", "L. Heck", "G. Tur", "D. Hakkani-T\u00fcr", "D. Yu", "G. Zweig"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Cartesian k-means", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "Conference on Computer Vision and Pattern Recognition, pages 3017\u20133024. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods on Natural Language Processing (EMNLP)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P.J. Price"], "venue": "Workshop on Speech and Natural Language, HLT \u201990. Association for Computational Linguistics", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1990}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "Conference on Artificial Intelligence. AAAI", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Swivel: Improving embeddings by noticing what\u2019s missing", "author": ["N. Shazeer", "R. Doherty", "C. Evans", "C. Waterson"], "venue": "arXiv preprint arXiv:1602.02215", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning binary codes for maximum inner product search", "author": ["F. Shen", "W. Liu", "S. Zhang", "Y. Yang", "H. Tao Shen"], "venue": "International Conference on Computer Vision. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)", "author": ["A. Shrivastava", "P. Li"], "venue": "Advances in neural information processing systems (NIPS)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems (NIPS)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "Advances in neural information processing systems (NIPS)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q.V. Le"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["T.-H. Wen", "D. Vandyke", "N. Mrksic", "M. Gasic", "L.M. Rojas-Barahona", "P.-H. Su", "S. Ultes", "S. Young"], "venue": "arXiv preprint arXiv:1604.04562", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning", "author": ["J.D. Williams", "G. Zweig"], "venue": "arXiv preprint arXiv:1606.01269", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey", "J. Klingner", "A. Shah", "M. Johnson", "X. Liu", "L. Kaiser", "S. Gouws", "Y. Kato", "T. Kudo", "H. Kazawa", "K. Stevens", "G. Kurian", "N. Patil", "W. Wang", "C. Young", "J. Smith", "J. Riesa", "A. Rudnick", "O. Vinyals", "G. Corrado", "M. Hughes", "J. Dean"], "venue": "arXiv preprint arXiv:1609.08144", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M.-Y. Hwang", "Y. Shi", "D. Yu"], "venue": "Interspeech", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "ABCNN: Attention-based convolutional neural network for modeling sentence pairs", "author": ["W. Yin", "H. Sch\u00fctze", "B. Xiang", "B. Zhou"], "venue": "Transactions of the Association for Computational Linguistics, 4", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Talking to machines (statistically speaking)", "author": ["S. Young"], "venue": "Interspeech", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 17, "context": "Early NLU systems parsed natural language with hand-crafted rules to explicit semantic representations, and used manually written state machines to generate specific responses from the output of parsing [18].", "startOffset": 203, "endOffset": 207}, {"referenceID": 30, "context": "These systems are brittle, and progress is slow [31].", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "For example, neural network models have been used to learn more robust parsers [14, 24, 29].", "startOffset": 79, "endOffset": 91}, {"referenceID": 23, "context": "For example, neural network models have been used to learn more robust parsers [14, 24, 29].", "startOffset": 79, "endOffset": 91}, {"referenceID": 28, "context": "For example, neural network models have been used to learn more robust parsers [14, 24, 29].", "startOffset": 79, "endOffset": 91}, {"referenceID": 6, "context": "In recent work, the components of task-oriented dialog systems have been implemented as neural networks, enabling joint learning of robust models [7, 26, 27].", "startOffset": 146, "endOffset": 157}, {"referenceID": 25, "context": "In recent work, the components of task-oriented dialog systems have been implemented as neural networks, enabling joint learning of robust models [7, 26, 27].", "startOffset": 146, "endOffset": 157}, {"referenceID": 26, "context": "In recent work, the components of task-oriented dialog systems have been implemented as neural networks, enabling joint learning of robust models [7, 26, 27].", "startOffset": 146, "endOffset": 157}, {"referenceID": 18, "context": "End-to-end systems avoid using hand-crafted explicit representations, by learning to map to and from natural language via implicit internal vector representations [19, 25].", "startOffset": 163, "endOffset": 171}, {"referenceID": 24, "context": "End-to-end systems avoid using hand-crafted explicit representations, by learning to map to and from natural language via implicit internal vector representations [19, 25].", "startOffset": 163, "endOffset": 171}, {"referenceID": 14, "context": "Vector representations of words, or word embeddings, have been widely adopted, particularly since the introduction of efficient computational learning algorithms that can derive meaningful embeddings from unlabeled text [15, 17, 20].", "startOffset": 220, "endOffset": 232}, {"referenceID": 16, "context": "Vector representations of words, or word embeddings, have been widely adopted, particularly since the introduction of efficient computational learning algorithms that can derive meaningful embeddings from unlabeled text [15, 17, 20].", "startOffset": 220, "endOffset": 232}, {"referenceID": 19, "context": "Vector representations of words, or word embeddings, have been widely adopted, particularly since the introduction of efficient computational learning algorithms that can derive meaningful embeddings from unlabeled text [15, 17, 20].", "startOffset": 220, "endOffset": 232}, {"referenceID": 22, "context": "This framework provides a direct path for end-to-end learning [23].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "With attention mechanisms and more layers, these systems are revolutionizing the field of machine translation [28].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "A similar system was initially used to deployed Google\u2019s Smart Reply system for Inbox by Gmail [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "In a broader context, Kurzweil\u2019s work outlines a path to create a simulation of the human neocortex (the outer layer of the brain where we do much of our thinking) by building a hierarchy of similarly structured components that encode increasingly abstract ideas as sequences [12].", "startOffset": 276, "endOffset": 280}, {"referenceID": 12, "context": "Similarly, the work on paragraph vectors shows that word embeddings can be back-propagated to arbitrary levels in a contextual hierarchy [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 5, "context": "paper (see section 4) are computationally inexpensive relative to RNN and convolutional network [6, 30] encoders.", "startOffset": 96, "endOffset": 103}, {"referenceID": 29, "context": "paper (see section 4) are computationally inexpensive relative to RNN and convolutional network [6, 30] encoders.", "startOffset": 96, "endOffset": 103}, {"referenceID": 10, "context": "[11], and use our models to give Smart Reply response suggestions to users of Inbox by Gmail (see figure 1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], we consider natural language response suggestion from a fixed set of candidates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] used a sequence-to-sequence model for P (y | x) and used a beam search over the", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] is a long short-term memory (LSTM) recurrent neural network [8] \u2013 an application of the sequence-to-sequence learning framework (Seq2Seq) [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[11] is a long short-term memory (LSTM) recurrent neural network [8] \u2013 an application of the sequence-to-sequence learning framework (Seq2Seq) [23].", "startOffset": 65, "endOffset": 68}, {"referenceID": 22, "context": "[11] is a long short-term memory (LSTM) recurrent neural network [8] \u2013 an application of the sequence-to-sequence learning framework (Seq2Seq) [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "This is similar to Deep Structured Semantic Models, which use feedforward networks to project queries and documents into a common space where the relevance of a document given a query is computed as the cosine distance between them [9].", "startOffset": 232, "endOffset": 235}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] to incorporate additional features beyond the message body, for example the subject line.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This hierarchical structure results in models that learn how to use each feature faster than a network that sees all the features at once, and also allows for learning deeper networks than is otherwise possible [2].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "For more background, we refer readers to the relevant works of [3, 5, 21, 22].", "startOffset": 63, "endOffset": 77}, {"referenceID": 4, "context": "For more background, we refer readers to the relevant works of [3, 5, 21, 22].", "startOffset": 63, "endOffset": 77}, {"referenceID": 20, "context": "For more background, we refer readers to the relevant works of [3, 5, 21, 22].", "startOffset": 63, "endOffset": 77}, {"referenceID": 21, "context": "For more background, we refer readers to the relevant works of [3, 5, 21, 22].", "startOffset": 63, "endOffset": 77}, {"referenceID": 4, "context": "Unlike the previous work in [5], we propose a hierarchical combination of vector quantization, orthogonal transformation and product quantization of the transformed vector quantization residuals.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "Product quantization works by decomposing the high-dimensional vectors into low-dimensional subspaces and then quantizing them separately [4].", "startOffset": 138, "endOffset": 141}, {"referenceID": 15, "context": "We use a learned rotation before product quantization as it has been shown to improve quantization error [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "hx V Q(hy) + (Rhx) PQ(ry) The distance computation can be performed very efficiently without reconstructing HQ(hy), instead utilizing a lookup table for asymmetric distance computation [10].", "startOffset": 185, "endOffset": 189}, {"referenceID": 2, "context": "Clustering [3]", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "ALSH [20]", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "The curve is produced by varying the number of approximate neighbors retrieved by our hierarchical quantization method and by Asymmetric LSH [22], and varying the number of leaves searched by the clustering algorithm of [3].", "startOffset": 141, "endOffset": 145}, {"referenceID": 2, "context": "The curve is produced by varying the number of approximate neighbors retrieved by our hierarchical quantization method and by Asymmetric LSH [22], and varying the number of leaves searched by the clustering algorithm of [3].", "startOffset": 220, "endOffset": 223}, {"referenceID": 2, "context": "89% recall with a speed-up factor over 10, outperforming the baselines of [3, 22].", "startOffset": 74, "endOffset": 81}, {"referenceID": 21, "context": "89% recall with a speed-up factor over 10, outperforming the baselines of [3, 22].", "startOffset": 74, "endOffset": 81}, {"referenceID": 0, "context": "The models are trained on CPUs across 50 machines using a distributed implementation of TensorFlow [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "This paper presents a computationally efficient machine-learned method for natural language response suggestion. Feed-forward neural networks using n-gram embedding features encode messages into vectors which are optimized to give message-response pairs a high dot-product value. An optimized search finds response suggestions. The method is evaluated in a large-scale commercial e-mail application, Inbox by Gmail. Compared to a sequence-to-sequence approach, the new system achieves the same quality at a small fraction of the computational requirements and latency.", "creator": "LaTeX with hyperref package"}}}