{"id": "1705.00673", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "MACA: A Modular Architecture for Conversational Agents", "abstract": "We propose a software architecture designed to ease the implementation of dialogue systems. The Modular Architecture for Conversational Agents (MACA) uses a plug-n-play style that allows quick prototyping, thereby facilitating the development of new techniques and the reproduction of previous work. The architecture separates the domain of the conversation from the agent's dialogue strategy, and as such can be easily extended to multiple domains. MACA provides tools to host dialogue agents on Amazon Mechanical Turk (mTurk) for data collection and allows processing of other sources of training data. The current version of the framework already incorporates several domains and existing dialogue strategies from the recent literature.", "histories": [["v1", "Mon, 1 May 2017 19:18:04 GMT  (90kb,D)", "http://arxiv.org/abs/1705.00673v1", "9 pages"], ["v2", "Wed, 3 May 2017 01:20:26 GMT  (0kb,I)", "http://arxiv.org/abs/1705.00673v2", "The architecture needs to be tested further. Sorry for the inconvenience. We should be putting up the paper up soon"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.AI cs.SE", "authors": ["hoai phuoc truong", "prasanna parthasarathi", "joelle pineau"], "accepted": false, "id": "1705.00673"}, "pdf": {"name": "1705.00673.pdf", "metadata": {"source": "CRF", "title": "MACA: A Modular Architecture for Conversational Agents", "authors": ["Hoai Phuoc Truong", "Prasanna Parthasarathi", "Joelle Pineau"], "emails": ["phuoc.truong2@mail.mcgill.ca", "prasanna.p@cs.mcgill.ca", "jpineau@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "These models have been applied to a variety of consumer areas, such as restaurant bookings (Kim and Banchs, 2014), flight bookings (Young, 2006), etc. However, the lack of tools for easy prototyping of newer models remains an obstacle to the development of new models and proper benchmarking compared to previous models. In addition, the different types of conversation agents - e.g. generative (Hochreiter and Schmidhuber, 1997; Serban et al., 2015, 2016), retrieval-based (Schatzmann et al.), phuoc.truong2 @ mail.mcgill.ca \u2020 prasanna.p @ cs.mcgill.ca @ jpineau @ cs.mcgill.ca2005a; Lowe et al., 2015a), slot-based dialogue agents (Png and Pineau, 2011), which support different mechanisms for developing dialogues."}, {"heading": "2 Related Work", "text": "Ravenclaw (Bohus and Rudnicky, 2003), proposed as the successor to Agenda (Allen et al., 2001), is a two-tiered dialog architecture that supports the rapid development of dialog agents. This flexible architecture provides a clear separation between domain knowledge and dialog agents and maintains a hierarchical task structure. Systems can be built on the architecture with the hierarchical task layout, but adding a new task requires rebuilding the hierarchy, which hinders application to new domains. In addition, a hierarchical architecture similar to Ravenclaw, called Task Completion Platform (TCP) (Crook et al., 2016), allows extensibility of domain knowledge with minimal changes to a configuration file. In addition, it allows target tasks to be easily defined using a TaskForm language to maintain slot information. Although TCP enables the expansion of explicit-based dialog agents to multiple domains."}, {"heading": "3 Architecture Description", "text": "The system is built as a pipeline with six main components: Input, Pre-Processing, Dialog Model, Post-Processing, Output and Listeners. Each component contains independent sub-components that interact with each other. All components within the architecture abstract their underlying implementations and thus allow for easy expansion. This helps block-by-block design of newer systems by maintaining the original functionality, but also providing a free hand in customizing each component."}, {"heading": "3.1 Component Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Domain Knowledge", "text": "Domain knowledge contains static background information on the topic of conversation. This can take the form of training data (e.g. transcribed conversations), constants, dictionaries, or limitations of produced answers (e.g. sentence length, forbidden phrases). Data stored in Domain Knowl-Edge must be independent of the model implementation and can be shared between different models and components."}, {"heading": "3.1.2 Input", "text": "The input module delivers or generates input expressions (i.e. statements, sentences) to the conversation pipeline. This component represents an abstract input device whose context source varies depending on the application. It can include a database of already collected conversations, a terminal interface (i.e. stdin) for real-time data acquisition, or a web interface to a data source (i.e. mTurk)."}, {"heading": "3.1.3 Preprocessing", "text": "The pre-processing module serves as a bridge between the raw data acquired via the input component and the input format required by the components of the dialog module. The system architect can choose to include one or more pre-processing operations within this module. By default, these pre-processing operations are performed in parallel and their results are fed into the next component as an array, allowing the dialog model to include multiple input presentations. Alternatively, the framework also allows these operations to be processed sequentially in a specified sequence (e.g. spell correction followed by stemming). Pre-processing operations currently implemented in MACA include: retrieving POS tags, removing stopwords, sentence tokenizing (Loper and Bird, 2002), byte-pair encoding (Gage, 1994) and can be expanded to include trained send2vec models (Le vec and Mikolov, 2013) that may also require micolov models."}, {"heading": "3.1.4 Dialogue Model", "text": "This module is the core of the architecture and contains implementations of agents capable of responding to the ready-made input information. It can have up to three subcomponents: Model Specific Pre-Processing, Model Internals, and Model Specific Post-Processing to accommodate the dialog processes with different interface requirements. Model Internals contains the central dialog model that can be an existing model, such as a POMDP (Png and Pineau, 2011), dual encoder (Lowe et al.), HRED agent (Serban et al., 2015), or a newly designed model. This submodule receives input from the Model Specific Pre-Processing submodule."}, {"heading": "3.1.5 Postprocessing", "text": "The post-processing component connects the dialog model with the output components. It allows the architect to select the response in the case of a multi-response retrieval, modify responses based on linguistic characteristics, or modify a response according to the call domain. It can also serve as a translation of text into system calls, which is useful when a dialog agent is placed as a front-end interface to another software system. Similar to the preprocessing module, this component includes one or more post-processing operations that process the output in parallel or succession, depending on the designer's specification. In addition, these post-processing operations within the post-processing component can also query the domain knowledge component for relevant data needed to generate text reactions."}, {"heading": "3.1.6 Output", "text": "Through the output component, the architecture provides a generic way to output the response to appropriate target groups depending on the application. Implemented options are currently command line, file-based, web-based, and database. Similar to the input component, the output component gives the architect flexibility to change the target of the output produced and separate the output programming logic from that of other components."}, {"heading": "3.1.7 Pubsub system/Listeners", "text": "In addition to the main pipeline described above, the proposed system also includes a passive sublayer for monitoring, recording and independent evaluation of the model. This subsystem allows the architect to select or connect a wide range of peripheral components (called listeners) to passively monitor the main system in terms of execution behavior and performance. In addition to several standard channels (see Operating Modes section below) to which the system writes and from which it reads, users can freely add their own channels to communicate between the main system and the sublayer hosting the periphery. Listeners are, as previously mentioned, optional modules that can be plugged in to passively monitor the system across multiple channels. These modules are useful if the architect is interested in monitoring system inputs and / or outputs or visualizing internal parameters or states of the dialog model at execution time. Passive monitoring logic can be introduced independently into the system without altering the other implementations."}, {"heading": "3.2 Operation modes", "text": "MACA can be operated in three different modes: data collection, training and execution. This section describes the data flow in the architecture together with abstract arrangements of the components of the framework in these different operating modes for several dialog models from the current literature."}, {"heading": "3.2.1 Data Collection Mode", "text": "In this mode, the two agents involved in the conversation, Alice and Bob, are considered to be the input component or dialog model component, respectively. Figure 2 describes a typical setup for the data acquisition process with said configuration. The conversation is recorded using a database listener that receives both input (context) and output (response) for each call, similar to the scheme described in Section 3.2.3. This setup implements the infrastructure required for two common dialog data capture scenarios. The first scenario is the collection of both contexts and responses, in which case both agents are human. In the second scenario, the goal is to collect human responses for a specific group of contexts. In this case, Agent Alice may be an implementation of the input component that retrieves contexts from a database, while Bob is a human agent that responds to the contexts found."}, {"heading": "3.2.2 Training Mode", "text": "The objective of the training and validation mode is to use the data gained in the data acquisition phase to train one or more dialog models as shown in Figure 3. Assuming that a dataset from the component \"Domain Knowledge\" is available, training data can be accessed as a batch from the component \"Input\" and fed into the component \"VoidPreprocessing.\" This component simply forwards the data as before to the component \"Dialog Model,\" which performs model training, and occasionally asks domain knowledge for validation data to verify training progress. As the system output is irrelevant within the training scenario, post-processing and output components with zero operations are implemented, which simply discard their received content. Once a certain validation accuracy is reached, the model can store its internals on the disk and terminate the system. In addition to the core training process, the architect can decide on the training step to send the training information to the listener."}, {"heading": "3.2.3 Execution Mode", "text": "The data flow in execution mode is illustrated in Figure 4. In this mode, all core components in the system are activated and active. As the dialog model has been successfully trained and refined, its internal states (e.g. weights, hyperparameters) are loaded into the dialog model component during system initialization. Input data is retrieved in real time (via a local user interface (e.g. terminal, GUI) or via an interface with the Internet (e.g. website, chat client). This input then enters the pipeline and passes through preprocessing, dialogue model, post processing and finally the output component. At the end of the pipeline, the output component is responsible for sending the generated responses to relevant target groups (e.g. pressure on Stdout, HTTP response,...).From the perspective of peripheral components, the call recording and system monitoring can take place via two standard channels: input and output channel for each listener to receive a specific input channel."}, {"heading": "4 Feature Highlights", "text": "As in the previous sections, MACA is used to integrate different types of dialogue partners; the architecture abstracts the implementation of libraries such as Theano (Theano Development Team, 2016), Tensorflow (Abadi et al.) or PyTorch. Its modular design allows to reproduce the results to date; support for experiments, enhancements and the development of dialogue partners for targeted tasks is also available."}, {"heading": "6 Case Studies", "text": "All studies carried out have the same template for the central configuration file, the contents of which are then modified according to the purpose of each study. Listing 1 shows the configuration template, which represents a system with a simple dialog agent that repeats its input (echo agent) The configuration file requires multiple attributes to be mentioned and provides a general view of the experiment carried out. The template contains the following attributes: input, output, preprocessing, post-processing, agent, domain knowledge and listener. The class sub-attribute of the attributes refers to the implementation of the called component in the Python class."}, {"heading": "6.1 Building a simple agent", "text": "The Echo Agent is designed to simply listen and save the input to a file; this is a good first test case for new users of MACA. In this setup, the input attribute is instantiated with StdinInputDevice, which is the command line input, and the output attribute is instantiated with FileOutputDevice, which writes the results to a file. Likewise, the instantiations of the other attributes, such as postprocessing, preprocessing, and domain knowledge, are instantiated with VoidPostprocessor, VoidPreprocessor, and EmptyDomainKnowledge, since the Echo Agent does not need them. \"The agent attribute is instantiated with the corresponding dialog agent, which in this case is Echo Agent. Together with these components, logingListener, which logs the system input and output to an output file:\" Index: \"is instantiated as a list component.\""}, {"heading": "6.2 Building a goal oriented system", "text": "Next, consider the use of MACA to build target-oriented agents for the restaurant, flight booking, and other toy domains. These slot-based agents are developed using the tools provided under the framework that help with hierarchical task definition and slot sharing across tasks. Calling target-oriented policies / sub-agents is done by describing slots - askQuery, Disambiguation Strategy, etc. As with providing multi-agent support, the architecture can treat multiple intentions with intent defined for each of them. For example, \"I would like to book a flight\" triggers the flight-booking policy that fills slots specifically for that task, \"based on the information provided in the domain knowledge, whereas\" What is a good restaurant nearby? \"the configuration file modification in the agent and domain knowledge attributes becomes in the information slot {2,\" Ask3 listing: \"Information Agent: Ask2.\""}, {"heading": "6.3 Building a neural response generation agent", "text": "We also used MACA to develop neural reaction generators based on the Hierarchical Encoder Decoder Framework (Serban et al., 2015)."}, {"heading": "6.3.1 HRED in training mode", "text": "The training mode of MACA was tested with the training process of a HRED agent. Modifications for the central configuration files for this configuration are listed in Listing 3. HREDTrainingInputDevice will simply call the training process by sending an initiate message to the model, while the dialogue model HREDAgent, which is configured for leg training mode, will start its regular training process and write the trained weights to the disk. The training data set will be specified using the subattribute prototype (in accordance with the HRED code base) within the training attribute of the agent. All other components of the pipeline will remain unchanged, since it is unnecessary to move or output the data. {\"RERED agent was used both with the Twitter corpus (Ritter et al., 2011) as well as with the Ubuntu Dialogue Corpus (Lowe et al., 5b.} {REDAD: 6,\" input {\"HRED: 2}:...\""}, {"heading": "6.3.2 HRED in execution mode", "text": "In execution mode, MACA was hosted on a local psiTurk (Gureckis et al., 2016) server that emulates mTurk. A layout where users can chat and rate model responses was provided, and user input was logged by a database listener through the public subarchitecture. In this scenario, the pre-trained HRED model can be considered a case of custom dialog agents adapted to MACA.1 \"agent\": {2 'class': HREDAgent, 3 'kwargs': {4 'ignore unknown words': True, 5 'normalize' data test ', 6' prototype 'HRED', 7 'train dialogs'."}, {"heading": "6.4 Building a neural response retrieval agent", "text": "Finally, we have developed an architecture that integrates a neural retrieval agent using the dual encoder method (Lowe et al., 2015a)."}, {"heading": "6.4.1 Dual Encoder in training mode", "text": "In Listing 5, changes are made to the template configuration to integrate a dual encoder dialog agent in training mode. Similar to the HRED model, the modules Input and Model are replaced in the template configuration. In the case of Dual Encoder, the specified data set is loaded into DomainKnowledge and made accessible after initialization. During the training process, RetrievalModelTrainingInputDevice retrieves the data from the specified training record via DomainKnowledge and transfers it to the dialog model, while the RetrievalModelAgent contains the relevant training parameters. Once the training is complete, RetrievalModelTrainingInputDevice issues a message to the agent to receive trained weights on Disk.1 \"Input\": {2 \"class\": RetrievalModellationsdatInputDevice: 3 \"kwargs,\" \"n epochs.\""}, {"heading": "6.4.2 Dual Encoder in execution mode", "text": "We also tested the dual encoder agent in execution mode, which represents an instance of adapting a model based on demand to the proposed framework, in which case the execution mode received input from a database of previously collected context sponse pairs. The configuration file for the dual encoder model is largely similar to the generic template, with changes to the agent attribute described in Listing 6.1 \"Preprocessing\": {2 \"Module\": [{{3 \"class\": RetrievalModelPreprocessor, 4 \"args\": ['. / retrieval / BPE / Twitter codes 5000.txt'] 5},... 7 'agent': {8 'class': RetrievalModelAgent, 9 'args': ['.. / twitter dataset / W twitter dataset:. \"ppe.pkl'], 10 'kwargs {' model: '11' parcoch {'13:' 13]"}, {"heading": "7 Discussion", "text": "MACA provides a unified architecture for dialog agents that supports plug-n-play of different types of dialog agents and different domains. We hope this will facilitate the rapid development of new models, but also foster reproducibility in dialog system research. Possible limitations of the current implementation of MACA include the simplicity of the public subsystem, the lack of support for distributed hosting of different components of the architecture, and the lack of support for parallel conversations. As a future work, the public subsystem could be improved by capturing a wider range of system information with more public subchannels. In addition, we plan to include new domains and agents as soon as they become available, along with comprehensive ML-based slot disambiguation modules."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G. S Corrado", "A. Davis", "J. Dean", "M. et. al Devin."], "venue": "arXiv:1603.04467 .", "citeRegEx": "Abadi et al\\.,? 2016", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Toward conversational human-computer interaction", "author": ["J. F Allen", "D. K Byron", "M. Dzikovska", "G. Ferguson", "L. Galescu", "A. Stent."], "venue": "AI magazine .", "citeRegEx": "Allen et al\\.,? 2001", "shortCiteRegEx": "Allen et al\\.", "year": 2001}, {"title": "Ravenclaw: Dialog management using hierarchical task decomposition and an expectation agenda", "author": ["D. Bohus", "A. I Rudnicky"], "venue": null, "citeRegEx": "Bohus and Rudnicky.,? \\Q2003\\E", "shortCiteRegEx": "Bohus and Rudnicky.", "year": 2003}, {"title": "Task completion platform: A self-serve multi-domain goal oriented dialogue platform", "author": ["PA Crook", "A Marin", "V Agarwal", "K Aggarwal", "T Anastasakos", "R Bikkula", "D Boies", "A Celikyilmaz", "S Chandramohan", "Z et. al Feizollahi."], "venue": "NAACL HLT .", "citeRegEx": "Crook et al\\.,? 2016", "shortCiteRegEx": "Crook et al\\.", "year": 2016}, {"title": "A new algorithm for data compression", "author": ["P. Gage."], "venue": "The C Users Journal .", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "psiturk: An open-source framework for conducting replicable behavioral experiments online", "author": ["T.M. Gureckis", "J. Martin", "J. McDonnell", "A.S. Rich", "D. Markant", "A. Coenen", "D. Halpern", "J.B. Hamrick", "P. Chan."], "venue": "Behavior research methods .", "citeRegEx": "Gureckis et al\\.,? 2016", "shortCiteRegEx": "Gureckis et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural computation .", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "R-cube: a dialogue agent for restaurant recommendation and reservation", "author": ["S. Kim", "R.E. Banchs."], "venue": "Asia-Pacific Signal and Information Processing Association, 2014 Annual Summit and Conference (APSIPA). IEEE.", "citeRegEx": "Kim and Banchs.,? 2014", "shortCiteRegEx": "Kim and Banchs.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov."], "venue": "ICML.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Nltk: The natural language toolkit", "author": ["E. Loper", "S. Bird."], "venue": "Proceedings of the ACL-02 Work-", "citeRegEx": "Loper and Bird.,? 2002", "shortCiteRegEx": "Loper and Bird.", "year": 2002}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau."], "venue": "arXiv:1506.08909 .", "citeRegEx": "Lowe et al\\.,? 2015a", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau."], "venue": "16th Annual Meeting of the Special Interest Group on Discourse and Dialogue.", "citeRegEx": "Lowe et al\\.,? 2015b", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean."], "venue": "arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bayesian reinforcement learning for pomdp-based dialogue systems", "author": ["S. Png", "J. Pineau."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on. IEEE.", "citeRegEx": "Png and Pineau.,? 2011", "shortCiteRegEx": "Png and Pineau.", "year": 2011}, {"title": "Datadriven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan."], "venue": "EMNLP.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["J. Schatzmann", "K. Georgila", "S. Young."], "venue": "6th SIGdial Workshop on DISCOURSE and DIALOGUE.", "citeRegEx": "Schatzmann et al\\.,? 2005a", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["J. Schatzmann", "K. Georgila", "S. Young."], "venue": "6th SIGdial Workshop on DISCOURSE and DIALOGUE.", "citeRegEx": "Schatzmann et al\\.,? 2005b", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Clippyscript: A programming language for multi-domain dialogue systems", "author": ["F. Seide", "S. McDirmid."], "venue": "Thirteenth Annual Conference of the International Speech Communication Association.", "citeRegEx": "Seide and McDirmid.,? 2012", "shortCiteRegEx": "Seide and McDirmid.", "year": 2012}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau."], "venue": "arXiv:1507.04808 .", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["I. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio."], "venue": "arXiv:1605.06069 .", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv:1605.02688 .", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Using pomdps for dialog management", "author": ["S. Young."], "venue": "Spoken Language Technology Workshop. IEEE.", "citeRegEx": "Young.,? 2006", "shortCiteRegEx": "Young.", "year": 2006}, {"title": "Dialport: Connecting the spoken dialog research community to real user data", "author": ["T. Zhao", "K. Lee", "M. Eskenazi."], "venue": "arXiv:1606.02562 .", "citeRegEx": "Zhao et al\\.,? 2016", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "These models have been applied to a variety of consumer domains, such as restaurant booking (Kim and Banchs, 2014), flight booking (Young, 2006), etc.", "startOffset": 92, "endOffset": 114}, {"referenceID": 21, "context": "These models have been applied to a variety of consumer domains, such as restaurant booking (Kim and Banchs, 2014), flight booking (Young, 2006), etc.", "startOffset": 131, "endOffset": 144}, {"referenceID": 6, "context": "(Hochreiter and Schmidhuber, 1997; Serban et al., 2015, 2016), retrieval-based (Schatzmann et al.", "startOffset": 0, "endOffset": 61}, {"referenceID": 21, "context": ", 2015a), slot-based (Young, 2006) or POMDP agents (Png and Pineau, 2011)\u2013 have different working mechanisms, which pose challenges to the development of a unified platform for conversational agents with multi-domain", "startOffset": 21, "endOffset": 34}, {"referenceID": 13, "context": ", 2015a), slot-based (Young, 2006) or POMDP agents (Png and Pineau, 2011)\u2013 have different working mechanisms, which pose challenges to the development of a unified platform for conversational agents with multi-domain", "startOffset": 51, "endOffset": 73}, {"referenceID": 2, "context": "Ravenclaw (Bohus and Rudnicky, 2003), proposed as a successor to Agenda (Allen et al.", "startOffset": 10, "endOffset": 36}, {"referenceID": 3, "context": "A hierarchical architecture similar to Ravenclaw, called Task Completion Platform (TCP) (Crook et al., 2016), addresses domain knowledge extensibility with minimal changes to a configuration file.", "startOffset": 88, "endOffset": 108}, {"referenceID": 17, "context": "Another notable architecture is ClippyScript (Seide and McDirmid, 2012), but its task definition is tied to a task condition by rule.", "startOffset": 45, "endOffset": 71}, {"referenceID": 22, "context": "As several researches focuses on proposing different architectures for dialogue models, there have also been some made in proposing efficient protocols for agent-agent interaction such as DialPort (Zhao et al., 2016), which provides tools for enabling multi-modal interaction between agents.", "startOffset": 197, "endOffset": 216}, {"referenceID": 9, "context": "Pre-processing operations currently implemented in MACA include: getting POS tags, removing stop-words, sentence tokenizing (Loper and Bird, 2002), Byte-Pair encoding (BPE) (Gage, 1994) and can be extended to accommodate", "startOffset": 124, "endOffset": 146}, {"referenceID": 4, "context": "Pre-processing operations currently implemented in MACA include: getting POS tags, removing stop-words, sentence tokenizing (Loper and Bird, 2002), Byte-Pair encoding (BPE) (Gage, 1994) and can be extended to accommodate", "startOffset": 173, "endOffset": 185}, {"referenceID": 8, "context": "trained sentence2vec model (Le and Mikolov, 2014), trained word2vec model (Mikolov et al.", "startOffset": 27, "endOffset": 49}, {"referenceID": 12, "context": "trained sentence2vec model (Le and Mikolov, 2014), trained word2vec model (Mikolov et al., 2013), etc.", "startOffset": 74, "endOffset": 96}, {"referenceID": 13, "context": "central dialogue model, which may be an existing model, such as a POMDP (Png and Pineau, 2011), Dual Encoder (Lowe et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 10, "context": "central dialogue model, which may be an existing model, such as a POMDP (Png and Pineau, 2011), Dual Encoder (Lowe et al., 2015a), HRED", "startOffset": 109, "endOffset": 129}, {"referenceID": 18, "context": "agent (Serban et al., 2015), or a newly designed model.", "startOffset": 6, "endOffset": 27}, {"referenceID": 0, "context": "The architecture abstracts the implementation details, similar to popular machine learning libraries such as Theano (Theano Development Team, 2016), Tensorflow (Abadi et al., 2016), or PyTorch.", "startOffset": 160, "endOffset": 180}, {"referenceID": 16, "context": "sponses; the framework also supports modelling dialogue tasks as an agent-agent interaction that can be used to test a dialogue agent against simulated users (Schatzmann et al., 2005b).", "startOffset": 158, "endOffset": 184}, {"referenceID": 18, "context": "We also used MACA to prototype neural response generation agents based on the Hierarchical Encoder-Decoder framework (Serban et al., 2015).", "startOffset": 117, "endOffset": 138}, {"referenceID": 14, "context": "The HRED agent was trained using both the Twitter Corpus (Ritter et al., 2011) and Ubuntu Dialogue Corpus (Lowe et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 11, "context": ", 2011) and Ubuntu Dialogue Corpus (Lowe et al., 2015b).", "startOffset": 35, "endOffset": 55}, {"referenceID": 5, "context": "In the data collection mode, MACA was hosted on a local psiTurk (Gureckis et al., 2016) server emulating mTurk.", "startOffset": 64, "endOffset": 87}, {"referenceID": 10, "context": "Finally, we built an architecture that incorporates a neural response retrieval agent operating using the Dual Encoder method (Lowe et al., 2015a).", "startOffset": 126, "endOffset": 146}], "year": 2017, "abstractText": "We propose a software architecture designed to ease the implementation of dialogue systems. The Modular Architecture for Conversational Agents (MACA) uses a plug-n-play style that allows quick prototyping, thereby facilitating the development of new techniques and the reproduction of previous work. The architecture separates the domain of the conversation from the agent\u2019s dialogue strategy, and as such can be easily extended to multiple domains. MACA provides tools to host dialogue agents on Amazon Mechanical Turk (mTurk) for data collection and allows processing of other sources of training data. The current version of the framework already incorporates several domains and existing dialogue strategies from the recent literature.", "creator": "LaTeX with hyperref package"}}}