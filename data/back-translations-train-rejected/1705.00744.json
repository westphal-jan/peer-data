{"id": "1705.00744", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "A Strategy for an Uncompromising Incremental Learner", "abstract": "Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these tricks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique phantom sampling. We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets through our strategy, we demonstrate that strict incremental learning could be achieved.", "histories": [["v1", "Tue, 2 May 2017 00:17:54 GMT  (2638kb,D)", "http://arxiv.org/abs/1705.00744v1", null], ["v2", "Mon, 17 Jul 2017 07:30:18 GMT  (4920kb,D)", "http://arxiv.org/abs/1705.00744v2", "Under review at IEEE Transactions of Neural Networks and Learning Systems"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ragav venkatesan", "hemanth venkateswara", "sethuraman panchanathan", "baoxin li"], "accepted": false, "id": "1705.00744"}, "pdf": {"name": "1705.00744.pdf", "metadata": {"source": "CRF", "title": "A Strategy for an Uncompromising Incremental Learner", "authors": ["Ragav Venkatesan", "Hemanth Venkateswara", "Sethuraman Panchanathan", "Baoxin Li"], "emails": ["ragav.venkatesan@asu.edu,", "hemanthv@asu.edu,", "panch@asu.edu,", "baoxin.li@asu.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it will be ready to leave the country to return to the EU."}, {"heading": "2. Proposed method", "text": "Although Sb and Si train at different speeds and start at different times, in this presentation we focus on systems that mimic the following chronology of events: 1. Sb trains a generative model Gb and a discriminatory model Nb for P (xb) and PNb (y). Si collects the models Gb and Nb and initializes a new model Ni with the parameters of Nb by adding new random parameters as appropriate. Expansion with new random parameters is required since Ni predicts over a wider range of labels.4. Using Di together with phantom sampling of Gb and Nb, Si trains the model Ni until convergence. This is an asymptotic special case of definition established in the previous section and therefore will be considered. Other designs could also be established and we will describe a brief approach in this section."}, {"heading": "3. Related Work", "text": "In recent years, the number of beneficiaries of student loans has increased significantly. (...) In recent years, the number of beneficiaries of student loans has increased dramatically. (...) The number of beneficiaries of student loans has doubled. (...) The number of beneficiaries of student loans has increased. (...) The number of beneficiaries of student loans is increasing. (...) The number of beneficiaries of student loans has doubled. (...) The number of beneficiaries of student loans is increasing. (...) The number of beneficiaries of student loans is increasing. (...) The number of beneficiaries of student loans is increasing. \""}, {"heading": "4. Experiments and Results", "text": "To demonstrate our strategy, we conduct thorough experiments with three benchmark datasets: MNIST datasets for handwritten character recognition, Street View case numbers (SVHN) datasets, and the CIFAR10 class dataset for visual object categorization [15, 22, 13]. In all of our experiments, the Sb's GAN, GB, and base networks are trained using independent processes. The network parameters of all of these models are written to drive, which simulates the transmission of the networks. Once trained, the datasets used to train and test these methods are deleted, killing the data membrane and the process. We begin Si as an independent process, taking into account the independence of the site, which uses a new dataset set set set set set in accordance with property 1. Networks Gb and Nb's parameters are loaded, but only in their feed-forward operations."}, {"heading": "4.1. MNIST dataset", "text": "For the MNIST dataset, we used a GAN-Gb for Gb, which takes 10 samples from a uniform 0-mean Gauss. The generator part of the Gb has three fully connected layers of 1200, 1200 and 784 neurons with ReLU activations for the first two and Tanh activation for the last layers, respectively. [21] The discriminator part of the Gb has two layers of 240 maxout-by-5 neurons [7]. This is an architecture used by Goodfellow et. al, narrow [5]. All of our discriminator networks at both sites Sb and Si are the same architecture, which for the MNIST dataset has two revolutionary layers of 20 and 50 neurons with filter sizes of 5 x 5 and 3 x 3, respectively, with a maximum pooling of 2 on both layers, followed by two fully connected layers of 800 neurons each. All layers in the discriminators are trained by a decompression weight with an IST and a decompression weight of 0.5."}, {"heading": "4.2. CIFAR 10 and SVHN datasets", "text": "For both datasets, we used a generator model in which 64 random samples are examined. Number or neurons in subsequent fully connected layers are 1200 or 5408, followed by two fractionally striped or transposed folding layers with filter sizes 3 x 3 or 5 x 5, respectively. Apart from the last layer that generates the 32x32 image, each layer has ReLU activation. The last layer uses a tanh activation. Our discriminator networks, including the discriminator part of the GANs, have six folding layers with neurons 20, 50, 50, 100, 100, and 250, respectively. Except for the first layer, which has a filter size of 5 x 5, each layer has filter sizes of 3 x 3. Every third layer of maxpools with 2. This is followed by two fully connected layers of 1024 nodes each. All activations are ReLU results or the CIFAR 10 dataset are discussed in Figure 4 and SVHN are discussed in Figure 5."}, {"heading": "5. Conclusions", "text": "In this paper, we have redefined the problem of incremental learning in its most rigorous form, so that it can be a more realistic model for important real-world applications. Using a novel sampling technique that incorporates generative models and distillation techniques, we implemented a strategy to hallucinate samples with appropriate targets using models previously trained and sent. Without access to historical data, we showed that we could still implement an uncompromising incremental learning system without loosening the constraints of our definitions. We show strong and conclusive results using three benchmark data sets to support our strategy."}], "references": [{"title": "Theano: A cpu and gpu math compiler in python", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proc. 9th Python in Science Conf, pages 1\u20137", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Catastrophic interference in connectionist networks: Can it be predicted", "author": ["R.M. French"], "venue": "can it be prevented? In Proceedings of the 6th International Conference on Neural Information Processing Systems, pages 1176\u20131177. Morgan Kaufmann Publishers Inc.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Pseudo-recurrent connectionist networks: An approach to the\u2019sensitivity-stability\u2019dilemma", "author": ["R.M. French"], "venue": "Connection Science, 9(4):353\u2013380", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Active long term memory networks", "author": ["T. Furlanello", "J. Zhao", "A.M. Saxe", "L. Itti", "B.S. Tjan"], "venue": "arXiv preprint arXiv:1606.02355", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, pages 2672\u20132680", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["I.J. Goodfellow", "M. Mirza", "D. Xiao", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6211", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "ICML (3), 28:1319\u20131327", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Types of incremental learning", "author": ["P. Jantke"], "venue": "AAAI Symposium on Training Issues in Incremental Learning, pages 23\u201325", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Less-forgetting learning in deep neural networks", "author": ["H. Jung", "J. Ju", "M. Jung", "J. Kim"], "venue": "arXiv preprint arXiv:1607.00122", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["J. Kirkpatrick", "R. Pascanu", "N. Rabinowitz", "J. Veness", "G. Desjardins", "A.A. Rusu", "K. Milan", "J. Quan", "T. Ramalho", "A. Grabska-Barwinska", "D. Hassabis", "C. Clopath", "D. Kumaran", "R. Hadsell"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "From n to n+1: Multiclass transfer incremental learning", "author": ["I. Kuzborskij", "F. Orabona", "B. Caputo"], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 3358\u20133365", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning without forgetting", "author": ["Z. Li", "D. Hoiem"], "venue": "Proceedings of the European Conf. on Computer Vision (ECCV), pages 614\u2013629. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["M. McCloskey", "N.J. Cohen"], "venue": "Psychology of learning and motivation, 24:109\u2013165", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "Proceedings of the European Conf. on Computer Vision (ECCV), pages 488\u2013501. Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance-based image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, 35(11):2624\u20132637", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Learn++", "author": ["M.D. Muhlbaier", "A. Topalis", "R. Polikar"], "venue": "nc: Combining ensemble of classifiers with dynamically weighted consult-and-vote for efficient incremental learning of new classes. IEEE Trans. on Neural Networks, 20(1):152\u2013 168", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Learn++: An incremental learning algorithm for supervised neural networks", "author": ["R. Polikar", "L. Upda", "S.S. Upda", "V. Honavar"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, part C (Applications and Reviews), 31(4):497\u2013508", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "iCaRL: Incremental classifier and representation learning", "author": ["S.-A. Rebuffi", "A. Kolesnikov", "C.H. Lampert"], "venue": "accepted to the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Incremental learning of NCM forests for large-scale image classification", "author": ["M. Ristin", "M. Guillaumin", "J. Gall", "L. Van Gool"], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 3654\u20133661", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Catastrophic forgetting", "author": ["A. Robins"], "venue": "rehearsal and pseudorehearsal. Connection Science, 7(2):123\u2013146", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, pages 2310\u20132318", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Diving deeper into mentee networks", "author": ["R. Venkatesan", "B. Li"], "venue": "arXiv preprint arXiv:1604.08220", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Herding dynamical weights to learn", "author": ["M. Welling"], "venue": "Proceedings of the ACM Intl. Conf. on Machine Learning (ICML), pages 1121\u20131128", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Errordriven incremental learning in deep convolutional neural network for large-scale image classification", "author": ["T. Xiao", "J. Zhang", "K. Yang", "Y. Peng", "Z. Zhang"], "venue": "Proceedings of the ACM Intl. Conf. on Multimedia (ACM-MM), pages 177\u2013 186", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "One of the earliest formalization of incremental learning comes from the work of Jantke [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "A temperature raised softmax was previously proposed as a means of distilling knowledge in the context of neural network compression [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "Not only does this provide supervision for generated samples, but will also serve as a regularizer while training a machine at Si, similar to the fashion described in [8].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "al, in 2014 and has since seen many advances [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Soft targets such as the one described and their use in producing ambiguous targets exemplifying the relationships between classes were proposed in [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 25, "context": "In this case, we will not be able to re-initialize the network Ni with new weights, but as long as we have phantom samples, we can use a technique similar to mentor nets or fitnets, using embeded losses between Nb and Ni and transfer knowledge about Db to Ni [27] [30].", "startOffset": 259, "endOffset": 263}, {"referenceID": 28, "context": "In this case, we will not be able to re-initialize the network Ni with new weights, but as long as we have phantom samples, we can use a technique similar to mentor nets or fitnets, using embeded losses between Nb and Ni and transfer knowledge about Db to Ni [27] [30].", "startOffset": 264, "endOffset": 268}, {"referenceID": 15, "context": "Catastrophic Forgetting: Early works by McCloskey, French and Robins outlines this issue [17, 2, 26].", "startOffset": 89, "endOffset": 100}, {"referenceID": 1, "context": "Catastrophic Forgetting: Early works by McCloskey, French and Robins outlines this issue [17, 2, 26].", "startOffset": 89, "endOffset": 100}, {"referenceID": 24, "context": "Catastrophic Forgetting: Early works by McCloskey, French and Robins outlines this issue [17, 2, 26].", "startOffset": 89, "endOffset": 100}, {"referenceID": 27, "context": "demonstrated that the choice of activation function affects catastrophic forgetting and introduced the Hard Winner Take All (HWTA) activation [29].", "startOffset": 142, "endOffset": 146}, {"referenceID": 5, "context": "argued that increased dropout works better at minimizing catastrophic forgetting compared to activation functions [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 16, "context": "develop a metric learning method to estimate the similarity (distance) between test samples and the nearest class mean (NCM) [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 17, "context": "develop a metric learning method to estimate the similarity (distance) between test samples and the nearest class mean (NCM) [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 23, "context": "The NCM approach has also been successfully applied to random forest based models for incremental learning in [25].", "startOffset": 110, "endOffset": 114}, {"referenceID": 30, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The Learn++ is an ensemble based approach for incremental learning [23] [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "The Learn++ is an ensemble based approach for incremental learning [23] [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "In [14], Kuzborskij et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The exemplars are chosen based on a herding mechanism that creates a representative set of samples based on a distribution [31].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": ", j] [16].", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "develop a closely related procedure to in [4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "The only difference compared to [16] is in the regularization of network parameters using weight decay and the network initialization.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "constrain the feature representations forDi to be similar to the feature representations for Db [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "model the probability P (wb|Db) and get an estimate for the important parameters in wb [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Closely related to the previous idea is pseudo-rehearsal proposed by Robins in 1995 [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "al, [3].", "startOffset": 4, "endOffset": 7}, {"referenceID": 20, "context": "To demonstrate our strategy we conduct thorough experiments on three benchmark datasets: MNIST dataset of handwritten character recognition, Street view housing numbers (SVHN) dataset and the CIFAR10 10-class visual object categorization dataset [15, 22, 13].", "startOffset": 246, "endOffset": 258}, {"referenceID": 12, "context": "To demonstrate our strategy we conduct thorough experiments on three benchmark datasets: MNIST dataset of handwritten character recognition, Street view housing numbers (SVHN) dataset and the CIFAR10 10-class visual object categorization dataset [15, 22, 13].", "startOffset": 246, "endOffset": 258}, {"referenceID": 24, "context": "Even a GAN that is not trained (implying temperature softmax gained for samples that are purely noise), also produce some recognition performance as was already demonstrated in the work by Robins [26].", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "generator part of the network has three fully-connected layers of 1200, 1200 and 784 neurons with ReLU activations for the first two and tanh activation for the last layers, respectively [21].", "startOffset": 187, "endOffset": 191}, {"referenceID": 6, "context": "The discriminator part of Gb has two layers of 240 maxout-by-5 neurons [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "al, closely [5].", "startOffset": 12, "endOffset": 15}, {"referenceID": 26, "context": "5 [28, 9].", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": "5 [28, 9].", "startOffset": 2, "endOffset": 9}], "year": 2017, "abstractText": "Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these tricks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique, phantom sampling.We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets through our strategy, we demonstrate that strict incremental learning could be achieved.", "creator": "LaTeX with hyperref package"}}}