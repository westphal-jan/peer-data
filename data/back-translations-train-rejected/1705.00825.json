{"id": "1705.00825", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Multi-view Unsupervised Feature Selection by Cross-diffused Matrix Alignment", "abstract": "Multi-view high-dimensional data become increasingly popular in the big data era. Feature selection is a useful technique for alleviating the curse of dimensionality in multi-view learning. In this paper, we study unsupervised feature selection for multi-view data, as class labels are usually expensive to obtain. Traditional feature selection methods are mostly designed for single-view data and cannot fully exploit the rich information from multi-view data. Existing multi-view feature selection methods are usually based on noisy cluster labels which might not preserve sufficient information from multi-view data. To better utilize multi-view information, we propose a method, CDMA-FS, to select features for each view by performing alignment on a cross diffused matrix. We formulate it as a constrained optimization problem and solve it using Quasi-Newton based method. Experiments results on four real-world datasets show that the proposed method is more effective than the state-of-the-art methods in multi-view setting.", "histories": [["v1", "Tue, 2 May 2017 07:12:59 GMT  (1739kb,D)", "http://arxiv.org/abs/1705.00825v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaokai wei", "bokai cao", "philip s yu"], "accepted": false, "id": "1705.00825"}, "pdf": {"name": "1705.00825.pdf", "metadata": {"source": "CRF", "title": "Multi-view Unsupervised Feature Selection by Cross-diffused Matrix Alignment", "authors": ["Xiaokai Wei", "Bokai Cao"], "emails": ["psyu}@uic.edu"], "sections": [{"heading": null, "text": "This year, we will be in a position to seek a solution that will enable us to achieve our objectives."}, {"heading": "II. RELATED WORK", "text": "In recent years, various methods have been proposed to jointly assess the quality of products. [6] The great advantage of these methods is that they can jointly assess the properties of individual products. [7] Among other things, the different methods have prevailed. [8] Compared to heuristic methods, they are able to improve the quality of products."}, {"heading": "III. FUSING DIFFERENT VIEWS BY CROSS DIFFUSION", "text": "We refer to n data samples with m views as {X (v) | v = 1,.., m}, X (v) = [x (v) 1, x (v) 2,..., x (v) n] and the number of characteristics in the v-th view as D (v). So the proposed CDMA-FS framework is a two-step approach (v). First, we fuse different cores into a robust similarity matrix through transverse diffusion. Second, we perform matrix adjustments for the characteristics in each view, so that the core constructed from the selected characteristics is best aligned with the fused matrix (Figure 2)."}, {"heading": "A. Cross Diffusion", "text": "Cross diffusion [18] aims to use the mutual improvement of different views inspired by co-training (P). The main idea of cross diffusion is to take a random walk using the transition probability from different angles in alternating ways. In the case of m = 2, the cross diffusion process can be defined as follows. P (1) t + 1 = P (1) \u00b7 P (2) \u00b7 P (2) \u00b7 P (2) \u00b7 P (2) \u00b7 P (2) \u00b7 P \u00b7 t \u00b7 (P (2) T (3), where P (1) t + 1 = P (2) t are the status matrices at the t-th iteration for view 1 and view 2. For the baseline values, we set P (1) 1 = P (1) and P (2) T (2) T (3), where P (1) t and P (2) t are the status matrices at the t-th iteration for view 1 and view 2."}, {"heading": "B. Extension to more than two views", "text": "Similar to the case of m = 2, P (v) t + 1 for m > 2 can be calculated as follows. P (v) t + 1 = P (v) \u00b7 1 m \u2212 1 \u2211 i 6 = v P (i) t \u00b7 (P (v)) T (8) The final state matrix is the average of the m matrices: P \u0445 = 1 m \u2211 v = 1 P (v) e (9) Since the transition probability may not be reliable for neighbours who are not closest to each other, we create a kNN diagram G. In the following section, we show how to use G to guide the selection of features for each view."}, {"heading": "IV. ALIGNING WITH CROSS-DIFFUSED MATRIX", "text": "It is the first time that the KRK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK"}, {"heading": "V. OPTIMIZATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Gradient Derivation with Relaxed Constraint", "text": "The \"0 / 1\" integer programming problem in Eq (13) is computationally intensive to optimize it. We relax the \"0 / 1\" restriction to s (v) p (p = 1,.., D (v)) to real values in the range of [0, 1] to make the optimization traceable as in [22]. We rewrite the sum limit to D (v) p = 1 s (v) p = d (v) in the form of a lagrange multiplier: min s (v) f = \u2212 Tr (HGHK (v)) + \u03bb (v) | | s (v) | 1s.t. 0 \u2264 s (v) p \u2264 1,."}, {"heading": "B. Projected Quasi-Newton Method", "text": "The question of whether it is a \"real\" or a \"false\" candidate is not new: \"It is not as if.\" \"It is not as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\" \"It is.\" \"\" It is as if. \"\" \"It is.\" \".\" \"\" \"It is...\" \".\" \"\". \"\" \".\" \"\". \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\". \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\" \"\". \".\" \"\" \".\" \".\" \"\". \"\" \"\" \".\". \"\" \".\" \".\" \"\". \".\" \"\". \"\" \".\" \".\" \".\" \".\". \"\". \"\" \"\". \".\". \"\" \"\" \".\". \".\". \".\" \"\" \"\" \"\". \".\". \"\". \".\". \".\" \".\" \"\". \".\" \".\". \".\" \"\" \"\". \".\". \".\" \"\". \".\". \"\". \".\" \"\". \".\" \"\" \".\". \".\" \".\". \"\" \".\". \"\" \"\". \".\" \"\" \".\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \".\" \".\". \".\". \".\" \"\" \"\". \".\" \".\" \".\". \".\" \"\". \"\". \".\" \"\" \".\" \".\". \".\". \".\". \".\". \".\""}, {"heading": "VI. PARAMETER SELECTION", "text": "Existing methods of multiview feature selection typically have 2-3 regularization parameters, and it is difficult to select suitable values for these parameters if class names are not available. In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, whose parameters are matched using all class names. However, such a parameter setting violates the assumption that there is no supervision. In practice, it is impossible to know the best parameter values, and this makes them less useful for real-world applications. For CDMA-FS, we provide guidelines for selecting the parameter level. Let's call the number of characteristics s (v) p = 1 as N (v) 1, which is influenced by the value of \u03bb. By pointing out that N (v) 1 is a monotonously non-increasing function of \u043c, we can keep for any view that N (v) is equal to 1 (within an area of the feature)."}, {"heading": "VII. EXPERIMENTS", "text": "In this section we compare the proposed method with modern base methods based on four data sets from the real world."}, {"heading": "A. Datasets", "text": "In our experiments, we use four publicly available real-world datasets. \u2022 Reuters Multilingual dataset 1: English and German news articles on six topics. Each language can be considered a view for the same article. \u2022 BBC Sport dataset 2: BBC news articles on five topics: athletics, cricket, football, rugby, tennis. Paragraphs in the news articles are used to construct two views. \u2022 CNN dataset 3: It consists of news articles from CNN with two views: news text and images in the news. \u2022 Blog catalog dataset 4: A subset of blog catalog posts in the categories {Cars, Software, Crafts, Football, Career & Jobs}. Two views are the text in posts and the keywords associated with the posts. The statistics of four real-world datasets are summarized in Table I."}, {"heading": "B. Baselines", "text": "We compare CDMA-FS with the use of all features and five other uncontrolled feature selection methods as follows: \u2022 All features: All original features are used for evaluation without selection. \u2022 LS: Laplacian Score [6] selects the features that maintain the local varied structure. \u2022 UDFS: Unsupervised Discriminative Feature Selection [26] is a pseudo-label-based approach with L2,1 regulation to exploit the local structure. \u2022 RSFS: Robust Spectral Feature Selection [16] selects features through a robust spectral analysis framework with low regression. \u2022 MVFS: Multi-View Feature Selection [17] is an uncontrolled feature selection for multi-view data based on pseudo-labels that are generated as consensus of spectral clusters on two views Multi-View Feature Selection [17]."}, {"heading": "C. Experiment setup", "text": "In this section, we evaluate the quality of selected characteristics based on their cluster performance. We use the popular coregularized spectral clustering [7] to cluster multi-view data 5. We set their \u03c3 as the median of paired Euclidean distances between data points and \u03bb = 0.1 as suggested in the paper. KMeans is then applied to these latent factors. We repeat the KMeans experiment 20 times (since it is an initialization) and report on the average performance. We vary the number of characteristics d in the range of {100, 200, 300, 400}. For each characteristic size d, we select suitable \u03bb in our method to evaluate the number of selected characteristics (with Score sp = 1) within the typical experimental environment for unguarded feature selection [26] [8] [8] and we use the accuracy and normalized mutual information (NMI) to evaluate the result of cluster formation as the following classes of accuracy."}, {"heading": "D. Results", "text": "Cluster accuracy and NMI on four datasets are shown in Tables II and III. It can be observed that feature selection is a useful technique for improving multi-view cluster performance. Compared to using all features, CDMA-FS with 400 features improves accuracy on BBCSport and BlogCatalog datasets by 26% and 15% respectively. Comparing with other feature selection methods, we can find that CDMA-FS performs favorably or comparably to the best performance of baseline methods whose parameters are matched using all class names. Considering that in practice it is not possible to know the best parameters for these basic methods (since we do not supervise them), their average performance better reflects the practical performance of these methods, which is far inferior to CDMA-FS."}, {"heading": "E. Parameter Sensitivity", "text": "In this section we examine how the regulation \u03b1 in the cross-diffusion process affects the quality of selected characteristics. Figure 3 shows that the performance \u03b1 is not very sensitive and CDMA-FS can do relatively well if \u03b1 > 10 \u2212 5. In contrast, the basic methods in Tables II and III tend to be more sensitive to the parameter values, since the average performance is significantly different from the best performance."}, {"heading": "VIII. CONCLUSION", "text": "High-dimensional multi-view data is a challenge for many machine learning tasks. Although feature selection methods can be useful to alleviate the curse of dimensionality, existing approaches can either not use information from multiple views at the same time or rely on cluster labels for this task. In this paper, we aim to obtain more precise information from multi-view data by learning a cross-diffuse matrix and using the information directly through matrix alignment. Experimental results show that CDMA-FS is able to select high-quality features from real data sets and significantly outperform basic methods."}], "references": [{"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "On kernel-target alignment", "author": ["N. Cristianini", "J. Shawe-Taylor", "A. Elisseeff", "J.S. Kandola"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Unsupervised feature selection with adaptive structure learning", "author": ["L. Du", "Y.-D. Shen"], "venue": "In KDD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Adaptive unsupervised multi-view feature selection for visual concept recognition", "author": ["Y. Feng", "J. Xiao", "Y. Zhuang", "X. L"], "venue": "In ACCV (1),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Co-regularized multi-view spectral clustering", "author": ["A. Kumar", "P. Rai", "H.D. III"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou", "H. Lu"], "venue": "In AAAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Algorithms for the assignment and transportation problems", "author": ["J. Munkres"], "venue": "Journal of the Society of Industrial and Applied Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1957}, {"title": "Zur Theorie der Matrices", "author": ["O. Perron"], "venue": "Mathematische Annalen,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1907}, {"title": "Robust unsupervised feature selection", "author": ["M. Qian", "C. Zhai"], "venue": "In IJCAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Unsupervised feature selection for multi-view clustering on text-image web news data", "author": ["M. Qian", "C. Zhai"], "venue": "In CIKM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm", "author": ["M. Schmidt", "E.V.D. Berg", "M.P. Friedl", "K. Murphy"], "venue": "In In AI & Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Online unsupervised multi-view feature selection", "author": ["W. Shao", "L. He", "C.-T. Lu", "X. Wei", "P.S. Yu"], "venue": "In ICDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Robust spectral learning for unsupervised feature selection", "author": ["L. Shi", "L. Du", "Y.-D. Shen"], "venue": "In ICDM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Unsupervised feature selection for multi-view data in social media", "author": ["J. Tang", "X. Hu", "H. Gao", "H. Liu"], "venue": "In SDM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Unsupervised metric fusion by cross diffusion", "author": ["B. Wang", "J. Jiang", "W. Wang", "Z.-H. Zhou", "Z. Tu"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Embedded unsupervised feature selection", "author": ["S. Wang", "J. Tang", "H. Liu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A new analysis of co-training", "author": ["W. Wang", "Z.-H. Zhou"], "venue": "In ICML, pages 1135\u20131142,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Community detection with partially observable links and node attributes", "author": ["X. Wei", "B. Cao", "W. Shao", "C.-T. Lu", "P.S. Yu"], "venue": "In IEEE International Conference on Big Data,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Nonlinear joint unsupervised feature selection", "author": ["X. Wei", "B. Cao", "P.S. Yu"], "venue": "In SDM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Unsupervised feature selection on networks: A generative view", "author": ["X. Wei", "B. Cao", "P.S. Yu"], "venue": "In AAAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Efficient partial order preserving unsupervised feature selection on networks", "author": ["X. Wei", "S. Xie", "P.S. Yu"], "venue": "In SDM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Unsupervised feature selection by preserving stochastic neighbors", "author": ["X. Wei", "P.S. Yu"], "venue": "In AISTATS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "l2, 1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "In IJCAI,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}], "referenceMentions": [{"referenceID": 6, "context": "How to effectively incorporate the abundant information from multiple views is critical for different application domains [7] [17].", "startOffset": 122, "endOffset": 125}, {"referenceID": 16, "context": "How to effectively incorporate the abundant information from multiple views is critical for different application domains [7] [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "For example, co-regularized spectral clustering [7], by enforcing consensus learning on latent factors, outperforms single-view clustering significantly.", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "State-of-the-art unsupervised multi-view feature selection approaches [17] [13] fuse information by generating intermediate cluster labels.", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "State-of-the-art unsupervised multi-view feature selection approaches [17] [13] fuse information by generating intermediate cluster labels.", "startOffset": 75, "endOffset": 79}, {"referenceID": 16, "context": "The advantages of our method compared to state-of-the-art approaches [17] [13] can be summarized as follows.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "The advantages of our method compared to state-of-the-art approaches [17] [13] can be summarized as follows.", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "Earlier unsupervised feature selection methods [6] [27] usually assign scores to each feature based on certain heuristics and neglect the correlation among features.", "startOffset": 47, "endOffset": 50}, {"referenceID": 26, "context": "Earlier unsupervised feature selection methods [6] [27] usually assign scores to each feature based on certain heuristics and neglect the correlation among features.", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.", "startOffset": 41, "endOffset": 44}, {"referenceID": 18, "context": "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "Compared to the heuristic-based methods [6] [27], the major advantage of L2,1-based approaches is that they can evaluate features jointly.", "startOffset": 40, "endOffset": 43}, {"referenceID": 26, "context": "Compared to the heuristic-based methods [6] [27], the major advantage of L2,1-based approaches is that they can evaluate features jointly.", "startOffset": 44, "endOffset": 48}, {"referenceID": 25, "context": "Unsupervised Discriminative Feature Selection (UDFS) [26] introduces pseudo-label based regression to better capture the information from the local structure.", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "Non-negative Discriminative Feature Selection (NDFS) [8] derives the cluster/pseudo labels from non-negative spectral analysis.", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "Robust Unsupervised Feature Selection (RUFS) [12] and Embedded Unsupervised Feature Selection (EUFS) [19] generate pseudo labels from non-negative matrix factorization.", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "Robust Unsupervised Feature Selection (RUFS) [12] and Embedded Unsupervised Feature Selection (EUFS) [19] generate pseudo labels from non-negative matrix factorization.", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "Robust Spectral Feature Selection (RSFS) [16] employs local kernel regression for the cluster indicators and Huber loss for the projection.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "To address this issue, Stochastic Neighbor-preserving Feature Selection (SNFS) [25] and Nonlinear Joint Feature Selection (NJFS) [22] are proposed, which can evaluate the non-linear usefulness of features.", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "To address this issue, Stochastic Neighbor-preserving Feature Selection (SNFS) [25] and Nonlinear Joint Feature Selection (NJFS) [22] are proposed, which can evaluate the non-linear usefulness of features.", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Recently, several pseudo label-based methods have been extended to multi-view setting [17] [13] [15] via cluster consensus learning.", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "Recently, several pseudo label-based methods have been extended to multi-view setting [17] [13] [15] via cluster consensus learning.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "Recently, several pseudo label-based methods have been extended to multi-view setting [17] [13] [15] via cluster consensus learning.", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "For example, adaptive Unsupervised Multi-view Feature Selection (AUMFS) [5] rely on spectral clustering on the combined similarity graphs obtained from different views.", "startOffset": 72, "endOffset": 75}, {"referenceID": 16, "context": "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.", "startOffset": 89, "endOffset": 92}, {"referenceID": 11, "context": "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "It can also be interpreted as a generalization of Parzen window estimators to functions on the local manifold [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "Cross diffusion [18] aims to exploit mutual enhancement of different views inspired by co-training [1].", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Cross diffusion [18] aims to exploit mutual enhancement of different views inspired by co-training [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 10, "context": "Under mild conditions that P and P are irreducible and aperiodic, the convergence of this process can be proved using Perron-Frobenius Theorem [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "The following theorem provides guarantee on the purity of components in the cross-diffused matrix [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "Theorem 1: If the K-nearest-neighbors is good to measure local affinity [20], P 2t+1 and P (2) 2t+1 are -good graphs.", "startOffset": 72, "endOffset": 76}, {"referenceID": 2, "context": "We achieve this by employing the matrix alignment technique [3] [21] as follows.", "startOffset": 60, "endOffset": 63}, {"referenceID": 20, "context": "We achieve this by employing the matrix alignment technique [3] [21] as follows.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "In this paper, we employ the unnormalized version of matrix alignment as in [3], which can be considered as the inner product between two vectorized matrices.", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "It is usually helpful to center the matrix for better matrix alignment performance as in observed in [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 16, "context": "Discussion Traditional sparse regression based methods [17] [13] rely on generating intermediate cluster labels and rank features by their linear regression coefficients.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Discussion Traditional sparse regression based methods [17] [13] rely on generating intermediate cluster labels and rank features by their linear regression coefficients.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": ", D) to real values in range of [0, 1] to make the optimization tractable as in [22].", "startOffset": 32, "endOffset": 38}, {"referenceID": 21, "context": ", D) to real values in range of [0, 1] to make the optimization tractable as in [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": ", L-BFGS [9]) use a positive definite approximation to the Hessian matrix\u2207f(st).", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "For example, LBFGS [9] uses the gradients in previous iterations to compute an approximate Hessian matrix.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "In our case, C is the [0, 1] box constraint on s.", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "[Proj[0,1](s )]p = min(1,max(0, s p )), \u2200p = 1, 2, .", "startOffset": 5, "endOffset": 10}, {"referenceID": 13, "context": "The optimization method [14] is two-level approach: at the outer level, L-BFGS updates are used to construct a sequence of quadratic approximations (with constraints) to the problem; at the inner level, a spectral projected gradient method optimizes the constrained subproblem approximately to generate a feasible direction.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "One might be concerned about the early termination of the spectral gradient descent subroutine, but in [14] it has been shown that the spectral gradient descent subroutine, even when terminated early, can give a descent direction, if we initialize it with st and we perform at least one spectral gradient descent iteration.", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, the parameters of which are tuned using all the class labels.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, the parameters of which are tuned using all the class labels.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, the parameters of which are tuned using all the class labels.", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "\u2022 LS: Laplacian Score [6] selects the features that preserve the local manifold structure.", "startOffset": 22, "endOffset": 25}, {"referenceID": 25, "context": "\u2022 UDFS: Unsupervised Discriminative Feature Selection [26] is a pseudo-label based approach with L2,1 regularization to exploit the local structure.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "\u2022 RSFS: Robust Spectral Feature Selection [16] selects features by robust spectral analysis framework with sparse regression.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "\u2022 MVFS: Multi-view Feature Selection [17] is unsupervised feature selection for multi-view data based on pseudo labels, which are generated as the consensus of spectral clustering on two views.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "\u2022 MVUFS: Multi-view Unsupervised Feature Selection [13] generates pseudo-labels by Non-negative Matrix Factorization and local kernel learning.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "We use the the popular coregularized spectral clustering [7] for clustering multi-view data 5.", "startOffset": 57, "endOffset": 60}, {"referenceID": 25, "context": "Following the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering.", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "Following the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering.", "startOffset": 83, "endOffset": 86}, {"referenceID": 24, "context": "Following the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "map(\u00b7) is a mapping function that maps each cluster label to a ground-truth label using Kuhn-Munkres Algorithm [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "In our experiments, we use the normalized mutual information as in previous work [8] [16].", "startOffset": 81, "endOffset": 84}, {"referenceID": 15, "context": "In our experiments, we use the normalized mutual information as in previous work [8] [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 7, "context": "We set k = 5 for the kNN neighbor size in the baseline methods and our approach following previous convention [8].", "startOffset": 110, "endOffset": 113}], "year": 2017, "abstractText": "Multi-view high-dimensional data become increasingly popular in the big data era. Feature selection is a useful technique for alleviating the curse of dimensionality in multi-view learning. In this paper, we study unsupervised feature selection for multi-view data, as class labels are usually expensive to obtain. Traditional feature selection methods are mostly designed for single-view data and cannot fully exploit the rich information from multi-view data. Existing multi-view feature selection methods are usually based on noisy cluster labels which might not preserve sufficient information from multi-view data. To better utilize multi-view information, we propose a method, CDMAFS, to select features for each view by performing alignment on a cross diffused matrix. We formulate it as a constrained optimization problem and solve it using Quasi-Newton based method. Experiments results on four real-world datasets show that the proposed method is more effective than the state-of-theart methods in multi-view setting.", "creator": "LaTeX with hyperref package"}}}