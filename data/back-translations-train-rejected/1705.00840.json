{"id": "1705.00840", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Pointed subspace approach to incomplete data", "abstract": "Incomplete data are often represented as vectors with filled missing attributes joined with flag vectors indicating missing components. In this paper we generalize this approach and represent incomplete data as pointed affine subspaces. This allows to perform various affine transformations of data, as whitening or dimensionality reduction. We embed such generalized missing data into a vector space by mapping pointed affine subspace (generalized missing data point) to a vector containing imputed values joined with a corresponding projection matrix. Such an operation preserves the scalar product of the embedding defined for flag vectors and allows to input transformed incomplete data to typical classification methods.", "histories": [["v1", "Tue, 2 May 2017 07:59:01 GMT  (250kb,D)", "http://arxiv.org/abs/1705.00840v1", "13 pages, 3 figures and 3 tables. arXiv admin note: text overlap witharXiv:1612.01480"]], "COMMENTS": "13 pages, 3 figures and 3 tables. arXiv admin note: text overlap witharXiv:1612.01480", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["{\\l}ukasz struski", "marek \\'smieja", "jacek tabor"], "accepted": false, "id": "1705.00840"}, "pdf": {"name": "1705.00840.pdf", "metadata": {"source": "CRF", "title": "Pointed subspace approach to incomplete data", "authors": ["Lukasz Struski", "Marek \u015amieja", "Jacek Tabor", "Struski \u015amieja Tabor"], "emails": ["lukasz.struski@uj.edu.pl", "marek.smieja@uj.edu.pl", "jacek.tabor@uj.edu.pl"], "sections": [{"heading": null, "text": "Keywords: incomplete data, SVM, linear transformations"}, {"heading": "1. Introduction", "text": "Incomplete data analysis is an important part of data processing and machine learning, as it appears in many practical problems."}, {"heading": "2. Related works", "text": "The most common approach to learning from incomplete data is known as deterministic imputation (McKnight et al., 2007). In this two-step process, the missing features are first filled in, and only then a standard classifier is applied to the complete data (Little and Rubin, 2014). Although the imputation-based techniques are easy for practitioners to use, they result in the loss of information whose characteristics are missing and do not take into account the reasons for the incompleteness. To preserve the information of missing attributes, an additional vector of binary flags can be used, which was discussed in the introduction. The second popular group of methods aims at building a probabilistic model of incomplete data that maximizes probability by applying the EM algorithm (Ghahramani and Jordan, 1994; Schafer, 1997). This makes it possible to generate the most likely values from the obtained probability distribution for missing attributes (McKnight et al)."}, {"heading": "3. Generalized incomplete data", "text": "In this section, we present the subaraceous approach to incomplete data. First, we define a generalized missing data point that enables the affine transformation of incomplete data. Then, we show how to embed generalized missing data in a vector space and select a baseline. Finally, we define a scalar product on the embedding space."}, {"heading": "3.1. Incomplete data as pointed affine subspaces", "text": "Incomplete data X can be understood as a sequence of pairs (xi, Ji) in which xi-RN and Ji-RN (1,.., N) indicate missing coordinates of xi. Therefore, we can associate a missing data point (x, J) with an affine subspace x + span (ej) J, where (ej) j is the canonical base of RN. Let us observe that x + span (ej) j is a set of all N-dimensional vectors that coincide with x on the coordinates of J. In this paper, we focus on the transformation of incomplete data by affine mappings. To this end, we generalize the above representation to arbitrary affine subspaces, or more precisely to affine subspaces that are not generated by canonical bases."}, {"heading": "3.2. Embedding of generalized missing data", "text": "The above representation is useful for understanding and performing affinity transformations of incomplete data, such as whitening, dimensionality reduction, or incorporating affinity constraints into the data. However, typical machine learning methods require vectors or some kind of kernel (or similarity) matrix as an input method. We show how to embed generalized missing data into a vector space. To get an exact form of pV, let us assume that (vj) j is an element of vector space and a linear subspace V. To represent a subspace V, we project a matrix of orthogonal projections onto V. To get an exact form of pV, we assume that the orthonormal base is V. Then the projection of y-RN can be calculated."}, {"heading": "3.3. Scalar product for SVM kernel", "text": "In order to apply most of the classification methods, it is necessary to consider the above-mentioned parameters. < M > J > S > S > S > S (1) S (2) S (2) S (2) S (2) S (2) S (2) S + S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2) S (2 S (2) S (2) S (2 S) S (2 S (2) S (2) S (2) S (2 S (2) S (2 S) S (2 S (2) S (2) S (2 S (2) S (2 S (2) S (2 S (2) S (2 S (2) S (2) S (2) S (2 S (2) S (2 S (2) S (2 S) S (2) S (2 S (2) S (2) S (2 S (2) S (2 S (2 S) S (2) S (2 S) S (2 S (2) S (2) S (2) S (2 S (2) S) S (2) S (2 S (2) S (2 S (2) S (2 S) S (2 S (2) S (2) S (2) S (2) S (2 S (2) S (2 S (2) S (2"}, {"heading": "4. Experiments", "text": "To illustrate our approach, we applied it to SVM classification experiments, which required the use of whitening operations before performing a classification phase. We used examples retrieved from the UCI repository, combined with two attribute removal strategies: random and structural. Finally, a real medical dataset was used that simulates a real process of missing characteristics. In all cases, the following procedure was used: First, we replaced missing characteristics with the use of one of four strategies mentioned in the paper: 1. Average: average value of the feature over the training set.2. Median: median of the feature over the training set.3. Zero imputation: missing characteristics were filled in with zeros. 4. Probable imputation: it was estimated in Section 3.2.For simplification, the mean and coance matrix values from a training set were estimated."}, {"heading": "4.1. UCI datasets", "text": "We used three UCI datasets (for datasets with more than two classes, we selected two of the most numerous classes): breast cancer (BC), ionosphere (IS) and yeast (Y) (Asuncion and Newman, 2007). In the first case, we randomly removed 90% of the attributes. In the second option, we defined a structural process for removing attributes. Specifically, we drew N-points x1,..., xN of a dataset X-RN. Then, for each x-X, we removed its i-th attribute with a probability sexp (\u2212 t-t-x-xi-\u03a3) in which N-X selected the Mahalanobis standard x in terms of performance and t > 0 in order to remove about 90% of attributes. The results presented in Table 1 show that there is no benefit in identifying missing attributes if the attributes are completely random."}, {"heading": "4.2. Medical data", "text": "In this application, we looked at a real angiological dataset from the Jagiellonian Center of Experimental Therapeutic, which included patient studies, http: / / jcet.eu / new _ en /. The goal was to find patients with atherosclerosis. Innovative medical tests are very expensive, time-consuming and in some cases cannot be successfully completed due to the condition of the patient. As a result, the research database contains many empty cells, which is the effect of a purely structural process. Since some parameters are displayed discreetly and real values at different scales, brightening the data is a natural pre-processing step. The results presented in Table 3 partially confirm the hypothesis that was assumed in previous experiments. In fact, the use of the proposed sub-room embedding has brought greater accuracy to alliance strategies, but the benefit from its application was not significant. It is difficult to decide which implantation strategy was optimal because all comparable results were obtained."}, {"heading": "5. Conclusion", "text": "In order to enable appropriate affine transformations of data, we presented incomplete data as apex affine subspaces and embedded them in a vector space by linking an apex subspace to a baseline connected to a corresponding projection matrix. In the same sense, we proposed selecting a baseline as the most likely point from a subspace that extends the known zero imputation strategy, which in most cases yielded the best performance in conducted classification experiments."}], "references": [{"title": "Compliance with cardiovascular disease prevention strategies: a review of the research", "author": ["Lora E Burke", "Jacqueline M Dunbar-Jacob", "Martha N Hill"], "venue": "Annals of Behavioral Medicine,", "citeRegEx": "Burke et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Burke et al\\.", "year": 1997}, {"title": "Libsvm: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Max-margin classification of data with absent features", "author": ["Gal Chechik", "Geremy Heitz", "Gal Elidan", "Pieter Abbeel", "Daphne Koller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chechik et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2008}, {"title": "Learning to classify with missing and corrupted features", "author": ["Ofer Dekel", "Ohad Shamir", "Lin Xiao"], "venue": "Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Learning from incomplete data with infinite imputations", "author": ["Uwe Dick", "Peter Haider", "Tobias Scheffer"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Dick et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dick et al\\.", "year": 2008}, {"title": "Supervised learning from incomplete data via an EM approach", "author": ["Zoubin Ghahramani", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ghahramani and Jordan.,? \\Q1994\\E", "shortCiteRegEx": "Ghahramani and Jordan.", "year": 1994}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Feature set embedding for incomplete data", "author": ["David Grangier", "Iain Melvin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grangier and Melvin.,? \\Q2010\\E", "shortCiteRegEx": "Grangier and Melvin.", "year": 2010}, {"title": "Quadratically gated mixture of experts for incomplete data classification", "author": ["Xuejun Liao", "Hui Li", "Lawrence Carin"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Liao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2007}, {"title": "Statistical analysis with missing data", "author": ["Roderick J.A. Little", "Donald B Rubin"], "venue": null, "citeRegEx": "Little and Rubin.,? \\Q2014\\E", "shortCiteRegEx": "Little and Rubin.", "year": 2014}, {"title": "Missing data: A gentle introduction", "author": ["Patrick E McKnight", "Katherine M McKnight", "Souraya Sidani", "Aurelio Jose Figueredo"], "venue": null, "citeRegEx": "McKnight et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McKnight et al\\.", "year": 2007}, {"title": "Analysis of incomplete multivariate data", "author": ["Joseph L Schafer"], "venue": "CRC Press,", "citeRegEx": "Schafer.,? \\Q1997\\E", "shortCiteRegEx": "Schafer.", "year": 1997}, {"title": "Second order cone programming approaches for handling missing and uncertain data", "author": ["Pannagadatta K Shivaswamy", "Chiranjib Bhattacharyya", "Alexander J Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shivaswamy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shivaswamy et al\\.", "year": 2006}, {"title": "Kernel methods for missing variables", "author": ["Alexander J Smola", "SVN Vishwanathan", "Thomas Hofmann"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics. Citeseer,", "citeRegEx": "Smola et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2005}, {"title": "Virtual screening methods that complement HTS", "author": ["Florence L Stahura", "Jurgen Bajorath"], "venue": "Combinatorial Chemistry & High Throughput Screening,", "citeRegEx": "Stahura and Bajorath.,? \\Q2004\\E", "shortCiteRegEx": "Stahura and Bajorath.", "year": 2004}, {"title": "Analytical kernel matrix completion with incomplete multi-view data", "author": ["David Williams", "Lawrence Carin"], "venue": "In Proceedings of the ICML Workshop on Learning With Multiple Views,", "citeRegEx": "Williams and Carin.,? \\Q2005\\E", "shortCiteRegEx": "Williams and Carin.", "year": 2005}, {"title": "Incomplete-data classification using logistic regression", "author": ["David Williams", "Xuejun Liao", "Ya Xue", "Lawrence Carin"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Williams et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "In medical diagnosis, a doctor may be unable to complete the patient examination due to the deterioration of health status or lack of patient\u2019s compliance (Burke et al., 1997); in object detection, the system has to recognize the shape from low resolution or corrupted images (Berg et al.", "startOffset": 155, "endOffset": 175}, {"referenceID": 14, "context": ", 2005); in chemistry, the complete analysis of compounds requires high financial costs (Stahura and Bajorath, 2004).", "startOffset": 88, "endOffset": 116}, {"referenceID": 10, "context": "Related works The most common approach to learning from incomplete data is known as deterministic imputation (McKnight et al., 2007).", "startOffset": 109, "endOffset": 132}, {"referenceID": 9, "context": "In this two-step procedure, the missing features are filled first, and only then a standard classifier is applied to the complete data (Little and Rubin, 2014).", "startOffset": 135, "endOffset": 159}, {"referenceID": 5, "context": "The second popular group of methods aims at building a probabilistic model of incomplete data which maximizes the likelihood by applying the EM algorithm (Ghahramani and Jordan, 1994; Schafer, 1997).", "startOffset": 154, "endOffset": 198}, {"referenceID": 11, "context": "The second popular group of methods aims at building a probabilistic model of incomplete data which maximizes the likelihood by applying the EM algorithm (Ghahramani and Jordan, 1994; Schafer, 1997).", "startOffset": 154, "endOffset": 198}, {"referenceID": 10, "context": "This allows to generate the most probable values from obtained probability distribution for missing attributes (random imputation) (McKnight et al., 2007) or to learn a decision function directly based on the distributional model.", "startOffset": 131, "endOffset": 154}, {"referenceID": 16, "context": "The second option was already investigated in the case of linear regression (Williams et al., 2005), kernel methods (Smola et al.", "startOffset": 76, "endOffset": 99}, {"referenceID": 13, "context": ", 2005), kernel methods (Smola et al., 2005; Williams and Carin, 2005) or by using second order cone programming (Shivaswamy et al.", "startOffset": 24, "endOffset": 70}, {"referenceID": 15, "context": ", 2005), kernel methods (Smola et al., 2005; Williams and Carin, 2005) or by using second order cone programming (Shivaswamy et al.", "startOffset": 24, "endOffset": 70}, {"referenceID": 12, "context": ", 2005; Williams and Carin, 2005) or by using second order cone programming (Shivaswamy et al., 2006).", "startOffset": 76, "endOffset": 101}, {"referenceID": 4, "context": "One can also estimate the parameters of the probability model and the classifier jointly, which was considered in (Dick et al., 2008; Liao et al., 2007).", "startOffset": 114, "endOffset": 152}, {"referenceID": 8, "context": "One can also estimate the parameters of the probability model and the classifier jointly, which was considered in (Dick et al., 2008; Liao et al., 2007).", "startOffset": 114, "endOffset": 152}, {"referenceID": 2, "context": "In (Chechik et al., 2008) a modified SVM classifier is trained by scaling the margin according to observed features only.", "startOffset": 3, "endOffset": 25}, {"referenceID": 3, "context": "The alternative approaches to learning a linear classifier, which avoid features deletion or imputation, are presented in (Dekel et al., 2010; Globerson and Roweis, 2006).", "startOffset": 122, "endOffset": 170}, {"referenceID": 6, "context": "The alternative approaches to learning a linear classifier, which avoid features deletion or imputation, are presented in (Dekel et al., 2010; Globerson and Roweis, 2006).", "startOffset": 122, "endOffset": 170}, {"referenceID": 7, "context": "Finally, in (Grangier and Melvin, 2010) the embedding mapping of feature-value pairs is constructed together with a classification objective function.", "startOffset": 12, "endOffset": 39}, {"referenceID": 11, "context": "Alternatively, if data satisfy missing at random assumption, then the EM algorithm can be applied to estimate the probability model describing data (Schafer, 1997).", "startOffset": 148, "endOffset": 163}, {"referenceID": 1, "context": "Finally, we calculated the scalar products (kernel matrices) for such representations of data and trained SVM classifier implemented in libsvm (Chang and Lin, 2011).", "startOffset": 143, "endOffset": 164}], "year": 2017, "abstractText": "Incomplete data are often represented as vectors with filled missing attributes joined with flag vectors indicating missing components. In this paper we generalize this approach and represent incomplete data as pointed affine subspaces. This allows to perform various affine transformations of data, as whitening or dimensionality reduction. We embed such generalized missing data into a vector space by mapping pointed affine subspace (generalized missing data point) to a vector containing imputed values joined with a corresponding projection matrix. Such an operation preserves the scalar product of the embedding defined for flag vectors and allows to input transformed incomplete data to typical classification methods.", "creator": "LaTeX with hyperref package"}}}