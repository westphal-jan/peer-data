{"id": "1705.00930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner", "abstract": "Impressive image captioning results are achieved in domains with plenty of training image and sentence pairs (e.g., MSCOCO). However, transferring to a target domain with significant domain shifts but no paired training data (referred to as cross-domain image captioning) remains largely unexplored. We propose a novel adversarial training procedure to leverage unpaired data in the target domain. Two critic networks are introduced to guide the captioner, namely domain critic and multi-modal critic. The domain critic assesses whether the generated sentences are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated sentence are a valid pair. During training, the critics and captioner act as adversaries -- captioner aims to generate indistinguishable sentences, whereas critics aim at distinguishing them. The assessment improves the captioner through policy gradient updates. During inference, we further propose a novel critic-based planning method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critics during inference further gives another 4.5% boost.", "histories": [["v1", "Tue, 2 May 2017 12:06:54 GMT  (1542kb,D)", "http://arxiv.org/abs/1705.00930v1", "10 pages, 6 figures"], ["v2", "Mon, 14 Aug 2017 15:54:32 GMT  (2599kb,D)", "http://arxiv.org/abs/1705.00930v2", "ICCV 2017"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["tseng-hung chen", "yuan-hong liao", "ching-yao chuang", "wan-ting hsu", "jianlong fu", "min sun"], "accepted": false, "id": "1705.00930"}, "pdf": {"name": "1705.00930.pdf", "metadata": {"source": "CRF", "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner", "authors": ["Tseng-Hung Chen", "Yuan-Hong Liao", "Ching-Yao Chuang", "Wan-Ting Hsu", "Jianlong Fu", "Min Sun"], "emails": ["{tsenghung@gapp,", "andrewliao11@gapp,", "cychuang@gapp,", "hsuwanting@gapp,", "sunmin@ee}.nthu.edu.tw", "jianf@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "This year, it will be in a position to achieve the objectives mentioned, but not yet in a position to achieve them."}, {"heading": "2. Related Work", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "3. Cross-domain Image Captioning", "text": "In the source domain, we get a set P = {(xn, y, n) n with a paired image xn1 and a paired set y (n) n with a paired image xn1 and a paired set y (n). In the target domain, we get two separate sets of information: a set of sample images y = {xn} n and a set of sample sets Y = {y} n. Note: Collecting paired data P in the source domain is typically more expensive than X and Y (in the target domain x). Captioning is the target y (in the default domain x)."}, {"heading": "3.1. Captioner as an Agent", "text": "In time t, the captioner assumes an action (i.e., a word yt = = a huge sentence after a stochastic sentence (yt | x, yt \u2212 1), where x \u2212 \u2212 is the observed image, yt \u2212 1 = [y1,..., yt \u2212 1] 2 is the generated subsentence, and \u03b8 is the parameter of politics. We use an existing CNN RNN model [32] as the model of politics. By sequentially generating each word yt from the political sentence (.) until the special end-of-sentence (EOS) token, a complete sentence y. In standard image capture, the following total loss per word J (n = N = 1 Tn = 1 Tn = 1 Loss (y-Of-sentence) n (EOS) token, a complete sentence y (EOS) token, a complete sentence y."}, {"heading": "3.2. Critics", "text": "The critics follow these two rules to assign a reward to each generated sentence. We introduce the Domain Critics and Multimodal Critics. The DC model consists of an encoder and a classifier. To address the domain shift in the sentence space, we first train a Domain Critique (DC) to classify sentences as \"source,\" \"target,\" or \"generated\" sentences. The DC model consists of an encoder and a classifier. A sentence y is first coded by CNN [18] with a sentence representation, then we perform the representation through a fully connected layer and a softmax layer to generate probabilities."}, {"heading": "3.3. Adversarial Training", "text": "Our cross-domain captioning system is summarized in Fig. 3. Both Captioner and Critics Cd and Cm learn together by pursuing competing objectives as described below. Given x, the Captioner generates a sentence y. It would be better if the sentence had a high reward, implying large values of Cd (target | y) and Cm (pair | x.y). In contrast, critics would prefer large values of Cd (generated | y) and Cm (generated | x, y), implying small values of Cd (target | y) and Cm (paired | x.y). We propose a novel, adversarial training method to iteratively update the Captioner and Critic in Algorithm 1. In short, we train the Captioner first using source domain length data based on cross entropy losses. We then update the Captioner and Critics iteratively with a ratio of Ng < where the Critics are updated more often;"}, {"heading": "3.4. Critic-based Planning", "text": "The quality of a generated word yt is typically measured by the political network \u03c0 (yt | \u00b7). In cross-domain subtitling, the learned critics can also be used to measure the quality of yt by calculating Q ((((x, yt \u2212 1), yt) using Equation 6. In this case, Q is an expected value that models the randomness of future words, so we call our method \"Critical Planning.\" Critical Planning uses both the learned political network and the critics. By default, we select y-t = arg maxy-p (y | \u00b7) as the generated word. However, if the difference between the maximum probability and the second-highest probability of \u03c0\u043a (\u00b7) is below a threshold (where the selection of y-t is ambiguous), we take the uppermost J-words {yjt} YY = 1 according to the word generated."}, {"heading": "4. Experiments", "text": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22] and Flickr30k [36] as the target datasets. We show that our method generalizes to large domain shift datasets (CUB-200 and Oxford-102) and regular domain shift datasets (Flickr30k and TGIF). We also show that critical planning can further improve the performance of large domain shift datasets. Finally, we conduct a deposit study on Flickr30k to show the contribution of various components."}, {"heading": "4.1. Implementation details", "text": "Data pre-processing. For the source domain data set, we select the MSCOCO training part [17], which contains 113, 287 images and 5 captions each. We trim vocabulary by dropping words with a frequency of less than 5, resulting in 10,066 words, including special Begin-OfSentence (BOS) and End-Of-Sentence (EOS) tokens. We use the same vocabulary in all experiments. For target domain data sets, we remove training sets that contain words from vocabulary that are not included in the vocabulary. (See supplemental material for detailed statistics.) Pre-training details. The architecture of our captioner is a CNN-LSTM with a hidden dimension of 512. Image functions are extracted using the pre-trained Resnet 101 [12] sentences and the sentences are presented as uniform encoding."}, {"heading": "4.2. Experimental Results", "text": "Next, we update the captioners using opposing training methods with unpaired data from training in target domains. Finally, we evaluate our method on four target domain datasets representing different levels of domain shift. The first is a CNN model that predicts semantic attributes and the latter is an LSTM model trained on unpaired text. In the end, the general DCC model combines both models with a linear layer trained on paired image capture data. For a fair comparison, we apply the following settings to DCC, where lexical classification is a ResNet 101 model and the language model is trained on target domain judgments."}, {"heading": "4.3. Ablation Study", "text": "In order to analyze the effectiveness of these two critics, we perform ablation comparisons with both models. Table 3 shows that the use of MC is insufficient, since MC does not know the sentence style in the target area. On the other hand, the use of DC only contributes significantly to this. Finally, the combination of MC and DC achieves the best performance for all assessment metrics. We argue that both MC and DC are indispensable for cross-domain image labeling."}, {"heading": "5. Conclusion", "text": "We propose a novel, hostile captioner training method for cross-domain captions. Of course, to further improve the process of captioning in the test, a novel critique-based planning method will be introduced. Our method consistently outperforms the basic methods in four challenging target domain data sets (two with large domain shift and two with regular domain shift). In the future, we want to improve the flexibility of our method by combining multiple critics in plug-and-play fashion."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand"], "venue": "NIPS workshop on Transfer and Multi-Task Learning: Theory meets Practice,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Guided open vocabulary image captioning with constrained beam search", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "CoRR, abs/1612.00576,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["D. Bahdanau", "P. Brakel", "K. Xu", "A. Goyal", "R. Lowe", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "ICLR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-adversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "JMLR, 17(59):1\u201335,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, pages 2672\u20132680,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["A. Goyal", "A. Lamb", "Y. Zhang", "S. Zhang", "A.C. Courville", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["L.A. Hendricks", "S. Venugopalan", "M. Rohrbach", "R. Mooney", "S. Kate", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Fcns in the wild: Pixel-level adversarial and constraint-based adaptation", "author": ["J. Hoffman", "D. Wang", "F. Yu", "T. Darrell"], "venue": "CoRR, abs/1612.02649,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Imageto-image translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "AAAI,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "TACL,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "A new dataset and benchmark on animated gif description", "author": ["Y. Li", "Y. Song", "L. Cao", "J. Tetreault", "L. Goldberg", "A. Jaimes", "J. Luo"], "venue": "CVPR.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimization of image description metrics using policy gradient methods", "author": ["S. Liu", "Z. Zhu", "N. Ye", "S. Guadarrama", "K. Murphy"], "venue": "CoRR, abs/1612.00370,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Automated flower classification over a large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "ICVGIP,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "ICLR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep representations of fine-grained visual descriptions", "author": ["S. Reed", "Z. Akata", "H. Lee", "B. Schiele"], "venue": "CVPR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Self-critical sequence training for image captioning", "author": ["S.J. Rennie", "E. Marcheret", "Y. Mroueh", "J. Ross", "V. Goel"], "venue": "CoRR, abs/1612.00563,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Captioning images with diverse objects", "author": ["S. Venugopalan", "L.A. Hendricks", "M. Rohrbach", "R.J. Mooney", "T. Darrell", "K. Saenko"], "venue": "CoRR, abs/1606.07770,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "Technical report,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "ICCV,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "AAAI,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2017}, {"title": "Title generation for user generated videos", "author": ["K.-H. Zeng", "T.-H. Chen", "J.C. Niebles", "M. Sun"], "venue": "ECCV,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 19, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 15, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 29, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 21, "context": "For instance, MSCOCO [23] mostly consists of images of large scene with more object instances, whereas CUB-200-2011 [33] (shortened as CUB-200 in the following) consists of cropped birds images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "For instance, MSCOCO [23] mostly consists of images of large scene with more object instances, whereas CUB-200-2011 [33] (shortened as CUB-200 in the following) consists of cropped birds images.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "[14, 30] propose to leverage an image dataset with category labels (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[14, 30] propose to leverage an image dataset with category labels (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": ", ImageNet [7]) and sentences on the web (e.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] propose to leverage image taggers at test time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Inspired by Generative Adversarial Networks (GANs) [10], we propose a novel adversarial training procedure to leverage unpaired images and sentences.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": ", tags [2]) in testing.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 65, "endOffset": 73}, {"referenceID": 25, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 86, "endOffset": 94}, {"referenceID": 25, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 86, "endOffset": 94}, {"referenceID": 33, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 120, "endOffset": 124}, {"referenceID": 21, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 113, "endOffset": 117}, {"referenceID": 33, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 19, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 15, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 29, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 31, "context": "[34] introduce an attention model that can automatically learn where to look depending on the generated words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 28, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 32, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 35, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 35, "context": "[38] propose a framework to jointly localize highlights in videos and generate their titles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Recently, the issue of exposure bias [26] has been well-addressed in sequence prediction tasks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "[5] propose a curriculum learning strategy to gradually ignore the guidance from supervision during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] introduce an adversarial training method as regularization between sampling mode and teacher-forced mode.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 3, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 22, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 26, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 11, "context": "Several works [13, 37] incorporate auxiliary models as rewards.", "startOffset": 14, "endOffset": 22}, {"referenceID": 34, "context": "Several works [13, 37] incorporate auxiliary models as rewards.", "startOffset": 14, "endOffset": 22}, {"referenceID": 11, "context": "[13] minimize a discriminative loss to ensure generated sentences be class specific.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[37] also introduce a critic to learn a reward function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] introduce a domain adaptation regularizer to learn the representation for sentiment analysis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] propose a gradient reversal layer for aligning the distribution of fea-", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[15] propose an unsupervised domain adversarial method for semantic segmentations in street scenes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "We utilize an existing CNN-RNN model [32] as the model of the policy.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "A sentence y is first encoded by CNN [18] with highway connection [19] into a sentence representation.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "A sentence y is first encoded by CNN [18] with highway connection [19] into a sentence representation.", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "8, the encoded image x and sentence c representations are fused via element-wise multiplication similar to [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 21, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "For source domain dataset, we select the MSCOCO training split from [17] which contains 113, 287 images, along with 5 captions each.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "The image features are extracted using the pre-trained Resnet-101 [12] and the sentences are represented as one-hot encoding.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "We first pre-train the captioner on source domain dataset via cross entropy objective using ADAM optimizer [20] with learning rate 5 \u00d7 10\u22124.", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "To further improve the performance, we use schedule sampling [5] to mitigate the exposure bias.", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "We train the captioner and critics using ADAM optimizer [20] with learning rate of 5 \u00d7 10\u22125.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "We apply dropout in training phase to prevent over-fitting, which also served as input noise similar to [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "We re-implement Deep Compositional Captioner (referred to as DCC) [14] as our baseline method.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "According to [22], there are more motion verbs (30% in TGIF vs.", "startOffset": 13, "endOffset": 17}], "year": 2017, "abstractText": "Impressive image captioning results are achieved in domains with plenty of training image and sentence pairs (e.g., MSCOCO). However, transferring to a target domain with significant domain shifts but no paired training data (referred to as cross-domain image captioning) remains largely unexplored. We propose a novel adversarial training procedure to leverage unpaired data in the target domain. Two critic networks are introduced to guide the captioner, namely domain critic and multi-modal critic. The domain critic assesses whether the generated sentences are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated sentence are a valid pair. During training, the critics and captioner act as adversaries \u2013 captioner aims to generate indistinguishable sentences, whereas critics aim at distinguishing them. The assessment improves the captioner through policy gradient updates. During inference, we further propose a novel critic-based planning method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critics during inference further gives another 4.5% boost.", "creator": "LaTeX with hyperref package"}}}