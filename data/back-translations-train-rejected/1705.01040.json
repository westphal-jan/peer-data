{"id": "1705.01040", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Maximum Resilience of Artificial Neural Networks", "abstract": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications poses a number of new verification and certification challenges. In particular, for ANN-enabled self-driving vehicles it is important to establish properties about the resilience of ANNs to noisy or even maliciously manipulated sensory input. We are addressing these challenges by defining resilience properties of ANN-based classifiers as the maximal amount of input or sensor perturbation which is still tolerated. This problem of computing maximal perturbation bounds for ANNs is then reduced to solving mixed integer optimization problems (MIP). A number of MIP encoding heuristics are developed for drastically reducing MIP-solver runtimes, and using parallelization of MIP-solvers results in an almost linear speed-up in the number (up to a certain limit) of computing cores in our experiments. We demonstrate the effectiveness and scalability of our approach by means of computing maximal resilience bounds for a number of ANN benchmark sets ranging from typical image recognition scenarios to the autonomous maneuvering of robots.", "histories": [["v1", "Fri, 28 Apr 2017 12:04:02 GMT  (466kb,D)", "https://arxiv.org/abs/1705.01040v1", "Timestamping research work conducted in the project"], ["v2", "Wed, 5 Jul 2017 11:27:46 GMT  (525kb,D)", "http://arxiv.org/abs/1705.01040v2", "Timestamp research work conducted in the project. version 2: fix some typos, rephrase the definition, and add some more existing work"]], "COMMENTS": "Timestamping research work conducted in the project", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.LO cs.SE", "authors": ["chih-hong cheng", "georg n\\\"uhrenberg", "harald ruess"], "accepted": false, "id": "1705.01040"}, "pdf": {"name": "1705.01040.pdf", "metadata": {"source": "CRF", "title": "Maximum Resilience of Artificial Neural Networks", "authors": ["Chih-Hong Cheng", "Georg N\u00fchrenberg", "Harald Ruess"], "emails": ["cheng@fortiss.org", "nuehrenberg@fortiss.org", "ruess@fortiss.org"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Preliminaries", "text": "We present some basic concepts of forward-looking artificial neural networks (ANN) [1], which consist of a sequence of layers labeled by l = 0, 1,., L, where 0 is the index of the input layer, L is the output layer, and all other layers are so-called hidden layers. For the purpose of this paper, we assume that any input is of limited domain. Superscripts (l) are used to index l-specific variables, but these superscripts can be omitted for input layers. Layers l consist of nodes n (l) i (so-called neurons), for i = 0, 1,., d (l), where d (l) is the dimension of the layer l. By convention nodes of index 0, we have a constant output 1; these bias nodes are commonly used to encode neuron activation thresholds."}, {"heading": "3 Arithmetic Encoding of Artificial Neural Networks", "text": "In a first step, we encode the behavior of ANNs with respect to linear arithmetic constraints (i = 1). In addition to [13], we consider tan \u2212 1, max-pooling, and softmax nodes, as they are often found in practice in many ANNs. These encodings are based on the input-output behavior of each node in the network, and the challenge is to handle the non-linearity resulting from nonlinear activation functions (e.g. ReLU and tan \u2212 1), max-pooling, and softmax nodes.Constraints for ReLU and tan \u2212 1 nodes, as defined in Figure 2, divided into an equality constraint (1) for the intermediate value in (l) i, and secondly, several linear constraints for encoding the nonlinear behavior of these nodes. im (l) i = 1, d \u2212 w (l)."}, {"heading": "4 Perturbation Bounds", "text": "We define concrete parameters for quantifying the load capacity of multi-classification neurons. Let us leave the initial situation (classified) as neurons (classified). This load capacity measurement is defined by all possible inputs of the network. In particular, our developments are not dependent on probability distributions of training and test data, as in previous work [3]. The maximum load capacity of these ANNs is achieved by solving corresponding MIP problems (see Fig. 5). Input images in MNIST are of the dimension 24 \u00d7 1 activation functions and are represented as vector a1. a576. Input layers of ANN-based multidigit classifiers for MNIST consist of 576 input neurons, and the output layer consists of 10 softmax neurons."}, {"heading": "5 Heuristic Problem Encodings", "text": "We list some simple but essential heuristics for efficiently solving MIP problems to verify ANNs. Note that these heuristics are not limited to calculating the resilience of ANNs, and may well be used for other verification tasks with ANNs.1. Smaller Big Ms by looking back at multiple levels. Data flow analysis in Section 3 essentially regards neurons at the same level as independent. Here, we propose a finer-grained analysis by considering a fixed number of previous layers at once. Finding the limit for the output of a neuron x (l) i, for example, can be understood as solving a much smaller MIP problem by considering neurons from layers l \u2212 1 and l \u2212 2. These MIP problems are independent for each node in these layers and can therefore be solved in parallel. For each node, we first set the upper limit as a variable that can be maximized."}, {"heading": "6 Implementation and Evaluation", "text": "It is about the question to what extent people are able to survive themselves and the question to what extent they feel able to survive themselves. (...) It is about the question to what extent people are able to survive themselves. (...) It is about the question to what extent people are able to survive themselves. (...) It is about the question to what extent they are able to survive themselves. (...) It is about the question to what extent people are able to survive themselves. (...) It is about the question to what extent they are able to survive themselves. (...) It is about the question to what extent they are able to survive themselves. (...) It is about the question to what extent they are able to survive themselves. (...) It is about the question to what extent they are able to survive themselves."}, {"heading": "7 Concluding Remarks", "text": "Our definition and calculation of maximum interference limits for ANNs using MIP-based optimization is novel. By developing specialized encoding heuristics and using parallelization, we demonstrate the scalability and possible applicability of our verification approach for neural networks in the real world. In addition, our verification techniques allow a formal and quantitative comparison of the resilience of different neural networks. Furthermore, interference limits provide a formal interface for decoupling the design of sensor sets from the design of the neural network itself. In our case, the network assumes a maximum sensor entry error for resilience, and the input sensor sets must be designed to guarantee the given error boundary. This type of contractual interfaces can form the basis for building modular safety cases for autonomous systems. Nevertheless, we consider the developments in this paper as only a tiny step toward realizing the full potential of artificial networks for their deployment as retractable safety systems."}, {"heading": "24. A. Ukil, V. H. Shah, and B. Deck. Fast computation of arctangent functions for", "text": "In ISIE = = pages 1206-1211. IEEE, 2011. 25. Y. Xu, T. K. Ralphs, L. Lada \u0301 nyi, and M. J. Saltzman. Calculation experience with a software framework for parallel integer programming. INFORMS Journal on Computing, 21 (3): 383-397, 2009.AppendixProposition 1 x (l) i = up (0, im (l) i) iff constraints (2a) to (4b) hold.First we establish a dilemma to support the project. Lemma 1 b (l) i = 1 up (l) i up 0.Proof. (\u21d2) Assume b (l) i = 1, then (3a) applies trivial and (3b) implicit and (l) i (l) i up 0. (l) Assume in (l) i up 0, then (3b) we hold."}], "references": [{"title": "Learning from data, volume 4", "author": ["Y.S. Abu-Mostafa", "M. Magdon-Ismail", "H.-T. Lin"], "venue": "AMLBook New York, NY, USA:,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Concrete problems in ai safety", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring neural net robustness with constraints", "author": ["O. Bastani", "Y. Ioannou", "L. Lampropoulos", "D. Vytiniotis", "A. Nori", "A. Criminisi"], "venue": "CoRR, abs/1605.07262,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "\u03bdZ-An Optimizing SMT Solver", "author": ["N. Bj\u00f8rner", "A.-D. Phan", "L. Fleckenstein"], "venue": "In TACAS, pages 194\u2013199. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Certification considerations for adaptive systems", "author": ["S. Bhattacharyya", "D. Cofer", "D. Musliner", "J. Mueller", "E. Engstrom"], "venue": "In ICUAS, pages 270\u2013279. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D.D. Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang", "X. Zhang", "J. Zhao", "K. Zieba"], "venue": "CoRR, abs/1604.07316,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints", "author": ["P. Cousot", "R. Cousot"], "venue": "In POPL, pages 238\u2013252. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1977}, {"title": "Linear programming and extensions", "author": ["G. Dantzig"], "venue": "Princeton university press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Review of nonlinear mixed-integer and disjunctive programming techniques", "author": ["I.E. Grossmann"], "venue": "Optimization and engineering, 3(3):227\u2013252,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Safety verification of deep neural networks", "author": ["X. Huang", "M. Kwiatkowska", "S. Wang", "M. Wu"], "venue": "CoRR, abs/1610.06940,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Convnetjs: Deep learning in your browser (2014)", "author": ["A. Karpathy"], "venue": "URL http://cs.stanford.edu/people/karpathy/convnetjs,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Reluplex: An efficient SMT solver for verifying deep neural networks", "author": ["G. Katz", "C.W. Barrett", "D.L. Dill", "K. Julian", "M.J. Kochenderfer"], "venue": "CoRR, abs/1702.01135,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "and C", "author": ["Y. LeCun", "C. Cortes"], "venue": "J. Burges. The mnist database of handwritten digits,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In CPVR, pages 427\u2013436,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "In Oakland, pages 582\u2013 597. IEEE,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "An abstraction-refinement approach to verification of artificial neural networks", "author": ["L. Pulina", "A. Tacchella"], "venue": "In CAV, pages 243\u2013257. Springer,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Challenging SMT solvers to verify neural networks", "author": ["L. Pulina", "A. Tacchella"], "venue": "AI Communications, 25(2):117\u2013135,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient approximations for the arctangent function", "author": ["S. Rajan", "S. Wang", "R. Inkol", "A. Joyal"], "venue": "IEEE Signal Processing Magazine, 23(3):108\u2013111,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Verification of Artificial Neural Networks", "author": ["K. Scheibler", "L. Winterer", "R. Wimmer", "B. Becker Toward"], "venue": "In MBMV, pages 30\u201340,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast computation of arctangent functions for embedded applications: A comparative analysis", "author": ["A. Ukil", "V.H. Shah", "B. Deck"], "venue": "In ISIE, pages 1206\u20131211. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational experience with a software framework for parallel integer programming", "author": ["Y. Xu", "T.K. Ralphs", "L. Lad\u00e1nyi", "M.J. Saltzman"], "venue": "INFORMS Journal on Computing, 21(3):383\u2013397,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications such as medical image processing or semi-autonomous vehicles poses a number of new assurance, verification, and certification challenges [2,5].", "startOffset": 221, "endOffset": 226}, {"referenceID": 4, "context": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications such as medical image processing or semi-autonomous vehicles poses a number of new assurance, verification, and certification challenges [2,5].", "startOffset": 221, "endOffset": 226}, {"referenceID": 13, "context": "For ANN-based end-to-end steering control of self-driving cars, for example, it is important to know how much noisy or even maliciously manipulated sensory input is tolerated [14].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "In the MIP reduction, a number of nonlinear expressions are linearized using a variant of the well-known big-M [10] encoding strategy.", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "We also define a dataflow analysis [7] for generating relatively small big-M as the basis for speeding up MIP solving.", "startOffset": 35, "endOffset": 38}, {"referenceID": 14, "context": "We demonstrate the effectiveness and scalability of our approach and encoding heuristics by computing maximum perturbation bounds for benchmark sets such as MNIST [15] and agent games [16].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "We demonstrate the effectiveness and scalability of our approach and encoding heuristics by computing maximum perturbation bounds for benchmark sets such as MNIST [15] and agent games [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 24, "context": "Moreover, parallelization of branch-and-bound [25] on different computing cores can yield, up to a certain threshold, linear speed-ups using a high-performance parallelization framework.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "An overview of concrete problems and various approaches to the safety of machine learning is provided in [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 16, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 50, "endOffset": 59}, {"referenceID": 17, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 50, "endOffset": 59}, {"referenceID": 8, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 50, "endOffset": 59}, {"referenceID": 18, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 252, "endOffset": 262}, {"referenceID": 12, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 252, "endOffset": 262}, {"referenceID": 10, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 252, "endOffset": 262}, {"referenceID": 3, "context": "These kinds of problems might also be addressed in SMT-based approaches either by using binary search over SMT or by using SMT solvers that support optimization such as \u03bdZ [4], but it is not clear how well these approaches scale to complex ANNs.", "startOffset": 172, "endOffset": 175}, {"referenceID": 12, "context": "Recent work also targets ReLU [13] or application of a single image [11,3] (point-wise robustness or computing measures by taking samples).", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "Recent work also targets ReLU [13] or application of a single image [11,3] (point-wise robustness or computing measures by taking samples).", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "Recent work also targets ReLU [13] or application of a single image [11,3] (point-wise robustness or computing measures by taking samples).", "startOffset": 68, "endOffset": 74}, {"referenceID": 12, "context": "Our proposed resilience measure for ANNs goes beyond [13,11,3] in that it applies to multi-classification network using the softmax descriptor.", "startOffset": 53, "endOffset": 62}, {"referenceID": 10, "context": "Our proposed resilience measure for ANNs goes beyond [13,11,3] in that it applies to multi-classification network using the softmax descriptor.", "startOffset": 53, "endOffset": 62}, {"referenceID": 2, "context": "Our proposed resilience measure for ANNs goes beyond [13,11,3] in that it applies to multi-classification network using the softmax descriptor.", "startOffset": 53, "endOffset": 62}, {"referenceID": 10, "context": "Moreover, our proposed measure is a property of the classification network itself rather than just a property of a single image (as in [11]) or by only taking samples from the classifier without guarantee (as in [3]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "Moreover, our proposed measure is a property of the classification network itself rather than just a property of a single image (as in [11]) or by only taking samples from the classifier without guarantee (as in [3]).", "startOffset": 212, "endOffset": 215}, {"referenceID": 0, "context": "We introduce some basic concepts of feed-forward artificial neural networks (ANN) [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 12, "context": "In addition to [13] we are also considering tan\u22121, max-pooling and softmax nodes as commonly found in many ANNs in practice.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "ReLU [1, 1]", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "ReLU [1, 1]", "startOffset": 5, "endOffset": 11}, {"referenceID": 9, "context": "linearity in ReLU constraints x (l) i = max(0, im (l) i ) is handled using the well-known big-M method [10], which introduces a binary integer variable b (l) i together with a positive constant M (l) i such that \u2212M (l) i \u2264 im (l) i and x (l) i \u2264M (l) i for all possible values of im (l) i and x (l) i .", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "We apply static analysis [7] based on interval arithmetic for propagating the bounded input values", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "(7) of [22]) are used, and tan\u22121(im) is approximated by \u03c04 im + 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "Otherwise, when considering the case im > 1 or im < \u22121, the symmetry condition of tan\u22121 [24] states that (1) if im > 0 then tan\u22121(im) + tan\u22121( 1 im ) = \u03c0 2 , and (2) if im < 0 then tan\u22121(im) + tan\u22121( 1 im ) = \u22122 .", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "We are assuming that all input values (at layer l = 0) are bounded, and the output of bias nodes is restricted by the singleton [1, 1] (the value of the bias is given by the weight of a bias node).", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "We are assuming that all input values (at layer l = 0) are bounded, and the output of bias nodes is restricted by the singleton [1, 1] (the value of the bias is given by the weight of a bias node).", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "Finally, the output of softmax nodes is a probability in [0, 1] which might also be further refined using interval arithmetic.", "startOffset": 57, "endOffset": 63}, {"referenceID": 2, "context": "In particular, our developments do not depend on probability distributions of training and test data as in previous work [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 14, "context": "We illustrate the underlying principles of maximum resilience using examples from the MNIST database [15] for digit recognition of input images (see Fig.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "All the networks were trained using ConvNetJS [12].", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "\u2013 For MNIST digit recognition [15] has 576 input nodes for the pixels of a gray-scale image, where we trained three networks with different numbers of neurons in the hidden layers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "By restricting ourselves to only verify a single input instance and by not minimizing \u03b4, the problem under verification (local robustness related to an input) is substantially simpler and is similar to those stated in [13,3].", "startOffset": 218, "endOffset": 224}, {"referenceID": 2, "context": "By restricting ourselves to only verify a single input instance and by not minimizing \u03b4, the problem under verification (local robustness related to an input) is substantially simpler and is similar to those stated in [13,3].", "startOffset": 218, "endOffset": 224}], "year": 2017, "abstractText": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications poses a number of new verification and certification challenges. In particular, for ANN-enabled self-driving vehicles it is important to establish properties about the resilience of ANNs to noisy or even maliciously manipulated sensory input. We are addressing these challenges by defining resilience properties of ANN-based classifiers as the maximum amount of input or sensor perturbation which is still tolerated. This problem of computing maximum perturbation bounds for ANNs is then reduced to solving mixed integer optimization problems (MIP). A number of MIP encoding heuristics are developed for drastically reducing MIP-solver runtimes, and using parallelization of MIP-solvers results in an almost linear speed-up in the number (up to a certain limit) of computing cores in our experiments. We demonstrate the effectiveness and scalability of our approach by means of computing maximum resilience bounds for a number of ANN benchmark sets ranging from typical image recognition scenarios to the autonomous maneuvering of robots.", "creator": "LaTeX with hyperref package"}}}