{"id": "1705.01080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "The N-Tuple Bandit Evolutionary Algorithm for Automatic Game Improvement", "abstract": "This paper describes a new evolutionary algorithm that is especially well suited to AI-Assisted Game Design. The approach adopted in this paper is to use observations of AI agents playing the game to estimate the game's quality. Some of best agents for this purpose are General Video Game AI agents, since they can be deployed directly on a new game without game-specific tuning; these agents tend to be based on stochastic algorithms which give robust but noisy results and tend to be expensive to run. This motivates the main contribution of the paper: the development of the novel N-Tuple Bandit Evolutionary Algorithm, where a model is used to estimate the fitness of unsampled points and a bandit approach is used to balance exploration and exploitation of the search space. Initial results on optimising a Space Battle game variant suggest that the algorithm offers far more robust results than the Random Mutation Hill Climber and a Biased Mutation variant, which are themselves known to offer competitive performance across a range of problems. Subjective observations are also given by human players on the nature of the evolved games, which indicate a preference towards games generated by the N-Tuple algorithm.", "histories": [["v1", "Sat, 18 Mar 2017 09:10:09 GMT  (208kb,D)", "http://arxiv.org/abs/1705.01080v1", "8 pages, 9 figure, 2 tables, CEC2017"]], "COMMENTS": "8 pages, 9 figure, 2 tables, CEC2017", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kamolwan kunanusont", "raluca d gaina", "jialin liu", "diego perez-liebana", "simon m lucas"], "accepted": false, "id": "1705.01080"}, "pdf": {"name": "1705.01080.pdf", "metadata": {"source": "CRF", "title": "The N-Tuple Bandit Evolutionary Algorithm for Automatic Game Improvement", "authors": ["Kamolwan Kunanusont", "Raluca D. Gaina", "Jialin Liu", "Simon M. Lucas"], "emails": ["sml}@essex.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTIONAutomatic game design algorithms are systems that are capable of designing orderly and playable games without human intervention, and the development of a \"playable\" game usually involves the adjustment of an appropriate set of game parameters. Manually, this could be time-consuming due to the large search area of game parameters. Evolutionary algorithms (EAs) are therefore used to develop game parameters, one of the first attempts being that of Togelius and Schmidhuber [1]. Their results inspired the use of EAs for game parameter tuning in later work. This paper presents the results of a project that focused exclusively on AI-informed game design, using three different evolutionary algorithms. The main goal is to explore the possibility of using AI controllers with different skill levels as human-player representations to develop suitable game parameter sets using the same fitness criteria."}, {"heading": "II. LITERATURE REVIEW", "text": "This section contains a brief review of several materials consulted as part of this research. Automatic game design is a sub-section of Game Artificial Intelligence that explores the idea of developing a system that can generate dynamic and playable games. One of the first attempts to use such a system was developed by Togelius and Schmidhuber [1]. Advantages of the work they led include the ability to automatically create several new and unique games using advanced calculation methods and the speed of execution. Their results suggested that evolutionary algorithms can actually be used to search a space of possible games. Nelson and Mateas used a generative process in their paper that relates to factoring a game design process that encompasses four interacting domains: abstract game mechanics, game representation, thematic content, and control systems that map the space of possible games."}, {"heading": "III. BACKGROUND", "text": "This section provides a brief description of the games and AI controllers used in the experiments."}, {"heading": "A. Space Battle game", "text": "The ships are convex-rectangular in shape, as shown in Figure 1, with the front displayed by a single acute angular point; the first player controls the blue ship, while the second player's ship is green; the starting positions of the player ships are shown as shown in Figure 1 (left); both players act simultaneously and have the same range of actions available, namely: Rotate the ship clockwise, rotate counter-clockwise, slide (move forward) and shoot a rocket. Rotation, acceleration and shooting actions can be performed together in one tick. Simply rotate the ship in the appropriate direction, without moving it from its current position. Therefore, the ship can only move forward when the thrust action is operated. If a player decides to shoot, a round rocket appears at the game location and moves forward in the ship's direction at a certain speed."}, {"heading": "B. AI controllers", "text": "These are divided into two groups: First, the Enemy Player Set, which contains all the agents mentioned in this section. Enemy Player is a developable parameter (see Section IV-B), so the EA has access to all possible enemies as it develops the game. To analyze the depth of dexterity of the games produced by the Evolutionary Algorithm, a second set, which includes only three players, is used. (1SLA), which delegates a non-skill player, Rotate and Shoot (RAS), which represents an intermediate skill of the player and Monte Carlo Tree Search (MCTS), which represents a skillful game.1) Do Nothing is a Dummy Controller, which does not have an action in each game.2) Random: The random controller returns a randomly chosen action at each time step, by all available means can prove that it can be beaten due to its unpredictable nature."}, {"heading": "C. Random Mutation Hill Climber (RMHC)", "text": "A Random Mutation Hill Climber is the simplest version of an evolutionary algorithm with only one person in the population. It begins by randomly assigning values to each gene of the individual. A gene is randomly selected for the mutation, the fitness value of the resulting individual is calculated (similar to the MEA score) and compared with the previous one. The better individual is retained for the next iteration of the algorithm, repeating it within the scope of the budget offered. In the implementation used for this work, both the parent and the offspring in each generation are evaluated. This algorithm is often used as a plaything in the literature due to the great results achieved with simultaneous simplicity. Buzdalov et al. [17] use an RMHC algorithm combined with Q-Learning for adaptive behavior and analyze its runtime complexity based on a modified OneMax problem (with an active fitness function that led the wrong algorithm in the direction)."}, {"heading": "IV. APPROACH", "text": "Three EA algorithms were used to develop the parameters of the game Space Battle Evolved, using a fitness function to distinguish good and bad players, and to optimize games to maximize depth of skill (see section IV-F). For each algorithm, 50 attempts were performed due to the noisy game environment, as the same set of parameters could yield different fitness values in different runs. Each iteration of the evaluation process was performed for 100 evaluations.The last games were statistically analyzed according to their fitness and tested by a number of human players who expressed their subjective opinions."}, {"heading": "A. Space Battle Evolved", "text": "Space Battle Evolved (see Figure 1 for an example) is a variant of the simple Space Battle game designed for this project. Starting from the original rules of Space Battle, there are three major changes to produce this variant. First, black holes have been created that have a defined range and burden nearby objects with additional forces to pull them into their centre.Players receive a penalty for each tick in which they remain within a certain distance from a center of the black hole.However, there are areas within the black holes where no penalty is imposed, so-called safe zones.Second, in this version, two other types of missiles have been added: a two-shot type that fires two normal rockets at an angle of 45 and \u2212 45 degrees from its direction, and a bomb type that explodes after a specified time or in collision with other objects within a large radius. Finally, due to the limited types of missiles available, players will now be able to collect bundles of 20 missiles with specified time or after a specified period of time (or after a specified period of time)."}, {"heading": "B. Evolvable game parameters", "text": "There were 30 developable game parameters in total, as summarized in Table I. These can be divided into 4 categories: Rockets, Black Holes, Resources, and Enemy. 1) Rockets related: 6 of the parameters refer to missiles, including the type of missile, their maximum speed, their cooldown time (how many game ticks until the player is allowed to launch a new missile), their radius, their time to live, and finally the explosion radius of the bomb (only for rocket types). As a primary way to gain a point advantage over the opponent (and thus possibly win the game), these key parameters could be considered. 2) Black Hole related: 21 of the game parameters concern black holes, 17 relating to locations of the black hole and the rest indicating properties of the black hole. We divided the game card into a grid and allowed the evolutionary algorithms to decide whether a black hole is contained in the center of each grid or not related to a grid size ranging from 1 to 4 (hence, the grid size could vary from 1 to 4)."}, {"heading": "C. Baseline algorithm", "text": "We used Random Mutation Hill Climber as the base algorithm. The algorithm uses an array of 30 parameters for evolution, each of which is initialized to a random value. A parameter is then randomly selected and mutated (1 random gene is changed to a different value).The fitness value of this mutated game is calculated by playing three games with the three AI controllers of different skill levels and following the method in section IV-F. If the mutated game ends up having a higher fitness than its parent, algorithm 1 Random Mutation Hill Climber (RMHC) 1: Input: Game Parameter List Parameters, Number of studies nTrials, 2: Number of ratings nEvals 3: Output: evolved parameter sets 4: BEGIN 5: Parameter lists."}, {"heading": "D. Biased Mutation RMHC", "text": "The biased mutation RMHC (B-RMHC) was inspired by the idea that different parameters influence the change in fitness values at different rates. That is, changing one parameter \u2190 could significantly affect fitness value more than others. Therefore, a biased mutation in the direction of more interesting parameters was used to obtain more varied games and accelerate development. The algorithm consists of two parts: parameter preprocessing and actual development. [The parameters were divided into two groups, separating the cells of the black hole (group B) from the rest (group A.) For the pre-processing of group A, the parameters were obtained random values to start with each parameter. Then, the importance metric was calculated based on the standard deviation from fitness in N tests, where N is the total number of values that the parameters tested can take."}, {"heading": "E. N-Tuple Bandit Evolutionary Algorithm", "text": "The evaluation mechanisms used in this paper are executed by stochastic agents, such as MCTS, and fairly expensive CPU times. Therefore, it is desirable to have an evolutionary algorithm that is able to work very efficiently, and also one that focuses on noise. N-Tuple Bandit EA meets these criteria. The algorithm works as follows: it selects a random point in the search space called a current point."}, {"heading": "F. Fitness evaluation", "text": "The fitness value of each game was evaluated with 3 moves by using 1SLA, RAS and MCTS as players. For each game played, both players were divided by 100 to lower the scale, then the winner received 1000 bonus points to prioritize the result in creating the final result. Tg = (S1 100 + W1) \u2212 (S2 100 + W2) (2) After the total score Tg for each game was calculated, it was included in the final fitness calculation according to Equation 3, where T1 is the fitness of the weak player (1SLA) \u2212 (S2 100 + W2) (2) After the total score Tg for each game was calculated, it was shown in the final fitness calculation according to Equation 3, where T1 is the fitness of the weak player (1SLA) \u2212 (S2 100 + W2) (2)."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "We apply the RMHC, the Biased Mutation RMHC (referred to as B-RMHC) and the N-Tuple Bandit Mutation Algorithm (referred to as N-Tuple) 50 times independently of each other to develop game instances, so that a total of 150 games are designed. 100 game evaluations are assigned to each of the algorithms during evolution. We then pick up some of the developed game instances so that human players can test and analyze their feedback."}, {"heading": "A. Selection of designed games by reevaluation", "text": "To select the game instance for human testing, each of the game instances developed is then evaluated 100 times, with each evaluation taking into account the results of three games played by the 1SLA, RAS and MCTS controllers (detailed in Section IV-F).The sorted average fitness scores above 100 ratings and standard errors are shown in Figure 2.The N-Tuple algorithm (green marker) surpasses both the RMHC HC HC HC HC HC HC and its variant. In addition, N-Tuple is more robust and exhibits more stable performance (negligible standard error). Of 50 game instances developed by the N-Tuple Bandit Mutation algorithm, only a few of them (far left in Figure 2) exhibit average fitness below zero. Nevertheless, the lowest average fitness is still much higher than most games developed by both RMHC and B-RMHC.A, and the two-tailed RMHC-Whitney-U test shows that the MHC-MHC-MHC-1 MHC-1 MHC-MHC-1 MHC-1 matches are the most significant differences between each of the MHC-1 MHC-MHC-1 MHC-1 matches."}, {"heading": "B. Evaluation by human players", "text": "We selected the games with the highest and lowest average fitness, designed by the 3 algorithms, and invited two human players to rate them. Human players were asked to play the 6 games and give feedback without being told the fitness level of each game. Figure 3 presents a screenshot of each game, as well as feedback from both players. The two human players rated the games differently according to game preference. Player A is more interested in the challenging aspect of the game and is more attracted to unusual game scenarios; Player B is less easily satisfied and found most games boring. Interestingly, although they rated the games differently, both have a preference for the game G3H (with the highest average fitness value, optimized by N-tuples) and reject the games G1H (with the highest average fitness value, optimized by RMHC) and G2H (with the highest average fitness value, optimized by B-RMHC)."}, {"heading": "C. Manual tuning of the evolved game", "text": "It is noteworthy that the game G2H uses a doNothing opponent. We manually changed the ENEMY ID parameter to use an MCTS controller as the opponent instead, and asked the two human players to play the edited game. Player A found the new game improved, but still a basic game with big rockets, not very interesting compared to the previous games. Player B, however, found the new game better, as the agent now moves on the map and has even increased his position in their personal rankings."}, {"heading": "VI. CONCLUSION", "text": "One of the great challenges in the game is the adjustment of the game parameters. In view of a number of parameter values, a new game instance is created. The difficulty of a game could change significantly if a single parameter of the game is changed. However, the choice of game parameters and the number of parameters to capture is not trivial."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper describes a new evolutionary algorithm<lb>that is especially well suited to AI-Assisted Game Design. The<lb>approach adopted in this paper is to use observations of AI<lb>agents playing the game to estimate the game\u2019s quality. Some<lb>of best agents for this purpose are General Video Game AI<lb>agents, since they can be deployed directly on a new game<lb>without game-specific tuning; these agents tend to be based on<lb>stochastic algorithms which give robust but noisy results and tend<lb>to be expensive to run. This motivates the main contribution<lb>of the paper: the development of the novel N-Tuple Bandit<lb>Evolutionary Algorithm, where a model is used to estimate the<lb>fitness of unsampled points and a bandit approach is used to<lb>balance exploration and exploitation of the search space. Initial<lb>results on optimising a Space Battle game variant suggest that<lb>the algorithm offers far more robust results than the Random<lb>Mutation Hill Climber and a Biased Mutation variant, which<lb>are themselves known to offer competitive performance across<lb>a range of problems. Subjective observations are also given by<lb>human players on the nature of the evolved games, which indicate<lb>a preference towards games generated by the N-Tuple algorithm.", "creator": "LaTeX with hyperref package"}}}