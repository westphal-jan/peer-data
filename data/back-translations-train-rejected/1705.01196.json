{"id": "1705.01196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning", "abstract": "Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of using Deep Reinforcement Learning to handle intersection problems. Combining several recent advances in Deep RL, were we able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate. Our analysis, and the solutions learned by the network point out several short comings of current rule-based methods. The fact that Deep RL policies resulted in collisions, although rarely, combined with the limitations of the policy to generalize well to out-of-sample scenarios suggest a need for further research.", "histories": [["v1", "Tue, 2 May 2017 22:57:36 GMT  (5830kb,D)", "http://arxiv.org/abs/1705.01196v1", "Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017)"]], "COMMENTS": "Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017)", "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["david isele", "akansel cosgun", "kaushik subramanian", "kikuo fujimura"], "accepted": false, "id": "1705.01196"}, "pdf": {"name": "1705.01196.pdf", "metadata": {"source": "CRF", "title": "Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning", "authors": ["David Isele", "Akansel Cosgun", "Kaushik Subramanian", "Kikuo Fujimura"], "emails": [], "sections": [{"heading": null, "text": "The current state of the art is a rules-based collision method based on variation (TC) in urban environments. To successfully navigate through an intersection, it is necessary to understand the dynamics of vehicles, interpret other drivers \"intentions, and behave predictably, so that other drivers can respond appropriately. Learning this behavior requires optimizing multiple opposing goals, including safety, efficiency, and minimizing traffic disruption. Balancing these trade relationships can be challenging even for human drivers: 20% of all accidents occur at intersections [1]. The ability to work optimally at traffic junctions can both enhance the capabilities of autonomous actors and increase safety through driver assistance when a human driver is in control. A number of rules-based strategies have already been applied to dealing with intersections, including cooperative [2] and heuristic [3] approaches requiring communication between vehicles and making them unscalable."}, {"heading": "II. APPROACH", "text": "We consider the use of intersections as an intensified learning problem and use a Deep Q Network (DQN) to learn the Q function state value."}, {"heading": "A. Reinforcement Learning", "text": "The sequence of states, actions, and rewards is typically formulated as the Markov decision-making process (MDP) < S, A, P, R, \u03b3 >, where S is the set of states and A is the set of actions the actor can perform. MDPs follow the Markov assumption that the probability of transition to a new state given the current state and action is independent of all previous states and actions p (st + 1 | st, at,.., s0, a0) = p (st + 1 | st, at). The probability of transition to a new state given the current state and action is independent of all previous states and actions p (st + 1 | st, at,.). The probability of transition P: S \u2192 S = 0,1 \u00b7 R \u00b7 the reward for previous states and actions p (st + 1 | st, at, at)."}, {"heading": "B. Q-learning", "text": "In Q-Learning [22], the action value function Q\u03c0 (s, a) is the expected return E [Rt | st = s, a] for a state-action pair following a policy \u03c0. In Deep Q-Learning [16], the optimal value function can be derived at any time by selecting the action with the maximum value maxa Q * (s, a). In Deep Q-Learning [16], the optimal value function is approximated with a neural network Q * (s, a) \u2248 Q (s, a) with the parameters \u03b8. The action value function is learned by iteratively minimizing the error between the expected return and the state-action value predicted by the network."}, {"heading": "C. Dynamic Frame Skipping", "text": "Often the same repeated actions are required over multiple time steps. It has recently been shown that allowing an agent to select actions over longer periods of time improves an agent's learning time. [19] For example, instead of researching through trial and error and building over a series of learning steps that eight time steps is the appropriate time for an agent to wait for a car to pass by, the agent only has to determine that an action \"waiting eight steps\" is appropriate. Dynamic frame skipping can be considered a simplified version of options [23], which has recently been explored by the Deep RL community. [24], [25], [26]."}, {"heading": "D. Prioritized Experience Replay", "text": "An experience replay buffer stores previous trajectories that can be scanned during learning. Advantage of using experience replay is that important sequences that occur less frequently can be scanned preferentially [20]. We use the simplified approach proposed by Jaderberg et al. [24], which avoids calculating a ranking and instead uses random samples to balance rewards across trajectories."}, {"heading": "E. State-Action Representations", "text": "Autonomous vehicles use a series of sensors and allow planning at multiple levels of abstraction, enabling a variety of state and action representations. An explorative search for representations showed that the choice of representation had a significant impact on the agent's ability to learn. In this essay, we present the two representations that we found good. Sequential Actions In Sequential Action Representation, the agent is provided with the desired path and the agent is determined to accelerate, slow down or maintain the speed at any time along the desired path. A bird's eye view of space is discredited into a grid in cartesian coordinates relative to the frame of reference of the car. This is a representation that could easily be constructed from the LiDAR scans of a car. Each car in space is represented by its control angle, its speed and its calculated time to collision. The angle of direction, the speed and the calculated time to collision are all represented as real values."}, {"heading": "III. EXPERIMENTS", "text": "We train two different DQNs (Sequential Actions and Time-to-Go) on a variety of intersection scenarios and compare performance with the heuristic Time-to-Collision (TTC) algorithm."}, {"heading": "A. Time-To-Collision (TTC) Policy", "text": "Here is an explanation of the algorithm. Consider an imaginary line starting from the front of the first-person vehicle, aligned with the longitudinal axis. We calculate the TTC with a vehicle other than the time it takes the vehicle to reach this imaginary line, provided it drives at a constant speed. Of all the vehicles in the scene, we consider this to be the minimum TTC value. If this value exceeds the TTC threshold, the first-person vehicle begins the crossing phase and follows the Intelligent Driver Model (IDM) [28] until the destination is reached. If it does not exceed this threshold, the first-person car continues to wait."}, {"heading": "B. Experimental setup", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "IV. RESULTS", "text": "The results of the study show that the share of women in the total population in the total population compared to the men compared to the women compared to the men compared to the men compared to the men compared to the men compared to the women compared to the men compared to the women compared to the women compared to the men compared to the men compared to the men compared to the women compared to the men compared to the men compared to the women compared to the men compared to the women compared to the women compared to the men compared to the women compared to the men compared to the men compared to the women compared to the men compared to the women compared to the men compared to the women compared to the men compared to the women compared to the men compared to the women compared to the women compared to the women compared to the men compared to the men compared to the women compared to the women compared to the men compared to the women compared to the men compared to the women compared to the men compared to the women compared to the women compared to the women compared to the period."}, {"heading": "A. Generalization and Transfer", "text": "We believe that the few collisions that occur are a symptom of the difficulty of generalizing the system. Instead of making greater efforts to reduce the number of collisions, we believe that it is more likely to serve to build a robust system to understand how the system generalizes. To do this, we operate the network that is trained from one scenario to all the other scenarios. We suspect that training on multiple scenarios will improve the performance of each and every task. This is one of the core principles of multi-task learning [34], it was recently demonstrated specifically on robots that learn CNN representations from physical actions [35], and it is the intended direction of our future research. However, with this first study, the focus was on gaining an understanding of how well a deep network system can generalize the transmission performance of level-of-sample data. Figure 5 shows the transmission performance for both sequential and time-to-NT Qs."}, {"heading": "B. Qualitative Analysis", "text": "The DQN strategies take into account the predictive behavior of traffic. DQNs can accurately predict that traffic in distant lanes will be over until the first-person car arrives in the lane. Also, the DQN driver can predict whether the coming traffic will have enough time to brake or not. The few collisions seem to refer to discretification effects where the car almost overlooks oncoming traffic. As a result, the TTC often waits until the road is completely clear and misses many opportunities to cross. We see that selecting the departure time leaves sufficient safety margin for approaching cars in distant lanes as the same safety margin is used to increase the gap in narrow lanes. As a result, the TTC often waits until the road is completely clear and misses many opportunities to cross."}, {"heading": "V. CONCLUSIONS", "text": "Dealing with unsignaled intersections remains a difficult task for autonomous vehicles, mainly due to the unpredictable behavior of agents. Rule-based intersection handling methods provide reliable and easy-to-interpret solutions, but result in suboptimal behavior and reduced task performance. We showed an initial system that uses Deep Q Networks for the specific problem of intersection management. By using the latest deep RL techniques, we were able to build networks that exceed a commonly used rule-based algorithm based on heuristic time-to-collision (TTC) in some metrics. While TTC achieved zero collision rates in all cases, DQN performed better in terms of task efficiency and success rate. Although DQN methods rarely caused collisions and are therefore unsuitable for implementation in the real world in their current form, further investigation via DQN is necessary to reduce the collision rate to zero. We saw that the determination of time is the most important part of the task to go."}], "references": [{"title": "Cooperative collision avoidance at intersections: Algorithms and experiments", "author": ["M.R. Hafner", "D. Cunningham", "L. Caminiti", "D. Del Vecchio"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 14, no. 3, pp. 1162\u20131175, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Autonomous vehicle control systems for safe crossroads", "author": ["J. Alonso", "V. Milan\u00e9s", "J. P\u00e9rez", "E. Onieva", "C. Gonz\u00e1lez", "T. De Pedro"], "venue": "Transportation research part C: emerging technologies, vol. 19, no. 6, pp. 1095\u20131110, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Extended time-to-collision measures for road traffic safety assessment", "author": ["M.M. Minderhoud", "P.H. Bovy"], "venue": "Accident Analysis & Prevention, vol. 33, no. 1, pp. 89\u201397, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Time-to-collision and collision avoidance systems", "author": ["R. van der Horst", "J. Hogema"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "A comparison of headway and time to collision as safety indicators", "author": ["K. Vogel"], "venue": "Accident analysis & prevention, vol. 35, no. 3, pp. 427\u2013 433, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "A reasoning framework for autonomous urban driving", "author": ["D. Ferguson", "C. Baker", "M. Likhachev", "J. Dolan"], "venue": "Intelligent Vehicles Symposium, 2008 IEEE. IEEE, 2008, pp. 775\u2013780.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Team AnnieWays autonomous system for the DARPA urban challenge 2007", "author": ["S. Kammel", "J. Ziegler", "B. Pitzer", "M. Werling", "T. Gindele", "D. Jagzent", "J. Sch\u00f6der", "M. Thuy", "M. Goebl", "F. von Hundelshausen"], "venue": "The DARPA Urban Challenge. Springer, 2009, pp. 359\u2013391.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Autonomous driving in urban environments: Boss and the urban challenge", "author": ["C. Urmson", "J. Anhalt", "D. Bagnell", "C. Baker", "R. Bittner", "M. Clark", "J. Dolan", "D. Duggins", "T. Galatali", "C. Geyer"], "venue": "Journal of Field Robotics, vol. 25, no. 8, pp. 425\u2013466, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards full automated drive in urban environments: A demonstration in gomentum station, california", "author": ["A. Cosgun", "L. Ma", "J. Chiu", "J. Huang", "M. Demir", "A.M. Anon", "T. Lian", "H. Tafish", "S. Al-Stouhi"], "venue": "IEEE Intelligent Vehicles Symposium (IV), 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "The MIT\u2013 Cornell collision and why it happened", "author": ["L. Fletcher", "S. Teller", "E. Olson", "D. Moore", "Y. Kuwata", "J. How", "J. Leonard", "I. Miller", "M. Campbell", "D. Huttenlocher"], "venue": "Journal of Field Robotics, vol. 25, no. 10, pp. 775\u2013807, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "arXiv preprint arXiv:1604.07316, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Belief state planning for navigating urban intersections", "author": ["M. Bouton", "A. Cosgun", "M.J. Kochenderfer"], "venue": "IEEE Intelligent Vehicles Symposium (IV), 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Intention-aware autonomous driving decision-making in an uncontrolled intersection", "author": ["W. Song", "G. Xiong", "H. Chen"], "venue": "Mathematical Problems in Engineering, vol. 2016, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic decisionmaking under uncertainty for autonomous driving using continuous pomdps", "author": ["S. Brechtel", "T. Gindele", "R. Dillmann"], "venue": "Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on. IEEE, 2014, pp. 392\u2013399.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning handeye coordination for robotic grasping with deep learning and largescale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "arXiv preprint arXiv:1603.02199, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, vol. 17, no. 39, pp. 1\u201340, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic action repetition for deep reinforcement learning", "author": ["A. Srinivas", "S. Sharma", "B. Ravindran"], "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "arXiv preprint arXiv:1511.05952, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental multi-step q-learning", "author": ["J. Peng", "R.J. Williams"], "venue": "Machine learning, vol. 22, no. 1-3, pp. 283\u2013290, 1996.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "arXiv preprint arXiv:1604.07255, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 3675\u20133683.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L.-J. Lin"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 293\u2013321, 1992.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "Congested traffic states in empirical observations and microscopic simulations", "author": ["M. Treiber", "A. Hennecke", "D. Helbing"], "venue": "Physical Review E, vol. 62, no. 2, p. 1805, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1805}, {"title": "Recent development and applications of SUMO\u2013simulation of urban mobility", "author": ["D. Krajzewicz", "J. Erdmann", "M. Behrisch", "L. Bieker"], "venue": "International Journal on Advances in Systems and Measurements (IARIA), vol. 5, no. 3\u20134, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Microscopic modeling of traffic flow: Investigation of collision free vehicle dynamics", "author": ["S. Krauss"], "venue": "Ph.D. dissertation, Deutsches Zentrum fuer Luft-und Raumfahrt, 1998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, no. 1, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-rmsprop, coursera: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "University of Toronto, Tech. Rep, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, pp. 41\u2013 75, 1997.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to push by grasping: Using multiple tasks for effective learning", "author": ["L. Pinto", "A. Gupta"], "venue": "arXiv preprint arXiv:1609.09025, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "A number of rule-based strategies have already been applied to intersection handling, including cooperative [2] and heuristic [3] approaches.", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "A number of rule-based strategies have already been applied to intersection handling, including cooperative [2] and heuristic [3] approaches.", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "The current state of the art is a rule-based method based on time-to-collision (TTC) [4], [5], which is a widely used heuristic as a safety indicator in the automotive industry [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "The current state of the art is a rule-based method based on time-to-collision (TTC) [4], [5], which is a widely used heuristic as a safety indicator in the automotive industry [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "The current state of the art is a rule-based method based on time-to-collision (TTC) [4], [5], which is a widely used heuristic as a safety indicator in the automotive industry [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "Variants of the TTC approach has been used for autonomous driving [7] and the DARPA urban challenge, where hand engineered hierarchical state machines were a popular approach to handle intersections [8], [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Variants of the TTC approach has been used for autonomous driving [7] and the DARPA urban challenge, where hand engineered hierarchical state machines were a popular approach to handle intersections [8], [9].", "startOffset": 199, "endOffset": 202}, {"referenceID": 7, "context": "Variants of the TTC approach has been used for autonomous driving [7] and the DARPA urban challenge, where hand engineered hierarchical state machines were a popular approach to handle intersections [8], [9].", "startOffset": 204, "endOffset": 207}, {"referenceID": 8, "context": "TTC is currently the method we employ on our autonomous vehicle [10].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "is problematic: in the DARPA Urban Challenge, one reason behind a collision between two autonomous cars was \u201cfailure to anticipate vehicle intent\u201d [11].", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "In imitation learning, the policy is learned from a human drivers [12], however this policy does not offer a solution if the agent finds itself in a state that is not part of the training data.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "(POMCP) have been shown to handle intersections [13], but rely on the existence of an accurate generative model.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "Offline learning tackles the intersection problem, often by using Markov Decision Processes (MDP) in the back-end [14], [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "Offline learning tackles the intersection problem, often by using Markov Decision Processes (MDP) in the back-end [14], [15].", "startOffset": 120, "endOffset": 124}, {"referenceID": 14, "context": "Given the recent success of Deep Learning on a variety of control problems, [16], [17], [18] we are interested in evaluating the effectiveness of Deep RL in the domain of intersection handling.", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "Given the recent success of Deep Learning on a variety of control problems, [16], [17], [18] we are interested in evaluating the effectiveness of Deep RL in the domain of intersection handling.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "Given the recent success of Deep Learning on a variety of control problems, [16], [17], [18] we are interested in evaluating the effectiveness of Deep RL in the domain of intersection handling.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "We use dynamic frame skipping [19] to expedite the system learning repeated actions, and prioritized reply [20] to ensure the network balances learning both positive and negative cases.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "We use dynamic frame skipping [19] to expedite the system learning repeated actions, and prioritized reply [20] to ensure the network balances learning both positive and negative cases.", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "advantage of the off-policy nature imposed by experience replay learning to calculate and train on the full n-step return [21] which we found greatly reduces the learning time of DQNs.", "startOffset": 122, "endOffset": 126}, {"referenceID": 20, "context": "In Q-learning [22], the action value function Q\u03c0(s,a) is the expected return E[Rt |st = s,a] for a state-action pair following a policy \u03c0 .", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "In Deep Q-learning [16], the optimal value function is approximated with a neural network Q\u2217(s,a) \u2248 Q(s,a;\u03b8) with parameters \u03b8 .", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "One way to make learning more efficient is to use n-step return[21] E[Rt |st = s,a] \u2248 rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7+ \u03b3rt+n\u22121 + \u03b3n maxat+n Q(st+n,at+n;\u03b8).", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "It was recently shown that allowing an agent to select actions over extended time periods improves the learning time of an agent [19].", "startOffset": 129, "endOffset": 133}, {"referenceID": 21, "context": "Dynamic frame skipping can viewed as a simplified version of options [23] which is recently starting to be explored by the Deep RL community.", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "[24], [25], [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], [25], [26].", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "[24], [25], [26].", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "In order to break correlations between sequential steps of the agent, experience replay is used [27].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "can be preferentially sampled [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "[24] which avoids the computation of a rank list and instead samples to balancing reward across trajectories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "If this value exceeds the TTC threshold, then the ego vehicle starts the crossing phase and follows the Intelligent Driver Model (IDM) [28] until the goal is reached.", "startOffset": 135, "endOffset": 139}, {"referenceID": 27, "context": "Experiments were run using the Sumo simulator [29], which is an open source traffic simulation package.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "and by using parameters that control driver imperfection (based on the Krauss stochastic driving model [30]).", "startOffset": 103, "endOffset": 107}, {"referenceID": 29, "context": "The sequential action network is a fully connected networks with leaky ReLU [31] activation functions.", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "Both networks are optimized using the RMSProp algorithm [32].", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "This allows us to train directly on the nstep return and forgo the added complexity of using target networks [33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 32, "context": "This is one of the core principles of multi-task learning [34], it has recently been demonstrated specifically on robots learning CNN representations from physical actions [35], and it is the intended direction of our future research.", "startOffset": 58, "endOffset": 62}, {"referenceID": 33, "context": "This is one of the core principles of multi-task learning [34], it has recently been demonstrated specifically on robots learning CNN representations from physical actions [35], and it is the intended direction of our future research.", "startOffset": 172, "endOffset": 176}], "year": 2017, "abstractText": "Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of using Deep Reinforcement Learning to handle intersection problems. Combining several recent advances in Deep RL, were we able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate. Our analysis, and the solutions learned by the network point out several short comings of current rule-based methods. The fact that Deep RL policies resulted in collisions, although rarely, combined with the limitations of the policy to generalize well to out-of-sample scenarios suggest a need for further research.", "creator": "LaTeX with hyperref package"}}}