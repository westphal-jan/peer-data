{"id": "1705.01197", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously Handling Multiple Intersections", "abstract": "We analyze how the knowledge to autonomously handle one type of intersection, represented as a Deep Q-Network, translates to other types of intersections (tasks). We view intersection handling as a deep reinforcement learning problem, which approximates the state action Q function as a deep neural network. Using a traffic simulator, we show that directly copying a network trained for one type of intersection to another type of intersection decreases the success rate. We also show that when a network that is pre-trained on Task A and then is fine-tuned on a Task B, the resulting network not only performs better on the Task B than an network exclusively trained on Task A, but also retained knowledge on the Task A. Finally, we examine a lifelong learning setting, where we train a single network on five different types of intersections sequentially and show that the resulting network exhibited catastrophic forgetting of knowledge on previous tasks. This result suggests a need for a long-term memory component to preserve knowledge.", "histories": [["v1", "Tue, 2 May 2017 23:05:56 GMT  (3281kb,D)", "http://arxiv.org/abs/1705.01197v1", "Submitted to IEEE International Conference on Intelligent Transportation Systems (ITSC 2017)"]], "COMMENTS": "Submitted to IEEE International Conference on Intelligent Transportation Systems (ITSC 2017)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["david isele", "akansel cosgun", "kikuo fujimura"], "accepted": false, "id": "1705.01197"}, "pdf": {"name": "1705.01197.pdf", "metadata": {"source": "CRF", "title": "Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously Handling Multiple Intersections", "authors": ["David Isele", "Akansel Cosgun", "Kikuo Fujimura"], "emails": [], "sections": [{"heading": null, "text": "In recent years, companies have increased their spending on Automated Driving (AD) research and development for good reason: AD promises to greatly reduce the number of fatalities and increase the productivity of society as a whole. Although AD technology has made significant advances in the last few years, current technology is not yet ripe for widespread adoption. Urban environments in particular pose significant challenges for AD, due to the unpredictable nature of pedestrians and vehicles in urban traffic. Dealing with intersections is one of the most difficult issues for Urban AD. Rule-based methods provide a predictable method of managing intersections. However, rule-based handling of intersections is not good because it is becoming increasingly difficult to design craftsmanship rules, as the algorithm designer has to deal with craftsmanship rules to find craftsmanship rules and parameters for different types of interfaces."}, {"heading": "II. RELATED WORK", "text": "Recently, there has been an increased interest in the use of machine learning techniques to control autonomous vehicles. In mimicking learning behavior, politics is learned from a human driver [1]. Online planners based on partially observable Monte Carlo Planning (POMCP) have shown that they deal with intersections [2] when the existence of a precise generative model is available, and Markov Decision Processes (MDP) have been deployed offline to address the interface problem [3], [4]. In addition, machine learning has been used to optimize the comfort of a number of safe trajectories [5]. Machine learning has greatly benefited from training on large amounts of data, helping a system learn general representations and preventing it from being based on random correlations in the data collected. In the absence of huge data sets, training can make similar improvements to several related tasks [6]."}, {"heading": "A. Reinforcement Learning", "text": "In reinforcement learning, an agent in state s takes an action a according to the guidelines parameterized by \u03b8. The agent switches to state s and receives a reward r. This survey is defined as experience e = (s, a, r, s \"). This is typically formulated as the Markov decision-making process (MDP) < S, A, P, R, \u03b3 >, where S is the set of states, A is the set of actions the agent is allowed to perform, P: S \u00b7 A \u2192 S is the state transition function, R: S \u00b7 A \u00b7 S \u2192 R is the reward function, and \u03b3 (0.1] is a discount factor that adds up the preference for previous rewards and provides stability in the event of infinite time horizons. MDPs follow the Markov assumption that the probability of transition to a new state in view of the current state and action is independent of all previous states and actions (p + 1, in the fog)."}, {"heading": "B. Q-learning", "text": "Q-Learning defines an optimal action value function Q-Learning (s, a) as the maximum expected return that can be achieved following any policy. This follows the dynamic programming properties of the Bellman equation, which states that if the values Q-Learning (s-Learning, a-Learning) are known to all a-Learning, then the optimal strategy is to select a \"Learning Agent\" that maximizes the expected value of \"r +\" Q-Learning \"(s-Learning)."}, {"heading": "C. Deep Neural Network setup", "text": "The DQN uses a folding neural network with two folding layers and a fully joined layer. The first folded layer has 32 6 x 6 filters with step two, the second folded layer has 64 3 x 3 filters with step 2. The fully connected layer has 100 nodes. All layers use leaky ReLU activation functions [24]. The last linear output layer has five outputs: a single action and a wait action on four time scales (1, 2, 4 and 8 time steps). The network is optimized with the RMSProp algorithm [25]. Our experience replay buffers have an allocation of 1,000 experiences. \u2212 For each learning iteration we try a stack of 60 experiences. Since the experience replay buffer forces extraordinary learning, we are able to calculate the return for each state buffer shareholder in the trajectory, and to insert the target space x for each step directly into the buffer \u2212 26."}, {"heading": "IV. KNOWLEDGE TRANSFER", "text": "We are interested in exchanging knowledge between different driving tasks. By exchanging knowledge from different tasks, we can shorten the learning time and create more general and capable systems. Ideally, the exchange of knowledge can be extended to a system that continues to learn even after its deployment [27] and can enable a system to accurately predict appropriate behavior in novel situations [28]. We study the behavior of different strategies of knowledge exchange in the field of autonomous driving."}, {"heading": "A. Direct copy", "text": "To show the magnitude of the transfer and the difference between tasks, we train a network on a single source task for 25,000 iterations. The unmodified network is then evaluated for each other task. We repeat this process by using each individual task as the source task."}, {"heading": "B. Fine tuning", "text": "Starting with a network trained to perform 10,000 iterations on a source task, we then refine a network for another 25,000 iterations on the second target task. We use 10,000 iterations because it demonstrates considerable learning but is suboptimal to highlight the potential benefits of the transfer. Fine tuning demonstrates jump-start and asymptotic performance as described by Taylor and Stone [8]."}, {"heading": "C. Reverse transfer", "text": "When a network improves the performance of a previous task, it is called reverse transfer. It is known that neural networks often forget previous tasks, which is called catastrophic forgetting [29], [30], [15]. In the case of forgetting, retention describes the amount of previous knowledge that the network retained in a new task after training. This value is formally difficult to define, as it must exclude any relevant knowledge for source tasks gained from training on the target task, and furthermore, retention may include aspects that are not quantifiable, such as network weight configurations. For example, a network may have catastrophic forgetting, but may actually have retained a weight configuration required for retraining the source task. Due to the difficulty of defining retention, we define empirical retention as the difference between direct copy and finely tuned direct copy of the same network."}, {"heading": "D. Lifelong Learning", "text": "Lifelong learning is the process of sequential learning of multiple tasks, where the goal is to optimize the performance of each task [27], [31]. Combining information from all previous tasks can be used to learn a new task in leaps and bounds. In a reciprocal manner, learning a new task can refine potentially existing knowledge for previous tasks. By using a single system that handles all tasks, the system is able to handle a broader range of problems and is likely to be more applicable to new problems.We study how a deep Q network works when it learns a sequence of tasks. The order in which tasks are addressed affects learning, and several groups have studied the effects of the sequence of tasks [32], [33]. For our experiments, we use a task order that shows that forgetting and holding on to these tasks is fixed for all experiments.We are interested in how the performance of each task changes over time. We test it periodically as a separate process that does not affect the performance of each task."}, {"heading": "V. EXPERIMENTAL SETUP", "text": "In recent years, it has become clear that the number of offenders who are able to cross the road is increasing."}, {"heading": "VI. RESULTS", "text": "In no case is a network that is aligned to a different task better than a network that is aligned to a different task, but we see that several tasks achieve similar performance with a transfer. In particular, we see that the single-track tasks (right, left and forward) are linked to each other. Fine tuning shows the fine tuning results. We see that in almost all cases the multi-track settings (left and right) are linked to each other, and that the single-track tasks are much better than the single-track tasks. Fine tuning shows the fine tuning results."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we look at the AD vehicle as a learning tool in a learning environment characterized by reinforcement, and analyze how the knowledge for dealing with a type of intersection, represented as the Deep Q Network, is transferred to other types of intersection points. We examined and compared four different transfer methods between different intersections (tasks): direct copying, fine tuning, feedback, and lifelong learning. Our results generally performed better than a randomly initialized network trained on task B. Third, when a network is trained on task A, but tested directly on task B. Second, a network initialized on the network of task A, and then worked out on task B, generally performed better than a randomly initialized network trained on task B. Third, when a network initialized on task A is polished on task B and tested again on task A, it yielded better results than a network that was directly copied from task B to task A. Finally, we examine a lifelong learning area in which we have five learning points to master."}], "references": [{"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "arXiv preprint arXiv:1604.07316, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Belief state planning for navigating urban intersections", "author": ["M. Bouton", "A. Cosgun", "M.J. Kochenderfer"], "venue": "IEEE Intelligent Vehicles Symposium (IV), 2017.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Probabilistic decisionmaking under uncertainty for autonomous driving using continuous pomdps", "author": ["S. Brechtel", "T. Gindele", "R. Dillmann"], "venue": "Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on. IEEE, 2014, pp. 392\u2013399.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Intention-aware autonomous driving decision-making in an uncontrolled intersection", "author": ["W. Song", "G. Xiong", "H. Chen"], "venue": "Mathematical Problems in Engineering, vol. 2016, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Safe, multiagent, reinforcement learning for autonomous driving", "author": ["S. Shalev-Shwartz", "S. Shammah", "A. Shashua"], "venue": "arXiv preprint arXiv:1610.03295, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, pp. 41\u2013 75, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "A Survey on Transfer Learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research, vol. 10, no. Jul, pp. 1633\u20131685, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "CNN Features off-the-shelf: an Astounding Baseline for Recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 512\u2013 519, Mar. 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "How transferable are features in deep neural networks ?", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Progressive neural networks", "author": ["A.A. Rusu", "N.C. Rabinowitz", "G. Desjardins", "H. Soyer", "J. Kirkpatrick", "K. Kavukcuoglu", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1606.04671, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowledge transfer for deep reinforcement learning with hierarchical experience replay", "author": ["H. Yin", "S.J. Pan"], "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2013, pp. 2310\u20132318.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["J. Kirkpatrick", "R. Pascanu", "N. Rabinowitz", "J. Veness", "G. Desjardins", "A.A. Rusu", "K. Milan", "J. Quan", "T. Ramalho", "A. Grabska-Barwinska"], "venue": "arXiv preprint arXiv:1612.00796, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["I.J. Goodfellow", "M. Mirza", "D. Xiao", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6211, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Incremental multi-step q-learning", "author": ["J. Peng", "R.J. Williams"], "venue": "Machine learning, vol. 22, no. 1-3, pp. 283\u2013290, 1996.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Dynamic action repetition for deep reinforcement learning", "author": ["A. Srinivas", "S. Sharma", "B. Ravindran"], "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "arXiv preprint arXiv:1604.07255, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 3675\u20133683.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, no. 1, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-rmsprop, coursera: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "University of Toronto, Tech. Rep, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Is learning the n-th thing any easier than learning the first?", "author": ["S. Thrun"], "venue": "Advances in neural information processing systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "Using task features for zeroshot knowledge transfer in lifelong learning", "author": ["D. Isele", "M. Rostami", "E. Eaton"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["M. McCloskey", "N.J. Cohen"], "venue": "Psychology of learning and motivation, vol. 24, pp. 109\u2013165, 1989.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1989}, {"title": "Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions", "author": ["R. Ratcliff"], "venue": "Psychological review, vol. 97, no. 2, pp. 285\u2013308, 1990.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1990}, {"title": "ELLA: An efficient lifelong learning algorithm", "author": ["P. Ruvolo", "E. Eaton"], "venue": "Proceedings of the International Conference on Machine Learning, vol. 28, pp. 507\u2013515, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "International Conference on Machine Learning, pp. 41\u201348, 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Active task selection for lifelong machine learning.", "author": ["P. Ruvolo", "E. Eaton"], "venue": "AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Recent development and applications of SUMO\u2013simulation of urban mobility", "author": ["D. Krajzewicz", "J. Erdmann", "M. Behrisch", "L. Bieker"], "venue": "International Journal on Advances in Systems and Measurements (IARIA), vol. 5, no. 3\u20134, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Microscopic modeling of traffic flow: Investigation of collision free vehicle dynamics", "author": ["S. Krauss"], "venue": "Ph.D. dissertation, Deutsches Zentrum fuer Luft-und Raumfahrt, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "driver [1].", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [2] if the existence of an accurate generative model is available, and Markov Decision Processes (MDP) have been used offline to address the intersection problem [3], [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [2] if the existence of an accurate generative model is available, and Markov Decision Processes (MDP) have been used offline to address the intersection problem [3], [4].", "startOffset": 277, "endOffset": 280}, {"referenceID": 3, "context": "Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [2] if the existence of an accurate generative model is available, and Markov Decision Processes (MDP) have been used offline to address the intersection problem [3], [4].", "startOffset": 282, "endOffset": 285}, {"referenceID": 4, "context": "Additionally, machine learning has been used to optimize comfort from a set of safe trajectories [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "In the absence of huge datasets, training on multiple related tasks can give similar improvement gains [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "A large breadth of research has investigated transferring knowledge from one system to another both in machine learning in general [7], and reinforcement learning specifically [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "A large breadth of research has investigated transferring knowledge from one system to another both in machine learning in general [7], and reinforcement learning specifically [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "The training time and sample complexity of deep networks make transfer methods particularly appealing [9], and has prompted in depth investigation to help understand its behavior [10].", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "The training time and sample complexity of deep networks make transfer methods particularly appealing [9], and has prompted in depth investigation to help understand its behavior [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "Recent work in deep reinforcement learning has looked at combining networks from different tasks to share information [11], [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Recent work in deep reinforcement learning has looked at combining networks from different tasks to share information [11], [12].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "And efforts have been made to enable a unified framework for learning multiple tasks through changes in architecture design [13] and modified objective functions [14] to address known problems like catastrophic forgetting [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "And efforts have been made to enable a unified framework for learning multiple tasks through changes in architecture design [13] and modified objective functions [14] to address known problems like catastrophic forgetting [15].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "And efforts have been made to enable a unified framework for learning multiple tasks through changes in architecture design [13] and modified objective functions [14] to address known problems like catastrophic forgetting [15].", "startOffset": 222, "endOffset": 226}, {"referenceID": 15, "context": "In order to optimize the expected return we use Q-learning [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "In Deep Q-learning [17], the optimal value function is approximated with a neural network Q\u2217(s,a) \u2248 Q(s,a;\u03b8).", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "One way to make learning more efficient is to use n-step return[18] E[Rt |st = s,a]\u2248 rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7+ \u03b3rt+n\u22121 + \u03b3n maxat+n Q(st+n,at+n;\u03b8).", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "It was recently shown that allowing an agent to select actions over extended time periods improves the learning time of an agent [19].", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "can viewed as a simplified version of options [20] which is recently starting to be explored by the Deep RL community.", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "[21], [22], [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[21], [22], [23].", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "[21], [22], [23].", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "All layers use leaky ReLU activation functions [24].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "The network is optimized using the RMSProp algorithm [25].", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "This allows us to train directly on the n-step return and forgo the added complexity of using target networks [26].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "Ideally knowledge sharing can be extended to involve a system that continues to learn after it has been deployed [27] and can enable a system to accurately predict appropriate behavior in novel situations [28].", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "Ideally knowledge sharing can be extended to involve a system that continues to learn after it has been deployed [27] and can enable a system to accurately predict appropriate behavior in novel situations [28].", "startOffset": 205, "endOffset": 209}, {"referenceID": 7, "context": "Fine tuning demonstrates the jumpstart and asymptotic performance as described by Taylor and Stone[8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 28, "context": "It is known that neural networks often forget earlier tasks in what is called catastrophic forgetting [29], [30], [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "It is known that neural networks often forget earlier tasks in what is called catastrophic forgetting [29], [30], [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "It is known that neural networks often forget earlier tasks in what is called catastrophic forgetting [29], [30], [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "Lifelong learning is the process of learning multiple tasks sequentially where the goal is to optimize the performance on every task [27], [31].", "startOffset": 133, "endOffset": 137}, {"referenceID": 30, "context": "Lifelong learning is the process of learning multiple tasks sequentially where the goal is to optimize the performance on every task [27], [31].", "startOffset": 139, "endOffset": 143}, {"referenceID": 31, "context": "investigated the effects of ordering [32], [33].", "startOffset": 37, "endOffset": 41}, {"referenceID": 32, "context": "investigated the effects of ordering [32], [33].", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "Experiments were run using the Sumo simulator [34], which is an open source traffic simulation package.", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "simulated by varying the speed distribution of the vehicles and by using parameters that control driver imperfection (based on the Krauss stochastic driving model [35]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 7, "context": "We see that in nearly all cases, pre-training with a different network gives a significant advantage in jumpstart [8] and in several cases there is an asymptotic benefit as well.", "startOffset": 114, "endOffset": 117}], "year": 2017, "abstractText": "We analyze how the knowledge to autonomously handle one type of intersection, represented as a Deep QNetwork, translates to other types of intersections (tasks). We view intersection handling as a deep reinforcement learning problem, which approximates the state action Q function as a deep neural network. Using a traffic simulator, we show that directly copying a network trained for one type of intersection to another type of intersection decreases the success rate. We also show that when a network that is pre-trained on Task A and then is fine-tuned on a Task B, the resulting network not only performs better on the Task B than an network exclusively trained on Task A, but also retained knowledge on the Task A. Finally, we examine a lifelong learning setting, where we train a single network on five different types of intersections sequentially and show that the resulting network exhibited catastrophic forgetting of knowledge on previous tasks. This result suggests a need for a long-term memory component to preserve knowledge.", "creator": "LaTeX with hyperref package"}}}