{"id": "1705.01206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Local Shrunk Discriminant Analysis (LSDA)", "abstract": "Dimensionality reduction is a crucial step for pattern recognition and data mining tasks to overcome the curse of dimensionality. Principal component analysis (PCA) is a traditional technique for unsupervised dimensionality reduction, which is often employed to seek a projection to best represent the data in a least-squares sense, but if the original data is nonlinear structure, the performance of PCA will quickly drop. An supervised dimensionality reduction algorithm called Linear discriminant analysis (LDA) seeks for an embedding transformation, which can work well with Gaussian distribution data or single-modal data, but for non-Gaussian distribution data or multimodal data, it gives undesired results. What is worse, the dimension of LDA cannot be more than the number of classes. In order to solve these issues, Local shrunk discriminant analysis (LSDA) is proposed in this work to process the non-Gaussian distribution data or multimodal data, which not only incorporate both the linear and nonlinear structures of original data, but also learn the pattern shrinking to make the data more flexible to fit the manifold structure. Further, LSDA has more strong generalization performance, whose objective function will become local LDA and traditional LDA when different extreme parameters are utilized respectively. What is more, a new efficient optimization algorithm is introduced to solve the non-convex objective function with low computational cost. Compared with other related approaches, such as PCA, LDA and local LDA, the proposed method can derive a subspace which is more suitable for non-Gaussian distribution and real data. Promising experimental results on different kinds of data sets demonstrate the effectiveness of the proposed approach.", "histories": [["v1", "Wed, 3 May 2017 00:20:31 GMT  (2731kb,D)", "http://arxiv.org/abs/1705.01206v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zan gao", "guotai zhang", "feiping nie", "hua zhang"], "accepted": false, "id": "1705.01206"}, "pdf": {"name": "1705.01206.pdf", "metadata": {"source": "CRF", "title": "Local Shrunk Discriminant Analysis (LSDA)", "authors": ["Zan Gao", "Guotai Zhang", "Feiping Nie", "Hua Zhang"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "II. RELATED WORK", "text": "A. Linear Discriminant AnalysisBecause the objective function of LDA is to make the distance between the different categories as close as possible and the same categories as possible. Thus, we assume that there are n samples for dimensionality reduction and an r \u2212 dimensional characteristic vector is used to represent each sample, i.e. that the individual samples are as close as possible,..., xn} where the samples for i = 1, 2,... n, and d is the dimension of learning in subspace. The goal of learning in subspace is to find an optimized transformation matrix W, Rr \u00d7 d, and each sample is then projected into a low-dimensional subspace."}, {"heading": "III. LOCAL SHRUNK DISCRIMINANT ANALYSIS", "text": "In fact, most people who are in a position to put themselves into the world, to put themselves into the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "IV. EXPERIMENTS AND DISCUSSION", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "V. CONCLUSION", "text": "In this paper, we have proposed a novel and universal, unsupervised and supervised learning dimensionality reduction method, based essentially on pattern shrinkage techniques; the main idea of our method is to simultaneously learn to shrink the pattern and the projector matrix, and to make the data more flexible to adapt the diverse structure that is more convenient for non-Gaussian distribution data and real data; the advantage of our method is threefold: first, our method can reduce the detection of the manifold structure; in particular, the shrinking Fig. 26: Performance analysis on the YaleB database when Gamm = 0, Gamma = INF and optimized Gamma are used appropriately, and from left to right, it respects unsupervised and supervised learning algorithms.Figure 27: Performance analysis on the Coil 100 database when Gamm = 0, Gamma = INF and optimized Gamma are applied to the Gamma format, and the Gamma algorithms are optimized to the left and right respectively."}], "references": [{"title": "Dimensionality reduction:A comparative review", "author": ["L.J.P.V.-D. Maaten", "E.O. Postma", "H.J. vandenHerik"], "venue": "Tech. rep.,Tilburg University,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Local linear transformation embedding", "author": ["C. Hou", "J. Wang", "Y. Wu", "D. Yi"], "venue": "Neurocomputing, vol. 72, no. 10, pp. 2368-2378", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "and F", "author": ["C. Hou", "C. Zhang", "Y. Wu"], "venue": "Nie, Multiple view semi-supervised dimensionality reduction,Pattern Recogn., vol. 43, no. 3, pp. 720-730", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Local coordinates alignment with global preservation for dimensionality reduction", "author": ["J. Chen", "Z. Ma", "Y. Liu"], "venue": "IEEE Trans. Neural Netw. Learn. Syst.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Feature learning for image classification via multiobjective genetic programming", "author": ["L. Shao", "L. Liu", "X. Li"], "venue": "IEEE Trans. Neural Netw. Learn. Syst.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "and Y", "author": ["L. Wang", "S. Chen"], "venue": "Wang, A unified algorithm for mixed L2,pminimizations and its application in feature selection, Comput. Optim.Appl., vol. 58, no. 2, pp. 409-421", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminant analysis for unsupervised feature selection", "author": ["J. Tang", "X. Hu", "H. Gao", "H. Liu"], "venue": "in Proc. 2014 SIAM Int. Conf. Data Min.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Trace ratio criterion for feature selection", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan"], "venue": "in Proc. 23rd AAAI Conf. Artif. Intell.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Eigenfaces vs", "author": ["P.N. Belhumeur", "J.P. Hepanha", "D.J. Kriegman"], "venue": "fisherfaces: recognition using class specific linear projection, IEEE. Trans. Pattern Analysis and Machine Intelligence, vol. 19, no. 7", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Face Recognition Using Kernel Based Fisher Discriminant Analysis", "author": ["Q. Liu", "R. Huang", "H. Lu", "S. Ma"], "venue": "Fifth Intl Conf. Automatic Face and Gesture Recognition", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "In D", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "Koller, Y. Bengio, D. Schuurmans and L. Bottou (Eds.), DiscLDA: Discriminative learning for dimensionality reduction and classification., Advances in Neural Information Processing Systems (NIPS), 21", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Locality Preserving Projections", "author": ["X. He", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems 16, Vancouver, British Columbia, Canada", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised Dimensionality Reduction via Harmonic Functions", "author": ["C. Hou", "F.Nie", "Y. Wu"], "venue": "Proceedingsof 8th International Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of Cognitive Neuroscience", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Neighborhood preserving embedding", "author": ["Xiaofei He", "Deng Cai", "Shuicheng Yan", "Hong-Jiang Zhang"], "venue": "In Proceedings of the 10th IEEE International Conference on Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Face recognition using laplacianfaces", "author": ["X. He", "S. Yan", "Y. Hu", "P. Niyogi", "H.-J. Zhang"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Effective Discriminative Feature Selection with Non-trivial Solutions", "author": ["H. Tao", "C. Hou", "F. Nie", "Y.Jiao", "D. Yi"], "venue": "IEEE TNNLS, eprint arXiv:1504.05408,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Quimet,M.: Out-of-sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering", "author": ["Y. Bengio", "J. Paiement", "P. Vincent", "O. Dellallaeu", "Roux", "N.L"], "venue": "Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Learning a subspace for clustering via pattern shrinking", "author": ["C. Hou", "F. Nie", "Y. Jiao", "C. Zhang", "Y. Wu"], "venue": "Inf. Process. Manage", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "A Convex Formulation for Spectral Shrunk Clustering", "author": ["X.-J. Chang", "F.-P. Nie", "Z.-G. Ma", "Y. Yang", "X.-F. Zhou"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Convergence theorems for generalized alternating minimization procedures", "author": ["A. Gunawardana", "W. Byrne"], "venue": "The Journal of Machine Learning Research, vol. 6, pp. 2049-2073", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Clustering and projected clustering with adaptive neighbors,In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "author": ["F. Nie", "X. Wang", "H. Huang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "A new simplex sparse learning model to measure data similarity for clustering", "author": ["J. Huang", "F. Nie", "H. Huang"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Unsupervised Large Graph Embedding", "author": ["Feiping Nie", "Wei Zhu", "Xuelong Li"], "venue": "The 31st AAAI Conference on Artificial Intelligence (AAAI), San Francisco,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}, {"title": "Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis", "author": ["Masashi Sugiyama", "Dimensionality"], "venue": "Journal of Machine Learning", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Subspace Clustering via New Low-Rank Model with Discrete Group Structure Constraint", "author": ["Feiping Nie", "Heng Huang"], "venue": "The 25th International Joint Conference on Artificial Intelligence (IJCAI),New York,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "A New Sparse Subspace Clustering Algorithm for Hyperspectral Remote Sensing Imagery,IEEE", "author": ["Han Zhai", "Hongyan Zhang", "Liangpei Zhang"], "venue": "GEOSCIENCE AND REMOTE SENSING LETTERS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Thus, dimensionality reduction is a hot and classical topic [1], which attempts to overcome the curse of the dimensionality and to extract relevant features [2], [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Thus, dimensionality reduction is a hot and classical topic [1], which attempts to overcome the curse of the dimensionality and to extract relevant features [2], [3].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "Thus, dimensionality reduction is a hot and classical topic [1], which attempts to overcome the curse of the dimensionality and to extract relevant features [2], [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 3, "context": "com) dimensionality is usually very low [4].", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 165, "endOffset": 169}, {"referenceID": 10, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 171, "endOffset": 175}, {"referenceID": 11, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 177, "endOffset": 181}, {"referenceID": 12, "context": "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 187, "endOffset": 191}, {"referenceID": 15, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 197, "endOffset": 201}, {"referenceID": 17, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 237, "endOffset": 241}, {"referenceID": 17, "context": "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.", "startOffset": 243, "endOffset": 247}, {"referenceID": 18, "context": "The difference between supervised and unsupervised dimensionality reduction algorithms lies in whether the ground truth is utilized or not in learning the transformation matrix [23].", "startOffset": 177, "endOffset": 181}, {"referenceID": 14, "context": "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "[16], [17] proposed Principal Component Analysis which is the most frequently used dimensionality reduction method.", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "[14], [20] proposed Locality Preserving Projections (LPP) in 2003 and Neighborhood Preserving Embedding (NPE) in 2005.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[14], [20] proposed Locality Preserving Projections (LPP) in 2003 and Neighborhood Preserving Embedding (NPE) in 2005.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "However, the motivations between PCA, LPP, NPE and LLE, Isomap and Laplacian Eigenmaps, are very different, and the original LLE, Isomap and Laplcacian Eigenmaps cannot deal with the out-of-sample problem directly [24], that is to say, they only can deal with the training samples, and obtain the low dimension embedding, but for test samples, they cannot directly calculated, analytically or cannot calculated at all.", "startOffset": 214, "endOffset": 218}, {"referenceID": 20, "context": "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].", "startOffset": 318, "endOffset": 322}, {"referenceID": 27, "context": "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].", "startOffset": 324, "endOffset": 328}, {"referenceID": 28, "context": "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].", "startOffset": 330, "endOffset": 334}, {"referenceID": 23, "context": "The similarity matrix [28], [29] A \u2208 Rn\u00d7n is composed of all the aij , which is utilized to characterize the manifold structure of original data points, i.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "The similarity matrix [28], [29] A \u2208 Rn\u00d7n is composed of all the aij , which is utilized to characterize the manifold structure of original data points, i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "shrinking [25], [26] may be helpful for us to find a suitable transformation matrix, thus, the pattern shrinking is employed in our model.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "shrinking [25], [26] may be helpful for us to find a suitable transformation matrix, thus, the pattern shrinking is employed in our model.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "In addition, in subspace learning via pattern shrinking, it cannot directly deal with the out-of-the-sample problem [24], where only the low dimensional embedding", "startOffset": 116, "endOffset": 120}, {"referenceID": 26, "context": "In fact, from the objective function, we can know that it is another local LDA [31].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "The pixel values are then scaled to [0,1] (divided by 256).", "startOffset": 36, "endOffset": 41}, {"referenceID": 22, "context": "As for LSDA, the change of sigma and gamma parameters will affect the performance of LSDA, thus, the alternating method is utilized [27], where at each iteration, we first fix the one variable, and then optimize", "startOffset": 132, "endOffset": 136}], "year": 2017, "abstractText": "Dimensionality reduction is a crucial step for pattern recognition and data mining tasks to overcome the curse of dimensionality. Principal component analysis (PCA) is a traditional technique for unsupervised dimensionality reduction, which is often employed to seek a projection to best represent the data in a least-squares sense, but if the original data is nonlinear structure, the performance of PCA will quickly drop. An supervised dimensionality reduction algorithm called Linear discriminant analysis (LDA) seeks for an embedding transformation, which can work well with Gaussian distribution data or single-modal data, but for non-Gaussian distribution data or multimodal data, it gives undesired results. What is worse, the dimension of LDA cannot be more than the number of classes. In order to solve these issues, Local shrunk discriminant analysis (LSDA) is proposed in this work to process the nonGaussian distribution data or multimodal data, which not only incorporate both the linear and nonlinear structures of original data, but also learn the pattern shrinking to make the data more flexible to fit the manifold structure. Further, LSDA has more strong generalization performance, whose objective function will become local LDA and traditional LDA when different extreme parameters are utilized respectively. What is more, a new efficient optimization algorithm is introduced to solve the non-convex objective function with low computational cost. Compared with other related approaches, such as PCA, LDA and local LDA, the proposed method can derive a subspace which is more suitable for non-Gaussian distribution and real data. Promising experimental results on different kinds of data sets demonstrate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}