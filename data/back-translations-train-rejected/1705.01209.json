{"id": "1705.01209", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Lifelong Metric Learning", "abstract": "The state-of-the-art online learning approaches is only capable of learning the metric for predefined tasks. In this paper, we consider lifelong learning problem to mimic \"human learning\", i.e., endow a new capability to the learned metric for a new task from new online samples and incorporating previous experiences and knowledge. Therefore, we propose a new framework: lifelong metric learning (LML), which only utilizes the data of the new task to train the metric model while preserving the original capabilities. More specifically, the proposed LML maintains a common subspace for all learned metrics, named lifelong dictionary, transfers knowledge from the common subspace to each new metric task with task-specific idiosyncrasy, and redefines the common subspace over time to maximize performance across all metric tasks. We apply online Passive Aggressive optimization to solve the proposed LML framework. Finally, we evaluate our approach by analyzing several multi-task metric learning datasets. Extensive experimental results demonstrate effectiveness and efficiency of the proposed framework.", "histories": [["v1", "Wed, 3 May 2017 00:31:55 GMT  (926kb,D)", "https://arxiv.org/abs/1705.01209v1", "7 pages, 3 figures"], ["v2", "Mon, 12 Jun 2017 15:09:20 GMT  (600kb,D)", "http://arxiv.org/abs/1705.01209v2", "10 pages, 6 figures"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["gan sun", "yang cong", "ji liu", "xiaowei xu"], "accepted": false, "id": "1705.01209"}, "pdf": {"name": "1705.01209.pdf", "metadata": {"source": "CRF", "title": "Lifelong Metric Learning", "authors": ["Gan Sun", "Yang Cong"], "emails": ["sungan@sia.cn", "congyang81@gmail.com", "jliu@cs.rochester.edu", "xwxu@ualr.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to go to another world, to go to another world, to find themselves in another world, to find themselves in another world."}, {"heading": "II. RELATED WORKS", "text": "Metric learning and its associated methods have a long history. Depending on whether metric learning involves multi-task learning, metric learning can be roughly categorized as: Single Metric Learning and Multi-Task Metric Learning."}, {"heading": "A. Single Metric Learning", "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world, \"he said in an interview with the\" New York Times. \""}, {"heading": "B. Multi-task Metric Learning", "text": "Based on the assumption that the relationships and information shared between the various tasks can be taken into account, multi-task learning [29], [30], [32], [33], [35], [36] is aimed at improving generalization performance by learning several related tasks at the same time. Furthermore, there are few multi-task metric learning methods designed to allow metric learning to benefit from training all tasks at the same time. Assuming that several tasks have a3 Mahalanobis metrics in common and each task has a task-specific metric, mtLMNN [11] adopts the LMNN formulation to the multi-task formulation. mtLMNN, however, is more complicated computationally, especially in the case of large dimensions. Specifically, + 1) d2 (t + 1) and d designate the number of tasks and the data dimension as standards to be optimized."}, {"heading": "III. LIFELONG METRIC LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Preliminaries", "text": "Suppose that there are m related tasks. (Xt, Yt) denotes the training pairs of the t-th task with {xti, Rd, i = 1,., nt}, where d and nt are the dimension and number of training samples of the t-th task. Let's define n = x m = 1 nt to be the total number of samples, m is the total number of tasks, and ft: Rd, Rd, Rd, Rd, R to be the similarity / distance metric of the t-th learning problem. ft is defined on the basis of a linear transformation, Lt: Rd, Rd, Rd to get a low dimensional representation, as: \u2022 similarity function: fLt (xti, xtj) = x T, T, T, Ltxtj =: 4, ij, Txt,."}, {"heading": "B. The Lifelong Metric Learning Problem", "text": "The original intention of multi-task metric learning is to learn an appropriate metric distance ft for the T-th task, using all the page information from the common learning set (X1, Y1), (X2, Y2),... Suppose that the loss of the T-th task is determined by the distance function ft (with metric Lt) and the pairs appearing in St and Dt: \"t (L T t) =\" t (ft, ij (L T t)))), as well as by (i, j) and (3), where \"t is an arbitrary loss function of the T-th task. However, learning new metric tasks without access to previously used training data is not considered by traditional metric multi-task learning. In the context of multi-task metric learning, a lifelong metric learning system can respond to a set of metric learning tasks' 1, '2,"}, {"heading": "C. Lifelong Metric Learning Framework (LML)", "text": "In order to model the correlation between the different Ptxti models, we must assume that the metric matrix ft for t-th task (1) is able to form a combination of the individual areas. (1) The fLt (xti, xtj) represents the similarity between the individual areas. (2) For each fLt (d) there is a low dimensional subspace size that can be captured by the orthonormal base. (2) For each fLt (d) there is a low dimensional subspace size spanned by the orthonormal base. (3) The pt1, ptd) is defined with the metric matrix. (3) Rt (xtj) fRt (xtj) = fRt (x) ti (x) ti, x) tj (4)."}, {"heading": "IV. MODEL OPTIMIZATION", "text": "This section discusses the detailed approach to optimizing our proposed LML strategy. (6) Since the problem in Eq (6) is not expressed in terms of L0 and Wt (1), the objective function can lead to a local optimum. (6) However, this approach is inefficient and inapplicable to lifelong learning with many tasks and data samples. (8) This is because the problem in Eq (6) needs to be optimized to recalculate the value of each Wt task (which becomes a time consuming task if we increase the number of tasks learned). (4) To solve this problem, we need to recalculate the value of each Wt task."}, {"heading": "A. Lifelong Dictionary L0 Initialization", "text": "A high-quality lifelong dictionary plays an important role in our model. To create a set of discriminatory base vectors in L0, we first divide the data into different clusters. For each confusion, we select J closest neighbors from each class (for J = | 10, 20, 50 | to count for different scales) and apply Fisher5 algorithm 1 Proximal method for solving Wt tasks. Input: W 0t, V0-Rd \u00d7 d,..., MAX-ITER do 3: Vi \u2212 1, and MAX-ITER output: Wt1: Initialize W 1t = W 0 t, t \u2212 1 = 0, t0 = 2: for i = 1,..., Metasch-ITER do 3: Vi \u2212 1 = W i + \u03b1i (W i \u2212 1t); 4: while true do 5: Compute Vi via Eq. (12); 6: Update via backrule."}, {"heading": "B. Solve Wt with Given L0", "text": "With the initialized lifelong dictionary L0, Wt is the variable in this subproblem. Furthermore, the optimization function can be rewritten as follows: min Wt f (Wt) + g (Wt), (11) where f (Wt) = 12x LT0 WtL0 \u2212 M \u0432 t-2F and g (Wt) = 0-1. Due to the non-smooth nature of g (Wt), we suggest the Proximal Gradient Method (FISTA) [40] with a fast global convergence rate to solve this optimization problem. In particular, the Proximal Operator of the \"1, off standard\" can be used to solve this subproblem: W i + 1t = argmin W12, W \u2212 W es + Proximal f (W it), f (W it)."}, {"heading": "C. Solve L0 with Given Tt\u2019s and Wt\u2019s", "text": "To evaluate the lifelong dictionary L0, we modify the formulation in Equation (7) to remove the minimization from Wt. In addition, we also remove the second term used to keep the new similarity / distance matrix close to the current one. Furthermore, we achieve this by using both the side information Tt (generated according to the basic metric model used) and Wt in the learned tasks. In the following, we try to apply the method of gradient descent to solve L0 in Equation (7).The gradient of 't in relation to L0 is: 1m m intuitive t = 1 \u2202't (ft, ij (L T 0 WtL0) \u2202 L0 + \u2202 (\u03b3 L0-2F) \u2202 L0, = 1m m intuitively t = 1 (WTt L04t) + \u03b3L0, (14) where 4t can be calculated using various SingleTaskner Learner functions using page information."}, {"heading": "D. Computational Complexity", "text": "For the complexity of our proposed algorithm, the main cost of calculation in each update of algorithm 2 is associated with two partial problems: One optimization problem is in Eq. (11), another is Eq. (14).1) For the problem in Eq. (11), each update of the LML system begins with the base of metric learning models for calculating Mt, we assume that this step has complexity O (Eq. (d, n)), where d \u00b2 is the number of characteristics, n = x m = 1 nt and ntis the triplet number of Tt in our work. Next, we must resolve the instance of the lasso, i.e., | W \u00b2 1, d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d \u00b2 d) d \u00b2 d \u00b2 d \u00b2 d) d)."}, {"heading": "E. Discussion", "text": "In this section, we will briefly discuss a learning method that is most closely related to our proposed learning algorithm. Perhaps the most relevant work for us in the context of multi-task metric learning is from [37], which defines metric learning as learning a sparse combination of locally discriminatory metrics generated from training data by means of clusters. However, the motivation for SCML and our LML differ significantly: \u2022 SCML aims to view metric learning as learning a sparse combination of basic elements taken from a basic set B = {bi} Ki = 1, which are two-dimensional column vectors. Instead of specifying the metric number of tasks, our LML focuses on transferring knowledge from precisely learned tasks to the new metric task using the common base, i.e. lifelong task learning. \u2022 In SCML, the metric matrix is represented by basic matrices that can only be induced by a standard ML-restriction."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we perform empirical comparisons with modern metric learning models for single and multiple tasks. First, we give the basic metric learning with our lifelong metric learning framework in Table I with two different functions: sMt (xi, xj) = x T i Mtxj and dMt (xi, xj) = (xi \u2212 xj) TMt (xi \u2212 xj), where xi and xj belong to the same class, while xi and xk come from different classes. Afterwards, the experiments are carried out with a series of real data sets."}, {"heading": "A. Comparison Algorithms and Evaluation", "text": "In our experiments, we compare our LML framework with individual metric learning models and multitascular metric learning models. \u2022 The single metric learning model includes: 1) the Euclidean distance (stEuc): the standard Euclidean distance in the attribute space; 2) OASIS (stOASIS) [9]: the classic metric learning model specified in Table I, whose iteration number in our work is 2 x 104; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for the k-next neighbouring classification; 4) the SCML global (stSCML) [37]: which is easy to combine the local basic elements into a higher global metric; 5) the LMNN Union (uLMNN-Union): is the LMNN metric, which is based on the pooling of all the educational data, i.e. \"all\" (\")."}, {"heading": "B. Real Datasets", "text": "In fact, it is such that most of us are in a position to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "C. Evaluating Lifelong Metric learning Framework", "text": "Are you looking at the results of this study? Look at the results of this study."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we examine how to insert metric tasks into the original metric system without retraining the entire system in a way that is too time-consuming, like most state-of-the-art online metric learning models. Specifically, we propose a Lifelong Metric Learning Framework (LML) that learns the \"Lifelong Dictionary\" as a common basis for all metric models, based on the assumption that all metric tasks are kept in a low-dimensional common subspace.9 1 2 3 4 Number of learned tasks 14 16 18 20 22 24 26 28 Er ro (%) Books DVD Electronics Kitchen Avg. ErrorFig. 5. The effect of the number of learned tasks. The horizontal and vertical axes are the number of learned tasks or classification errors of each task. The initial classification error of each task is achieved by means of lengthy metric learning."}], "references": [{"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research, vol. 10, no. Feb, pp. 207\u2013244, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust distance metric learning with auxiliary knowledge.", "author": ["Z.-J. Zha", "T. Mei", "M. Wang", "Z. Wang", "X.-S. Hua"], "venue": "in International Joint Conference on Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Semi-supervised metric learning using pairwise constraints.", "author": ["M.S. Baghshah", "S.B. Shouraki"], "venue": "in International Joint Conference on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Survey on distance metric learning and dimensionality reduction in data mining", "author": ["F. Wang", "J. Sun"], "venue": "Data Mining and Knowledge Discovery, vol. 29, no. 2, pp. 534\u2013564, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Informationtheoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 209\u2013216.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "CVPR. IEEE, 2009, pp. 413\u2013420.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Decomposition-based transfer distance metric learning for image classification", "author": ["Y. Luo", "T. Liu", "D. Tao", "C. Xu"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 9, pp. 3789\u20133801, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Advances in neural information processing systems, 2009, pp. 761\u2013768.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "An online algorithm for large scale image similarity learning", "author": ["G. Chechik", "U. Shalit", "V. Sharma", "S. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 306\u2013314.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Self-supervised online metric learning with low rank constraint for scene categorization", "author": ["Y. Cong", "J. Liu", "J. Yuan", "J. Luo"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 8, pp. 3179\u20133191, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Large margin multi-task metric learning", "author": ["S. Parameswaran", "K.Q. Weinberger"], "venue": "Advances in neural information processing systems, 2010, pp. 1867\u20131875.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task low-rank metric learning based on common subspace", "author": ["P. Yang", "K. Huang", "C.-L. Liu"], "venue": "International Conference on Neural Information Processing. Springer, 2011, pp. 151\u2013159.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "An overview and empirical comparison of distance metric learning methods", "author": ["P. Moutafis", "M. Leng", "I.A. Kakadiaris"], "venue": "IEEE transactions on cybernetics, vol. 47, no. 3, pp. 612\u2013625, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep multimodal distance metric learning using click constraints for image ranking", "author": ["J. Yu", "X. Yang", "F. Gao", "D. Tao"], "venue": "IEEE transactions on cybernetics, 2017.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2017}, {"title": "Person reidentification by minimum classification error-based kiss metric learning", "author": ["D. Tao", "L. Jin", "Y. Wang", "X. Li"], "venue": "IEEE transactions on Cybernetics, vol. 45, no. 2, pp. 242\u2013252, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Distance-based image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 11, pp. 2624\u20132637, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Information-theoretic semi-supervised metric learning via entropy regularization", "author": ["G. Niu", "B. Dai", "M. Yamada", "M. Sugiyama"], "venue": "Neural computation, vol. 26, no. 8, pp. 1717\u20131762, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval and clustering", "author": ["S.C. Hoi", "W. Liu", "S.-F. Chang"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 6, no. 3, p. 18, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning bregman distance functions and its application for semi-supervised clustering", "author": ["L. Wu", "R. Jin", "S.C. Hoi", "J. Zhu", "N. Yu"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 2089\u2013 2097.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Low-rank similarity metric learning in high dimensions.", "author": ["W. Liu", "C. Mu", "R. Ji", "S. Ma", "J.R. Smith", "S.-F. Chang"], "venue": "Association for the Advancement of Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Robust transfer metric learning for image classification", "author": ["Z. Ding", "Y. Fu"], "venue": "IEEE Transactions on Image Processing, vol. 26, no. 2, pp. 660\u2013670, 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "G.E. Hinton", "S.T. Roweis", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2005, pp. 513\u2013520.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "Advances in Neural Information Processing Systems, vol. 15, no. 505-512, 2002, p. 12.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust structural metric learning.", "author": ["D. Lim", "G.R. Lanckriet", "B. McFee"], "venue": "International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Soml: Sparse online metric learning with application to image retrieval.", "author": ["X. Gao", "S.C. Hoi", "Y. Zhang", "J. Wan", "J. Li"], "venue": "Association for the Advancement of Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Online learning in the embedded manifold of low-rank matrices", "author": ["U. Shalit", "D. Weinshall", "G. Chechik"], "venue": "Journal of Machine Learning Research, vol. 13, no. Feb, pp. 429\u2013458, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 5, no. 4, pp. 287\u2013364, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, no. 1, pp. 41\u201375, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Integrating low-rank and group-sparse structures for robust multi-task learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "KDD, 2011, pp. 42\u201350.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning task grouping and overlap in multitask learning", "author": ["A. Kumar", "H.D. III"], "venue": "International Conference on Machine Learning, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Statistics and Computing, vol. 20, no. 2, pp. 231\u2013252, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, no. 73, pp. 243\u2013272, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "P.D. Ravikumar", "S. Sanghavi", "C. Ruan"], "venue": "Advances in neural information processing systems, 2010, pp. 964\u2013972.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "The benefit of multitask representation learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "Journal of Machine Learning Research, vol. 17, no. 81, pp. 1\u201332, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Latent maxmargin multitask learning with skelets for 3-d action recognition", "author": ["Y. Yang", "C. Deng", "D. Tao", "S. Zhang", "W. Liu", "X. Gao"], "venue": "IEEE transactions on cybernetics, vol. 47, no. 2, pp. 439\u2013448, 2017.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2017}, {"title": "Sparse compositional metric learning", "author": ["Y. Shi", "A. Bellet", "F. Sha"], "venue": "arXiv preprint arXiv:1404.4105, 2014.  10", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Ella: An efficient lifelong learning algorithm.", "author": ["P. Ruvolo", "E. Eaton"], "venue": "International Conference on Machine Learning (1),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 7, no. Mar, pp. 551\u2013585, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "ACL, vol. 7, 2007, pp. 440\u2013447.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.", "startOffset": 188, "endOffset": 191}, {"referenceID": 7, "context": "However, most state-of-the art online metric learning models [8], [9], [10] can only achieve online learning from fixed predefined t (t > 0) metric tasks and cannot add the new", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "However, most state-of-the art online metric learning models [8], [9], [10] can only achieve online learning from fixed predefined t (t > 0) metric tasks and cannot add the new", "startOffset": 66, "endOffset": 69}, {"referenceID": 9, "context": "However, most state-of-the art online metric learning models [8], [9], [10] can only achieve online learning from fixed predefined t (t > 0) metric tasks and cannot add the new", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "To achieve this goal, most state-of-the-arts [11], [12] should storage training data of all tasks and retrain their models in a time consuming way.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "To achieve this goal, most state-of-the-arts [11], [12] should storage training data of all tasks and retrain their models in a time consuming way.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "[13], [14], [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13], [14], [15].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "[13], [14], [15].", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "in the projection space and [1] proposes the most widelyused Mahalanohis distance learning Large Margin Nearest Neighbors (LMNN), i.", "startOffset": 28, "endOffset": 31}, {"referenceID": 22, "context": ", learning a Mahalanobis distance metric for kNN classification for labeled training examples; models based on pairs/triplets, for instance, [23] searches for a clustering that puts the similar pairs into the same clusters", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "and dissimilar pairs into different clusters; [24] promotes input sparsity by imposing a group sparsity penalty on the learned metric and a trace constraint to encourage output sparsity; [20] proposes a novel low-rank metric learning algorithm to yield bilinear similarity functions which can be applicable to highdimensional data domains.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "and dissimilar pairs into different clusters; [24] promotes input sparsity by imposing a group sparsity penalty on the learned metric and a trace constraint to encourage output sparsity; [20] proposes a novel low-rank metric learning algorithm to yield bilinear similarity functions which can be applicable to highdimensional data domains.", "startOffset": 187, "endOffset": 191}, {"referenceID": 8, "context": "For the online metric learning, [9] designs an Online Algorithm for Scalable Image Similarity learning (OASIS), for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "In order to overcome over-fitting problem, OMLLR [10] proposes a novel online metric learning model with the low rank constraint, where low-rank metric enables to reduce storage of metric matrices.", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "[25] incorporates large-scale high-dimensional dataset into sparse online metric learning,", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In addition, LORETA [26] describes an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold.", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "To incorporate the benefits of both online learning and Mahalanobis distance, LEGO [8] using a Log-Det regularization per instance loss, is guaranteed", "startOffset": 83, "endOffset": 86}, {"referenceID": 26, "context": "Furthermore, more details can also be found in two surveys [27] and [28].", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "Furthermore, more details can also be found in two surveys [27] and [28].", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],", "startOffset": 143, "endOffset": 147}, {"referenceID": 29, "context": "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],", "startOffset": 155, "endOffset": 159}, {"referenceID": 31, "context": "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],", "startOffset": 161, "endOffset": 165}, {"referenceID": 32, "context": "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],", "startOffset": 167, "endOffset": 171}, {"referenceID": 33, "context": "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],", "startOffset": 173, "endOffset": 177}, {"referenceID": 34, "context": "[35], [36] aims to improve generalization performance by learning multiple related tasks simultaneously.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[35], [36] aims to improve generalization performance by learning multiple related tasks simultaneously.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "common Mahalanobis metric and each task has a task-specific metric, mtLMNN [11] adopts the LMNN formulation to the multi-task learning.", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "Based on low-rank based assumptions, [12] presents transformation matrix to the problem of multi-task metric learning by learning a common subspace for all tasks and an individual metric for each task, where each individual metric is restricted in the common subspace.", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "In addition, mtSCML [37] constructs a common basis set, multi-metric are regularized to be relevant across tasks (as favored by the group sparsity).", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Moreover, motivated by [12], Theorem 1 gives the detail mathematical description.", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "However, as shown in [38], this approach is inefficient and inapplicable to lifelong learning with many tasks and data samples.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "(6) by applying the online passive aggressive (PA) [39] optimization strategy, i.", "startOffset": 51, "endOffset": 55}, {"referenceID": 39, "context": "Due to the non-smooth nature of g(Wt), we propose the proximal gradient method (FISTA) [40] with a fast global convergence rate to solve this optimization problem.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "(14) OASIS [9] Similarity `t(Mt) = max ( 0, 1\u2212 sMt (xi, xj) + sMt (xi, xk) ) \u2211", "startOffset": 11, "endOffset": 14}, {"referenceID": 36, "context": "SCML [37] Distance `t(Mt) = max ( 0, 1\u2212 dMt (xi, xj) + dMt (xi, xk) ) \u2211", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "In other words, the convergence rate of Algorithm 1 can achieve O(1/T ) as shown in [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 36, "context": "Perhaps the most relevant work to ours in the context of multi-task metric learning is from [37], which frames metric learning as learning a sparse combination of locally discriminative metrics that are generated from the training data via clustering.", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "The single metric learning model includes: 1) Euclidean distance (stEuc): the standard Euclidean distance in feature space; 2) OASIS (stOASIS) [9]: the classical online metric learning model which is given in Table I, and its iteration number is 2\u00d710 in our paper; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for k-nearest neighbor classification; 4) SCML-global (stSCML) [37]: which is simply to combine the local basis elements into a higher-rank global metric; 5) LMNN-union (uLMNN): is the LMNN metric obtained on the union of the training data of all tasks (i.", "startOffset": 143, "endOffset": 146}, {"referenceID": 0, "context": "The single metric learning model includes: 1) Euclidean distance (stEuc): the standard Euclidean distance in feature space; 2) OASIS (stOASIS) [9]: the classical online metric learning model which is given in Table I, and its iteration number is 2\u00d710 in our paper; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for k-nearest neighbor classification; 4) SCML-global (stSCML) [37]: which is simply to combine the local basis elements into a higher-rank global metric; 5) LMNN-union (uLMNN): is the LMNN metric obtained on the union of the training data of all tasks (i.", "startOffset": 282, "endOffset": 285}, {"referenceID": 36, "context": "The single metric learning model includes: 1) Euclidean distance (stEuc): the standard Euclidean distance in feature space; 2) OASIS (stOASIS) [9]: the classical online metric learning model which is given in Table I, and its iteration number is 2\u00d710 in our paper; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for k-nearest neighbor classification; 4) SCML-global (stSCML) [37]: which is simply to combine the local basis elements into a higher-rank global metric; 5) LMNN-union (uLMNN): is the LMNN metric obtained on the union of the training data of all tasks (i.", "startOffset": 432, "endOffset": 436}, {"referenceID": 10, "context": "\u2022 multi-task LMNN (mtLMNN) [11]: common metric defined by M0 picks up general trends across multiple datasets and Mt specializes the metric further for each particular task.", "startOffset": 27, "endOffset": 31}, {"referenceID": 36, "context": "\u2022 multi-task SCML (mtSCML) [37]: this multi-task metric learning model considers that all learned metrics can be expressed as combinations of the same basis subset B, though with different weights for each task.", "startOffset": 27, "endOffset": 31}, {"referenceID": 37, "context": "\u2022 Lifelong multi-task (ELLA) [38]: whose formulation is realized by the following objective function:", "startOffset": 29, "endOffset": 33}, {"referenceID": 40, "context": "As shown in Table IV, Sentiment [41] consists of Amazon reviews on four product types (kitchen appliances, DVDs, books and electronics).", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": ", \u03b3 \u2016L0\u20162F , and employ FISTA [40] to efficiently optimize such a convex problem.", "startOffset": 30, "endOffset": 34}], "year": 2017, "abstractText": "The state-of-the-art online learning approaches are only capable of learning the metric for predefined tasks. In this paper, we consider lifelong learning problem to mimic \u201chuman learning\u201d, i.e., endowing a new capability to the learned metric for a new task from new online samples and incorporating previous experiences and knowledge. Therefore, we propose a new metric learning framework: lifelong metric learning (LML), which only utilizes the data of the new task to train the metric model while preserving the original capabilities. More specifically, the proposed LML maintains a common subspace for all learned metrics, named lifelong dictionary, transfers knowledge from the common subspace to each new metric task with task-specific idiosyncrasy, and redefines the common subspace over time to maximize performance across all metric tasks. For model optimization, we apply online passive aggressive optimization algorithm to solve the proposed LML framework, where the lifelong dictionary and task-specific partition are optimized alternatively and consecutively. Finally, we evaluate our approach by analyzing several multi-task metric learning datasets. Extensive experimental results demonstrate effectiveness and efficiency of the proposed framework.", "creator": "LaTeX with hyperref package"}}}