{"id": "1705.01253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "The Forgettable-Watcher Model for Video Question Answering", "abstract": "A number of visual question answering approaches have been proposed recently, aiming at understanding the visual scenes by answering the natural language questions. While the image question answering has drawn significant attention, video question answering is largely unexplored.", "histories": [["v1", "Wed, 3 May 2017 04:46:33 GMT  (3407kb,D)", "http://arxiv.org/abs/1705.01253v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["hongyang xue", "zhou zhao", "deng cai"], "accepted": false, "id": "1705.01253"}, "pdf": {"name": "1705.01253.pdf", "metadata": {"source": "CRF", "title": "The Forgettable-Watcher Model for Video Question Answering", "authors": ["Hongyang Xue", "Zhou Zhao", "Deng Cai"], "emails": ["hyxue@outlook.com,", "zhaozhou@zju.edu.cn,", "dengcai@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a way in which people are able to put themselves at the centre, without having to put themselves at the centre."}, {"heading": "2 Dataset", "text": "The TGIF dataset [Li et al., 2016] is collected by Li et al. from Tumblr. GIFs are for a short time almost identical to small video clips. Li et al. have cleaned up the original data and excluded the GIFs with catoon, static and textual content. Later, the animated GIFs were commented on via crowdsourcing service. The TGIF dataset contains 102,068 GIFs and a total of 287,933 descriptions, with each GIF corresponding to several descriptions. Each description consists of one or two sets."}, {"heading": "2.1 TGIF-QA", "text": "To generate the question-and-answer pairs from the descriptions, we use the modern question-generation approach [Heilman and Smith, 2010]. We focus on the questions of the types What / When / Who / Whose / Where and How many. Our question-and-answer task is multiplexed and the generated data contains only question-and-truth-answer pairs. Therefore, we have to generate false alternatives for each question. We provide each question with 8 candidate answers. We describe how we generate the alternative answers for each type of question in the following subsections."}, {"heading": "How Many", "text": "The question How many questions refers to counting some objects in the video. To generate reasonable alternatives, we first collect all the questions as many questions in our data set and collect the numbers in their answers. All Arabic numbers are converted to English word representations. After eliminating the low frequency numbers, we find that most answers contain numbers from one to eight. We discard the questions whose answers exceed eight, and replace the basic truth numbers with [one, two, three, four, five, six, seven, eight] to generate the answers of the 8 candidates. A typical example is shown in the upper right corner of Figure 1."}, {"heading": "Who", "text": "We collect the words in the answers from all the who questions, then we filter the words to get all the nouns, then we filter out all the abstract and low frequency nouns to form an entity list, and the entity word in the basic truth answers is selected and replaced by random samples from the entity list to generate 8 alternatives."}, {"heading": "Whose", "text": "Whose questions relate to the facts about belonging. There are two ways to represent belonging: one is by the possessive pronoun such as \"mine,\" \"yours,\" \"being,\" etc.; the other is to use the possessive case of nouns such as \"man,\" \"girl,\" \"cats,\" etc. For the first type of possessive pronoun, we replace the pronouns with random samples from the possessive pronoun list to generate the alternatives; for the second, we replace the nouns as well as the who questions."}, {"heading": "Other Questions", "text": "For the other types of questions, we simply replace the nouns in the answer phrases to select candidates."}, {"heading": "2.2 Dataset Split", "text": "At the end, our data set contains 117,854 videos and 287,763 question-candidate pairs. We split our TGIFQA data set into three parts for training, validation, and test. The training data set contains 230,689 question-answer pairs from 94,425 videos. The validation data set contains 24,696 pairs from 10,096 videos, and the test record 32,378 pairs from 13,333 videos."}, {"heading": "3 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Task description", "text": "Multiple choice video QA consists in selecting an answer to a given question q, video information v and candidate selection (alternatives) a = {a1, \u00b7 \u00b7, a8}. A video is a sequence of picture frames v = {f1, f2, \u00b7 \u00b7 \u00b7}. A question is a sequence of natural words q = {q1, q2, \u00b7 \u00b7 \u00b7}. Each alternative of candidate responses is also a sequence of natural words ai = {a1i, a2i, \u00b7 \u00b7 \u00b7}. We formulate the VideoQA task as selecting the best answer among the alternatives. In the other word, we define a loss function L (v, q, ai). We consider the QA problem as a classification problem and the best answer is selected when it achieves minimal losses a = argmaxai L (v, q, ai)."}, {"heading": "3.2 Model", "text": "In order to answer a question, we have to find the most informative frame or combine the information of several frames.In the following, we propose three models: the re-watcher, the re-reader and the forgettable watcher. The re-watcher model meticulously processes the question phrase sequence; the re-reader model makes full use of the information of the video frames along the time dimension; and the forgettable watcher combines both. We call qai the concatenation of question q and answer ai after word embedding; all our models take the question and an alternative as a whole sentence, which we call the QA sentence; the QA sentence and the visual feature sequence are then incorporated into our models to create a score for the alternative answer in the sentence. In the following sections, we will call the QA sentence non-binding."}, {"heading": "Re-Watcher", "text": "Our model first encodes the video features and QA features with two separate bi-directional single layers LSTMs (Graves, 2012). The LSTMs contain an embedding layer that maps the video features and textual features into a common feature space. We call the outputs of the forward and backward LSTMs yf (t) and yb (t). The encoding u of a QA set of length | c | is formed by concatenating the final outputs of the forward and backward LSTMs, u = [yfc (|), ybc (1), ybc (1)].For the video frames that encode output for each image at a time t is yv (t) = [yf (t), yb (t)]]]. The representation r (i) of the videos to a set token i is formed by a weighted sum of these output ectors."}, {"heading": "Re-Reader", "text": "The Re-Watcher model imitates a person who constantly watches the video while reading the QA set. Video functions related to the QA set are cumulated, and the Re-Watcher model is designed from the opposite perspective (see Figure 2. Center).This model imitates a person who cannot remember the whole question well. Every time he looks at a frame, he looks at the whole question retrospectively. We call the encoding output of the QA set in Token i yc (i) = [yfc (i), y b (i)].The display w (t) of the video frames at a given time t is derived from the weighted sum of the QA set encoding outputs and the previous display w (t \u2212 1): m (t, i) = tanh (Wcmyc (i) = tanh (Wcmyc (i) = Wwmw) (value of myww = mw (t) from \u2264 Wv (t) (\u2212 t)."}, {"heading": "Forgettable-Watcher", "text": "We combine the re-watcher and the re-reader models into a forgettable-watcher model (see fig. 2 on the right), which meticulously combines the visual and textual attributes; the entire video is viewed again when a word is read and the whole sentence is read again during viewing; then the presentations are combined to obtain the score: gF = FC (tanh (Wrgr (| c |) + Wwgw (N) + Wcgu))."}, {"heading": "Baseline method", "text": "We also use a simple model (see Figure 3) that is augmented by the VQA model [Antol et al., 2015]. The VQA model is designed to answer questions with only a single image. We extend the model for our task by encoding the video images and the QA sentences directly with two separate bidirectional LSTMs. The final encoding outputs of both bidirectional LSTMs are then combined to generate the score."}, {"heading": "4 Experiments and Results", "text": "We evaluate all methods on our TGIF-QA dataset, which is described in the Dataset section."}, {"heading": "4.1 Data Preprocessing", "text": "We try all the videos to have the same number of images to reduce redundancy. If the video does not have enough images to scan, its last frame is repeated. We extract the visual characteristics of each frame with VGGNet [Simonyan and Zisserman, 2014]. The 4096-dimensional feature vector of the first fully connected layer is taken as a visual feature. Word2Vec [Mikolov et al., 2013], which is trained on GoogleNews, is used to embed each word as a 300-dimensional real vector. Afterwards, we associate each question with its 8 alternatives to generate 8 candidate QA sentences."}, {"heading": "4.2 Implementation Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Optimization", "text": "We train all our models with the Adam Optimizer [Kingma and Ba, 2015] to minimize loss, with an initial learning rate of 0.002, exponential decay rates for the first and second instantaneous estimates set to 0.1 and 0.001, respectively, batch size set to 100, a gradient scheme applied to the gradient standard within 10, and an early stop strategy applied to stop training when validation accuracy no longer improves."}, {"heading": "Model Details", "text": "Visual features and word embedding are encoded to dimensionality 2048 or 1024 by two separate bi-directional LSTMs and then mapped to a common feature space of dimensionality 1024. The Re-Watcher component stores size 512 and outputs the final combined feature of dimensionality 512. Finally, the combined feature is divided into three fully connected size layers (512, 256, 128)."}, {"heading": "Evaluation Metrics", "text": "Since our task is to answer the multiple-choice question, we use classification accuracy to evaluate our models. However, there are some cases where both decisions can answer the question. This motivates us to also use the WUPS measurement [Malinowski and Fritz, 2014] with 0.0 and 0.9 as thresholds such as the open-ended tasks."}, {"heading": "4.3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline method", "text": "The basic method is an extension of the VQA model [Antol et al., 2015] without our verification or re-reading mechanisms. We call it the simple model and its result is presented in the simple section in Table 1. We can see that the simple model performs much worse than the other three models."}, {"heading": "Our methods", "text": "The \"Forgettable Watcher\" model outperforms the other two models because it shares the mechanisms of the \"Re-Watcher\" and the \"Re-Watcher.\" On the other hand, the \"Re-Reader\" model performs worse than the \"Re-Watcher\" model. This implies that the \"Re-Watcher\" mechanism is more important."}, {"heading": "Results on different questions", "text": "The result is in Table 2. All methods perform better than others in the \"where\" and \"when\" questions. This can be attributed to two reasons: First, the \"where\" and \"when\" questions are easier to answer, since these two questions usually concern the large video scene. In most cases, a single frame may be sufficient to answer the questions; second, the candidate alternatives produced in the data set generation may be too simple to distinguish; and the method of data set generation is less effective in creating good alternatives for these two questions, while it can produce high-quality alternatives for the other question types."}, {"heading": "5 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Image-QA", "text": "Answering pictorial visual questions has recently aroused considerable research interest. [Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016] The aim of Image-QA is to answer questions that can only be answered by an image without additional information. Image QA tasks can be categorized into two types depending on the way answers are generated. The first type is answered as an open question [Ren et al., 2015; Malinowski and Fritz, 2014] where answers are given only by the questions. Since the answers generated by the algorithms are usually not the exact words that people expect, measures such as the WUPS 0.9 and WUPS 0.0 based on the Wu-Palmer (WUPS) similarity [Malinowski et al.) similarity [Malinowski and Fritz, 2014 will be used to measure the other type of answer to the Zu]."}, {"heading": "5.2 Question Generation", "text": "Automatic Questioning is an open research topic in natural language processing. It is originally proposed for educational purposes [Gates en, 2008]. In our situation, the questions generated must be as diverse as possible so that they can match the property of questions generated by human commentators. Among the questions, the generation of approaches [Rus and Arthur, 2009; Gates, 2008; Heilman and Smith, 2010] we use the method of Heilman and Smith [Heilman and Smith, 2010] to generate our video QA pairs from video description datasets. Their approach generates questions in open domains. A similar idea was used by RenTable 1: Video QA Results. We evaluate the basic method in the first line (Straightforward). Our proposed three models are reported in the following lines. Accuracy denotes the classification accuracy when the alternatives for each question are considered as classes. WUPS 0.0 and WUPS 0.9 are the UPS WUPS 90.9 and WUPS 90.90.9 respectively."}, {"heading": "5.3 Video-QA", "text": "In addition, previous work usually combines other textual information. Tapaswi et al. [Tapaswi et al., 2016] combine videos with diagrams, subtitles, and scripts to generate answers. Tu et al. [Tu et al., 2014] also combine video and text data to answer questions. Zhu et al. [Zhu et al., 2015] collect a dataset for the type of questions that are \"blank filled in.\" Mazaheri et al. [Mazaheri et al., 2016] also take into account the blank problem of filling in. Compared to ImageQA, video QA is more problematic because of the additional time dimension. Useful information is scattered across different frames. Time coherence needs to be taken into account to better understand the videos. Our proposed method focuses on the multiple-choice type of questions, where the candidates essentially represent phrases of QA answers."}, {"heading": "6 Conclusion and Future Works", "text": "We propose to collect a large video QA dataset by automatically converting it from the video description dataset. To tackle the video QA task, we propose two mechanisms: the re-view and re-reading mechanisms, and then combine them into an effective forgettable watcher model. In the future, we will improve the quality and quantity of our dataset."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433,", "citeRegEx": "Antol et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizwiz: nearly real-time answers to visual questions", "author": ["Bigham et al", "2010] Jeffrey P Bigham", "Chandrika Jayant", "Hanjie Ji", "Greg Little", "Andrew Miller", "Robert C Miller", "Robin Miller", "Aubrey Tatarowicz", "Brandyn White", "Samual White"], "venue": "In Proceedings of the 23nd annual ACM sym-", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "Advances in Neural Information Processing Systems, pages 2296\u20132304,", "citeRegEx": "Gao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatically generating reading comprehension look-back strategy: questions from expository texts", "author": ["Donna M Gates"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Gates. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Proceedings of the National Academy of Sciences", "author": ["Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes. Visual turing test for computer vision systems"], "venue": "112(12):3618\u20133623,", "citeRegEx": "Geman et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 15\u201335", "author": ["Alex Graves. Neural networks. In Supervised Sequence Labelling with Recurrent Neural Networks"], "venue": "Springer,", "citeRegEx": "Graves. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Good question! statistical ranking for question generation", "author": ["Heilman", "Smith", "2010] Michael Heilman", "Noah A Smith"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Tgif: A new dataset and benchmark on animated gif description", "author": ["Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "Li et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ma et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Mateusz Malinowski", "Mario Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input"], "venue": "pages 1682\u20131690,", "citeRegEx": "Malinowski and Fritz. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Video fill in the blank with merging lstms", "author": ["Amir Mazaheri", "Dong Zhang", "Mubarak Shah"], "venue": "arXiv preprint arXiv:1610.04062,", "citeRegEx": "Mazaheri et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "ICLR,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han. Image question answering using convolutional neural network with dynamic parameter prediction"], "venue": "June", "citeRegEx": "Noh et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel. Exploring models", "data for image question answering"], "venue": "pages 2953\u20132961,", "citeRegEx": "Ren et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "National Science Foundation", "author": ["Vasile Rus", "C Graesser Arthur. The question generation shared task", "evaluation challenge. In The University of Memphis"], "venue": "Citeseer,", "citeRegEx": "Rus and Arthur. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "Simonyan and Zisserman. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["Tapaswi et al", "2016] Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "IEEE MultiMedia", "author": ["Kewei Tu", "Meng Meng", "Mun Wai Lee", "Tae Eun Choe", "Song-Chun Zhu. Joint video", "text parsing for understanding events", "answering queries"], "venue": "21(2):42\u201370,", "citeRegEx": "Tu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola. Stacked attention networks for image question answering"], "venue": "June", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Uncovering temporal context for video question and answering", "author": ["Linchao Zhu", "Zhongwen Xu", "Yi Yang", "Alexander G Hauptmann"], "venue": "arXiv preprint arXiv:1511.04670,", "citeRegEx": "Zhu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "Zhu et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "One of the highlevel tasks towards scene understanding is the visual question answering [Antol et al., 2015].", "startOffset": 88, "endOffset": 108}, {"referenceID": 4, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 2, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 19, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 13, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 9, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 0, "context": "In image-based question answering (Visual-QA) [Antol et al., 2015], most current collection methods require humans to generate the question-answer pairs [Antol et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": ", 2015], most current collection methods require humans to generate the question-answer pairs [Antol et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 94, "endOffset": 142}, {"referenceID": 10, "context": ", 2015], most current collection methods require humans to generate the question-answer pairs [Antol et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 94, "endOffset": 142}, {"referenceID": 8, "context": "The TGIF (Tumblr GIF) dataset [Li et al., 2016] is a large-scale video description dataset.", "startOffset": 30, "endOffset": 47}, {"referenceID": 0, "context": "We also extend the VQA model [Antol et al., 2015] as a baseline method.", "startOffset": 29, "endOffset": 49}, {"referenceID": 8, "context": "The TGIF dataset [Li et al., 2016] is collected by Li et al.", "startOffset": 17, "endOffset": 34}, {"referenceID": 5, "context": "Our model first encodes the video features and QA features with two separate bi-directional single layer LSTMs [Graves, 2012].", "startOffset": 111, "endOffset": 125}, {"referenceID": 19, "context": "The representation r(i) of the videos for each QA-sentence token i is formed by a weighted sum of these output vectors (similar to the attention mechanism in Image-QA [Yang et al., 2016]) and the previous representation r(i\u2212 1):", "startOffset": 167, "endOffset": 186}, {"referenceID": 0, "context": "This model is extended from the VQA model [Antol et al., 2015].", "startOffset": 42, "endOffset": 62}, {"referenceID": 16, "context": "We extract the visual features of each frame with VGGNet [Simonyan and Zisserman, 2014].", "startOffset": 57, "endOffset": 87}, {"referenceID": 12, "context": "For the questions and answers, the Word2Vec [Mikolov et al., 2013] trained on GoogleNews is employed to embed each word as a 300-dimensional real vector.", "startOffset": 44, "endOffset": 66}, {"referenceID": 7, "context": "We train all our models using the Adam optimizer [Kingma and Ba, 2015] to minimize the loss.", "startOffset": 49, "endOffset": 70}, {"referenceID": 10, "context": "This motivates us to also apply the WUPS measure [Malinowski and Fritz, 2014] with 0.", "startOffset": 49, "endOffset": 77}, {"referenceID": 0, "context": "The baseline method is an extension of the VQA model [Antol et al., 2015] without our re-watching or re-reading mechanisms.", "startOffset": 53, "endOffset": 73}, {"referenceID": 4, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 0, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 2, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 19, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 13, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 14, "context": "The first type is called the Open-Ended question answering [Ren et al., 2015; Malinowski and Fritz, 2014] where answers are produced given only the questions.", "startOffset": 59, "endOffset": 105}, {"referenceID": 10, "context": "The first type is called the Open-Ended question answering [Ren et al., 2015; Malinowski and Fritz, 2014] where answers are produced given only the questions.", "startOffset": 59, "endOffset": 105}, {"referenceID": 10, "context": "0 based on the Wu-Palmer (WUPS) similarity [Malinowski and Fritz, 2014] are employed to measure the answer accuracy.", "startOffset": 43, "endOffset": 71}, {"referenceID": 21, "context": "The other type is called the Multiple-Choice question answering [Zhu et al., 2016] where both the question and several candidate answers are presented.", "startOffset": 64, "endOffset": 82}, {"referenceID": 21, "context": "It is observed that the approaches for the Open-Ended question answering usually cannot produce high-quality long answers [Zhu et al., 2016].", "startOffset": 122, "endOffset": 140}, {"referenceID": 14, "context": "And most Open-Ended question answering approaches only focus on one-word answers [Ren et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 81, "endOffset": 127}, {"referenceID": 10, "context": "And most Open-Ended question answering approaches only focus on one-word answers [Ren et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 81, "endOffset": 127}, {"referenceID": 10, "context": "Malinowski et al [Malinowski and Fritz, 2014] collected their dataset with the human annotations.", "startOffset": 17, "endOffset": 45}, {"referenceID": 0, "context": "[Antol et al., 2015] manually collected a large-scale free-form Image-QA dataset.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "[Gao et al., 2015] also manually collected the FM-IQA (Freestyle Multilingual Image Question Answering) dataset with the help of the Amazon Mechanical Turk platform.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "In contrast, we propose to automatically convert existing video description dataset [Li et al., 2016] into question answering dataset.", "startOffset": 84, "endOffset": 101}, {"referenceID": 3, "context": "It is originally proposed for educational purpose [Gates, 2008].", "startOffset": 50, "endOffset": 63}, {"referenceID": 15, "context": "Among the question generation approaches [Rus and Arthur, 2009; Gates, 2008; Heilman and Smith, 2010], we employ the method from Heilman and Smith [Heilman and Smith, 2010] to generate our video-QA pairs from video description datasets.", "startOffset": 41, "endOffset": 101}, {"referenceID": 3, "context": "Among the question generation approaches [Rus and Arthur, 2009; Gates, 2008; Heilman and Smith, 2010], we employ the method from Heilman and Smith [Heilman and Smith, 2010] to generate our video-QA pairs from video description datasets.", "startOffset": 41, "endOffset": 101}, {"referenceID": 14, "context": "[Ren et al., 2015] to turn image description datasets into Image-QA datasets.", "startOffset": 0, "endOffset": 18}, {"referenceID": 18, "context": "[Tu et al., 2014] also combine video and text data for question answering.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "[Zhu et al., 2015] collect a dataset for \u201dfill-in-the-blank\u201d type of questions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "[Mazaheri et al., 2016] also consider the fill-in-the-blank problem.", "startOffset": 0, "endOffset": 23}], "year": 2017, "abstractText": "A number of visual question answering approaches have been proposed recently, aiming at understanding the visual scenes by answering the natural language questions. While the image question answering has drawn significant attention, video question answering is largely unexplored. Video-QA is different from Image-QA since the information and the events are scattered among multiple frames. In order to better utilize the temporal structure of the videos and the phrasal structures of the answers, we propose two mechanisms: the re-watching and the re-reading mechanisms and combine them into the forgettable-watcher model. Then we propose a TGIF-QA dataset for video question answering with the help of automatic question generation. Finally, we evaluate the models on our dataset. The experimental results show the effectiveness of our proposed models.", "creator": "LaTeX with hyperref package"}}}