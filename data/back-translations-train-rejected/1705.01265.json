{"id": "1705.01265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "On the effectiveness of feature set augmentation using clusters of word embeddings", "abstract": "Word clusters have been empirically shown to offer important performance improvements on various tasks. Despite their importance, their incorporation in the standard pipeline of feature engineering relies more on a trial-and-error procedure where one evaluates several hyper-parameters, like the number of clusters to be used. In order to better understand the role of such features we systematically evaluate their effect on four tasks, those of named entity segmentation and classification as well as, those of five-point sentiment classification and quantification. Our results strongly suggest that cluster membership features improve the performance.", "histories": [["v1", "Wed, 3 May 2017 06:33:37 GMT  (16kb)", "http://arxiv.org/abs/1705.01265v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["georgios balikas", "ioannis partalas", "massih-reza amini"], "accepted": false, "id": "1705.01265"}, "pdf": {"name": "1705.01265.pdf", "metadata": {"source": "CRF", "title": "On the e\u0080ectiveness of feature set augmentation using clusters of word embeddings", "authors": ["Georgios Balikas", "Ioannis Partalas", "Massih-Reza Amini"], "emails": ["georgios.balikas@imag.fr", "ioannis.partalas@gmail.com", "Massih-Reza.Amini@imag.fr", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar Xiv: 170 5.01 265v 1 [cs.C L] 3M ay2 017Word clusters have been empirically identified for important performance improvements in various tasks. Despite their importance, their inclusion in the standard pipeline of feature engineering relies more on a trial-and-error process in which several hyperparameters are evaluated, such as the number of clusters to be used. To better understand the role of such features, we systematically evaluate their impact on four tasks, namely entity segmentation and classification, as well as the five-point sentiment classification and quantification. Our results strongly suggest that cluster membership characteristics improve performance. KEYWORDSText Mining; Word Embeddings; Unsupervised LearningACM Reference Format: Georgios Balikas, Ioannis Partalas and Massih-Reza Amini. 2016."}, {"heading": "1 INTRODUCTION", "text": "It is not only a matter of time before such a process will occur, but also of time before such a process will occur. (...) It is a matter of time before such a process will occur. (...) It is a matter of time before such a process will occur. (...) It is a matter of time before such a process will occur. (...) It is a matter of time before such a process will occur. (...) It is a matter of time before such a process will occur. (...) It is a matter of time before such a process will occur. (...) It is a matter of time before such a process will occur. \"(...) It is a matter of time before such a process will occur.\""}, {"heading": "2 WORD CLUSTERS", "text": "Word embeddings associates words with dense, low-dimensional vectors. Several models have been suggested to train them.1 Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14], and GloVe (glove) [18] have been shown to be effective. Training does not require annotated data and can be performed with large amounts of text. Such a model can be considered a function f, which projects a wordw into a D-dimensional space: f (w).RD, where D is preset. Here, we focus on applications that use Twi data and focus on creative vocabularies, abbreviations, and slung.To train the embeddings for the tasks in our experimental study, we use 36 million English tweets collected between August and September 2017."}, {"heading": "3 EXPERIMENTAL EVALUATION", "text": "In order to ensure the reproducibility of our results, we opt for public data sets published with prefabricated tensile / test parts. We use the designated tensile parts to train the proposed methods and the test parts to report the results of the evaluation measures we use."}, {"heading": "3.1 Named-Entity Recognition in Twitter", "text": "It is a question of whether and in what form the people in the USA, in which they live, in the USA, in Europe, in the USA, in the EU, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3.2 Five-Point Sentiment Classi cation", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3.3 Five-point Sentiment anti cation", "text": "In fact, it is such that most of them are able to outdo themselves, and that they are able to outdo themselves, \"he said in an interview with the German Press Agency.\" We have it in hand, \"he added,\" but we have it in hand, \"and he added:\" We have it in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, hand, in hand, in hand, hand, in hand, in hand, in hand, hand, in hand in hand, hand in hand, in hand, hand in hand, hand in hand, in hand, hand in hand, hand in hand, in hand, hand in hand, in hand in hand, in hand, hand in hand, in hand, hand in hand, in hand, in hand, hand in hand in hand, in hand, in hand in hand, in hand in hand, in hand, in hand, in hand in hand, in hand in hand, in hand, in hand, in hand, in hand in hand in hand, in hand, in hand, in hand in hand in hand, in hand, in hand in hand, in hand in hand, in hand, in hand in hand, in hand in hand, in hand, in hand, in hand, in hand in hand in hand, in hand, in hand, in hand, in hand, in hand in hand in hand, in hand in hand in hand, in hand, in hand, in hand, in hand in hand in hand in hand, in hand, in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand in hand in hand, in hand, in hand, in hand in hand in hand, in hand, in hand, in hand, in hand, in hand, in"}, {"heading": "4 CONCLUSION", "text": "We have shown empirically that the inclusion of cluster membership characteristics in the feature extraction pipeline of NER, sentiment classification, and quantification tasks influences task performance, and the fact that the performance improvements in the four tasks we examined are consistent underscores their usefulness for both practitioners and researchers. Although our study does not identify a clear winner in terms of the type of word vectors (Skipgram, Cbow, or GloVe), the results suggest that one should try embedding low-dimensional Skip-gram programs (D = 40), high number of clusters (e.g. K-500, 1000, 2000), and small window values w (e.g. w = 5), as the results obtained with these results are consistently competitive."}], "references": [{"title": "k-means++: \u008ce advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In ACM-SIAM@Discrete algorithms", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "TwiSE at SemEval-2016 Task 4: Twi\u008aer Sentiment Classi\u0080cation", "author": ["Georgios Balikas", "Massih-Reza Amini"], "venue": "SemEval@NAACL-HLT", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Class-Based n-gram Models of Natural Language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "E\u0081cient programmable learning to search", "author": ["Hal Daum\u00e9 III", "John Langford", "St\u00e9phane Ross"], "venue": "CoRR abs/1406.1837", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A Paraphrase and Semantic Similarity Detection System for User Generated Short-Text Content on Microblogs", "author": ["Kuntal Dey", "Ritvik Shrivastava", "Saroj Kaushik"], "venue": "In COLING", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Sentiment \u008banti\u0080cation", "author": ["Andrea Esuli", "Fabrizio Sebastiani"], "venue": "IEEE Intelligent Systems 25,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "\u008bantifying counts and costs via classi\u0080cation", "author": ["George Forman"], "venue": "Data Mining and Knowledge Discovery 17,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Revisiting Embedding Features for Simple Semi-supervised Learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In EMNLP", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "2016. Monday mornings are my fave : ) #not Exploring the Automatic Recognition of Irony in English tweets", "author": ["Cynthia Van Hee", "Els Lefever", "V\u00e9ronique Hoste"], "venue": "In COLING", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu"], "venue": "In SIGKDD", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Ordinal Text \u008banti\u0080cation", "author": ["Giovanni Da San Martino", "Wei Gao", "Fabrizio Sebastiani"], "venue": "In SIGIR", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "E\u0081cient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Je\u0082rey Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "SemEval-2016 Task 4: Sentiment Analysis in Twi\u008aer", "author": ["Preslav Nakov", "Alan Ri\u008aer", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov"], "venue": "SemEval@NAACL-HLT", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters", "author": ["Olutobi Owoputi", "BrendanO\u2019Connor", "ChrisDyer", "KevinGimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In NAACL", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Learning to Search for Recognizing Named Entities in Twi\u008aer", "author": ["Ioannis Partalas", "C\u00e9dric Lopez", "Nadia Derbas", "Ruslan Kalitvianski"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Je\u0082rey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Named Entity Recognition in Tweets: An Experimental Study", "author": ["Alan Ri\u008aer", "Sam Clark", "Mausam", "Oren Etzioni"], "venue": "In EMNLP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "\u008ce earth mover\u2019s distance as a metric for image retrieval", "author": ["Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas"], "venue": "International journal of computer vision 40,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "A Vector Space Model for Automatic Indexing", "author": ["Gerard Salton", "A. Wong", "C.S. Yang"], "venue": "Commun. ACM 18,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1975}, {"title": "Results of the WNUT16 Named Entity Recognition", "author": ["Benjamin Strauss", "Bethany E. Toma", "Alan Ri\u008aer", "Marie Catherine de Marne\u0082e", "Wei Xu"], "venue": "Shared Task. InWNUT@COLING", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "E\u0082ect of Non-linear Deep Architecture in Sequence Labeling", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "In IJCNLP", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "author": ["\u008ceresa Wilson", "Janyce Wiebe", "Paul Ho\u0082mann"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "Although useful and o\u0089en state-of-the-art, adapting such solutions across tasks can be tricky and time-consuming [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 20, "context": "\u008cerefore, simple yet general and powerful feature extraction approaches that performwell across several tasks are valuable [23].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "However, as shown in [24] linear models perform be\u008aer in high-dimensional discrete spaces compared to continuous ones.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "nnnnnnn main reason of the high performance of the vector space model [21] in tasks like text classi\u0080cation with linear classi\u0080ers.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "[2013] use Brown clusters in a POS tagger [3], Kiritchenko et al.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "Importantly, our evaluation compared to previous work [9] that focuses on old and well studied datasets uses recent and challenging Twi\u008aer datasets.", "startOffset": 54, "endOffset": 57}, {"referenceID": 11, "context": "Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14] and GloVe (glove) [18] have been shown to be e\u0082ective.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14] and GloVe (glove) [18] have been shown to be e\u0082ective.", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14] and GloVe (glove) [18] have been shown to be e\u0082ective.", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "\u008ce k-means clusters are initialized using \u201ck-means++\u201d as proposed in [1], while the algorithm is run for 300 iterations.", "startOffset": 69, "endOffset": 72}, {"referenceID": 19, "context": "edu/projects/glove/ Workshop on Noisy User-generated Text [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "Learning algorithm Our model for solving the task is a learning to search for structured prediction tasks approach [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 14, "context": "More speci\u0080cally we follow the feature extraction and learning algorithm choices of [17], which was ranked 2nd among 10 participants in the aforementioned competition [22].", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "More speci\u0080cally we follow the feature extraction and learning algorithm choices of [17], which was ranked 2nd among 10 participants in the aforementioned competition [22].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "To evaluate the performance of cluster membership features on Twi\u008aer data though, we use the se\u008aing of Task 4 of SemEval-2016 challenge, entitled \u201cSentiment Analysis in Twitter\u201d and, in particular, the dataset released by the organizers for subtask C \u201cTweet classi\u0080cation according to a \u0080ve-point scale\u201d, described in detail at [15].", "startOffset": 328, "endOffset": 332}, {"referenceID": 12, "context": "\u008ce evaluation measure selected in [15] is the macro-averaged MeanAbsolute Error (MAEM ).", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Also, the advantage of using the macro- version instead of the standard version of the measure is the robustness against the class imbalance in the data, whose description we omit due to space limitations and can be found at [15].", "startOffset": 225, "endOffset": 229}, {"referenceID": 1, "context": "Learning algorithmTo demonstrate the e\u0081ciency of clustermembership features we rely on the system of [2] which was ranked 1st among 11 participants and uses a Logistic Regression as a learning algorithm.", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "We follow the same feature extraction steps which consist of extracting n-gram and character n-gram features, partof-speech counts as well as sentiment scores using standard sentiment lexicons such as the Bing Liu\u2019s [11] and the MPQA lexicons [25].", "startOffset": 216, "endOffset": 220}, {"referenceID": 22, "context": "We follow the same feature extraction steps which consist of extracting n-gram and character n-gram features, partof-speech counts as well as sentiment scores using standard sentiment lexicons such as the Bing Liu\u2019s [11] and the MPQA lexicons [25].", "startOffset": 243, "endOffset": 247}, {"referenceID": 1, "context": "\u008ce complete description of the features is available at [2].", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "Results To evaluate the performance of the proposed feature augmentation technique, we present in Table 2 the MAEM scores for di\u0082erent se\u008aings on the o\u0081cial test set of [15].", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "\u008ce achieved score improves the state-of-the art on the dataset, which to the best of our knowledge was by [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 15, "context": "Note, also, that using the clusters produced by the outof-domain embeddings trained onWikipedia that were released as part of [18] performs suprisingly well.", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "In the rest, we show the e\u0082ect of the features derived from the word embeddings clusters in the \u0080ve-point quanti\u0080cation problem, which was the Subtask E of the SemEval-2016 \u201cSentiment Analysis in Twi\u008aer\u201d task [15].", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "sociated with subjects (described in full detail at [15]), and, hence, quanti\u0080cation is performed for the tweets of a subject.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "Learning Algorithm To perform the quanti\u0080cation task, we rely on a classify and count approach [7], which was shown e\u0082ective in a related binary quanti\u0080cation problem [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Learning Algorithm To perform the quanti\u0080cation task, we rely on a classify and count approach [7], which was shown e\u0082ective in a related binary quanti\u0080cation problem [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 17, "context": "\u008ce evaluation measure for the problem is the EarthMovers Distance (EMD) [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "243\u2020 described at [13]) skipgram40,w5 .", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "where |C | is number of categories (\u0080ve in our case) and p\u0302(ci ) and p(ci ) are the true and predicted prevalence respectively [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 12, "context": "243) performance in the challenge [15], held by the method of [13].", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "243) performance in the challenge [15], held by the method of [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "\u008ce improvement over the method of [13] however, does not necessarily entail that classify and count should be preferred for the task.", "startOffset": 34, "endOffset": 38}], "year": 2017, "abstractText": "Word clusters have been empirically shown to o\u0082er important performance improvements on various tasks. Despite their importance, their incorporation in the standard pipeline of feature engineering relies more on a trial-and-error procedure where one evaluates several hyper-parameters, like the number of clusters to be used. In order to be\u008aer understand the role of such features we systematically evaluate their e\u0082ect on four tasks, those of named entity segmentation and classi\u0080cation as well as, those of \u0080ve-point sentiment classi\u0080cation and quanti\u0080cation. Our results strongly suggest that cluster membership features improve the performance.", "creator": "LaTeX with hyperref package"}}}