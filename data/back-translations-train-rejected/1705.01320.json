{"id": "1705.01320", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks", "abstract": "We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers.", "histories": [["v1", "Wed, 3 May 2017 09:13:10 GMT  (58kb,D)", "https://arxiv.org/abs/1705.01320v1", null], ["v2", "Thu, 18 May 2017 07:32:33 GMT  (57kb,D)", "http://arxiv.org/abs/1705.01320v2", null], ["v3", "Wed, 2 Aug 2017 09:21:18 GMT  (58kb,D)", "http://arxiv.org/abs/1705.01320v3", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["ruediger ehlers"], "accepted": false, "id": "1705.01320"}, "pdf": {"name": "1705.01320.pdf", "metadata": {"source": "CRF", "title": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks", "authors": ["R\u00fcdiger Ehlers"], "emails": [], "sections": [{"heading": null, "text": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks Ru \ufffd diger Ehlers University of Bremen and DFKI GmbH, Bremen, Germany We present an approach to verification of feed-forward neural networks, in which all nodes have a piecemeal linear activation function. Such networks are commonly used in deep learning and have proven difficult for modern satisfaction modulo theory (SMT) and holistic linear programming (ILP). Starting point of our approach is the addition of a global linear approximation of the overall behavior of the network to the verification problem, which is helpful in the SMT-like argument about network behavior. We present a specialized verification algorithm that uses this approximation in a search process in which additional node phases for the nonlinear nodes in the network result from partial node phase assignments, similar to the spread of the classical SAT solution."}, {"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Preliminaries", "text": "We look at multi-layer (perceptron) networks with linear, ReLU, and MaxPool nodes in this work. Such networks are formally defined as directional acyclic weighted graphs G = (V, W, B, T), where V is a set of nodes, where V is a set of nodes, E \u00b2 V \u00b7 V is a set of edges, W: E \u2192 R assigns a weight to each edge of the network, B: V \u2192 R assigns a node bias to each node, and T assigns a type to each node on the network, which is used from a set of available types T (Input, Linear, ReLU, MaxPool). Nodes without incoming edges are referred to as input nodes, and we assume that T (v) = input for each such node of vertices that do not have an output edge, we are also referred to as output nodes. A feed-forward neural network with input nodes \u2192 output nodes represents an Rm m."}, {"heading": "3 Efficient Verification of Feed-forward Neural Networks", "text": "In this thesis we deal with the following verification problem: Definition 1 Given a feed-forward neural network G implementing a function f: Rn \u2192 Rm and a series of linear constraints on the really evaluated variables V = {x1,.. whenever we implement a function f: Rn \u2192 Rm, the verification problem of the neural network (NN) is to find a node value assignment function a for V that meets the entry and exit nodes of G and for which we have f (x1,.,., xn) = (y1,.,.,., ym) or to conclude that no such node assignment function exists. Restriction to conjunctions of linear properties in Definition 1 was carried out for the sake of simplicity. The verification of arc boolean combinations of linear properties of linear properties can be found in Definition 1."}, {"heading": "3.1 Linear Approximation of Neural Network Value Assignment Functions", "text": "Let G = (V, E, W, B, T) be a network representing a function f: Rn \u2192 Rm. We want to build a system of linear constraints that uses V as a set of variables that come close to f, i.e., so that each node function a is a correct solution to the linear constraint system, and the constraints are as narrow as possible. The main difficulty in constructing such a constraint system is that the ReLU and the MaxPool nodes do not have linear input-output behavior (until their phases are fixed), so we need to approximate them linearly. Figure 1 shows the activation function of a ReLU node, in which we designate the weighted sum of the input signals at the node (and its bias) as variable c."}, {"heading": "3.2 Search process and Infeasible Subset Finding", "text": "In view of a phase attachment for all ReLU and MaxPool nodes in a network, let us check whether there is a node value assignment function with these phases (and so that the verification restriction for the node values is fulfilled), we can reduce it to a linear programming problem. To this end, let us extend the linear program that was built using the approach from the previous subsection (using V as a variable for the node values), by applying the following constraints: \u2022 For each \u2264 0 phase selected for a ReLU node v, let us add the constraints v = 0 and \u0445E (v \u2032, v) - E W (v, v) - W, v - W (W), v - W (W)."}, {"heading": "3.3 Implied Node Phase Inference during Partial Phase Fixture Checking", "text": "In the subnode conversion check of section 3.2, we ask for the linear conversion phase to minimize the difference between the individual nodes. (...) In the subnode conversion phase (...), however, we do not use an optimization function apart from the elastic filter step, since it was not necessary to check the feasibility of a subnode conversion phase. (...) In the frequent case that the subnode conversion phase is feasible (...), we define an optimization function that enables us to check the feasibility of additional unfeasible and feasible subnode conversion phase. (...) The feasible fixings are knotted so that they or a partial attachment of the same is evaluated later, no linear programming has to be performed. (...) Considering a subnode conversion phase V. (...) We use a subnode conversion 110 subnode conversion period (= V)."}, {"heading": "3.4 Detecting Implied Phases", "text": "If the SAT solver has fixed a new node phase, the selected phases together can imply other node phases. Let's take, for example, the net dump from Figure 2. There are two ReLU nodes named r1 and r2 and a MaxPool node. Let's assume that the first analysis of the network (Section 3.1) found that the value of the node r1 is between 0.0 and 1.5 and the value of the node r2 is between 0.1 and 2.0. First, the SAT solver can unconditionally detect that the node r2 is in the \u2265 0 phase. Then, if the SAT solver sometime decides that the node r1 should be in the \u2264 0 phase, it fixes the value of r1 to 0. Since the flow from r2 has a lower limit > 0, we can deduce that the phase should be set from m to (r2, m). Similar considerations can also be made for the phase that leads into a node."}, {"heading": "3.5 Overview of the Integrated Solver", "text": "To conclude this section, we need to discuss how to combine the additional clauses presented in it. (Algorithm 1 already displays the overall approach.) In the first step, the upper and lower limits are calculated for all nodes. (...) The solver then prepares an empty partial evaluation for the SAT variables and an empty list in which additional clauses are generated by the LP instance analysis. (...) In the main loop of the algorithm, the first step is taken to perform most of the steps of the SAT solution, such as the spread of conflicts & analyses. We assume that the partial evaluation is always characterized by decision levels, so that traceability can always be performed when additional clauses from the SAT instance are mixed."}, {"heading": "4 Experiments", "text": "It is written in C + + and is based on the linear programming tool GLPK 4.611 and the SAT solver Minisat 2.2.0 [ES03]. While using GLPK as it is, we have modified the main search procedure of Minisat to implement algorithm 1. We repeat the initial tightening process of Section 3.1 until the cumulative changes in \u2212 \u2192 min and \u2212 \u2212 \u2212 \u2192 max fall below 1.0. We also abort the process when 5000 node approximation updates have been performed (so as not to spend too much time in the process for very large networks), provided that its limits have been updated at least three times for each node. All numerical calculations are performed with double precision, and we have used no compensation for numerical inaccuracies in the code, except for the use of a fixed safety margin = 0.0001."}, {"heading": "4.1 Collision Avoidance", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4.2 MNIST Digit Recognition", "text": "This is a classic machine learning problem, and we are using a simplified version of Caffe's version of the MNIST network [LBBH98] for our experiments. The Caffe version is different from the original network, which has essentially linear node activation capabilities. Figure 5 (a) - (b) shows some sample digits from the MNIST dataset. All images are gray-scale and have 28 x 4 pixels. Our simplified network uses the following layers: \u2022 An input layer with 28 x 28 nodes \u2022 a revolutionary network layer with 3 x 13 x 13 nodes, where each node has 16 incoming edges \u2022 a pooling layer with 3 x 4 x 4 x 4 nodes, where each node has 16 nodes."}, {"heading": "5 Conclusion", "text": "In this paper, we presented a new approach to verifying feed-forward neural networks with piecemeal linear activation functionality. Our main idea was to generate a linear approximation of overall network behavior that can be added to SMT or ILP instances that encode neural network verification problems, and to use the approach in a specialized approach that includes several additional neural network verification techniques grouped around a SAT solver for selecting the node phases in the network. We considered two case studies from different application areas. The approach allows for arbitrary convex verification of conditions, and we used it to define a noise model for testing the robustness of a network to detect handwritten digitalisms. We made the approach presented in this paper as open source software, hoping that it would be easier to verify neural network verification, and to fix architectures."}, {"heading": "Acknowledgements", "text": "This work was partly financed by the Institutional Strategy of the University of Bremen, financed by the German Excellence Initiative."}], "references": [{"title": "In Annual Conference on Neural Information Processing Systems (NIPS)", "author": ["Osbert Bastani", "Yani Ioannou", "Leonidas Lampropoulos", "Dimitrios Vytiniotis", "Aditya V. Nori", "Antonio Criminisi. Measuring neural net robustness with constraints"], "venue": "pages 2613\u20132621,", "citeRegEx": "BIL+16", "shortCiteRegEx": null, "year": 2016}, {"title": "INFORMS Journal on Computing", "author": ["John W. Chinneck", "Erik W. Dravnieks. Locating minimal infeasible constraint sets in linear programs"], "venue": "3(2):157\u2013168,", "citeRegEx": "CD91", "shortCiteRegEx": null, "year": 1991}, {"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv/CoRR, 1511.07289,", "citeRegEx": "CUH15", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast linear-arithmetic solver for DPLL(T)", "author": ["Bruno Dutertre", "Leonardo Mendon\u00e7a de Moura"], "venue": "18th International Conference on Computer Aided Verification (CAV), pages 81\u201394,", "citeRegEx": "DdM06", "shortCiteRegEx": null, "year": 2006}, {"title": "In 6th International Conference on Theory and Applications of Satisfiability Testing", "author": ["Niklas E\u00e9n", "Niklas S\u00f6rensson. An extensible SAT-solver"], "venue": "(SAT). Selected Revised Papers, pages 502\u2013518,", "citeRegEx": "ES03", "shortCiteRegEx": null, "year": 2003}, {"title": "volume 185 of Frontiers in Artificial Intelligence and Applications", "author": ["John Franco", "John Martin. A History of Satisfiability"], "venue": "chapter 1, pages 3\u201374. IOS Press, February", "citeRegEx": "FM09", "shortCiteRegEx": null, "year": 2009}, {"title": "In 29th International Conference on Computer Aided Verification (CAV)", "author": ["Xiaowei Huang", "Marta Kwiatkowska", "Sen Wang", "Min Wu. Safety verification of deep neural networks"], "venue": "Springer,", "citeRegEx": "HKWW17", "shortCiteRegEx": null, "year": 2017}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv/CoRR, 1408.5093,", "citeRegEx": "JSD+14", "shortCiteRegEx": null, "year": 2014}, {"title": "Reluplex: An efficient SMT solver for verifying deep neural networks", "author": ["Guy Katz", "Clark W. Barrett", "David L. Dill", "Kyle Julian", "Mykel J. Kochenderfer"], "venue": "29th International Conference on Computer Aided Verification (CAV). Springer,", "citeRegEx": "KBD+17", "shortCiteRegEx": null, "year": 2017}, {"title": "Decision Procedures \u2013 An Algorithmic Point of View", "author": ["Daniel Kroening", "Ofer Strichman"], "venue": "Springer,", "citeRegEx": "KS08", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "LBBH98", "shortCiteRegEx": null, "year": 1998}, {"title": "In 22nd International Conference on Computer Aided Verification (CAV)", "author": ["Luca Pulina", "Armando Tacchella. An abstraction-refinement approach to verification of artificial neural networks"], "venue": "pages 243\u2013257,", "citeRegEx": "PT10", "shortCiteRegEx": null, "year": 2010}, {"title": "Challenging SMT solvers to verify neural networks", "author": ["Luca Pulina", "Armando Tacchella"], "venue": "AI Commun., 25(2):117\u2013135,", "citeRegEx": "PT12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117,", "citeRegEx": "Sch15", "shortCiteRegEx": null, "year": 2015}, {"title": "In Formal Methods in Computer-Aided Design (FMCAD)", "author": ["Karsten Scheibler", "Felix Neubauer", "Ahmed Mahdi", "Martin Fr\u00e4nzle", "Tino Teige", "Tom Bienm\u00fcller", "Detlef Fehrer", "Bernd Becker. Accurate ICP-based floating-point reasoning"], "venue": "pages 177\u2013184,", "citeRegEx": "SNM+16", "shortCiteRegEx": null, "year": 2016}, {"title": "In MBMV Workshop 2015", "author": ["Karsten Scheibler", "Leonore Winterer", "Ralf Wimmer", "Bernd Becker. Towards verification of artificial neural networks"], "venue": "Chemnitz, Germany, pages 30\u201340,", "citeRegEx": "SWWB15", "shortCiteRegEx": null, "year": 2015}, {"title": "In Road Vehicle Automation 2", "author": ["Michael Wagner", "Philip Koopman. A philosophy for developing trust in self-driving cars"], "venue": "pages 163\u2013171. Springer International Publishing,", "citeRegEx": "WK15", "shortCiteRegEx": null, "year": 2015}, {"title": "In British Machine Vision Conference (BMVC)", "author": ["Qian Yu", "Yongxin Yang", "Yi-Zhe Song", "Tao Xiang", "Timothy M. Hospedales. Sketcha-net that beats humans"], "venue": "pages 7.1\u20137.12,", "citeRegEx": "YYS+15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": ", [Sch15]), the artificial intelligence research community has learned a lot about engineering these networks, such that they nowadays achieve a very good classification precision and outperform human classifiers on some tasks, such as sketch recognition [YYS+15].", "startOffset": 2, "endOffset": 9}, {"referenceID": 16, "context": "But if we do not have formal specifications, how can we assure the safety of such a system? The classical approach to tackle this problem is to construct safety cases [WK15].", "startOffset": 167, "endOffset": 173}, {"referenceID": 11, "context": "Pulina and Tacchella [PT10] present an approach for neurons with non-linear activation functions that only scales to small networks.", "startOffset": 21, "endOffset": 27}, {"referenceID": 15, "context": "[SWWB15] consider the bounded model checking problem for an inverse pendulum control scenario with non-linear system dynamics and a non-linear neuron activation function, and despite employing the state-of-the-art SMT solver iSAT3 [SNM+16] and even extending this solver to deal better with the resulting problem instances, their experiments show that the resulting verification problem is already challenging for neural networks with 26 nodes.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "In deep learning [Sch15], many works use networks whose nodes have piece-wise linear activation functions.", "startOffset": 17, "endOffset": 24}, {"referenceID": 6, "context": "[HKWW17] describe such an approach that is based on propagating constraints through the layers of a network.", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "While the approximation can also be used as additional constraints in SMT solving and improves the computation times of the SMT solver, we apply it in a customized solver that uses the elastic filtering algorithm from [CD91] for minimal infeasible linear constraint set finding in case of conflicts, and combine it with a specialized procedure for inferring implied node phases.", "startOffset": 218, "endOffset": 224}, {"referenceID": 5, "context": "For more details on SAT solving, the interested reader is referred to [FM09].", "startOffset": 70, "endOffset": 76}, {"referenceID": 9, "context": "Even though linear programming was shown to have polynomial-time complexity, it has been observed that in practice [KS08], it is often faster to apply the Simplex algorithm, which is an exponential-time algorithm.", "startOffset": 115, "endOffset": 121}, {"referenceID": 1, "context": "As an alternative, we present a new approach that combines 1) linear approximation of the overall NN behavior, 2) irreducible infeasible subset analysis for linear constraints based on elastic filtering [CD91], 3) inferring possible safe node phase choices from feasibility checking of partial node phase valuations, and 4) performing unit-propagation-like reasoning on node phases.", "startOffset": 203, "endOffset": 209}, {"referenceID": 1, "context": "To achieve this, we employ elastic filtering [CD91].", "startOffset": 45, "endOffset": 51}, {"referenceID": 3, "context": "Satisfiability modulo theory solvers typically employ cheaper procedures to compute minimal infeasible subsets of linear constraints, such as the one by Duterte and de Moura [DdM06], but the high number of constraints in the linear approximation of the network behavior that are independent of node phase selections seems to make the approach less well-suited, as our experiments with the SMT solver Yices that uses this approach suggest.", "startOffset": 174, "endOffset": 181}, {"referenceID": 4, "context": "0 [ES03].", "startOffset": 2, "endOffset": 8}, {"referenceID": 10, "context": "The Caffe framework comes with some example architectures, and we use a simplified version of Caffe\u2019s version of the lenet network [LBBH98] for our experiments.", "startOffset": 131, "endOffset": 139}, {"referenceID": 2, "context": "But even if more advanced activation functions such as exponential linear units [CUH15] shall be used in learning, they can still be applied to learn an initial model, which is then linearly approximated with ReLU nodes and fine-tuned by an additional learning process.", "startOffset": 80, "endOffset": 87}, {"referenceID": 6, "context": "[HKWW17] does not suffer from this limitation, it cannot handle general verification properties, which we believe to be important.", "startOffset": 0, "endOffset": 8}], "year": 2017, "abstractText": "We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers. The starting point of our approach is the addition of a global linear approximation of the overall network behavior to the verification problem that helps with SMT-like reasoning over the network behavior. We present a specialized verification algorithm that employs this approximation in a search process in which it infers additional node phases for the non-linear nodes in the network from partial node phase assignments, similar to unit propagation in classical SAT solving. We also show how to infer additional conflict clauses and safe node fixtures from the results of the analysis steps performed during the search. The resulting approach is evaluated on collision avoidance and handwritten digit recognition case studies.", "creator": "LaTeX with hyperref package"}}}