{"id": "1705.01346", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Going Wider: Recurrent Neural Network With Parallel Cells", "abstract": "Recurrent Neural Network (RNN) has been widely applied for sequence modeling. In RNN, the hidden states at current step are full connected to those at previous step, thus the influence from less related features at previous step may potentially decrease model's learning ability. We propose a simple technique called parallel cells (PCs) to enhance the learning ability of Recurrent Neural Network (RNN). In each layer, we run multiple small RNN cells rather than one single large cell. In this paper, we evaluate PCs on 2 tasks. On language modeling task on PTB (Penn Tree Bank), our model outperforms state of art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English translation task, our model increases BLEU score for 0.39 points than baseline model.", "histories": [["v1", "Wed, 3 May 2017 10:22:22 GMT  (429kb)", "http://arxiv.org/abs/1705.01346v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["danhao zhu", "si shen", "xin-yu dai", "jiajun chen"], "accepted": false, "id": "1705.01346"}, "pdf": {"name": "1705.01346.pdf", "metadata": {"source": "CRF", "title": "Going Wider: Recurrent Neural Network With Parallel Cells", "authors": ["Danhao Zhu", "Si Shen", "Xin-Yu Dai", "Jiajun Chen"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people are able to determine for themselves what they want and what they want. It's not that people are able to decide whether they want to or not. It's that they don't want to do it. It's that they don't want to do it. It's that they don't want to do it. It's that they don't want to do it. It's that they don't want to do it. It's that they do it. It's that they do it. It's that they do it. It's that they do it. It's that they do it. It's that way. It's that way. It's that way. It's that way. It's that way. It's that way. \"It's that way.\" It's that way. \"It's that way.\" It is. \"It is that way.\""}, {"heading": "3.2 Machine Translation", "text": "Dre rf\u00fc rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "1 Chinese \u5370\u5ea6 \u56fd\u9632\u90e8 \u5728 \u5176 \u53d1\u8868 \u7684 \u4e00 \u9879 \u8fdc\u666f \u89c4\u5212 \u4e2d \u4e5f \u8ba4", "text": "The Indian Ministry of Defense of Golden Translation also stated in its long-term plan that China does not pose a military threat to India. Baseline Translation in its recently released Plan 2 Corporate interests include LDC2002E18, LDC2003E07, LDC2003E14, Hansard's stake in LDC2004T07, LDC2004T08 and LDC2005T06."}, {"heading": "2 Chinese \u8499\u65b9 \u611f\u8c22 \u4e2d\u65b9 \u591a\u5e74 \u6765 \u5bf9 \u8499\u53e4 \u63d0\u4f9b \u7684 \u63f4\u52a9 \u3002", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Empirical Study of PCs", "text": "In this section, we will conduct an empirical study on the task of speech modeling to show whether parallel cells work as we expected, such as differential cells that learn different traits, and compare the performance of PCs with average modeling techniques. 4.1 Different cells for different traits Unlike conditional random fields or support vector machines, neural networks do not have explicit traits. Moreover, text is not as easy to represent as images to see what traits have been extracted. As a result, we will indirectly examine how the result changes when we mask different cells. We assume that predicting a word requires a set of traits offered by cells. Consequently, when we mask a cell, when predicting the target strongly depends on the traits in the cell, the perxity clearly shows and vice versa."}, {"heading": "4.2 comparison with model averaging", "text": "A PC RNN with m / n hidden units and n parallel cells consumes as much computing and memory resource as n small naive RNN with m / n hidden units. Therefore, we want to compare the performance between PC RNN and its counterpart ensemble models that use the same resource. Table 3 compares the results of model averaging. 2 Model1 costs as much resource as 6 Model3. However, 2 Model1 exceed 10 Model3 in model averaging. 38 Model2 achieve earlier model averaging perplexity. However, we exceed this by using only 10 Model1. We can conclude that PCs can still considerably exceed naive RNN with equivalent or less computing and memory resources after model averaging. Model 1: Parallel _ Cells _ LSTM _ 1950h _ 3wide in Table 1, with 3 parallel cells and 1950 hidden units."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed the Parallel Cells technique for RNN. Parallel cells can reduce the effects of unrelated features caused by recurring connections. In all 3 evaluation tasks, models with PCs achieve significant improvements over results with fewer parameters. We examined the results of the speech modeling task by masking cells. Results show strong evidence that different cells handle different characteristics. Furthermore, we found that even after averaging the model, PCs can still outperform the base model and get new state-of-the-art perplexity. There is much to do in the line of PCs. For example, parallel cells with different hidden units may be better because an equally dividing strategy is really arbitrary. In addition, we can test the performance of PCs on many downstream models based on RNN references [Mikolov et al., 2010] T. Mikolov, M. Karafi\u00e1t, L. Burget, J. Cernock\u00fd and S. Khupur."}, {"heading": "1054-1063, 2016", "text": "[Wu et al., 2016] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's Neural machine translation system: Bridging the gap between human and machine translation, 1997. [Cho et al., 2014] Kyunghyun Cho. Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio. On the Properties ofNeural Machine Translation, 9 (8): Encoder Decoder Approaches. In EMNLP 2014, pp. 103-111."}], "references": [{"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH, pages 1045\u20131048", "citeRegEx": "Mikolov et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv:1409.2329", "citeRegEx": "Zaremba et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Minh-Thang Luong and Christopher D", "author": ["T Luong M", "D Manning C"], "venue": "Manning.", "citeRegEx": "Luong and Manning. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Klaus Macherey", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao"], "venue": "et al.", "citeRegEx": "Wu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural computation", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["Cho et al", "2014]Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "EMNLP", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "Lecun et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Attention mechanisms for vision in a dynamic world", "author": ["Burt", "1988] Burt P J"], "venue": "International Conference on Pattern Recognition. Pages 977-987,", "citeRegEx": "Burt and J.,? \\Q1988\\E", "shortCiteRegEx": "Burt and J.", "year": 1988}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["al. Denil et", "M 2012] Denil", "L Bazzani", "H Larochelle"], "venue": "Neural Computation,", "citeRegEx": "et et al\\.,? \\Q2012\\E", "shortCiteRegEx": "et et al\\.", "year": 2012}, {"title": "2015", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Neural machine translation by jointly learning to align and translate. ICLR", "citeRegEx": "Bahdanau et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective Approaches to Attention based Neural Machine Translation", "author": ["Luong et al", "2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "et al", "author": ["J Chung", "C Gulcehre", "H Cho K"], "venue": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. Eprint Arxiv,", "citeRegEx": "Chung et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Character-aware neural language models", "author": ["Kim et al", "2016]Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "AAAI", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting[J", "author": ["al. Srivastava et", "N 2014]Srivastava", "G Hinton", "A Krizhevsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "et et al\\.,? \\Q2014\\E", "shortCiteRegEx": "et et al\\.", "year": 2014}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Papineni et al", "2002] Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2002\\E", "shortCiteRegEx": "al. et al\\.", "year": 2002}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580", "citeRegEx": "Hinton et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["P Wang", "Y Qian", "K Soong F"], "venue": "A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding. arXiv preprint arXiv:1511.00215,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Recurrent Neural Network (RNN) has been widely applied for sequence modeling. In RNN, the hidden states at current step are full connected to those at previous step, thus the influence from less related features at previous step may potentially decrease model\u2019s learning ability. We propose a simple technique called parallel cells (PCs) to enhance the learning ability of Recurrent Neural Network (RNN). In each layer, we run multiple small RNN cells rather than one single large cell. In this paper, we evaluate PCs on 2 tasks. On language modeling task on PTB (Penn Tree Bank), our model outperforms state of art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English translation task, our model increases BLEU score for 0.39 points than baseline model. 1.Introduction Recurrent neural network (RNN) has been one of the most powerful sequence models in natural language processing. Many important applications achieve state of the art performance with RNN, including language modeling(Mikolov et al.,2010; Zaremba et al., 2014 ), machine translation (Luong and Manning, 2016; Wu et al., 2016) and so on. The features learned by RNN are stored in the hidden states. At each time step, the cell extracts features from data and updates its hidden states. The left side of figure 1 concisely shows the transition of hidden states in na\u00efve RNN. We can see that all units at the previous step are fully connected to all units at the current step. Thus, each pair of features can affect each other. Such design is not reality because many features are not that related. The influence between unrelated features may harm the learning ability of models. We can expect learning models automatically set the weight of all unnecessary connections to zero. However, in practice, because the data size is limited and the algorithms are not that strong, these unrelated connections will harm the learning ability. For example, that is why we have to do feature selections before training. To address the problem, many successful neural models benefit from replacing global connection with local connection. For example, Long Short-Term Memory(LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units(GRU) (Cho et al., 2014) have been the most popular RNN cells. In the core, such models use gates to control the data flow, allow part of connections to be activated. Another example is Convolution Neural Networks(CNN)(Lecun et al, 1998), one of the most successful models in deep learning nowadays. CNN uses local receptive fields to extract features from previous feature map. With local receptive fields, neurons can extract elementary visual features such as oriented edges, end-points, corners.", "creator": "\u00fe\u00ff"}}}