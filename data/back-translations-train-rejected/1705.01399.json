{"id": "1705.01399", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Answer Set Programming for Non-Stationary Markov Decision Processes", "abstract": "Non-stationary domains, where unforeseen changes happen, present a challenge for agents to find an optimal policy for a sequential decision making problem. This work investigates a solution to this problem that combines Markov Decision Processes (MDP) and Reinforcement Learning (RL) with Answer Set Programming (ASP) in a method we call ASP(RL). In this method, Answer Set Programming is used to find the possible trajectories of an MDP, from where Reinforcement Learning is applied to learn the optimal policy of the problem. Results show that ASP(RL) is capable of efficiently finding the optimal solution of an MDP representing non-stationary domains.", "histories": [["v1", "Wed, 3 May 2017 13:13:51 GMT  (1704kb,D)", "http://arxiv.org/abs/1705.01399v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["leonardo a ferreira", "reinaldo a c bianchi", "paulo e santos", "ramon lopez de mantaras"], "accepted": false, "id": "1705.01399"}, "pdf": {"name": "1705.01399.pdf", "metadata": {"source": "CRF", "title": "Answer Set Programming for Non-Stationary Markov Decision Processes", "authors": ["Leonardo A. Ferreira", "Reinaldo A. C. Bianchi", "Paulo E. Santos"], "emails": ["leonardo.ferreira@metodista.br", "rbianchi@fei.edu.br", "psantos@fei.edu.br", "mantaras@iiia.csic.es"], "sections": [{"heading": null, "text": "Keywords Non-Determinism \u00b7 Markov Decision Processes \u00b7 Response Programming \u00b7 Action Languages"}, {"heading": "1 Introduction", "text": "In fact, it is as if most of them are able to survive themselves, and that they are able to survive themselves by going in search of themselves. (...) In fact, it is as if they are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves, as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "2 Background", "text": "In fact, it is true that most people are able to comply with the rules. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "3 Combining ASP and MDP", "text": "(This section is the main contribution of this work, the ASP (RL) method, which is a combination of ASP and MDP to solve non-stationary decision-making problems. (This section is the main contribution of this work.)"}, {"heading": "4 Experiments", "text": "In fact, it is the case that one will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in the position in which they are."}, {"heading": "5 Results", "text": "In this section, we will use the situations described above to compare the learning processes of SARSA and Q-Learning with those of ASQ (SARSA) and ASP (QLearning), where SARSA and Q-Learning are used together with ASP. However, this comparison is performed using two different criteria: the return (p, a, s \") of the episode and the number of steps required to achieve the target state and the deviation from the mean square (RMSD) of the activity value function, with the time returning t \u2212 1, corresponding to Equation 5. RMSD = number of steps required to achieve the target status and the root-mean square deviation (s) of the activity value function. Figures 2, 4, 6 and 7 represent measurements for the four algorithms used in the three situations considered."}, {"heading": "6 Discussion", "text": "The results shown in the previous section represent the best, worst and average cases of the proposed ASP (RL) methodology. The first map (Figure 1a) represents the worst case for ASP (RL).As can be seen in the diagrams in Figures 2, 3, 4, 5, 6,7, the performance of ASP (QLearning) and ASP (SARSA) is the same as that of Q-Learning and SARSA. This is due to the fact that the reduction of the groups of states and measures is minimal (since there are no restrictions on this map) and the ASP (RL) methods use the same S and A as the RL methodology. The best case is presented in the last map (Figure 1d).In this case, there is only one practicable policy and thus this is the optimal policy, although the learning process has been carried out, in situations such as these learning processes are not necessary as there is only one practicable policy."}, {"heading": "7 Related Work", "text": "The method proposed in this paper is consistent with the work being done in [27,22] where ASP is used to describe the domain and RL is used in the search for the optimal solution. Although both proposals combine similar tools, they differ from each other. While the current work formalizes an MDP in terms of the answer, the method proposed in [27,22] only finds an answer to the problem where each atom in this sentence defines a hierarchical POMDP that needs to be solved. A related approach is the combination of ASP with the cost of action [15,25]. Although this method also uses a logic program to describe the domain, it uses a method that differs from RL to find the cost of action."}, {"heading": "8 Conclusion", "text": "The proposed approach, called ASP (RL), uses a combination of Answer Set Programming (ASP) and Reinforcement Learning (RL), in which ASP provides the set of states and actions in areas where unforeseen changes may occur, while RL is used to approximate a value-action function through environmental interactions. In ASP (RL), Answer Set Programming is used as a tool for reasoning and knowledge revision and reinforcement learning to learn how to solve an MDP without the need for an explicit stationary reward function. Experiments have been conducted in a changing network world, the results of which show that using ASP to determine the set of states and actions effectively reduces the search space to find the optimal policy of Markov Decision Processes in uncomplex areas."}], "references": [{"title": "Cplus 2ASP: Computing action language C + in answer set programming", "author": ["J. Babb", "J. Lee"], "venue": "P. Cabalar, T.C. Son (eds.) Logic Programming and Nonmonotonic Reasoning, vol. 8148, pp. 122\u2013134. Springer Berlin Heidelberg", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Action language BC+", "author": ["J. Babb", "J. Lee"], "venue": "Journal of Logic and Computation", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Answer set based design of knowledge systems", "author": ["M. Balduccini", "M. Gelfond", "M. Nogueira"], "venue": "Annals of Mathematics and Artificial Intelligence 47(1-2), 183\u2013219", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Planning with the USA-Advisor", "author": ["M. Balduccini", "M. Gelfond", "M. Nogueira", "R. Watson"], "venue": "3rd NASA International workshop on Planning and Scheduling for Space. Houston, Texas", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "The USA-Advisor", "author": ["M. Balduccini", "M. Gelfond", "R. Watson", "M. Nogueira"], "venue": "G. Goos, J. Harmanis, J. van Leeuwen, T. Eiter, W. Faber, M.l. Truszczy\u0144ski (eds.) Logic Programming and Nonmotonic Reasoning, vol. 2173, pp. 439\u2013442. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic reasoning with answer sets", "author": ["C. Baral", "M. Gelfond", "N. Rushton"], "venue": "Theory and Practice of Logic Programming 9(1), 57", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "On the theory of dynamic programming", "author": ["R. Bellman"], "venue": "Proceedings of the National Academy of Sciences 38(8), 716\u2013719", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1952}, {"title": "A Markovian decision process", "author": ["R. Bellman"], "venue": "Indiana University Mathematics Journal 6(4), 679\u2013684", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1957}, {"title": "Applied dynamic programming, 4 edn", "author": ["R.E. Bellman", "S.E. Dreyfus"], "venue": "Princeton Univ. Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1971}, {"title": "Experts in a markov decision process", "author": ["E. Even-dar", "S.M. Kakade", "Y. Mansour"], "venue": "L.K. Saul, Y. Weiss, L. Bottou (eds.) Advances in Neural Information Processing Systems 17, pp. 401\u2013408. MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Online markov decision processes", "author": ["E. Even-Dar", "S.M. Kakade", "Y. Mansour"], "venue": "Mathematics of Operations Research 34(3), 726\u2013736", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Answer set solving in practice", "author": ["M. Gebser", "R. Kaminski", "B. Kaufmann"], "venue": "Morgan & Claypool Publishers", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "The stable model semantics for logic programming", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "R. Kowalski, Bowen, Kenneth (eds.) Proceedings of International Logic Programming Conference and Symposium, pp. 1070\u20131080. MIT Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1988}, {"title": "Causal and probabilistic reasoning in P-log", "author": ["M. Gelfond", "N. Rushton"], "venue": "Heuristics, Probabilities and Causality. A tribute to Judea Pearl pp. 337\u2013359", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Planning in action language BC while learning action costs for mobile robots", "author": ["P. Khandelwal", "F. Yang", "M. Leonetti", "V. Lifschitz", "P. Stone"], "venue": "Proceedings of the TwentyFourth International Conference on Automated Planning and Scheduling, ICAPS 2014, Portsmouth, New Hampshire, USA, June 21-26, 2014", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Answer set programming and plan generation", "author": ["V. Lifschitz"], "venue": "Artificial Intelligence 138(1\u20132), 39 \u2013 54", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Elaboration tolerance", "author": ["J. McCarthy"], "venue": "Proc. of the Fourth Symposium on Logical Formalizations of Commonsense Reasoning (Common Sense 98), vol. 98. London, UK", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "An A-Prolog decision support system for the space shuttle", "author": ["M. Nogueira", "M. Balduccini", "M. Gelfond", "R. Watson", "M. Barry"], "venue": "G. Goos, J. Hartmanis, J. van Leeuwen, I.V. Ramakrishnan (eds.) Practical Aspects of Declarative Languages, vol. 1990, pp. 169\u2013 183. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Rl-tops: An architecture for modularity and re-use in reinforcement learning", "author": ["M.R. Ryan", "M.D. Pendrith"], "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning, pp. 481\u2013487. Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Using abstract models of behaviours to automatically generate reinforcement learning hierarchies", "author": ["M.R.K. Ryan"], "venue": "In Proceedings of The 19th International Conference on Machine Learning, pp. 522\u2013529. Morgan Kaufmann", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Saturated path-constrained MDP: Planning under uncertainty and deterministic model-checking constraints", "author": ["J. Sprauel", "F. Teichteil-K\u00f6nigsbuch", "A. Kolobov"], "venue": "Proc. of 28th AAAI Conf. on Artificial Intelligence (AAAI), pp. 2367\u20132373", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Mixing non-monotonic logical reasoning and probabilistic planning for robots", "author": ["M. Sridharan", "M. Gelfond", "S. Zhang", "J. Wyatt"], "venue": "Workshop on Hybrid Reasoning @ IJCAI 2015", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning an introduction \u2013 Second edition, in progress (Draft)", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning from delayed rewards", "author": ["Watkins", "C.J.C.H."], "venue": "PhD thesis, University of Cambridge England", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1989}, {"title": "Planning in answer set programming while learning action costs for mobile robots", "author": ["F. Yang", "P. Khandelwal", "M. Leonetti", "P. Stone"], "venue": "AAAI Spring 2014 Symposium on Knowledge Representation and Reasoning in Robotics (AAAI-SSS)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Markov decision processes with arbitrary reward processes", "author": ["J.Y. Yu", "S. Mannor", "N. Shimkin"], "venue": "Mathematics of Operations Research 34(3), 737\u2013757", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Mixed logical inference and probabilistic planning for robots in unreliable worlds", "author": ["S. Zhang", "M. Sridharan", "J.L. Wyatt"], "venue": "IEEE Transactions on Robotics 31(3), 699\u2013713", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "John McCarthy defined Elaboration Tolerance as \u201cthe ability to accept changes to a person\u2019s or a computer program\u2019s representation of facts about a subject without having to start all over\u201d [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 8, "context": "As these changes may not be known a priori, the environment cannot be modelled as a stationary MDP due to the Curse of Dimensionality [9], which describes the growth in the set of states when considering the number of variables involved in the description of a state.", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "known as policy (\u03c0), is a sequence of non-deterministic actions that leads the agent from an initial state to a goal state [7,9].", "startOffset": 123, "endOffset": 128}, {"referenceID": 8, "context": "known as policy (\u03c0), is a sequence of non-deterministic actions that leads the agent from an initial state to a goal state [7,9].", "startOffset": 123, "endOffset": 128}, {"referenceID": 6, "context": "A problem such as this may have more than one feasible solution, thus it is possible to use the Bellman\u2019s Principle of Optimality [7,9] as a criterion to define which of the feasible policies can be considered as the optimal policy (\u03c0\u2217).", "startOffset": 130, "endOffset": 135}, {"referenceID": 8, "context": "A problem such as this may have more than one feasible solution, thus it is possible to use the Bellman\u2019s Principle of Optimality [7,9] as a criterion to define which of the feasible policies can be considered as the optimal policy (\u03c0\u2217).", "startOffset": 130, "endOffset": 135}, {"referenceID": 8, "context": "an optimal policy with regard to the state resulting from the first decision\u201d [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "Markov Decision Process (MDP) [8] can be used to formalise Sequential Decision Making Problems.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "\u2013 S is the set of states at any time step; \u2013 A is the set of allowed actions in the states s \u2208 S; \u2013 T : S\u00d7A\u00d7S 7\u2192 [0, 1] is the transition function that gives the probability of reaching the future state s\u2032 \u2208 S by performing action a \u2208 A in the current state s \u2208 S; \u2013 R : S \u00d7 A \u00d7 S 7\u2192 R is the reward function that returns a real value for reaching a state s\u2032 \u2208 S after performing an action a \u2208 A in a state s \u2208 S.", "startOffset": 113, "endOffset": 119}, {"referenceID": 22, "context": "Two well-known methods of RL are SARSA [23] and Q-Learning [24,23].", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "Two well-known methods of RL are SARSA [23] and Q-Learning [24,23].", "startOffset": 59, "endOffset": 66}, {"referenceID": 22, "context": "Two well-known methods of RL are SARSA [23] and Q-Learning [24,23].", "startOffset": 59, "endOffset": 66}, {"referenceID": 14, "context": "Answer Set Programming (ASP) is a declarative non-monotonic logic programming language that has been used with great success to describe and provide solutions for NP-complete problems, such as planning and scheduling [15,25].", "startOffset": 217, "endOffset": 224}, {"referenceID": 24, "context": "Answer Set Programming (ASP) is a declarative non-monotonic logic programming language that has been used with great success to describe and provide solutions for NP-complete problems, such as planning and scheduling [15,25].", "startOffset": 217, "endOffset": 224}, {"referenceID": 2, "context": "Furthermore, ASP can be used for problems with large search space, such as the Reaction Control System of a Space Shuttle [3,5,4,18].", "startOffset": 122, "endOffset": 132}, {"referenceID": 4, "context": "Furthermore, ASP can be used for problems with large search space, such as the Reaction Control System of a Space Shuttle [3,5,4,18].", "startOffset": 122, "endOffset": 132}, {"referenceID": 3, "context": "Furthermore, ASP can be used for problems with large search space, such as the Reaction Control System of a Space Shuttle [3,5,4,18].", "startOffset": 122, "endOffset": 132}, {"referenceID": 17, "context": "Furthermore, ASP can be used for problems with large search space, such as the Reaction Control System of a Space Shuttle [3,5,4,18].", "startOffset": 122, "endOffset": 132}, {"referenceID": 12, "context": "One important aspect of ASP is its non-monotonic semantics (based on the Stable Model Semantics [13]), which respects the rationality principle that states that \u201cone shall not believe anything one is not forced to believe\u201d [13].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "One important aspect of ASP is its non-monotonic semantics (based on the Stable Model Semantics [13]), which respects the rationality principle that states that \u201cone shall not believe anything one is not forced to believe\u201d [13].", "startOffset": 223, "endOffset": 227}, {"referenceID": 15, "context": "There are two types of negation in ASP: strong (or \u201cclassical\u201d) and weak, which in ASP represents negation as failure [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "Given an ASP program \u03a0 and a set M of atoms of \u03a0, a reduct program \u03a0M is obtained from \u03a0 by [13]:", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "The action language BC+ is defined over the stable model semantics and allows for some useful ASP constructs, such as a high-level description of actions and their effects, as a consequence of its structured abstract representation of transition systems [2].", "startOffset": 254, "endOffset": 257}, {"referenceID": 1, "context": "Given an action description D expressed in BC+, a stable model for the sequence PFm(D) of propositional formulae describes a path of length m in a transition system D [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "Logic programs were written in BC+ [2] and translated to ASP language using CPLUS2ASP [1], which uses iClingo [12] to find answer sets.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Logic programs were written in BC+ [2] and translated to ASP language using CPLUS2ASP [1], which uses iClingo [12] to find answer sets.", "startOffset": 86, "endOffset": 89}, {"referenceID": 11, "context": "Logic programs were written in BC+ [2] and translated to ASP language using CPLUS2ASP [1], which uses iClingo [12] to find answer sets.", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "The method proposed in this paper is in line with the work reported in [27,22] where ASP is used to find a description of the domain and RL is applied in the search for the optimal solution.", "startOffset": 71, "endOffset": 78}, {"referenceID": 21, "context": "The method proposed in this paper is in line with the work reported in [27,22] where ASP is used to find a description of the domain and RL is applied in the search for the optimal solution.", "startOffset": 71, "endOffset": 78}, {"referenceID": 26, "context": "While the present work formalises an MDP from the answer sets, the method proposed in [27,22] finds only one answer set for the problem, where each atom in this set defines a hierarchical POMDP that has to be solved.", "startOffset": 86, "endOffset": 93}, {"referenceID": 21, "context": "While the present work formalises an MDP from the answer sets, the method proposed in [27,22] finds only one answer set for the problem, where each atom in this set defines a hierarchical POMDP that has to be solved.", "startOffset": 86, "endOffset": 93}, {"referenceID": 14, "context": "A related approach is the combination of ASP with action costs [15,25].", "startOffset": 63, "endOffset": 70}, {"referenceID": 24, "context": "A related approach is the combination of ASP with action costs [15,25].", "startOffset": 63, "endOffset": 70}, {"referenceID": 13, "context": "Another work that also deals with sequential decision making is P-Log [14, 6] which calculates transition probabilities from sampling the environment, but without considering the cost of performing an action.", "startOffset": 70, "endOffset": 77}, {"referenceID": 5, "context": "Another work that also deals with sequential decision making is P-Log [14, 6] which calculates transition probabilities from sampling the environment, but without considering the cost of performing an action.", "startOffset": 70, "endOffset": 77}, {"referenceID": 20, "context": "Also related to our work is Saturated Path-Constrained MDP (SPC-MDP) [21].", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "However, while the approach described in [21] uses a Dynamic Programming algorithm to find the solutions, ASP(RL) uses the interaction with the environment in order to approximate the action-value function in non-stationary decision making problems, which (to the best of our knowledge) has never been attempted before.", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "Works that are somewhat related to our approach, but can be used when searching for the optimal policy are the ones that deals with changing reward functions, such as [10,11,26].", "startOffset": 167, "endOffset": 177}, {"referenceID": 10, "context": "Works that are somewhat related to our approach, but can be used when searching for the optimal policy are the ones that deals with changing reward functions, such as [10,11,26].", "startOffset": 167, "endOffset": 177}, {"referenceID": 25, "context": "Works that are somewhat related to our approach, but can be used when searching for the optimal policy are the ones that deals with changing reward functions, such as [10,11,26].", "startOffset": 167, "endOffset": 177}, {"referenceID": 18, "context": "Another approach that is somewhat related to ASP(RL) is hierarchical MDPs (such as the works of [19,20]), which can also be incorporated such as the method proposed by [27,22] described in the beginning of this section.", "startOffset": 96, "endOffset": 103}, {"referenceID": 19, "context": "Another approach that is somewhat related to ASP(RL) is hierarchical MDPs (such as the works of [19,20]), which can also be incorporated such as the method proposed by [27,22] described in the beginning of this section.", "startOffset": 96, "endOffset": 103}, {"referenceID": 26, "context": "Another approach that is somewhat related to ASP(RL) is hierarchical MDPs (such as the works of [19,20]), which can also be incorporated such as the method proposed by [27,22] described in the beginning of this section.", "startOffset": 168, "endOffset": 175}, {"referenceID": 21, "context": "Another approach that is somewhat related to ASP(RL) is hierarchical MDPs (such as the works of [19,20]), which can also be incorporated such as the method proposed by [27,22] described in the beginning of this section.", "startOffset": 168, "endOffset": 175}], "year": 2017, "abstractText": "Non-stationary domains, where unforeseen changes happen, present a challenge for agents to find an optimal policy for a sequential decision making problem. This work investigates a solution to this problem that combines Markov Decision Processes (MDP) and Reinforcement Learning (RL) with Answer Set Programming (ASP) in a method we call ASP(RL). In this method, Answer Set Programming is used to find the possible trajectories of an MDP, from where Reinforcement Learning is applied to learn the optimal policy of the problem. Results show that ASP(RL) is capable of efficiently finding the optimal solution of an MDP representing non-stationary domains.", "creator": "LaTeX with hyperref package"}}}