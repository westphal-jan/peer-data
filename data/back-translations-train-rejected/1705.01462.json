{"id": "1705.01462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Ternary Neural Networks with Fine-Grained Quantization", "abstract": "We propose a novel fine-grained quantization method for ternarizing pre-trained full precision models, while also constraining activations to 8-bits. Using this method, we demonstrate minimal loss in classification accuracy on state-of-the-art topologies without additional training. This enables a full 8-bit inference pipeline, with best reported accuracy using ternary weights on ImageNet dataset. Further, we also provide an improved theoretical formulation that forms the basis for a higher quality solution with this approach. Our method involves ternarizing the original weight tensor in groups of $N$ weights. Using $N=4$, we achieve Top-1 accuracy within $3.7\\%$ and $5.8\\%$ of the baseline full precision result for Resnet-101 and Resnet-50 respectively, while eliminating $75\\%$ of all multiplications. We also study the impact of group size on both performance and accuracy. With a group size of $N=64$, we eliminate $\\approx99\\%$ of the multiplications; however, this introduces a significant drop in accuracy, which necessitates fine tuning the parameters (re-training) at lower precision. To address this, we re-train Resnet-50 with 8-bit activations and ternary weights, improving the Top-1 accuracy to within $4\\%$ of the full precision result with $&lt;30\\%$ additional overhead. Our final quantized model can run on a full 8-bit compute pipeline using 2-bit weights and has the potential of up to $16\\times$ improvement in performance compared to baseline full-precision models.", "histories": [["v1", "Tue, 2 May 2017 10:15:21 GMT  (647kb,D)", "https://arxiv.org/abs/1705.01462v1", null], ["v2", "Thu, 11 May 2017 09:19:55 GMT  (652kb,D)", "http://arxiv.org/abs/1705.01462v2", null], ["v3", "Tue, 30 May 2017 17:10:24 GMT  (675kb,D)", "http://arxiv.org/abs/1705.01462v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["naveen mellempudi", "abhisek kundu", "dheevatsa mudigere", "dipankar das", "bharat kaul", "pradeep dubey"], "accepted": false, "id": "1705.01462"}, "pdf": {"name": "1705.01462.pdf", "metadata": {"source": "CRF", "title": "Ternary Neural Networks with Fine-Grained Quantization", "authors": ["Naveen Mellempudi", "Abhisek Kundu", "Dheevatsa Mudigere", "Dipankar Das", "Bharat Kaul", "Pradeep Dubey"], "emails": [], "sections": [{"heading": null, "text": "We propose a novel fine-grained quantization method (FGQ) to renarize prefabricated models with full accuracy while limiting activations to 8 and 4 bits at the same time. By this method, we show minimal losses in classification accuracy on state-of-the-art topologies without additional training. We offer an improved theoretical formulation that forms the basis for a higher-quality solution with FGQ. Our method involves ternarizing the original weight sensor in groups of N-weights. With N = 4, we achieve a top-1 accuracy within 3.7% or 4.2% of the base result with full accuracy for Resnet-101 or Resnet-50, respectively, while eliminating 75% of all multiplications. These results enable a complete 8 / 4-bit inference pipeline with the best reported accuracy using ternary weights on ImageNet datasets, with a potential of 9 x improvements in performance."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to live, in which they live, in which they live in which they live, in which they are able to move, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they"}, {"heading": "2 Related Work", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "3 Ternary Conversion of Trained Network", "text": "Our goal is to convert the weights W trained with full precision into ternary values {\u2212 \u03b1, 0, + \u03b1}, \u03b1 \u2265 0, without any retraining, using a threshold approach similar to [13]: the i-th element W-i = character (Wi), if | Wi | > \u0445, and 0 otherwise. Then, the elementary error E (\u03b1, \u0445) = HW-\u03b1W, + 2F and an optimal ternary representation \u03b1 W-1, 2,..., n (1) where n is the size of W (W-Rn) is assumed that weights learning different types of characteristics can follow different distributions. Combining all weights together represents a mixture of different distributions."}, {"heading": "3.1 Our Formulation", "text": "The calculation of the individual components W (i) and W (i) compensates for information losses and better maintains the underlying distributions. However, such a solution leads to a significant improvement in accuracy and does not reduce the number of multiplications leading to a less efficient implementation. Therefore, let us find a trade-off between achieving higher accuracy and reducing the total number of multiplications. We divide the number of n indices in k disjoint subsets, c1, c2, c2, ck, with cardinality."}, {"heading": "3.2 Weight Grouping", "text": "Our method (2) is agnostic about how the (complete precision) weights are grouped, but the effects of grouping - which allows them to be solved more efficiently as independent sub-problems - are an independent problem to be explored; the peculiarities of the activation mechanism and the memory layout used to access these weight groups are an independent problem to be explored.The primary goal of grouping is to minimize the dynamic range within each group and distribute the weights in such a way that the smaller groups have a uniform distribution, which helps reduce the complexity of finding an optimal solution (\u03b1) for each independent sub-problem by using either analytical or brute force techniques. However, it is essential to realize the full potential of accumulation to ensure that the grouping mechanism itself does not introduce a significant overhead. Similarity-based clustering algorithms such as K-agents, although they are better at finding an optimal grouping of weights, are not capable of implementing more efficiently even in a better way."}, {"heading": "4.1 Discussion", "text": "In order to realize the full performance potential of ternary networks, the inference platform must operate at throughput close to the precision of the weights, which would increase the amount of memory bandwidth required to enable activations by 16 \u00d7 and a computing engine that is much wider to deliver the desired throughput. Building such a solution around the full precision of activations would be prohibitive in terms of ranges and performance requirements, whereas it is easier to build such a solution if the activations are 8 or 4 bit.Figure4a shows that the performance accuracy Vs is commercially available for Resnet-50 [18] for an FGQ-based 8-bit inference design. Our model projects the lower limit of power potential based on the percentage of FMA operations that can be converted into ternary accumulations at any group size N. Ideally, where N equals the total number of weights in the layer, the best case potential would be 16 versus the base power x."}, {"heading": "5 Conclusion", "text": "We propose a fine-grained ternarization method that exploits local correlations in the dynamic range of parameters to minimize the impact of quantization on overall accuracy. We demonstrate near SOTA accuracy on Imagenet datasets using pre-trained models with quantified networks without retraining. Using external weights on Resnet-101 and Resnet-50 with 8-bit activations, our results are within \u2248 4% of full accuracy (FP32). With 4-bit activations, we see a further decrease in accuracy of approximately 3%. To the best of our knowledge, these are the highest accuracies reported using external weights and low precision activities. Our weight grouping approach allows us to obtain solutions that can be tailored to specific hardware, as well as to universal hardware based on the accuracy and performance requirements. Smaller group sizes with general gaps = 4% achieve the best computing accuracy in the larger range (75% and 75% are better suited to the larger ones)."}, {"heading": "5.1 Proof of Lemma 1", "text": "Let n specify the number of elements. Let f (x) = \u03bbe \u2212 \u03bbx be the pdf of the exponential distribution with the parameters \u03bb > 0 and F (x) = 1 \u2212 e \u2212 \u03bbx be the cdf. Then, | I \u00b2 n \u00b2 x > f (x) dx = n (1 \u2212 F (\u0445) = ne \u2212 \u03bb \u00b2 In addition, i \u00b2 I \u00b2 | Wi \u00b2 x > xf (x) dx = x \u00b2 (\u03bbe \u2212 \u03bbx) xdx \u00b2 (\u03bb + 1) e \u2212 \u03bb \u0432\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441flifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifli"}], "references": [{"title": "Deep learning. Book in preparation for", "author": ["Yoshua Bengio", "Ian Goodfellow", "Aaron Courville"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "8-bit approximations for parallelism in deep learning", "author": ["Tim Dettmers"], "venue": "arXiv preprint arXiv:1511.04561,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Binarized neural networks", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Google supercharges machine learning tasks with tpu custom chip", "author": ["N Jouppi"], "venue": "Google Blog, May,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Ternary weight networks", "author": ["Fengfu Li", "Bo Zhang", "Bin Liu"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["Daisuke Miyashita", "Edward H Lee", "Boris Murmann"], "venue": "arXiv preprint arXiv:1603.01025,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "In ECCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Imagenet pre-trained models with batch normalization", "author": ["Marcel Simon", "Erik Rodner", "Joachim Denzler"], "venue": "arXiv preprint arXiv:1612.01452v2,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Finn: A framework for fast, scalable binarized neural network inference", "author": ["Yaman Umuroglu", "Nicholas J Fraser", "Giulio Gambardella", "Michaela Blott", "Philip Leong", "Magnus Jahre", "Kees Vissers"], "venue": "arXiv preprint arXiv:1612.07119,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Accelerating deep convolutional networks using low-precision and sparsity", "author": ["Ganesh Venkatesh", "Eriko Nurvitadhi", "Debbie Marr"], "venue": "arXiv preprint arXiv:1610.00324,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Dynamically scaled fixed point arithmetic", "author": ["Darrell Williamson"], "venue": "In Communications, Computers and Signal Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "venue": "poster at International Conference on Learning Representations,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Trained ternary quantization", "author": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1612.01064,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Today\u2019s deep learning models achieve state-of-the-art results on a wide variety of tasks including Computer Vision, Natural Language Processing, Automatic Speech Recognition and Reinforcement Learning [1].", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 1, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 7, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 23, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 14, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 12, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 4, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 24, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 20, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 18, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 10, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 13, "context": "Most of the current solutions are focused on compressing the model [14, 16], going as low as binary weights, which allows storing the model on the limited on-chip local memory.", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "Most of the current solutions are focused on compressing the model [14, 16], going as low as binary weights, which allows storing the model on the limited on-chip local memory.", "startOffset": 67, "endOffset": 75}, {"referenceID": 7, "context": "There have been a few solutions [8, 9] using lower precision representation for activations, however they necessitate specialized hardware for efficient implementation.", "startOffset": 32, "endOffset": 38}, {"referenceID": 8, "context": "There have been a few solutions [8, 9] using lower precision representation for activations, however they necessitate specialized hardware for efficient implementation.", "startOffset": 32, "endOffset": 38}, {"referenceID": 10, "context": "To address both the aforementioned system and application requirements, there is a general trend to move towards a full lower precision inference pipeline [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "This is evident from the advent of 8-bit and sub 8-bit hardware such as Google\u2019s TPU [11] and other main stream GPU1, CPU offerings.", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "69% with 4-bit activations (2w-4a), on the ImageNet dataset[3] using a pre-trained Resnet-101 model (no re-training).", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "To the best of our knowledge, these are the highest reported accuracies in this category on ImageNet dataset[17].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "Demonstrate the general applicability of FGQ, with state-of-art results (2w-8a, 2w-4a) on smaller models such as Resnet-50 and Alexnet[12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "[20] have show that 8-bit dynamically scaled fixed point representation [22] can be used to speed up convolution neural networks using general purpose CPU hardware by carefully choosing right data layout, compute batching and using implementation optimized for target hardware.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[20] have show that 8-bit dynamically scaled fixed point representation [22] can be used to speed up convolution neural networks using general purpose CPU hardware by carefully choosing right data layout, compute batching and using implementation optimized for target hardware.", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "[5] have", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "There have also been recent efforts exploring 8-bit floating point representation [4], however such schemes have the additional overhead of reduced precision since the exponent is replicated for each value.", "startOffset": 82, "endOffset": 85}, {"referenceID": 13, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 15, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 8, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 1, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 7, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 24, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 5, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 22, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 20, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 5, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 14, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 15, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 13, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 3, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 13, "context": "[14, 16] propose low precision networks with binary weights, while retaining the activations in full precision.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[14, 16] propose low precision networks with binary weights, while retaining the activations in full precision.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[14] use a stochastic binarization scheme, achieving state-ofart (SOTA) accuracies on smaller data-sets (MNIST, CIFAR10, SVHN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] demonstrate near-SOTA accuracies on the large ImageNet data-set using AlexNet topology.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Lower precision for activations have also been used, [8] use 1-bit for both weights and activations for smaller networks.", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "For larger Imagenet-class networks [9], use 2-bit activations and binary weights showing reasonable accuracies.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "However, both these [8, 9] use specialized data representation requiring custom hardware for efficient implementation.", "startOffset": 20, "endOffset": 26}, {"referenceID": 8, "context": "However, both these [8, 9] use specialized data representation requiring custom hardware for efficient implementation.", "startOffset": 20, "endOffset": 26}, {"referenceID": 23, "context": "Other solutions such as [24], employ a more tailored approach with different precision for each - weights (1-bit), activations (2-bits) and gradients (6-bits); implemented with special-purpose hardware.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "[13] introduces a theoretical formulation for ternary weight network using a threshold based approach (symmetric threshold \u00b1\u2206) with one scaling factor for each layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "To increase model capacity [25] modify this solution to use two symmetric thresholds (\u00b1\u2206) and two scaling factors (separately for positive and negative weights).", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "[23] have proposed a post-facto incremental quantization approach, which aims to find the optimal representation using an iterative method, constraining weights to either 0 or powers of 2, using a 5-bit representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Alternatively, [15] used log quantization method on pre-trained models and achieved good accuracy by tuning the bit length for each layer without re-training.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "Achieving near-SOTA accuracy on the Imagenet dataset with deeper networks [7], without any training in low precision (for both weights and activations) is still a challenge.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "We use a threshold (\u2206 > 0) based approach similar to [13]: i-th element \u0174i = sign(Wi), if |Wi| > \u2206, and 0 otherwise.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "Note that for \u2206p = \u2206n = \u2206, I\u2206 = I \u2206 \u222a I \u2212 \u2206 , and (4) reproduces the formulation in ([13]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Based on such refined analysis, we observe significant improvement in the theoretical ternary error over Gaussian assumption of [13] (Figure 1).", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "Figure 3: Schematic describing our low precision experimental setup in Caffe, to emulate finegrained quantization (FGQ) with ternary weights and 8-bit activations For experimental results, we focused on Resnet50 and Resnet-101[7] using ILSVRC-2012[3] dataset, to demonstrate the efficacy of our method on large, sophisticated models using 2-bit weights and 8-bit activations (2w-8a).", "startOffset": 226, "endOffset": 229}, {"referenceID": 2, "context": "Figure 3: Schematic describing our low precision experimental setup in Caffe, to emulate finegrained quantization (FGQ) with ternary weights and 8-bit activations For experimental results, we focused on Resnet50 and Resnet-101[7] using ILSVRC-2012[3] dataset, to demonstrate the efficacy of our method on large, sophisticated models using 2-bit weights and 8-bit activations (2w-8a).", "startOffset": 247, "endOffset": 250}, {"referenceID": 11, "context": "Further, towards establishing the broader applicability of FGQ we demonstrate state-of-the-art accuracy also for Alexnet[12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Our setup consists of a modified version of Caffe[10] that emulates low-precision dynamic fixed point (DFP2) computations described in Fig.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "Networks Our Baseline FGQ-N4 2w-8a FGQ-N4 2w-4a INQ 5w-32a [23] dLAC 2w-32a [21] DoReFa 1w-4a-32g [24]", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "Networks Our Baseline FGQ-N4 2w-8a FGQ-N4 2w-4a INQ 5w-32a [23] dLAC 2w-32a [21] DoReFa 1w-4a-32g [24]", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Networks Our Baseline FGQ-N4 2w-8a FGQ-N4 2w-4a INQ 5w-32a [23] dLAC 2w-32a [21] DoReFa 1w-4a-32g [24]", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "the highest reported accuracies using 2w-8a and 2w-4a on Imagenet dataset[3] using state-of-the-art networks.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "To understand the general applicability of our method to a wider range of networks, we apply FGQ to the smaller Alexnet[12] model.", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Hence, we compare with [21, 23, 24], which are the closest in terms of the networks used and/or the target precision.", "startOffset": 23, "endOffset": 35}, {"referenceID": 22, "context": "Hence, we compare with [21, 23, 24], which are the closest in terms of the networks used and/or the target precision.", "startOffset": 23, "endOffset": 35}, {"referenceID": 23, "context": "Hence, we compare with [21, 23, 24], which are the closest in terms of the networks used and/or the target precision.", "startOffset": 23, "endOffset": 35}, {"referenceID": 23, "context": "Our Alexnet result using FGQ-N is comparable to previously published result[24] which is 6% away from the baseline using 1w-4a while also employing training in low-precision with full precision gradients.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "Table 1 has a comparison with previous reported results from[23] using 5-bit weights and [21] using ternary weights.", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "Table 1 has a comparison with previous reported results from[23] using 5-bit weights and [21] using ternary weights.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Figure4a shows the performance Vs accuracy trade-off for Resnet-50[18] for a FGQ based 8-bit inference design.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "For [23] the baseline full precision a Top-1 accuracy is 73.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "22%, for [21] it is 76% and for [24] it is 55.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "22%, for [21] it is 76% and for [24] it is 55.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "We have trained low-precision (2w-8a) ResNet-50[18] at group size N=64 on ImageNet[3] dataset to recover the accuracy lost because of ternarization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "We have trained low-precision (2w-8a) ResNet-50[18] at group size N=64 on ImageNet[3] dataset to recover the accuracy lost because of ternarization.", "startOffset": 82, "endOffset": 85}], "year": 2017, "abstractText": "We propose a novel fine-grained quantization (FGQ) method to ternarize pretrained full precision models, while also constraining activations to 8 and 4-bits. Using this method, we demonstrate minimal loss in classification accuracy on state-of-the-art topologies without additional training. We provide an improved theoretical formulation that forms the basis for a higher quality solution using FGQ. Our method involves ternarizing the original weight tensor in groups of N weights. Using N = 4, we achieve Top-1 accuracy within 3.7% and 4.2% of the baseline full precision result for Resnet-101 and Resnet-50 respectively, while eliminating 75% of all multiplications. These results enable a full 8/4-bit inference pipeline, with best reported accuracy using ternary weights on ImageNet dataset, with a potential of 9\u00d7 improvement in performance. Also, for smaller networks like AlexNet, FGQ achieves state-of-the-art results. We further study the impact of group size on both performance and accuracy. With a group size of N = 64, we eliminate \u2248 99% of the multiplications; however, this introduces a noticeable drop in accuracy, which necessitates fine tuning the parameters at lower precision. We address this by fine-tuning Resnet-50 with 8-bit activations and ternary weights at N = 64, improving the Top-1 accuracy to within 4% of the full precision result with < 30% additional training overhead. Our final quantized model can run on a full 8-bit compute pipeline using 2-bit weights and has the potential of up to 15\u00d7 improvement in performance compared to baseline full-precision models.", "creator": "LaTeX with hyperref package"}}}