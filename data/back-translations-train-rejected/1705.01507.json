{"id": "1705.01507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "XES Tensorflow - Process Prediction using the Tensorflow Deep-Learning Framework", "abstract": "Predicting the next activity of a running process is an important aspect of process management. Recently, artificial neural networks, so called deep-learning approaches, have been proposed to address this challenge. This demo paper describes a software application that applies the Tensorflow deep-learning framework to process prediction. The software application reads industry-standard XES files for training and presents the user with an easy-to-use graphical user interface for both training and prediction. The system provides several improvements over earlier work. This demo paper focuses on the software implementation and describes the architecture and user interface.", "histories": [["v1", "Wed, 3 May 2017 16:48:51 GMT  (385kb,D)", "http://arxiv.org/abs/1705.01507v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joerg evermann", "jana-rebecca rehse", "peter fettke"], "accepted": false, "id": "1705.01507"}, "pdf": {"name": "1705.01507.pdf", "metadata": {"source": "CRF", "title": "XES Tensorflow \u2013 Process Prediction using the Tensorflow Deep-Learning Framework", "authors": ["Joerg Evermann", "Jana-Rebecca Rehse", "Peter Fettke"], "emails": [], "sections": [{"heading": null, "text": "Keywords: process management, process intelligence, process prediction, deep learning, neural networks"}, {"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "2 Recurrent Neural Networks Overview", "text": "A recursive neural network (RNN) is one in which network cells can maintain state information by returning the state output to themselves, often by means of a form of long-term short-term memory cells (LSTM) [3]. To make this feedback comprehensible in the context of back-propagation, network cells are copied or \"rolled out\" for a number of steps. Figure 1 shows an example of RNN with LSTM cells; detailed descriptions can be found in [2,6].3 TensorflowOur software implementation is based on the open-source tensor flow deep learning framework5. Tensors are generalizations of matrices to more than two dimensions, i.e. n-dimensional arrays. A tensor flow application creates a computational graph using tensor operations. A loss function is a graph node that is optimized by the input of tenor values."}, {"heading": "4 Training", "text": "Our software system consists of two separate applications, one for training a deep learning model and another for predicting using a trained model.5 This section describes the training application.5 https: / / www.tensorflow.org"}, {"heading": "4.1 XES Parser", "text": "While the XES standard allows traces and events to omit globally declared attributes, it does not specify default values for missing attributes, so the XES parser does not consider incomplete or empty traces. String-typical attributes are treated as categorical variables, their unique values (categories) are numbered consecutively, each encoded as an integer in 0... li, where li is the number of unique values for attribute i. Date-typical attributes are converted to relative time differences from the previous event. Users can choose to standardize them or scale them to days, hours, minutes, or seconds to perform meaningful loss functions. Numeric attributes are standardized. In multi-attribute event classifiers, the parser constructs common attributes by concatenating or classifying them."}, {"heading": "4.2 Inputs", "text": "RNN Training runs in \"Epochs.\" In each epoch, the entire training data set is used. Before each epoch, the state of the RNN is reset to zero, while trained network parameter values are maintained. Within each epoch, the training runs in batches of size b with averaged gradients to avoid excessively large variations in gradients. Batch size b can be freely chosen by the user. For each unrolled step, the RNN accepts a floating point input tensor Is, i, Rb, p, where b is the batch size and p can be freely selected. Number and date time predictors are directly encoded, each yields a floating point sensor Is, i, i, Rb, 1 for predictor attributes i. Category attributes coded as integral category numbers are encoded using an embedding matrix egg, Rli, ki, ki, ki, ki, ki, ki, ki, ki, ki, ki."}, {"heading": "4.3 Model", "text": "In our approach, the user can simultaneously train a model based on several predicted event characteristics (\"target variables\"), such as predicting the activity and resource of the next event. For this, the user can choose whether to split the RNN layers between the different target variables or construct a separate RNN for each target variable."}, {"heading": "4.4 Outputs", "text": "For a categorical predicted variable i, it is multiplied by an output projection POs, i-Rp \u00b7 li to generate Os, i-Rb \u00b7 li = Os, i-Rb \u00b7 li = Os, i-Rb \u00b7 li = Os \u00b7 POs, i. A softmax function is used to generate probabilities over the li different categories Ss, i-Rb \u00b7 li = softmaxli (Os, i). Subsequently, a cross-entropy loss function Li = HSs, i (Ts, i) is applied, comparing the output probabilities with \"uniformly\" encoded targets T. The one-hot coding is a vector, where the element specifies the target category number one and the rest is set to zero. For numerical attributes, the output Os is multiplied by a POs, i-Rp \u00b7 1 output projection, giving OIS value (POIS = MIS), this POs = MAS (MIS = 2)."}, {"heading": "4.5 Logging and TensorBoard Integration", "text": "Our software logs summary information about the ratio of correct predictions and the value of the loss function for each training step. At the end of the training, the embedding matrices for all categorical predictor variables are stored together with the calculation curve. This information can be read by the Tensorboard tool (fig. 2) to visualize the training performance, graph structure and embedding matrices using t-SNE or the projection of major components in two or three dimensions. Finally, the entire trained network is stored in a \"frozen\" compact form that is loaded into the prediction application."}, {"heading": "5 Prediction", "text": "The prediction application loads a trained model that is stored by the training application, as well as the corresponding training configuration, and predicts trace suffixes from trace prefixes. Trace prefixes are read from XES files. As the trained model expects stacks of size b, a maximum of b trace prefixes are loaded at the same time. Network status is initialized to zero and trace prefixes are entered into the trained network, encoded as described in paragraph 4.2. Network output for the last element of a trace prefix is the prediction for the attributes of the next event. For categorical attributes, the integer output that specifies the category number is translated back to the string value. For file-time typical attributes, the attribute value is calculated by predicting the predicted value to the attribute value of the previous event. The prefix can then be added after the prefix has been predicted and the prefix can be repeated at any length."}, {"heading": "6 Software Implementation", "text": "Our software is implemented in Python 2.7 and therefore uses Tensorflow 0.12 and Tkinter for the user interface. Figure 3 shows the main configuration screen that guides the user through the selection of an XES file, the configuration of the RNN and training parameters, up to the formation of the model. Figure 4 shows the main configuration screen with sections for multi-attribute classifiers, global event and fall attributes, RNN and training parameters, and a choice of optimizer. Any classifier, global event and fall attributes can be included as a predictor, and classifiers and global event attributes can be selected as predicted attributes (targets). The user can specify the embedding dimensions for categorical attributes; the default values are the square root of the number of categories. The RNN configuration allows the user to predict the batch size, the number of unrolled steps, the number of NN levels, the number of training segments."}, {"heading": "7 Conclusion", "text": "We presented a flexible deep learning software for predicting business processes using industry standard XES event logs. The software provides an easy-to-use graphical user interface for configuring predictors, targets and parameters of the deep learning prediction method. We performed an initial validation of the software to verify the correct functioning of the software tool.Using the 2012 and 2013 BPIC data sets with the model and training parameters reported in [2], this software tool replicates their training results in [2]. Current work with this software tool is ongoing and focuses on using various combinations of predictors and targets enabled by our flexible approach to handling predictors and generating RNN inputs to improve state-of-the-art prediction performance."}, {"heading": "1. Evermann, J., Rehse, J.R., Fettke, P.: A deep learning approach for predicting process behaviour at runtime. In: PRAISE Workshop at the 14th International Conference on BPM (2016)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2. Evermann, J., Rehse, J.R., Fettke, P.: Predicting process behaviour using deep learning. Decision Support Systems (2017), (in press)", "text": "3. Hochreiter, S., Schmidhuber, J.: Long-term memory. Neural Computation 9 (8), 1735-1780 (1997) 4. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521, 436-444 (2015) 5. Schmidhuber, J.: Deep learning in neural networks: An overview. Neural Networks61, 85-117 (2015) 6. Tax, N., Verenich, I., Rosa, M.L., Dumas, M.: Predictive Business Process Monitoring with LSTM Neural Networks. CoRR abs / 1612.02130 (2016) 7. XES Working Group: IEEE standard for eXtensible Event Stream (XES) for achieving interoperability in event logs and event streams. IEEE Std 1849-2016 (2016)"}], "references": [{"title": "A deep learning approach for predicting process behaviour at runtime", "author": ["J. Evermann", "J.R. Rehse", "P. Fettke"], "venue": "PRAISE Workshop at the 14th International Conference on BPM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting process behaviour using deep learning", "author": ["J. Evermann", "J.R. Rehse", "P. Fettke"], "venue": "Decision Support Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8), 1735\u20131780", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521, 436\u2013444", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61, 85\u2013117", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Predictive business process monitoring with LSTM neural networks", "author": ["N. Tax", "I. Verenich", "M.L. Rosa", "M. Dumas"], "venue": "CoRR abs/1612.02130", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "IEEE standard for eXtensible Event Stream (XES) for achieving interoperability in event logs and event streams", "author": ["XES Working Group"], "venue": "IEEE Std 1849-2016", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Recently, deep learning [4,5] with artificial neural networks has become an import predictive method due to innovations in neural network architectures, the availability of high-performance GPU and cluster-based systems, and the opensourcing of of multiple software frameworks at a high level of abstraction.", "startOffset": 24, "endOffset": 29}, {"referenceID": 4, "context": "Recently, deep learning [4,5] with artificial neural networks has become an import predictive method due to innovations in neural network architectures, the availability of high-performance GPU and cluster-based systems, and the opensourcing of of multiple software frameworks at a high level of abstraction.", "startOffset": 24, "endOffset": 29}, {"referenceID": 0, "context": "An initial application of RNN to BPI [1] used the executing activity of each trace event as both predictor and predicted variable.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "A more systematic study [2], including cross validation, showed significant overfitting of the earlier results, i.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Both [2] and [1] use approaches in which each \u201cword\u201d (e.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Both [2] and [1] use approaches in which each \u201cword\u201d (e.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "In contrast, an independent, parallel effort [6] eschews the use of embeddings, encoding event information as numbered categories in one-hot form.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "Additionally, [2] demonstrates that predicted suffixes are similar to targets by showing similar replay fitness on a model mined from the target traces.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "Whereas [2,6] use only the activity name and lifecycle transition of an event, and [2] also includes resource information of an event as predictor, our application can use any case- or event-level attribute as predictor.", "startOffset": 8, "endOffset": 13}, {"referenceID": 5, "context": "Whereas [2,6] use only the activity name and lifecycle transition of an event, and [2] also includes resource information of an event as predictor, our application can use any case- or event-level attribute as predictor.", "startOffset": 8, "endOffset": 13}, {"referenceID": 1, "context": "Whereas [2,6] use only the activity name and lifecycle transition of an event, and [2] also includes resource information of an event as predictor, our application can use any case- or event-level attribute as predictor.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "Whereas [2] concatenates activity name, lifecycle transition and resource information into an input string, which is then assigned a category number and embedded in a vector space for input to the neural network, our approach assigns category numbers to each input attribute separately, then embeds them into their own vector spaces and concatenates the embedding vectors to form the input vector.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "4), as this is where our approach differs from [2,6].", "startOffset": 47, "endOffset": 52}, {"referenceID": 5, "context": "4), as this is where our approach differs from [2,6].", "startOffset": 47, "endOffset": 52}, {"referenceID": 3, "context": "More details on neural networks in general can be found in [4,5], and, applied to process prediction, in [2,6].", "startOffset": 59, "endOffset": 64}, {"referenceID": 4, "context": "More details on neural networks in general can be found in [4,5], and, applied to process prediction, in [2,6].", "startOffset": 59, "endOffset": 64}, {"referenceID": 1, "context": "More details on neural networks in general can be found in [4,5], and, applied to process prediction, in [2,6].", "startOffset": 105, "endOffset": 110}, {"referenceID": 5, "context": "More details on neural networks in general can be found in [4,5], and, applied to process prediction, in [2,6].", "startOffset": 105, "endOffset": 110}, {"referenceID": 2, "context": "A recurrent neural network (RNN) is one in which network cells can maintain state information by feeding the state output back to themselves, often using a form of long short term memory (LSTM) cells [3].", "startOffset": 200, "endOffset": 203}, {"referenceID": 1, "context": "1 shows an example RNN with LSTM cells; detailed descriptions can be found in [2,6].", "startOffset": 78, "endOffset": 83}, {"referenceID": 5, "context": "1 shows an example RNN with LSTM cells; detailed descriptions can be found in [2,6].", "startOffset": 78, "endOffset": 83}, {"referenceID": 6, "context": "Training data is read from XES log files [7], beginning with global attribute and event classifier definitions.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "Using the BPIC 2012 and 2013 datasets with the model and training parameters reported in [2], this software tool replicates their training results in [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Using the BPIC 2012 and 2013 datasets with the model and training parameters reported in [2], this software tool replicates their training results in [2].", "startOffset": 150, "endOffset": 153}], "year": 2017, "abstractText": "Predicting the next activity of a running process is an important aspect of process management. Recently, artificial neural networks, so called deep-learning approaches, have been proposed to address this challenge. This demo paper describes a software application that applies the Tensorflow deep-learning framework to process prediction. The software application reads industry-standard XES files for training and presents the user with an easy-to-use graphical user interface for both training and prediction. The system provides several improvements over earlier work. This demo paper focuses on the software implementation and describes the architecture and user interface.", "creator": "LaTeX with hyperref package"}}}