{"id": "1705.01626", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks", "abstract": "Popular deep learning frameworks require users to fine-tune their memory usage so that the training data of a deep neural network (DNN) fits within the GPU physical memory. Prior work tries to address this restriction by virtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be utilized for memory allocations. Despite its merits, virtualizing memory can incur significant performance overheads when the time needed to copy data back and forth from CPU memory is higher than the latency to perform the computations required for DNN forward and backward propagation. We introduce a high-performance virtualization strategy based on a \"compressing DMA engine\" (cDMA) that drastically reduces the size of the data structures that are targeted for CPU-side allocations. The cDMA engine offers an average 2.6x (maximum 13.8x) compression ratio by exploiting the sparsity inherent in offloaded data, improving the performance of virtualized DNNs by an average 32% (maximum 61%).", "histories": [["v1", "Wed, 3 May 2017 21:07:47 GMT  (3923kb,D)", "http://arxiv.org/abs/1705.01626v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AR", "authors": ["minsoo rhu", "mike o'connor", "niladrish chatterjee", "jeff pool", "stephen w keckler"], "accepted": false, "id": "1705.01626"}, "pdf": {"name": "1705.01626.pdf", "metadata": {"source": "CRF", "title": "Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks", "authors": ["Minsoo Rhu", "Mike O\u2019Connor", "Niladrish Chatterjee", "Jeff Pool", "Stephen W. Keckler"], "emails": ["skeckler}@nvidia.com"], "sections": [{"heading": null, "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "A. Deep Neural Networks", "text": "Today's most popular deep neural networks can generally be categorized as Convolutionary Neural Networks (CNNs) for image recognition, or Recursive Neural Networks (RNNs) for video subtitling, speech recognition, and natural language processing. Both CNNs and RNNs are designed with a combination of multiple layers, especially the Convolutionary Layers (CONV), Activation Layers (ACTV), Pooling Layers (POOL), and Fully Connected Layers (FC). A deep neural network is divided into two functional modules: (a) the Extraction Layers, which learn to extract meaningful features from an input, and (b) the Classification Layers, which analyze and classify the extracted functions to categorize input to a predefined initial category."}, {"heading": "B. Training versus Inference", "text": "A neural network requires training that is used for an inference task. Training a DNN involves learning and updating the weights of the network, which is typically done using the backpropagation algorithm [16]. Figure 1 shows the three-step process for each training pass: (1) forward propagation, (2) deriving the error magnitude between incrementalization and soil truth, and (3) propagation of the inference error backwards across the backward propagation network. Forward propagation is a serialized, layer-by-layer calculation process that is performed from the first (input) layer to the last (output) layer in a sequential manner (from left to right in Figure 1). Each layer applies a mathematical operation (such as a convoluted operation for CONV layers) to achieve input activation."}, {"heading": "C. Data Layout for Activation Maps", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "A. Case Study: Activation Sparsity in AlexNet", "text": "Figure 4 shows the change in the average output density over time as the network is formed for a better image classification. We define the average output density (AVGDensity) as the number of output activations divided by the total activation of the same 50 images. Accordingly, the average activation economy is equal (1 \u2212 AVGDensity). Figure 5 shows a visualization of the activity over time (X axis), and the spatial distribution within each activation card."}, {"heading": "B. Effects of Training on Sparsity", "text": "In addition to AlexNet, we investigated the rarity of activations for larger, deeper, and newer CNNs, including OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14]. Figure 6 shows that the per-shift thriftiness measurements of these networks are very similar, reinforcing the above observations. In the six networks we examine in this paper, we observe an average activation span of 62% (maximum 93%) over the entire training periods. Figure 7 shows that AlexNet's behavior as a function of training time, including the loss value calculated at the end of the network and the activation densities of the four convoluted layers."}, {"heading": "V. COMPRESSING DMA ENGINE", "text": "To address the performance bottlenecks associated with moving activation cards between the GPU and CPU memory, we use the few activation cards to compress them before transferring them over the PCIe bus. The general approach is similar to compressing pages before moving them to the memory of a virtual storage system [35]. Our compression DMA engine (cDMA) requires the choice of an efficient and effective compression algorithm and a mechanism to use this algorithm to compress activation cards as they move between the GPU and CPU memory."}, {"heading": "A. Compression Algorithm", "text": "This year, the number of job-related redundancies is many times higher than in previous years."}, {"heading": "B. Compressing DMA Engine Architecture", "text": "This year, it has come to the point where there is only one person who will be able to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to see the world, to explore the world, to discover the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to see the world, to see the world, to see the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world."}, {"heading": "C. Design Overheads", "text": "(de) it is expected that the existing GPU compression units for cDMA will be used to minimize design costs, we will assume that the cDMA compression hardware will complement the existing hardware for conservative estimation, however, our cDMA unit may enable existing DRAM compression measures to minimize DRAM bandwidth. (d) it will use the FreePDK process kits and cover the resulting range with a conservative cell size reduction from 0.46 nm to 28 nm. (d) it will dominate 50% cell usage through the design. (d) it will include the six (d) compression units. (d)"}, {"heading": "D. Software Interface", "text": "This year it is more than ever before in the history of the city."}, {"heading": "VII. RESULTS", "text": "This section evaluates the efficiency of cDMA compression, the savings in PCIe traffic, and the impact of cDMA on energy efficiency and performance. The three compression algorithms discussed in Section V. are referred to as RL (runlength encoding), ZV (zero value compression), and ZL (zlib) in all the numbers discussed in this paragraph. vDNN is evaluated using the memory management policy that provides memory scalability that outsources all activation cards. We have also established an oracular baseline (orac) that eliminates the PCIe bottleneck completely by always hiding offload / prefetch latency in performance measurement within the DNN calculation."}, {"heading": "A. Compression Efficiency", "text": "Figure 11 shows the maximum compression ratio per layer that has been achieved in a given network, and the average network-wide compression ratio for each of the three compression algorithms and three data layouts. While the results presented in this section assume a 4KB compression window, we also examined window sizes of up to 64KB and found that our results did not change greatly. The maximum network-wide compression ratio per layer determines how much DRAM bandwidth cDMA must provide to generate the compressed capitalizations at a sufficiently high rate to fully saturate the PCIe bandwidth. The average network-wide compression ratio reflects the reduction in PCIe traffic provided by cDMA. Overall, our ZVC algorithm provides the best average compression rate in all three data layouts (an average of 2.6x). Despite its simple design, the efficiency of NetZNet NetVC-NetNetNetNet-NetCompression-Net-Net-NetIR provides the net-netnet capitalizations in all three data layouts (an average of 2.6x)."}, {"heading": "B. Performance", "text": "Figure 13 summarizes the performance of cDMA compared to vDNN and the oracular baseline. While zlib offers the highest compression rate for SqueezeNet and GoogLeNet (8% and 30% higher than ZVC), the resulting performance improvements are marginal, offering an average acceleration of 0.7% over ZVC (maximum 2.2% for GoogLeNet). zlib's meager performance advantage is twofold: (1) a significant fraction of the swapping latency is already obscured by the DNN forward and backward propagation operations, and (2) the higher compression rates achieved by zlib are the best option for layers where RLE and ZVC are already able to largely hide the swapping latencies. ZVC's simple compression algorithm and robustness across different data layouts make it the best option for DNN virtualization."}, {"heading": "C. Energy Efficiency", "text": "The current CUDA software stack does not allow users to change the DRAM read bandwidth or PCIe transfer bandwidth, making it difficult to accurately measure the impact of cDMA on energy efficiency. Instead, we offer a qualitative comparison of cDMA energy efficiency versus vDNN. The primary energy costs that cDMA imposes on the vDNN are (1) the average 2.6-fold increase in DRAM read bandwidth, corresponding to the average network-wide compression rate of ZVC, for fetching activations from DRAM for cDMA compression; and (2) the (de) compression units and buffers within the GPU. Based on the analysis of cDMA's overheads in Section V-C, we expect the energy costs of the additional compression logic and its buffers to be negligible, as cDMA primarily uses the existing compression units in GPU memory controllers."}, {"heading": "VIII. RELATED WORK", "text": "This year, it has reached the stage where it will be able to take the lead."}], "references": [{"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS), December 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Framewise Phoneme Classification With Bidirectional LSTM and Other Neural Network Architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, pp. 602\u2013610, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu", "C. Zhang", "Z. Zhang"], "venue": "Proceedings of the Workshop on Machine Learning Systems, December 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design", "author": ["M. Rhu", "N. Gimelshein", "J. Clemons", "A. Zulfiqar", "S.W. Keckler"], "venue": "Proceedings of the International Symposium on Microarchitecture (MICRO), October 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition.", "author": ["K. Simonyan", "A. Zisserman"], "venue": "https://arxiv. org/abs/1409.1556,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient- Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, November 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS), December 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Architectural Support for Address Translation on GPUs: Designing Memory Manage- 12  ment Units for CPU/GPUs with Unified Address Spaces", "author": ["B. Pichai", "L. Hsu", "A. Bhattacharjee"], "venue": "Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS), March 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Supporting x86-64 Address Translation for 100s of GPU Lanes", "author": ["J. Power", "M. Hill", "D. Wood"], "venue": "Proceedings of the International Symposium on High-Performance Computer Architecture (HPCA), February 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward High-Performance Paged-Memory for GPUs", "author": ["T. Zheng", "D. Nellans", "A. Zulfiqar", "M. Stephenson", "S.W. Keckler"], "venue": "Proceedings of the International Symposium on High-Performance Computer Architecture (HPCA), March 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical Evaluation of Rectified Activations in Convolutional Network.", "author": ["B. Xu", "N. Wang", "T. Chen", "M. Li"], "venue": "https://arxiv. org/abs/1505.00853,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Deeply Learned Face Representations Are Sparse, Selective, and Robust", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Cnvlutin: Ineffectual-Neuron-Free Deep Convolutional Neural Network Computing", "author": ["J. Albericio", "P. Judd", "T. Hetherington", "T. Aamodt", "N.E. Jerger", "A. Moshovos"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Speech 2: End-To-En Speech Recognition in English and Mandarin.", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Persistent RNNs: Stashing Recurrent Weights On-Chip", "author": ["G. Diamos", "S. Sengupta", "B. Catanzaro", "M. Chrzanowski", "A. Coates", "E. Elsen", "J. Engel", "A. Hannun", "S. Satheesh"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), June 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Long Short Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780, November 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "https://arxiv.org/abs/ 1312.6229,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Network in Network.", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "https: //arxiv.org/abs/1312.4400,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and <0.5MB Model Size.", "author": ["F. Iandola", "S. Han", "M. Moskewicz", "K. Ashraf", "W.J. Dally", "K. Keutzer"], "venue": "https://arxiv.org/ abs/1602.07360,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "The Case for Compressed Cache in Virtual Memory Systems", "author": ["P. Wilson", "S. Kaplan", "Y. Smaragdakis"], "venue": "Proceedings of USENIX, June 1999.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1999}, {"title": "Results of a Prototype Television Bandwidth Compression Scheme", "author": ["A. Robinson", "C. Cherry"], "venue": "Proceedings of the IEEE, vol. 55, pp. 356\u2013364, March 1967.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1967}, {"title": "Frequent Value Locality and  Value-centric Data Cache Design", "author": ["Y. Zhang", "J. Yang", "R. Gupta"], "venue": "Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS), November 2000.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Gzip on a Chip: High Performance Lossless Data Compression on FPGAs Using OpenCL", "author": ["M. Abdelfattah", "A. Hagiescu", "D. Singh"], "venue": "Proceedings of the International Workshop on OpenCL, May 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Lossless and Lossy Memory I/O Link Compression for Improving Performance of GPGPU Workloads", "author": ["V. Sathish", "M. Schulte", "N. Kim"], "venue": "Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), September 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "A Case for Toggle-Aware Compression for GPU Systems", "author": ["G. Pekhimenko", "E. Bolotin", "N. Vijaykumar", "O. Mutlu", "T.C. Mowry", "S.W. Keckler"], "venue": "Proceedings of the International Symposium on High-Performance Computer Architecture (HPCA), March 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Demystifying GPU Microarchitecture Through Microbenchmarking", "author": ["H. Wong", "M.M. Papadopoulou", "M. Sadooghi-Alvandi", "A. Moshovos"], "venue": "Proceedings of the International Symposium on Performance Analysis of Systems Software (ISPASS), March 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "CACTI: An Integrated Cache and Memory Access Time, Cycle Time, Area, Leakage, and Dynamic Power Model.", "author": ["HP Labs"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, June 2014.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1929}, {"title": "Improving the Speed of Neural Networks on CPUs", "author": ["V. Vanhoucke", "A. Senior", "M. Mao"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, December 2011.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Compressing Deep Convolutional Networks Using Vector Quantization.", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "https:// arxiv.org/abs/1412.6115,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Comparing Biases for Minimal Network Construction with Back-propagation", "author": ["S. Hanson", "L. Pratt"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS), November 1989.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimal Brain Damage", "author": ["Y. LeCun", "S. Denker", "S. Solla"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS), November 1990.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1990}, {"title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon", "author": ["B. Hassibi", "D. Stork"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS), November 1993.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning Both Weights and Connections for Efficient Neural Networks", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS), December 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization 13  and Huffman Coding", "author": ["S. Han", "H. Mao", "W. Dally"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR), May 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "DianNao: A Small-footprint High-throughput Accelerator for Ubiquitous Machine-learning", "author": ["T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Y. Chen", "O. Temam"], "venue": "Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS), March 2014.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Y. Chen", "T. Luo", "S. Liu", "S. Zhang", "L. He", "J. Wang", "L. Li", "T. Chen", "Z. Xu", "N. Sun", "O. Temam"], "venue": "Proceedings of the International Symposium on Microarchitecture (MICRO), December 2014.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Eyeriss: An Energy- Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Y. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "Proceedings of the International Solid State Circuits Conference (ISSCC), February 2016.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M. Horowitz", "W. Dally"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2016.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2016}, {"title": "Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks", "author": ["Y. Chen", "J. Emer", "V. Sze"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2016.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2016}, {"title": "RedEye: Analog ConvNet Image Sensor Architecture for Continuous Mobile Vision", "author": ["R. LiKamWa", "Y. Hou", "M. Polansky", "Y. Gao", "L. Zhong"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2016.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Minerva: Enabling Low-Power, High-Accuracy Deep Neural Network Accelerators", "author": ["B. Reagen", "P. Whatmough", "R. Adolf", "S. Rama", "H. Lee", "S. Lee", "J. Miguel", "H. Lobato", "G. Wei", "D. Brooks"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2016.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory", "author": ["P. Chi", "S. Li", "C. Xu", "T. Zhang", "J. Zhao", "Y. Liu", "Y. Wang", "Y. Xie"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2016.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars", "author": ["A. Shafiee", "A. Nag", "N. Muralimanohar", "R. Balasubramonian", "J.P. Strachan", "M. Hu", "R.S. Williams", "V. Srikumar"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2016.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "ShiDianNao: Shifting Vision Processing Closer to the Sensor", "author": ["Z. Du", "R. Fasthuber", "T. Chen", "P. Ienne", "L. Li", "T. Luo", "X. Feng", "Y. Chen", "O. Temam"], "venue": "Proceedings of the International Symposium on Computer Architecture (ISCA), June 2015.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "One Weird Trick For Parallelizing Convolutional Neural Networks.", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) are now the driving technology for numerous application domains, such as computer vision [1], speech recognition [2], and natural language processing [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Deep neural networks (DNNs) are now the driving technology for numerous application domains, such as computer vision [1], speech recognition [2], and natural language processing [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "To facilitate the design and study of DNNs, a large number of machine learning (ML) frameworks [4, 5, 6, 7, 8, 9, 10] have been developed in recent years.", "startOffset": 95, "endOffset": 117}, {"referenceID": 3, "context": "training, prior work proposed to virtualize the memory usage of DNNs (vDNN) such that ML researchers can train larger and deeper neural networks beyond what is afforded by the physical limits of GPU memory [12].", "startOffset": 206, "endOffset": 210}, {"referenceID": 4, "context": "The trend in deep learning is to employ larger and deeper networks that leads to large memory footprints that oversubscribe GPU memory [13, 14, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 5, "context": "The trend in deep learning is to employ larger and deeper networks that leads to large memory footprints that oversubscribe GPU memory [13, 14, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 0, "context": "comes from the ReLU [1] layers that are extensively used in DNNs.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": ", sigmoid, tanh, and ReLU [1]) to the input feature maps.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "Training a DNN involves learning and updating the weights of the network, which is typically done using the backpropagation algorithm [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 7, "context": "agation algorithm and how contemporary GPUs implement each layer\u2019s DNN computations and memory allocations can be found in [17, 12].", "startOffset": 123, "endOffset": 131}, {"referenceID": 3, "context": "agation algorithm and how contemporary GPUs implement each layer\u2019s DNN computations and memory allocations can be found in [17, 12].", "startOffset": 123, "endOffset": 131}, {"referenceID": 8, "context": "[19] and Power et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[20] proposed TLB designs that leverage the unique memory access patterns of GPUs for optimizing the throughput of memory address translations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[21] studied architectural solutions for closing the performance gap between page-migration based virtual memory and software-directed direct-memory-access (DMA) copy operations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[12] therefore proposed an application-level virtual memory management solution specifically tailored for", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "maps occupy more than 90% of the GPU-side memory allocations [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "8GB/sec [12] versus 200 MB/sec [21]) as the data movements are orchestrated by GPU\u2019s DMA copyengine.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "8GB/sec [12] versus 200 MB/sec [21]) as the data movements are orchestrated by GPU\u2019s DMA copyengine.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "networks [12], are amenable for compression, which will drastically alleviate the PCIe bottleneck of virtualized DNNs.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "As noted by multiple prior works [22, 23, 24], such sparsity of activations are originated", "startOffset": 33, "endOffset": 45}, {"referenceID": 12, "context": "As noted by multiple prior works [22, 23, 24], such sparsity of activations are originated", "startOffset": 33, "endOffset": 45}, {"referenceID": 13, "context": "As noted by multiple prior works [22, 23, 24], such sparsity of activations are originated", "startOffset": 33, "endOffset": 45}, {"referenceID": 0, "context": "by the extensive use of ReLU [1] layers that follow (almost) every single layer in the feature extraction modules.", "startOffset": 29, "endOffset": 32}, {"referenceID": 14, "context": "Nonetheless, we believe our proposal is equally applicable for some popular recurrent neural networks that extensively employ sparsity-inducing ReLU layers, including the GEMVbased (general-matrix-vector-multiplication) RNNs employed by Baidu for speech recognition [26, 27] and language translation [28] services.", "startOffset": 266, "endOffset": 274}, {"referenceID": 15, "context": "Nonetheless, we believe our proposal is equally applicable for some popular recurrent neural networks that extensively employ sparsity-inducing ReLU layers, including the GEMVbased (general-matrix-vector-multiplication) RNNs employed by Baidu for speech recognition [26, 27] and language translation [28] services.", "startOffset": 300, "endOffset": 304}, {"referenceID": 16, "context": "cDMA is less well-suited for RNNs based on LSTMs [29] or GRUs [30], as they employ sigmoid and tanh activation functions rather than ReLUs.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "sparsity by using AlexNet [1] as a running example.", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "1, when the validation accuracy stops increasing with the current learning rate [1, 34].", "startOffset": 80, "endOffset": 87}, {"referenceID": 20, "context": "The overall approach is somewhat similar to compressing pages prior to moving them to backing storage in a virtual memory system [35].", "startOffset": 129, "endOffset": 133}, {"referenceID": 21, "context": "As a result, we investigate a simple scheme using run-length encoding (RLE) [36] to compress the activation maps.", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "We therefore investigate a simple yet highly effective approach based on Frequent-value compression [37] that is used to compress out the zero-valued elements.", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "Dedicated FPGA and ASIC solutions [39, 40] are capable of reaching approximately 2.", "startOffset": 34, "endOffset": 42}, {"referenceID": 24, "context": "GPUs already perform compression operations within the memory controllers today [41, 42, 43], but the compression operations of our cDMA are somewhat backwards compared to existing systems.", "startOffset": 80, "endOffset": 92}, {"referenceID": 25, "context": "GPUs already perform compression operations within the memory controllers today [41, 42, 43], but the compression operations of our cDMA are somewhat backwards compared to existing systems.", "startOffset": 80, "endOffset": 92}, {"referenceID": 26, "context": "As a result, based on a 350 ns latency from the time the DMA engine requests data from GPU memory to the time it arrives at the DMA engine [45] and the 200GB/sec compression read bandwidth, the DMA engine needs a 70KB (200GB/sec\u00d7350 ns) buffer, shown as block \u201cB\u201d in Figure 9.", "startOffset": 139, "endOffset": 143}, {"referenceID": 27, "context": "3 [46]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 3, "context": "We faithfully model the vDNN memory management policy as described in [12], which is interfaced to the latest version of cuDNN (v5) [11].", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "State-of-the-art DNN libraries refactor the convolution operations into a dense matrix-multiplication operation for GPU acceleration [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "[12], leaving more than an average 336 \u2212 100 = 236 GB/sec of memory bandwidth available for our cDMA engine to fetch activation maps from the GPU memory without affecting the throughput of DNN computations using cuDNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Dropout [50] is employed for the fully-connected layers with a rate of 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "We study DNNs that show state-ofthe-art performance in ImageNet [25]: AlexNet [1], OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 78, "endOffset": 81}, {"referenceID": 17, "context": "We study DNNs that show state-ofthe-art performance in ImageNet [25]: AlexNet [1], OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "We study DNNs that show state-ofthe-art performance in ImageNet [25]: AlexNet [1], OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "We study DNNs that show state-ofthe-art performance in ImageNet [25]: AlexNet [1], OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "We study DNNs that show state-ofthe-art performance in ImageNet [25]: AlexNet [1], OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 5, "context": "We study DNNs that show state-ofthe-art performance in ImageNet [25]: AlexNet [1], OverFeat [31], NiN [32], VGG [13], SqueezeNet [33], and GoogLeNet [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 18, "context": "at the original authors\u2019 websites [32, 33].", "startOffset": 34, "endOffset": 42}, {"referenceID": 19, "context": "at the original authors\u2019 websites [32, 33].", "startOffset": 34, "endOffset": 42}, {"referenceID": 29, "context": "[51] explored quantization in activations by fixing the data type to a 8-bit integer as opposed to 32-bit floating point.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[52] proposed vector quantization methods for compressing the weights of DNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Network pruning strategies have also been explored extensively by prior literature [53, 54, 55, 56, 57].", "startOffset": 83, "endOffset": 103}, {"referenceID": 32, "context": "Network pruning strategies have also been explored extensively by prior literature [53, 54, 55, 56, 57].", "startOffset": 83, "endOffset": 103}, {"referenceID": 33, "context": "Network pruning strategies have also been explored extensively by prior literature [53, 54, 55, 56, 57].", "startOffset": 83, "endOffset": 103}, {"referenceID": 34, "context": "Network pruning strategies have also been explored extensively by prior literature [53, 54, 55, 56, 57].", "startOffset": 83, "endOffset": 103}, {"referenceID": 35, "context": "Network pruning strategies have also been explored extensively by prior literature [53, 54, 55, 56, 57].", "startOffset": 83, "endOffset": 103}, {"referenceID": 18, "context": "The Network-in-Network [32] is an approach that tries to increase the representational power of DNNs by exploiting 1 \u00d7 1 convolutional layers.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "GoogLeNet [14] and SqueezeNet [33] extensively use these 1 \u00d7 1 layers for reducing the dimension of each layer\u2019s output activations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "GoogLeNet [14] and SqueezeNet [33] extensively use these 1 \u00d7 1 layers for reducing the dimension of each layer\u2019s output activations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 36, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 37, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 38, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 39, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 40, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 41, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 42, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 43, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 44, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 13, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 45, "context": "CNNs [58, 59, 60, 61, 62, 63, 64, 65, 66, 24, 67].", "startOffset": 5, "endOffset": 49}, {"referenceID": 46, "context": "Nonetheless, with a multiGPU DNN platform [70, 71] where 4 to 8 GPUs share the same communication channel, the bandwidth allocated per each single GPU is still 10\u201320 GB/sec, similar to PCIe (gen3).", "startOffset": 42, "endOffset": 50}], "year": 2017, "abstractText": "Popular deep learning frameworks require users to fine-tune their memory usage so that the training data of a deep neural network (DNN) fits within the GPU physical memory. Prior work tries to address this restriction by virtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be utilized for memory allocations. Despite its merits, virtualizing memory can incur significant performance overheads when the time needed to copy data back and forth from CPU memory is higher than the latency to perform the computations required for DNN forward and backward propagation. We introduce a high-performance virtualization strategy based on a \u201ccompressing DMA engine\u201d (cDMA) that drastically reduces the size of the data structures that are targeted for CPU-side allocations. The cDMA engine offers an average 2.6\u00d7 (maximum 13.8\u00d7) compression ratio by exploiting the sparsity inherent in offloaded data, improving the performance of virtualized DNNs by an average 32% (maximum 61%).", "creator": "LaTeX with hyperref package"}}}