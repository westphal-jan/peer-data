{"id": "1705.01813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2017", "title": "Fast k-means based on KNN Graph", "abstract": "In the era of big data, k-means clustering has been widely adopted as a basic processing tool in various contexts. However, its computational cost could be prohibitively high as the data size and the cluster number are large. It is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration. In this paper, a novel solution towards the scalability issue of k-means is presented. In the proposal, k-means is supported by an approximate k-nearest neighbors graph. In the k-means iteration, each data sample is only compared to clusters that its nearest neighbors reside. Since the number of nearest neighbors we consider is much less than k, the processing cost in this step becomes minor and irrelevant to k. The processing bottleneck is therefore overcome. The most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast $k$-means itself. Comparing with existing fast k-means variants, the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. As it is tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the same scale of clustering, it would take 3 years for traditional k-means.", "histories": [["v1", "Thu, 4 May 2017 12:27:28 GMT  (165kb,D)", "http://arxiv.org/abs/1705.01813v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["cheng-hao deng", "wan-lei zhao"], "accepted": false, "id": "1705.01813"}, "pdf": {"name": "1705.01813.pdf", "metadata": {"source": "CRF", "title": "Fast k-means based on KNN Graph", "authors": ["Cheng-Hao Deng", "Wan-Lei Zhao"], "emails": [], "sections": [{"heading": null, "text": "Index terms - rapid clustering, k-mean, k-nearest neighbor graph."}, {"heading": "1 INTRODUCTION", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 RELATED WORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 k -means Variants", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2.2 K-Nearest Neighbor Graph Construction", "text": "The KNN graph is built primarily to support the search for the closest neighbors [32], [42]. It is also the key data structure in manifold learning and machine learning, etc. [32]. Basically, it tries to find the closest neighbors for each data point. If it is built in a brutal manner, its time complexity is O (d \u00b7 n2), where both d and n could be very large. As a result, it is mathematically expensive to create an exact KNN graph. For this reason, recent work [32], [33], [42], [43] aims to search for an approximate but efficient solution. In [42] an approximate KNN graph graph graph graph graph graph graph graph-graph graph-graph graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph."}, {"heading": "3 PRELIMINARIES", "text": "In order to facilitate our discussions in later sections, two important k-mean variants will be reviewed, namely boost k-means (BKM) [16] and two means (2M) trees [31]. As will be shown later, our acceleration scheme is based on boost means instead of traditional k-means, as the former always produce clusters of higher quality."}, {"heading": "3.1 Boost k -means", "text": "As an extension of the incremental k-mean [1], Boost k-mean enables an optimization iteration for the entire l2 space. In contrast to other k-mean variants, the boost-kmeans iteration is driven by an explicit objective function. In view of the cluster Sr = 1 \u00b7 \u00b7 k, the composite vector of a cluster is defined as Dr = \u2211 xi-Sr xi. The objective function of the boost k-mean is written as I = k \u2211 r = 1 D \u2032 rDr nr, (2), which is derived directly from the equation. 1. With this objective function, the traditional k-mean cluster is revised to a stochastic optimization procedure. Each time, a sample is randomly selected and searches for a better redistribution that leads to a maximum increase of I. Namely, the variation of the functional value resulting from the possible movement (shift xi \u2212 Sv) is the optimization of Su \u2212 me."}, {"heading": "3.2 Two Means Tree", "text": "Two Means (2M) Tree [31] is a variant of hierarchical bipartition of k means. It has been adopted in the construction of KNN diagrams for its high efficiency [31]. Alg. 1 shows the general approach of two middle trees. Similar to the bipartition of k means, the samples are recursively divided into two blocks each time until k clusters are created. Unlike bipartition of k means, a further step is taken at the end of each bipartition, the resulting two blocks are adapted to the same size. Their complexity is the same as the bipartition of kmittel, namely O (d \u00b7 n \u00b7 log (n)), which is even faster than a round k mean iteration. In this paper, two means tree are used only to generate an initial k-middle partition. To improve its performance, the modified addition kmeans is integrated into the bipartition process (step 8 in Alg. 1)."}, {"heading": "4 KNN GRAPH BASED k -MEANS", "text": "In this section, our solution to the problem of Kmean scalability is presented. First, a general procedure is presented for boosting k-means with the support of KNN graphs. To support rapid clustering, the process of KNN graph construction should be sufficiently fast, otherwise it will become a further processing obstacle. In order to solve this problem, a novel, lightweight KNN graph construction method is also introduced."}, {"heading": "4.1 Motivation", "text": "As shown in Fig. 2, there is a strong correlation between the proximity of the data samples and their affiliation to a cluster. This correlation could be interpreted either from the side of the k-mean cluster or from the side of the KNN graph construction. \u2022 From the cluster side, when the KNN list of each sample is known, clustering is a process of arranging close neighbors in a cluster. Consequently, in a sample, clustering only has to check its closest neighbors with the clusters. \u2022 If the data samples are already divided into small clusters, the KNN graph construction within each cluster is performed by an exhaustive pair-wise comparison. Consequently, the KNN graph construction is performed in a very efficient manner."}, {"heading": "4.2 Fast k -means Driven by KNN Graph", "text": "In the face of a KNN diagram, Boost k-means procedure presented in [16] is revised as Alg. 2. In each step of the optimization iteration, a random sample is selected and then all clusters in which the respective neighbors are located are collected. Therefore, the selected sample is checked with these clusters to search for the best move. Iteration ends until the convergence state is reached. Algorithm 2. GK averages (Xn \u00b7 d, Gn \u00d7 B) 1: Input: Matrix Xn \u00d7 D, k, KNN Graph 2: Output: S1, \u00b7 \u00b7, Sr \u00b7, \u00b7 Sk 3: cLabel = TwoMeans (Xn \u00d7 D, k) is only possible then."}, {"heading": "4.3 KNN Graph Construction with Fast k -means", "text": "As discussed in Section 1, a KNN chart is required as an input chapter to determine the size of the cluster. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "4.4 Discussion on Parameters", "text": "In Alg. 2 and Alg. 3, in addition to cluster number k, three parameters are involved: The parameter \u03c4 in Alg. 2 controls the quality of the KNN diagram. According to our observation, it is sufficient to specify the parameter \u03c4 = 10 for cluster formation. If Alg. 3 is called to create the KNN diagram for the ANNS task, it could be configured, i.e. 32. The parameter \u043e controls the size of the cluster used for the KNN graph construction. Larger parameters lead to a better quality of the KNN diagram, while also causing more pairs of comparisons. For this reason, a trade-off must be carried out. According to our observation, the recommended range for implementation is [40, 100]. The parameter \u0435 controls the number of neighbors that a sample should take into account during rapid k-mean clustering. This, in turn, determines the number of clusters visiting a sample."}, {"heading": "4.5 Complexity Analysis", "text": "This section analyzes the complexity of Alg. 2 and Alg. 3. As shown above, the GC mean (Alg. 2) consists of two main parts, namely two means of initialization and a fast k-mean clustering. In the first part, the complexity of the 2M tree minitization is O (d \u00b7 n \u00b7 log (k)) [16]. In the second part, the total complexity is O (d \u00b7 n \u00b7 log (k) + t \u00b7 d \u00b7 n \u00b7 \u0432), where t is the number of iterations. From the above analysis, the cluster number k has only a very small effect on the cluster complexity."}, {"heading": "5 EXPERIMENTS ON CLUSTERING TASK", "text": "This section examines the performance of GK funds in comparison to K funds and their representative variants, such as Boost k funds (BKM) [16], Cap k funds [27] and Mini-Batch [20]. AKM [22] and HKM [45] are not regarded as performing worse than Cap k funds, it is reported in [27]. Prior to comparisons with other k funds, the performance of GK funds is examined under different configurations. To achieve this, we try to see how well GK funds work if they are based on conventional K funds rather than Boost k funds. In addition, GK funds are also tested when the approximate KNN graph is supplied by NN Descent [32]. To do this, we will look for the best configuration we can currently set for GK funds."}, {"heading": "5.1 Evaluation Protocol", "text": "Similar to [16], [26], the average distortion (or mean square error [30]) is used to evaluate the cluster quality. Essentially, it is the average distance between the samples and their cluster centrifuge that is given in Eq.4. From the equation, it is nothing more than to take the average via the k-mean objective function Equation 1. The lower the distortion value, the better the quality of the cluster result. This measurement is the same as within the cluster sum of quadratic distortions (WCSSD) in [27].E = \u2211 q (xi) = r-Cr-xi-2n. (4) In order to investigate the relationship between the quality of the KNN graph and the quality of the cluster result, the average recall of the KNN graph is also taken into account. In our evaluation only the recall of the closest samples is measured."}, {"heading": "5.2 Configuration Test", "text": "In this section, various configurations are tested for Alg. 2. As we discussed in Section 4, Alg. 2 could be supported by other KNN graph construction algorithms. Furthermore, we have also pointed out that a similar acceleration scheme in Alg. 2 is feasible for traditional k averages. In this section, three 40000 40200 40600 40800 410000,6 0.7 0.8 1A vera geD isto rtio nRecallKGraph + GK averages GK averages Samples GK averages Fig. 4. Configuration test for Alg. 2. Alg. 2 is tested with the support of KGraph [32]. In addition, Alg. 2 is modified to do clusters with traditional k-Means. Different configurations are tested. In the first run, Alg. 2 is tested with KNN graphs."}, {"heading": "5.3 Clustering Quality", "text": "This year, as never before, it will be able to retaliate, to retaliate."}, {"heading": "5.4 Scalability Test on Image Clustering", "text": "This year, it's so far that it will be able to put itself at the top, \"he said.\" But it's too early to say what we need to do, \"he said.\" It's still too early to do it, \"he said.\" It's still too early to do it, \"he said.\" But it's still too early to do it. \""}, {"heading": "6 CONCLUSION", "text": "In this paper we have presented our solution to the problem of scalability of k-means. We show that fast k-means clustering can be achieved with the support of an approximate KNN graph. Specifically, only one sample has to be compared with clusters in which its nearest neighbours live in the k-means iteration. Therefore, the cluster complexity is irrelevant for the cluster number. As shown in the work, a hundredfold to thousandfold acceleration is achieved, especially for very large n and k-means. Since the fast k-means is based on boost-k-means, it also shows a very high cluster quality. Overall, the proposed GKmeans shows a significantly better trade-off between cluster quality and efficiency compared to existing solutions. Furthermore, the beauty of this algorithm lies in the design of a fast KNN-graph construction process. The KNN graph is structured 9 by naming GK-means itself in an intertwining development process."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work is supported by the National Natural Science Foundation of China with a grant of 61572408."}], "references": [{"title": "Empirical and theoretical comparisons of selected criterion functions for document clustering", "author": ["Y. Zhao", "G. Karypis"], "venue": "Machine Learning, vol. 55, pp. 311\u2013331, Jun. 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Aggregating local descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "Trans. PAMI, vol. 34, pp. 1704\u20131716, Sep. 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Web scale photo hash clustering on a single machine", "author": ["Y. Gong", "M. Pawlowski", "F. Yang", "L. Brandy", "L. Bourdev", "R. Fergus"], "venue": "CVPR, pp. 19\u201327, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "ICCV, Oct. 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory, vol. 28, pp. 129\u2013137, Mar. 1982.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["MacQueen", "James"], "venue": "Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, vol. 1, pp. 281\u2013297, 1967.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1967}, {"title": "Mean shift, mode seeking, and clustering", "author": ["Y. Cheng"], "venue": "Trans. PAMI, vol. 17, pp. 790\u2013799, Aug. 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H. peter Kriegel", "J. Sander", "X. Xu"], "venue": "IEEE Transactions on Knowledge and Data Engineering, pp. 226\u2013231, 1996.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computin, vol. 17, pp. 395\u2013416, Aug. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Clustering millions of faces by identity", "author": ["C. Otto", "D. Wang", "A. Jain"], "venue": "Trans. PAMI, pp. 1\u201314, Mar. 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Birch: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, vol. 25, pp. 103\u2013114, Jun. 1996.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Clustering by fast search and find of density peaks", "author": ["A. Rodriguez", "A. Laio"], "venue": "Science, vol. 344, no. 6191, pp. 1492\u20131496, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "P.S. Yu", "Z.-H. Zhou", "M. Steinbach", "D.J. Hand", "D. Steinberg"], "venue": "Knowledge and Information System, vol. 14, pp. 1\u201337, Dec. 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM- SIAM Symposium on Discrete Algorithms, pp. 1027\u20131035, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "KDD Workshop on Text Mining, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Boost k-means", "author": ["W.-L. Zhao", "C.-H. Deng", "C.-W. Ngo"], "venue": "arXiv preprint arXiv:1610.02483, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "NIPS, pp. 10\u201318, Dec. 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Discrete and Computational Geometry, vol. 45, pp. 596\u2013 616, Mar. 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Trans. PAMI, vol. 24, no. 7, pp. 881\u2013 892, 2002.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "Proceedings of the 19th international conference on World wide web, pp. 1177\u20131178, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable k-means++", "author": ["B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 7, pp. 622\u2013633, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "CVPR, pp. 1\u20138, Jun. 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Scalable k-means by ranked retrieval", "author": ["A. Broder", "L. Garcia-Pueyo", "V. Josifovski", "S. Vassilvitskii", "S. Venkatesan"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pp. 233\u2013242, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantize and conquer: A dimensionality-recursive solution to clustering, vector quantization, and image retrieval", "author": ["Y. Avrithis"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 3024\u20133031, Dec. 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximate gaussian mixtures for large scale vocabularies", "author": ["Y. Avrithis", "Y. Kalantidis"], "venue": "ECCV, pp. 15\u201328, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Web-scale image clustering revisited", "author": ["Y. Avrithis", "Y. Kalantidis", "E. Anagnostopoulos", "I.Z. Emiris"], "venue": "ICCV, pp. 1502\u20131510, Dec. 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast approximate k-means via cluster closures", "author": ["J. Wang", "J. Wang", "Q. Ke", "G. Zeng", "S. Li"], "venue": "CVPR, pp. 3037\u20133044, Jun. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast and accurate k-means for large datasets", "author": ["A. Meyerson", "A. Wong"], "venue": "NIPS, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Using the triangle inequality to accelerate", "author": ["C. Elkan"], "venue": "ICML, 2003.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Trans. PAMI, vol. 33, pp. 117\u2013128, Jan. 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Which spatial partition trees are adaptive to intrinsic dimension", "author": ["N. Verma", "S. Kpotufe", "S. Dasgupta"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 565\u2013574, Jun. 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient k-nearest neighbor graph construction for generic similarity measures", "author": ["W. Dong", "C. Moses", "K. Li"], "venue": "WWW, pp. 577\u2013 586, Mar. 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "EFANNA: An extremely fast approximate nearest neighbor search algorithm based on knn graph", "author": ["C. Fu", "D. Cai"], "venue": "arXiv preprint arXiv:1609.07228, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs", "author": ["Y.A. Malkov", "D. Yashunin"], "venue": "arXiv preprint arXiv:1603.09320, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "An efficient k-means clustering algorithm: analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Trans. PAMI, vol. 24, pp. 881\u2013892, Jun. 2002.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Commun. ACM, vol. 18, pp. 509\u2013517, Sep. 1975.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1975}, {"title": "Accelerating exact k-means algorithms with geometric reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 277\u2013281, Aug. 1999.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1999}, {"title": "Scalable k-means by ranked retrieval", "author": ["A. Broder", "L. Garcia-Pueyo", "V. Josifovski", "S. Vassilvitskii", "S. Venkatesan"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pp. 233\u2013242, Feb. 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and exact out-of-core kmeans clustering", "author": ["A. Goswami", "R. Jin", "G. Agrawal"], "venue": "Fourth IEEE International Conference on Data Mining, pp. 83\u201390, Nov. 2004.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical clustering algorithms for document datasets", "author": ["Y. Zhao", "G. Karypis"], "venue": "Data Mining and Knowledge Discovery, vol. 10, pp. 141\u2013168, Mar. 2005.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast approximate knn graph construction for high dimensional data via recursive lanczos bisection", "author": ["J. Chen", "H. ren Fang", "Yousef"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 1989\u20132012, Dec. 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1989}, {"title": "Scalable k-nn graph construction for visual descriptors", "author": ["J. Wang", "J. Wang", "G. Zeng", "Z. Tu", "R. Gan", "S. Li"], "venue": "CVPR, pp. 1106\u2013 1113, Jun. 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate nearest neighbor search on high dimensional data\u2014experiments, analyses, and improvement", "author": ["W. Li", "Y. Zhang", "Y. Sun", "W. Wang", "W. Zhang", "X. Lin"], "venue": "arXiv preprint arXiv:1610.02455, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["M. Muja", "D.G. Lowe"], "venue": "Trans. PAMI, vol. 36, pp. 2227\u20132240, Nov. 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "IJCV, vol. 60, pp. 91\u2013110, Nov. 2004.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "YFCC100M: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.-J. Li"], "venue": "Communications of the ACM, vol. 59, pp. 64\u2013 73, Feb. 2016.  10", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pp. 1532\u20131543, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Rapid biologically-inspired scene classification using features shared with visual attention", "author": ["C. Siagian", "L. Itti"], "venue": "Trans. PAMI, vol. 29, pp. 300\u2013312, Feb 2007.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 143, "endOffset": 146}, {"referenceID": 3, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 4, "context": "Since the general k-means algorithm [5], [6] was proposed in 1982, continuous efforts have been made to search for better solution for this issue.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Since the general k-means algorithm [5], [6] was proposed in 1982, continuous efforts have been made to search for better solution for this issue.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 135, "endOffset": 139}, {"referenceID": 10, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 165, "endOffset": 169}, {"referenceID": 5, "context": "Among these algorithms, k-means [6] remains popular for its simplicity, efficiency and moderate but stable performance under different contexts.", "startOffset": 32, "endOffset": 35}, {"referenceID": 12, "context": "It is known as one of top ten most popular algorithms in data mining [13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "Recent researches have been working on improving its clustering quality [14], [15], [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Recent researches have been working on improving its clustering quality [14], [15], [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "Recent researches have been working on improving its clustering quality [14], [15], [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "Thanks to the introduction of incremental optimization strategy in [16], k-means is able to converge to considerably lower distortion.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "Moreover, according to [17], [18], in its worst case, the running time for k-means could be exponential against the size of input samples.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Moreover, according to [17], [18], in its worst case, the running time for k-means could be exponential against the size of input samples.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Algorithm presented in [29] demonstrates faster speed and maintains relatively high quality.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "Experiments have been conducted on SIFT100K [30] with traditional k-means [5] and two-means tree [31].", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "Experiments have been conducted on SIFT100K [30] with traditional k-means [5] and two-means tree [31].", "startOffset": 74, "endOffset": 77}, {"referenceID": 30, "context": "Experiments have been conducted on SIFT100K [30] with traditional k-means [5] and two-means tree [31].", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "k-means and its variants two-means tree [31] are tested on SIFT100K [30].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "k-means and its variants two-means tree [31] are tested on SIFT100K [30].", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "Moreover, comparing with the KNN graph construction algorithms that are specifically designed for approximate nearest neighbor search [32], [33], [34], our algorithm requires much lower computational cost.", "startOffset": 134, "endOffset": 138}, {"referenceID": 32, "context": "Moreover, comparing with the KNN graph construction algorithms that are specifically designed for approximate nearest neighbor search [32], [33], [34], our algorithm requires much lower computational cost.", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "Moreover, comparing with the KNN graph construction algorithms that are specifically designed for approximate nearest neighbor search [32], [33], [34], our algorithm requires much lower computational cost.", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "[14], [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[14], [21].", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "According to [14], k-means iteration also converges faster due to the careful selection on the initial cluster centroids.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Although the number of scanning rounds has been reduced to a few in [21], the extra computational cost is still inevitable.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Recently, a new variant called boost k-means is proposed [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "The \u201cegg-chicken\u201d loop in k-means has been simplified as a stochastic optimization process, which is also known as incremental k-means [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 34, "context": "[35] proposed to index dataset in a KD Tree [36] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[35] proposed to index dataset in a KD Tree [36] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 44, "endOffset": 48}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "A recent work [38] takes similar way to speed-up the nearest neighbor search by indexing dataset with inverted file structure.", "startOffset": 14, "endOffset": 18}, {"referenceID": 19, "context": "Namely, methods in [20], [39] only pick a small portion of the whole dataset to update the clustering centroids each time.", "startOffset": 19, "endOffset": 23}, {"referenceID": 38, "context": "Namely, methods in [20], [39] only pick a small portion of the whole dataset to update the clustering centroids each time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "In [27], only the \u201cactive points\u201d, which are the samples located on the cluster boundaries, are considered to be swapped between clusters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Another easy way to reduce the number of comparisons between samples and centroids is to conduct the clustering in a top-down hierarchical manner [1], [40], [41].", "startOffset": 146, "endOffset": 149}, {"referenceID": 39, "context": "Another easy way to reduce the number of comparisons between samples and centroids is to conduct the clustering in a top-down hierarchical manner [1], [40], [41].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "The clustering complexity of k-means is reduced from O(t\u00b7k\u00b7n\u00b7d) to O(t\u00b7log(k)\u00b7n\u00b7d) [16].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "However, poor clustering performance is achieved in the usual case as it breaks the Lloyd\u2019s condition [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "KNN graph is primarily built to support nearest neighbor search [32], [42].", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "KNN graph is primarily built to support nearest neighbor search [32], [42].", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "It is also the key data structure in the manifold learning and machine learning, etc [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 36, "endOffset": 40}, {"referenceID": 40, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "In [42], an approximate KNN graph is built efficiently by divide-and-conquer strategy.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "Recent works [33], [43] could be viewed as improvements over this work.", "startOffset": 13, "endOffset": 17}, {"referenceID": 41, "context": "Recent works [33], [43] could be viewed as improvements over this work.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "In 2011, a very successful KNN graph construction algorithm called NN Descent/KGraph [32] has been proposed.", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "According to [32], its empirical time complexity is onlyO(n).", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": "Algorithm presented in [33] faces similar problem.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "In order to facilitate our discussions in later sections, two important k-means variants are reviewed, namely, boost k-means (BKM) [16] and two means (2M) tree [31].", "startOffset": 131, "endOffset": 135}, {"referenceID": 30, "context": "In order to facilitate our discussions in later sections, two important k-means variants are reviewed, namely, boost k-means (BKM) [16] and two means (2M) tree [31].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "As an extension of incremental k-means [1], boost k-means allows the optimization iteration to be feasible for the whole l2 space.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "According to [16], it is able to converge to a much better local optima in comparison to k-means and its variants.", "startOffset": 13, "endOffset": 17}, {"referenceID": 30, "context": "Two means (2M) tree [31] is a variant of hierarchical bisecting k-means.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "It has been adopted in KNN graph construction for its high speed efficiency [31].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "Given a KNN graph is ready, boost k-means procedure presented in [16] is revised as Alg.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Comparing with the procedure presented in boost kmeans [16], there are basically two major modifications.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Firstly, the initial clusters are initialized by two means tree, whose complexity is only O(n\u00b7log(k)\u00b7d) [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "The experiment is conducted on SIFT100K [30].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "Moreover, comparing with other KNN graph construction algorithms [32], [34], [33], Alg.", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "Moreover, comparing with other KNN graph construction algorithms [32], [34], [33], Alg.", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "Moreover, comparing with other KNN graph construction algorithms [32], [34], [33], Alg.", "startOffset": 77, "endOffset": 81}, {"referenceID": 31, "context": "Furthermore, it is at least two times faster than NN Descent [32] and small world graph construction [34].", "startOffset": 61, "endOffset": 65}, {"referenceID": 33, "context": "Furthermore, it is at least two times faster than NN Descent [32] and small world graph construction [34].", "startOffset": 101, "endOffset": 105}, {"referenceID": 31, "context": "According to our observation, although the quality of KNN graph (measured by recall) is usually lower than that of NN Descent [32], it is able to achieve similar or even better performance than the methods presented in [34], [44].", "startOffset": 126, "endOffset": 130}, {"referenceID": 33, "context": "According to our observation, although the quality of KNN graph (measured by recall) is usually lower than that of NN Descent [32], it is able to achieve similar or even better performance than the methods presented in [34], [44].", "startOffset": 219, "endOffset": 223}, {"referenceID": 42, "context": "According to our observation, although the quality of KNN graph (measured by recall) is usually lower than that of NN Descent [32], it is able to achieve similar or even better performance than the methods presented in [34], [44].", "startOffset": 225, "endOffset": 229}, {"referenceID": 29, "context": "For instance, it takes less than 3ms to fulfill a query on 100 million SIFTs [30] with its recall above 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "For the first part, the complexity of 2M tree initialization is O(d\u00b7n\u00b7log(k)) [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 29, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 55, "endOffset": 58}, {"referenceID": 45, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 69, "endOffset": 73}, {"referenceID": 46, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 82, "endOffset": 86}, {"referenceID": 46, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 127, "endOffset": 131}, {"referenceID": 47, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "such as boost k-means (BKM) [16], closure k-means [27] and Mini-Batch [20].", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "such as boost k-means (BKM) [16], closure k-means [27] and Mini-Batch [20].", "startOffset": 50, "endOffset": 54}, {"referenceID": 19, "context": "such as boost k-means (BKM) [16], closure k-means [27] and Mini-Batch [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "AKM [22] and HKM [45] are not considered as inferior performance to closure k-means is reported in [27] .", "startOffset": 4, "endOffset": 8}, {"referenceID": 43, "context": "AKM [22] and HKM [45] are not considered as inferior performance to closure k-means is reported in [27] .", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "AKM [22] and HKM [45] are not considered as inferior performance to closure k-means is reported in [27] .", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "Additionally, GK-means is also tested when the approximate KNN graph is supplied by NN Descent [32].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Similar as [16], [26], the average distortion (or mean squared error [30]) is adopted to evaluate the clustering quality.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "Similar as [16], [26], the average distortion (or mean squared error [30]) is adopted to evaluate the clustering quality.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "Similar as [16], [26], the average distortion (or mean squared error [30]) is adopted to evaluate the clustering quality.", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "This measure is the same as within-cluster sum of squared distortions (WCSSD) in [27].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "2 is tested with the support of KGraph [32].", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "2 is supplied with KNN graph from NN Descent [32], which is denoted as \u201cKGraph+GK-means\u201d run.", "startOffset": 45, "endOffset": 49}], "year": 2017, "abstractText": "In the era of big data, k-means clustering has been widely adopted as a basic processing tool in various contexts. However, its computational cost could be prohibitively high as the data size and the cluster number are large. It is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration. In this paper, a novel solution towards the scalability issue of k-means is presented. In the proposal, k-means is supported by an approximate k-nearest neighbors graph. In the k-means iteration, each data sample is only compared to clusters that its nearest neighbors reside. Since the number of nearest neighbors we consider is much less than k, the processing cost in this step becomes minor and irrelevant to k. The processing bottleneck is therefore overcome. The most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast k-means itself. Comparing with existing fast k-means variants, the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. As it is tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the same scale of clustering, it would take 3 years for traditional k-means.", "creator": "LaTeX with hyperref package"}}}