{"id": "1705.02077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Crowdsourcing Argumentation Structures in Chinese Hotel Reviews", "abstract": "Argumentation mining aims at automatically extracting the premises-claim discourse structures in natural language texts. There is a great demand for argumentation corpora for customer reviews. However, due to the controversial nature of the argumentation annotation task, there exist very few large-scale argumentation corpora for customer reviews. In this work, we novelly use the crowdsourcing technique to collect argumentation annotations in Chinese hotel reviews. As the first Chinese argumentation dataset, our corpus includes 4814 argument component annotations and 411 argument relation annotations, and its annotations qualities are comparable to some widely used argumentation corpora in other languages.", "histories": [["v1", "Fri, 5 May 2017 03:43:35 GMT  (285kb,D)", "http://arxiv.org/abs/1705.02077v1", "6 pages,3 figures,This article has been submitted to \"The 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC2017)\""]], "COMMENTS": "6 pages,3 figures,This article has been submitted to \"The 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC2017)\"", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mengxue li", "shiqiang geng", "yang gao", "haijing liu", "hao wang"], "accepted": false, "id": "1705.02077"}, "pdf": {"name": "1705.02077.pdf", "metadata": {"source": "CRF", "title": "Crowdsourcing Argumentation Structures in Chinese Hotel Reviews", "authors": ["Mengxue Li", "Shiqiang Geng", "Yang Gao", "Haijing Liu", "Hao Wang"], "emails": ["mengxue2015@iscas.ac.cn,", "gsqwxh@163.com,", "wanghao}@iscas.ac.cn"], "sections": [{"heading": "1. Introduction", "text": "In customer reviews, users generally not only give their opinions on the products / services, but also provide reasons to support their opinions. \u2022 Consider, for example, the following review extract posted on Tripadvisor.com: Example 1: 1. There was an old, small, black TV. 3. Clause 1: Provides the customer's opinion (or claim) on the devices in the room, and clauses 2: and 3: are reasons / evidence to support the claim. 4. Argumentation structures are known as arguments [11], and the techniques for automatically extracting arguments and their relationships (e.g. support / attack) are known as the basis of argument."}, {"heading": "2. Related Work", "text": "First, we review existing argumentation corpus for customer reviews; in particular, we highlight the argumentation models they used to define arguments; then we review crowdsourcing work for argumentation comments and some related tasks, such as the annotation of discourse structures; an argumentation model indicates the definition of arguments, such as which components an argument consists of, what kind of relationships between different argumentation and argument components are permitted; and Xiv: 170 5.02 077v 1 [cs.C L] 5M ay2 017"}, {"heading": "2.1. Argumentation Corpora", "text": "A comprehensive review of argumentation corpora goes beyond the scope of this paper; good reviews can be found, for example, in [5], [10], [16]. Here, we review only argumentation corpora constructed from customer reviews. Wachsmuth et al. [19] created the ArguAna corpus, which consists of 2.1k hotel reviews posted on Tripadvisor.com. Rather than directly marking arguments, they comment on statements and polarities between sensations. A statement is \"at least one clause and at most one sentence that is meaningful in itself.\" They designed a rules-based tool to segment statements, and used crowdsourcing to comment on the mood and characteristics (e.g. location, services, facilities) in each statement. Results suggested that crowdsourcing workers can reliably identify the feelings of statements (approval rating 72.8%), but have controversies to identify statements (rejection rate of 3.3%) as an argument resource for Argua."}, {"heading": "2.2. Crowdsourcing for Argumentation Annotation and Related Tasks", "text": "Ghosh et al. [3] proposed an annotation mechanism for commenting on arguments and their relationships in blog comments: they hired crowdsourcing workers to mark claims-premise relationships; note that the argument segments were provided a priori (commented on by domain experts), and the crowdsourcing workers were asked to mark only the types of argument components and relationships; they reported that crowdsourcing workers scored 0.45-0.55 IRA points (in terms of multi-\u03c0 [2]), suggesting that the agreement was moderate; they also said that IRA is a widely used measure to evaluate the quality of the annotation; there are several methods for calculating the IRA score; in this essay, we will show the calculation method used for each IRA score; larger IRA values indicate a higher correlation between the annotators and therefore indicate that the indications are indicative."}, {"heading": "3. Pre-Study", "text": "In this section, we describe how we conceive our annotation policy. In particular, we make some preliminary attempts at annotation to decide how to segment annotation clauses (e.g. by rule-based automated methods or by crowdsourcing workers) and what reasoning models we use, i.e., which reasoning components constitute an argument, and what relationships between the reasoning components are allowed. As most existing English annotation policies exist, we comment on twenty English and ten Chinese hotel reviews (untitled) from Tripadvisor.com to design our guideline. Five comments participate in the experiments; they are all native Chinese speakers who argue in English. Annotation policies are conducted on the Brat [17] open source annotation platform that we segment for comments, we test two approaches: annotation based on segmentation (i.e."}, {"heading": "4. Crowdsourcing Experiments", "text": "Our crowdsourcing experiment is being conducted as an optional task in the social media mining course of the Chinese Academy of Sciences in 2017. Over four hundred MSc students are registered for this course, over 90% are Chinese native speakers. We call on them to voluntarily participate in our experiment and inform them that the participating students can in return receive the corpus and its statistics, which they can use in their final project to train some machine learning algorithms. Finally, 388 students participate and we give them an hour-long tutorial to help them go through the policy and illustrate some examples. Crowdsourcing is being performed on the Brat platform [17] to view the hotel reviews assigned to him / her. To help the annotators, we can reduce errors and expand the original brat system so that we only have legal relationships (see Fig. 1), we are allowed to remind the recipients if the recipients do not exist, and if the recipients do not."}, {"heading": "5. Post-Processing and Corpora Generation", "text": "We identify the less committed students and remove their comments in Section 5.1, calculate the confidence value for comments, and create the final corpora in Section 5.2, and perform the error analysis in Section 5.3."}, {"heading": "5.1. Remove Less-Devoted Students\u2019 Annotations", "text": "The gold standard texts are similar to the examples in the policy, so we believe that students whose comments differ greatly from the gold standard notes are less devoted. However, for each sentence in a gold standard text, we calculate the consent of all students (with respect to \u03b1U) to this sentence and rate that consent. If a student's approval of this sentence falls into the bottom 10%, we increase the degree of that student's less devoted assessment by 1. Students whose degree of less devoted assessment is equal to or greater than 2 are classified as less devoted and3. At the moment, for a hotel review, \u03b1U only takes into account the comments for constituent types and ignores the agreement for comments on mood and relationships because we find that consent for mood and relationships is highly correlated to the comment component of the word."}, {"heading": "5.2. Dealing With Controversial Annotations and Obtain the Final Corpora", "text": "By manually reading and analyzing the comments, we find that hotel reviews generally fall into two categories: i) simple reviews, in which the argumentation component type of each sentence is fairly clear and their relationships are easy to identify; and ii) controversial reviews, in which a high percentage (over 30%) of sentences correspond to multiple argumentation types, and their terms are highly dependent on their contexts. For simple reviews, we can get the comments for the argumentation component, feelings and relationships; for the controversial reviews, although many sentences have controversial comments, we may still be able to find some less controversial sentences and get their comments. Thus, we build two companies based on the comments we have collected: one consists of simple reviews and the other consists of the relatively less controversial judgments in controversial reviews, so that we can extract as much useful information from the comments as possible from the results that are greater than, or 0.6."}, {"heading": "5.3. Error Analysis", "text": "To investigate the discrepancies in the annotations, we created Confusion Probability Matrices (CPM) [1] for argumentation component notes. A CPM contains the conditional probabilities that a commentator specifies a certain label (column) because another commentator has selected the label in the line for a certain point. For example, CPM for the simple review corpus and the less controversial sentences corpus are shown in Table 7, and the upper left cell in Table 7 means that if some other commentators have designated a clause as MajorClaim, a commentator with 0.481 probability will also label this clause as MajorClaim. From Table 7, we can see that there is no significant confusion between annotations in the less controversial judgments corpus, and we believe that the reason for this is the strict criterion being the selection of less controversial sentences."}, {"heading": "6. Conclusion", "text": "In this thesis, we present the first Chinese argumentation corpus and present the crowdsourcing technique with which we built this corpus. The argumentation model used in the corpus extends some classic models and we believe it is suitable for product evaluations in general. In particular, we use clustering technology to aggregate comments that can not only resolve comment conflicts, but also provide a trustworthiness. The quality of the comments of our corpus is comparable to some widely used argumentation corpus in other languages. To stimulate further research, we make the corpus publicly accessible 4."}], "references": [{"title": "Managing uncertainty in semantic tagging. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840\u2013850", "author": ["Silvie Cinkov\u00e1", "Martin Holub", "Vincent Kr\u0131\u0301\u017e"], "venue": "Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Measuring nominal scale agreement among many raters", "author": ["J.L. Fleiss"], "venue": "Psychological Bulletin,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1971}, {"title": "Analyzing argumentative discourse units in online interactions", "author": ["D. Ghosh", "S. Muresan", "N. Wacholder", "M. Aakhus", "M. Mitsui"], "venue": "In Proc. of Workshop on Argumentation Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Argumentation mining on the web from information seeking perspective", "author": ["I. Habernal", "J. Eckle-Kohler", "I. Gurevych"], "venue": "In Proc. of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Argumentation mining in user-generated web discourse", "author": ["I. Habernal", "I. Gurevych"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Algorithm as 136: A kmeans clustering algorithm", "author": ["John A Hartigan", "Manchek A Wong"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1979}, {"title": "Rapid development of a corpus with discourse annotations using two-stage crowdsourcing", "author": ["D. Kawahara", "Y. Machida", "T. Shibata", "S. Kurohashi"], "venue": "In Proc. of COLING,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Content analysis: An introduction to its methodology", "author": ["Krippendorff Klaus"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1980}, {"title": "Measuring the reliability of qualitative text analysis data", "author": ["Klaus Krippendorff"], "venue": "Quality & quantity,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Argumentation mining: State of the art and emerging trends", "author": ["M. Lippi", "P. Torroni"], "venue": "ACM Transactions on Internet Technology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Argumentation mining: Where are we now, where do we want to be and how do we get there", "author": ["Marie-Francine Moens"], "venue": "In Proc. of Forum on Information Retrieval Evaluation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Argumentation mining: the detection, classification and structure of arguments in text", "author": ["R.M. Palau", "M.-F. Moens"], "venue": "In Proc. of ICAIL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Language resources for studying argument", "author": ["C. Reed", "R. Mochales-Palau", "G. Rowe", "M.-F. Moens"], "venue": "In Proc. of LREC,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proc. of EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Annotating argument components and relations in persuasive essays", "author": ["C. Stab", "I. Gurevych"], "venue": "In Proc. of COLING,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["C. Stab", "I. Gurevych"], "venue": "arXiv preprint,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "brat: a web-based tool for nlp-assisted text annotation", "author": ["P. Stenetorp", "S. Pyysalo", "G. Topic", "T. Ohta", "S. Ananiadou", "J. Tsujii"], "venue": "In Proc. of ECAL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Some facets of argument mining for opinion analysis", "author": ["M.P. Garcia Villalba", "P. Saint-Dizier"], "venue": "In Proc. of COMMA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "A review corpus for argumentation analysis", "author": ["H. Wachsmuth", "M. Trenkmann", "B. Stein", "G. Engels", "T. Palakarska"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Semiautomated argumentative analysis of online product reviews", "author": ["A. Wyner", "J. Schneider", "K. Atkinson", "T. JM Bench-Capon"], "venue": "In Proc. of COMMA,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Such discourse structures are known as arguments [11], and the techniques for automatically extracting arguments and their relations (e.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "support/attack) from natural language texts are known as argumentation mining [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "legal documents [12], persuasive essays [16], newspapers and court cases [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "legal documents [12], persuasive essays [16], newspapers and court cases [13].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "legal documents [12], persuasive essays [16], newspapers and court cases [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Crowdsourcing has been widely recognised as a reliable and economic method for some annotating tasks [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "[5], [10], [16].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[5], [10], [16].", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "[5], [10], [16].", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "[19] built the ArguAna corpus, consisting of 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Garcia Villalba and Saint-Dizier [18] investigated suitable argumentation models for customer reviews.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "[20] built a corpus consisting of 84 reviews (posted on Amazon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed an annotation mechanism to annotate arguments and their relations in blog comments: they hired crowdsourcing workers to label claims-premises relations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "55 IRA score (in terms of multi-\u03c0 [2]), suggesting the agreement is moderate; also, they suggested", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "[7] designed a two-stage crowdsourcing mechanism to annotate two levels of discourse relations in Japanese texts crawled from multiple online genres.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "The annotation is performed on the brat [17] open-source annotation platform.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "As for clause segmenting, we test two approaches: subsentence based segmenting [19] (i.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "As for the argumentation model, we consider three candidate models: the Premises-Claim-MajorClaim (PCM) model [15] for persuasive essays, the extended ClaimPremises (ECP) model [4] for long Web documents and the extended Toulmin\u2019s (ETM) model [4] for short Web documents.", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": "As for the argumentation model, we consider three candidate models: the Premises-Claim-MajorClaim (PCM) model [15] for persuasive essays, the extended ClaimPremises (ECP) model [4] for long Web documents and the extended Toulmin\u2019s (ETM) model [4] for short Web documents.", "startOffset": 177, "endOffset": 180}, {"referenceID": 3, "context": "As for the argumentation model, we consider three candidate models: the Premises-Claim-MajorClaim (PCM) model [15] for persuasive essays, the extended ClaimPremises (ECP) model [4] for long Web documents and the extended Toulmin\u2019s (ETM) model [4] for short Web documents.", "startOffset": 243, "endOffset": 246}, {"referenceID": 3, "context": "Premises are allowed to support/attack claims, but claims are not allowed to support other claims, because this will lead to cascading support, which overcomplicates the annotating process [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 8, "context": "The average IRA in terms of Krippendorff\u2019s \u03b1U [9] for their annotations is 0.", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "The crowdsourcing is performed on the brat [17] platform.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "We employ the K-means clustering technique [6] to perform the argument component aggregation, which can perform the above two subtasks as a whole.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Some statistics of the argument components annotations in easy reviews are presented in Table 3, and according to our statistics, the average of each review is annotated by three students; as for IRA metrics, besides \u03b1U , we also use percentage agreement, multi-\u03c0 [2] and Krippendorff\u2019s \u03b1 [8].", "startOffset": 264, "endOffset": 267}, {"referenceID": 7, "context": "Some statistics of the argument components annotations in easy reviews are presented in Table 3, and according to our statistics, the average of each review is annotated by three students; as for IRA metrics, besides \u03b1U , we also use percentage agreement, multi-\u03c0 [2] and Krippendorff\u2019s \u03b1 [8].", "startOffset": 289, "endOffset": 292}, {"referenceID": 0, "context": "In order to study the disagreements in the annotations, we created confusion probability matrices (CPM) [1] for", "startOffset": 104, "endOffset": 107}], "year": 2017, "abstractText": "Argumentation mining aims at automatically extracting the premises-claim discourse structures in natural language texts. There is a great demand for argumentation corpora for customer reviews. However, due to the controversial nature of the argumentation annotation task, there exist very few large-scale argumentation corpora for customer reviews. In this work, we novelly use the crowdsourcing technique to collect argumentation annotations in Chinese hotel reviews. As the first Chinese argumentation dataset, our corpus includes 4814 argument component annotations and 411 argument relation annotations, and its annotations qualities are comparable to some widely used argumentation corpora in other languages.", "creator": "LaTeX with hyperref package"}}}