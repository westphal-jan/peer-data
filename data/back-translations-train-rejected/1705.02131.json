{"id": "1705.02131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Joint RNN Model for Argument Component Boundary Detection", "abstract": "Argument Component Boundary Detection (ACBD) is an important sub-task in argumentation mining; it aims at identifying the word sequences that constitute argument components, and is usually considered as the first sub-task in the argumentation mining pipeline. Existing ACBD methods heavily depend on task-specific knowledge, and require considerable human efforts on feature-engineering. To tackle these problems, in this work, we formulate ACBD as a sequence labeling problem and propose a variety of Recurrent Neural Network (RNN) based methods, which do not use domain specific or handcrafted features beyond the relative position of the sentence in the document. In particular, we propose a novel joint RNN model that can predict whether sentences are argumentative or not, and use the predicted results to more precisely detect the argument component boundaries. We evaluate our techniques on two corpora from two different genres; results suggest that our joint RNN model obtain the state-of-the-art performance on both datasets.", "histories": [["v1", "Fri, 5 May 2017 08:49:14 GMT  (457kb,D)", "http://arxiv.org/abs/1705.02131v1", "6 pages, 3 figures, submitted to IEEE SMC 2017"]], "COMMENTS": "6 pages, 3 figures, submitted to IEEE SMC 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["minglan li", "yang gao", "hui wen", "yang du", "haijing liu", "hao wang"], "accepted": false, "id": "1705.02131"}, "pdf": {"name": "1705.02131.pdf", "metadata": {"source": "CRF", "title": "Joint RNN Model for Argument Component Boundary Detection", "authors": ["Minglan Li", "Yang Gao", "Hui Wen", "Yang Du", "Haijing Liu", "Hao Wang"], "emails": ["wanghao}@iscas.ac.cn"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and they will be able to understand the rules that they have imposed on themselves."}, {"heading": "2. Related Work", "text": "In this section, we will first review the ACBD techniques and then work that applies RNN to applications related to ACBD, such as sequence tagging and text classification."}, {"heading": "2.1. Boundary Detection", "text": "Most existing ACBD methods consist of two consecutive subtasks: Identifying argumentative sentences (i.e. sentences containing some argumentation components) and recognizing component boundaries [16]. Levy et al. [14] identify context-dependent claims in Wikipedia articles by means of a cascade of classifiers. They first use logistical regression to identify sentences with topic-related claims (the topic is provided a priori), and then recognize the boundaries of claims and candidate boundaries to identify the most relevant claims for the topic. However, the importance of topic information is questionable, since Lippi and Torroni [15] achieve a similar result in the first subtask without using the topic information. Goudas et al. [5] propose an ACBD technique and test it on a corpus constructed from social media texts. They first use a variety of classifiers to perform the first subtask, then use a second subtask to perform a convincing CRF."}, {"heading": "2.2. RNN on Similar Tasks", "text": "RNN techniques, in particular Long Short-Term Memory (LSTM) [8], have recently been successfully applied to sequence labeling and text classification for various NLP problems. Graves et al. [6] propose a bidirectional RNN for speech recognition that takes into account the context on both sides of each word. However, for sequential markup tasks with strong interdependencies between output markers, the performance of RNN is not ideal. To address this problem, instead of modelling tagging decisions independently, Huang et al. [9] and Lample et al. [13] apply a sequential CRF to jointly decipher labels for the entire sequence. RNN has also been successfully used in text classification. Lai et al. [12] propose a recursive evolutionary neural network that extends a maximum label layer according to bidirectional RNN."}, {"heading": "3. Models", "text": "We consider a sentence in the document as a sequence of tokens / words and label the argument boundaries with the IOB tagset: A word is referred to as \"B\" if it is the first token in an argument component, \"I\" if it is an argument component but does not lead, and \"O\" if it is not contained in any argument component. In this section, we will first review some common techniques for labeling sequences, then present our common RNN model that can distinguish argumentative from non-argumentative sentences and use this information to recognize boundaries, in Section 3.5."}, {"heading": "3.1. Bi-LSTM", "text": "RNN [4] is a neural architecture designed to handle sequential data. RNN takes as input a vector X = [xt] T1 and returns a feature vector sequence ~ h = [ht] T1 at any time step. [2] A primary goal of RNN is to capture dependencies over long distances. However, in many real applications, standard RNN are often distorted toward their most recent inputs in the sequence [2]. LSTM [8] alleviates this problem by using a memory cell and three gates (an input gate i, a forget gate f, and an output gate o) to establish a compromise between the influence of the new input state and the previous state on2. \u2212 In this work, we leave [x] T1 the short-hand notation for vector [x1, \u00b7, xT] where T, a gene f, and an output gate o), a new input gate where T is \u2212 T and \u2212 T \u2212 T \u2212 T \u2212 T \u2212 T \u2212 T \u2212 T \u2212 T \u2212 T \u2212 where the input gate is."}, {"heading": "3.2. CRF", "text": "CRF [11] is widely used in sequence marking tasks. For a given sequence X and its labeling y, CRF gives a real value as follows: Score (X, y) = T \u2211 t = 2\u0445 (yt \u2212 1, yt) + T \u2211 t = 1\u03c6 (yt), where \u03c6 (yt) represents the unusual potential for labeling at position t and swaps (yt \u2212 1, yt) the paired potential of labeling at position t and t \u2212 1. The probability of the y given X can be determined from the score: p (y | X) = 1 Z exp (Score (X, y)). (1) In the face of a new input Xnew, the goal of the CRF is to find a labeling y \u0445 for Xnew, the conditional probability of which is maximized: y \u0445 = argmax y (p (y | Xnew)). (2) The method for achieving the optimal labeling of the CRF is to decode a chain that is executed (in the case of both models)."}, {"heading": "3.3. Bi-LSTM-CRF", "text": "To address this problem, Huang et al. [9] propose the Bi-LSTM CRF method, which extends a CRF layer after the output of Bi-LSTM to explicitly model the dependencies between the output tags. Figure 1 illustrates the structure of the Bi-LSTM CRF networks. For a given input set X = [xt] T1, h = [\u2212 \u2192 hL; \u2190 \u2212 hR] is the output of Bi-LSTM, where \u2212 \u2192 hL and \u2190 \u2212 hR are the output of the forward and backward directed LSTM, respectively. The connection layer C is used to connect the structural features s and the output of Bi-LSTM."}, {"heading": "3.4. Attention based RNN for Classification", "text": "Lai et al. [12] combine the word embedding and representational output of Bi-LSTM as a feature representation for text classification, with each input word weighted equally. However, as the meaning of the words differs, such a balance strategy fails to highlight the really important information. To address this problem, the Attention Mechanism [21] is proposed. As the name suggests, the weight vector calculates a weight vector to measure the meaning of each word and aggregates these informative words to form a sentence vector. In particular, f (xt) = tanh (Waxt + ba), \u03b1t = exp (Uaf (xt)) \u0432i exp (U af (xi)), a =. t \u03b1t \u00b7 xt.The sentence vector a is the weighted sum of word embedding xt, weighted by the term Wektor, provides additional supporting information, especially the information that requires a longer term."}, {"heading": "3.5. Joint RNN Model", "text": "In the proposed model, a Bi-LSTM reads the initial set both forward and backwards and generates the hidden states h = [ht] T1. For the argumentative classification of the set, as mentioned in Section 3.4, an attention mechanism is used to aggregate the input words into a sentence vector a. Max-pooling operation is used to capture the key components of the latent information. The argumentative state p of the set is then predicted by the combination of vector a, vector r (output by max-pooling operation) and relative position characteristic. For the sequence labeling in boundary recognition, we use the precompiled hidden states h of the Bi-LSTM. At each time step, we combine each hidden state with the relative position characteristic s and the predicted argumentative state p generated by the above-mentioned classification operation."}, {"heading": "4. Experiments", "text": "We will first present the argumentation corpora on which we will test our techniques in Section 4.1, introduce our experimental settings in Section 4.2, and present and analyze the empirical results in Section 4.3."}, {"heading": "4.1. Datasets", "text": "We evaluate neural network-based ACBD techniques on two distinct corpus: the persuasive corpus of essays [25] and the Wikipedia corpus [1]. The persuasive corpus of essays consists of three types of argumentation components: main claims, assertions, and premises. The corpus contains 402 English essays on a variety of topics, consisting of 7116 sentences and 147271 tokens (words). The Wikipedia corpus contains 315 Wikipedia articles grouped into 33 topics, and 1392 context-dependent assertions were commented in aggregate. A context-dependent assertion is \"a general, concise statement that directly supports or disputes the given topic,\" i.e. assertions that do not support / attack the assertion are not commented on. Note that the Wikipedia corpus is very unbalanced: only 2% sentences are argumentative (i.e. contain some argument components)."}, {"heading": "4.2. Experiment Settings", "text": "On the persuasive essay corpus, in accordance with [25], we use precision (P), recall (R), and macro F1 as evaluation yardsticks, using the same trait / test split ratio: 322 essays are used in training, and the remaining 80 essays are used in the test. On the Wikipedia corpus, in accordance with [14], a predicted claim is considered true Positive if and only if it accurately matches a labeled claim. For all articles on 33 topics, we randomly select 1 / 33 of all sentences to serve as a test set, and the remaining sentences are used in training. As the corpus is very unbalanced, we apply a random subsample to the training set to ensure that the ratio between non-argumentative and argumentative sentences is 4: 1. In experiments on both corpus, we randomly select 10% data in the training set to serve as a validation set for the epoch training."}, {"heading": "4.3. Results and Discussion", "text": "The performance of the different methods on the persuasive essays is shown in Table 1. Note that the performance of the CRF is achieved from [25]. Bi-LSTM. 825 macro F1 thanks to the context information captured by the LSTM layer. Adding a CRF layer to Bi-LSTM can significantly improve performance and can achieve comparable results with the CRF method, which uses a number of hand-picked characteristics. The third row in Table 1 indicates the performance of the Bi-LSTM-CRF with the mentioned arguments for each set, i.e. feature p in Figure 3 are the bottomless characteristics."}, {"heading": "5. Conclusion", "text": "In this paper, we present the first family of deep learning-based algorithms for the recognition of argumentation components (ACBD). In particular, we propose a novel common model that combines an attention-based classification RNN to predict argumentative or non-argumentative information and a Bi-LSTM-CRF network to identify the exact boundary. We compare the common model empirically with Bi-LSTM, BiLSTM-CRF and some state-of-the-art ACBD methods on two benchmark corporations; the results suggest that our common model outperforms all other methods, indicating that our common model can effectively use the argumentative or non-information to improve the performance of boundary mapping."}], "references": [{"title": "A benchmark dataset for automatic detection of claims and evidence in the context of controversial topics", "author": ["Ehud Aharoni", "Anatoly Polnarov", "Tamar Lavee", "Daniel Hershcovich", "Ran Levy", "Ruty Rinott", "Dan Gutfreund", "Noam Slonim"], "venue": "In Proceedings of the First Workshop on Argumentation Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "On the role of discourse markers for discriminating claims and premises in argumentative discourse", "author": ["Judith Eckle-Kohler", "Roland Kluge", "Iryna Gurevych"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Argument extraction from news, blogs, and social media", "author": ["Theodosis Goudas", "Christos Louizos", "Georgios Petasis", "Vangelis Karkaletsis"], "venue": "In Hellenic Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learning, ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint arXiv:1603.01360,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Context dependent claim detection", "author": ["Ran Levy", "Yonatan Bilu", "Daniel Hershcovich", "Ehud Aharoni", "Noam Slonim"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Context-independent claim detection for argument mining", "author": ["Marco Lippi", "Paolo Torroni"], "venue": "In IJCAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Argumentation mining: State of the art and emerging trends", "author": ["Marco Lippi", "Paolo Torroni"], "venue": "ACM Transactions on Internet Technology (TOIT),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "author": ["Bing Liu", "Ian Lane"], "venue": "arXiv preprint arXiv:1609.01454,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Argumentation mining: Where are we now, where do we want to be and how do we get there", "author": ["Marie-Francine Moens"], "venue": "In Post- Proceedings of the 4th and 5th Workshops of the Forum for Information Retrieval Evaluation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1929}, {"title": "Annotating argument components and relations in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "In COLING,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Identifying argumentative discourse structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "In EMNLP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "arXiv preprint arXiv:1604.07370,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Argumentation mining aims at automatically extracting arguments from natural language texts [19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "An argument is a basic unit people use to persuade their audiences to accept a particular state of affairs [3], and it usually consists of one or more argument components, for example a claim and some premises offered in support of the claim.", "startOffset": 107, "endOffset": 110}, {"referenceID": 24, "context": "As a concrete example, consider the essay excerpt below (obtained from the essay corpus in [25]):", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "[16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "A typical argumentation mining pipeline consists of three consecutive subtasks [24]: i) separating argument components from non-argumentative texts, ii) classifying the type (e.", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "Although neural network based approaches have been recently used in some related Natural Language Processing (NLP) tasks, such as linguistic sequence labelling [9] and named entity recognition (NER) [13], applying neural network to ACBD is challenging because the length of an argument component is much longer than that of a name/location in NER: [23] reports that an argument component includes 24.", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": "Although neural network based approaches have been recently used in some related Natural Language Processing (NLP) tasks, such as linguistic sequence labelling [9] and named entity recognition (NER) [13], applying neural network to ACBD is challenging because the length of an argument component is much longer than that of a name/location in NER: [23] reports that an argument component includes 24.", "startOffset": 199, "endOffset": 203}, {"referenceID": 22, "context": "Although neural network based approaches have been recently used in some related Natural Language Processing (NLP) tasks, such as linguistic sequence labelling [9] and named entity recognition (NER) [13], applying neural network to ACBD is challenging because the length of an argument component is much longer than that of a name/location in NER: [23] reports that an argument component includes 24.", "startOffset": 348, "endOffset": 352}, {"referenceID": 13, "context": "In fact, it has been reported in [14], [25] that separating argumentative and non-argumentative texts is often subtle even to human annotators.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "In fact, it has been reported in [14], [25] that separating argumentative and non-argumentative texts is often subtle even to human annotators.", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "First, since an argument component often consists of considerable number of words, it is essential to jointly considering multiple words\u2019 labels so as to detect argument components\u2019 boundaries; hence, we propose a bidirectional Recurrent Neural Network (RNN) [4] with a Conditional Random Field (CRF) [11] layer above it, as both RNN and CRF are widely recognized as effective methods for considering contextual information.", "startOffset": 259, "endOffset": 262}, {"referenceID": 10, "context": "First, since an argument component often consists of considerable number of words, it is essential to jointly considering multiple words\u2019 labels so as to detect argument components\u2019 boundaries; hence, we propose a bidirectional Recurrent Neural Network (RNN) [4] with a Conditional Random Field (CRF) [11] layer above it, as both RNN and CRF are widely recognized as effective methods for considering contextual information.", "startOffset": 301, "endOffset": 305}, {"referenceID": 15, "context": "sentences that include some argument components) and detecting the component boundaries [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "[14] identify context-dependent claims in Wikipedia articles by using a cascade of classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "However, the importance of topic information is questionable, as Lippi and Torroni [15] achieve a similar result on the first subtask without using the topic information.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "[5] propose a ACBD technique and test it on a corpus constructed from social media texts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Stab and Gurevych [25] employ a CRF model with four kinds of hand-craft features (structural, syntactic, lexical and probability features) to perform ACBD on their persuasive essay corpus.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "RNN techniques, especially Long Short-Term Memory (LSTM) [8], have recently been successfully applied to sequence labeling and text classification tasks in various NLP problems.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "[6] propose a bidirectional RNN for speech recognition, which takes the context on both sides of each word into account.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] and Lample et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] apply a sequential CRF to jointly decode labels for the whole sequence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] propose a Recurrent Convolutional Neural Network, which augments a max-pooling layer after bidirectional RNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Liu and Lane [17] propose an attentionbased bidirectional RNN model to perform these two tasks simultaneously; the method achieves state-of-the-art performance on both tasks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "RNN [4] is a neural architecture designed for dealing with sequential data.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "However, in many real applications, standard RNN are often biased towards their most recent inputs in the sequence [2] .", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "LSTM [8] alleviates this problem by using a memory cell and three gates (an input gate i, a forget gate f, and an output gate o) to tradeoff between the influence of the new input state and the previous state on", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "Bidirectional LSTM (Bi-LSTM) [7] is proposed to combat this problem.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "CRF [11] is widely used in sequence labeling tasks.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "[9] propose the Bi-LSTM-CRF method, which augments a CRF layer after the output of Bi-LSTM, so as to explicitly model the dependencies between the output labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] combine the word embeddings and representation output by Bi-LSTM as the", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The attention mechanism [21] is proposed to tackle this problem.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "We evaluate the neural network based ACBD techniques on two different corpora: the persuasive essays corpus [25] and the Wikipedia corpus [1].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "We evaluate the neural network based ACBD techniques on two different corpora: the persuasive essays corpus [25] and the Wikipedia corpus [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 24, "context": "On the persuasive essay corpus, in line with [25], we use precision (P), recall (R) and macro-F1 as evaluation metrics, and use the same train/test split ratio: 322 essays are used in training, and the remaining 80 essays are used in test.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "On the Wikipedia corpus, in line with [14], a predicted claim is considered as True Positive if and only if it precisely matches a labeled claim.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "All RNN are trained using the Adam update rule [10] with initial learning rate 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "To mitigate overfitting, we apply both the dropout method [22] and L2 weight regularization.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "In experiments on persuasive essays, we use Google\u2019s word2vec [18] 300-dimensional as the pretrained word embeddings, and set the hidden size to 150 and use only one hidden layer in LSTM; on Wikipedia articles, we use glove\u2019s 300-dimensional embeddings [20], and let the hidden size of LSTM be 80.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "In experiments on persuasive essays, we use Google\u2019s word2vec [18] 300-dimensional as the pretrained word embeddings, and set the hidden size to 150 and use only one hidden layer in LSTM; on Wikipedia articles, we use glove\u2019s 300-dimensional embeddings [20], and let the hidden size of LSTM be 80.", "startOffset": 253, "endOffset": 257}, {"referenceID": 24, "context": "[25]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "16 TK [15] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "168 TK + Topic [15] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "[14] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "CRF is obtained from [25].", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "the feature p in Figure 3 are ground-truth labels; surprisingly, this method even outperforms the \u201chuman upperbound\u201d performance3 reported in [25], validating our assumption that the sentences\u2019 argumentative-or-not information is helpful for the ACBD task.", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "Also note that, the results reported in [14] are obtained from a slightly older version of the dataset, containing only 32 topics (instead of 33) and 976 claims (instead of 1332).", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "Similar observations are also made in [15].", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "Argument Component Boundary Detection (ACBD) is an important sub-task in argumentation mining; it aims at identifying the word sequences that constitute argument components, and is usually considered as the first sub-task in the argumentation mining pipeline. Existing ACBD methods heavily depend on task-specific knowledge, and require considerable human efforts on feature-engineering. To tackle these problems, in this work, we formulate ACBD as a sequence labeling problem and propose a variety of Recurrent Neural Network (RNN) based methods, which do not use domain specific or handcrafted features beyond the relative position of the sentence in the document. In particular, we propose a novel joint RNN model that can predict whether sentences are argumentative or not, and use the predicted results to more precisely detect the argument component boundaries. We evaluate our techniques on two corpora from two different genres; results suggest that our joint RNN model obtain the state-of-the-art performance on both datasets.", "creator": "LaTeX with hyperref package"}}}