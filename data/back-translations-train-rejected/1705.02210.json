{"id": "1705.02210", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "SLDR-DL: A Framework for SLD-Resolution with Deep Learning", "abstract": "This paper introduces an SLD-resolution technique based on deep learning. This technique enables neural networks to learn from old and successful resolution processes and to use learnt experiences to guide new resolution processes. An implementation of this technique is named SLDR-DL. It includes a Prolog library of deep feedforward neural networks and some essential functions of resolution. In the SLDR-DL framework, users can define logical rules in the form of definite clauses and teach neural networks to use the rules in reasoning processes.", "histories": [["v1", "Fri, 5 May 2017 13:32:54 GMT  (659kb,D)", "http://arxiv.org/abs/1705.02210v1", "12 pages, 5 figures"]], "COMMENTS": "12 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["cheng-hao cai"], "accepted": false, "id": "1705.02210"}, "pdf": {"name": "1705.02210.pdf", "metadata": {"source": "CRF", "title": "SLDR-DL: A Framework for SLD-Resolution with Deep Learning", "authors": ["Cheng-Hao Cai"], "emails": ["chenghao.cai@outlook.com"], "sections": [{"heading": null, "text": "Keywords: Automated Reasoning, Deep Learning, Logic Programming, Resolution, Neural Networks."}, {"heading": "1 Introduction", "text": "The name \"SLD resolution\" is the abbreviation for SL resolution for certain clauses [1, 2], while the name \"SL resolution\" is the abbreviation for linear resolution with selection function [3]. Within the SLDR-DL framework, computers can think and learn rationally by using certain clauses [4] and deep neural networks [5]. The core concept of this framework is to train neural networks through successful resolution processes and to use the trained neural networks to heuristically guide new resolution processes. The SLDR-DL framework has two objectives: The first is to simulate the interaction between learning and arguing: systems should learn from reasoning processes and use learned experiences to guide new thought processes."}, {"heading": "2 Related Works", "text": "SLD resolution [1] is a basic technique of automated thinking, and has been used in many areas of artificial intelligence. Prologue, for example, is a programming language based on this technique [7, 8]. Mathematical thought processes such as pattern matching, variable substitution and implications can be simulated in Prologue [6]. Agents of faith-desire-intention (BDI) can also be developed using this technique [9, 10]. Recently, many researchers have explored how deep learning can be used to realize thinking: For example, Irving et al. [11] have developed DeepMath, which uses deep neural networks to select possible premises in automated theorem testing processes. In addition, Serafini and Garcez [12] have proposed Real Logic for the integration of learning and arguing. In the field of amplification, Garnelo et al. [13] have attempted to teach deep neural networks in order to generate symbols and form representations."}, {"heading": "3 SLD-Resolution with Deep Learning", "text": "SLD resolution with deep learning is a basic technique of the SLDR DL framework. It enables deep neural networks to guide new resolution processes after learning from old and successful resolution processes."}, {"heading": "3.1 SLD-Resolution", "text": "SLD resolution [1] is a process that determines whether a goal can be achieved with a set of specific clauses. It is based on unification, specific clauses and resolution. In this section, we assume that readers are familiar with these techniques and that only essential definitions and simple examples are used to improve readability."}, {"heading": "3.1.1 Unification", "text": "Unification [15] is one of the core algorithms in logic programming. It can cause two terms to become equivalent terms by substitution: Definition 1 (Term). A term is a constant, a variable, or a radio gate followed by a sequence of terms. Formally, it is defined as: t: = c | v | (f, t1, t2, \u00b7 \u00b7 \u00b7, tm) (1), where c is a constant, v is a variable, f is a radio gate, and t1, t2, \u00b7 tm are terms. A term can be used to represent facts. For example, if (Love, x, y) means \"x loves y\" and (Knowledge, p, q) means \"p knows, q,\" then \"Haibara knows that Conan loves Ran\" can be represented as (Knowledge, Haibara, (Love, Conan, Ran)."}, {"heading": "3.1.2 Definite Clauses", "text": "Definitive propositions [4] are used to represent relationships between terms, in particular their implication relations: Definition 3 (Definitive Proposition). A defined proposition is an implication relationship between several premises and a single conclusion. Formally, it is defined as: p1, p2, p2, pn = \u21d2 q (3), where p1, p2, \u00b7 \u00b7, pn are premises, \"the logical AND,\" \"\u21d2\" is the implication symbol, and q is a conclusion. All premises and the conclusion are terms. Definition 4 (disjunction form): The disjunction form of the definition set p1, p2, p2, and q is: q, p1, p2, pn (4), where... \"pn\" is the logical OR, and \"\u00ac\" is the logical NOT. In this formula, q is referred to as a positive letter, p2, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, p1, 2, p1, p1, 2, p1, p1, 1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, 1, p1, 1, 1, 1, 1, 1, 1, 1, 1, p1,"}, {"heading": "3.1.3 Resolution", "text": "The resolution algorithm [1] can decide whether a goal is satisfactory or not. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (Target) (Target) (Target) is a clause with an empty conclusion. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 (Target) \u2212 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (Target) (Target) \u00b7 \u00b7 (Target) (Definition 6 (Rule) is a definitive clause with a conclusion. \u2212 \u2212 p2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (Target) \u2212 (Target) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pn (Target)."}, {"heading": "3.2 Deep Neural Networks", "text": "Definition 8 (Deep Feedforward Neural Network): A Deep Feedforward Neural Network (DFNN) [5] is a neural network that is satisfactory: (1) It has 5 or more hidden layers. (2) Two adjacent layers are fully interconnected. (3) It has no recursive connections. A DFNN can map an input vector to an output vector. Definition 9 (Back Propagation): Backpropagation [17] is a supervised learning method of neural networks. Faced with an input vector, an upstream neural network can map it to an output vector, calculate an error between the output vector and a target vector, and use backpropagation to transfer the error to different layers and update the neural network."}, {"heading": "3.3 The SLDR-DL Framework", "text": "The SLDR-DL framework is a combination of SLD resolution and DFNNs. It enables deep neural networks to manage and learn resolution processes."}, {"heading": "3.3.1 The Framework Structure", "text": "The core component of the SLDR-DL framework is an implementation of the SLD resolution with DFNNs.Definition 10 (SLD resolution with DFNNs).SLD resolution with DFNNs is adapted from the standard SLD resolution (see [1] and Definition 7).When resolving a target, the following strategy is used: First, a target dictionary is encoded on an input vector. Second, a trained neural network is used to map the input vector to an output vector. Third, the output vector is decrypted to a ranking of rules. Finally, rules are applied to the target according to the ranking. Methods of encoding and decoding are used in 3.3.2.In the above process, the neural network is used to predict the ranking of the rules."}, {"heading": "1 V ble", "text": "2 Bigger 3 1 4 2 5 4"}, {"heading": "3.3.2 Encoding and Decoding", "text": "To enable neural networks to conduct resolution processes, encoding and decoding are required, as discussed in Section 3.3.1: (1) Selected literals should be encoded as input vectors; (2) rules should be encoded as target vectors; (3) output vectors should be decoded as ranking rules. In the SLDR DL framework, we have implemented the following encoding or decoding methods: \u2022 Given a symbol set s, a predefined depth d, and a predefined width b, a negative literal l is encoded as a vector through the following steps: First, all variables of l are replaced by a notation \"V.\" Let lNV mark this new expression. Second, lNV is rewritten to a closed term lComp with depth d and width b. All positions exceed the depth and width are replaced by an encoding of YY elements."}, {"heading": "3.3.3 The Education of SLDR-DL Systems", "text": "We use the word \"education\" instead of \"training,\" because the process of optimizing an SLDR system usually leads from simple problems to complex problems and requires the interaction between learning and thinking, and this process is similar to the process of educating a person. In other words, resolution in SLDR-DL is a heuristic search process that can optimize its search strategy through learning. Prior to learning, it can solve simple goals, but solving complex goals can fail because the search space can be huge. After proper learning, the search space can be reduced so that the complex goals can be successfully solved. Therefore, training an SLDR-DL system usually requires a schedule in which problems are sorted from the simplest to the hardest. According to the schedule, the system tries to solve simple problems at the beginning, works out solution records and learns the records. Then, the system goes on to more complex problems and learns until all problems are solved."}, {"heading": "3.3.4 A Prolog Library of Deep Neural Networks", "text": "The SLDR-DL framework also provides a prolog library that supports essential neural network calculations. Specifically, the library now supports: \u2022 Matrix addition and multiplication. \u2022 The back propagation algorithm of feedforward neural networks. \u2022 The Softmax classification. For details of the above functions, see [19]. To expand the use of the framework, additional functions will be added to the library in the future."}, {"heading": "4 A Practical Guide", "text": "In order to set up and use an SLDR DL system, users must define a rule set, a symbol set and a neural network, which should be encoded in Prolog (preferably SWI Prolog) [7]."}, {"heading": "4.1 Defining a Rule Set", "text": "A rule is defined in the following format: [\u2212 p1, \u2212 p2, \u2212 p3, \u00b7 \u00b7, \u2212 pn, + q] (13), where \"\u2212\" denotes a negative literal (premise) and \"+\" denotes a positive literal (conclusion). Specifically, the number of negative literals can be zero, and the rule becomes an assertion [+ q]. Figure 1 provides an example of a rule set where MaxRuleID is the maximum rule ID. It is important to note that when defining a rule set, we use the prolog convention: A symbol is a constant if it is a number, or its first letter is a lowercase letter, where MaxRuleID is the maximum rule ID."}, {"heading": "4.2 Defining a Symbol Set", "text": "A symbol set is defined as a list of symbols with their unique IDs. A symbol is defined in the following format: [\"Symbol ID,\" \"Symbol\"] (14) Figure 2 shows an example of a symbol set where MaxSymbolID is the maximum ID of symbols."}, {"heading": "4.3 Defining a Neural Network", "text": "A neural network can be defined as a list of layers: [\"Input Level,\" \"Hidden Level 1,\" \"Hidden Level 2,\" \u00b7 \u00b7, \"\" Hidden Level N, \"\" Output Level \"] (15) Each layer can be initialized by: Layer init (\" Layer name, \"\" Input dimension, \"\" Output dimension, \"\" Activation type, \"\" Randomization scale \") (16) Figure 3 provides an example of defining a neural network."}, {"heading": "4.4 Learning and Reasoning", "text": "The framework provides a core function called \"dnn sl resolution\": dnn sl resolution (\"Goal,\" \"Rule Set,\" \"Symbol Set,\" \"Neural Network,\" \"Method,\" \"SearchDepth,\" \"Result\") (17) Both learning and thinking processes are based on the core function. Figure 4 provides an example of how to use the core function, where G1, G2, G3 and G4 are targets, \"Learning (N, R)\" is used to define the number of learning periods N and the learning rate R, \"Input (B, D)\" is used to define the width B and depth D of the encodings, and \"Output (Y)\" is used to define the dimension of the decodings Y."}, {"heading": "5 Summary", "text": "The SLDR-DL framework enables interaction between resolution and deep learning. In the framework, users can define logical rules in the form of specific clauses, define neural networks, and teach neural networks to use logical rules. Neural networks can learn from successful resolution processes and then use lessons learned to guide new resolution processes. To expand the use of this framework, we will expand and refine it in the future with additional features."}], "references": [{"title": "Predicate logic as programming language", "author": ["Robert A. Kowalski"], "venue": "In IFIP Congress,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "Contributions to the theory of logic programming", "author": ["Krzysztof R. Apt", "Maarten H. van Emden"], "venue": "J. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1982}, {"title": "Linear resolution with selection function", "author": ["Robert A. Kowalski", "Donald Kuehner"], "venue": "Artif. Intell.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1971}, {"title": "On sentences which are true of direct unions of algebras", "author": ["Alfred Horn"], "venue": "J. Symb. Log.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1951}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7533):436\u2013444, May", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "The computer modelling of mathematical reasoning", "author": ["Alan Bundy"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1983}, {"title": "The birth of prolog", "author": ["Alain Colmerauer", "Philippe Roussel"], "venue": "In History of Programming Languages Conference (HOPL-II), Preprints,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "AgentSpeak(L): BDI agents speak out in a logical computable language. In Agents Breaking Away, 7th European Workshop on Modelling Autonomous Agents in a Multi-Agent World, Eindhoven", "author": ["Anand S. Rao"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Programming Multi-Agent Systems in AgentSpeak using Jason (Wiley Series in Agent Technology)", "author": ["Rafael H Bordini", "Jomi Fred Bner", "Michael Wooldridge"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Deepmath - deep sequence models for premise selection", "author": ["Geoffrey Irving", "Christian Szegedy", "Alexander A. Alemi", "Niklas E\u00e9n", "Fran\u00e7ois Chollet", "Josef Urban"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge", "author": ["Luciano Serafini", "Artur S. d\u2019Avila Garcez"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Towards deep symbolic reinforcement learning", "author": ["Marta Garnelo", "Kai Arulkumaran", "Murray Shanahan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Learning of humanlike algebraic reasoning using deep feedforward neural networks", "author": ["Chenghao Cai", "Dengfeng Ke", "Yanyan Xu", "Kaile Su"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Unification theory. In Handbook of Automated Reasoning (in 2 volumes)", "author": ["Franz Baader", "Wayne Snyder"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Logic in computer science modelling and reasoning about systems (2", "author": ["Michael Huth", "Mark Dermot Ryan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Theory of the backpropagation neural network", "author": ["Robert Hecht-Nielsen"], "venue": "Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1988}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "ACL", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The name \u201cSLD-resolution\u201d is the abbreviation of SL-resolution for definite clauses [1, 2], while the name \u201cSL-resolution\u201d is the abbreviation of linear resolution with selection function [3].", "startOffset": 84, "endOffset": 90}, {"referenceID": 1, "context": "The name \u201cSLD-resolution\u201d is the abbreviation of SL-resolution for definite clauses [1, 2], while the name \u201cSL-resolution\u201d is the abbreviation of linear resolution with selection function [3].", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "The name \u201cSLD-resolution\u201d is the abbreviation of SL-resolution for definite clauses [1, 2], while the name \u201cSL-resolution\u201d is the abbreviation of linear resolution with selection function [3].", "startOffset": 188, "endOffset": 191}, {"referenceID": 3, "context": "In the SLDR-DL framework, computers can reason and learn to reason by using definite clauses [4] and deep neural networks [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "In the SLDR-DL framework, computers can reason and learn to reason by using definite clauses [4] and deep neural networks [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "The second is to solve the problem of combinatorial explosion in automated reasoning [6]: When a problem becomes complex, its search tree of reasoning often grows rapidly.", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "SLD-resolution [1] is a fundamental technique of automated reasoning.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "For instance, Prolog is a programming language based on this technique [7, 8].", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "Mathematical reasoning processes, such as pattern matching, variable substitution and implication, can be simulated in Prolog [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "Also, belief-desire-intention (BDI) agents can be developed with this technique [9, 10].", "startOffset": 80, "endOffset": 87}, {"referenceID": 8, "context": "Also, belief-desire-intention (BDI) agents can be developed with this technique [9, 10].", "startOffset": 80, "endOffset": 87}, {"referenceID": 9, "context": "[11] have developed DeepMath which uses deep neural networks to select possible premises in automated theorem proving processes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Also, Serafini and Garcez [12] have proposed Real Logic for the integration of learning and reasoning.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "[13] have tried to teach deep neural networks to generate symbols and build representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] have explored the possibility of using deep feedforward neural networks to guide algebraic reasoning processes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "1 SLD-Resolution SLD-Resolution [1] is a process deciding whether a goal is satisfiable with a set of definite clauses.", "startOffset": 32, "endOffset": 35}, {"referenceID": 13, "context": "1 Unification Unification [15] is one of the core algorithms in logic programming.", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "2 Definite Clauses Definite clauses [4] are used to represent relations between terms, especially their implication relations: Definition 3 (Definite Clause).", "startOffset": 36, "endOffset": 39}, {"referenceID": 14, "context": "Formula (3) and Formula (4) can be proved to be logically equivalent [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "3 Resolution The resolution algorithm [1] can decide whether or not a goal is satisfiable: Definition 5 (Goal).", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "A deep feedforward neural network (DFNN) [5] is a neural network satisfying: (1) It has 5 or more than 5 hidden layers.", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "Back-propagation [17] is a supervised learning method of neural networks.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "SLD-resolution with DFNNs is adapted from the standard SLD-resolution (see [1] and Definition 7).", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "Finally, the input vector and the target vector are used to train the neural network with the back-propagation algorithm [17].", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "Finally, lList is represented as a vector by using the one-hot encoding [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "\u2022 A rule is encoded to a vector via the one-hot encoding [18], according to its unique ID in a rule set.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Details of the above functions can be found from [19].", "startOffset": 49, "endOffset": 53}], "year": 2017, "abstractText": "This paper introduces an SLD-resolution technique based on deep learning. This technique enables neural networks to learn from old and successful resolution processes and to use learnt experiences to guide new resolution processes. An implementation of this technique is named SLDR-DL. It includes a Prolog library of deep feedforward neural networks and some essential functions of resolution. In the SLDR-DL framework, users can define logical rules in the form of definite clauses and teach neural networks to use the rules in reasoning processes.", "creator": "LaTeX with hyperref package"}}}