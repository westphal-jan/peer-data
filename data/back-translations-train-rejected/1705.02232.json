{"id": "1705.02232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2017", "title": "Spherical Wards clustering and generalized Voronoi diagrams", "abstract": "Gaussian mixture model is very useful in many practical problems. Nevertheless, it cannot be directly generalized to non Euclidean spaces. To overcome this problem we present a spherical Gaussian-based clustering approach for partitioning data sets with respect to arbitrary dissimilarity measure. The proposed method is a combination of spherical Cross-Entropy Clustering with a generalized Wards approach. The algorithm finds the optimal number of clusters by automatically removing groups which carry no information. Moreover, it is scale invariant and allows for forming of spherically-shaped clusters of arbitrary sizes. In order to graphically represent and interpret the results the notion of Voronoi diagram was generalized to non Euclidean spaces and applied for introduced clustering method.", "histories": [["v1", "Thu, 4 May 2017 16:27:28 GMT  (466kb,D)", "http://arxiv.org/abs/1705.02232v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marek \\'smieja", "jacek tabor"], "accepted": false, "id": "1705.02232"}, "pdf": {"name": "1705.02232.pdf", "metadata": {"source": "CRF", "title": "Spherical Wards clustering and generalized Voronoi diagrams", "authors": ["Marek \u015amieja", "Jacek Tabor"], "emails": ["marek.smieja@ii.uj.edu.pl", "jacek.tabor@ii.uj.edu.pl"], "sections": [{"heading": null, "text": "In fact, it is as if most people are able to understand themselves and to understand what they are doing. (...) It is as if people are able to understand themselves. (...) It is as if they were able to understand the world. (...) It is as if they were able to understand the world. (...) It is as if they were able to understand the world. (...) It is as if they were able to understand the world. (...) It is as if they were able to think in the world. (...) It is as if they were able to understand the world. (...) It is as if they were able to understand the world. (...) It is as if they were able to understand the world. (...) It is as if they were able to think in the world. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is as if. (...) It is. (...) It is. (...) It is. (it is. (...) It is. (it is. (...) It is. (it is. (...) It is. (it is. (it is.) It is. (it is. (...) It is. (it is. (it is.) It is. (it is. (it is.) It is. (it is. (it is.). (it is. (it is.). (it is. (it is. (it is.). (it is. (it is.). (it is.). (it is. (it is. (it is.). (it is. (it is. (it is.). (it is.). (it is. (it is.). (it is. (it is.). (it is. (it is.). (it is. (it is. (it is.) It is. (it is. (it is.) It is. (it is. (it is.). (it is. (it is. (it is.) It is. (it is. (it is.). (it is.). (it is. (it is.). (it is.). (it is. (it is. (it is"}, {"heading": "II. RELATED WORKS", "text": "Hierarchical clustering is probably one of the most popular methods for partitioning data on the basis of (dis-) similarity scales [15]. The well-known k-mean algorithm [16] can also be adapted to non-Euclidean data by defining a medoid [17] that plays a role of a generalized mean term, or by using the Wards method [9], [11] which reformulates the cluster sum of squares within a cluster without imagining the cluster mean. Despite the widespread use of these methods, they are sometimes unable to detect groups with complex structures and different sizes. Many modifications have also been considered to describe clusters with arbitrary shapes [18], [19]. Spectral clustering uses eigenvectors of the similarity matrix to divide elements into groups [20]. Another problem of clustering non-euclidean data set is the appropriate selection of clusters."}, {"heading": "III. CLUSTERING METHOD", "text": "The proposed SWARDS clustering is a combination of spherical cross-entropy clustering (SCEC) [10] with a generalized Wards approach [9], [11]. In this section, we first present a basic notation and recall the Wards version of Kmean. We then show how SCEC can be generalized to non-Euclidean datasets using the Wards method."}, {"heading": "A. Wards method", "text": "In general, the k-mean method aims to produce a splitting of the dataset that optimizes a quadratic error function. For a group Y | RN, the sum of the squares within k is quadratically defined as: ss (Y) = \u2211 y-y-y-2, where mY is a mean of Y. The k-mean searches for a division of the X-RN into k quadratic disjoint sets Y1,.., Yk so that the mode of operation j = 1 ss (Yj) is minimal. Note that the formulas above cannot be used directly for non-vector data, as the mean for general datasets is not well defined. There are several alternatives [25], [26], [28], [29] which make it possible to partially overcome this difficulty as kmedoids [30] or k-clustering [31]. The technique related to kclustering and k-means is the generalized quadratic function that [11] plays the fundamental [11] function of the investigation of the [11]."}, {"heading": "B. Spherical Wards criterion function", "text": "Another advantage of the CEC is that clustering is carried out at a comparable time to the computationally efficient k method. For further details, the reader refers to [10].Spherical Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entropy Entro"}, {"heading": "C. Clustering algorithm", "text": "It can be shown that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5). We will now discuss its technical aspects in order to minimize the SWARDS criterion function (5). More specifically, in the iteration phase, we always go beyond all elements of the X level."}, {"heading": "IV. GENERALIZED VORONOI DIAGRAM", "text": "There is a natural problem with how the cluster results can be represented graphically. It is clear that we can mark the elements of each cluster with different names. In practice, however, it is usually clearer to indicate the division of the entire space. In this section, we show that for each criterion function in the non-Euclidean space, we can naturally obtain an equivalence of the Voronoi diagram."}, {"heading": "A. Classical diagram", "text": "Let us remember that in the case of the classic version of the Voronoi diagram (k mean method), the point x is associated with this cluster, whose center is closest to x. Specifically, it is classified to this cluster Yi, which minimizes d (x; mi), where mi is an average of Yi. We would like to mention that one can consider the alternative to the Voronoi diagrams, as described in [35]. It provides the division of the data, but does not induce a natural division of space (see [35] for more details). To generalize the notion of the Voronoi diagram to non-Euclidean space (Wards k mean), we must be able to calculate the distance of a point from the center of the cluster (without using it in the calculations)."}, {"heading": "B. Diagram for arbitrary criterion function", "text": "This will be useful to construct a division of space in the case of the SWARDS Method. The results obtained are in line with the classic Y diagram in the case of Wards k means presented in the previous paragraph.Let X be a room with a difference and let Y represent our data. We expand X by introducing a weight function: X 3 x \u2192 Y (x). We define the operations D < \u00b7 > and ss (\u00b7)."}, {"heading": "V. EXPERIMENTS", "text": "In this section we will discuss some basic features and possible applications of the proposed cluster method and present a short evaluation study. The implementation of SWARDS is available at http: / / www.ii.uj.edu.pl / \u02dc smieja / sWards-app.zip1."}, {"heading": "A. Synthetic data sets", "text": "To demonstrate the capabilities of SWARDS, we examined its resistance to scale change and its sensitivity to unbalanced data. We compared clustering results with those obtained with related methods that can be applied to non-Euclidean spaces: Wards k-means and Spectral Clustering (Kernlab R package was used to implement this algorithm [36]. Since SWARDS automatically detects the resulting number of groups, we performed it with 10 initial clusters, while the other methods used the number of groups returned by SWARDS2. The value of the parameter N (dimension of space) for SWARDS was set automatically with the use of the MLE method [12] \u2212 to provide more stable results, each algorithm was executed 10 times and the result with the lowest value of the criterion function became chosen.Example 5.1: Scale invarianceIn the first experiment, we examined the inventory of algorithms on scale change."}, {"heading": "B. Dimension estimation", "text": "In order to apply the SWARDS criterion Function in the case of arbitrary non-Euclidean cases, it was divided into three DS groups. For most DS groups, the value of the dimension parameter N seems to have to be defined automatically. In the previous subsection, we showed that reasonable cluster results can be achieved by calculating this value using the MLE method [12], [13]. We will experimentally show how the cluster effects differ when the value of N changes is shown. Example 5.3: Cluster number recognition Let us first consider the effects of the value of the parameter N on the detection of the resulting number of groups. To this end, a mouse-like group (see Figure 2) is clustered with different values of N start groups. The resulting number of groups is specified in Figure 6.The immediate observation is that the increase in the value of N results is determined by the increase in the number of identifiable groups."}, {"heading": "C. Applications", "text": "In this section we show that the proposed method is very useful for the analysis of biological models."}, {"heading": "D. Evaluation", "text": "In all experiments, the initial number of clusters for SWARDS was set twice as high as the actual number of groups. To establish the match between the cluster results, the other methods studied assumed the number of groups SWARDS returned as input clusters. As a measure of matches between the partitions, the Rand Index (RI) was used [38], which is defined as the ratio between pairs of true positives and false negatives, and all pairs of examples. Values close to 1 indicate that two partitions are very similar. MLE was used to calculate the optimal value of parameter N. Two types of dissimilarity measures were taken into account: the Euclidean distance and the dissimilarity determined by the Gaussian radial base function (RBF). The value of the sigma for RBF was determined as the median of the square of DS distances, which were determined in most cases."}, {"heading": "VI. CONCLUSION", "text": "In this paper, a generalization of spherical cross-entropy clustering to non-Euclidean spaces was presented; the proposed method uses a Wards approach to modify the cross-entropy criterion function in the case of arbitrary data sets; as a result, the obtained method allows the division of non-vector data into spherically shaped clusters of any size; it is a scalable invariant technique that automatically detects the final group number; our method works in a comparable time to the generalized Wards method, while the cluster effects are similar to those of GMM when focusing on spherical Gaussian distributions in Euclidean spaces. Furthermore, we have the idea of the Voronoi diagram for the case of arbitrary criteria function based on the Wards approach generalizing, which leads to identical results in the case of classical methods as k-means, while allowing a formal division of data space when focusing on non-Euclidean methods."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was partly financed by the Polish Ministry of Science and Higher Education from the 2013-2015 budget for science, grant number IP2012 055972 and by the National Science Centre (Poland), grant number 2014 / 13 / B / ST6 / 01792."}], "references": [{"title": "The EM algorithm and extensions", "author": ["G. McLachlan", "T. Krishnan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Unsupervised learning of finite mixture models", "author": ["M.A.T. Figueiredo", "A.K. Jain"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 24, no. 3, pp. 381\u2013396, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Waveform quantization of speech using Gaussian mixture models", "author": ["J. Samuelsson"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, 2004, vol. 1. IEEE, 2004, pp. I\u2013165.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Feature selection for high-dimensional genomic microarray data", "author": ["E.P. Xing", "M.I. Jordan", "R.M. Karp"], "venue": "Proceedings of the 18th International Conference on Machine Learning, vol. 1. Citeseer, 2001, pp. 601\u2013608.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Time series classification using Gaussian mixture models of reconstructed phase spaces", "author": ["R.J. Povinelli", "M.T. Johnson", "A.C. Lindgren", "J. Ye"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 16, no. 6, pp. 779\u2013783, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Handbook of chemoinformatics", "author": ["J. Gasteiger"], "venue": "Wiley Online Library,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Investigation of classification methods for the prediction of activity in diverse chemical libraries", "author": ["S.L. Dixon", "H.O. Villar"], "venue": "Journal of Computer-Aided Molecular Design, vol. 13, no. 5, pp. 533\u2013545, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Web document clustering: A feasibility demonstration", "author": ["O. Zamir", "O. Etzioni"], "venue": "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1998, pp. 46\u201354.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Generalized Ward and related clustering problems", "author": ["V. Batagelj"], "venue": "Classification and Related Methods of Data Analysis, pp. 67\u201374, 1988.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1988}, {"title": "Cross-entropy clustering", "author": ["J. Tabor", "P. Spurek"], "venue": "Pattern Recognition, vol. 47, no. 9, pp. 3046\u20133059, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Cluster-analyse-algorithmen", "author": ["H. Spath"], "venue": "M\u00fcnchen und Wien, 1975.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1975}, {"title": "Maximum likelihood estimation of intrinsic dimension", "author": ["E. Levina", "P. Bickel"], "venue": "Ann Arbor MI, vol. 48109, p. 1092, 2004.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Comments on \u201dMaximum likelihood estimation of intrinsic dimension\u201d by E. Levina and P. Bickel (2004)", "author": ["D.J.C. MacKay", "Z. Ghahramani"], "venue": "2012, available from http://www.inference.phy.cam.ac.uk/mackay/dimension/.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical clustering schemes", "author": ["S.C. Johnson"], "venue": "Psychometrika, vol. 32, no. 3, pp. 241\u2013254, 1967.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1967}, {"title": "Algorithm AS 136: A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied Statistics, pp. 100\u2013108, 1979.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1979}, {"title": "A simple and fast algorithm for k-medoids clustering", "author": ["H.S. Park", "C.H. Jun"], "venue": "Expert Systems with Applications, vol. 36, no. 2, pp. 3336\u2013 3341, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2004, pp. 551\u2013556.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Constrained kmeans clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "Proceedings of the 18th International Conference on Machine Learning, vol. 1, 2001, pp. 577\u2013584.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J.C. Burges"], "venue": "Proceedings of the 24th International Conference on Machine learning. ACM, 2007, pp. 1159\u20131166.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "A clustering technique for digital communications channel equalization using radial basis function networks", "author": ["S. Chen", "B. Mulgrew", "P.M. Grant"], "venue": "Neural Networks, IEEE Transactions on, vol. 4, no. 4, pp. 570\u2013590, 1993.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Regularization in the selection of radial basis function centers", "author": ["M.J.L. Orr"], "venue": "Neural Computation, vol. 7, no. 3, pp. 606\u2013623, 1995.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "An algorithm for selecting a good value for the parameter c in radial basis function interpolation", "author": ["S. Rippa"], "venue": "Advances in Computational Mathematics, vol. 11, no. 2-3, pp. 193\u2013210, 1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Density-based clustering", "author": ["H.P. Kriegel", "P. Kr\u00f6ger", "J. Sander", "A. Zimek"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 1, no. 3, pp. 231\u2013240, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys (CSUR), vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "K-means clustering: A half-century synthesis", "author": ["D. Steinley"], "venue": "British Journal of Mathematical and Statistical Psychology, vol. 59, no. 1, pp. 1\u201334, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Clustering by means of medoids", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": "1987.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1987}, {"title": "A sublinear time approximation scheme for clustering in metric spaces", "author": ["P. Indyk"], "venue": "Foundations of Computer Science, 1999. 40th Annual Symposium on. IEEE, 1999, pp. 154\u2013159.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "A statistical method for evaluating systematic relationships", "author": ["R.R. Sokal", "C.D. Michener"], "venue": "Univ. Kans. Sci. Bull., vol. 38, pp. 1409\u20131438, 1958.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1958}, {"title": "Image segmentation with use of cross-entropy clustering", "author": ["M. \u015amieja", "J. Tabor"], "venue": "Proceedings of the 8th International Conference on Computer Recognition Systems CORES 2013. Springer, 2013, pp. 403\u2013409.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection of elliptical shapes via cross-entropy clustering", "author": ["J. Tabor", "K. Misztal"], "venue": "Pattern Recognition and Image Analysis. Springer, 2013, pp. 656\u2013663.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Hartigan\u2019s Method: k-means Clustering without Voronoi", "author": ["M. Telgarsky", "A. Vattani"], "venue": "Proceedings of International Conference on Artifcial Intelligence and Statistics (AISTATS), 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "kernlab-an S4 package for kernel methods in r", "author": ["A. Karatzoglou", "A. Smola", "K. Hornik", "A. Zeileis"], "venue": "2004.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Random walk models in biology", "author": ["E.A. Codling", "M.J. Plank", "S. Benhamou"], "venue": "Journal of the Royal Society Interface, vol. 5, no. 25, pp. 813\u2013834, 2008.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W.M. Rand"], "venue": "Journal of the American Statistical Association, vol. 66, no. 336, pp. 846\u2013850, 1971.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1971}, {"title": "Appearance-based Object Recognition using SVMs: Which Kernel Should I Use?", "author": ["B. Caputo", "K. Sim", "F. Furesjo", "A. Smola"], "venue": "Proceedings of NIPS workshop on Statistical Methods for Computational Experiments in Visual Processing and Computer", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Distribution-based clustering, such as Gaussian mixture model (GMM), has been proven to be very useful in many practical problems [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "g in chemoinformatics where finding of chemical compounds acting on specific disease is rare [6], [7] or in Natural Language Processing where the numbers of documents that belong Data set with dissimilarity measure", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "g in chemoinformatics where finding of chemical compounds acting on specific disease is rare [6], [7] or in Natural Language Processing where the numbers of documents that belong Data set with dissimilarity measure", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "to particular domains are different [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "The method is easy to implement and has the same numerical complexity as the k-means version adapted to non Euclidean spaces [9].", "startOffset": 125, "endOffset": 128}, {"referenceID": 9, "context": "Proposed SWARDS method is a combination of spherical variant of Cross-Entropy Clustering (CEC) [10] with the generalized Wards approach [9], [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "Proposed SWARDS method is a combination of spherical variant of Cross-Entropy Clustering (CEC) [10] with the generalized Wards approach [9], [11].", "startOffset": 136, "endOffset": 139}, {"referenceID": 10, "context": "Proposed SWARDS method is a combination of spherical variant of Cross-Entropy Clustering (CEC) [10] with the generalized Wards approach [9], [11].", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": "Applied Wards approach allows for a generalization of the notion of within cluster sum of squares for the case of any dissimilarity measure [9], [11].", "startOffset": 140, "endOffset": 143}, {"referenceID": 10, "context": "Applied Wards approach allows for a generalization of the notion of within cluster sum of squares for the case of any dissimilarity measure [9], [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "Therefore, to adapt spherical CEC criterion function to general case we recommend to estimate its value from data with use of Maximum Likelihood Estimator of intrinsic dimension [12], [13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 12, "context": "Therefore, to adapt spherical CEC criterion function to general case we recommend to estimate its value from data with use of Maximum Likelihood Estimator of intrinsic dimension [12], [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 8, "context": "In the Wards method we replace it by a generalization of distance of point x from the center of cluster Y given by [9]", "startOffset": 115, "endOffset": 118}, {"referenceID": 13, "context": "The practical properties of proposed method are illustrated and examined on synthetic data sets and examples retrieved form the UCI repository [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 14, "context": "The hierarchical clustering is probably one of the most popular methods to partition data based on any kind of (dis)similarity measure [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.", "startOffset": 186, "endOffset": 189}, {"referenceID": 10, "context": "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.", "startOffset": 191, "endOffset": 195}, {"referenceID": 17, "context": "A lot of modifications were also considered to describe clusters with arbitrary shapes [18], [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "A lot of modifications were also considered to describe clusters with arbitrary shapes [18], [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "Spectral Clustering uses eigenvectors of similarity matrix to divide elements into groups [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "Examples showed that interesting effects can be obtained by applying Gaussian radial basis function (RBF) [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "The difficulty is that there is no unified methodology how to choose the radius of this function for particular situation [22], [23].", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "The difficulty is that there is no unified methodology how to choose the radius of this function for particular situation [22], [23].", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "In order to perform a distribution-based clustering a GMM is widely used in Euclidean space [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 23, "context": "On the other hand, a family of density based clustering such as DBSCAN [24] can be applied for non Euclidean data.", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "The proposed SWARDS clustering is a combination of spherical Cross-Entropy Clustering (SCEC) [10] with a generalized Wards approach [9], [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "The proposed SWARDS clustering is a combination of spherical Cross-Entropy Clustering (SCEC) [10] with a generalized Wards approach [9], [11].", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "The proposed SWARDS clustering is a combination of spherical Cross-Entropy Clustering (SCEC) [10] with a generalized Wards approach [9], [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "The technique related to kclustering and k-means is the generalized Wards method [9], [11] which plays the basic role in our investigations.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "The technique related to kclustering and k-means is the generalized Wards method [9], [11] which plays the basic role in our investigations.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "1: [11] If Y \u2282 R , then \u2211", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Given two subsets Y,Z of X we define a function [31] connected with the average linkage function (also called average neighbor function) [32], [27] as:", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "Given two subsets Y,Z of X we define a function [31] connected with the average linkage function (also called average neighbor function) [32], [27] as:", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "As a generalized within cluster Y \u2282 X sum of squares we put [9]:", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "Wards Optimization Problem [9].", "startOffset": 27, "endOffset": 30}, {"referenceID": 9, "context": "The Cross-Entropy Clustering (CEC) is a kind of distribution-based clustering which divides an Euclidean data set into groups such that each group is described by optimally fitted Gaussian probability distribution [10].", "startOffset": 214, "endOffset": 218}, {"referenceID": 9, "context": "For more details the reader is referred to [10], [33], [34].", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "For more details the reader is referred to [10], [33], [34].", "startOffset": 49, "endOffset": 53}, {"referenceID": 31, "context": "For more details the reader is referred to [10], [33], [34].", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": ", Yk of X the associated criterion function is defined by [10]", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "2: If Y \u2282 R then [10]:", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "In the present study we apply the Maximum Likelihood Estimation (MLE) of intrinsic dimension of X proposed in [12] and modified in [13].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "In the present study we apply the Maximum Likelihood Estimation (MLE) of intrinsic dimension of X proposed in [12] and modified in [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": ", xn} the maximum likelihood estimator of a dimension N of X calculated for each x \u2208 X equals [12]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": ", n} to obtain the final estimator of N [13].", "startOffset": 40, "endOffset": 44}, {"referenceID": 8, "context": "One can show that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5).", "startOffset": 69, "endOffset": 72}, {"referenceID": 15, "context": "One can show that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5).", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "One can show that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5).", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Clearly, the procedure is not deterministic and leads to a local minimum of (5) [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "3: [11] Let Y \u2282 X and x \u2208 X .", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "We would like to mention that one can consider the alternative to the Voronoi diagrams as described in [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "It provides the partition of data but does not induce a natural partition of the space (see [35] for more details).", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "1: [11] Let x \u2208 R be fixed and Y \u2282 R be a subset of R with mean mY .", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "We compared the clustering results with the ones obtained with use of related methods which can be applied for non Euclidean spaces: Wards k-means and Spectral Clustering (kernlab R package was used for the implementations of this algorithm [36]).", "startOffset": 241, "endOffset": 245}, {"referenceID": 11, "context": "The value of parameter N (dimension of space) for SWARDS was set automatically with use of MLE method [12], [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "The value of parameter N (dimension of space) for SWARDS was set automatically with use of MLE method [12], [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "In the previous subsection we showed that the reasonable clustering results can be obtained calculating this value using MLE method [12], [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "In the previous subsection we showed that the reasonable clustering results can be obtained calculating this value using MLE method [12], [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 34, "context": "It is assumed that a population follows a random walk model P (x, n, t) on a plane [37], where at each unit of time an instance moves randomly in one of four directions: left, right, up or down.", "startOffset": 83, "endOffset": 87}, {"referenceID": 34, "context": "It is worth to mention that a probability distribution of a population converges to spherical Gaussian one [37].", "startOffset": 107, "endOffset": 111}, {"referenceID": 35, "context": "Partitions agreement measured by Rand index [38] for Wards k-means equals 96% while for SWARDS is 98%.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "We carried out the experiments on selected UCI data sets [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "As a measure of agreements between partitions the Rand index (RI) was used [38].", "startOffset": 75, "endOffset": 79}, {"referenceID": 36, "context": "The value of sigma for RBF was estimated as a median of the squared distances between all pairs of data set elements [39].", "startOffset": 117, "endOffset": 121}], "year": 2017, "abstractText": "Gaussian mixture model is very useful in many practical problems. Nevertheless, it cannot be directly generalized to non Euclidean spaces. To overcome this problem we present a spherical Gaussian-based clustering approach for partitioning data sets with respect to arbitrary dissimilarity measure. The proposed method is a combination of spherical Cross-Entropy Clustering with a generalized Wards approach. The algorithm finds the optimal number of clusters by automatically removing groups which carry no information. Moreover, it is scale invariant and allows for forming of spherically-shaped clusters of arbitrary sizes. In order to graphically represent and interpret the results the notion of Voronoi diagram was generalized to non Euclidean spaces and applied for introduced clustering method.", "creator": "TeX"}}}