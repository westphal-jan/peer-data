{"id": "1705.02269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Sequential Attention: A Context-Aware Alignment Function for Machine Reading", "abstract": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (Who did What and CNN datasets) and show that it dramatically improves a strong baseline like the Stanford Reader. The resulting model is competitive with the state of the art.", "histories": [["v1", "Fri, 5 May 2017 15:37:11 GMT  (816kb,D)", "http://arxiv.org/abs/1705.02269v1", "4 pages"], ["v2", "Mon, 26 Jun 2017 22:25:55 GMT  (428kb,D)", "http://arxiv.org/abs/1705.02269v2", "To appear in ACL 2017 2nd Workshop on Representation Learning for NLP. Contains additional experiments in section 4 and a revised Figure 1"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian brarda", "philip yeres", "samuel r bowman"], "accepted": false, "id": "1705.02269"}, "pdf": {"name": "1705.02269.pdf", "metadata": {"source": "CRF", "title": "Sequential Attention", "authors": ["Sebastian Brarda", "Philip Yeres", "Samuel R. Bowman"], "emails": ["sb5518@nyu.edu", "pcy214@nyu.edu", "bowman@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "This article presents a method that addresses this problem by inserting explicit context sensitivity into the soft-attention scoring functionality. We show the effectiveness of this approach in the task of cloze-style reading comprehension. A problem in cloze style consists of a passage p, a question q and an answer drawn from the entities mentioned in the passage. In particular, we use the CNN Dataset (Hermann et al., 2015), which introduces the task in cloze-style reading understanding."}, {"heading": "2 Related Work", "text": "In addition to the Stanford reader model by Chen et al. (2016), several other modelling approaches have been developed to address these reading comprehension tasks. Seo et al. (2016) introduced the bi-directional attention flow, which consists of a multi-step hierarchical process to represent context at different levels of granularity; it uses the concatenation of password representation, question representation and the element-by-element product of these vectors in their attention flow layer. This is a more complex variant of the classic two-step concept, which multiplies this concatenated vector by a vector of weights, generating attention scales. Dhingra et al. (2016) recently proposed a gated Attention Reader that integrates a multi-hop structure with a novel attention mechanism, which essentially represents query-specific representations of the characters in the doctrinal document, which multiply the attention prediction by a product."}, {"heading": "3 Modeling", "text": "Considering the tuple (passage, question, answer), our goal is to predict Pr (a | d, q) where a refers to the answer, d to the passage, and q to the question. We define the words of each passage and question as d = d1,.., dm or q = q1,..., ql, with exactly one Qi containing the placeholder @, which is a space that can be correctly filled in by the answer. With calibrated probabilities Pr (a | d, q), we take the argmaxa Pr (a | d, q), where possible limited to a subset of anonymized entity symbols present in d. In this section, we present two models for this distribution: the Stanford Reader and our Advanced Sequential Attention Model."}, {"heading": "3.1 Stanford Reader", "text": "Encryption Each word of the vocabulary, including anonymized units, is mapped to a ddimensional vector by embedding the matrix E-Rd-V-V. For convenience, we call the vectors of the passage and question d = d1,.., dm and q = q1,..., ql. Stanford Reader (Chen et al., 2016) uses bidirectional GRUs (Cho et al., 2014) to encode the passage and questions. For the passage, the hidden state is defined: hi = concat (\u2212 \u2192 hi, \u2190 \u2212 hi). Where contextual embedding di each word in the passage is encoded in both directions. For the question, the last hidden representation of each direction is linked: j = concat (\u2212 jl, \u2190 \u2212 j1).Attention and response selection In this case, a two-dimensional attention layer (Luong al, 2015 jat, each direction \u2212 where \u2212 is used)."}, {"heading": "3.2 Sequential Attention", "text": "Our model uses the idea of attention feeding (Luong et al., 2015) in different ways. Luong et al. (2015) feed their original attention vectors into the next RNN step by linking them to the previous hidden state. The goal is to make the model fully aware of the previous alignment choices. In the sequential attention model, we define the vectors instead of a single value \u03b1i for each word in the passage by using a two-dimensional term. Where hi is the hidden representation and ci is the context representation with a pseudo-bilinear term2. Instead of making the dot product as in the two-dimensional term, we perform an element-way multiplication to end up with a vector instead of a scalar."}, {"heading": "4 Experiments and Results", "text": "For CNN, we used the anonymized version of the dataset published by Hermann et al. (2015), which contained 380,298 trainings, 3,924 developers and 3,198 test examples. For WDW, we used Onishi et al. (2016) the data generation script to reproduce their WDW data, resulting in 127,786 trainings, 10,000 developers and 10,000 test examples.3 We used the strict version of WDW, which is a more difficult task and for their leaderboard.4Training We implemented all our models in Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) and used the Stanford Reader et al (Chen et al., 2016) the open source implementation as a reference. We used largely the same hyperparameters as Chen et al. (2016) in2Note that doing softmax over the sum of Stanford terms of the Vector.We would not perform the training."}, {"heading": "4.1 Results", "text": "Who did what in our experiments, Stanford Reader got a 65.6% accuracy on the strict WDW record 69 increased numbers Finally compared to the 64% that Onishi et al. (2016) reported them. Sequential Attention Reader got 67.21%, which reached the second position in the ranking, only to exceed the 71.2% of the gated Attention Reader with qe-comm (Li et al., 2016) features and fixed GloVe embedding. However, GA readers with no features and fixed embedding leads significantly lower at 67%. We have not used these types of features in our experiments, so it is likely that our model will be even better with those in future work. We experimented with fixed embedding, but the results were5The GloVe word vectors used were pre-trained with 6 billion tokens, with an unencrypted vocab of 400K words, and were retrieved from Wikipedia 2014 and Gigaword 5.6200, we also tried to increase the number of vocabularies to 200.We finally increased the number of vocabulary used."}, {"heading": "4.2 Discussion", "text": "The difference between our sequential attention and the classical two-dimensional attention is that we get the distributed representation of the two-dimensional similarity on each component and pass this contextual information on to the attention via other words. In other words, if we maintain the two-dimensional attention layer mathematically: \u03b1i = softmaxi (jWshi), it only cares about the absolute value of the resulting \u03b1i (the amount of attention it gives to that word). If we maintain the vector \u03b3i, we can also know what dimensions the distributed representation of matter weighted in this decision was. If we use this information to feed a new GRU, it helps the model learn how to allocate the attention surrounding words. Compared to sequential attention (SA), the bidirectional attention flow is a much more complex architecture with a quantum representation for each word in the question of attention (unlike the semi-dimensional attention that only requires)."}, {"heading": "5 Conclusion and Discussion", "text": "In this paper, we have developed a novel and simple model with a sequential attention mechanism that works close to the state of the art on CNN and WDW datasets by enhancing the bilinear attention mechanism with an additional bidirectional RNN layer. This additional layer enables the flow of attentive context information to calculate the attention weight for each token. For future work, we would like to run our model on other datasets such as SQuAD and Marco. We also believe that some elements of the sequential attention model could be mixed with ideas used in recent research by Dhingra et al. (2016) and Seo et al. (2016)."}, {"heading": "Acknowledgements", "text": "This work was the result of a semester project for the NYU course DS-GA 3001, Natural Language Understanding with Distributed Representations. Bowman thanks for the support of a Google Faculty Research Award."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "CoRR abs/1606.02858. http://arxiv.org/abs/1606.02858.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "CoRR abs/1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov."], "venue": "CoRR abs/1606.01549. http://arxiv.org/abs/1606.01549.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Lasagne: First release. https://doi.org/10.5281/zenodo.27878", "author": ["man", "Gbor Takcs", "Peter de Rivaz", "Jon Crall", "Gregory Sanders", "Kashif Rasul", "Cong Liu", "Geoffrey French", "Jonas Degrave"], "venue": null, "citeRegEx": "man et al\\.,? \\Q2015\\E", "shortCiteRegEx": "man et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "CoRR abs/1506.03340. http://arxiv.org/abs/1506.03340.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."], "venue": "CoRR abs/1702.00887. http://arxiv.org/abs/1702.00887.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "author": ["Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu."], "venue": "CoRR abs/1607.06275. http://arxiv.org/abs/1607.06275.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "CoRR abs/1508.04025. http://arxiv.org/abs/1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David A. McAllester."], "venue": "CoRR abs/1608.05457. http://arxiv.org/abs/1608.05457.", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Min Joon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "CoRR abs/1611.01603. http://arxiv.org/abs/1611.01603.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "CoRR abs/1605.02688. http://arxiv.org/abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Soft attention (Bahdanau et al., 2014), a differentiable method for selecting the inputs for a component of a model from a set of possibilities, has been crucial to the success of artificial neural network models for natural language understanding tasks like reading comprehension that take short passages as inputs.", "startOffset": 15, "endOffset": 38}, {"referenceID": 5, "context": "In particular, we use the CNN dataset (Hermann et al., 2015), which introduced the task into widespread use in evaluating neural networks for language understanding, and the newer and more carefully quality-controlled Who did What dataset (Onishi et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 9, "context": ", 2015), which introduced the task into widespread use in evaluating neural networks for language understanding, and the newer and more carefully quality-controlled Who did What dataset (Onishi et al., 2016).", "startOffset": 186, "endOffset": 207}, {"referenceID": 1, "context": "We evaluate this scoring function within the context of the Stanford Reader (Chen et al., 2016), and show that it yields dramatic improvements in performance.", "startOffset": 76, "endOffset": 95}, {"referenceID": 3, "context": "On both datasets, it is outperformed only by the Gated Attention Reader (Dhingra et al., 2016), which in some cases has access to features not explicitly seen by our model.", "startOffset": 72, "endOffset": 94}, {"referenceID": 1, "context": "In addition to Chen et al. (2016)\u2019s Stanford Reader model, there have been several other modeling approaches developed to address these reading comprehension tasks.", "startOffset": 15, "endOffset": 34}, {"referenceID": 3, "context": "Dhingra et al. (2016) recently proposed a Gated-Attention Reader which integrates a multihop structure with a novel attention mechanism, essentially building query specific representations of the tokens in the document to improve predic-", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "Luong et al. (2015) study several issues in the design of soft attention models in the context of translation, and propose the use of a bilinear scoring function.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "In work largely concurrent to our own, Kim et al. (2017) explore the use of conditional random fields (CRFs) to impose a variety of constraints on attention distributions achieving strong results on several sentence level tasks.", "startOffset": 39, "endOffset": 57}, {"referenceID": 1, "context": "The Stanford Reader (Chen et al., 2016) uses bidirectional GRUs (Cho et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 2, "context": ", 2016) uses bidirectional GRUs (Cho et al., 2014) to encode the passage and questions.", "startOffset": 32, "endOffset": 50}, {"referenceID": 8, "context": "Attention and answer selection In this case, a bilinear attention layer (Luong et al., 2015) is used, where matrix Ws is a learned parameter:", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "Our model uses the idea of attention input-feeding (Luong et al., 2015) in a different way.", "startOffset": 51, "endOffset": 71}, {"referenceID": 8, "context": "Our model uses the idea of attention input-feeding (Luong et al., 2015) in a different way. Luong et al. (2015) feed their original attention vectors1,", "startOffset": 52, "endOffset": 112}, {"referenceID": 5, "context": "anonymized version of the dataset released by Hermann et al. (2015), containing 380,298 training, 3,924 dev, and 3,198 test examples.", "startOffset": 46, "endOffset": 68}, {"referenceID": 5, "context": "anonymized version of the dataset released by Hermann et al. (2015), containing 380,298 training, 3,924 dev, and 3,198 test examples. For WDW we used Onishi et al. (2016)\u2019s data generation script to reproduce their WDW data, yielding 127,786 training, 10,000 dev, and 10,000 test examples.", "startOffset": 46, "endOffset": 171}, {"referenceID": 1, "context": ", 2015) and used the Stanford Reader (Chen et al., 2016) open source implementation as a reference.", "startOffset": 37, "endOffset": 56}, {"referenceID": 1, "context": ", 2015) and used the Stanford Reader (Chen et al., 2016) open source implementation as a reference. We largely used the same hyperparameters as Chen et al. (2016) in", "startOffset": 38, "endOffset": 163}, {"referenceID": 10, "context": "the Stanford Reader: |V | = 50K, embedding size d = 100, 100 dimensional GloVe (Pennington et al., 2014) word embeddings5 for initialization, hidden size h = 128.", "startOffset": 79, "endOffset": 104}, {"referenceID": 7, "context": "2% from the Gated Attention Reader with qe-comm (Li et al., 2016) features and fixed GloVe embeddings.", "startOffset": 48, "endOffset": 65}, {"referenceID": 8, "context": "6% accuracy on the strict WDW dataset compared to the 64% that Onishi et al. (2016) they reported.", "startOffset": 63, "endOffset": 84}, {"referenceID": 1, "context": "None of these changes resulted in significant performance improvements in accordance with Chen et al. (2016).", "startOffset": 90, "endOffset": 109}, {"referenceID": 1, "context": "on exactly the same CNN data used by Chen et al. (2016).", "startOffset": 37, "endOffset": 56}, {"referenceID": 1, "context": "6% reported by Chen et al. (2016). Our model achieved an accuracy of 77.", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "6% reported by Chen et al. (2016). Our model achieved an accuracy of 77.1% which, to the best of our knowledge, has only been surpassed by Dhingra et al. (2016) with their GatedAttention Reader model.", "startOffset": 15, "endOffset": 161}, {"referenceID": 3, "context": "Also, we think that some elements of the Sequential Attention model could be mixed with ideas applied in recent research from Dhingra et al. (2016) and Seo et al.", "startOffset": 126, "endOffset": 148}, {"referenceID": 3, "context": "Also, we think that some elements of the Sequential Attention model could be mixed with ideas applied in recent research from Dhingra et al. (2016) and Seo et al. (2016). We also believe that the Sequential Attention mechanism may benefit other tasks as well, such as neural machine translation.", "startOffset": 126, "endOffset": 170}], "year": 2017, "abstractText": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (Who did What and CNN) and show that it dramatically improves a strong baseline like the Stanford Reader. The resulting model is competitive with the state of the art.", "creator": "LaTeX with hyperref package"}}}