{"id": "1705.02302", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions", "abstract": "The driving force behind convolutional networks - the most successful deep learning architecture to date, is their expressive power. Despite its wide acceptance and vast empirical evidence, formal analyses supporting this belief are scarce. The primary notions for formally reasoning about expressiveness are efficiency and inductive bias. Efficiency refers to the ability of a network architecture to realize functions that require an alternative architecture to be much larger. Inductive bias refers to the prioritization of some functions over others given prior knowledge regarding a task at hand. In this paper we provide a high-level overview of a series of works written by the authors, that through an equivalence to hierarchical tensor decompositions, analyze the expressive efficiency and inductive bias of various architectural features in convolutional networks (depth, width, convolution strides and more). The results presented shed light on the demonstrated effectiveness of convolutional networks, and in addition, provide new tools for network design.", "histories": [["v1", "Fri, 5 May 2017 17:09:58 GMT  (1454kb,D)", "https://arxiv.org/abs/1705.02302v1", null], ["v2", "Mon, 8 May 2017 16:54:48 GMT  (2602kb,D)", "http://arxiv.org/abs/1705.02302v2", null], ["v3", "Tue, 9 May 2017 06:15:41 GMT  (2602kb,D)", "http://arxiv.org/abs/1705.02302v3", null], ["v4", "Sat, 10 Jun 2017 19:07:18 GMT  (3049kb,D)", "http://arxiv.org/abs/1705.02302v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["nadav cohen", "or sharir", "yoav levine", "ronen tamari", "david yakira", "amnon shashua"], "accepted": false, "id": "1705.02302"}, "pdf": {"name": "1705.02302.pdf", "metadata": {"source": "CRF", "title": "Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions", "authors": ["Nadav Cohen", "Yoav Levine", "Ronen Tamari", "Amnon Shashua", "TAMARI YAKIRA SHASHUA"], "emails": ["COHENNADAV@CS.HUJI.AC.IL", "OR.SHARIR@CS.HUJI.AC.IL", "YOAVLEVINE@CS.HUJI.AC.IL", "RONENT@CS.HUJI.AC.IL", "DAVIDYAKIRA@CS.HUJI.AC.IL", "SHASHUA@CS.HUJI.AC.IL"], "sections": [{"heading": null, "text": "The primary terms for formal reasoning about expressivity are efficiency and inductive bias. Expressive efficiency refers to the ability of a network architecture to realize functions that require an alternative architecture to be much larger. Inductive bias refers to the prioritization of some functions over others because there is prior knowledge of a task. In this paper, we provide an overview of a series of works by the authors that analyze the expressive efficiency and inductive bias of various Convolutionary Network Architecture features (depth, width, steps, etc.) through an equivalence to hierarchical tensor compositions. The presented results shed light on the proven effectiveness of Convolutionary Networks and also provide new tools for network design. 1Keywords: Convolutional Networks, Expressiveness, Hierarchical Tensor Decompositions"}, {"heading": "1. Introduction", "text": "Since the work of Krizhevsky et al. (2012), they have established themselves in the field of visual recognition, and more recently they have also provided state-of-the-art results in speech and word processing tasks (see, for example, van den Oord et al. (2016); Kalchbrenner et al. (2016). Unlike classical deep network architectures, such as the multi-layered perceptron (Feed-forward fully connected neural network - Rosenblatt (1961)), the use of a modern Constitutional Network requires the setting of dozens or even hundreds of architectural parameters. Namely, in addition to the basic decisions of network depth, the width of each layer, and the non-linear activations (e.g. Sigmoid or ReLU - Nair and Hinton (2010), one has to decide on the type of pooling operator in each layer."}, {"heading": "2. Expressive Efficiency and Inductive Bias", "text": "In fact, most of them are able to survive on their own if they are not able to flourish."}, {"heading": "2.1. Questions on the Expressive Efficiency and Inductive Bias of Convolutional Networks", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave and how they want to behave."}, {"heading": "3. Convolutional Arithmetic Circuits and Hierarchical Tensor Decompositions", "text": "In order to analyze expression efficiency and inductive bias, we need to look at other types of real networks, in particular the questions set forth in sec. 2.1. (Let's look at a family of models called the \"convolutional arithmetic circus.\" (Let's look at a particular selection of non-linearity) They arise from their close relationship to various mathematical fields (tensor analysis, measurement theory, theoretical physics, graph theory, and more), which make them particularly adaptable to theoretical analysis. (We will see in sec. 4.1 how mathematical machinery is developed for analyzing revolutionary arithmetic circuits.) We can adapt them to other types of theoretical analysis."}, {"heading": "4.1. Convolutional Rectifier Networks", "text": "However, we have shown how the mathematical machinery developed for the analysis of revolutionary arithmetic circuits is facilitated by its equivalence to take into account decompositions (sec. 3).The central operator in hierarchical tensor decompositions is the outer product, also known as the tensor product."}, {"heading": "9. Conclusion", "text": "Expressive efficiency and inductive bias are the primary terms for formal considerations of expressive power - the driving force behind Convolutionary Networks. Perhaps more important than their role in formalizing common beliefs and explaining empirically observed phenomena is the potential of expressive efficiency and inductive bias to provide new tools for network design. Expressive efficiency can be seen as enhancing the expressive power of a network, whereas inductive bias is the better use of expressive resources in the face of task requirements. Mounting empirical evidence repeatedly shows that both methods lead directly to improved performance (especially accuracy). By using equivalence to hierarchical tensor decompositions, we analyzed the expressive efficiency and inductive bias of various architectural features in Convolutionary Networks. Specifically, we investigated the effects of network depth, layer width, geometry of pool windows, and overlapping network schematics as examples of preemptive steps, rather than preemptive schematics and preemptive steps."}, {"heading": "Acknowledgments", "text": "This work, as well as all the work it reviewed, was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), the ISF Center, and the European Research Council (TheoryDL Project). Nadav Cohen is supported by a Google Doctoral Fellowship in Machine Learning."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The driving force behind convolutional networks \u2013 the most successful deep learning architecture to date, is their expressive power. Despite its wide acceptance and vast empirical evidence, formal analyses supporting this belief are scarce. The primary notions for formally reasoning about expressiveness are efficiency and inductive bias. Expressive efficiency refers to the ability of a network architecture to realize functions that require an alternative architecture to be much larger. Inductive bias refers to the prioritization of some functions over others given prior knowledge regarding a task at hand. In this paper we overview a series of works written by the authors, that through an equivalence to hierarchical tensor decompositions, analyze the expressive efficiency and inductive bias of various convolutional network architectural features (depth, width, strides and more). The results presented shed light on the demonstrated effectiveness of convolutional networks, and in addition, provide new tools for network design. 1", "creator": "LaTeX with hyperref package"}}}