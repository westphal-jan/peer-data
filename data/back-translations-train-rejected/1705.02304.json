{"id": "1705.02304", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Deep Speaker: an End-to-End Neural Speaker Embedding System", "abstract": "We present Deep Speaker, a neural speaker embedding system that maps utterances to a hypersphere where speaker similarity is measured by cosine similarity. The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN and GRU architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity. Experiments on three distinct datasets suggest that Deep Speaker outperforms a DNN-based i-vector baseline. For example, Deep Speaker reduces the verification equal error rate by 50% (relatively) and improves the identification accuracy by 60% (relatively) on a text-independent dataset. We also present results that suggest adapting from a model trained with Mandarin can improve accuracy for English speaker recognition.", "histories": [["v1", "Fri, 5 May 2017 17:10:16 GMT  (649kb,D)", "http://arxiv.org/abs/1705.02304v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chao li", "xiaokong ma", "bing jiang", "xiangang li", "xuewei zhang", "xiao liu", "ying cao", "ajay kannan", "zhenyao zhu"], "accepted": false, "id": "1705.02304"}, "pdf": {"name": "1705.02304.pdf", "metadata": {"source": "CRF", "title": "Deep Speaker: an End-to-End Neural Speaker Embedding System", "authors": ["Chao Li", "Xiaokong Ma", "Bing Jiang", "Xiangang Li", "Xuewei Zhang", "Xiao Liu", "Ying Cao", "Ajay Kannan", "Zhenyao Zhu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Two common recognition tasks are the verification of speaker networks (which determine whether the claimed identity of a speaker is true or false) and the identification of speakers (which classify the identity of an unknown voice among a number of speakers). Verification and identification algorithms may require the speaker to utter a specific phrase (text-based recognition) or to approach the audio transcript agnostically (non-text recognition).In all of these sub-tasks, embedding methods can be used to move utterances into a space where the speakers resemble each other. Although many algorithms have shifted the state of art in recent years [1]. [2] This framework can still be a challenging recognition of speakers, the traditional approach to speech recognition of speakers leads to ivectors [3] and probable linear discrimination (PLDA)."}, {"heading": "2 Related Work", "text": "Traditionally, i-vectors are used to model variability between loudspeakers [3]. i-vector-based loudspeaker recognition models perform classifications using cosmic similarities between i-vectors or advanced techniques such as PLDA [20], heavy tailed PLDA [21] and Gauss-PLDA [5]. There are several papers that replace parts of the traditional loudspeaker recognition system with DNN. One approach is to train a GMM on bottleneck functions extracted from a DNN, and then extract i-vectors [22]. Another DNN-based approach uses an acoustic speech recognition DNN instead of a UBM-GMM to generate frame vector computing [4]."}, {"heading": "3 Deep Speaker", "text": "Figure 1 illustrates the architecture of Deep Speaker. Raw audio is first pre-processed using the steps described in Section 4.3. Then we use a feed-forward DNN to extract features from the pre-processed audio. We experiment with two different core architectures: a ResNetstyle [16] deep CNN and a Deep Speech 2 (DS2) -style architecture consisting of curved layers followed by GRU layers. Details of these networks are described in Section 3.1. An average layer converts frame-level input into a speaker representation at speaker level. Subsequently, an affine layer and a length normalization layer form the temporally pooled features onto a speaker embedding, as illustrated in Section 3.2. Finally, the triplet loss layer works on pairs of embedding by minimizing the cosinine similarities of embedding of pairs from the same speaker and minimizing those from different speakers as described in Section 3.3."}, {"heading": "3.1 Neural Network Architecture", "text": "As mentioned above, we use two types of deep architectures for extracting frame level audio features."}, {"heading": "3.1.1 Residual CNN", "text": "ResNet [16] has been proposed to simplify the training of very deep CNNs. ResNet consists of a number of stacked residual blocks (ResBlocks). Each ResBlock contains direct connections between the outputs of the lower layer and the inputs of the higher layer, as shown in Figure 2. ResBlock is defined as ash = z (x, Wi) + x, (1), where x and h are the input and output of the considered layers, and z is the mapping function of the stacked nonlinear layer. Note that identity shortcut connections of x do not add additional parameters and computational complexity. Table 1 shows the details of the proposed ResCNN architecture. As described in Figure 2, the ResBlock contains two convolutional layers with 3 x 3 filters and 1 x stripes. Each block has an identical structure and the skip connection is the identity mapping of x."}, {"heading": "3.1.2 GRU Network", "text": "We are also experimenting with recurring frame-level feature extraction networks because they have proven well in speech recognition [19]. [23] showed that a GRU with an LSTM is comparable to a properly initialized forge-gate bias, and their best variants compete with each other. We chose GRUs because previous speech recognition experiments [18] on smaller datasets showed that GRU and LSTM achieve similar accuracy on the same number of parameters, but the GRUs were quicker to train and less likely to diverge from each other. Details of the proposed GRU architecture are in Table 2. A 5 x 5 cm filter size, 2 x 2 cm step conversion layer (as in the ResCNN architecture) reduces dimensionality in both the time and frequency domains, allowing faster calculation of the GRU layer. Following the convolutional layer, three forward GRU layers are recurring with 1024 units in the time dimension."}, {"heading": "3.2 Speaker Embedding", "text": "Unlike the pooling layer in [7], we do not use a standard deviation of the outputs of the image plane. Activation of the image plane h is calculated as follows: h = 1T T \u2212 1 \u2211 t = 0 x (t) (3), where T is the number of images in the enunciation. An affine layer then projects the representation of the image plane into a 512-dimensional embedding. We normalize embedding in order to have a uniform standard, and use cosine similarity between pairs in the objective function: cos (xi, xj) = x T i xj (4), where xi and xj are two embedding."}, {"heading": "3.3 Triplet Loss and Selection", "text": "We model the probability of embedding xi and xj belonging to the same speaker by their cosmic similarity in Equation (4), which allows us to use the triplet loss function as in FaceNet [11]. As shown in Figure 3, triplet loss takes as input three examples, an anchor (one utterance by a particular speaker), a positive example (another utterance by the same speaker) and a negative example (one utterance by another speaker). We try to make updates that the cosmic similarity between the anchor and the positive example is greater than the cosmic similarity between the anchor and the negative example [11]. Formally, sapi \u2212 s is an i (5) where sapi is the cosmic similarity between the anchor and the positive example, and the positive example is greater than the cosmic similarity between the anchor and the negative example."}, {"heading": "3.4 Softmax Pre-training", "text": "To avoid sub-optimal local minimums early in training, it was suggested to use semi-hard negative examples, since they are further away from the anchor than the positive specimen, but still hard, because the AN-cosine similarity is similar to the AP-cosine similarity. That is, sani + \u03b1 > s ap i > s an i. In the early stages, the model trains only with semi-hard negative examples, followed by not even training with hard negative ones. However, proper planning of \"semi-hard\" samples is not easy because of the variability of model and data sets. Instead of the semi-hard rebound, we use a softmax and cross-entropy loss to pre-train the model. It uses a classification layer [8] to replace the length normalization and triplet loss layers in the standard deep speaker architecture."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and Evaluation", "text": "# spkr # utt # utt # utt / utt # utt # 5 # 5 # 5 # 5 # 5 # 5 # 5 # 5 # 6 # 6 # 6 # 6 # 6 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 8 # 8 # 8 # 7 # 8 # 8 # 7 # 8 # 8 # 7 # 8 # 8 # 8 # 7 # 8 # 8 # 7 # 8 # 8 # 8 # 8 # 8 # 8 7 # 8 # 8 # 8 7 7 # 7 7 7 7 # 7 7 7 # 7 # 7 # 7 # 7 # 7 # 7 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 7 # 8 # 8 # 7 # 8 # 8 # 7 # 8 # 8 # 7 # 8 # 8 # 8 # 8 # 8 # 7 # 7 # 7 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 7 # 7 # 7 # 7 # 8 # 8 # 8 # 8 # 8 # 7 # 8 # 8 # 8 # 8 # 7 # 7 # 8 # 8 # 7 7 # 8 # 7 7 7 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 7 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 8 # 7 # 8 # 7 # 8 7 # 8 7 # 8 7 # 8 7 # 8 7 # 8 7 # 8 7 7 # 8 7 # 8 7 7 7 7 7 7 7 7 7 7 7 7 7"}, {"heading": "4.2 Baseline DNN i-vector System", "text": "The basic model of the DNN i vector is based on [4]. Raw audio is converted into 40-dimensional log-mel filter bank (Fbank) coefficients and 20-dimensional MFCC with a frame length of 25ms. Delta and acceleration are appended to the input, and an energy-based frame-level VAD selects features that match the language frames. A seven-layer DNN containing 600 input nodes, 1024 nodes in each hidden layer and 4682 output nodes is trained with cross-entropy, using the alignment of an HMM-GMM model. The DNN input layer consists of 15 frames (7 frames on each side of the frame for which predictions are made), with each frame corresponding to 40-dimensional Fbank coefficients. DNN is used to provide the rear probability in the proposed frame for the 4,682 senses defined by a decision tree."}, {"heading": "4.3 Training Methodology", "text": "As described in Section 3.4, the Deep Speaker models are trained in two stages: Softmax pre-training and fine adjustment of triplet loss. In both stages we use synchronous SGD with 0.99 impulse [24], with a linear decrease in learning rate from 0.05 to 0.005. The model is prepared for 10 epochs with a minibatch size of 64 and finely tuned for 15 epochs with triplet loss with a minibatch size of 128. Training pairs are reshuffled in each epoch. The margin is set to 0.1 in 5 A development record for hyperparameter setting and premature stopping."}, {"heading": "5 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Speaker-independent Experiments on UIDs", "text": "First, we compare the DNN-based i-vector system and Deep Speaker on the UIDs dataset with the Train50k partition as a training set and the Eva200 partition as a test set. Deep Speaker models are trained with different neural network architectures (GRU or ResCNN) and different strategies (Softmax, Triplet loss or Softmax + Triplet loss, i.e. Softmax pre-training followed by Triplet loss fine tuning).The results are listed in Table 5. All Deep Speaker models achieve remarkable improvements over the baseline, approximately 50-80% relative reduction over EER and 60-70% relative improvement over ACC."}, {"heading": "5.1.1 Softmax Pre-training", "text": "Training with \"softmax + triplet\" loss achieves the best performance, followed by pure treble losses and pure Softmax losses in decreasing order. Specifically, ResCNN achieves a relative reduction of 63.62% and 17.10% for EER and 47.53% and 31.33% for errors (1 \u2212 ACC) compared to pure Softmax loss and triplet loss models. The pre-trained GRU architecture achieves a relative reduction of 48.89% and 14.24% for EER and 38.05% and 33.59% for identification errors compared to the other two GRU systems. The results confirm the benefits of end-to-end training and Softmax pre-training."}, {"heading": "5.1.2 Network Architecture", "text": "While the GRU architecture outperforms ResCNN with soft-max-only training, ResCNN outperforms GRU layers after triplet loss fine tuning. As shown in Table 5, GRU has an 11.58% lower EER and 6.09% lower error rate compared to ResCNN after soft-max training. After triplet loss training, ResCNNs had a 19.49% lower EER and 10.88% lower error rate than GRUs. Since the \"Softmax + Triplet Loss\" training strategy achieves the best performance for both GRU and ResCNN, we will use it in the following experiments and also omit this label to save time. Time is another important consideration when selecting network architectures. We measure the training speed of a model as the number of minibatches that can be processed per second. In this experiment, ResCNN can process 0.23 minibatches per second while GRU processes 0.44 minibatches per second."}, {"heading": "5.1.3 System Combination", "text": "In order to merge ResCNN and GRU, we examine two methods: the embedding of fusion and score fusion. In the first method, we add the embedding of speakers from both models together, followed by length normalization and cosine score. In the second method, we first normalize the values based on mean and variance calculated from all scores, and then add them up. Table 6 shows that compared to the best single system (ResCNN), both fused systems improve the baselines of the single system. In particular, the score fusion method achieves the best performance with a reduction of EER of 7.17% and 13.37%, respectively."}, {"heading": "5.1.4 Amount of Training Data", "text": "We are not experimenting with the i-vector baseline here, as it is too time-consuming and computationally expensive. 11For example, the total variance matrix T in the i-vector model is too difficult to calculate on a large dataset. In practice, people usually train the i-vectors using subsets of a large dataset. In fact, we have tried to train i-vector systems. It is clear that the use of tens of millions of samples results in a performance boost. Compared to using only about 1 / 5 of the data, using the full dataset reduces the identification error and the EER by 17.94% and 21.65% for ResCNN and 15.16% and 13.88% for GRU."}, {"heading": "5.1.5 Utterances Number for Enrollment", "text": "In order to investigate how the inclusion of the enrollment expression affects the recognition tasks, we select 1 to 5 embeddings per person. Speaker embeddings are determined by averaging the embeddings of the enrollment expression. As before, the attempts to verify and identify the speakers were created by randomly selecting for each embedding of an AP and 99 AN speakers. In total, 280,000 attempts were made. Table 8 shows that the EER decreases and the ACC increases as the number of enrollment expressions increases, albeit with decreasing yield. These results have an impact on the design decisions of production language recognition systems. In particular, the use of too many enrollment expressions would result in minimal performance gains while the inference time increases, making it more difficult to enroll new users and increase memory usage."}, {"heading": "5.2 Text-dependent Experiments on XiaoDu", "text": "The \"on Train50k\" flag indicates that the Deep Speaker models are trained on Train50k only, while the \"finely tuned\" flag indicates that we first trained the model on Train50k, then fine-tuned the XiaoDu data set with triplet loss for Deep Speaker systems and used i-vector extraction for the DNN i-vector system. Interestingly, the DNN i-vector base system achieves the best performance when it only uses XiaoDu to train the models. There are two possible reasons for this: First, the XiaoDu data set is too small to train complex deep models like Deep Speaker. Second, the text-dependent speaker verification is larger and no obvious improvements have been made."}, {"heading": "1 13.79 / 51.72 2.23 / 90.53 2.77 / 89.50", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 10.37 / 63.21 1.39 / 95.36 1.70 / 94.64", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 8.21 / 71.04 1.29 / 96.56 1.56 / 96.47", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 7.57 / 75.02 1.13 / 96.83 1.37 / 97.07", "text": "Task limits the lexicon and phonetic variability so that the i-vector extractor, which is based on factor analysis, can better cover the variability of the speaker in the small dataset. To our surprise, we find that deep speaker models trained on Train50k only achieve slightly better performance than models trained on XiaoDu only. That is, models trained on text-based datasets can perform well in text-based tasks. We believe that the superior performance is a result of the amount of training data. Fine-tuning of the traditional DNN i-vector system does not significantly improve performance, while fine-tuned ResCNN and GRU networks exceed the DNN i-vector system by 16.76% and 18.24% relative reduction on EER and similar ACC. This shows that pre-training on large text-independent datasets can be helpful in covering data-loaded modal text scenarios, and that greater diversity of data sets can be helpful."}, {"heading": "5.3 Text-independent Experiments on MTurk", "text": "The experimental results of MTurk in Table 10 show that Deep Speaker works across all languages: the flag \"on Train50k\" means that the Deep Speaker models are trained on Train50k only, while the flag \"finetuned\" means that models are trained on Train50k first and then refined on the MTurk data set with triplet loss. Note that this is not a trivial task, as Mandarin and English sound different. We do not report finely tuned results for the DNN i vector here. As Mandarin and English have different phone sets, the ASR-DNN model is difficult to adjust. By comparing the different systems trained on MTurk only, ResCNN and GRU reduce EER by 12.11% and 9.79% and error by 15.02% and 19.38% compared to the DNN i vector system. Interestingly, models trained exclusively on the Mandarin Train50k data set \"did not perform well on finetunclassified English models, even with no fine tuning."}, {"heading": "5.4 Time Span Experiments on UIDs", "text": "Loudspeaker detection systems usually struggle with time robustness between login and test time. Votes change over time, as do their appearance. We test the robustness of our model over a wide range of periods using the Eva200 dataset. In Table 11, the first column splits the different periods, \"1 week\" means the time span for registration and verification is less than 1 week, \"1 month\" means less than 1 month, but more than 1 week, and \"3 months\" means less than 3 months, but more than 1 month. The performance of all systems decreases with the time span between login and test, but ResCNN can still achieve the best performance with the same time span."}, {"heading": "6 Conclusion", "text": "In this paper, we introduce a novel end-to-end speaker embedding scheme called Deep Speaker. The proposed system learns directly to map speaker utterances to a hypersphere in which kosine similarities directly correspond to a measurement of speaker similarity. We experiment with two different neural network architectures (ResCNN and GRU) to extract the acoustic characteristics at the frame level. For metric learning, a triple loss layer based on cosine similarities is proposed, along with a batch-global negative selection between GPUs. Softmax pre-courses are used to achieve better performance. Experiments show that the Deep Speaker algorithm significantly improves the text-independent speaker recognition system compared to the traditional DNN-based i-vector approach. In the Mandarin dataset UIDs, the EER decreases relatively by about 50%, and the error rate decreases by 60% of the same turret error set by 30%."}, {"heading": "7 Acknowledgments", "text": "We would like to thank Liang Gao and Yuanqing Lin for their support and great insights into speaker recognition. We would also like to thank Sanjeev Satheesh, Adam Coates and Andrew Ng for their useful discussions and thoughts. Our work would not have been possible without the data support of Hongyun Zeng, Yue Pan, Jingwen Cao and Limei Han."}], "references": [{"title": "SVM Based Speaker Verification using a GMM Supervector Kernel and NAP Variability Compensation", "author": ["W.M. Campbell", "D.E. Sturim", "D.A. Reynolds", "A. Solomonoff"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, Toulouse", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint Factor Analysis versus Eigenchannels in Speaker Recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transaction on Audio Speech and Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Trans. ASLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Probabilistic Linear Discriminant Analysis Of Ivector Posterior Distributions", "author": ["S. Cumani", "P. Laface", "P. Torino"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "N. Shazeer"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Neural Network-Based Speaker Embeddings for End- To-End Speaker Verification", "author": ["S. David", "G. Pegah", "P. Daniel", "G.R. Daniel", "C Yishay", "K. Sanjeev"], "venue": "IEEE Spoken Language Technology Workshop (SLT),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "and Javier Gonzalez-Dominguez", "author": ["E. Variani", "X. Lei", "E. McDermott", "I. Moreno"], "venue": "Deep neural networks for small footprint text-dependent speaker verification, in IEEE International Conference on Acoustics, Speech, and Signal Processing", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Acoustic modelling from the signal domain using cnns, Interspeech 2016", "author": ["P. Ghahremani", "V. Manohar", "D. Povey", "S. Khudanpur"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "FaceNet: A Unified Embedding for Face Recognition and Clustering", "author": ["F. Schroff", "J. Philbin"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep convolutional neural networks with layer-wise context expansion and attention", "author": ["Yu", "Dong", "Xiong", "Wayne", "Droppo", "Jasha", "Stolcke", "Andreas", "Ye", "Guoli", "Li", "Jinyu", "Zweig", "Geoffrey"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. C ho", "B. Van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Learning the speech front-end with raw waveform cldnns", "author": ["Sainath", "Tara N", "Weiss", "Ron J", "Senior", "Andrew", "Wilson", "KevinW", "Vinyals", "Oriol"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["S.J.D. Prince"], "venue": "Proc. International Conference on Computer Vision (ICCV), Rio de Janeiro, Brazil", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Full-Covariance UBM and Heavy-tailed PLDA in I-vector Speaker Verification", "author": ["F. Castaldo", "M.J. Alam", "J.H. Cernocky"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Deep bottleneck features for ivector based text-independent speaker verification", "author": ["S. Ghalehjegh", "R. Rose"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Neural Networks, 16(10):14291451", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Though many algorithms have pushed the state-of-the-art over the past couple years [1][2][3][4][5][6][7], speaker recognition is still a challenging task.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "Though many algorithms have pushed the state-of-the-art over the past couple years [1][2][3][4][5][6][7], speaker recognition is still a challenging task.", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "Though many algorithms have pushed the state-of-the-art over the past couple years [1][2][3][4][5][6][7], speaker recognition is still a challenging task.", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "Though many algorithms have pushed the state-of-the-art over the past couple years [1][2][3][4][5][6][7], speaker recognition is still a challenging task.", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "Though many algorithms have pushed the state-of-the-art over the past couple years [1][2][3][4][5][6][7], speaker recognition is still a challenging task.", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "Though many algorithms have pushed the state-of-the-art over the past couple years [1][2][3][4][5][6][7], speaker recognition is still a challenging task.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "Though many algorithms have pushed the state-of-the-art over the past couple years [1][2][3][4][5][6][7], speaker recognition is still a challenging task.", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "The traditional speaker recognition approach entails using ivectors [3] and probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "The traditional speaker recognition approach entails using ivectors [3] and probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "This framework can be decomposed into three stages [4]:", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": ", mel-frequency cepstral coefficients (MFCC) [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "Recently, deep neural network (DNN) acoustic models have also been used to extract sufficient statistics [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "A PLDA model is then used to produce verification scores by comparing i-vectors from different utterances [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "An alternative DNNbased approach uses a classification layer [8], combining both Step 1 and Step 2.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "[6] and [7] introduced end-to-end neural speaker verification systems, combining all three steps.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] and [7] introduced end-to-end neural speaker verification systems, combining all three steps.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "[6] used the last frame output of a long short-term memory (LSTM) [9] model as an utterance-level speaker embedding, while [7] used a network-in-network (NIN) [10] nonlinearity followed by an utterance-level pooling layer to aggregate frame-level representations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[6] used the last frame output of a long short-term memory (LSTM) [9] model as an utterance-level speaker embedding, while [7] used a network-in-network (NIN) [10] nonlinearity followed by an utterance-level pooling layer to aggregate frame-level representations.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "[6] used the last frame output of a long short-term memory (LSTM) [9] model as an utterance-level speaker embedding, while [7] used a network-in-network (NIN) [10] nonlinearity followed by an utterance-level pooling layer to aggregate frame-level representations.", "startOffset": 123, "endOffset": 126}, {"referenceID": 9, "context": "[6] used the last frame output of a long short-term memory (LSTM) [9] model as an utterance-level speaker embedding, while [7] used a network-in-network (NIN) [10] nonlinearity followed by an utterance-level pooling layer to aggregate frame-level representations.", "startOffset": 159, "endOffset": 163}, {"referenceID": 5, "context": "Both [6] and [7] were trained using the same distance metric.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "Both [6] and [7] were trained using the same distance metric.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "In this paper, we extend the end-to-end speaker embedding systems proposed in [6] and [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "In this paper, we extend the end-to-end speaker embedding systems proposed in [6] and [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "The model is trained using triplet loss [11], which minimizes the distance between embedding pairs from the same speaker and maximizes the distance between pairs from different speakers.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "CNNs have also recently been applied to speech recognition with good results [12][13][14][15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "Since deep networks can better represent long utterances than shallow networks [15], we propose a deep residual CNN (ResCNN), inspired by residual networks (ResNets) [16].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "We also investigate stacked gated recurrent unit (GRU) [17] layers as an alternative for frame-level feature extraction, since they have proven to be effective for speech processing applications [18][19].", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "We also investigate stacked gated recurrent unit (GRU) [17] layers as an alternative for frame-level feature extraction, since they have proven to be effective for speech processing applications [18][19].", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "Like [7], we use a distance-based loss function to discriminate between same-speaker and different-speaker utterance pairs.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "However, unlike the PLDA-like loss function in [7], we train our networks so that cosine similarity in the embedding space directly corresponds to utterance similarity.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Traditionally, i-vectors have been used to model inter-speaker variability [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "i-vector-based speaker recognition models perform classification using cosine similarity between i-vectors or more advanced techniques such as PLDA [20], heavy-tailed PLDA [21], and Gauss-PLDA [5].", "startOffset": 148, "endOffset": 152}, {"referenceID": 16, "context": "i-vector-based speaker recognition models perform classification using cosine similarity between i-vectors or more advanced techniques such as PLDA [20], heavy-tailed PLDA [21], and Gauss-PLDA [5].", "startOffset": 172, "endOffset": 176}, {"referenceID": 4, "context": "i-vector-based speaker recognition models perform classification using cosine similarity between i-vectors or more advanced techniques such as PLDA [20], heavy-tailed PLDA [21], and Gauss-PLDA [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 17, "context": "One approach is to train a GMM on bottleneck features extracted from a DNN, and then extract i-vectors [22].", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "Another DNN-based approach uses an acoustic speech recognition DNN instead of a UBM-GMM to produce frame posteriors for i-vector computation [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 7, "context": "[8] trained DNNs to classify speakers with frame-level acoustic features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] trained an LSTM for textdependent speaker verification, which acheived a 2% equal error rate (EER) on the \u201cOk Google\u201d benchmark.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] also train an end-to-end text-independent speaker verification system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Like [6], the objective function separates same-speaker and different-speaker pairs, the same scoring done during verification.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Our paper uses different architectures than [6] and [7] that balance inference time with model depth and also draw from state-ofthe-art speech recognition systems.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "Our paper uses different architectures than [6] and [7] that balance inference time with model depth and also draw from state-ofthe-art speech recognition systems.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "We experiment with two different core architectures: a ResNetstyle [16] deep CNN and the Deep Speech 2 (DS2)-style architecture consisting of convolutional layers followed by GRU layers.", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "ResNet [16] was proposed to ease the training of very deep CNNs.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "Notably, when the number of channels increases, projection shortcuts are not used as in [16], because they increased the number of parameters without yielding significant improvement.", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "We also experiment with recurrent networks for frame-level feature extraction because they have worked well for speech recognition [19].", "startOffset": 131, "endOffset": 135}, {"referenceID": 18, "context": "[23] showed that a GRU is comparable to an LSTM with a properly initialized forget gate bias, and their best variants are competitive with each other.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Unlike in the pooling layer in [7], we do not use standard deviation of framelevel outputs.", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "We model the probability of embeddings xi and xj belonging to the same speaker by their cosine similarity in Equation (4), allowing us to use the triplet loss function like in FaceNet [11].", "startOffset": 184, "endOffset": 188}, {"referenceID": 10, "context": "We seek to make updates such that the cosine similarity between the anchor and the positive example is larger than the cosine similarity between the anchor and the negative example [11].", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "To avoid suboptimal local minima early-on in training, [11] proposed using semi-hard negative exemplars, as they are further away from the anchor than the positive exemplar, but still hard because the AN cosine similarity is close to the AP cosine similarity.", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "It uses a classification layer [8] to replace the length normalization and triplet loss layers in the standard Deep Speaker architecture described in Figure 1.", "startOffset": 31, "endOffset": 34}, {"referenceID": 19, "context": "Secondly, while triplet selection is faster with larger minibatches, smaller mini-batches usually yield better generalization in Stochastic Gradient Descent (SGD) [24].", "startOffset": 163, "endOffset": 167}, {"referenceID": 2, "context": "Since gender labels are not available in all our datasets, the evaluation here is not split by gender, unlike the NIST speaker recognition evaluations (SREs) in [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "The baseline DNN i-vector model is built based on [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 19, "context": "99 momentum [24], with a linear decreasing learning rate schedule from 0.", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "We present Deep Speaker, a neural speaker embedding system that maps utterances to a hypersphere where speaker similarity is measured by cosine similarity. The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN and GRU architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity. Experiments on three distinct datasets suggest that Deep Speaker outperforms a DNN-based i-vector baseline. For example, Deep Speaker reduces the verification equal error rate by 50% (relatively) and improves the identification accuracy by 60% (relatively) on a text-independent dataset. We also present results that suggest adapting from a model trained with Mandarin can improve accuracy for English speaker recognition.", "creator": "LaTeX with hyperref package"}}}