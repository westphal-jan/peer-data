{"id": "1705.02395", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow", "abstract": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training.", "histories": [["v1", "Wed, 26 Apr 2017 20:47:36 GMT  (443kb,D)", "http://arxiv.org/abs/1705.02395v1", "Preprint of paper accepted for the Proc. of the 21st International Conference on Evaluation and Assessment in Software Engineering, 2017"]], "COMMENTS": "Preprint of paper accepted for the Proc. of the 21st International Conference on Evaluation and Assessment in Software Engineering, 2017", "reviews": [], "SUBJECTS": "cs.CL cs.HC cs.LG cs.SE", "authors": ["markus borg", "iben lennerstad", "rasmus ros", "elizabeth bjarnason"], "accepted": false, "id": "1705.02395"}, "pdf": {"name": "1705.02395.pdf", "metadata": {"source": "CRF", "title": "On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow", "authors": [], "emails": ["markus.borg@ri.se", "iben.lennerstad@gmail.com", "firstname.lastname@cs.lth.se"], "sections": [{"heading": null, "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves, we are able to achieve our objectives, and we are able to achieve the objectives that we have set ourselves."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "Stack Overflow is the dominant technical Q & A platform for software developers, with 101 million monthly unique visitors (March 2017).The information available on Stack Overflow has been extensively studied in the software engineering community, primarily through text mining, but also through qualitative analysis.Fig. 1 shows an example of a stack overflow question with an answer in which we highlight snippets of text related to performance. Treude et al. examined the nature of the questions asked and the quality of the answers and found that the information is particularly useful for code reviews and conceptual questions, and for novice developers [14].Soliman et al. found that stack overflow contains information that is relevant and useful for decisions within the software architecture, and identified a list of words that can be used to automatically classify such information. [15] Topic modeling was used to identify which topics are discussed and relationships between them."}, {"heading": "III. METHOD", "text": "It is indeed the case that we are able to go in search of a solution that will enable us to find a solution."}, {"heading": "IV. RESULTS AND LESSONS LEARNED", "text": "We start with this section, where we report on the reliability of the interviewers. Results of our group notes after 8 iterations confirmed the challenge of the notes near the SVM hyperlevel. Despite annotation criteria that have evolved over the course of 8 AL iterations, the 12 comments we have received are very different. The presence of borderline cases is obvious, but we suspect that the arrangement between the first and second authors was stronger than within the entire group, and that there will continue to be improvement during the remaining iterations without identifying specific patterns. The presence of borderline cases is obvious, but we believe that the arrangement between the first and second authors is stronger than within the entire group."}, {"heading": "V. CONCLUSION AND IMPLICATIONS FOR FUTURE TEXT MINING", "text": "The primary lesson is that AL and Text Mining seem to be a difficult combination, at least for short texts such as stack overflow posts. Unlike image classification tasks 3, human stack overflow annotators must interpret incomplete information with limited context. However, we argue that awareness of this intrinsic challenge can be used by AL to supplement a traditional annotation process, i.e., AL can be used to identify the borderline cases that are worth discussing. Based on our experience, we present two recommendations for text mining software repositories. First, we refer to Karen Zack's viral tweets, such as, \"chihuahua or muffin.\""}, {"heading": "ACKNOWLEDGMENT", "text": "The work is partly supported by a research grant for the ORION project (reference number 20140218) of the Knowledge Foundation in Sweden, the Wallenberg Autonomous Systems and Software Program (WASP) and the Industrial Excellence Center EASE - Embedded Applications Software Engineering4."}], "references": [{"title": "The Unreasonable Effectiveness of Data,", "author": ["F. Pereira", "P. Norvig", "A. Halevy"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Scaling to Very Very Large Corpora for Natural Language Disambiguation,", "author": ["M. Banko", "E. Brill"], "venue": "in Proc. of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Comparing Person- and Processcentric Strategies for Obtaining Quality Data on Amazon Mechanical Turk,", "author": ["T. Mitra", "C. Hutto", "E. Gilbert"], "venue": "in Proc. of the 33rd Annual ACM Conference on Human Factors in Computing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "CARIAL: Cost-Aware Software Reliability Improvement with Active Learning,", "author": ["B. Sun", "G. Shu", "A. Podgurski", "S. Ray"], "venue": "in Proc. of the 5th International Conference on Software Testing, Verification and Validation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "An Adaptive Approach with Active Learning in Software Fault Prediction,", "author": ["H. Lu", "B. Cukic"], "venue": "Proc. of the 8th International Conference on Predictive Models in Software Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Defect Prediction between Software Versions with Active Learning and Dimensionality Reduction,", "author": ["H. Lu", "E. Kocaguneli", "B. Cukic"], "venue": "in Proc. of the 25th International Symposium on Software Reliability Engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Active Learning Literature Survey,", "author": ["B. Settles"], "venue": "University of Wisconsin- Madison, Tech. Rep. Computer Sciences Technical Report", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Combining Self Learning and Active Learning for Chinese Named Entity Recognition,", "author": ["Y. Lin", "C. Sun", "W. Xiaolong", "W. Xuan"], "venue": "Journal of Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "An Active Learning Framework for Hyperspectral Image Classification Using Hierarchical Segmentation,", "author": ["Z. Zhang", "E. Pasolli", "M. Crawford", "J.C. Tilton"], "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Annotating Handwritten Characters with Minimal Human Involvement in a Semi-supervised Learning Strategy,", "author": ["J. Richarz", "S. Vajda", "G. Fink"], "venue": "in Proc. of the International Conference on Frontiers in Handwriting Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "What are Developers Talking About? An Analysis of Topics and Trends in Stack Overflow,", "author": ["A. Barua", "S. Thomas", "A. Hassan"], "venue": "Empirical Software Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "How do Programmers Ask and Answer Questions on the Web?", "author": ["C. Treude", "O. Barzilay", "M. Storey"], "venue": "NIER track,\u201d in Proc. of the 33rd International Conference on Software Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Architectural Knowledge for Technology Decisions in Developer Communities: An Exploratory Study with StackOverflow,", "author": ["M. Soliman", "M. Galster", "A. Salama", "M. Riebisch"], "venue": "in Proc. of the 13th Working IEEE/IFIP Conference on Software Architecture,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Why, When, and What: Analyzing Stack Overflow Questions by Topic, Type, and Code,", "author": ["M. Allamanis", "C. Sutton"], "venue": "in Proc. of the 10th Working Conference on Mining Software Repositories,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Mining StackOverflow to Turn the IDE into a Self-confident Programming Prompter,", "author": ["L. Ponzanelli", "G. Bavota", "M. Di Penta", "R. Oliveto", "M. Lanza"], "venue": "in Proc. of the 11th Working Conference on Mining Software Repositories,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Mining StackOverflow to Filter out Offtopic IRC Discussion,", "author": ["S. Chowdhury", "A. Hindle"], "venue": "Proc. of the 12th Working Conference on Mining Software Repositories,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Software Intelligence: The Future of Mining Software Engineering Data,", "author": ["A. Hassan", "T. Xie"], "venue": "Proc. of the FSE/SDP Workshop on Future of Software Engineering Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Kocaganeli, \u201cThe Inductive Software Engineering Manifesto: Principles for Industrial Data Mining,", "author": ["T. Menzies", "C. Bird", "T. Zimmermann", "W. Schulte"], "venue": "in Proc. of the International Workshop on Machine Learning Technologies in Software Engineering,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,", "author": ["D. Yarowsky"], "venue": "Proc. of the 33rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "Mining Challenge 2015: Comparing and Combining Different Information Sources on the Stack Overflow Data Set,", "author": ["A. Ying"], "venue": "in Proc. of the 12th Working Conference on Mining Software Repositories,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "From Theories to Queries: Active Learning in Practice,", "author": ["B. Settles"], "venue": "Proc. of the JMRL Workshop on Active Learning and Experimental Design,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Mistakes and How to Avoid Mistakes in Using Intercoder Reliability Indices,", "author": ["G. Feng"], "venue": "Methodology: European Journal of Research Methods for the Behavioral and Social Sciences,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "The Role of Deliberate Artificial Design Elements in Software Engineering Experiments,", "author": ["J. Hannay", "M. Jorgensen"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "TuneR: A Framework for Tuning Software Engineering Tools with Hands-On Instructions in R,", "author": ["M. Borg"], "venue": "Journal of Software: Evolution and Process,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For example, applying natural language related machine learning to text at web scale [1] has enabled many of the advances in the last decade.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "beaten by simpler alternatives as more data are used for training [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "intelligence micro-tasks for micro-payments has radically changed the way many researchers work [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "AL has been used for software fault prediction, successfully reducing the need for human intervention [4], [5], [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "AL has been used for software fault prediction, successfully reducing the need for human intervention [4], [5], [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "AL has been used for software fault prediction, successfully reducing the need for human intervention [4], [5], [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": ", for creating large training sets for speech recognition and information extraction [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Several studies show that AL can successfully be combined with self-training, which is a method to extend the training set by automatic labeling of a trained classifier [8], [9], [10], but the techniques have not previously been used for text mining Stack Overflow.", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "Several studies show that AL can successfully be combined with self-training, which is a method to extend the training set by automatic labeling of a trained classifier [8], [9], [10], but the techniques have not previously been used for text mining Stack Overflow.", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Several studies show that AL can successfully be combined with self-training, which is a method to extend the training set by automatic labeling of a trained classifier [8], [9], [10], but the techniques have not previously been used for text mining Stack Overflow.", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "In this paper, we address using machine learning to extract external experiences from the software engineering community by text mining Stack Overflow, the leading technical Q&A platform for software developers [13].", "startOffset": 211, "endOffset": 215}, {"referenceID": 11, "context": "investigated the type of questions asked and the quality of the answers and found that the information is particularly useful for code reviews and conceptual questions, and for novice developers [14].", "startOffset": 195, "endOffset": 199}, {"referenceID": 12, "context": "found that Stack Overflow contains information relevant to and useful for decisions within software architectural design, and have idenitified a list of words that may be used to automatically classify such information [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 10, "context": ", that mobile app development is increasing faster than web development [13].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "[16], [17] and for filtering out off-topic posts, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16], [17] and for filtering out off-topic posts, e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": ", in chat channels [18].", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "Uncertainty sampling is a simple technique that selects examples where the classifier is least certain on which label to apply [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 16, "context": "AL enables a shift of focus from momentary data analysis to a process with a feedback loop [19], [20].", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "AL enables a shift of focus from momentary data analysis to a process with a feedback loop [19], [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "Self-training (or bootstrap learning [21]) is one such method that extends the training set with the unlabelled examples classified with the highest degree of certainty.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "This complements AL with uncertainty sampling well, since it maximizes the available confident labels [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "In the preparation step, we downloaded the dataset used for the MSR Mining Challenge in 2015 containing 43,336,603 posts [22].", "startOffset": 121, "endOffset": 125}, {"referenceID": 20, "context": ", the classifier finds the optimal hyperplane separating two categories of examples [24].", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "Active learning After the preparation, the first and second authors alternated manual annotation of the next 100 posts2 closest to the SVM hyperplane \u2013 we refer to each such annotation batch [25] as an AL iteration.", "startOffset": 191, "endOffset": 195}, {"referenceID": 22, "context": "Finally, we calculated Krippendorff\u2019s \u03b1 to assess inter-rater reliability, as recommended for difficult nominal tasks [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "However, our work is not a case of publication bias as we aim only to exhibit the existence of a phenomenon [27] \u2013 a beneficial application of self-training when text mining software repositories.", "startOffset": 108, "endOffset": 112}, {"referenceID": 24, "context": "tuning [28] would probably identify even better settings.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "As highlighted by Settles [25], while evolving annotation criteria", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "Furthermore, we expect that further improvements from selftraining would be possible, and plan to conduct systematic parameter optimization as the next step [28].", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator\u2019s work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training. Keywords-text mining, classification, active learning, selftraining, human annotation.", "creator": "LaTeX with hyperref package"}}}