{"id": "1705.03071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2017", "title": "Geometry of Optimization and Implicit Regularization in Deep Learning", "abstract": "We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.", "histories": [["v1", "Mon, 8 May 2017 20:12:08 GMT  (359kb,D)", "http://arxiv.org/abs/1705.03071v1", "This survey chapter was done as a part of Intel Collaborative Research institute for Computational Intelligence (ICRI-CI) \"Why &amp; When Deep Learning works -- looking inside Deep Learning\" compendium with the generous support of ICRI-CI. arXiv admin note: substantial text overlap witharXiv:1506.02617"]], "COMMENTS": "This survey chapter was done as a part of Intel Collaborative Research institute for Computational Intelligence (ICRI-CI) \"Why &amp; When Deep Learning works -- looking inside Deep Learning\" compendium with the generous support of ICRI-CI. arXiv admin note: substantial text overlap witharXiv:1506.02617", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["behnam neyshabur", "ryota tomioka", "ruslan salakhutdinov", "nathan srebro"], "accepted": false, "id": "1705.03071"}, "pdf": {"name": "1705.03071.pdf", "metadata": {"source": "CRF", "title": "Geometry of Optimization and Implicit Regularization in Deep Learning", "authors": ["Behnam Neyshabur", "Ryota Tomioka"], "emails": ["BNEYSHABUR@TTIC.EDU", "RYOTO@MICROSOFT.COM", "RSALAKHU@CS.CMU.EDU", "NATI@TTIC.EDU"], "sections": [{"heading": null, "text": "Keywords: Deep Learning, Implicit Regularization, Geometry of Optimization, Path Norm, PathSGD"}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Implicit Regularization", "text": "In other words, we must engage with a system in which we do not engage with a system in which we do not engage with a system but with a system in which we do not engage with a system but with a system in which we engage with a system. (...) We must engage with a system in which we engage with a system. (...) We must engage with a system in which we engage with a system. (...) We must engage with a system in which we engage with a system. (...) We must engage with a system in which we engage with a system. (...) We must engage with a system in which we engage with a system. (...) We must engage with a system. (...) We must engage with a system. (...) We must engage with a system. (...) We must engage with a system. (...) We must engage with a system. (...)"}, {"heading": "3. The Geometry of Optimization: Rescaling and Unbalanceness", "text": "In this section we will consider the behavior of the Euclidean geometry under rescaling and imbalance (1). (1) This is one of the special properties of the RELU activation function. (1) This is when we do not change the function calculated by the network. (2) This is when we define the function of the rescaling function as such so that the weights of the network w: E \u2192 R, a constant c > 0, and a node of the rescaling function can be multiplied. (2) This is what we (w) maps w: E \u2192 R, a constant c > 0, and a node of the rescaling function can be multiplied."}, {"heading": "4. Magnitude/Scale measures for deep networks", "text": "It is not as if this is some kind of regulation. (3) Another form of regulation that is proving to be very effective in RELU networks is the maximum regulation of all units of the incoming edge to the unit3 (Goodfellow et al., 2013; Srivastava et al., 2014); the maximum regulation of all units of the incoming edge to the unit3 (Goodfellow et al., 2013; Srivastava et al., 2014); the maximum regulation of all units of the incoming edge to the unit3 (Goodfellow et al., 2014); (Srivastava et al., 2014); and the maximum regulation of all units of the incoming edge to the unit3 (Goodfellow et al., 2013; Srivastava et al., 2014)."}, {"heading": "5. Path-SGD: An Approximate Path-Regularized Steepest Descent", "text": "We are not able to decide whether we (t) like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t), like we (t, like we (t), like we (t), like we (t), like we (t, like we (t), like we (t), like we (t, like we (t), like we (t), like we (t), like we (t), like we (t, like we (t), like we (t, like we (t), like we (t), like we (t, like we (t), like we (t, like we (t), like we (t)."}, {"heading": "6. Experiments on Path-SGD", "text": "We compare \"2-path SGD\" to two commonly used depth optimization methods, SGD and AdaGrad. We conduct our experiments on four common benchmark datasets: the standard MNIST datasets of handwritten digits (LeCun et al., 1998); CIFAR-10 and CIFAR-100 DatasetsData Set Dimensionality Classes Training Set Test SetCIFAR-10 3072 (32 x 32 colors) 10 50000 10000 potentially balanced errors (32 x 32 colors) 100 50000 10000MNIST 784 (28 x 28 grayscale) 10 60000 10000 10000 10000 color (32 x 32 colors) 10 73257 26032 tiny images of natural scenes (Krizhevsky and Hinton, 2009); and Street View House Numbers (SVHN) dataset with color images collected by Google Street View."}, {"heading": "7. Discussion", "text": "On this basis, we have re-examined the choice of Euclidean geometry based on the weights of RELU networks, proposed an alternative optimisation method that roughly corresponds to a different geometry, and demonstrated that the use of such an alternative geometry can be beneficial. In this work, we demonstrate the success of a concept, and we expect that Path-SGD will also be beneficial in large-scale training for very deep Constitutional Networks. Combining Path-SGD with AdaGrad, with dynamics or with other optimisation euristics could further improve the results. Although we believe that Path-SGD is a very good optimisation method and is a simple plug-in for SGD, we hope that this work will inspire others to update other geometries, other regulators and perhaps better rules."}, {"heading": "Acknowledgments", "text": "The research was partly funded by the NSF Prize IIS-1302662 and Intel ICRI-CI."}], "references": [{"title": "Neural network learning: Theoretical foundations", "author": ["Martin Anthony", "Peter L Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett.,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 1999}, {"title": "From average case complexity to improper learning", "author": ["Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz"], "venue": null, "citeRegEx": "Daniely et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Path-sgd: Path-normalized optimization in deep neural networks", "author": ["Behnam Neyshabur", "Ruslan R Salakhutdinov", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Norm-based capacity control in neural networks", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "venue": "In The 28th Conference on Learning Theory,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "In search of the real inductive bias: On the role of implicit regularization in deep learning", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "venue": "International Conference on Learning Representations (ICLR) workshop track,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": [], "venue": "In Foundations of Computer Science,", "citeRegEx": "Sherstov.,? \\Q2006\\E", "shortCiteRegEx": "Sherstov.", "year": 2006}, {"title": "Introduction to the Theory of Computation", "author": ["Michael Sipser"], "venue": "Thomson Course Technology,", "citeRegEx": "Sipser.,? \\Q2006\\E", "shortCiteRegEx": "Sipser.", "year": 2006}, {"title": "Rank, trace-norm and max-norm", "author": ["Nathan Srebro", "Adi Shraibman"], "venue": "In Learning Theory,", "citeRegEx": "Srebro and Shraibman.,? \\Q2005\\E", "shortCiteRegEx": "Srebro and Shraibman.", "year": 2005}, {"title": "On the universality of online mirror descent", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srebro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}], "referenceMentions": [{"referenceID": 8, "context": "That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 (Sherstov, 2006; Daniely et al., 2014)\u2014not if the algorithm tries to fit such a network, not even if it tries to fit a much larger network, and in fact no matter how the algorithm represents predictors.", "startOffset": 305, "endOffset": 343}, {"referenceID": 1, "context": "That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 (Sherstov, 2006; Daniely et al., 2014)\u2014not if the algorithm tries to fit such a network, not even if it tries to fit a much larger network, and in fact no matter how the algorithm represents predictors.", "startOffset": 305, "endOffset": 343}, {"referenceID": 11, "context": "There is therefore also a strong link between regularization for optimization and regularization for learning: optimization may provide implicit regularization in terms of its corresponding geometry, and for ideal optimization performance the optimization geometry should be aligned with inductive bias driving the learning (Srebro et al., 2011).", "startOffset": 324, "endOffset": 345}, {"referenceID": 0, "context": "With sigmoidal activation it is between \u03a9(S) and O(S) (Anthony and Bartlett, 1999).", "startOffset": 54, "endOffset": 82}, {"referenceID": 5, "context": "Magnitude/Scale measures for deep networks Following Neyshabur et al. (2015b), we consider the grouping of weights going into each node of the network.", "startOffset": 53, "endOffset": 78}, {"referenceID": 10, "context": "This definition of max-norm is a bit different than the one used in the context of matrix factorization (Srebro and Shraibman, 2005).", "startOffset": 104, "endOffset": 132}, {"referenceID": 5, "context": "1 (Neyshabur et al. (2015b)) \u03c6p(w) = min w\u0303\u223cw ( \u03bcp,\u221e(w\u0303) )d", "startOffset": 3, "endOffset": 28}, {"referenceID": 3, "context": "We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits (LeCun et al., 1998); CIFAR-10 and CIFAR-100 datasets", "startOffset": 111, "endOffset": 131}, {"referenceID": 2, "context": "of tiny images of natural scenes (Krizhevsky and Hinton, 2009); and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View (Netzer et al.", "startOffset": 33, "endOffset": 62}, {"referenceID": 4, "context": "of tiny images of natural scenes (Krizhevsky and Hinton, 2009); and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View (Netzer et al., 2011).", "startOffset": 182, "endOffset": 203}, {"referenceID": 2, "context": "of tiny images of natural scenes (Krizhevsky and Hinton, 2009); and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View (Netzer et al., 2011). Details of the datasets are shown in Table 1. In all of our experiments, we trained feed-forward networks with two hidden layers, each containing 4000 hidden units. We used mini-batches of size 100 and the step-size of 10\u2212\u03b1, where \u03b1 is an integer between 0 and 10. To choose \u03b1, for each dataset, we considered the validation errors over the validation set (10000 randomly chosen points that are kept out during the initial training) and picked the one that reaches the minimum error faster. We then trained the network over the entire training set. All the networks were trained both with and without dropout. When training with dropout, at each update step, we retained each unit with probability 0.5. The optimization results are shown in Figure 3. For each of the four datasets, the plots for objective function (cross-entropy), the training error and the test error are shown from left to right where in each plot the values are reported on different epochs during the optimization. The dropout is used for the experiments on CIFAR-100 and SVHN. Please see Neyshabur et al. (2015a) for a more complete set of experimental results.", "startOffset": 34, "endOffset": 1291}, {"referenceID": 2, "context": "of tiny images of natural scenes (Krizhevsky and Hinton, 2009); and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View (Netzer et al., 2011). Details of the datasets are shown in Table 1. In all of our experiments, we trained feed-forward networks with two hidden layers, each containing 4000 hidden units. We used mini-batches of size 100 and the step-size of 10\u2212\u03b1, where \u03b1 is an integer between 0 and 10. To choose \u03b1, for each dataset, we considered the validation errors over the validation set (10000 randomly chosen points that are kept out during the initial training) and picked the one that reaches the minimum error faster. We then trained the network over the entire training set. All the networks were trained both with and without dropout. When training with dropout, at each update step, we retained each unit with probability 0.5. The optimization results are shown in Figure 3. For each of the four datasets, the plots for objective function (cross-entropy), the training error and the test error are shown from left to right where in each plot the values are reported on different epochs during the optimization. The dropout is used for the experiments on CIFAR-100 and SVHN. Please see Neyshabur et al. (2015a) for a more complete set of experimental results. We can see in Figure 3 that not only does Path-SGD often get to the same value of objective function, training and test error faster, but also the plots for test errors demonstrate that implicit regularization due to steepest descent with respect to path-regularizer leads to a solution that generalizes better. This provides further evidence on the role of implicit regularization in deep learning. The results suggest that Path-SGD outperforms SGD and AdaGrad in two different ways. First, it can achieve the same accuracy much faster and second, the implicit regularization by Path-SGD leads to a local minima that can generalize better even when the training error is zero. This can be better analyzed by looking at the plots for more number of epochs which we have provided in Neyshabur et al. (2015a). We should also point that Path-SGD can be easily combined with", "startOffset": 34, "endOffset": 2147}], "year": 2017, "abstractText": "We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.", "creator": "LaTeX with hyperref package"}}}