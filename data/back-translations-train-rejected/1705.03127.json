{"id": "1705.03127", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Word and Phrase Translation with word2vec", "abstract": "Word and phrase tables are key inputs to machine translations, but costly to produce. New unsupervised learning methods represent words and phrases in a high-dimensional vector space, and these monolingual embeddings have been shown to encode syntactic and semantic relationships between language elements. The information captured by these embeddings can be exploited for bilingual translation by learning a transformation matrix that allows to match relative positions across two monolingual vector spaces. This method aims to identify high-quality candidates for word and phrase translation more cost-effectively from unlabeled data. This paper expands the scope of previous attempts of bilingual translation to four languages (English, German, Spanish, and French). It shows how to process the source data, train a neural network to learn the high-dimensional embeddings for individual languages and expands the framework for testing their quality beyond the English language.. Furthermore, it shows how to learn bilingual transformation matrices and obtain candidates for word and phrase translation, and assess their quality.", "histories": [["v1", "Tue, 9 May 2017 00:09:38 GMT  (831kb)", "http://arxiv.org/abs/1705.03127v1", "11 pages"], ["v2", "Wed, 10 May 2017 06:04:24 GMT  (831kb)", "http://arxiv.org/abs/1705.03127v2", "11 pages"], ["v3", "Thu, 11 May 2017 02:18:47 GMT  (831kb)", "http://arxiv.org/abs/1705.03127v3", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["stefan jansen"], "accepted": false, "id": "1705.03127"}, "pdf": {"name": "1705.03127.pdf", "metadata": {"source": "CRF", "title": "Word and Phrase Translation with word2vec", "authors": ["Stefan Jansen"], "emails": [], "sections": [{"heading": null, "text": "Word and Phrase Translation with word2vec Stefan Jansen, Applied Artificial IntelligenceWord and phrase tables are key inputs to machine translations, but expensive to produce. New unattended learning methods represent words and phrases in a high-dimensional vector space, and these monolingual embeddings have been shown to encode syntactic and semantic relationships between language elements. The information gathered through these embeddings can be used for bilingual translations by learning a transformation matrix that allows relative positions to be mapped across two monolingual vector spaces, aimed at identifying high-quality candidates for word and phrase translations more cheaply from unlabeled data. This work expands the scope of previous attempts at bilingual translations into four languages (English, German, Spanish and French), demonstrating how to process the source data, train a neural network to expand the high-dimensional framework for the English language beyond its one-dimensional one-dimensional one-language and one-language ones."}, {"heading": "The Problem", "text": "A key term for statistical machine translation is bilingual mapping of words and phrases generated from parallel, i.e. already translated, corpora. Creating such high-quality labeled data is costly, and these cost constraints provide, given the large number of bilingual language pairs. word2vec [6] is an unsupervised learning method that generates a distributed representation [11] of words and phrases in a common high-dimensional vector space. word2vec uses a neural network to capture the relationship between language elements and the context in which they occur. Specifically, the network learns to predict neighbors within a given text window for each word in the vocabulary. As a result, the relative locations of language elements in this space reflect the co-occurrence of text materials."}, {"heading": "The word2vec Method", "text": "word2vec follows a tradition of learning continuous vectors to represent words [9] using neural networks [12]. The word2vec approach was developed with the aim of increasing the accuracy of recording multiple similarities along syntactic and semantic dimensions [5], while reducing computational complexity to enable learning vectors beyond the usual 50-100 dimensions and enabling training on more than a few hundred million words [6]."}, {"heading": "Feed-Forward Networks", "text": "The calculation costs remained high, however, because NNLM combines input, projection, hidden and output layer. N neighbors (the input context in 1-of-V encoding) for each word in the vocabulary V to predict a probability distribution across the vocabulary. Context is extended to a higher dimensionality in the projection layer and then passed through a nonlinear hidden layer. Output probability distribution can be achieved through a Softmax layer or, more efficiently, a hierarchical Softmax layer that uses a balanced binarity of the Huffman tree to reduce output complexity to log2 (V) or log2 (unigram perplexity (V) or 6)."}, {"heading": "Recurrent Neural Networks", "text": "Recurrent Neural Networks (RNN) avoid the need to specify the size N of the context and can display more diverse patterns than NNLM [13]. RNN have input, hidden and output and no projection layer, but add a recursive matrix that connects the hidden layer to itself to allow time-delayed effects or short-term memory."}, {"heading": "The Skip-Gram Model", "text": "The computational complexity or number of parameters corresponds to the matrix multiplication required for back propagation during the learning process by stochastic gradient descent. Mikolov et al's work focuses on simpler models to learn word vectors and then train NNLM using these vector representations. The result is two architectures that eliminate the hidden layer and learn word vectors by predicting a word from its context, as above, or alternatively predicting the context for each word in the vocabulary. The Continuous Bag-of-Word Model (CBOW) calculates the vectors of words in a window before and after the target word for its prediction, as above. The model is called \"bag of words\" because the word order is not matter.Unigram Perplexity: 1The Continuous Skip-Gram Model, in a window before and after the target word for its prediction, as above."}, {"heading": "Architecture Refinements", "text": "In order to improve the accuracy of the Skip-Gram model or increase the speed of the training, several architectural refinements have been proposed, namely the use of candidate samples for a more efficient formulation of the objective function, and the subsampling of frequently occurring words. (1) Searching for word representations that predict surrounding words with high accuracy requires the SkipGram model for a specific order of words w1, w2, wN, maximizes the following goal of average log exceedance of all N-target words and their respective contexts. The probability predicted for each context word can be based on the inner product of the vector representations of the input and the source candidate, standardized on the requirements of probability distribution across all N-size words."}, {"heading": "Hyper Parameter Choices", "text": "This year, it has come to the point where it will be able to retaliate, \"he said.\" We've never lost so much time, \"he said.\" We're not there yet, \"he said.\" But we're not there yet as far as we imagined. \""}, {"heading": "Learning a Projection Matrix", "text": "The result of each monolingual skip-gram model is an embedded dimensional vector containing the size of the vocabulary for the language and the corresponding embed size. Therefore, there is a vector for each word in the source language, and for each word in the target language there is a vector. We need to find a translation matrix so that the matrix for a correctly translated pair translates approximately the vector positions, so that the solution can be found by solving the optimization problem of gradient descent to minimize the above losses. The resulting translation matrix will estimate the expected position of a suitable translation for each word in the source language that provides its vector representation, and use cosine distance to identify the next candidates. In practice, the translation matrix is estimated based on known translations obtained via Google Translate API."}, {"heading": "The Data: Wikipedia", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Corpus Size & Diversity", "text": "The empirical application of word2vec word and phrase translation uses monolingual Wikipedia corporations available online in the four languages shown in Table 2. With 5.3 million articles, over 2 billion tokens and 8.2 million unique word forms, the English corpus is more than twice the size of the second largest German corpus. High-level statistics for the four language corpus highlight some differences between languages that can affect translation performance. Thus, the German corpus contains more unique word forms than the English corpus, while it contains only 35% of the number of tokens. The number of tokens per unique word form is 85 for German, 187 for French, 216 for Spanish and 254 for English. In order to exclude less meaningful materials such as detours and others, articles with less than 50 words were excluded, resulting in the reduced sample sizes of Wk \u00b2 \u00b2 Wk \u00b2 Wk \u00b2 Wk \u00b2 Wk \u00b2 Wk \u00b2 Wk \u00b2 Wk \u00b2 Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk Wk Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk \u00b2 Wk Wk \u00b2 Wk Wk \u00b2 Wk Wk Wk \u00b2 Wk Wk \u00b2 Wk \u00b2 Wk Wk Wk Wk Wk Wk Wk Wk Wk \u00b2 Wk Wk \u00b2 Wk Wk \u00b2 Wk Wk Wk Wk Wk \u00b2 Wk Wk Wk \u00b2 Wk Wk Wk \u00b2 Wk Wk."}, {"heading": "Language Ar9cles Sentences Tokens Word Forms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Parts-of-Speech & Entity Tags", "text": "To gain further insight into the differences in language structure, the corpora of br Spacy (English & German) and Stanford CoreNLP (French & Spanish) sentence parsing were analyzed, revealing significantly shorter sentences in German compared to longer sentences in Spanish and French."}, {"heading": "Phrase Modeling", "text": "I used a simple approach to identify bigrams that represent common phrases. Specifically, I used the following evaluation formula to identify pairs of words that are more likely to occur together: sets a minimum score for each unique phrase. To enable phrases that consist of more than two unique phrases, bigrams were rated repeatedly, combined after each iteration when their score exceeded a threshold.After three iterations, the threshold was gradually lowered."}, {"heading": "Hyper Parameter Tuning", "text": "Since model training with the entire corpus is fairly resource-intensive (90 minutes per epoch for the English corpus on 16 cores), we tested different model configurations and pre-processing techniques on smaller subsets of data, each containing 100 million tokens. Models used text input at different stages of pre-processing: \u2022 Raw: unprocessed text \u2022 clean: remote punctuation and symbols recognized by language parsers \u2022 ngrams: identified by phrase modeling processThe following hyperparameters were tested (baseline bold): \u2022 D: embedding dimensionality (100, 200, 300) \u2022 k: NCE candidate stitch size (10, 25, 50) \u2022 t: subsample threshold (1e-3 vs user-defined threshold, set to subsample 0.2% of the most common words) \u2022 C: context window size (3, 5, 8) negative value for the school environment (j \u2212 wi)."}, {"heading": "Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Monolingual Benchmarks", "text": "Mikolov et al report the following P @ 1 accuracy for the word2vec Skip-Gram model in English:"}, {"heading": "Vector Size # Words Epochs Accuracy", "text": "The training runs at about 200,000 words / second plus a test evaluation time per epoch and lasts about 5 minutes on an AWS 2 C4.4xlarge instance with 16 cores."}, {"heading": "Monolingual word2vec", "text": "For the full Wikipedia corpus of each language, different input formats and hyperparameters were applied to improve results and test robustness, taking into account sampling results from above while adapting to the larger dataset (e.g. a lower number of negative samples is recommended for larger data).Models with raw inputs and phrases (ngram) were tested using noise-contrast estimation and negative sampling, with the latter not changing the results (as expected).Embedding ranged from 200-250, negative samples from 15-25, and the initial rate from 0.025-0.03, with linear sampling dropping to 0.001. Substantial threshold varied between 5e-4 and 1e-3. The most significant impacts resulted from the reduced word size by reducing the minimum number for a world to be included from 5 to the smaller samples to at least 25, which decreased the number of available analogs in the English vocabulary and in particular analogy."}, {"heading": "Performance across Analogies", "text": "In most other areas, there are more alternative matches or synonyms, and performance is often strong at the P @ 5 level, but not necessarily at the P @ 1 level. Currency-country relations are poorly understood for all languages, which may be because the input text contains less information about them (the original tests were conducted on Google news portals with arguably more economic content)."}, {"heading": "Translation", "text": "Mikolv et al. Report introduced word2vec for bilingual translation and tested the translation from English to Spanish using the WMT11 corpus. It delivered 575 million tokens and 127 000 unique words in English, as well as 84 million tokens and 107 000 unique words. Authors reported an accuracy of 33% on P @ 1 and an accuracy of 51% on P @ 5. The accuracy of P @ 5 is probably a better measure given that there are often several valid translations, while the dictionary contains only one solution (the Google Translation API does not offer multiple options). I used the most powerful models to test the translation quality from English to one of the other languages, learning the transformation matrix from a training set of 5000 most popular words with matching translations in the process. Results of the test of 2,500 words per language were best for Spanish with P @ 1 at 47.2% and 2.5% @ 6%."}, {"heading": "P@k Spanish French German", "text": "In fact, it is the case that most of them are able to move without being able to play by the rules, and that they are able to play by the rules. (...) In fact, it is the case that they are able to play by the rules. (...) In fact, it is the case that they are able to play by the rules. (...) In fact, it is the case that they are able to play by the rules. (...) It is the case that they are able to play by the rules. (...) In fact, it is as if they are able to play by the rules. (...) It is as if they are able to play by the rules. (...)"}], "references": [{"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics ,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Nature", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal of Machine Learning Research ,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Scaling learning algorithms towards AI. In: Large-Scale Kernel Machines", "author": ["Y. Bengio", "Y. LeCun"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Hyv \u0308arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael U Gutmann", "Aapo"], "venue": "The Journal ofMachine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "word2vec [6] is an unsupervised learning method that generates a distributed representation [11] of words and phrases in a shared high-dimensional vector space.", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "word2vec stands in a tradition of learning continuous vectors to represent words [9] using neural networks [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "Recurrent Neural Networks (RNN) avoid the need to specify the size N of the context, and can represent more varied patterns then NNLM [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "One alternative is the Hierarchical Softmax [12] that reduces the number of computations to by representing the output layer as a balanced binary tree.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "An alternative to the Hierarchical Softmax function that reduces the number of computations required for inference and back-propagation is Noise Contrastive Estimation (NCE) [14].", "startOffset": 174, "endOffset": 178}], "year": 2017, "abstractText": "Word and phrase tables are key inputs to machine translations, but costly to produce. New unsupervised learning methods represent words and phrases in a high-dimensional vector space, and these monolingual embeddings have been shown to encode syntactic and semantic relationships between language elements. The information captured by these embeddings can be exploited for bilingual translation by learning a transformation matrix that allows to match relative positions across two monolingual vector spaces. This method aims to identify high-quality candidates for word and phrase translation more cost-effectively from unlabeled data.", "creator": "Pages"}}}