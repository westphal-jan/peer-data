{"id": "1705.03151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Phonetic Temporal Neural Model for Language Identification", "abstract": "Deep neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID methods, although this information has been used in the conventional phonetic LID systems with a great success. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system but accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is a reminiscence of the old phonetic LID methods, but the phonetic knowledge here is much richer: it is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16-OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional i-vector approach on short utterances and in noisy conditions.", "histories": [["v1", "Tue, 9 May 2017 02:46:21 GMT  (1604kb,D)", "http://arxiv.org/abs/1705.03151v1", null], ["v2", "Mon, 22 May 2017 11:23:34 GMT  (1536kb,D)", "http://arxiv.org/abs/1705.03151v2", "Submitted to TASLP"], ["v3", "Fri, 25 Aug 2017 05:23:26 GMT  (1538kb,D)", "http://arxiv.org/abs/1705.03151v3", "Submitted to TASLP"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["zhiyuan tang", "dong wang", "yixiang chen", "lantian li", "andrew abel"], "accepted": false, "id": "1705.03151"}, "pdf": {"name": "1705.03151.pdf", "metadata": {"source": "CRF", "title": "Phonetic Temporal Neural Model for Language Identification", "authors": ["Zhiyuan Tang", "Dong Wang", "Yixiang Chen", "Lantian Li", "Andrew Abel"], "emails": ["dong99@mails.tsinghua.edu.cn)."], "sections": [{"heading": null, "text": "Keywords - Language Identification; Deep Neuronal Networks; Long short-term memoryI. INTRODUCTION Language Identification (LID) is suitable for a wide range of applications, such as mixing (code-switching) speech recognition. People use many cues to distinguish languages, and if more cues are used, better accuracy can be achieved. Based on various cues, different LID approaches have been developed."}, {"heading": "A. Cues for language identification", "text": "There are more than 5,000 languages in the world, and each language has its own distinct characteristics at different levels, from acoustics to semantics [1] - [3]. A few studies have been conducted to investigate how our people use these characteristics as keywords to distinguish languages [4]. For example, Muthusamy [5] found that familiarity with a language is an important factor influencing LID accuracy, and longer speech samples are easier to identify. In addition, people can easily tell which keywords they use to perform identification, including phonemic use, and the prosody familiarity with a language was conducted by several researchers by modifying the speech samples to promote one or more factors. Mori et al found that people are able to identify Japanese and English relatively reliably."}, {"heading": "B. LID approaches", "text": "These function-based methods use strong statistical models based on raw acoustic characteristics to make the LID decision. The statistical model used includes Gaussian mixture models (GMMs) [11], hidden Markov models (HMMs) [13], [14], neural networks that contain temporal information have also been shown to be effective [15], and support vector machines (SVMs). Recently, Xiv: 170 5.03 151v 1 [cs.C L ay2 0172 low-rank GMM model called \"i-vector model.\""}, {"heading": "C. Motivation of the paper", "text": "All current neural LID methods are based on acoustic characteristics, such as Mel filter banks (Fbanks) or Mel frequency receiver coefficients (MFCCs), with phonetic information largely overlooked, which may have significantly impeded the performance of neural LID. Intuitively, it is a long-standing hypothesis that languages are discriminated against by phonetic properties, either in the form of telephone sequences, telephone posteriors or phonetic bottle neck characteristics, which are represented by acoustic characteristics, and are therefore more invariant in terms of noise and channels. Pragmatically, phonetic information in the form of either telephone sequences or phonetic bottle neck characteristics has been shown to significantly improve LID accuracy, both in the conventional PRLM approach and in the modern ivectorial system."}, {"heading": "D. Paper organization", "text": "The rest of the paper is structured as follows: The model structures of the PTN approach are presented in Section II, followed by the implementation details in Section III. The experiments are reported in Section IV, and some conclusions and future work are presented in Section V."}, {"heading": "II. PHONETIC NEURAL MODELING FOR LID", "text": "Although the phonetic approach treats phonetic information as auxiliary knowledge, the PTN approach is more aggressive and uses phonetic information as the only input of the RNN-LID system. Both are shown in Fig. 1.inputASR / LID labels: LID-OutputinputDNN DNN-OutputinputDNN (a) (b) Fig. 1: LID models use phonetic information: (a) the phonetically conscious acoustic RNN; (b) the PTN model. Both models consist of a phonetic DNN to generate phonetic features. The LID model in our study is an LSTM-RNN."}, {"heading": "A. Phonetic aware acoustic neural model", "text": "The natural idea of using phonetic information in the acoustic RNN-LID system is to treat it as auxiliary knowledge, which we call a phonetically conscious approach. Intuitively, this can be seen as a method of knowledge fusion that uses both the phonetic and the acoustic characteristics to learn LID models. Figure 1 (a) illustrates the diagram of this model. A phonetic DNN model (which can be in any structure, such as FFDNN, RNN, TDNN) is used to generate phonetic characteristics at the frame level. This characteristic can be read from any location of the phonetic DNN, e.g. from the output or the last hidden layer, and in our study can be transmitted to the LID model, an LSTM-RNN. This propagated phonetic information can be accepted in different ways by the LID model."}, {"heading": "B. Phonetic temporal neural model", "text": "The second model, which we call the PTN model, replaces the acoustic characteristic with the phonetic characteristic, and thus relies entirely on the characteristics of the phonetic representation. Importantly, this learning is based on the RNN model, so the temporal patterns of the phonetic characteristics can be learned. This PTN system is shown in Figure 1 (b). Although the PTN model is a specific, \"aggressive\" case of the phonetic conscious approach, the success of this model provides a deeper insight into the LID task as it rediscovers the importance of the temporal characteristics of phonetic representations."}, {"heading": "C. Understanding PTN", "text": "In fact, it is the case that most people are in a position to go into another world, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which, in which they, in which, in which they, in which they, in which they, live, in which they, in which they, live, in which, in which they, in which they, in which they, in which they, in which they, in which they, in which, live, in which they, in which they, live, in which they, in which they, in which, in"}, {"heading": "III. MODEL STRUCTURE", "text": "The phonetic DNN can be implemented in different DNN structures, although we chose two typical models in our study, one is RNN and the other is TDNN. Both models can learn long-term phonetic patterns and have shown great advantages in speech recognition [40], [41]. For the neural LID model, we chose the LSTM-RNN. Another reason for this choice is that LSTM-RNN performs well in both the pure neural LID approach [32] and the neural hybrid LID approach [37]. Another reason is that the RNN model can learn the temporal properties of speech signals, which is consistent with our motivation to model phonetic dynamics as in the conventional PRLM approach."}, {"heading": "A. LSTM-RNN LID", "text": "The LSTM-RNN model used in this study is a single-layer RNN model in which the hidden units are LSTM. The structure proposed by Sak et al. [42] is used as shown in Fig. 2.The associated calculation is given as follows: it = \u03c3 (Wixxt + Wrrt \u2212 1 + Wicct \u2212 1 + bi) ft = \u03c3 (Wfxxt + Wfrrt \u2212 1 + Wfcct \u2212 1 + bf) ct = ft ct \u2212 1 + it g (Wcxxt + Wcrrt \u2212 1 + bc) ot = \u03c3 (Woxxt + Worrt \u2212 1 + Wocct + bo) mt = ot h (ct) rt = Wpmmt yt = Wyppt + byIn the above equations, the W terms denote the weight matrices and the cells associated with the cells the recursive form."}, {"heading": "B. Phonetic aware acoustic neural LID", "text": "In the phonetic awareness model, the phonetic feature is read from the phonetic DNN and passed on to the LID RNN as additional information to support the acoustic neural LID. The phonetic feature can be read either from the output (phone posterior) or from the last hidden layer (logits) and can be transferred to various components of the RNN LID model, such as the input / forgetting / output gate and / or the non-linear activation functions. Fig.3 (a) illustrates a simple configuration where the phonetic DNN is a TDNN model and the feature is read from the last hidden layer. The phonetic feature is transferred to the non-linear function g (\u00b7). In this configuration, most of the LID RNN calculation remains the same except that the cell value should be updated as follows: ct = ft ct \u2212 1 + g (Wxxt + WW \u00b2) \u2212 dB."}, {"heading": "C. Phonetic temporal neural LID", "text": "The phonetic approach assumes that the phonetic temporal characteristics cover most of the information for speech discrimination, so the acoustic characteristic is no longer important. Therefore, it removes all acoustic characteristics and uses the phonetic characteristic as the only input of the LID RNN, as shown in Fig. 3 (b). It is interesting to compare the PTN approach with other LID approaches. First, it can be considered a new version of the conventional PRLM approach, in particular the latest PRLM implementation with RNN as LM [44].The main difference is that the PTN approach compares phonetic characteristics with other LID approaches. First, it can be considered a new version of the conventional PRLM approach, with the phonetic M implementation using RNN as LM [44].The main difference is that the PTN approach uses phonetic properties using sephonetic properties, while the phonetic M approach uses tophonetic LNN."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Databases and configurations", "text": "The experiments were conducted on two databases: the Babel database and the AP16-OLR database. Babel corpus6 was collected as part of the IARPA (Intelligence Advanced Research Projects Activity) Babel program, with the aim of developing language technologies for low-resource languages. The sampling rate is 8 kHz and the sample size is 16 bits. In this paper, we selected speaking data from seven languages from the Babel corpus to perform the study: Assamese, Bengali, Cantonese, Pashto Tagalog and Turkish. For each language, a training dataset and a development dataset were officially provided, the training datasets contain both speech and scripted speech and the development datasets."}, {"heading": "B. Babel: baseline of bilingual LID", "text": "As a first step, we build three baseline LID systems, one based on the i-vector model, and the other two based on LSTM-RNN, using the voice data of two languages of Babel: Assamese and Georgian (AG).For the i-vector baseline, the UBM comprises 2, 048 Gaussian components and the dimensionality of the i-vectors is 400. The static acoustic features include 12-dimensional MFCs and the log energy. These static features are derived by their first and second order, resulting in 39-dimensional feature vectors."}, {"heading": "C. Babel: phonetic aware bilingual LID", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "D. Babel: PTN for bilingual LID", "text": "In the phonetic experiments mentioned above, the phonetic characteristic is used as additional information. In this section, we evaluate the PTN architecture, where the phonetic characteristic completely replaces the acoustic characteristics (Fbanks).The experiment is performed with two phonetic DNN models: AG-TDNN-MLT and SWB-TDNN-ASR. The results are presented in Table IV. For a better comparison, we will also test the special case of the phonetically conscious acoustic LID (Ph. Aware), where both the phonetic and the acoustic characteristics are used as LID-RNN input (Ph + Fb).This is the same as the PTN model, but includes additional phonetic temporal characteristics as phonetic characteristics. The results are presented in the second group of Table IV."}, {"heading": "E. Babel: Phonetic knowledge or deep structure?", "text": "The performance with only the phonetic function (as in PTN) is interesting. One question that can be asked is: how does this performance advantage over the RNN LID baseline get achieved? We discussed the phonetic perspective and the transfer learning perspective. These two perspectives can help to learn more abstract characteristics. If the latter reason is more important than a similar deep structure with only the LID labels, it can work similarly well. To answer this question, we design the following three experiments to test the contribution of phonetic information (transfer learning) and deep architecture (deep learning)."}, {"heading": "F. Babel: PTN on seven languages", "text": "We evaluate different LID models on the different languages of the Babel corpus. First, we present the i-vector baseline and the LSTM-RNN LID baseline. For the i-vector system, a linear discrimination analysis (LDA) is used to promote language-related information before SVMs are trained. The dimensionality of the LDA projection space is set to 6. For the phonetically conscious RNN and the PTN systems, two phonetic DNNs are evaluated, AG-TDNN-MLT and SWB-TDNN-ASR. For the phonetically conscious system, the g function of the LID model LSTM-RNN is chosen as the receiver. The results are shown in Tabel VI. It is evident that both the phonetically conscious and the PTN system outperform the i-vector baseline and the acoustic RNN-LID baseline fairly well."}, {"heading": "G. AP16-OLR: PTN on seven languages", "text": "In this section we will transfer the phonetic approaches to the AP16-OLR database. In comparison, the speech signals in AP16-OLR are broad (the sampling rate of 16 Hz) and the acoustic environment is less noisy. In addition, the speech data of the individual languages is very limited (10 hours per language), so we assume that the formation of a phonetic DNN model is not practicable."}, {"heading": "H. AP16-OLR: utterance duration effect", "text": "In order to demonstrate the relative advantage of the RNN system and the i-vector system in expressions of varying length, we select the expressions at least 5 seconds from the AP16OLR test set and compose 10 test sets by splitting these expressions into small expressions of varying duration, from 0.5 seconds to 5 seconds, with the step set to 0.5 seconds. Each group contains 5, 907 expressions, and each expression in a group is a random segment that is cut out from the original utterance.The performance of the i-vector system and the PTN system on the 10 test sets is shown in Fig. 4, with respect to Cavg or EER respectively. It is clear that the RNN system is more effective in short expressions, and if the duration of the expression is more than 3 seconds, the i-vector system will gain, especially with respect to EER. The continuous distribution of the test expressions of the Babel corpus and AP16-OLR database is clear in Fig. 5. It is clear that the V16 in the test system is longer than the AP16 in relation to AP16 in general."}, {"heading": "I. AP16-OLR: noise robustness", "text": "We are testing the hypothesis that the RNN system is more resistant to noise. First, white noise is added to the AP16-OLR test set with different SNR levels, and the noise-enhanced data is tested on two systems: the i-vector base system and the best PTN system in Table VII, i.e. the one with CH-TDNN ASR as phonetic DNN. The results of the two above systems with different levels of white noise are shown in Table VIII. It is evident that the PTN system 10 is more noise-resistant: with more noise corruptions, the gap between the i-vector system and the PTN system becomes less significant, and the PTN system is even better than the Ivector system in relation to Cavg when the noise level is high (SNR = 10). This can be seen more clearly in Figure 6, where the performance deterioration rates compared to the noise-free state are geared."}, {"heading": "V. CONCLUSIONS", "text": "Our experiments with the Babel database and the AP16-OLR database have shown that the PTN approach can dramatically improve performance, even better than a phonetic approach that treats the phonetic trait as auxiliary information, which has shown that phonetic time information is much more informative than raw acoustic information used to distinguish languages. This has been a longstanding belief of LID researchers in the PRLM era, but has been questioned since the popularity of the i-vector approach. In future work, we will improve the performance of the neural LID approach for long sentences by allowing the LSTM-RNN to learn long-term patterns, e.g. through multi-scale RNNs [46]."}], "references": [{"title": "Spoken language characterization", "author": ["M. Harper", "M. Maxwell"], "venue": "Spring handbook of speech processing. Sringer, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Perceptual benchmarks for automatic language identification", "author": ["Y.K. Muthusamy", "N. Jain", "R.A. Cole"], "venue": "Acoustics, Speech, and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on, vol. 1. IEEE, 1994, pp. I\u2013333.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Spoken language recognition-a step toward multilinguality in speech processing", "author": ["J. Navratil"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 6, pp. 678\u2013685, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Development of an automatic identification system of spoken languages: Phase i", "author": ["D. Cimarusti", "R. Ives"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201982., vol. 7. IEEE, 1982, pp. 1661\u20131663.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1982}, {"title": "Language identification using noisy speech", "author": ["J. Foil"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201986., vol. 11. IEEE, 1986, pp. 861\u2013864.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1986}, {"title": "Approaches to language identification using gaussian mixture models and shifted delta cepstral features.", "author": ["P.A. Torres-Carrasquillo", "E. Singer", "M.A. Kohler", "R.J. Greene", "D.A. Reynolds", "J.R. Deller Jr."], "venue": "Interspeech,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Automatic language identification using gaussian mixture and hidden markov models", "author": ["M.A. Zissman"], "venue": "Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993 IEEE International Conference on, vol. 2. IEEE, 1993, pp. 399\u2013402.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "Comparing gaussian mixture and neural network modelling approaches to automatic language identification of speech", "author": ["J. Willmore", "R. Price", "W. Roberts"], "venue": "Aust. Int. Conf. Speech Sci. & Tech, 2000, pp. 74\u201377.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic language identification using discrete hidden markov model.", "author": ["K. Wong", "M.-h. Siu"], "venue": "in INTERSPEECH. Citeseer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Speaker-independent, textindependent language identification by hmm.", "author": ["S. Nakagawa", "Y. Ueda", "T. Seino"], "venue": "in ICSLP, vol", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "Identifying language from speech: An example of high-level, statisticallybased feature extraction", "author": ["S.C. Kwasny", "B.L. Kalman", "W. Wu", "A.M. Engebretson"], "venue": "Proceedings 14th Annual Conference of the Cognitive Science Society, 1992.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "A segmental approach to automatic language identification", "author": ["Y.K. Muthusamy"], "venue": "Ph.D. dissertation, Jawaharlal Nehru Technological University, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Language recognition with support vector machines", "author": ["W.M. Campbell", "E. Singer", "P.A. Torres-Carrasquillo", "D.A. Reynolds"], "venue": "ODYSSEY04-The Speaker and Language Recognition Workshop, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Language recognition via i-vectors and dimensionality reduction", "author": ["N. Dehak", "A.-C. Pedro", "D. Reynolds", "R. Dehak"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2011, pp. 857\u2013860.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Language recognition in ivectors space", "author": ["D. Mart\u0131nez", "O. Plchot", "L. Burget", "O. Glembek", "P. Matejka"], "venue": "Proceedings of Interspeech, Firenze, Italy, pp. 861\u2013864, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparison of four approaches to automatic language identification of telephone speech", "author": ["M.A. Zissman"], "venue": "IEEE Transactions on speech and audio processing, vol. 4, no. 1, p. 31, 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Brno university of technology system for nist 2005 language recognition evaluation", "author": ["P. Matejka", "L. Burget", "P. Schwarz", "J. Cernocky"], "venue": "Speaker and Language Recognition Workshop, 2006. IEEE Odyssey 2006: The. IEEE, 2006, pp. 1\u20137.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Segment-based automatic language identification", "author": ["T.J. Hazen", "V.W. Zue"], "venue": "The Journal of the Acoustical Society of America, vol. 101, no. 4, pp. 2323\u20132331, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Different size multilingual phone inventories and context-dependent acoustic models for language identification.", "author": ["D. Zhu", "M. Adda-Decker", "F. Antoine"], "venue": "InterSpeech,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Lvcsr-based language identification", "author": ["T. Schultz", "I. Rogina", "A. Waibel"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP- 96. Conference Proceedings., 1996 IEEE International Conference on, vol. 2. IEEE, 1996, pp. 781\u2013784.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "Robust spoken language identification using large vocabulary speech recognition", "author": ["J.L. Hieronymus", "S. Kadambe"], "venue": "Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on, vol. 2. IEEE, 1997, pp. 1111\u20131114.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Modeling prosody for language identification on read and spontaneous speech", "author": ["J.-L. Rouas", "J. Farinas", "F. Pellegrino", "R. Andr\u00e9-Obrecht"], "venue": "Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, vol. 6. IEEE, 2003, pp. I\u201340.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "O. Plchot", "D. Martinez", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5337\u20135341.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "An end-to-end approach to language identification in short utterances using convolutional neural networks", "author": ["A. Lozano-Diez", "R. Zazo Candil", "J. Gonz\u00e1lez Dom\u0131\u0301nguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH. International Speech and Communication Association, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "LIDsenone extraction via deep neural networks for end-to-end language identification", "author": ["M. Jin", "Y. Song", "I. Mcloughlin", "L.-R. Dai", "Z.-F. Ye"], "venue": "Proc. of Odyssey, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification using time delay neural network d-vector on short utterances", "author": ["M. Kotov", "M. Nastasenko"], "venue": "Speech and Computer: 18th International Conference, SPECOM 2016, Budapest, Hungary, August 23-27, 2016, Proceedings, vol. 9811. Springer, 2016, p. 443.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked long-term tdnn for spoken language recognition", "author": ["D. Garcia-Romero", "A. McCree"], "venue": "Interspeech 2016, pp. 3226\u20133230, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic language identification using  12 long short-term memory recurrent neural networks.", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez- Rodriguez", "P.J. Moreno"], "venue": "in Interspeech,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "A divide-andconquer approach for language identification based on recurrent neural networks", "author": ["G. Gelly", "J.-L. Gauvain", "V. Le", "A. Messaoudi"], "venue": "Interspeech 2016, pp. 3231\u20133235, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification in short utterances using long short-term memory (LSTM) recurrent neural networks", "author": ["R. Zazo", "A. Lozano-Diez", "J. Gonzalez-Dominguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "PloS one, vol. 11, no. 1, p. e0146917, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.-R. Dai"], "venue": "Electronics Letters, vol. 49, no. 24, pp. 1569\u20131570, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Study of senone-based deep neural network approaches for spoken language recognition", "author": ["L. Ferrer", "Y. Lei", "M. McLaren", "N. Scheffer"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 24, no. 1, pp. 105\u2013116, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigation of senone-based longshort term memory rnns for spoken language recognition", "author": ["Y. Tian", "L. He", "Y. Liu", "J. Liu"], "venue": "Odyssey 2016, pp. 89\u201393, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Transfer learning for speech and language processing", "author": ["D. Wang", "T.F. Zheng"], "venue": "Proceedings of Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). IEEE, 2015, pp. 1225\u20131237.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers", "author": ["J.-T. Huang", "J. Li", "D. Yu", "L. Deng", "Y. Gong"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 7304\u20137308.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE transactions on acoustics, speech, and signal processing, vol. 37, no. 3, pp. 328\u2013339, 1989.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1989}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014, pp. 338\u2013342.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "On the use of phone-gram units in recurrent neural networks for language identification", "author": ["C. Salamea", "L.F. D\u2019Haro", "R. de C\u00f3rdoba", "R. San-Segundo"], "venue": "Odyssey 2016, pp. 117\u2013123, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "arXiv preprint arXiv:1609.01704, 2016.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "A couple of studies have been conducted to investigate how our humans use these properties as cues to distinguish languages [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "For example, Muthusamy [5] found that familiarity with a language is an important factor affecting the LID accuracy, and longer speech samples are easier to be identified.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Navratil [7] evaluated the importance of various types of knowledge, including lexical, phonotactic and prosodic, by humans asked to identify five languages including Chinese, English, French, German and Japanese.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "For instance, Cimarusti used LPC features [8], and Foil et al.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "[9] studied format features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Dynamic features that involve temporal information were also demonstrated to be effective [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "low-rank GMM model called \u2018i-vector model\u2019 was proposed and achieved significant success [18], [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "low-rank GMM model called \u2018i-vector model\u2019 was proposed and achieved significant success [18], [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Multiple variants of the PRLM were proposed, for example, the parallel phone recognition followed by LM (PPRLM) [20], [21], and phone recognition on a multilingual phone set [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "Multiple variants of the PRLM were proposed, for example, the parallel phone recognition followed by LM (PPRLM) [20], [21], and phone recognition on a multilingual phone set [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "Multiple variants of the PRLM were proposed, for example, the parallel phone recognition followed by LM (PPRLM) [20], [21], and phone recognition on a multilingual phone set [22].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": ", syllables [23] and words [24], [25].", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": ", syllables [23] and words [24], [25].", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": ", syllables [23] and words [24], [25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "[9] studied format and prosodic features and found that formant features are more discriminative.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[26] studied a rhythmic model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Muthusamy [16] used pitch variation, duration and syllable rate.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "The duration and pitch patterns were also used by Hazen [22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "[27], who proposed an approach based on a feed-forward deep neural network (FFDNN), which accepts raw acoustic features and produces frame-level LID decisions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": ", [33], [34].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": ", [33], [34].", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "For example, Song et al [35] used DNN to extract phonetic feature for the i-vector system, and Ferrer et al [36] proposed an DNN i-vector approach that uses posteriors produced by an phone-discriminative FFDNN to compute the Baum-Welch statistics.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "For example, Song et al [35] used DNN to extract phonetic feature for the i-vector system, and Ferrer et al [36] proposed an DNN i-vector approach that uses posteriors produced by an phone-discriminative FFDNN to compute the Baum-Welch statistics.", "startOffset": 108, "endOffset": 112}, {"referenceID": 32, "context": "Tian et al [37] extended this study by using an RNN to produce the posteriors.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "Intuitively, it is a long-standing hypothesis that languages are discriminated by phonetic properties, either distributional or temporal; theoretically, phonetic features represent information at a higher level than that represented by acoustic features, and so are more invariant with respect to noise and channels; pragmatically, it has been demonstrated that phonetic information, either in the form of phone sequences, phone posteriors or phonetic bottleneck features, can significantly improve LID accuracy, in both the conventional PRLM approach [12] and the more modern ivector system [35]\u2013[37].", "startOffset": 552, "endOffset": 556}, {"referenceID": 30, "context": "Intuitively, it is a long-standing hypothesis that languages are discriminated by phonetic properties, either distributional or temporal; theoretically, phonetic features represent information at a higher level than that represented by acoustic features, and so are more invariant with respect to noise and channels; pragmatically, it has been demonstrated that phonetic information, either in the form of phone sequences, phone posteriors or phonetic bottleneck features, can significantly improve LID accuracy, in both the conventional PRLM approach [12] and the more modern ivector system [35]\u2013[37].", "startOffset": 592, "endOffset": 596}, {"referenceID": 32, "context": "Intuitively, it is a long-standing hypothesis that languages are discriminated by phonetic properties, either distributional or temporal; theoretically, phonetic features represent information at a higher level than that represented by acoustic features, and so are more invariant with respect to noise and channels; pragmatically, it has been demonstrated that phonetic information, either in the form of phone sequences, phone posteriors or phonetic bottleneck features, can significantly improve LID accuracy, in both the conventional PRLM approach [12] and the more modern ivector system [35]\u2013[37].", "startOffset": 597, "endOffset": 601}, {"referenceID": 7, "context": ", PRLM [12], but has been largely overlooked since the popularity of the i-vector approach.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "Phonetic DNN feature i-vector [35] PTN (proposed)", "startOffset": 30, "endOffset": 34}, {"referenceID": 30, "context": "Until recently, Song et al [35] rediscovered the value of phonetic features in the i-vector model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 33, "context": "Transfer learning perspective: The second perspective to understand the PTN approach is transfer learning [38].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "This property was known in the token-based LID [20], however it is more important for the phonetic neural models, as training the phonetic DNN often requires a large amount of speech data which is often not available for the target languages and the operating conditions that are under test.", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "Moreover, it is also possible to train the phonetic DNN with multilingual, multi-conditional data [39], resulting in robust and reliable phonetic feature extraction.", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "Both the two models can learn long-term phonetic patterns and have shown great advantage in speech recognition [40], [41].", "startOffset": 111, "endOffset": 115}, {"referenceID": 36, "context": "Both the two models can learn long-term phonetic patterns and have shown great advantage in speech recognition [40], [41].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "One reason for this choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural LID approach [32] and the neural-probabilistic hybrid LID approach [37].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "One reason for this choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural LID approach [32] and the neural-probabilistic hybrid LID approach [37].", "startOffset": 173, "endOffset": 177}, {"referenceID": 16, "context": "Another reason is that the RNN model can learn the temporal properties of speech signals, which is in accordance with our motivation to model the phonetic dynamics as in the conventional PRLM approach [21].", "startOffset": 201, "endOffset": 205}, {"referenceID": 37, "context": "[42] is used, as shown in Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "The picture is reproduced from [42].", "startOffset": 31, "endOffset": 35}, {"referenceID": 38, "context": "The natural stochastic gradient descent (NSGD) algorithm [43] is employed to train the model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 39, "context": "Firstly, it can be regarded as a new version of the conventional PRLM approach, particularly the recent PRLM implementation using RNN as the LM [44].", "startOffset": 144, "endOffset": 148}, {"referenceID": 40, "context": "All the experiments were conducted with Kaldi [45].", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "This is reasonable and is consistent with the human experiment that subjects who are familiar with the target languages perform better on LID tasks [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 41, "context": ", by multi-scale RNNs [46].", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "Deep neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID methods, although this information has been used in the conventional phonetic LID systems with a great success. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system but accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is a reminiscence of the old phonetic LID methods, but the phonetic knowledge here is much richer: it is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional ivector approach on short utterances and in noisy conditions. Keywords\u2014Language identification; Deep neural networks; Long short-term memory", "creator": "LaTeX with hyperref package"}}}