{"id": "1705.03260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Evidence for the size principle in semantic and perceptual domains", "abstract": "Shepard's Universal Law of Generalization offered a compelling case for the first physics-like law in cognitive science that should hold for all intelligent agents in the universe. Shepard's account is based on a rational Bayesian model of generalization, providing an answer to the question of why such a law should emerge. Extending this account to explain how humans use multiple examples to make better generalizations requires an additional assumption, called the size principle: hypotheses that pick out fewer objects should make a larger contribution to generalization. The degree to which this principle warrants similarly law-like status is far from conclusive. Typically, evaluating this principle has not been straightforward, requiring additional assumptions. We present a new method for evaluating the size principle that is more direct, and apply this method to a diverse array of datasets. Our results provide support for the broad applicability of the size principle.", "histories": [["v1", "Tue, 9 May 2017 10:21:49 GMT  (2249kb,D)", "http://arxiv.org/abs/1705.03260v1", "6 pages, 4 figures, To appear in the Proceedings of the 39th Annual Conference of the Cognitive Science Society"]], "COMMENTS": "6 pages, 4 figures, To appear in the Proceedings of the 39th Annual Conference of the Cognitive Science Society", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["joshua c peterson", "thomas l griffiths"], "accepted": false, "id": "1705.03260"}, "pdf": {"name": "1705.03260.pdf", "metadata": {"source": "CRF", "title": "Evidence for the size principle in semantic and perceptual domains", "authors": ["Joshua C. Peterson"], "emails": ["(peterson.c.joshua@gmail.com)", "griffiths@berkeley.edu)"], "sections": [{"heading": "Introduction", "text": "In the secondary work of Shepard (1987), the idea of stimulus similarity was concretized through its interpretation of multiple information, showing that the generalization of probabilities follows an exponential law in relation to an internal psychological space. (1) Shepard describes this phenomenon as a phenomenon called x (what Shepard calls a \"consistent subset\"), an exponentially decreasing function of distance (d) in psychological space. (1) Shepard calls it the universal law of generalization, in which it is applied to every intelligent agent, throughout the universe. This result has been used in numerous cognitive models that evoke similarity (e.g. Nosofsky, 1986; Kruschke, 1992). It could be argued that the generalization from one stimulus to another does not adequately describe."}, {"heading": "Evaluating the Size Principle", "text": "In this section, we describe previous work to evaluate the size principle and explain the details of our approach."}, {"heading": "Previous work", "text": "It's not as if it's a purely formal matter, but it's not as if it's a purely formal matter. It's not as if it's a purely formal matter. It's as if it's a formal matter. It's as if it's a formal matter. It's as if it's a formal matter. It's not as if it's a formal matter. It's as if it's a formal matter. It's as if it's a formal matter. It's as if it's a formal matter. It's as if it's a formal matter. It's not as if it's a formal matter. It's as if it's a formal matter."}, {"heading": "Semantic Hypothesis Spaces", "text": "First, we evaluate the evidence for the size principle based on two sets of data sets in which people assess the similarity of words. Both data sets contain human similarity assessments of pairs of nouns that correspond to specific objects (e.g., \"zebra\" and \"lion\") and lists of binary attribute names that are associated with each object and can be filtered by the frequency of mention. Note these attributes as \"semantic\" because they are linguistic descriptions of general concepts that exclude information at the perception level associated with actual instances of this concept, or that is remembered when the instance is perceived."}, {"heading": "Semantic Dataset Group 1", "text": "According to Navarro and Perfors (2010), the first evaluation data set consists of similarities and feature matrices from the Leuven Natural Concept Database (De Deyne et al., 2008). It includes data for 15 categories (kitchen utensils, clothing, vegetables, occupations, fish, sports, birds, fruits, reptiles, insects, tools, vehicles, musical instruments, mammals and weapons), each with about 20-30 specimens. Binary feature matrices for each category each contain about 200-300 unique features. The feature descriptions are much more comprehensive than just visually visible features (e.g. \"has wings,\" \"eats fruit,\" \"is attracted by shiny objects\")."}, {"heading": "Semantic Dataset Group 2", "text": "The second set of data consisted of 17 similarity matrices from different sources throughout the literature, and the experimental contexts and methods were very different from those of group 1. All but one of these data sets (SIMLEX) were taken from the similarity data repository on Dr. Michael Lee's website (http: / / faculty.sites.uci.edu / mdlee / similarity-data /).SIMLEX was taken from a larger set of word similarity data (Hill, Reichart, & Korhonen, 2016).The majority of data sets (Birds, Clothing1, Clothing2, Fish, Fruit1, 2, Furniture1, Furniture2, Tools, Vegetables1, Vegetables2, Weapons1 and Weapons2) were taken from Romney, Brewer and Batchelder (1993).For data sets like these (Vegetables1, Vegetables1, Vegetables2, McVegetabree, and Weapons2)."}, {"heading": "Analysis & Results", "text": "For each set of data, we calculated the elementary multiplication of each row pair in F and used non-negative smallest squares to trace this matrix back to the corresponding empirical similarity values. We then calculated the log of all non-zero characteristics, as well as the log of character sizes (column sums of the F matrix for which there was a corresponding non-zero weight), and the resulting log weights and protocol feature sizes are normalized and plotted for each subgroup in Figures 1 and 2. Red lines indicate perfect increases in characteristics as predicted by the size principle, whereas black lines fit best with the actual data. Correlation coefficients are shown in Tables 1 and 2 along with a number of other statistics to be discussed. Average Peper and Spearman correlations in the group were significant."}, {"heading": "Perceptual Hypothesis Spaces", "text": "While evidence for the magnitude principle seems to be evident from studies of semantic hypotheses spaces, there have been no attempts to verify the functioning of the principle for concrete objects, especially in complex, real instances of these objects such as nature images. The true-to-life representations of such instances are complex and contain innumerable details that are not included in semantic descriptions of the general case, making it difficult to unambiguously describe the characteristics. At this point, we offer a method to overcome this challenge by using representations learned from deep neural networks."}, {"heading": "Perceptual Features", "text": "Recent work (Peterson et al., 2016) has shown that deep image function spaces can be used to approximate human similarity judgments for complex natural stimuli. For our analysis, we extracted image functions from an enhanced version of Alexnet with a binarized last hidden plane (Wu, Lin, & Tang, 2015), allowing comparison with both imperceptible binary features (i.e. features from the previous section) and non-binary perception functions (i.e. previous work on predicting similarity)."}, {"heading": "Stimuli & Data Collection", "text": "According to Peterson et al. (2016), we received pairs of image similarity ratings for 5 sets of 120 images (animals, fruits, furniture, vegetables, vehicles) using Amazon Mechanical Turk. Examples of images in each data set are shown in Figure 4. The sets of images represent basic level categories, with 20-40 subcategories each. Subjects rated at least 4 unique image pairs and we required at least 10 unique subjects to rate each possible pair. Each experiment yielded a 120 x 120 similarity matrix."}, {"heading": "Analysis & Results", "text": "As before, we calculated the pair multiplication of each row pair in F (120 frames x 4096 neural characteristics) and regressed this matrix to the corresponding empirical similarity values. The resulting weights and characteristic quantities are plotted for each category in Figure 3, and the corresponding correlations are in Table 3. Like the previous semantic datasets, only a small fraction of the total characteristics are affected by non-zero weights, although the average percentage was much lower (4% considered). Since the complete characteristic set is intended to characterize 1000 predominantly qualitatively different categories from which they were learned (Deng et al., 2009), while characteristics from the semantic datasets were only relevant for the objects in each group.In all five datasets, the correlation coefficients are moderate, negative and significant at the level \u03b1 = 0.001."}, {"heading": "Discussion", "text": "In fact, it is the case that most of them are in a position to go into another world, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}], "references": [{"title": "Exemplar by feature applicability matrices and other dutch normative data for semantic concepts", "author": ["S. De Deyne", "S. Verheyen", "E. Ameel", "W. Vanpaemel", "M.J. Dry", "W. Voorspoels", "G. Storms"], "venue": "Behavior research methods,", "citeRegEx": "Deyne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Deyne et al\\.", "year": 2008}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In Computer vision and pattern recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Alcove: An exemplar-based connectionist model of category learning", "author": ["J.K. Kruschke"], "venue": "Psychological review,", "citeRegEx": "Kruschke,? \\Q1992\\E", "shortCiteRegEx": "Kruschke", "year": 1992}, {"title": "Solving least squares problems", "author": ["C.L. Lawson", "R.J. Hanson"], "venue": null, "citeRegEx": "Lawson and Hanson,? \\Q1995\\E", "shortCiteRegEx": "Lawson and Hanson", "year": 1995}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "author": ["K. McRae", "G.S. Cree", "M.S. Seidenberg", "C. McNorgan"], "venue": "Behavior research methods,", "citeRegEx": "McRae et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McRae et al\\.", "year": 2005}, {"title": "Similarity, feature discovery, and the size principle", "author": ["D.J. Navarro", "A.F. Perfors"], "venue": "Acta Psychologica,", "citeRegEx": "Navarro and Perfors,? \\Q2010\\E", "shortCiteRegEx": "Navarro and Perfors", "year": 2010}, {"title": "Attention, similarity, and the identification\u2013categorization relationship", "author": ["R.M. Nosofsky"], "venue": "Journal of experimental psychology: General,", "citeRegEx": "Nosofsky,? \\Q1986\\E", "shortCiteRegEx": "Nosofsky", "year": 1986}, {"title": "Adapting deep network features to capture psychological representations", "author": ["J. Peterson", "J. Abbott", "T. Griffiths"], "venue": "In Proceedings of the 38th annual conference of the cognitive science society", "citeRegEx": "Peterson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peterson et al\\.", "year": 2016}, {"title": "Predicting clustering from semantic structure", "author": ["A.K. Romney", "D.D. Brewer", "W.H. Batchelder"], "venue": "Psychological Science,", "citeRegEx": "Romney et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Romney et al\\.", "year": 1993}, {"title": "Toward a universal law of generalization for psychological science", "author": ["R.N. Shepard"], "venue": null, "citeRegEx": "Shepard,? \\Q1987\\E", "shortCiteRegEx": "Shepard", "year": 1987}, {"title": "Additive clustering: Representation of similarities as combinations of discrete overlapping properties", "author": ["R.N. Shepard", "P. Arabie"], "venue": "Psychological Review,", "citeRegEx": "Shepard and Arabie,? \\Q1979\\E", "shortCiteRegEx": "Shepard and Arabie", "year": 1979}, {"title": "Generalization, similarity, and bayesian inference", "author": ["J.B. Tenenbaum", "T.L. Griffiths"], "venue": "Behavioral and brain sciences,", "citeRegEx": "Tenenbaum and Griffiths,? \\Q2001\\E", "shortCiteRegEx": "Tenenbaum and Griffiths", "year": 2001}, {"title": "Adjustable bounded rectifiers: Towards deep binary representations. arXiv preprint arXiv:1511.06201", "author": ["Z. Wu", "D. Lin", "X. Tang"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "In the seminal work of Shepard (1987), the notion of stimulus similarity was made concrete through its interpretation as stimulus generalization.", "startOffset": 23, "endOffset": 38}, {"referenceID": 3, "context": "This result has been used in numerous cognitive models that invoke similarity (e.g., Nosofsky, 1986; Kruschke, 1992).", "startOffset": 78, "endOffset": 116}, {"referenceID": 11, "context": "To capture this, Tenenbaum and Griffiths (2001) extended Shepard\u2019s original Bayesian derivation of the law to rationally integrate information about multiple instances.", "startOffset": 17, "endOffset": 48}, {"referenceID": 5, "context": "(7) Using the link between hypotheses and features, Navarro and Perfors (2010) made their second contribution: an alternative derivation showing that the relationship predicted by the size principle can hold even in the absence of random sampling.", "startOffset": 52, "endOffset": 79}, {"referenceID": 5, "context": "(7) Using the link between hypotheses and features, Navarro and Perfors (2010) made their second contribution: an alternative derivation showing that the relationship predicted by the size principle can hold even in the absence of random sampling. They argued that learners encode the similarity structure of the world by learning a set of features F that efficiently approximate that structure. Under this view, a \u201ccoherent\u201d feature is said to be one for which all objects that possess that feature exhibit high similarity. If a learner seeks a set of features that are high in coherence, the size principle emerges even in the absence of sampling since the variability in the distribution of similarities between objects sharing a feature is a function of |hk|. The third contribution that Navarro and Perfors (2010) made was to evaluate this prediction using data from the Leuven Natural Concept Database (LNCD; De Deyne et al.", "startOffset": 52, "endOffset": 819}, {"referenceID": 6, "context": "The method adopted by Navarro and Perfors (2010) depends on the derived relationship between the similarity of objects that share a feature and the number of those objects.", "startOffset": 22, "endOffset": 49}, {"referenceID": 5, "context": "Following Navarro and Perfors (2010), the first evaluation dataset is comprised of similarity and feature matrices from the Leuven Natural Concept Database (De Deyne et al.", "startOffset": 10, "endOffset": 37}, {"referenceID": 8, "context": "Recent work (Peterson et al., 2016) has provided evidence that deep image feature spaces can be used to approximate human similarity judgments for complex natural stimuli.", "startOffset": 12, "endOffset": 35}, {"referenceID": 8, "context": "120 images (animals, fruits, furniture, vegetables, vehicles) using Amazon Mechanical Turk, following Peterson et al. (2016). Examples of images in each dataset are given in Figure 4.", "startOffset": 102, "endOffset": 125}, {"referenceID": 1, "context": "full feature set is meant to characterize 1000 mostly qualitatively distinct categories from which they were learned (Deng et al., 2009), whereas features from the semantic datasets were relevant only to the objects in each group, this discrepancy is to be expected.", "startOffset": 117, "endOffset": 136}, {"referenceID": 8, "context": "These results may also have implications for the method of estimating human psychological representations recently proposed by Peterson et al. (2016). In this work, it was shown that human similarity judgments for natural images can be estimated by a linear transformation of deep network features, and the current results imply that this transformation is perhaps partly accounted for by the size principle.", "startOffset": 127, "endOffset": 150}], "year": 2017, "abstractText": "Shepard\u2019s Universal Law of Generalization offered a compelling case for the first physics-like law in cognitive science that should hold for all intelligent agents in the universe. Shepard\u2019s account is based on a rational Bayesian model of generalization, providing an answer to the question of why such a law should emerge. Extending this account to explain how humans use multiple examples to make better generalizations requires an additional assumption, called the size principle: hypotheses that pick out fewer objects should make a larger contribution to generalization. The degree to which this principle warrants similarly law-like status is far from conclusive. Typically, evaluating this principle has not been straightforward, requiring additional assumptions. We present a new method for evaluating the size principle that is more direct, and apply this method to a diverse array of datasets. Our results provide support for the broad applicability of the size principle.", "creator": "LaTeX with hyperref package"}}}