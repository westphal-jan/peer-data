{"id": "1705.03261", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers", "abstract": "Drug-drug interaction (DDI) is a vital information when physicians and pharmacists prepare for the combined use of two or more drugs. Thus, several DDI databases are constructed to avoid mistakenly medicine administering. In recent years, automatically extracting DDIs from biomedical text has drawn researchers' attention. However, the existing work need either complex feature engineering or NLP tools, both of which are insufficient for sentence comprehension. Inspired by the deep learning approaches in natural language processing, we propose a recurrent neural network model with multiple attention layers for DDI classification. We evaluate our model on 2013 SemEval DDIExtraction dataset. The experiments show that our model classifies most of the drug pairs into correct DDI categories, which outperforms the existing NLP or deep learning method.", "histories": [["v1", "Tue, 9 May 2017 10:22:48 GMT  (698kb,D)", "http://arxiv.org/abs/1705.03261v1", null], ["v2", "Thu, 18 May 2017 15:54:36 GMT  (3244kb,D)", "http://arxiv.org/abs/1705.03261v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zibo yi", "shasha li", "jie yu", "qingbo wu"], "accepted": false, "id": "1705.03261"}, "pdf": {"name": "1705.03261.pdf", "metadata": {"source": "CRF", "title": "Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers", "authors": ["Zibo Yi", "Shasha Li", "Jie Yu", "Qingbo Wu"], "emails": ["qingbo.wu}@nudt.edu.cn"], "sections": [{"heading": null, "text": "I believe that most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "II. RELATED WORK", "text": "Chowdhury [15] and Thomas et al. [12] proposed methods that use linguistic phenomena and two-step principles of machine learning to classify DDIs. FBK-irst [11] is a follow-up work that applies the core method to and exceeds the existing model. Neural network-based approaches have been suggested by several papers. Liu et al. [10] Name CNN for the first time for DDI extraction that exceeds traditional machine learning methods. Limited by the size of the conventional kernel, CNN can only extract features of continuous 3 to 5 words instead of distant words. Liu et al. [9] proposed dependency-based CNN for dealing with distant but relevant words. Sahu et al. [13] proposed LSTM-based methods and exceeds the CNN approach."}, {"heading": "III. PROPOSED MODEL", "text": "In this section we present our bidirectional recursive neural network with multiple attention layer model. An overview of our architecture is in Fig. 1. For a given instance describing the details of two or more drugs, the model represents each word as a vector in the embedding layer. Then, the bidirectional RNN layer generates a sentence matrix, each column vector being the semantic representation of the corresponding word. The attention layer at word level transforms the sentence matrix into vector representation. Then, the attention layer at sentence level generates a final representation for the instance by combining several relevant sentences as these sentences have the same drug pair. Followed by a Softmax classifier, the model classifies the drug pair in the given instance as a specific DDI."}, {"heading": "A. Embedding Layer", "text": "For a sentence S = (w1, w2,..., wt), which contains two drugs wu, wv, each word is embedded in a d = dWE + 2dPE dimension space (dWE, dPE are the dimension of the word embedding and position embedding). wi, pi are the uniform representation (column vector) of word and relative distance. Ew, Ep are word and position embedding matrix. After embedding, the sentence is represented by S = (x1, x2,..., xt), where exi = (Ewwi) T, (Eppi \u2212 u) T, (Eppi \u2212 v) T) T (1) Then the word sequence is fed to the RNN plane. Note that the sentence is filled with 0 if the length is less than t."}, {"heading": "B. Bidirectional RNN Encoding Layer", "text": "The words in the sequence are read one by one by RNN's gated recurrent unit (GRU). The GRU takes the current word xi and the hidden state of the previous GRU hi \u2212 1 as input. The current GRU encodes hi \u2212 1 and xi into a new hidden state hi (its dimension is ie, a hyperparameter), which can be regarded as information that the GRU remembers. Fig.2 shows the details in GRU. The reset gate ri selectively forgets information provided by the previous GRU. Then the hidden state h'i is updated. The update gate zi updates the information according to h'i and hi \u2212 1. The following equations describe these procedures. Note that ri stands for elementary multiplication (Wrxi + Urhi \u2212 1) (2) h'i = 1 (Wxi + U), 1's (Rxi + U), 1's, 1's, 1's, 1's, 1's, 1's (RZ)."}, {"heading": "C. Word Level Attention", "text": "The purpose of the attention layer at the word level is to extract the sentence representation (also known as attribute vector) from the encoded matrix. We use attention at the word level instead of the maximum pool, because the attention mechanism can determine the meaning of each encoded word in each line of H. Consider N the attention vector (column vector), a denotes the filter that gives weight to each element in the line of H. The following equations show the attention operation, which is also shown in Fig. 1.a = Softmax (\u03c9Ttanh (H) (7) h = Tanh (HaT) (8) h \u043c denotes the final attribute vector. Several approaches [13], [17] use this vector and Softmax classifier for classification. Inspired by [23] we suggest attention at the sentence level to combine the information of other sentences for improved DDI classification."}, {"heading": "D. Sentence Level Attention", "text": "The previous layers capture the characteristics only on the basis of the given sentence. However, other sentences may contain information that contributes to the understanding of this sentence. It is useful to consider other relevant instances when determining the interaction of two drugs on the basis of the given sentence. In our implementation, it is assumed that the instances that contain the same drug pair. the relevant instances are designated by S = {h * 1, h * 2,..., h * N}, with h * i being the sentence attribute vector. ei stands for how well the instance h * i corresponds to its DDI r (vector representation of a specific DDI). a is a diagonal attention matrix multiplied by which the characteristic vector h * i can concentrate on the most representative characteristics. ei = h * T i (9) \u03b1i = exp (ei) \u0445 Nk = 1 exp (ek) (10) \u03b1i is the softest result of ei."}, {"heading": "E. Classification and Training", "text": "A given set S = (w1, w2,..., wt) is finally represented by the feature vector s. Then, we give it to a Softmax classifier. Let C denote the set of all types of DDI. The output o-R | C | is the probability of each class belonging to S. o = Softmax (Ms + d) (12) We use the cross entropy cost function and the L2 regularization as optimization target. For the i-th set, Yi denotes the most uniform representation of the label where the model outputs Oi. Cross entropy is li = \u2212 lnY Ti oi. For a mini-batchM = {S1, S2,..., SM} the optimization target is: J (\u03b8) = \u2212 1 | M | p | MI = 1-Ti = 1 lnY entropy is the starting point for Oi."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets and Evaluation Metrics", "text": "We use the DDI corpus of the 2013 DDI Extraction Challenge [25] to train and test our model, and the DDIs in this corpus are divided into five categories. We give the definitions of these categories and their sample sets, as shown in Table I. This standard data set consists of training set and test set. We use the same metrics as in other drug interaction literature. [9] - [13], [26]: the overall accuracy, recall and F1 value of the test set. C means the set of \"False, Mechanism, Effect, Advise, Int.\" The precision and recall of each DDI c \"C\" is calculated by Pc = # DDI is c \"and is classified as c #. All cases classified as c\" False, Mechanism, Effect, Advise, Int \"are classified as c.\" Then the precision and recall of each DDI c \"c\" c \"is calculated as\" c. \""}, {"heading": "B. Hyperparameter Settings and Training", "text": "We use TensorFlow to implement the proposed model. Input of each word is an ordered triple (word, relative position 1, relative position 2), the sentence represented as a matrix is fed into the model. Output of the model is a | C | dimension vector representing the probabilities of being the corresponding DDI. It is the network, the parameters and hyperparameters that determine the output vector. Parameters of the network are adjusted during the training, with the hyperparameters adjusted by hand. Hyperparameters after tuning are as follows: Embedding the dimension dWE = 100, Embedding the dimension dPE = 10, Lowering the dimension of the hidden state dh = 230, the probability of failure Prd = 0.5. The word embedding is initialized by pre-trained word vectors initialized with the help of GloVe [27], while others are initialized randomly."}, {"heading": "C. Experimental Results", "text": "We store our model every 100 steps and predict all the DDIs of the instances in the test set. F1 score of these predictions is shown in Fig. 4. To show that the attention layer of the sentence layer is effective, we drop that layer and then directly use h * for the Softmax classification (see Fig. 1). The result is presented with the curve \"RNN + Dynamic Word Embedding + ATT,\" which shows that the attention layer of the sentence layer contributes to a more accurate model. We let the embedding of static or static words embedding for a DDI extraction task be better, while other conditions are all the same. The \"RNN + Static Word Embedding + 2ATT\" shows that updating the word embedding provides better performance in relation to the extraction task."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In summary, we propose a recurrent neural network with multiple layers of attention to extract DDIs from biomedical text; the sentence-level attention layer, which combines other sentences with the same drugs, has been added to our model; the experiments show that our model outperforms the most advanced DDI extraction systems; task-relevant word embedding and two layers of attention improved performance to some extent; the imbalance of classes and ambiguity of semantics cause most misclassifications; we believe that the generation of instance through generative adversarial networks would cover the lack of instances in certain categories; it is also useful to use remote studies (using other relevant materials) to supplement knowledge; and to obtain a better functioning DDI extraction system."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by NSFC under grants 61303191, 61303190, 61402504, 61103015."}], "references": [{"title": "Discovering drug-drug interactions: a text-mining and reasoning approach based on properties of drug metabolism", "author": ["L. Tari", "S. Anwar", "S. Liang", "J. Cai", "C. Baral"], "venue": "Bioinformatics, vol. 26, no. 18, pp. 547\u201353, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Incidence of adverse drug reactions in hospitalized patients: a meta-analysis of prospective studies", "author": ["J. Lazarou", "B.H. Pomeranz", "P.N. Corey"], "venue": "Jama, vol. 279, no. 15, pp. 1200\u20131205, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Sfinxa drug-drug interaction database designed for clinical decision support systems", "author": ["Y. B\u00f6ttiger", "K. Laine", "M.L. Andersson", "T. Korhonen", "B. Molin", "M.- L. Ovesj\u00f6", "T. Tirkkonen", "A. Rane", "L.L. Gustafsson", "B. Eiermann"], "venue": "European journal of clinical pharmacology, vol. 65, no. 6, pp. 627\u2013633, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Crediblemeds: Independent information on medicines", "author": ["P.R. Shankar"], "venue": "Australasian Medical Journal, vol. 7, no. 1, p. 149, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhanced theophylline clearance secondary to phenytoin therapy", "author": ["S.J. Sklar", "J.C. Wagner"], "venue": "Annals of Pharmacotherapy, vol. 19, no. 1, p. 34, 1985.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1985}, {"title": "A novel algorithm for analyzing drug-drug interactions from medline literature", "author": ["Y. Lu", "D. Shen", "M. Pietsch", "C. Nagar", "Z. Fadli", "H. Huang", "Y.C. Tu", "F. Cheng"], "venue": "Scientific Reports, vol. 5, p. 17357, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel feature-based approach to extract drugdrug interactions from biomedical text", "author": ["Q.C. Bui", "P.M.A. Sloot", "E.M.V. Mulligen", "J.A. Kors"], "venue": "Bioinformatics, vol. 30, no. 23, pp. 3365\u201371, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Dependency-based convolutional neural network for drug-drug interaction extraction", "author": ["S. Liu", "K. Chen", "Q. Chen", "B. Tang"], "venue": "IEEE International Conference on Bioinformatics and Biomedicine, 2016, pp. 1074\u20131080.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Drug-drug interaction extraction via convolutional neural networks", "author": ["S. Liu", "B. Tang", "Q. Chen", "X. Wang"], "venue": "Computational and Mathematical Methods in Medicine, vol. 2016, pp. 1\u20138, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Fbk-irst: A multi-phase kernel based approach for drug-drug interaction detection and classification that exploits linguistic information", "author": ["M.F.M. Chowdhury", "A. Lavelli"], "venue": "Atlanta, Georgia, USA, vol. 351, p. 53, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Wbi-ddi: Drugdrug interaction extraction using majority voting", "author": ["P. Thomas", "M. Neves", "T. Rocktschel", "U. Leser"], "venue": "DDI Challenge at Semeval, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Drug-drug interaction extraction from biomedical text using long short term memory network", "author": ["S.K. Sahu", "A. Anand"], "venue": "arXiv preprint arXiv:1701.08303, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Vorobkalov, Retrieval of Drug-Drug Interactions Information from Biomedical Texts: Use of TF-IDF for Classification", "author": ["P.N.M.P. Melnikov"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Exploiting the scope of negations and heterogeneous features for relation extraction: A case study for drugdrug interaction extraction.", "author": ["M.F.M. Chowdhury", "A. Lavelli"], "venue": "in HLT-NAACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Extraction and classification of drug-drug interaction from biomedical text using a two-stage classifier", "author": ["M. Rastegar-Mojarad"], "venue": "2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Attentionbased bidirectional long short-term memory networks for relation classification", "author": ["P. Zhou", "W. Shi", "J. Tian", "Z. Qi", "B. Li", "H. Hao", "B. Xu"], "venue": "Meeting of the Association for Computational Linguistics, 2016, pp. 207\u2013212.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Bidirectional long short-term memory networks for relation classification.", "author": ["S. Zhang", "D. Zheng", "X. Hu", "M. Yang"], "venue": "PACLIC,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Relation classification via convolutional deep neural network.", "author": ["D. Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao"], "venue": "in COLING,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Relation classification via multi-level attention cnns", "author": ["L. Wang", "Z. Cao", "G.D. Melo", "Z. Liu"], "venue": "Meeting of the Association for Computational Linguistics, 2016, pp. 1298\u20131307.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Relation extraction: Perspective from convolutional neural networks", "author": ["T.H. Nguyen", "R. Grishman"], "venue": "The Workshop on Vector Space Modeling for Natural Language Processing, 2015, pp. 39\u201348.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Wordnet: An electronic lexical database", "author": ["D. Lin"], "venue": "Computational Linguistics, vol. 25, no. 2, pp. 292\u2013296, 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Y. Lin", "S. Shen", "Z. Liu", "H. Luan", "M. Sun"], "venue": "Meeting of the Association for Computational Linguistics, 2016, pp. 2124\u20132133.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "The ddi corpus: an annotated corpus with pharmacological substances and drug-drug interactions", "author": ["M. Herrero-Zazo", "I. Segura-Bedmar", "P. Martnez", "T. Declerck"], "venue": "Journal of Biomedical Informatics, vol. 46, no. 5, p. 914, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Drug drug interaction extraction from biomedical literature using syntax convolutional neural network.", "author": ["Z. Zhao", "Z. Yang", "L. Luo", "H. Lin", "J. Wang"], "venue": "Bioinformatics, vol. 32,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing, 2014, pp. 1532\u20131543.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Drug-drug interaction (DDI) is a situation when one drug increases or decreases the effect of another drug [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "DDI is a common cause of illness, even a cause of death [2].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "These databases such as SFINX [3], KEGG [4], CredibleMeds [5] help physicians and pharmacists avoid most adverse drug reactions.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "These databases such as SFINX [3], KEGG [4], CredibleMeds [5] help physicians and pharmacists avoid most adverse drug reactions.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "For instance, The sentence \u201cWith combined use, clinicians should be aware, when phenytoin is added, of the potential for reexacerbation of pulmonary symptomatology due to lowered serum theophylline concentrations [6]\u201d, which is from a pharmacotherapy report, describe the side effect of phenytoin and theophylline\u2019s combined use.", "startOffset": 213, "endOffset": 216}, {"referenceID": 0, "context": "There has been many efforts to automatically extract DDIs from natural language [1], [7]\u2013[13], mainly medical literature and clinical records.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "There has been many efforts to automatically extract DDIs from natural language [1], [7]\u2013[13], mainly medical literature and clinical records.", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "There has been many efforts to automatically extract DDIs from natural language [1], [7]\u2013[13], mainly medical literature and clinical records.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "\u2022 Text analysis and statistics based approach [1], [7], [14].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "\u2022 Text analysis and statistics based approach [1], [7], [14].", "startOffset": 51, "endOffset": 54}, {"referenceID": 12, "context": "\u2022 Text analysis and statistics based approach [1], [7], [14].", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "\u2022 Feature based machine learning approach [8], [11], [12], [15], [16].", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "\u2022 Feature based machine learning approach [8], [11], [12], [15], [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "\u2022 Feature based machine learning approach [8], [11], [12], [15], [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "\u2022 Feature based machine learning approach [8], [11], [12], [15], [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "\u2022 Feature based machine learning approach [8], [11], [12], [15], [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "\u2022 Deep learning based approach [9], [10], [13].", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "\u2022 Deep learning based approach [9], [10], [13].", "startOffset": 36, "endOffset": 40}, {"referenceID": 11, "context": "\u2022 Deep learning based approach [9], [10], [13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Chowdhury [15] and Thomas et al.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "[12] proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "FBK-irst [11] is a follow-on work which applies kernel method to the existing model and outperforms it.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "[10] employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] proposed dependency-based CNN to handle distant but relevant words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[13] proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In recent years, attention mechanism and various neural networks are applied to relation extraction [17]\u2013[21].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "In recent years, attention mechanism and various neural networks are applied to relation extraction [17]\u2013[21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "Convolutional deep neural network are utilized for extracting sentence level features in [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "Then the sentence level features are concatenated with lexical level features, which are obtained by NLP toolkit WordNet [22], followed by a multilayer perceptron (MLP) to classify the entities\u2019 relation.", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In addition, the word and position embedding are trained automatically instead of keeping constant as in [19].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "[20] introduce multi-level attention mechanism to CNN in order to emphasize the keywords and ignore the non-critical words during relation detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] utilize long short-term memory network (LSTM), a typical RNN model, to represent sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Attention mechanism is added to bidirectional LSTM in [17] for relation extraction.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "Several approaches [13], [17] use this vector and softmax classifier for classification.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "Several approaches [13], [17] use this vector and softmax classifier for classification.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "Inspired by [23] we propose the sentence level attention to combine the information of other sentences for a improved DDI classification.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "We optimize the parameters of objective function J(\u03b8) with Adam [24], which is a variant of mini-batch stochastic gradient descent.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "We use the DDI corpus of the 2013 DDIExtraction challenge [25] to train and test our model.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "We use the same metrics as in other drug-drug interaction extraction literature [9]\u2013[13], [26]: the overall precision, recall, and F1 score on testing set.", "startOffset": 80, "endOffset": 83}, {"referenceID": 11, "context": "We use the same metrics as in other drug-drug interaction extraction literature [9]\u2013[13], [26]: the overall precision, recall, and F1 score on testing set.", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "We use the same metrics as in other drug-drug interaction extraction literature [9]\u2013[13], [26]: the overall precision, recall, and F1 score on testing set.", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "The word embedding is initialized by pre-trained word vectors using GloVe [27], while other parameters are initialized randomly.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "WBI [12] Two stage SVM classification 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "6090 FBK-ist [11] Hand crafted features + SVM 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "6510 SCNN [26] Two stage syntax CNN 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "[10] CNN + Pre-trained WE 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "6975 DCNN [9] Dependency-based CNN + Pretrained WE 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 11, "context": "[13] bidirectional LSTM + ATT 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] shows that updating word embedding at the time of other parameters being trained makes a better performance in relation extraction task.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Drug-drug interaction (DDI) is a vital information when physicians and pharmacists prepare for the combined use of two or more drugs. Thus, several DDI databases are constructed to avoid mistakenly medicine administering. In recent years, automatically extracting DDIs from biomedical text has drawn researchers\u2019 attention. However, the existing work need either complex feature engineering or NLP tools, both of which are insufficient for sentence comprehension. Inspired by the deep learning approaches in natural language processing, we propose a recurrent neural network model with multiple attention layers for DDI classification. We evaluate our model on 2013 SemEval DDIExtraction dataset. The experiments show that our model classifies most of the drug pairs into correct DDI categories, which outperforms the existing NLP or deep learning method.", "creator": "LaTeX with hyperref package"}}}