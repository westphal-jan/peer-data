{"id": "1705.03389", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Logical Parsing from Natural Language Based on a Neural Translation Model", "abstract": "Semantic parsing has emerged as a significant and powerful paradigm for natural language interface and question answering systems. Traditional methods of building a semantic parser rely on high-quality lexicons, hand-crafted grammars and linguistic features which are limited by applied domain or representation. In this paper, we propose a general approach to learn from denotations based on Seq2Seq model augmented with attention mechanism. We encode input sequence into vectors and use dynamic programming to infer candidate logical forms. We utilize the fact that similar utterances should have similar logical forms to help reduce the searching space. Under our learning policy, the Seq2Seq model can learn mappings gradually with noises. Curriculum learning is adopted to make the learning smoother. We test our method on the arithmetic domain which shows our model can successfully infer the correct logical forms and learn the word meanings, compositionality and operation orders simultaneously.", "histories": [["v1", "Tue, 9 May 2017 15:35:25 GMT  (250kb)", "http://arxiv.org/abs/1705.03389v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["liang li", "pengyu li", "yifan liu", "tao wan", "zengchang qin"], "accepted": false, "id": "1705.03389"}, "pdf": {"name": "1705.03389.pdf", "metadata": {"source": "CRF", "title": "Logical Parsing from Natural Language Based on a Neural Translation Model", "authors": ["Liang Li", "Pengyu Li", "Yifan Liu", "Tao Wan", "Zengchang Qin"], "emails": ["zcqin}@buaa.edu.cn"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "A. Related work", "text": "We adopt the general encoder decoder framework based on neural networks, which is augmented by the attention mechanism q [16], which allows the model to learn a soft alignment between utterances and logical forms. [17] and [13], both of which use the Seq2Seq model to map natural language to logical forms in tree structure without handmade features, but our work goes a step further by using the naming of logical forms as a learning goal and considering logical forms as latent variables. Reducing search space is a major challenge in low-level oversight. A common approach is to restrict the amount of possible logical form compositions that can significantly reduce search space, but also limit expressivity [18]."}, {"heading": "II. BACKGROUND:SEQUENCE-TO-SEQUENCE MODEL AND ATTENTION MECHANISM", "text": "Before we present our model, we briefly describe the Seq2Seq model and the attention mechanism."}, {"heading": "A. Sequence-to-sequence model", "text": "The Seq2Seq model takes a source sequence X = (x1, x2,..., xT) as input and outputs a translated sequence Y = (y1, y2,..., yT \u2032). The model maximizes the probability of generating Y conditioned on X: p (y1,..., yT \u2032 | x1, x2,..., xT). Within this framework, an encoder reads the input sequence word by word into a vector c through a recursive neural network (RNN).ht = f (xt, ht \u2212 1) (1) and c = q (h1). Within this framework, an encoder reads the input sequence word by word into a vector c through a recursive neural network (RNN)."}, {"heading": "B. Attention mechanism", "text": "The traditional Seq2Seq model predicts each word from the same context vector c, which eludes the source sequence information and renders the alignment inaccurate. To solve this problem, an attention mechanism is introduced that allows the decoder to focus on the source sequence when predicting, rather than on a compressed vector. [16] In Seq2Seq with attention mechanism, each yi in Y corresponds to a context vector ci instead of c. The conditional probability in Equation 4 is derived from the context vector ci (yi | y1,..., yi \u2212 1, x) = g (yi \u2212 1, si, ci), (6) in which the hidden state si = f (yi \u2212 1, si \u2212 1, ci) is calculated. (7) The context vector ci is a weighted average of all hidden states {ht} T = 1 of the encoder, defined as ci = T = 1yr, an input value (8)."}, {"heading": "III. LEARNING FROM DENOTATIONS", "text": "We use the threefold < u, s, d > to denote the linguistic objects where u is an enunciation, s is a logical form and d is the denomination of s. We use to denote the translation of the enunciation into its logical form, and we use [s] to denote the logical form. Each training database consists of the pair < u, d >, without explicitly saying its correct logical form. By denoting the correct logical form as the target of learning, the Seq2Seq model is trained to place a high probability on those that are consistent logical forms that execute to the correct denotation d. If the space of logical forms is large, the search for the correct logical form might be cumbersome. Furthermore, unlike the previous study, which included previous knowledge such as word embedding [20], object categories [9], our model in this mathematical expression example does not have such knowledge and the input of the logical meaning of the form."}, {"heading": "A. Dynamic programming on denotations", "text": "Our first step is to generate all logical forms that have the correct names. Formally, given the name d, we want to create a logical form set for candidates that meets the naming requirements. Previous work used the bar search to create candidates, but it is difficult to restore the complete form set due to circumcision. We note that a name can correspond to several logical forms, which leads to an increase in the number of different names, is much slower than the number of logical forms. We use dynamic programming on names to restore the complete set, following the work of [15], [21]. A necessary condition for dynamic programming to work is the denotationally invariant semantic function g, so that the denotation of the resulting logical form g (s1, s2) depends only on the names of s1 and s2. In the arithmetic domain, the result of an equation can be recursively calculated."}, {"heading": "B. Filter candidate logical forms", "text": "To filter out erroneous logical shapes, we use the fact that similar expressions should have similar logical shapes to reduce the search space. We build a base case group B (Table I) in which multiple < ub, sb > pairs of different expression lengths are stored. Specifically, we iterate an input pair < ui, di > to find the base case that is most similar to the input expression. We simply use the common characteristics as a feature similarity measurement function. After we find the base case that comes closest to the ub and ui expression, we use the corresponding base case forms s < b = logical characteristics as a feature similarity measurement function."}, {"heading": "IV. EXPERIMENTAL STUDIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset", "text": "Each utterance consists of integers from one to five and four operators \"plus,\" \"minus,\" \"times,\" \"divide.\" Each utterance represents a legal arithmetic expression. Even the scope is quite limited, the search space for an equation with a length of seven is still numerous: 54% 43 = 4 \u043c 104, which is caused by the rich composition of arithmetic equations. Furthermore, this nature leads to a denotation that can correspond with much more logical forms compared to other areas, which leads to increasing noise for conclusions. We choose two ways to represent logical forms, both represented by Arabic numerals and executable operators, with one inserted with parentheses to denote the computational order being linearized, while the other assumes to represent the computational order without parentheses."}, {"heading": "B. Settings", "text": "For the convenience of data pre-processing and vectorization, an endsign < eos > is appended to each input set to note the end of the input, and we manually set a maximum length for the input and output set, automatically filling the blank with a \"PAD\" marker. We construct 3 layers of LSTM on the encoder and decoder side, each with 20 hidden units per layer. An embedding and a Softmax layer are inserted as the first and last layer. Dropout is used to regulate the model at a constant rate of 0.3. Dropout operators are used between different LSTM layers and for the hidden layers in front of the Softmax classifier. This technique can significantly reduce overmatch, especially for small-size datasets. The dimensions of the hidden vector and word embedding are set to 20.We use the RMSactualization algorithm to zero to 0.001 and 0.001."}, {"heading": "C. Results and analysis", "text": "We report the results with the Seq2Seq model using two variants, i.e., with brackets specifying the arithmetic sequence and without parentheses as logical forms. To compare the performance of weak supervision, we also test the performance of traditional training with the logical form of the gold standard. The result is reported on the accuracy of the denotation - the proportion of input sentences is translated into the correct denotation. Table II provides the comparison. Overall, the accuracy with logical forms as supervision is much higher than with denotations, for the reason that training with gold logical forms does not produce any noise. On the other hand, the false logical forms inevitably affect the performance of our model. For example, an expression \"five plus three times four\" may be wrongly translated into [5 + (4 conventional 3), which has the correct denotation. This is reasonable because we use bagging forms to measure the similarity of words, and this method takes into account the logical form, but all of these forms are not returned by logical forms, at least, by our prediction is correct."}, {"heading": "V. CONCLUSION", "text": "In this paper, we present a neural network model for assigning natural language to its representations of meaning with a single designation as supervision; we use dynamic programming to derive logical forms from terms; and we use similarity measurements to reduce the search space; we also apply a curriculum learning strategy to smooth and accelerate the learning process; within the framework of policy training through prediction, our model has the ability to self-correct; we apply our model to the arithmetic field and experimental results show that this model can learn word meanings and compositionality without resources for domain or representation-specific characteristics; a major problem that has remained in our work is that the model can confuse the order of predictions caused by the inherent weakness of the similarity measurement of words; this model could be improved by some sequence-based similarity measurements."}], "references": [{"title": "A fully statistical approach to natural language interfaces", "author": ["S. Miller", "D. Stallard", "R. Bobrow", "R. Schwartz"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics, 2002, pp. 55\u201361.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A statistical semantic parser that integrates syntax and semantics", "author": ["R. Ge", "R.J. Mooney"], "venue": "Conference on Computational Natural Language Learning, 2005, pp. 9\u201316.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A generative model for parsing natural language to meaning representations", "author": ["W. Lu", "H.T. Ng", "W.S. Lee", "L.S. Zettlemoyer"], "venue": "Conference on Empirical Methods in Natural Language Processing, EMNLP 2008, Proceedings of the Conference, 25-27 October 2008, Honolulu, Hawaii, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL, 2008, pp. 783\u2013792.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to map sentences to logical form", "author": ["L. Zettlemoyer"], "venue": "Eprint Arxiv, pp. 658\u2013666, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M.W. Chang", "D. Roth"], "venue": "Fourteenth Conference on Computational Natural Language Learning, 2010, pp. 18\u201327.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Meeting of the Association for Computational Linguistics: Human Language Technologies, 2011, pp. 590\u2013599.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Weakly supervised training of semantic parsers", "author": ["J. Krishnamurthy", "T.M. Mitchell"], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2012, pp. 754\u2013765.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale semantic parsing without question-answer pairs", "author": ["S. Reddy", "M. Lapata", "M. Steedman"], "venue": "Transactions of the Association for Computational Linguistics, vol. 2, pp. 377\u2013392, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer"], "venue": "Transactions of the Association for Computational Linguistics, vol. 1, pp. 49\u201362, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang"], "venue": "Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, 2015, pp. 1332\u20131342.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, vol. 4, pp. 3104\u20133112, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Computer Science, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "Eprint Arxiv, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Ko\u010disk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Computer Science, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring logical forms from denotations", "author": ["P. Pasupat", "P. Liang"], "venue": "arXiv preprint arXiv:1606.06900, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Computer Science, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": "Meeting of the Association for Computational Linguistics, 2016, pp. 33\u201343.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Compositional semantic parsing on semistructured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "arXiv preprint arXiv:1508.00305, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011, pp. 529\u2013539.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["C. Liang", "J. Berant", "Q. Le", "K.D. Forbus", "N. Lao"], "venue": "2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Programming by demonstration using version space algebra", "author": ["T. Lau", "S.A. Wolfman", "P. Domingos", "D.S. Weld"], "venue": "Machine Learning, vol. 53, no. 1, pp. 111\u2013156, 2003.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Bringing machine learning and compositional semantics together", "author": ["P. Liang", "C. Potts"], "venue": "Annual Review of Linguistics, vol. 1, no. 1, pp. 355\u2013376, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "Computer Science, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a natural language interface with neural programmer", "author": ["A. Neelakantan", "Q.V. Le", "M. Abadi", "A. Mccallum", "D. Amodei"], "venue": "2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "Computer Science, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory.", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B.V. Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Computer Science, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The early works use logical forms as supervision [1]\u2013[4], given a set of input sentences and their corresponding logical forms, learning a statistical semantic parser by weighting a set of rules mapping lexical items and syntactic patterns to their logical forms.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "The early works use logical forms as supervision [1]\u2013[4], given a set of input sentences and their corresponding logical forms, learning a statistical semantic parser by weighting a set of rules mapping lexical items and syntactic patterns to their logical forms.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "It has been successfully applied in different fields including question-answering [5]\u2013[8] and robot navigation [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "It has been successfully applied in different fields including question-answering [5]\u2013[8] and robot navigation [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "It has been successfully applied in different fields including question-answering [5]\u2013[8] and robot navigation [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "[10] build semantic parsers in 7 different domains and hand engineer a separate grammar for each domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The rise of Seq2Seq model [11] provides an alternative method to tackle the mapping problem and no more manual grammars are needed.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 185, "endOffset": 189}, {"referenceID": 13, "context": "The ability to deal with sequences with changeable length as input and/or output has translated this model into applications including machine translation [11], [12], syntactic parsing [13], and question answering [14].", "startOffset": 214, "endOffset": 218}, {"referenceID": 14, "context": "The number of logical forms grows exponentially as their size increases and inferring from denotations inevitably induces spurious logical forms \u2212 those that do not represent the original sentence semantics but get the correct answer accidently [15].", "startOffset": 245, "endOffset": 249}, {"referenceID": 15, "context": "We adopt the general encoder-decoder framework based on neural networks augmented with attention mechanism [16], which allows the model to learn soft alignment between utterances and logical forms.", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "Our work is related to [17] and [13], both of which use the Seq2Seq model to map natural language to logical forms in tree structure without handengineered features.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "Our work is related to [17] and [13], both of which use the Seq2Seq model to map natural language to logical forms in tree structure without handengineered features.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "A common approach is to constrain the set of possible logical form compositions, which can significantly reduce the searching space but also constrain the expressivity [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "[19] use random walks to generate logical forms and use denotation to cut down the searching space during learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] adopt a similar method to narrow down the options and allow more complex semantics to be composed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Different from generating logical forms forwardly as mentioned above, an alternative method is to use denotation to infer logical forms using dynamic programming [15], [21].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "Different from generating logical forms forwardly as mentioned above, an alternative method is to use denotation to infer logical forms using dynamic programming [15], [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "Our work is similar to [22] in the sense that we both focus on the arithmetic domain and learn from denotations directly.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "There are some other related work, Neural Programmer [23] augmented with a small set of arithmetic and logic operations is able to perform complex reasoning and has shown success in question answering [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "There are some other related work, Neural Programmer [23] augmented with a small set of arithmetic and logic operations is able to perform complex reasoning and has shown success in question answering [24].", "startOffset": 201, "endOffset": 205}, {"referenceID": 24, "context": "Neural Turing Machines [25] can infer simple algorithms such as copying and sorting with external memory.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": ", hT ) = hT , and f is a non-linear transformation which can be either a long-short term memory unit (LSTM) [26] or a gated recurrent unit (GRU) [27].", "startOffset": 108, "endOffset": 112}, {"referenceID": 26, "context": ", hT ) = hT , and f is a non-linear transformation which can be either a long-short term memory unit (LSTM) [26] or a gated recurrent unit (GRU) [27].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "where \u25e6 is an element-wise multiplication, T is an affine transformation, \u03c3 is the logistic sigmoid that restricts its input to [0,1], it, ft and ot are the input, forget, and output gates of the LSTM, and ct is the memory cell activation vector.", "startOffset": 128, "endOffset": 133}, {"referenceID": 11, "context": "The encoder employs bidirectionality, encoding the sentences in both the forward and backward directions, an approach adopted in machine translation [12], [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "The encoder employs bidirectionality, encoding the sentences in both the forward and backward directions, an approach adopted in machine translation [12], [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "To address this problem, attention mechanism is introduced to allow decoder focusing on the source sequence instead of a compressed vector upon prediction [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 19, "context": "Additionally, different from the previous study which incorporates prior knowledge such as word embedding [20], object categories [9], our model in this mathematical expression learning example has no such knowledge and has to learn the meanings of input utterance.", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "Additionally, different from the previous study which incorporates prior knowledge such as word embedding [20], object categories [9], our model in this mathematical expression learning example has no such knowledge and has to learn the meanings of input utterance.", "startOffset": 130, "endOffset": 133}, {"referenceID": 14, "context": "We use dynamic programming on denotations to recover the full set, following the work of [15], [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "We use dynamic programming on denotations to recover the full set, following the work of [15], [21].", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Semantic parsing has emerged as a significant and powerful paradigm for natural language interface and question answering systems. Traditional methods of building a semantic parser rely on high-quality lexicons, hand-crafted grammars and linguistic features which are limited by applied domain or representation. In this paper, we propose a general approach to learn from denotations based on Seq2Seq model augmented with attention mechanism. We encode input sequence into vectors and use dynamic programming to infer candidate logical forms. We utilize the fact that similar utterances should have similar logical forms to help reduce the searching space. Under our learning policy, the Seq2Seq model can learn mappings gradually with noises. Curriculum learning is adopted to make the learning smoother. We test our method on the arithmetic domain which shows our model can successfully infer the correct logical forms and learn the word meanings, compositionality and operation orders simultaneously.", "creator": "LaTeX with hyperref package"}}}