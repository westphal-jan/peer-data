{"id": "1705.03520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Integral Policy Iterations for Reinforcement Learning Problems in Continuous Time and Space", "abstract": "Policy iteration (PI) is a recursive process of policy evaluation and improvement to solve an optimal decision-making, e.g., reinforcement learning (RL) or optimal control problem and has served as the fundamental to develop RL methods. Motivated by integral PI (IPI) schemes in optimal control and RL methods in continuous time and space (CTS), this paper proposes on-policy IPI to solve the general RL problem in CTS, with its environment modeled by an ordinary differential equation (ODE). In such continuous domain, we also propose four off-policy IPI methods---two are the ideal PI forms that use advantage and Q-functions, respectively, and the other two are natural extensions of the existing off-policy IPI schemes to our general RL framework. Compared to the IPI methods in optimal control, the proposed IPI schemes can be applied to more general situations and do not require an initial stabilizing policy to run; they are also strongly relevant to the RL algorithms in CTS such as advantage updating, Q-learning, and value-gradient based (VGB) greedy policy improvement. Our on-policy IPI is basically model-based but can be made partially model-free; each off-policy method is also either partially or completely model-free. The mathematical properties of the IPI methods---admissibility, monotone improvement, and convergence towards the optimal solution---are all rigorously proven, together with the equivalence of on- and off-policy IPI. Finally, the IPI methods are simulated with an inverted-pendulum model to support the theory and verify the performance.", "histories": [["v1", "Tue, 9 May 2017 20:01:34 GMT  (1585kb,D)", "http://arxiv.org/abs/1705.03520v1", "20 pages, 2 figures (i.e., 6 sub-figures), 2 tables, 5 main ideal algorithms, and 1 algorithm for implementation. For a summary, a short simplified RLDM-conf. version is available at &lt;this http URL&gt;"]], "COMMENTS": "20 pages, 2 figures (i.e., 6 sub-figures), 2 tables, 5 main ideal algorithms, and 1 algorithm for implementation. For a summary, a short simplified RLDM-conf. version is available at &lt;this http URL&gt;", "reviews": [], "SUBJECTS": "cs.AI cs.SY", "authors": ["jae young lee", "richard s sutton"], "accepted": false, "id": "1705.03520"}, "pdf": {"name": "1705.03520.pdf", "metadata": {"source": "CRF", "title": "Integral Policy Iterations for Reinforcement Learning Problems in Continuous Time and Space ?", "authors": ["Jae Young Lee", "Richard S. Sutton"], "emails": ["jyounglee@ualberta.ca", "rsutton@ualberta.ca"], "sections": [{"heading": null, "text": "Political Iteration (PI) is a recursive process of policy evaluation and improvement to solve optimal decision-making, e.g. reinforcement learning (RL) or an optimal control problem and served as the basis for the development of RL methods. Motivated by integral PI (IPI) programs in optimal control and RL methods in continuous time and space (CTS), this paper proposes political IPI methods to solve the general RL problem in CTS with its environment modelled by an ordinary differential equation (ODE). In such a continuous range we also propose four non-political IPI methods - two are the ideal PI forms that take advantage or Q functions, and the other two are natural extensions of the existing non-political IPI schemes to our general RL framework. Compared to the IPI methods in optimal control, the proposed IPI schemes can be applied to more general situations and no initial stabilization of policy is required."}, {"heading": "1 Introduction", "text": "Political Iteration (PI) is a recursive process to solve an optimal decision-making / control problem by switching between policy evaluation and evaluation to maintain the value function in relation to current policy (a.k.a. the current Control Law in Control Theory) and policy improvements to improve policy by optimizing the received value function (Sutton and Barto, 2017; Lewis and Vrabie, 2009). PI was first proposed by Howard (1960) in the stochastic environment as the Markov Decision Process (MDP) and is strongly relevant to enhanced learning (RL) and approximate dynamic programming (ADP). PI has served as a fundamental principle for the development of RL and ADP methods, especially when modeling the underlying environment or by approaching a discrete space. There is also model-free off and off - The authors gratefully recognize the support of Alberta Innovates technology Futures, the Alberta Machine Intelligence Institute at Google and Deepta Research Council Canada."}, {"heading": "2 Preliminaries", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 On-policy Integral Policy Iteration (IPI)", "text": "Algorithm 1a: On-Policy IPI for the General Case (1) - (2) 1 Initialize: {2 \u2192 U, the initially permissible policy; 0, the time difference; 2 i \u00b2 0; 3 repeat 4 Policy Evaluation: Given policy options, find the solution vi: X \u2192 R to the Bellman equation: for each x-solution, vi (x) = example v2 = example v2."}, {"heading": "4 Extensions to Off-policy IPI Methods", "text": "In this section, we propose a set of fully / partially model-free IPI methods that are effectively the same as the IPI guidelines, but use data generated by a behavioral policy rather than applying the target policy. To this end, we first introduce the concept of action-dependent (AD) policies. (Definition 2) For a non-empty subset U0 U of action space U0 (starting with a function of action space U0 (beginning with a function of action space U0), it is said that a policy about U0 (beginning with a function of action space U0) x (beginning with a function of action space U0), if: (1) \u00b5 (t, x, u) = u for all x and all u U0). (2) For each fixed u-policy U0 and U2) is a policy (starting with a policy that may not be stationary."}, {"heading": "5 Inverted-Pendulum Simulation Examples", "text": "To support the theory and verify the performance, we present the simulation results of the IPI methods, which are applied to the 2ndorder. (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.)."}, {"heading": "6 Conclusions", "text": "In this paper, we have proposed the IPI scheme and four non-policy IPI methods (IAPI, IQPI, IEPI and ICPI), which solve the general RL problem formulated in CTS. We have proven their mathematical properties of admissibility, monotonous improvement and convergence, along with the equivalence of on and off policy methods. It has been shown that IPI can be partially made model-free within policy by changing its policy improvement, and the off-policy methods are partially model-free (IEPI), completely model-free (IAPI, IQPI) or model-free, but only incrementally implementable in the u-AC environment (ICPI). The off-policy methods have been discussed and compared with each other, as in Table 1. Numerical simulations have been performed with the 2nd order inverse pendulum model to support the theory and verify performance, and the results with all algorithms have been roughly the same. Contrary to the IPI stabilization framework, we do not propose in the QGB stability policy section of the IPI, initially the stability policy is not QGB."}, {"heading": "A Proof of Lemma 2", "text": "In this proof, we focus first on the case N = 1 and then generalize the result. According to Theorem 2, Tv * is a permissible value function and fulfills v * Tv *, but Tv * v * da v * is the optimal value function. Consequently, Tv * = v * and v * is a fixed point of T.Claim A.1 v * is the unique fixed point of T.Evidence. To show uniqueness, let us assume that Va is another fixed point of T, and let Va \u00b2 be the next policy achieved by political improvement in relation to the fixed point v\u03c0. Then \u03c0 \u00b2 is permissible by Theorem 2, and it is obvious that \u2212 ln \u00b2 (Tv\u03c0) (x) = h (x), x \u00b7 X \u00b7 X)), x \u00b2 X, is vindicated by the VII point and \"(11) for the permissible policy."}, {"heading": "B Proof of Theorem 7", "text": "For the proof we use the following problem regarding the conversion from a time integral equation to an algebraic equation (IAPI). Furthermore, the proof is provided in Appendix C.Lemma B.1."}, {"heading": "C Proof of Lemma B.1", "text": "The proof is provided using the following claim. Claim C1 Let us be a policy starting with t and w: X \u00b7 U is a continuous function. If there is such a policy, that for all x-X, 0 = E\u00b5 [...], for all x-X-X-X-X (...), for all x-X-X-X (...), for all x-X-X-X (...), for all x-X-X-X (...), for all x-X-X-X (...), for all x-X-X-X (...), for all x-X-X-X-X (...), for all x-X-X-X-X (...), for all x-X-X-X-X (...), for all X-X (...)."}], "references": [{"title": "Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network", "author": ["M. Abu-Khalaf", "F.L. Lewis"], "venue": "HJB approach. Automatica,", "citeRegEx": "Abu.Khalaf and Lewis,? \\Q2005\\E", "shortCiteRegEx": "Abu.Khalaf and Lewis", "year": 2005}, {"title": "Nesterov\u2019s accelerated gradient and momentum as approximations to regularised update descent", "author": ["B. Aleksandar", "G. Lever", "D. Barber"], "venue": "arXiv preprint arXiv:1607.01981v2,", "citeRegEx": "Aleksandar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Aleksandar et al\\.", "year": 2016}, {"title": "Optimal control: linear quadratic methods", "author": ["B. Anderson", "J.B. Moore"], "venue": null, "citeRegEx": "Anderson and Moore,? \\Q1989\\E", "shortCiteRegEx": "Anderson and Moore", "year": 1989}, {"title": "Advantage updating", "author": ["III L.C. Baird"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Baird,? \\Q1993\\E", "shortCiteRegEx": "Baird", "year": 1993}, {"title": "Galerkin approximations of the generalized Hamilton-Jacobi-Bellman equation", "author": ["R.W. Beard", "G.N. Saridis", "J.T. Wen"], "venue": null, "citeRegEx": "Beard et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Beard et al\\.", "year": 1997}, {"title": "On the converse of banach \u201cfixed-point principle", "author": ["C. Bessaga"], "venue": "Colloquium Mathematicae,", "citeRegEx": "Bessaga,? \\Q1959\\E", "shortCiteRegEx": "Bessaga", "year": 1959}, {"title": "Reinforcement learning in continuous time and space", "author": ["K. Doya"], "venue": "Neural computation,", "citeRegEx": "Doya,? \\Q2000\\E", "shortCiteRegEx": "Doya", "year": 2000}, {"title": "Regularized policy iteration", "author": ["A.M. Farahmand", "M. Ghavamzadeh", "S. Mannor", "C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Farahmand et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2009}, {"title": "Real analysis: modern techniques and their applications", "author": ["G.B. Folland"], "venue": null, "citeRegEx": "Folland,? \\Q1999\\E", "shortCiteRegEx": "Folland", "year": 1999}, {"title": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons", "author": ["N. Fr\u00e9maux", "H. Sprekeler", "W. Gerstner"], "venue": "PLoS Comput. Biol.,", "citeRegEx": "Fr\u00e9maux et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fr\u00e9maux et al\\.", "year": 2013}, {"title": "Nonlinear dynamical systems and control: a Lyapunov-based approach", "author": ["W.M. Haddad", "V. Chellaboina"], "venue": null, "citeRegEx": "Haddad and Chellaboina,? \\Q2008\\E", "shortCiteRegEx": "Haddad and Chellaboina", "year": 2008}, {"title": "Dynamic drogramming and Markov processes", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "Howard,? \\Q1960\\E", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Handbook of metric fixed point theory", "author": ["W. Kirk", "B. Sims"], "venue": "Springer Science & Business Media,", "citeRegEx": "Kirk and Sims,? \\Q2013\\E", "shortCiteRegEx": "Kirk and Sims", "year": 2013}, {"title": "On an iterative technique for Riccati equation computations", "author": ["D. Kleinman"], "venue": "IEEE Trans. Autom. Cont.,", "citeRegEx": "Kleinman,? \\Q1968\\E", "shortCiteRegEx": "Kleinman", "year": 1968}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Construction of suboptimal control sequences", "author": ["R.J. Leake", "Liu", "R.-W"], "venue": "SIAM Journal on Control,", "citeRegEx": "Leake et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Leake et al\\.", "year": 1967}, {"title": "Integral Q-learning and explorized policy iteration for adaptive optimal control of continuous-time linear systems", "author": ["J.Y. Lee", "J.B. Park", "Y.H. Choi"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "On integral generalized policy iteration for continuous-time linear quadratic regulations", "author": ["J.Y. Lee", "J.B. Park", "Y.H. Choi"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Integral reinforcement learning for continuous-time input-affine nonlinear systems with simultaneous invariant explorations", "author": ["J.Y. Lee", "J.B. Park", "Y.H. Choi"], "venue": "IEEE Trans. Neural Networks and Learning Systems,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Reinforcement learning and adaptive dynamic programming for feedback control", "author": ["F.L. Lewis", "D. Vrabie"], "venue": "IEEE Circuits and Systems Magazine,", "citeRegEx": "Lewis and Vrabie,? \\Q2009\\E", "shortCiteRegEx": "Lewis and Vrabie", "year": 2009}, {"title": "Lusin\u2019s Theorem and Bochner integration", "author": ["P.A. Loeb", "E. Talvila"], "venue": "Scientiae Mathematicae Japonicae,", "citeRegEx": "Loeb and Talvila,? \\Q2004\\E", "shortCiteRegEx": "Loeb and Talvila", "year": 2004}, {"title": "Data-based approximate policy iteration for affine nonlinear continuous-time optimal control", "author": ["B. Luo", "Wu", "H.-N", "T. Huang", "D. Liu"], "venue": "design. Automatica,", "citeRegEx": "Luo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2014}, {"title": "Toward off-policy learning control with function approximation", "author": ["H.R. Maei", "C. Szepesv\u00e1ri", "S. Bhatnagar", "R.S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Maei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2010}, {"title": "Q-learning and pontryagin\u2019s minimum principle", "author": ["P. Mehta", "S. Meyn"], "venue": "In Proc. IEEE Int. Conf. Decision and Control, held jointly with the Chinese Control Conference (CDC/CCC),", "citeRegEx": "Mehta and Meyn,? \\Q2009\\E", "shortCiteRegEx": "Mehta and Meyn", "year": 2009}, {"title": "Optimal outputfeedback control of unknown continuous-time linear systems using off-policy reinforcement learning", "author": ["H. Modares", "F.L. Lewis", "Jiang", "Z.-P"], "venue": "IEEE Trans. Cybern.,", "citeRegEx": "Modares et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Modares et al\\.", "year": 2016}, {"title": "Adaptive dynamic programming", "author": ["J.J. Murray", "C.J. Cox", "G.G. Lendaris", "R. Saeks"], "venue": "IEEE Trans. Syst. Man Cybern. Part C-Appl. Rev.,", "citeRegEx": "Murray et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2002}, {"title": "Approximate dynamic programming: solving the curses of dimensionality", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "Powell,? \\Q2007\\E", "shortCiteRegEx": "Powell", "year": 2007}, {"title": "An approximation theory of optimal control for trainable manipulators", "author": ["G.N. Saridis", "C.S.G. Lee"], "venue": "IEEE Trans. Syst. Man Cybern.,", "citeRegEx": "Saridis and Lee,? \\Q1979\\E", "shortCiteRegEx": "Saridis and Lee", "year": 1979}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Second Edition in Progress,", "citeRegEx": "Sutton and Barto,? \\Q2017\\E", "shortCiteRegEx": "Sutton and Barto", "year": 2017}, {"title": "Elementary real analysis", "author": ["B.S. Thomson", "J.B. Bruckner", "A.M. Bruckner"], "venue": null, "citeRegEx": "Thomson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Thomson et al\\.", "year": 2001}, {"title": "Online adaptive algorithm for optimal control with integral reinforcement learning", "author": ["K.G. Vamvoudakis", "D. Vrabie", "F.L. Lewis"], "venue": "Int. J. Robust and Nonlinear Control,", "citeRegEx": "Vamvoudakis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vamvoudakis et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "the current control law in control theory) and policy improvement to improve the policy by optimizing it using the obtained value function (Sutton and Barto, 2017; Lewis and Vrabie, 2009).", "startOffset": 139, "endOffset": 187}, {"referenceID": 19, "context": "the current control law in control theory) and policy improvement to improve the policy by optimizing it using the obtained value function (Sutton and Barto, 2017; Lewis and Vrabie, 2009).", "startOffset": 139, "endOffset": 187}, {"referenceID": 11, "context": "PI was first proposed by Howard (1960) in the stochastic environment known as Markov decision process (MDP) and is strongly relevant to reinforcement learning (RL) and approximate dynamic programming (ADP).", "startOffset": 25, "endOffset": 39}, {"referenceID": 26, "context": "Another advantage is that compared to backward-in-time dynamic programming, the forward-intime computation of PI like the other ADP methods (Powell, 2007) alleviates the problem known as the curse of dimensionality.", "startOffset": 140, "endOffset": 154}, {"referenceID": 14, "context": ", Lagoudakis and Parr, 2003; Farahmand, Ghavamzadeh, Mannor, and Szepesv\u00e1ri, 2009; Maei, Szepesv\u00e1ri, Bhatnagar, and Sutton, 2010). Here, offpolicy PI is a class of PI methods whose policy evaluation is done while following a policy, termed as a behavior policy, which is possibly different from the target policy to be evaluated; if the behavior and target policies are same, it is called an on-policy method. When the MDP is finite, all the on- or off-policy PI methods converge towards the optimal solution in finite time. Another advantage is that compared to backward-in-time dynamic programming, the forward-intime computation of PI like the other ADP methods (Powell, 2007) alleviates the problem known as the curse of dimensionality. In continuing tasks, a discount factor \u03b3 is normally introduced to PI and RL to suppress the future reward and thereby have a finite return. Sutton and Barto (2017) gives a comprehensive overview of PI, ADP, and RL algorithms with their practical applications and recent success in the RL field.", "startOffset": 2, "endOffset": 906}, {"referenceID": 28, "context": ", Leake and Liu, 1967; Kleinman, 1968; Saridis and Lee, 1979; Beard, Saridis, and Wen, 1997; Abu-Khalaf and Lewis, 2005, to name a few); Murray, Cox, Lendaris, and Saeks (2002) proposed a PI algorithm along with the trajectory-based policy evaluation that does not rely on the system model and can be viewed as a deterministic Monte-Carlo policy evaluation (Sutton and Barto, 2017).", "startOffset": 357, "endOffset": 381}, {"referenceID": 19, "context": "Motivated by those two approaches above, Vrabie and Lewis (2009) recently proposed a partially model-free PI scheme called integral PI (IPI), which is more relevant to RL/ADP in that the Bellman equation associated with its policy evaluation is of a temporal difference form\u2014see (Lewis and Vrabie, 2009) for a comprehensive overview.", "startOffset": 279, "endOffset": 303}, {"referenceID": 0, "context": ", Leake and Liu, 1967; Kleinman, 1968; Saridis and Lee, 1979; Beard, Saridis, and Wen, 1997; Abu-Khalaf and Lewis, 2005, to name a few); Murray, Cox, Lendaris, and Saeks (2002) proposed a PI algorithm along with the trajectory-based policy evaluation that does not rely on the system model and can be viewed as a deterministic Monte-Carlo policy evaluation (Sutton and Barto, 2017).", "startOffset": 93, "endOffset": 177}, {"referenceID": 0, "context": ", Leake and Liu, 1967; Kleinman, 1968; Saridis and Lee, 1979; Beard, Saridis, and Wen, 1997; Abu-Khalaf and Lewis, 2005, to name a few); Murray, Cox, Lendaris, and Saeks (2002) proposed a PI algorithm along with the trajectory-based policy evaluation that does not rely on the system model and can be viewed as a deterministic Monte-Carlo policy evaluation (Sutton and Barto, 2017). Motivated by those two approaches above, Vrabie and Lewis (2009) recently proposed a partially model-free PI scheme called integral PI (IPI), which is more relevant to RL/ADP in that the Bellman equation associated with its policy evaluation is of a temporal difference form\u2014see (Lewis and Vrabie, 2009) for a comprehensive overview.", "startOffset": 93, "endOffset": 448}, {"referenceID": 10, "context": "On the other hand, the aforementioned PI methods in CTS were all designed via Lyapunov\u2019s stability theory (Haddad and Chellaboina, 2008) to guarantee that the generated policies are all asymptotically stable and thereby yield finite returns (at least on a bounded region around an equilibrium state), provided that so is the initial policy.", "startOffset": 106, "endOffset": 136}, {"referenceID": 6, "context": ", those in (Doya, 2000; Mehta and Meyn, 2009; Fr\u00e9maux, Sprekeler, and Gerstner, 2013), this stability-based approach rather restricts the class of the cost (or reward) and the dynamics.", "startOffset": 11, "endOffset": 85}, {"referenceID": 23, "context": ", those in (Doya, 2000; Mehta and Meyn, 2009; Fr\u00e9maux, Sprekeler, and Gerstner, 2013), this stability-based approach rather restricts the class of the cost (or reward) and the dynamics.", "startOffset": 11, "endOffset": 85}, {"referenceID": 9, "context": "Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al., 2013) for an extension of Doya (2000)\u2019s continuous actor-critic using spiking neural networks.", "startOffset": 224, "endOffset": 246}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE.", "startOffset": 35, "endOffset": 52}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE.", "startOffset": 35, "endOffset": 89}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE. Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al.", "startOffset": 35, "endOffset": 146}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE. Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al., 2013) for an extension of Doya (2000)\u2019s continuous actor-critic using spiking neural networks.", "startOffset": 35, "endOffset": 413}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE. Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al., 2013) for an extension of Doya (2000)\u2019s continuous actor-critic using spiking neural networks. Mehta and Meyn (2009) defined the Hamiltonian function as a Q-function and then proposed a Q-learning method in CTS based on stochastic approximation.", "startOffset": 35, "endOffset": 492}, {"referenceID": 18, "context": "(1) Motivated by the work of IPI (Vrabie and Lewis, 2009; Lee et al., 2015) in the optimal control framework, we propose the corresponding on-policy IPI scheme in the general RL framework and then prove its mathematical properties of admissibility/monotone-improvement", "startOffset": 33, "endOffset": 75}, {"referenceID": 6, "context": "(2) Extending on-policy IPI in Section 3, we propose four off-policy IPI methods in CTS\u2014two named integral advantage PI (IAPI) and integral Q-PI (IQPI) are the ideal PI forms of advantage updating (Baird III, 1993; Doya, 2000) and Q-learning in CTS, and the other two named integral explorized PI (IEPI) and integral C-PI (ICPI) are the natural extensions of the existing off-policy IPI methods (Lee et al.", "startOffset": 197, "endOffset": 226}, {"referenceID": 18, "context": "(2) Extending on-policy IPI in Section 3, we propose four off-policy IPI methods in CTS\u2014two named integral advantage PI (IAPI) and integral Q-PI (IQPI) are the ideal PI forms of advantage updating (Baird III, 1993; Doya, 2000) and Q-learning in CTS, and the other two named integral explorized PI (IEPI) and integral C-PI (ICPI) are the natural extensions of the existing off-policy IPI methods (Lee et al., 2015) to our general RL problem.", "startOffset": 395, "endOffset": 413}, {"referenceID": 5, "context": "Here, we emphasize that Doya (2000)\u2019s VGB greedy policy improvement is also developed under this u-AC setting, and ICPI provides its model-free version.", "startOffset": 24, "endOffset": 36}, {"referenceID": 29, "context": "Hence, vi(x) converges to v\u0302\u2217(x) by monotone convergence theorem (Thomson et al., 2001), the pointwise convergence vi \u2192 v\u0302\u2217.", "startOffset": 65, "endOffset": 87}, {"referenceID": 20, "context": "Next, by Lusin\u2019s theorem (Loeb and Talvila, 2004), for any \u03b5 > 0 and any compact set \u03a9 \u2282 X , there exists a compact subset E \u2286 \u03a9 such that |\u03a9\\E| < \u03b5 and the restriction v\u0302\u2217|E is continuous.", "startOffset": 25, "endOffset": 49}, {"referenceID": 29, "context": "Hence, the monotone sequence vi converges to v\u0302\u2217 uniformly on E (and on any compact subset of X if v\u0302\u2217 is continuous over X ) by Dini\u2019s theorem (Thomson et al., 2001).", "startOffset": 144, "endOffset": 166}, {"referenceID": 5, "context": "By Lemma 2 and Bessaga (1959)\u2019s converse of the Banach\u2019s fixed point principle, there exists a metric d on Va such that (Va, d) is a complete metric space and T is a contraction (and thus continuous) under d.", "startOffset": 15, "endOffset": 30}, {"referenceID": 6, "context": "Rearranging it with respect to u, we obtain the explicit closed-form expression of (26) also known as the VGB greedy policy (Doya, 2000):", "startOffset": 124, "endOffset": 136}, {"referenceID": 6, "context": "10 This includes the frameworks in (Doya, 2000; Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009) as special cases.", "startOffset": 35, "endOffset": 99}, {"referenceID": 0, "context": "10 This includes the frameworks in (Doya, 2000; Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009) as special cases.", "startOffset": 35, "endOffset": 99}, {"referenceID": 6, "context": "12 See also (Doya, 2000; Abu-Khalaf and Lewis, 2005).", "startOffset": 12, "endOffset": 52}, {"referenceID": 0, "context": "12 See also (Doya, 2000; Abu-Khalaf and Lewis, 2005).", "startOffset": 12, "endOffset": 52}, {"referenceID": 2, "context": "Hence, the application of the standard LQR theory (Anderson and Moore, 1989) shows that", "startOffset": 50, "endOffset": 76}, {"referenceID": 18, "context": "\u2019 See (AbuKhalaf and Lewis, 2005; Vrabie and Lewis, 2009; Lee et al., 2015) for the u-AC case (27)\u2013(29) with \u03b3 = 1 and R \u2264 0.", "startOffset": 6, "endOffset": 75}, {"referenceID": 6, "context": "(Baird III, 1993; Doya, 2000), which is defined as", "startOffset": 0, "endOffset": 29}, {"referenceID": 3, "context": "13 Our general Q-function q\u03c0 includes the previously proposed Qfunctions in CTS as special cases\u2014Baird III (1993)\u2019s Q-function (\u03ba1 = 1, \u03ba2 = 1/\u2206t); h\u03c0 for \u03b3 \u2208 (0, 1) (\u03ba1 = \u03ba2 = \u2212 ln \u03b3), and its generalization for \u03b3 \u2208 (0, 1] (any \u03ba1 = \u03ba2 > 0) both recognized as Q-functions by Mehta and Meyn (2009).", "startOffset": 97, "endOffset": 114}, {"referenceID": 3, "context": "13 Our general Q-function q\u03c0 includes the previously proposed Qfunctions in CTS as special cases\u2014Baird III (1993)\u2019s Q-function (\u03ba1 = 1, \u03ba2 = 1/\u2206t); h\u03c0 for \u03b3 \u2208 (0, 1) (\u03ba1 = \u03ba2 = \u2212 ln \u03b3), and its generalization for \u03b3 \u2208 (0, 1] (any \u03ba1 = \u03ba2 > 0) both recognized as Q-functions by Mehta and Meyn (2009).", "startOffset": 97, "endOffset": 298}, {"referenceID": 3, "context": "As mentioned by Baird III (1993), a bad scaling between v\u03c0 and a\u03c0 in q\u03c0 , e.", "startOffset": 16, "endOffset": 33}, {"referenceID": 16, "context": "In this paper, we name it integral explorized policy iteration (IEPI) following the perspectives of Lee et al. (2012) and present its policy evaluation and improvement loop in Algorithm 4a.", "startOffset": 100, "endOffset": 118}, {"referenceID": 6, "context": "In this case, the maximization process in the policy improvement is simplified to the update rule (30) also known as the VGB greedy policy (Doya, 2000).", "startOffset": 139, "endOffset": 151}, {"referenceID": 6, "context": "(1) As in IEPI, the complicated maximization in the policy improvement of IAPI and IQPI has been replaced by the simple update rule (53), which is a kind of modelfree VGB greedy policy (Doya, 2000).", "startOffset": 185, "endOffset": 197}, {"referenceID": 16, "context": "Instead, its derivation was based on the value function with singularly-perturbed actions (Lee et al., 2012).", "startOffset": 90, "endOffset": 108}, {"referenceID": 6, "context": "IAPI has the constraint (40) on ai and \u03c0i in the policy evaluation that reflects the equality a\u03c0i(x, \u03c0i(x)) = 0 similarly to advantage updating (Baird III, 1993; Doya, 2000).", "startOffset": 144, "endOffset": 173}, {"referenceID": 6, "context": "(1) the uniqueness of the target solution (vi, ci) = (v\u03c0i , c\u03c0i) of the Bellman equation (39) for Z\u03c4 given by (52); (2) the exploration of a smaller space X \u00d7 {uj}j=0, rather than the whole state-action space X \u00d7 U ; (3) the simple update rule (53) in policy improvement, the model-free version of the VGB greedy policy (Doya, 2000), in place of the complicated maximization over U for each x \u2208 X such as (41) and (43) in IAPI and IQPI.", "startOffset": 320, "endOffset": 332}, {"referenceID": 6, "context": "the VGB greedy policy (Doya, 2000)), rather than performing the maximization (26).", "startOffset": 22, "endOffset": 34}, {"referenceID": 6, "context": "Note that this model is exactly same to that used by Doya (2000) except that the action U\u03c4 , the torque input, is coupled with the term \u2018cos \u03b8\u03c4 \u2019 rather than the constant \u20181,\u2019 which makes our problem more realistic and challenging.", "startOffset": 53, "endOffset": 65}, {"referenceID": 18, "context": "Also note that the IPI methods achieved our learning objective without using an initial stabilizing policy that is usually required in the optimal control setting under the total discounting \u03b3 = 1 (e.g., Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009; Lee et al., 2015).", "startOffset": 197, "endOffset": 273}], "year": 2017, "abstractText": "Policy iteration (PI) is a recursive process of policy evaluation and improvement to solve an optimal decision-making, e.g., reinforcement learning (RL) or optimal control problem and has served as the fundamental to develop RL methods. Motivated by integral PI (IPI) schemes in optimal control and RL methods in continuous time and space (CTS), this paper proposes on-policy IPI to solve the general RL problem in CTS, with its environment modelled by an ordinary differential equation (ODE). In such continuous domain, we also propose four off-policy IPI methods\u2014two are the ideal PI forms that use advantage and Q-functions, respectively, and the other two are natural extensions of the existing off-policy IPI schemes to our general RL framework. Compared to the IPI methods in optimal control, the proposed IPI schemes can be applied to more general situations and do not require an initial stabilizing policy to run; they are also strongly relevant to the RL algorithms in CTS such as advantage updating, Q-learning, and value-gradient based (VGB) greedy policy improvement. Our on-policy IPI is basically model-based but can be made partially model-free; each off-policy method is also either partially or completely model-free. The mathematical properties of the IPI methods\u2014admissibility, monotone improvement, and convergence towards the optimal solution\u2014are all rigorously proven, together with the equivalence of onand off-policy IPI. Finally, the IPI methods are simulated with an inverted-pendulum model to support the theory and verify the performance.", "creator": "LaTeX with hyperref package"}}}