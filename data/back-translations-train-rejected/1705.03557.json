{"id": "1705.03557", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "DeepTingle", "abstract": "DeepTingle is a text prediction and classification system trained on the collected works of the renowned fantastic gay erotica author Chuck Tingle. Whereas the writing assistance tools you use everyday (in the form of predictive text, translation, grammar checking and so on) are trained on generic, purportedly \"neutral\" datasets, DeepTingle is trained on a very specific, internally consistent but externally arguably eccentric dataset. This allows us to foreground and confront the norms embedded in data-driven creativity and productivity assistance tools. As such tools effectively function as extensions of our cognition into technology, it is important to identify the norms they embed within themselves and, by extension, us. DeepTingle is realized as a web application based on LSTM networks and the GloVe word embedding, implemented in JavaScript with Keras-JS.", "histories": [["v1", "Tue, 9 May 2017 22:12:19 GMT  (1270kb,D)", "http://arxiv.org/abs/1705.03557v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ahmed khalifa", "gabriella a b barros", "julian togelius"], "accepted": false, "id": "1705.03557"}, "pdf": {"name": "1705.03557.pdf", "metadata": {"source": "CRF", "title": "DeepTingle", "authors": ["Ahmed Khalifa", "Gabriella A. B. Barros", "Julian Togelius"], "emails": ["ahmed.khalifa@nyu.edu,", "gabriella.barros@gmail.com", "julian@togelius.com"], "sections": [{"heading": "Introduction", "text": "It is indeed the case that most of us do not know what to do and what to do, and do not know what to do and what to do, and do not know what to do and what to do. It is not that they should act as if they do, as if they do, as if they do, as if they do, as if they do, as if they do, as if they do, as if they do, as if they do, as if they do, as if they do."}, {"heading": "Background", "text": "This work builds on a number of modern machine learning methods, particularly in the form of deep learning."}, {"heading": "Word Embedding", "text": "Word embedding is a technique for converting words into an n-dimensional vector of real numbers that is able to capture the probable properties of the words in the current text. The primary goal is to reduce the dimensionality of the word space to a point where it can be easily processed. Each dimension in the vector represents a linguistic context, and the representation should preserve the properties of the original word (Goldberg and Levy 2014). Such mappings have been achieved using various techniques, such as neural networks (Bengio, Ducharme and Vincent 2003), main component analysis (Lebret and Collobert 2013) and probability models (Globerson et al. 2007).A popular method is skipping words with negative sampling training, a context predictive approach implemented in word2vec models (Mikolov et al. 2013)."}, {"heading": "Neural Networks and Recurrent Neural Networks", "text": "Neural networks (NNs) are a machine learning technique originally inspired by the way the human brain works (Hornik, Stinchcombe and White 1989).The basic unit of an NN is a neuron. Neurons obtain vectors as input and output values by applying a nonlinear function to the multiplication of said vectors and a set of weights. They are normally grouped in layers, and neurons in the same layer cannot be connected to each other. Neurons in a particular layer are fully connected to all neurons in the following layer. NNs can be trained using the back propagation algorithm. Baking propagation updates network weights by taking small steps toward minimizing the error measured by the network.A recursive neural network (RNN) is a special case of neural networks (RNN).The output of each layer depends not only on the input of the previous layer, but also on the output of the STN."}, {"heading": "Natural Language Generation", "text": "Approaches to natural language generation can be divided into two categories: rules or template-based and machine learning (Tang et al. 2016). Rule-based (or template-based) approaches (Cheyer and Guzzoni 2014; Mirkovic and Cavedon 2011) were considered the norm for most systems, with handmade rules / templates. However, these tend to be too specialized and not well generalized for different areas, and a large number of templates are needed to generate high-quality text even in a small area. Some efforts have been made to generate the template from a corpus using statistical methods (Mairesse et al. 2010; Mairesse and Young 2014; Oh and Rudnicky 2000), but these still require a lot of time and expertise."}, {"heading": "Creativity Assistance Tools", "text": "Goel and Joyner argue that scientific discoveries can be considered a creative task, and propose MILA-S, an interactive system aimed at promoting scientific modeling (Goel and Joyner 2015), which enables the creation of conceptual models of ecosystems that are evaluated using simulations. CAHOOTS is a chat system that is able to suggest images as possible jokes (Wen et al. 2015a). STANDUP (Waller et al. 2009) supports children who use augmentative and alternative communication to generate word games and jokes. Co-creative systems can also help in the creation of fictional ideas. Llano et al. (2014) describe three basic ideas methods using ConceptNet, ReVerb and bisociative discoveries, while I-get (Ojha, Lee and Lee 2015) utilize the ability of users to develop visual ideas."}, {"heading": "DeepTingle", "text": "This section discusses the methodology used in DeepTingle. DeepTingle consists of two main components: the neural network, which is responsible for learning and predicting words in the corpus, and a number of co-creativity tools designed to support writing or style transfer of text. The tools described (Predictive Tingle and Tingle Classics) are available online at http: / / www.deeptingle.net.Our training set includes all Chuck Tingle books published by November 2016: a total of 109 short stories and 2 novels (each with 11 chapters) to create a corpus of 3,044,178 characters. Text has been pre-edited by eliminating all punctuation except dots, commas, semicolons, question marks and apostrophes, and the remaining punctuation marks, excluding apostrophes, have been treated as separate words."}, {"heading": "Network Architecture", "text": "We experimented with different architectures. Our original intuition was to mimic the architecture of different Twitter bots. Twitter's limitation to 140 characters per tweet influenced the strategy of most of the bots trained in the neural network. They tend to work according to a character-by-character approach and produce the next character based on previous characters, not on words. Similarly, our first architecture, shown in Figure 1, was inspired by this representation. The numbers in the figure represent the size of the data flow between the network layers. The neural network consists of 3 layers: 2 LSTM layers followed by a Softmax layer. A Softmax layer uses Softmax function to convert the output of the neural network into the probability distribution of any other output class (Bridle 1990). In our case, classes are different letters. The size of the input and output is 57 because that represents the total number of different characters in Chuck Tingle's novels. The input is presented as a hot vector."}, {"heading": "Network training", "text": "The network training consisted of two phases. The first is to train the embedding layer separately, using GloVe and all Chuck Tingle stories in the corpus. In the second phase, we trained the remaining part of the network. Our reasoning for such an approach was to speed up the learning process. Dropout is used to increase network accuracy against unknown input words (missing words). Figure 3 shows the effect of the dropout on network accuracy. The graph shows that using 20% as a dropout value yields the highest accuracy, without sacrificing accuracy at 0% missing words. We used a recently proposed optimization technology, the Adam Optimizer (Kingma and Ba 2014), to train the network with a fixed learning rate (0.0001).This technique achieves a minimum value faster than conventional baking propagation. We experimented with different time steps for the set M and the preceding ST6 phrases, which were graded for us by the time steps."}, {"heading": "Predictive Tingle", "text": "Predictive Tingle is a writing tool built on the aforementioned network. Its goal is to make suggestions for the next word to be written based on what the user has written so far. It does this by pre-processing and encoding the user's input, feeding it into the network, and decoding the highest ranked results, which are presented as suggestions. As the user writes, the system goes through two phases: substitution and suggestion. Whenever a new word is written, predictable Tingle checks whether the word appears in a Tinglenary, a dictionary of all words from Chuck Tingle's books. When the word appears, nothing changes in this step. Otherwise, the system searches for the word in the dictionary that comes closest to input by using Levenshtein's string comparison (Levenshtein 1966). The input is then taken back by the said word substituted. As soon as the last substitution stage of the system searches for the last possible word for the suggested word, the input key is returned to the previous one."}, {"heading": "Tingle Classics", "text": "Tingle Classics aims to answer the question: \"What would happen if classical literature were actually written by Chuck Tingle?\" The user can select a line from a series of opening lines of famous and / or classic books (e.g. 1984 by George Orwell or Moby-dick by Herman Melville), and the system uses the line to generate a story by repeatedly predicting the next word in a sentence. Users can also parameterise the amount of words generated and determine whether words that do not appear in Tingle's works should be converted into words from the corpus."}, {"heading": "Results", "text": "A third tool, called Tingle Translator, aims to transfer Chuck Tingle's writing style to any text using NN and word embedding. Unfortunately, the embedding space for Chuck Tingle's novels is too small compared to the word embedding learned from Wikipedia articles, resulting in a failed attempt to establish a meaningful relationship between the two embedding. Using a neural network to bridge this gap has not been a success, and as such, Tingle Translator is not discussed in this work, which remains a possibility for future work."}, {"heading": "Network Training", "text": "I don't think it's just a matter of time before there's going to be a change. Examples 1 and 2 show that there's a generative story where every new word depends on the previous 6 words. I've been out on the streets leading to my friend's house. As I stumbled onto the chamber and then go into the parking lot and call my girlfriend to confirm my status as a normal, red-blooded, heterosexual person. However, despite my best efforts, I find out that I've always made my way. Kirk says with a laugh that he feels the hardness of my cock against his back. You get excited back, mate? No. I certainly don't feel like it."}, {"heading": "User Study", "text": "We conducted a user study to compare the text generated by DeepTingle with Chuck Tingle's original text. We also wanted to confirm whether a neural network actually had an advantage over a simpler representation, such as a Markov chain model. We trained a Markov chain using the same data set and chose state size 3, as it empirically achieved the best results without losing generalization capability.In the user study, the user had to answer three questions: \"Which text is grammatically more correct?\"; \"Which text is more interesting?\"; and \"Which text is more coherent?\" The user could choose one of four options: \"Left text is better,\" \"Correct text is better,\" \"Both are identical.\" We collected about 146 different grammars that are used once for the comparison chain, the latter not representing the same chain. \""}, {"heading": "Predictive Tingle", "text": "Figure 6 shows a screenshot of the system: Above, we have a brief description of what Predictive Tingle is; below, on the right, a text field in which the user can write text; and, on the right, a purple suggestion button that updates each time the user presses the space bar. In this example, the user wrote \"It was raining in New York\" and pressed the Enter key one after the other so that the system could stop typing. As a result, \"It was raining in New York City. It didn't take long for the familiar orgasmic sensations to bubble back in me and spread through my veins like seething erotic poison.\""}, {"heading": "Tingle Classics", "text": "The last part of the tools is Tingle Classics, shown in Figure 7. From top to bottom, the screen displays the name and description of the tool, followed by a list of books selected by the user. A button, \"Generate!,\" triggers the word generation. A line, bottom right, shows the initial line for the selected book. Two configuration options can be found in the order: the option to turn the substitution on and off, and the amount of words to be generated. Finally, the history generated is sketched at the very bottom of the page. When the substitution is selected, each word in the original texture that does not appear in the Tingle corpus is transformed into a Tingle word, ensuring that each word appears in the input vector."}, {"heading": "Conclusion and Future Work", "text": "This paper proposes a two-part system consisting of a deep neural network formed through a specific literary corpus and a writing tool built on the network. Our corpus consists exclusively of works by renowned author Chuck Tingle. This corpus represents a large number of stories that are diverse in setting and context, but similar in structure. Its controversial topics negate the \"neutral\" standard of writing aids currently available. We trained a six-layered architecture in which GloVe is embedded, LSTMs, dense and soft layers that are able to predict word sequences. Our system allows users to write word suggestions in real time, and to explore the intersection of classical literature and the fantastic erotic niche that Tingle embodies. We are excited about how much deeper we can take DeepTingle to improve the architecture of the system to increase the accuracy of the missing words."}, {"heading": "Acknowledgments", "text": "We thank Marco Scirea, who helped us come up with ideas for this work, Philip Bontrager, for useful discussions, Scott Lee and Daniel Gopstein, for their support and enthusiasm. We thank NVidia Corporation of GPUS for a gift to the NYU Game Innovation Lab. Gabriella Barros thanks CAPES and the Science Without Borders Program, BEX 1372713-3. Most of this paper was written by people."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["R. Ducharme", "P. Vincent"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Y. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Y. et al\\.", "year": 2003}, {"title": "J", "author": ["Bridle"], "venue": "S.", "citeRegEx": "Bridle 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "and Guzzoni", "author": ["A. Cheyer"], "venue": "D.", "citeRegEx": "Cheyer and Guzzoni 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Euclidean embedding of cooccurrence data", "author": ["Globerson"], "venue": "Journal of Machine Learning Research 8(Oct):2265\u20132295", "citeRegEx": "Globerson,? \\Q2007\\E", "shortCiteRegEx": "Globerson", "year": 2007}, {"title": "D", "author": ["A.K. Goel", "Joyner"], "venue": "A.", "citeRegEx": "Goel and Joyner 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Levy", "author": ["Y. Goldberg"], "venue": "O.", "citeRegEx": "Goldberg and Levy 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "K", "author": ["A.K. Hoover", "P.A. Szerlip", "Stanley"], "venue": "O.", "citeRegEx": "Hoover. Szerlip. and Stanley 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Multilayer feedforward networks are universal approximators. Neural networks 2(5):359\u2013366", "author": ["Stinchcombe Hornik", "K. White 1989] Hornik", "M. Stinchcombe", "H. White"], "venue": null, "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "J", "author": ["Kantosalo, A.", "Toivanen"], "venue": "M.; Xiao, P.; and Toivonen, H.", "citeRegEx": "Kantosalo et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Ba", "author": ["D. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Collobert", "author": ["R. Lebret"], "venue": "R.", "citeRegEx": "Lebret and Collobert 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "V", "author": ["Levenshtein"], "venue": "I.", "citeRegEx": "Levenshtein 1966", "shortCiteRegEx": null, "year": 1966}, {"title": "G", "author": ["Liapis, A.", "Yannakakis"], "venue": "N.; and Togelius, J.", "citeRegEx": "Liapis. Yannakakis. and Togelius 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "M", "author": ["Llano"], "venue": "T.; Hepworth, R.; Colton, S.; Gow, J.; Charnley, J.; Lavrac, N.; Znidar\u0161ic, M.; Perov\u0161ek, M.; Granroth-Wilding, M.; and Clark, S.", "citeRegEx": "Llano et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Young", "author": ["F. Mairesse"], "venue": "S.", "citeRegEx": "Mairesse and Young 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Phrase-based statistical language generation using graphical models and active learning", "author": ["Mairesse"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Mairesse,? \\Q2010\\E", "shortCiteRegEx": "Mairesse", "year": 2010}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Cavedon", "author": ["D. Mirkovic"], "venue": "L.", "citeRegEx": "Mirkovic and Cavedon 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A", "author": ["A.H. Oh", "Rudnicky"], "venue": "I.", "citeRegEx": "Oh and Rudnicky 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "I-get: A creativity assistance tool to generate perceptual pictorial metaphors", "author": ["Lee Ojha", "A. Lee 2015] Ojha", "H.-K. Lee", "M. Lee"], "venue": "In Proceedings of the 3rd International Conference on Human-Agent Interaction,", "citeRegEx": "Ojha et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ojha et al\\.", "year": 2015}, {"title": "C", "author": ["J. Pennington", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Pennington. Socher. and Manning 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["M. Roemmele", "Gordon"], "venue": "S.", "citeRegEx": "Roemmele and Gordon 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ropossum: An authoring tool for designing, optimizing and solving cut the rope levels", "author": ["Shaker Shaker", "N. Togelius 2013] Shaker", "M. Shaker", "J. Togelius"], "venue": "AIIDE", "citeRegEx": "Shaker et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shaker et al\\.", "year": 2013}, {"title": "Tanagra: A mixed-initiative level design tool", "author": ["Whitehead Smith", "G. Mateas 2010] Smith", "J. Whitehead", "M. Mateas"], "venue": "In Proceedings of the Fifth International Conference on the Foundations of Digital Games,", "citeRegEx": "Smith et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2010}, {"title": "Tanagra: Reactive planning and constraint solving for mixed-initiative level design", "author": ["Whitehead Smith", "G. Mateas 2011] Smith", "J. Whitehead", "M. Mateas"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games 3(3):201\u2013215", "citeRegEx": "Smith et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2011}, {"title": "A neural network approach to contextsensitive generation of conversational responses", "author": ["Sordoni"], "venue": "arXiv preprint arXiv:1506.06714", "citeRegEx": "Sordoni,? \\Q2015\\E", "shortCiteRegEx": "Sordoni", "year": 2015}, {"title": "G", "author": ["I. Sutskever", "J. Martens", "Hinton"], "venue": "E.", "citeRegEx": "Sutskever. Martens. and Hinton 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Context-aware natural language generation with recurrent neural networks. arXiv preprint arXiv:1611.09900", "author": ["Tang"], "venue": null, "citeRegEx": "Tang,? \\Q2016\\E", "shortCiteRegEx": "Tang", "year": 2016}, {"title": "D", "author": ["A. Waller", "R. Black", "OMara"], "venue": "A.; Pain, H.; Ritchie, G.; and Manurung, R.", "citeRegEx": "Waller et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "2015a. Omg ur funny! computer-aided humor with an application to chat", "author": ["Wen"], "venue": "In Proceedings of the 6th International Conference on Computational Creativity,", "citeRegEx": "Wen,? \\Q2015\\E", "shortCiteRegEx": "Wen", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745", "author": ["Wen"], "venue": null, "citeRegEx": "Wen,? \\Q2015\\E", "shortCiteRegEx": "Wen", "year": 2015}, {"title": "P", "author": ["Werbos"], "venue": "J.", "citeRegEx": "Werbos 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "G", "author": ["Yannakakis"], "venue": "N.; Liapis, A.; and Alexopoulos, C.", "citeRegEx": "Yannakakis. Liapis. and Alexopoulos 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Drawcompileevolve: Sparking interactive evolutionary art with human creations", "author": ["Zhang"], "venue": "In International Conference on Evolutionary and Biologically Inspired Music and Art,", "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "DeepTingle is a text prediction and classification system trained on the collected works of the renowned fantastic gay erotica author Chuck Tingle. Whereas the writing assistance tools you use everyday (in the form of predictive text, translation, grammar checking and so on) are trained on generic, purportedly \u201cneutral\u201d datasets, DeepTingle is trained on a very specific, internally consistent but externally arguably eccentric dataset. This allows us to foreground and confront the norms embedded in data-driven creativity and productivity assistance tools. As such tools effectively function as extensions of our cognition into technology, it is important to identify the norms they embed within themselves and, by extension, us. DeepTingle is realized as a web application based on LSTM networks and the GloVe word embedding, implemented in JavaScript with Keras-JS.", "creator": "LaTeX with hyperref package"}}}