{"id": "1705.03597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Solving Multi-Objective MDP with Lexicographic Preference: An application to stochastic planning with multiple quantile objective", "abstract": "In most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm \\textbf{FLMDP} that could solve general multi-objective MDP with lexicographic reward preference.", "histories": [["v1", "Wed, 10 May 2017 03:13:30 GMT  (10kb)", "http://arxiv.org/abs/1705.03597v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yan li", "zhaohan sun"], "accepted": false, "id": "1705.03597"}, "pdf": {"name": "1705.03597.pdf", "metadata": {"source": "CRF", "title": "Solving Multi-Objective MDP with Lexicographic Preference: An application to stochastic planning with multiple quantile objective", "authors": ["Yan Li", "Zhaohan Sun"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 5.03 597v 1In most common settings of the Markov Decision Process (MDP), an agent evaluates a policy based on the expectation of a (discounted) reward amount. However, this criterion may not be appropriate in many applications from two perspectives: First, the expectation of accumulated rewards in risk aversion situations is not robust enough, this is the case when the distribution of the accumulated reward is greatly distorted; another problem is that many applications naturally take several objectives into account when evaluating a policy, for example in autonomous driving, an agent must balance speed and safety in selecting the right decision. In this paper, we consider the evaluation of a policy based on a sequence of quantities he induces to a number of target states; our idea is to reformulate the original problem into a multi-objective MDP problem, defining lexicographic preference naturally. To calculate an optimal policy, we suggested a FLIP algorithm with an MDP that could solve an MDP objective."}, {"heading": "1 Introduction", "text": "Most classic MDP problems consider maximizing a scalar reward as an expectation [3]. < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p > p\" p > p > p \"p > p\" p > p \"p > p\" p > p \"p > p > p\" p > p > p > p > p > p > p \"p\" p \"p > p\" p \"p\" p > p \"p\" p \"p > p > p\" p > p \"p\" p > p \"p\" p > p \"p\" p \"p > p\" p \"p > p\" p \"p > p\" p > p \"p\" p \"p > p\" p > p \"p\" p > p \"p\" p > p \"p > p\" p \"p\" p > p \"p\" p \"p > p\" p \"p > p\" p > p \"p\" p \"p > p\" p \"p > p\" p \"p\" p \"p > p\" p \"p > p\" p \"p\" p > p \"p > p\" p \"p > p\" p \"p\" p > p \"p\" p > p \"p\" p \"p\" p > p \"p\" p \"p > p\" p \"p\" p > p \"p\" p \"p\" p > p \"p\" p \"p > p > p > p\" p \"p\" p \"p\" p > p > p > p \"p > p\" p \"p\" p > p > p \"p\" p > p > p \"p\" p \"p > p > p\" p \"p > p\""}, {"heading": "2 Problem Definition", "text": "We consider the problem of the finite horizon here as a multi-purpose Markov decision-making process described by a tuple (S, A, P, R), where: \u2022 S is finite action space. \u2022 G is finite action space. \u2022 T is finite horizon. \u2022 P is transition function given by: P (s, a, s) = P (s, s, a), i.e. the probability of transition from s to action a. \u2022 R = [R1, R2,.., Rk] is reward vector, where each component defines Ri (s, a, s \u00b2) reward by proceeding from state s, act a and transition to state s \u2032. Without loss of universality, we can assume that G = {g1,.., gn} is the reward vector, where each component Ri (s, s \u00b2) defines the reward state T by proceeding from state s, action and transition to state s \u2032, without loss of universality, we can assume {1 that the loss of universality."}, {"heading": "Finding Optimal Policy", "text": "Considering that our approach to determining the optimal policy is a series of optimization procedures, we will later show that these could be converted into a multi-objective MDP with lexicographic preference. Algorithmic Scheme 11. Means \"0\" = \"all possible policies.\" After finding a series of optimization procedures, we will later show that they can be converted into a multi-objective MDP with lexicographic preference."}, {"heading": "3 Multi-Quantile-Objective MDP", "text": "In this section, we first present a problem that generalizes the 1 of Hugo and Weng."}, {"heading": "Multi-Objective MDP with Lexicographic Preference", "text": "Definition 1. Remember that a dot u \u00b2 is lexicographically greater than 0, if ui = 0 is defined for i = 1.2 \u00b7 \u00b7 j and uj > 0 for about 1 6 j 6 n, we write u = (u1, u2 \u00b7 \u00b7 un) > l 0. We then define our lexicographic order index as j, which is the first index in the vector that is strictly greater than zero. So let's say u \u00b2 is lexicographically greater than v \u00b2 if u \u2212 v \u00b2 > l 0A multiobjective MDP differs from the standard MDP that it is reward vector R (s, a) = [R1 (s, a)... RL (s, a) and the associated value vector V (s) = [V1 (s) > l),."}, {"heading": "4 Solving Multi-Quantile-Objective MDP", "text": "The solution of Multiquantile-Objective MDP lies in the general situation of solving Multiquantile-Objective MDP with lexicographic preferences. A natural solution is to turn the original problem into a sequence of Confined Case MDP and solve this sequence of Confined MDP iteratively. In the next subsection, we proposed an algorithm that can directly solve general Multi-Objective MDP with lexicographic preferences, making the solution of Multiquantile-Objective MDP only a special case."}, {"heading": "Constrained MDP formulation", "text": "Step 1: Optimization of objective V \u03c0 1: Optimization of objective V \u0445 1: Maximization of maximum efficiency. Step 2: Optimization of objective V \u03c0i with limitations: Minimization \u03c0V \u03c0i subject to V \u03c0j > V \u043dij, j = 1:... Procedure as in 2: to step L. It is easy to see that in step i the limitations in the optimization process naturally limit the algorithm to searching for strategies in the class that is identical to the class, i.e. the correctness of this transformation is guaranteed. Altman [6] has shown that an optimal randomized policy can be found in such a limited MDP, Chen and Feinberg [4] have also shown how to find optimal strategies for determining."}, {"heading": "Lexicographic Markov Decision Process", "text": "In this subsection we introduce an algorithm FLMDP, the general lexicographic MDP in finite horizon (in particular, it can be used to obtain our previous multiquantile objective MDP.Let V \u03c0i, t: L \u00b7 S \u00b7 T \u00b7 R \u00b7 the expected reward by using the policy \u03c0 in decision epochs t, t + 1, \u00b7 \u00b7 T, here, for simplicity, we leave the reward of the final state equal to zero, so V \u03c0i, t the reward can be represented asV \u03c0i, t (s) = E \u03c0 st = s [T \u2211 n = tRi (sn, an)] Note that although in our problem Ri () the reward of the final state is equal to zero, we can solve this problem by taking Ri (st, at) = E [Ri (st + 1) [Ri (st, st + 1))) with expectation w.r.t"}, {"heading": "5 Conclusion", "text": "Our contribution consists of two folds: firstly, we formulate the problem into a multi-objective MDP problem; secondly, our algorithm to solve this problem could also solve a general multi-objective MDP problem with a limited horizon, state space and action space; expansion to an infinite state space or action space could also be done with slight modification; we refer to our possible future work here: Pineda et.al [1] has shown that a limited MDP could be converted into a sequence of multi-objective MDP with lexicographic preference and additional slip variables, i.e. if lexicographic MDP with slip variables could be efficiently solved, then the solution of a restricted MDP follows. For a finite horizon, we believe that a similar dynamic programming algorithm to solve lexicographic MDP with slip variables could be invented."}], "references": [{"title": "Revisiting Multi-Objective MDPs with relaxed Lexicographic Preferences", "author": ["Luis Pineda", "Kyle H. Wray", "Shlomo Zilberstein"], "venue": "AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multi-Objective MDPs with Conditional Lexicographic Reward Preferences", "author": ["Kyle H. Wray", "Shlomo Zilberstein", "Abdel-Illah Mouaddib"], "venue": "In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence (AAAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Preference Order Dynamic Programming", "author": ["L.G. Mitten"], "venue": "Management Science. Volume 21,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1974}, {"title": "Constrained Markov Decision Process", "author": ["Eitan Altman"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Ordinal Dynamic Programming", "author": ["Matthew J. Sobel"], "venue": "Management Science", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1975}, {"title": "Quantile Reinforcement Learning", "author": ["Hugo Gilbert", "Paul Weng"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard S", "Andrew G. Barto"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Markov Decision Processes with Ordinal Rewards: Reference Point- Based Preferences", "author": ["Weng", "Paul"], "venue": "In ICAPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "For example, in self-autonomous driving one need to balance speed and safety [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "However in practice it is hard to evaluate and analyze the projected problem since there might be many viable Pareto optimal solutions to the original problem [1].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs.", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "al [2] also consider a more general setting when lexicographical order depends on initial state and slack for higher objective value is allowed for improvement over lower priority objective.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Based on this motivation, Hugo and Weng [8] proposed quantile based reinforcement learning algorithm which seeks to optimize certain lower/upper quantile on the random outcome.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "< \u03c4L \u2208 [0, 1], following our motivation in Introduction section, our procedure to find optimal policy is a series of optimization procedure, we will show later this could be reshaped into multi-objective MDP with lexicographic preference.", "startOffset": 7, "endOffset": 13}, {"referenceID": 5, "context": "In this section we first present a lemma that generalize the Lemma 1 of Hugo and Weng [8], this lemma fully characterize the q \u03c4i Lemma 1.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "Altman [6] has shown that an optimal randomized policy could be found in such constrained MDP, Chen and Feinberg [4] also showed how to find optimal deterministic policy.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "al [1] has showed that constrained MDP could be reshaped into a sequence of multi-objective MDP with lexicographic preference and additional slack variables, thus if one could solve lexicographic MDP with slack variable efficiently, then solution of constrained MDP follows.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "In most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm FLMDP that could solve general multi-objective MDP with lexicographic reward preference.", "creator": "LaTeX with hyperref package"}}}