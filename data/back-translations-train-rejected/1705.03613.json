{"id": "1705.03613", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "An initialization method for the k-means using the concept of useful nearest centers", "abstract": "The aim of the k-means is to minimize squared sum of Euclidean distance from the mean (SSEDM) of each cluster. The k-means can effectively optimize this function, but it is too sensitive for initial centers (seeds). This paper proposed a method for initialization of the k-means using the concept of useful nearest center for each data point.", "histories": [["v1", "Wed, 10 May 2017 06:19:04 GMT  (388kb)", "http://arxiv.org/abs/1705.03613v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hassan ismkhan"], "accepted": false, "id": "1705.03613"}, "pdf": {"name": "1705.03613.pdf", "metadata": {"source": "CRF", "title": "An initialization method for the k-means using the concept of useful nearest centers", "authors": ["Hassan Ismkhan"], "emails": ["h.ismkhan@bonabu.ac.ir,", "esmkhan@gmail.com"], "sections": [{"heading": null, "text": "The goal of the k mean is to minimize the square sum of the Euclidean distance from the mean (SSEDM) of each cluster. k mean can effectively optimize this function, but is too sensitive to initial centers (seeds). In this paper, a method for initializing the k mean was proposed that uses the concept of the closest useful center for each data.Keywords: k mean, initialization, k mean + +, useful centers, useful nearest neighbor."}, {"heading": "1. Introduction", "text": "In fact, it is as if it were a pure disinfectant capable of correcting, correcting and correcting the aforementioned errors."}, {"heading": "2. The proposed initialization algorithm", "text": "Before specifying the proposed initialization method, we must specify two simple definitions: the concept of the next center: a center C is useful for a data point P, if it is not a useless closest center P. Example 3.1. The proposed initialization method selects the first center with the smallest value on the first axis, then it is a useful center for P, because C2 does not meet the definition 2. The proposed initialization method selects the first center with the smallest value on the first axis, then it is in each of the next iterations, a data point P with the largest value ofau the second."}, {"heading": "Acknowledgment", "text": "This research is supported by the University of Bonab, via author personal grant.References [1] J. B. MacQueen, \"Some methods for classification and analysis of multivariate observations,\" in Fifth BerkeleySymposium on Mathematical Statistics and Probability, Berkeley, California, 1967. [2] H. Ismkhan, \"A.1D-C: A novel fast automatic heuristic to handle-large-scale one-dimensional clustering,\" Applied Soft Computing, vol. 52, pp. 1200-1209, 2017. [3] P. Drineas, A. M. Frieze, R. Kannan, S. Vempala and V. Vinay, \"Clustering large graphs via the singular valuedecomposition,\" Machine Learning, vol. 56, no. 1-3, pp. 9-33, 2004."}], "references": [{"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "Fifth Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, California, 1967.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1967}, {"title": "A.1D-C: A novel fast automatic heuristic to handle large-scale one-dimensional clustering", "author": ["H. Ismkhan"], "venue": "Applied Soft Computing, vol. 52, p. 1200\u20131209, 2017.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["P. Drineas", "A.M. Frieze", "R. Kannan", "S.S. Vempala", "V. Vinay"], "venue": "Machine Learning, vol. 56, no. 1-3, pp. 9-33, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "P.S. Yu", "Z.-H. Zhou", "M. Steinbach", "D.J. Hand", "D. Steinberg"], "venue": "Knowledge and Information Systems, vol. 14, no. 1, pp. 1-37, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["T.F. Gonzalez"], "venue": "Theoretical Computer Science, vol. 38, no. 2\u20133, p. 293\u2013306, 1985.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1985}, {"title": "A new initialization technique for generalized Lloyd iteration", "author": ["I. Katsavounidis", "C.-C.J. Kuo", "Z. Zhang"], "venue": "IEEE Signal Processing Letters, vol. 1, no. 10, p. 144\u2013146, 1994.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "k-means++ : The Advantages of Careful Seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "the eighteenth annual ACM- SIAM symposium on Discrete algorithms, New Orleans, Louisiana, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "The aim of the k-means [1] is to group these data points into the k disjoint subsets Si (0\u2264i\u2264k), such that minimize the sum of squared Euclidean distances to the mean of each subset (SSEDM) as its objective function.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Although exactly minimizing SSEDM can not guarantee the best quality for clustering solution [2], and exactly minimizing this function is an NP-Hard problem [3], not only this function is popular to measure quality of clustering solutions, it is also used as an objective function in the kmeans, which is one of the most influential data mining algorithms [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Although exactly minimizing SSEDM can not guarantee the best quality for clustering solution [2], and exactly minimizing this function is an NP-Hard problem [3], not only this function is popular to measure quality of clustering solutions, it is also used as an objective function in the kmeans, which is one of the most influential data mining algorithms [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Although exactly minimizing SSEDM can not guarantee the best quality for clustering solution [2], and exactly minimizing this function is an NP-Hard problem [3], not only this function is popular to measure quality of clustering solutions, it is also used as an objective function in the kmeans, which is one of the most influential data mining algorithms [4].", "startOffset": 356, "endOffset": 359}, {"referenceID": 4, "context": "References [5] [6] propose an initialization method, namely Maxmin, which chooses the first center randomly, and in the rest, it selects a data point with largest distance to its nearest center as a new center.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "References [5] [6] propose an initialization method, namely Maxmin, which chooses the first center randomly, and in the rest, it selects a data point with largest distance to its nearest center as a new center.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "Among famous initialization methods, k-means++ [7] is one of the most accurate algorithm and easy to be implemented.", "startOffset": 47, "endOffset": 50}], "year": 2017, "abstractText": "The aim of the k-means is to minimize squared sum of Euclidean distance from the mean (SSEDM) of each cluster. The k-means can effectively optimize this function, but it is too sensitive for initial centers (seeds). This paper proposed a method for initialization of the k-means using the concept of useful nearest center for each data point.", "creator": "Microsoft\u00ae Word 2013"}}}