{"id": "1705.03633", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Inferring and Executing Programs for Visual Reasoning", "abstract": "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.", "histories": [["v1", "Wed, 10 May 2017 07:08:23 GMT  (3905kb,D)", "http://arxiv.org/abs/1705.03633v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["justin johnson", "bharath hariharan", "laurens van der maaten", "judy hoffman", "li fei-fei", "c lawrence zitnick", "ross girshick"], "accepted": false, "id": "1705.03633"}, "pdf": {"name": "1705.03633.pdf", "metadata": {"source": "CRF", "title": "Inferring and Executing Programs for Visual Reasoning", "authors": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Judy Hoffman", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross Girshick"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2. Related Work", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3. Method", "text": "We develop a learnable compositional model for answering visual questions. Our model takes as input an image x and a visual question q over the image. The model selects an answer A to the question from a fixed set A. Internally, the model predicts a program z, which represents the thought steps required to answer the question. The model then executes the predicted program on the image and generates a distribution of answers. To this end, we organize our system into two components: a program generator, z = \u03c0 (q), which predicts programs from questions, and an executor, a = \u03c6 (x, z), which executes a program z on a picture x to predict an answer. Both the program generator and the executor are neural networks, which are learned from data. Unlike previous work [1, 2] we do not design hayristics for creating or executing the programs manually. We present learning procedures both for settings in which (some) basic truth programs are available to us during some of the training, as well as for those we need to see during some of the training."}, {"heading": "3.1. Programs", "text": "Like all programming languages, our programs are defined by syntax rules for building valid programs, and semantics defines the behavior of valid programs. We focus on learning the semantics for a fixed syntax. Specifically, we fix the syntax by specifying a set of F functions f, each of which has a fixed character. As we are interested in answering visual questions, we insert a special constant scene into the vocabulary that represents the visual characteristics of the image. We present valid programs z as syntax trees in which each node contains a function f-F and in which each node has as many children as the type of the function f."}, {"heading": "3.2. Program generator", "text": "The program generator z = \u03c0 (q) predicts programs z from natural language questions q, which are represented as sequences of words. To serialize the syntax tree, which represents a non-sequential discrete structure, into a sequence of functions, we use a prefix traverse. This allows us to implement the program generator with a standard LSTM sequence sequence model; see [39] for details. When decoding at test date, we simply use the Argmax function at each time step. The resulting sequence of functions is converted into a syntax tree; this is straightforward because the uniformity of each function is known. Some generated sequences do not correspond to the prefix traversals of a tree. If the sequence is too short (some functions do not have enough children), we fill the sequence with scene constants. If the sequence is too long (some functions do not have parents), then unused functions are discarded."}, {"heading": "3.3. Execution engine", "text": "In the face of a predicted program z and an input image x, the executor executes the program on the image, a = \u03c6 (x, z) to predict a response a. The executor is implemented using a neural module network [2]: The executor z is used to compose a question-specific neural network composed of a row of modules 3. For each function f = F, the executor maintains a neural network module mf. In the face of a program z, the executor creates a neural network m (z) by mapping each function f to its corresponding module f in the order defined by the program 3. 3. 3. For each function f = F 4. 4. The outputs of the \"child modules\" are used as inputs in their corresponding \"parent module\" 4. \"Our modules use a generic architecture as opposed to [2]. A module of indetermination n x x x x x x x receives n characteristics of the form C x H x W and generates a characteristic map of the form C x x x 14. Each module x x x x is a standardized 101 x x x x x x x x x x x x x x."}, {"heading": "3.4. Training", "text": "Given a VQA dataset containing (x, q, z, a) tuples with ground truth programs z, we can train both the program generator and the executor in a supervised manner. Specifically, we can (1) use pairs (q, z) of questions and corresponding programs to train the program generator, resulting in the formation of a standard sequence-to-sequence model; and (2) use triplets (x, z, a) of the image, program, and answer to train the executor by calculating the required gradients (as in [2]) with back propagation. Noting ground truth programs for free-form questions in the natural language is expensive, so in practice we have few or no ground truth programs. To address this problem, we opt for the program generator and the executor together to train on (x, q, a), tripling the gross program and without truth programs."}, {"heading": "4. Experiments", "text": "All questions are equipped with ground-truth programs that allow experiments with varying levels of oversight. First, we conduct experiments that use strong oversight in the form of ground-truth programs. We show that in this highly monitored environment, the combination of program generator and executor on CLEVR works much better than alternative methods. Next, we show that this strong performance is maintained when a small number of ground-truth programs that capture only a fraction of the variety of questions are used for training. Finally, we evaluate the ability of our models to perform compositional generalizations and generalize to free-form questions posed by humans. Code that reproduces the results of our experiments is available at https: / / / github.com / facebookresearch / clevr-iep."}, {"heading": "4.1. Baselines", "text": "Johnson et al. [19] tested several VQA models on CLEVR. We reproduce these models as baselines here.Q-type mode: This baseline predicts the most common answer for each question type in CLEVR.LSTM: Similar to [3, 33], questions are processed with learned word embeddings, followed by a word level LSTM [15]. The final LSTM hidden state is passed to a multi-layered perceptron (MLP) that predicts a distribution via answers. This method does not use image information and can therefore only model question-related distortions. CNN + LSTM: Images and questions are encoded using Convolutionary Network Functions (CNN) or concluding LSTM hidden states. These features are linked and passed on to an MLP that predicts an answer distribution.CNN + LSTM + SA [46]: Questions and images are forwarded via a CNN feature (CNN hidden or LM), respectively, the final attributes are given to an STM."}, {"heading": "4.2. Strongly and semi-supervised learning", "text": "First, we experiment with a model that has been trained with full monitoring: we use the soil truth programs for all Ques-2See complementary material, for example, analysis of CLEVR issues. Results show that our model with strong monitoring can achieve near-perfect accuracy on CLEVR (even if it surpasses the Mechanical Turk Workers). In practical scenarios, the soil truth programs are not available for all questions. We use the semi-supervised training process described in Section 3.4 to determine how many soil truth programs are needed to match fully su-perforated models. First, the program generator is trained in a supervised manner using a small number of questions and soil truth programs; second, the executor is trained on all CLEVR issues and not on all CLEVR programs."}, {"heading": "4.3. What do the modules learn?", "text": "In order to gain additional insight into what the modules have learned in the executor, we visualized the parts of the image used to answer various questions; see Figure 3. In particular, the figure shows the norm of gradient of the sum of predicted response values (softmax inputs) with respect to the final characteristic map. This visualization reveals several important aspects of our model. First, even with complicated reference expressions that involve spatial relations, intersections and unification of constraints, etc., it clearly looks for the right objects. Second, the examples show that changing a single module (swapping violet / blue, left / right and / or) results in drastic changes in both the predicted response and model attention, showing that the individual modules actually fulfill their intended functions."}, {"heading": "4.4. Generalizing to new attribute combinations", "text": "Johnson et al. [19] proposed the CLEVR-CoGenT dataset to investigate the ability of VQA models to perform compositional generalizations; the dataset contains data under two different conditions: in state A, all cubes are gray, blue, brown, or yellow, and all cylinders are red, green, violet, or cyan; in state B, cubes and cylinders exchange color palettes. Johnson et al. [19] found that VQA models are poorly trained on state A data, suggesting that the models are not well able to generalize to new conditions. We conducted experiments with our model on CLEVRCoGenT: in figure 5, we report on the accuracy of the semi-monitored variant of our model, which was trained on state A data and evaluated on state B data."}, {"heading": "4.5. Generalizing to new question types", "text": "Our experiments in Section 4.2 showed that relatively few soil-truth programs are required to effectively train our model. However, due to the large number of unique programs in CLEVR, it is impossible to cover all possible programs with a small set of soil-truth programs; however, due to the synthetic nature of the CLEVR questions, it is possible that a small number of programs could cover all possible program structures. In real scenarios, models should be able to generalize questions with novel program structures without observing the associated soil-truth programs. To test this, we divide CLEVR questions into two categories based on their soil-truth programs: short and long. CLEVR questions are divided into question families in which all questions in the same family share the same program structure. A question is short if their question family has an average program length of less than 16; otherwise, it is lengthy."}, {"heading": "4.6. Generalizing to human-posed questions", "text": "The fact that the questions in the CLEVR benchmark were generated algorithmically may favor some approaches over others. In particular, natural language tends to be more ambiguous than the algorithmically generated questions. We conducted an experiment to assess the extent to which the skills acquired in the CLEVR are fine. To this end, we collected a new data set that is difficult for an intelligent robot to answer. Workers were brought into contact with the answers in the CLEVR data collection."}, {"heading": "5. Discussion and Future Work", "text": "Our results show that our model is capable of generalizing to novel scenes and questions, and even deriving programs for free-format human questions from its learned modules. Although these results are encouraging, there are still many questions that cannot be approached with our fixed modules. For example, the question \"What color is the object with a unique shape?\" requires a model for identifying unique shapes for which no module is currently available. Adding new modules to our model is easy due to our generic module design, but the automatic identification and learning of new modules without program monitoring is still an open problem. One way forward is the development of a Turing-complete module set; this would allow all programs to be expressed without learning new modules. For example, by adding ternary operators (if / then / otherwise) and loops (for / do) the question \"What color does the object with a unique shape?\" an answer can be given to all programs without learning new modules."}, {"heading": "6. Conclusion", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "B. Neural Module Network parses", "text": "The method closest to us is that of Andreas et al. [1]. Their dynamic neural module networks first perform a dependency parse of the sentence; heuristics is then used to generate a series of layout fragments from the dependency sparse.These fragments are heuristically combined, resulting in a series of candidate layouts; the final network layout is then selected from these candidates by a learned ranking step.Unfortunately, we found that the parser used in [1] for VQA questions did not perform well in the longer questions in CLEVR. In Table 8, we show random questions from the CLEVR training together with the layout fragments calculated with the parser from [1].For many questions, the parser fails by resorting to the fragment (whatever); if this happens, the resulting module network will not respect the structure of the question at all. [For questions that do not fall back on the standard layout]."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "NAACL", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Zitnick", "D. Parikh"], "venue": "ICCV", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepcoder: Learning to write programs", "author": ["M. Balog", "A. Gaunt", "M. Brockschmidt", "S. Nowozin", "D. Tarlow"], "venue": "ICLR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Making neural programming architectures generalize via recursion", "author": ["J. Cai", "R. Shin", "D. Song"], "venue": "ICLR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Visual dialog", "author": ["A. Das", "S. Kottur", "K. Gupta", "A. Singh", "D. Yadav", "J. Moura", "D. Parikh", "D. Batra"], "venue": "CVPR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "arXiv:1606.01847", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Compact bilinear pooling", "author": ["Y. Gao", "O. Beijbom", "N. Zhang", "T. Darrell"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "author": ["Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "K", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwinska", "S. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou", "A. Badia", "K. Hermann", "Y. Zwols", "G. Ostrovski", "A. Cain", "H. King", "C. Summerfield", "P. Blunsom"], "venue": ". Kavukcuoglu, and D. Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Modeling relationships in referential expressions with compositional modular networks", "author": ["R. Hu", "M. Rohrbach", "J. Andreas", "T. Darrell", "K. Saenko"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "and L", "author": ["A. Jabri", "A. Joulin"], "venue": "van der Maaten. Revisiting visual question answering baselines. In ECCV", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "L", "author": ["J. Johnson", "B. Hariharan"], "venue": "van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "NIPS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual question answering: Datasets", "author": ["K. Kafle", "C. Kanan"], "venue": "algorithms, and future challenges. In arXiv preprint arXiv:1610.01465", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural GPUs learn algorithms", "author": ["\u0141. Kaiser", "I. Sutskever"], "venue": "ICLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "ACL, pages 423\u2013430", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "et al", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": "Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Neural random-access machines", "author": ["K. Kurach", "M. Andrychowicz", "I. Sutskever"], "venue": "ICLR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "Behavioral and Brain Sciences", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["C. Liang", "J. Berant", "Q. Le", "K.D. Forbus", "N. Lao"], "venue": "arXiv preprint arXiv:1611.00020", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning dependencybased compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "ACL", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning models for actions and person-object interactions with transfer to question answering", "author": ["A. Mallya", "S. Lazebnik"], "venue": "ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "ICLR", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural programmer-interpreters", "author": ["S. Reed", "N. De Freitas"], "venue": "ICLR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "ImageNet large scale visual recognition challenge. IJCV", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "End-toend memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "NIPS", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CVPR", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "ICLR", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning, 8(23)", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1992}, {"title": "Understanding Natural Language", "author": ["T. Winograd"], "venue": "Academic Press", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1972}, {"title": "and A", "author": ["Q. Wu", "D. Teney", "P. Wang", "C. Shen", "A. Dick"], "venue": "van den Hengel. Visual question answering: A survey of methods and datasets. In arXiv preprint arXiv:1607.05910", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning simple algorithms from examples", "author": ["W. Zaremba", "T. Mikolov", "A. Joulin", "R. Fergus"], "venue": "ICML", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement learning neural turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1505.00521", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual7W: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 27, "context": "Such compositional reasoning is a hallmark of human intelligence, and allows people to solve a plethora of problems using a limited set of basic skills [28].", "startOffset": 152, "endOffset": 156}, {"referenceID": 25, "context": "Direct input-output mapping works well for classifying images [26] and detecting objects [10] for a small, fixed set of categories.", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "Direct input-output mapping works well for classifying images [26] and detecting objects [10] for a small, fixed set of categories.", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "However, it fails to outperform strong baselines on tasks that require the model to understand an exponentially large space of objects, attributes, actions, and interactions, such as visual question answering (VQA) [3, 51].", "startOffset": 215, "endOffset": 222}, {"referenceID": 50, "context": "However, it fails to outperform strong baselines on tasks that require the model to understand an exponentially large space of objects, attributes, actions, and interactions, such as visual question answering (VQA) [3, 51].", "startOffset": 215, "endOffset": 222}, {"referenceID": 18, "context": "Current models fail to do such reasoning [19].", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "to learn dataset biases but not reasoning [7, 18, 19].", "startOffset": 42, "endOffset": 53}, {"referenceID": 17, "context": "to learn dataset biases but not reasoning [7, 18, 19].", "startOffset": 42, "endOffset": 53}, {"referenceID": 18, "context": "to learn dataset biases but not reasoning [7, 18, 19].", "startOffset": 42, "endOffset": 53}, {"referenceID": 0, "context": "Our model builds on prior work on neural module networks that incorporate compositional reasoning [1, 2].", "startOffset": 98, "endOffset": 104}, {"referenceID": 1, "context": "Our model builds on prior work on neural module networks that incorporate compositional reasoning [1, 2].", "startOffset": 98, "endOffset": 104}, {"referenceID": 18, "context": "We evaluate our model on the recently released CLEVR dataset [19], which has proven to be challenging for stateof-the-art VQA models.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Visual question answering (VQA) is a popular proxy task for gauging the quality of visual reasoning systems [21, 44].", "startOffset": 108, "endOffset": 116}, {"referenceID": 43, "context": "Visual question answering (VQA) is a popular proxy task for gauging the quality of visual reasoning systems [21, 44].", "startOffset": 108, "endOffset": 116}, {"referenceID": 2, "context": "Like the CLEVR dataset, benchmark datasets for VQA typically comprise a set of questions on images with associated answers [3, 32, 40, 25, 51]; both questions and answers are generally posed in natural language.", "startOffset": 123, "endOffset": 142}, {"referenceID": 31, "context": "Like the CLEVR dataset, benchmark datasets for VQA typically comprise a set of questions on images with associated answers [3, 32, 40, 25, 51]; both questions and answers are generally posed in natural language.", "startOffset": 123, "endOffset": 142}, {"referenceID": 39, "context": "Like the CLEVR dataset, benchmark datasets for VQA typically comprise a set of questions on images with associated answers [3, 32, 40, 25, 51]; both questions and answers are generally posed in natural language.", "startOffset": 123, "endOffset": 142}, {"referenceID": 24, "context": "Like the CLEVR dataset, benchmark datasets for VQA typically comprise a set of questions on images with associated answers [3, 32, 40, 25, 51]; both questions and answers are generally posed in natural language.", "startOffset": 123, "endOffset": 142}, {"referenceID": 50, "context": "Like the CLEVR dataset, benchmark datasets for VQA typically comprise a set of questions on images with associated answers [3, 32, 40, 25, 51]; both questions and answers are generally posed in natural language.", "startOffset": 123, "endOffset": 142}, {"referenceID": 2, "context": "Many systems for VQA employ a very similar architecture [3, 8, 9, 31, 33, 34, 45]: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers.", "startOffset": 56, "endOffset": 81}, {"referenceID": 7, "context": "Many systems for VQA employ a very similar architecture [3, 8, 9, 31, 33, 34, 45]: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers.", "startOffset": 56, "endOffset": 81}, {"referenceID": 8, "context": "Many systems for VQA employ a very similar architecture [3, 8, 9, 31, 33, 34, 45]: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers.", "startOffset": 56, "endOffset": 81}, {"referenceID": 30, "context": "Many systems for VQA employ a very similar architecture [3, 8, 9, 31, 33, 34, 45]: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers.", "startOffset": 56, "endOffset": 81}, {"referenceID": 32, "context": "Many systems for VQA employ a very similar architecture [3, 8, 9, 31, 33, 34, 45]: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers.", "startOffset": 56, "endOffset": 81}, {"referenceID": 33, "context": "Many systems for VQA employ a very similar architecture [3, 8, 9, 31, 33, 34, 45]: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers.", "startOffset": 56, "endOffset": 81}, {"referenceID": 44, "context": "Many systems for VQA employ a very similar architecture [3, 8, 9, 31, 33, 34, 45]: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers.", "startOffset": 56, "endOffset": 81}, {"referenceID": 17, "context": "Recent work has questioned whether such systems are capable of developing visual reasoning capabilities: (1) very simple baseline models were found to perform competitively on VQA benchmarks by exploiting biases in the data [18, 50, 11] and (2) experiments on CLEVR, which was designed to control such biases, revealed that current systems do not learn to reason about spatial relationships or to learn disentangled representations [19].", "startOffset": 224, "endOffset": 236}, {"referenceID": 49, "context": "Recent work has questioned whether such systems are capable of developing visual reasoning capabilities: (1) very simple baseline models were found to perform competitively on VQA benchmarks by exploiting biases in the data [18, 50, 11] and (2) experiments on CLEVR, which was designed to control such biases, revealed that current systems do not learn to reason about spatial relationships or to learn disentangled representations [19].", "startOffset": 224, "endOffset": 236}, {"referenceID": 10, "context": "Recent work has questioned whether such systems are capable of developing visual reasoning capabilities: (1) very simple baseline models were found to perform competitively on VQA benchmarks by exploiting biases in the data [18, 50, 11] and (2) experiments on CLEVR, which was designed to control such biases, revealed that current systems do not learn to reason about spatial relationships or to learn disentangled representations [19].", "startOffset": 224, "endOffset": 236}, {"referenceID": 18, "context": "Recent work has questioned whether such systems are capable of developing visual reasoning capabilities: (1) very simple baseline models were found to perform competitively on VQA benchmarks by exploiting biases in the data [18, 50, 11] and (2) experiments on CLEVR, which was designed to control such biases, revealed that current systems do not learn to reason about spatial relationships or to learn disentangled representations [19].", "startOffset": 432, "endOffset": 436}, {"referenceID": 11, "context": "For example, models such as neural Turing machines [12, 13], memory networks [41, 38], and stack-augmented recurrent networks [20] add explicit memory components to neural networks to facilitate learning of reasoning processes that involve long-term memory.", "startOffset": 51, "endOffset": 59}, {"referenceID": 12, "context": "For example, models such as neural Turing machines [12, 13], memory networks [41, 38], and stack-augmented recurrent networks [20] add explicit memory components to neural networks to facilitate learning of reasoning processes that involve long-term memory.", "startOffset": 51, "endOffset": 59}, {"referenceID": 40, "context": "For example, models such as neural Turing machines [12, 13], memory networks [41, 38], and stack-augmented recurrent networks [20] add explicit memory components to neural networks to facilitate learning of reasoning processes that involve long-term memory.", "startOffset": 77, "endOffset": 85}, {"referenceID": 37, "context": "For example, models such as neural Turing machines [12, 13], memory networks [41, 38], and stack-augmented recurrent networks [20] add explicit memory components to neural networks to facilitate learning of reasoning processes that involve long-term memory.", "startOffset": 77, "endOffset": 85}, {"referenceID": 19, "context": "For example, models such as neural Turing machines [12, 13], memory networks [41, 38], and stack-augmented recurrent networks [20] add explicit memory components to neural networks to facilitate learning of reasoning processes that involve long-term memory.", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "Module networks are an example of reasoningaugmented models that use a syntactic parse of a question to determine the architecture of the network [1, 2, 16].", "startOffset": 146, "endOffset": 156}, {"referenceID": 1, "context": "Module networks are an example of reasoningaugmented models that use a syntactic parse of a question to determine the architecture of the network [1, 2, 16].", "startOffset": 146, "endOffset": 156}, {"referenceID": 15, "context": "Module networks are an example of reasoningaugmented models that use a syntactic parse of a question to determine the architecture of the network [1, 2, 16].", "startOffset": 146, "endOffset": 156}, {"referenceID": 23, "context": "The main difference between our models and existing module networks is that we replace hand-designed off-the-shelf syntactic parsers [24], which perform very poorly on complex questions such as those in CLEVR [19], by a learnt program generator that can adapt to the task at hand.", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "The main difference between our models and existing module networks is that we replace hand-designed off-the-shelf syntactic parsers [24], which perform very poorly on complex questions such as those in CLEVR [19], by a learnt program generator that can adapt to the task at hand.", "startOffset": 209, "endOffset": 213}, {"referenceID": 29, "context": "Often, the goal is to answer natural language questions using a knowledge base [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Recent approaches to semantic parsing involve a learnt programmer [29].", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search [4], or of a recurrent network that decodes a vectorial program representation into the actual program [22, 27, 35, 47, 48, 49].", "startOffset": 150, "endOffset": 153}, {"referenceID": 21, "context": "Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search [4], or of a recurrent network that decodes a vectorial program representation into the actual program [22, 27, 35, 47, 48, 49].", "startOffset": 253, "endOffset": 277}, {"referenceID": 26, "context": "Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search [4], or of a recurrent network that decodes a vectorial program representation into the actual program [22, 27, 35, 47, 48, 49].", "startOffset": 253, "endOffset": 277}, {"referenceID": 34, "context": "Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search [4], or of a recurrent network that decodes a vectorial program representation into the actual program [22, 27, 35, 47, 48, 49].", "startOffset": 253, "endOffset": 277}, {"referenceID": 46, "context": "Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search [4], or of a recurrent network that decodes a vectorial program representation into the actual program [22, 27, 35, 47, 48, 49].", "startOffset": 253, "endOffset": 277}, {"referenceID": 47, "context": "Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search [4], or of a recurrent network that decodes a vectorial program representation into the actual program [22, 27, 35, 47, 48, 49].", "startOffset": 253, "endOffset": 277}, {"referenceID": 48, "context": "Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search [4], or of a recurrent network that decodes a vectorial program representation into the actual program [22, 27, 35, 47, 48, 49].", "startOffset": 253, "endOffset": 277}, {"referenceID": 35, "context": "The recurrent networks may incorporate compositional structure that allows them to learn new programs by combining previously learned sub-programs [36].", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "1Memory is likely indispensable in more complex settings such as visual dialogues or SHRDLU [6, 43].", "startOffset": 92, "endOffset": 99}, {"referenceID": 42, "context": "1Memory is likely indispensable in more complex settings such as visual dialogues or SHRDLU [6, 43].", "startOffset": 92, "endOffset": 99}, {"referenceID": 0, "context": "In contrast to prior work [1, 2], we do not manually design heuristics for generating or executing the programs.", "startOffset": 26, "endOffset": 32}, {"referenceID": 1, "context": "In contrast to prior work [1, 2], we do not manually design heuristics for generating or executing the programs.", "startOffset": 26, "endOffset": 32}, {"referenceID": 38, "context": "This allows us to implement the program generator using a standard LSTM sequence-tosequence model; see [39] for details.", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "The execution engine executes the program on the image by assembling a neural module network [2] mirroring the structure of the predicted program.", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "The execution engine is implemented using a neural module network [2]: the program z is used to assemble a question-specific neural network that is composed from a set of modules.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Our modules use a generic architecture, in contrast to [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "Each unary module is a standard residual block [14] with two 3\u00d73 convolutional layers.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "The Scene module takes visual features as input (conv4 features from ResNet-101 [14] pretrained on ImageNet [37]) and passes these features through four", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "The Scene module takes visual features as input (conv4 features from ResNet-101 [14] pretrained on ImageNet [37]) and passes these features through four", "startOffset": 108, "endOffset": 112}, {"referenceID": 45, "context": "CNN+LSTM+SA [46] 68.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "2 Human\u2020 [19] 96.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "Specifically, we can (1) use pairs (q, z) of questions and corresponding programs to train the program generator, which amounts to training a standard sequence-to-sequence model; and (2) use triplets (x, z, a) of the image, program, and answer to train the execution engine, using backpropagation to compute the required gradients (as in [2]).", "startOffset": 338, "endOffset": 341}, {"referenceID": 41, "context": "Instead we replace the argmaxes with sampling and use REINFORCE [42] to estimate gradients on the outputs of the program generator; the reward for each of its outputs is the negative zero-one loss of the execution engine, with a moving-average baseline.", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "We evaluate our model on the recent CLEVR dataset [19].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "[19] tested several VQA models on CLEVR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "LSTM: Similar to [3, 33], questions are processed with learned word embeddings followed by a word-level LSTM [15].", "startOffset": 17, "endOffset": 24}, {"referenceID": 32, "context": "LSTM: Similar to [3, 33], questions are processed with learned word embeddings followed by a word-level LSTM [15].", "startOffset": 17, "endOffset": 24}, {"referenceID": 14, "context": "LSTM: Similar to [3, 33], questions are processed with learned word embeddings followed by a word-level LSTM [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 45, "context": "CNN+LSTM+SA [46]: Questions and images are encoded using a CNN and LSTM as above, then combined using two rounds of soft spatial attention; a linear transform of the attention output predicts the answer.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "The models that are most similar to ours are neural module networks [1, 2].", "startOffset": 68, "endOffset": 74}, {"referenceID": 1, "context": "The models that are most similar to ours are neural module networks [1, 2].", "startOffset": 68, "endOffset": 74}, {"referenceID": 18, "context": "Unfortunately, neural module networks use a hand-engineered, off-the-shelf parser to produce programs, and this parser fails2 on the complex questions in CLEVR [19].", "startOffset": 160, "endOffset": 164}, {"referenceID": 18, "context": "[19] proposed the CLEVR-CoGenT dataset for investigating the ability of VQA models to perform compositional generalization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] found that VQA models trained on data from Condition A performed poorly on data from Condition B, suggesting the models are not well capable of generalizing to new conditions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "ods in Condition B, it still appears to suffer from the problems identified by [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Inspired by VQA [3], workers on Amazon Mechanical Turk were asked to write questions about CLEVR images that would be hard for a smart robot to answer; workers were primed with questions from CLEVR and restricted to answers in CLEVR.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "This paper fits into a long line of work on incorporating symbolic representations into (neural) machine learning models [4, 5, 29, 36].", "startOffset": 121, "endOffset": 135}, {"referenceID": 4, "context": "This paper fits into a long line of work on incorporating symbolic representations into (neural) machine learning models [4, 5, 29, 36].", "startOffset": 121, "endOffset": 135}, {"referenceID": 28, "context": "This paper fits into a long line of work on incorporating symbolic representations into (neural) machine learning models [4, 5, 29, 36].", "startOffset": 121, "endOffset": 135}, {"referenceID": 35, "context": "This paper fits into a long line of work on incorporating symbolic representations into (neural) machine learning models [4, 5, 29, 36].", "startOffset": 121, "endOffset": 135}, {"referenceID": 0, "context": "Our generic program representation, learnable program generator and universal design for modules makes our model much more flexible than neural module networks [1, 2] and thus more easily extensible to new problems and domains.", "startOffset": 160, "endOffset": 166}, {"referenceID": 1, "context": "Our generic program representation, learnable program generator and universal design for modules makes our model much more flexible than neural module networks [1, 2] and thus more easily extensible to new problems and domains.", "startOffset": 160, "endOffset": 166}, {"referenceID": 38, "context": "In all experiments our program generator is an LSTM sequence-to-sequence model [39].", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "During supervised training of the program generator, we use Adam [23] with a learning rate of 5\u00d7 10\u22124 and a batch size of 64; we train for a maximum of 32,000 iterations, employing early stopping based on validation set accuracy.", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "The execution engine uses a Neural Module Network [2] to compile a custom neural network architecture based on the predicted program from the program generator.", "startOffset": 50, "endOffset": 53}, {"referenceID": 18, "context": "For ground-truth programs, the root of the tree is a function corresponding to one of the question types from the CLEVR dataset [19], such as count or query shape.", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "For predicted programs the root of the program tree could in principle be any function, but in practice we find that trained models tend only to Layer Output size Input image 3\u00d7 224\u00d7 224 ResNet-101 [14] conv4 6 1024\u00d7 14\u00d7 14 Conv(3\u00d7 3, 1024\u2192 128) 128\u00d7 14\u00d7 14 ReLU 128\u00d7 14\u00d7 14 Conv(3\u00d7 3, 128\u2192 128) 128\u00d7 14\u00d7 14 ReLU 128\u00d7 14\u00d7 14", "startOffset": 198, "endOffset": 202}, {"referenceID": 36, "context": "The ResNet-101 model is pretrained on ImageNet [37] and remains fixed while the execution engine is trained.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Previous implementations of Neural Module networks [1, 2] used different architectures for each module type, customizing the module architecture to the function the module was to perform.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "Previous implementations of Neural Module networks [1, 2] used different architectures for each module type, customizing the module architecture to the function the module was to perform.", "startOffset": 51, "endOffset": 57}, {"referenceID": 13, "context": "In contrast we use a generic design for our modules: each module is a small residual block [14]; the exact architectures used for our unary and binary modules are shown in Tables 5 and 6 respectively.", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "In initial experiments we used Batch Normalization [17] after each convolution in the modules, but we found that this prevented the model from converging.", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "When training the execution engine alone (using either ground-truth programs or predicted programs from a fixed program generator), we train using Adam [23] with a learning rate of 1 \u00d7 10\u22124 and a batch size of 64; we train for a maximum of 200,000 iterations and employ early stopping based on validation set accuracy.", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "We reimplement the baselines used in [19]: LSTM.", "startOffset": 37, "endOffset": 41}, {"referenceID": 45, "context": "However rather than concatenating these representations, they are fed to two consecutive Stacked Attention layers [46] with a hidden dimension of 512 units; this results in a 512-dimensional vector which is fed to a linear layer to predict answer scores.", "startOffset": 114, "endOffset": 118}, {"referenceID": 45, "context": "[46]; this also matches the CNN+LSTM+SA model used in [19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[46]; this also matches the CNN+LSTM+SA model used in [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Unfortunately we found that the parser used in [1] for VQA questions did not perform well on the longer questions in CLEVR.", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "[1] for parsing questions from the VQA dataset [3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[1] for parsing questions from the VQA dataset [3].", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "Each parse gives a set of layout fragments separated by semicolons; in [1] these fragments are combined to produce candidate layouts for the module network.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "ments computed using the parser from [1].", "startOffset": 37, "endOffset": 40}], "year": 2017, "abstractText": "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.", "creator": "LaTeX with hyperref package"}}}