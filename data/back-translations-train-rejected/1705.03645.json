{"id": "1705.03645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "A Survey of Deep Learning Methods for Relation Extraction", "abstract": "Relation Extraction is an important sub-task of Information Extraction which has the potential of employing deep learning (DL) models with the creation of large datasets using distant supervision. In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead.", "histories": [["v1", "Wed, 10 May 2017 08:05:44 GMT  (544kb,D)", "http://arxiv.org/abs/1705.03645v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shantanu kumar"], "accepted": false, "id": "1705.03645"}, "pdf": {"name": "1705.03645.pdf", "metadata": {"source": "CRF", "title": "A Survey of Deep Learning Methods for Relation Extraction", "authors": ["Shantanu Kumar"], "emails": ["ee1130798@iitd.ac.in"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to move, to move, to move and to move."}, {"heading": "2 Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Supervised Training", "text": "Early work on relationship extraction using deep learning employed monitored training datasets Xiv: 170 5.03 645v 1 [cs.C L] May 10, 201 7, previously used by non-deep learning models. These datasets required intense human annotation, which meant that the data contained high-quality tuples with little to no noise. However, human annotations can be time consuming, as a result of which these datasets were generally small. Both of the datasets mentioned below contain data samples in which the document set is already labeled with designated units of interest and the relationship class expressed between the entity pair is predictable. ACE 2005 dataset The automatic content extraction dataset contains 599 documents related to messages and e-mail and contains relationships that are divided into 7 main types, of which 6 important relationship types include enough instances (an average of 700 instances per relationship type) and this data set will be used for training and Task-8 dataseval in 2010.This dataset type is a Task-8."}, {"heading": "2.2 Distant Supervision", "text": "In order to avoid the tedious task of manually building relationship extraction records, Mintz et al. (2009) proposed a remote supervision approach to automatically generate large amounts of training data, agreeing documents with known KBs, using the assumption that if there is a relationship between an entity pair in the KB, then any document containing mention of the entity pair would express that relationship. It is easy to realize that this remote supervision assumption is a very strong assumption and that any document containing the entity pair may not express the relationship between the pair, e.g. for the tuple (Microsoft, founder of Microsoft) in the database and the document \"Bill Gatess turn to philanthropy was linked to the antitrust problems Microsoft in the US and the European Union,\" the document does not express the relationship between the entity pair even though it contains both entities."}, {"heading": "3 Basic Concepts", "text": "The following section discusses some of the basic concepts used in most deep learning models for relationship extraction."}, {"heading": "3.1 Word Embeddings", "text": "Word embedding (Mikolov et al., 2013; Pennington et al., 2014) is a form of word distribution in a vocabulary in which each word is expressed as a vector in a low dimensional space (low relative to the size of the vocabulary). Word embedding aims to capture the syntactic and semantic information about the word. It is learned using unattended methods via large unlabeled text corpora. It is implemented using an embedding matrix E-R | V | \u00b7 dw, where dw is the dimensionality of the embedding space and | V | the size of the vocabulary."}, {"heading": "3.2 Positional Embeddings", "text": "In the task of relation extraction, input to the model using word embedding (as introduced by Zeng et al. (2014)) also usually encodes the relative distance of each word to the units in the sentence. The idea is that words closer to the target units usually contain more useful information about the relationship class. Position embedding consists of the relative distance of the current word to the units. In the sentence \"Bill Gates is the founder of Microsoft.\" For example, the relative distance of the word \"founder\" to the unit \"Bill Gates\" is 3 and the distance \"Microsoft\" -2. The distance is then encoded in a dp-dimensional embedding. Finally, the total sentence x can be expressed as a sequence of vectors x = {w1, w2,..., wm} in which each word is encoded with Rd and d = dw + dp \u00b7."}, {"heading": "3.3 Convolutional Neural Networks", "text": "To further encode the sentences, deep learning models for relation extraction usually use Convolutionary Neural Network Layers to capture features of the ngram plane, similar to Collobert et al. (2011). The Convolutionary Layer works as follows: If an input set x is formed as a sequence of vectors x = {w1, w2,..., wm}, wi-Rd, if l is the window size for the Convolutionary Layer Core, then the vector for the i-th window (qi-R (d-l)) is formed by concatenating the input vectors for this window, qi = wi: i + l \u2212 1; (1 \u2264 i-m \u2212 l + 1) (1) A single Convolutionary Core would then consist of a weight vector W-R (d \u00b7 l) and a preload b-R, and the output for the i-th window is calculated as pi = f (W-qi + b) (2), where f is the activation function."}, {"heading": "4 Supervised learning with CNNs", "text": "The early work that used deep learning for relationship extraction functioned in the supervised training paradigm with the above-mentioned hand-annotated corpus. This model attempted to assign a relationship class label to each sentence that contained a mention of the focused entity pair by modelling the problem as a multi-class classification problem."}, {"heading": "4.1 Simple CNN model (Liu et al., 2013)", "text": "This work is perhaps the earliest to attempt to use a CNN to automatically learn features instead of manual craft features. It builds an end-to-end network that first encodes the input sentence using word vectors and lexical features followed by a revolutionary core layer, a single layer of neural networks, and a Softmax output layer to give a probability distribution across all relationship classes. The model uses synonym vectors instead of word vectors by assigning a single vector to each synonym class instead of giving each word a vector. However, it fails to exploit the real representation power of word embeddings. Embeddings are not trained in an unattended manner on the corpus, but randomly assigned to each synonym class. In addition, the model also attempts to integrate some lexical features with word lists, POS lists, and entity type lists of the kernel state that the model matches."}, {"heading": "4.3 CNN with multi-sized window kernels (Nguyen and Grishman, 2015)", "text": "This work was one of the last in the supervised area of relation extraction, building on the work of Liu et al. (2013) and Zeng et al. (2014). The model completely refrains from external lexical features in order to enrich the representation of the input sentence, and lets CNN learn the required characteristics itself. Their architecture is similar to that of Zeng et al. (2014), consisting of word and position embedding followed by folding and max pooling. In addition, they also integrate winding grains of different window sizes to capture wider ranges of n-gram characteristics. By experimenting with different iterations, they have found that the use of grains with 2-3-4-5 window lengths provides them with the best performance. The authors also initialize the word through pre-trained word embedding exercised with word2vec (Mikolov et al., 2013), giving them a significant boost over random vectors and static 2vectors."}, {"heading": "5 Multi-instance learning models with distant supervision", "text": "As previously mentioned, Riedel et al. (2010) relaxed the remote supervision assumption by modeling the task as a multi-level learning problem so that they could take advantage of the large training data generated by remote supervision while being robust to the noise in the labels. Multi-instancelearning is a form of supervised learning in which a label is given to a bag of instances rather than to a single instance. In the context of RE, each entity pair defines a bag and the bag consists of all sentences that contain the mention of the entity pair. Instead of giving each sentence a class label, each bag of the relation unit is labeled instead. Riedel et al. (2010) model this based on the assumption that \"if there is a relationship between an entity pair, then at least one document in the bag for the entity pair must reflect that relationship.\""}, {"heading": "5.1 Piecewise Convolutional Neural Networks (Zeng et al., 2015)", "text": "The PCNNs model uses the multi-instance learning paradigm, using a neural network model, to build a relation extractor with remote monitoring data = 2016. The neural network architecture is similar to the models of (Zeng et al., 2014) and (Nguyen and Grishman, 2015) previously discussed, with an important contribution to the number of pieces of max-pooling within the set. The authors claim that the max-pooling layer drastically reduces the size of the hidden layer and is also insufficient to capture the structure between the entities in the set. This can be avoided by maximizing the merging into different segments of the set, rather than the whole set. It is claimed that each set can of course be divided into 3 segments based on the positions of the 2 entities in focus. By getting a piecemeal max pool within each of the segments, we get a richer representation, while maintaining a vector independent of the input."}, {"heading": "6 Results", "text": "Figure 3 summarizes the results of the various multi-level learning models applied to the remote supervision data set created by Riedel et al. (2010). It shows the results for 3 non-deep learning models, namely Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011) and MIML (Surdeanu et al., 2012). We also see the performance of the deep learning models discussed in the previous sections. It is observed that all deep learning models perform significantly better than the non-deep learning models. The use of the Multiinstance Multilabel (MIMLCNN) mechanism with the CNN model further improves the curve over the PCNN model. However, since the selective attention mechanism applied to the PCNN model provides the best performance of all models, it is interesting to note the increase in performance in the PCNN curve compared to the PCNN + Att curve. Since the attention mechanism is a crossover mechanism to the MIMLM curve, it is more effective than the IMLM curve to work with the IMLIMLM."}, {"heading": "7 Concluding Remarks", "text": "With the introduction of remote monitoring for the extraction of relationships by Mintz et al. (2009), modeling of the task as a multi-instance problem was widely accepted, and the use of this mechanism also provides enough data for in-depth learning models that can be trained in multi-instance settings that take into account the labeling noise in the data. Subsequent work has attempted to manage the interaction between relationships by exploiting relationship pathways (Zeng et al., 2016) and relationship class bonds (Ye et al., 2016) to further improve performance. Relationships such as Father of and Mother of can also be exploited to make instances for spouse of. However, these improvements only work at the level of NC formation and inference methods of the model. As far as the aspect of deep learning is concerned, the architecture of CNN or PCNN that is used to code sentences to extract instances for spouse of is."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives."], "venue": "The semantic web, Springer, pages 722\u2013735.", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "SIGMOD \u201908. AcM, pages 1247\u20131250.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Language modeling with gated convolutional networks", "author": ["Yann N Dauphin", "Angela Fan", "Michael Auli", "David Grangier."], "venue": "arXiv preprint arXiv:1612.08083 .", "citeRegEx": "Dauphin et al\\.,? 2016", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Knowledgebased weak supervision for information extraction", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": null, "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Relation extraction with multi-instance multilabel convolutional neural networks", "author": ["Xiaotian Jiang", "Quan Wang", "Peng Li", "Bin Wang."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Pa-", "citeRegEx": "Jiang et al\\.,? 2016", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun."], "venue": "Proceedings of ACL. volume 1, pages 2124\u20132133.", "citeRegEx": "Lin et al\\.,? 2016", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Convolution neural network for relation extraction", "author": ["ChunYang Liu", "WenBo Sun", "WenHan Chao", "Wanxiang Che."], "venue": "International Conference on Advanced Data Mining and Applications. Springer, pages 231\u2013242.", "citeRegEx": "Liu et al\\.,? 2013", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Relation extraction: Perspective from convolutional neural networks", "author": ["Thien Huu Nguyen", "Ralph Grishman."], "venue": "Proceedings of NAACL-HLT . pages 39\u201348.", "citeRegEx": "Nguyen and Grishman.,? 2015", "shortCiteRegEx": "Nguyen and Grishman.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 148\u2013163.", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning."], "venue": "Proceedings of the 2012 joint conference on empirical methods in natural language processing and com-", "citeRegEx": "Surdeanu et al\\.,? 2012", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "EMNLP. pages 1422\u2013 1432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Jointly extracting relations with class ties via effective deep ranking", "author": ["Hai Ye", "Wenhan Chao", "Zhunchen Luo."], "venue": "arXiv preprint arXiv:1612.07602 .", "citeRegEx": "Ye et al\\.,? 2016", "shortCiteRegEx": "Ye et al\\.", "year": 2016}, {"title": "Comparative study of cnn and rnn for natural language processing", "author": ["Wenpeng Yin", "Katharina Kann", "Mo Yu", "Hinrich Sch\u00fctze."], "venue": "arXiv preprint arXiv:1702.01923 .", "citeRegEx": "Yin et al\\.,? 2017", "shortCiteRegEx": "Yin et al\\.", "year": 2017}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao."], "venue": "EMNLP. pages 1753\u20131762.", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In COLING", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Incorporating relation paths in neural relation extraction", "author": ["Wenyuan Zeng", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun."], "venue": "arXiv preprint arXiv:1609.07479 .", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "like Freebase (Bollacker et al., 2008) and DBpedia (Auer et al.", "startOffset": 14, "endOffset": 38}, {"referenceID": 0, "context": ", 2008) and DBpedia (Auer et al., 2007) which are a source for useful information are far from complete and can be extended using such systems.", "startOffset": 20, "endOffset": 39}, {"referenceID": 10, "context": "Mintz et al. (2009) proposed a distant supervision method for producing large amount of training data by aligning KB facts with texts.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "SemEval-2010 Task 8 dataset This dataset is a freely available dataset by Hendrickx et al. (2009) which contains 10,717 samples which are divided as 8,000 for training and 2,717 for testing.", "startOffset": 74, "endOffset": 98}, {"referenceID": 10, "context": "To avoid the laborious task of manually building datasets for relation extraction, Mintz et al. (2009) proposed a distant supervision approach for automatically generating large amounts of training data.", "startOffset": 83, "endOffset": 103}, {"referenceID": 13, "context": "To alleviate this problem and reduce the noise, Riedel et al. (2010) relaxed the distant supervision", "startOffset": 48, "endOffset": 69}, {"referenceID": 9, "context": "Word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are a form of distributional representations for the words in a vocabulary, where each word is expressed as a vector in a low dimensional space (low w.", "startOffset": 16, "endOffset": 63}, {"referenceID": 12, "context": "Word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are a form of distributional representations for the words in a vocabulary, where each word is expressed as a vector in a low dimensional space (low w.", "startOffset": 16, "endOffset": 63}, {"referenceID": 18, "context": "the entities in the sentence, with the help of positional embeddings (as introduced by Zeng et al. (2014)).", "startOffset": 87, "endOffset": 106}, {"referenceID": 7, "context": "(Sourced from (Lin et al., 2016))", "startOffset": 14, "endOffset": 32}, {"referenceID": 8, "context": "1 Simple CNN model (Liu et al., 2013)", "startOffset": 19, "endOffset": 37}, {"referenceID": 18, "context": "PCNN Zeng et al. (2015) Yes (1 sentence per bag) Word2Vec Yes No Yes (Piecewise in a sentence)", "startOffset": 5, "endOffset": 24}, {"referenceID": 7, "context": "PCNN + Att Lin et al. (2016) Yes (Attention weighted sum over bag) Word2Vec Yes No Yes (Piecewise and Full)", "startOffset": 11, "endOffset": 29}, {"referenceID": 19, "context": "2 CNN model with max-pooling (Zeng et al., 2014)", "startOffset": 29, "endOffset": 48}, {"referenceID": 11, "context": "3 CNN with multi-sized window kernels (Nguyen and Grishman, 2015)", "startOffset": 38, "endOffset": 65}, {"referenceID": 8, "context": "vised domain for relation extraction which built upon the works of Liu et al. (2013) and Zeng et al.", "startOffset": 67, "endOffset": 85}, {"referenceID": 8, "context": "vised domain for relation extraction which built upon the works of Liu et al. (2013) and Zeng et al. (2014). The model completely gets rid of exterior lexical features to enrich the representation of the input sentence and lets the CNN learn the required features itself.", "startOffset": 67, "endOffset": 108}, {"referenceID": 9, "context": "The authors also initialize the word embedding matrix using pre-trained word embeddings trained with word2vec (Mikolov et al., 2013), which gives them a significant boost over random vectors and static-word2vec vectors.", "startOffset": 110, "endOffset": 132}, {"referenceID": 17, "context": "similar to Zeng et al. (2014) consisting of word and positional embeddings followed by convolution and max-pooling.", "startOffset": 11, "endOffset": 30}, {"referenceID": 13, "context": "As mentioned previously, Riedel et al. (2010) relaxed the distant supervision assumption by modeling the task as a multi-instance learning problem, so that they could exploit the large training data created by distant supervision while being robust to the noise in the labels.", "startOffset": 25, "endOffset": 46}, {"referenceID": 13, "context": "Riedel et al. (2010) model this using the assumption that \u201dif a relation exists between an entity pair, then at least one document in the bag for the entity pair must reflect that relation\u201d.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "1 Piecewise Convolutional Neural Networks (Zeng et al., 2015)", "startOffset": 42, "endOffset": 61}, {"referenceID": 19, "context": "The neural network architecture is similar to the models by (Zeng et al., 2014) and (Nguyen and Grishman, 2015) discussed previously, with one", "startOffset": 60, "endOffset": 79}, {"referenceID": 11, "context": ", 2014) and (Nguyen and Grishman, 2015) discussed previously, with one", "startOffset": 12, "endOffset": 39}, {"referenceID": 9, "context": "deep learning models like the distant-supervision based model by Mintz et al. (2009), the multi instance learning method MultiR proposed by Hoffmann et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 5, "context": "(2009), the multi instance learning method MultiR proposed by Hoffmann et al. (2011) and the multi-instance multilabel model MIML by Surdeanu et al.", "startOffset": 62, "endOffset": 85}, {"referenceID": 5, "context": "(2009), the multi instance learning method MultiR proposed by Hoffmann et al. (2011) and the multi-instance multilabel model MIML by Surdeanu et al. (2012), on", "startOffset": 62, "endOffset": 156}, {"referenceID": 13, "context": "the dataset by Riedel et al. (2010) (Figure 3).", "startOffset": 15, "endOffset": 36}, {"referenceID": 18, "context": "(Sourced from (Zeng et al., 2015))", "startOffset": 14, "endOffset": 33}, {"referenceID": 7, "context": "2 Selective Attention over Instances (Lin et al., 2016)", "startOffset": 37, "endOffset": 55}, {"referenceID": 7, "context": "from the bag, Lin et al. (2016) used an attention", "startOffset": 14, "endOffset": 32}, {"referenceID": 6, "context": "3 Multi-instance Multi-label CNNs (Jiang et al., 2016)", "startOffset": 34, "endOffset": 54}, {"referenceID": 18, "context": "The authors of this paper address the information loss problem in Zeng et al. (2015) by using a crossdocument max-pooling layer.", "startOffset": 66, "endOffset": 85}, {"referenceID": 10, "context": "It shows the results for 3 non deep learning models namely Mintz (Mintz et al., 2009), MultiR (Hoffmann et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 5, "context": ", 2009), MultiR (Hoffmann et al., 2011) and MIML (Surdeanu et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 14, "context": ", 2011) and MIML (Surdeanu et al., 2012).", "startOffset": 17, "endOffset": 40}, {"referenceID": 11, "context": "Figure 3 summarizes the results of the various multi-instance learning models applied on the distant supervision dataset created by Riedel et al. (2010). It shows the results for 3 non deep learning models namely Mintz (Mintz et al.", "startOffset": 132, "endOffset": 153}, {"referenceID": 7, "context": "(Sourced from (Lin et al., 2016) and (Jiang et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 6, "context": ", 2016) and (Jiang et al., 2016))", "startOffset": 12, "endOffset": 32}, {"referenceID": 20, "context": "Some very recent works in the field also try to exploit the interaction between the relations by exploiting relation paths (Zeng et al., 2016) and relation class ties", "startOffset": 123, "endOffset": 142}, {"referenceID": 10, "context": "With the introduction of distant supervision for relation extraction by Mintz et al. (2009), modeling the task as Multi-instance problem has been widely adopted.", "startOffset": 72, "endOffset": 92}, {"referenceID": 16, "context": "(Ye et al., 2016) to improve the performance further.", "startOffset": 0, "endOffset": 17}, {"referenceID": 17, "context": "Even though NLP literature does not support a clear distinction between the domains where CNNs or RNNs perform better, recent works have shown that each provide complementary information for text classification tasks (Yin et al., 2017).", "startOffset": 217, "endOffset": 235}, {"referenceID": 15, "context": "Where RNNs perform well on document-level sentiment classification (Tang et al., 2015), some works have shown", "startOffset": 67, "endOffset": 86}, {"referenceID": 3, "context": "CNNs to outperform LSTMs on language modeling tasks (Dauphin et al., 2016).", "startOffset": 52, "endOffset": 74}], "year": 2017, "abstractText": "Relation Extraction is an important subtask of Information Extraction which has the potential of employing deep learning (DL) models with the creation of large datasets using distant supervision. In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead.", "creator": "LaTeX with hyperref package"}}}