{"id": "1705.03670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Deep Speaker Feature Learning for Text-independent Speaker Verification", "abstract": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional time-delay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce high-quality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames.", "histories": [["v1", "Wed, 10 May 2017 09:30:42 GMT  (977kb,D)", "http://arxiv.org/abs/1705.03670v1", "deep neural networks, speaker verification, speaker feature"]], "COMMENTS": "deep neural networks, speaker verification, speaker feature", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["lantian li", "yixiang chen", "ying shi", "zhiyuan tang", "dong wang"], "accepted": false, "id": "1705.03670"}, "pdf": {"name": "1705.03670.pdf", "metadata": {"source": "CRF", "title": "Deep Speaker Feature Learning for Text-independent Speaker Verification", "authors": ["Lantian Li", "Yixiang Chen", "Ying Shi", "Zhiyuan Tang", "Dong Wang"], "emails": ["lilt13@mails.tsinghua.edu.cn,", "wangdong99@mails.tsinghua.edu.cn,", "chenxy@cslt.riit.tsinghua.edu.cn", "shiying@cslt.riit.tsinghua.edu.cn", "tangzy@cslt.riit.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "3. CT-DNN for feature learning", "text": "This structure is an extension of the model proposed in [10], using revolutionary layers to extract local discriminatory patterns from the temporal frequency space, and time-delayed layers to increase the effective time context for each frame. We call this structure CT-DNN.Figure 1 illustrates the CT-DNN structure used in this work. It consists of a Convolutionary (CN) component and a Time-Delayed (TD) component connected by a bottleneck layer, consisting of 512 hidden units. The Convolutionary Component comprises two CN layers, each followed by a maximum pooling. This component is used to learn local patterns useful in displaying speaker characteristics. The TD component comprises two TD layers, each followed by a P-standard layer. This component is used to expand the temporal context."}, {"heading": "4. Experiments", "text": "In this section we first present the database used in the experiments and then report on the results with the i-vector and dvector systems. All experiments were performed with the Kaldi toolkit [18]."}, {"heading": "4.1. Database", "text": "The training data and test data are presented as follows. \u2022 Training Set: It consists of 2, 500 male and 2, 500 female speakers, with 95, 167 expressions randomly selected from the Fisher database, and each speaker has approximately 120 seconds of speech segments. \u2022 Evaluation Set: It consists of 500 male and 500 female speakers randomly selected from the Fisher database. There is no overlap between the speakers of the training set and the evaluation set. For each speaker, 10 expressions are used for enrollment and the rest for the test.The test was conducted under 4 conditions, each with a different setting in the length of enrollment and test expressions. Terms are shown in Table 1. All test conditions include pooled male and gender test results, so we presented the same trend results."}, {"heading": "4.2. Model settings", "text": "We built an i-vector system as a baseline. The raw feature includes 19-dimensional MFCCs plus the log energy. This raw feature is complemented by its derivatives of first and second order, resulting in a 60-dimensional feature vector. This MFCC feature was used by the i-vector model. UBM consisted of 2 048 Gaussian components, and the dimensionality of the i-vector space was 400. The dimensionality of the LDA projection room was set to 150. The entire system was reciprocated with the Kaldi SRE08. In the d-vector system, the architecture was based on Figure 1. The input feature was 40-dimensional fbanks with a symmetrical 4-frame window for splitting the adjacent frames, resulting in a total of 9 frames. The number of output units was 5 000, corresponding to the number of speakers in the training data. The loudspeakers were extracted from the last layer of the sample results (the results of the 1)."}, {"heading": "4.3. Main results", "text": "The results of the i-vector system and d-vector system in terms of equal error rate (EER%) are reproduced in Table 2. 1http: / / data.cslt.orgtwo systems have been trained with the entire training set, and the results are reported for different conditions.It can be observed that improving the length of test expressions for both systems always improves performance. However, it seems that the performance improvement for the iVector system is more significant than the d-vector system. This is understandable, since the i-vector system is based on the statistical pattern of characteristics for the construction of speaker vectors, so that more speaker vectors will help. In contrast, the d-vector system uses a simple average of characteristics to represent a speaker, so that the contribution of more speaker frames is marginal. The most interesting observation is the clear advantage of the d-vector system in the C (30-3) condition, which are briefly outlined in statements."}, {"heading": "4.4. Training data size", "text": "The results of the best i-vector system (i-vector + PLDA) and the best d-vector system (d-vector + LDA) are reported in each of the four test conditions.The results are shown in Figure 2, where each image represents a specific test condition.It turns out that in all test conditions, for both the i-vector and d-vector systems, better performance is achieved with more training data, but the i-vector system seems to benefit more from big data.This differs somewhat from our experience that deep neural models require more data than probable models. It also differs from the observation in [14], where the d-vector system achieves more performance improvements than the i-vector system when the number of speakers became very large (102k)."}, {"heading": "4.5. Feature discrimination", "text": "To test the quality of the learned speaker function, we use tSNE [19] to take some feature samples from 20 loudspeakers. Samples are selected in two ways: (a) randomly from all the speaker's speech functions; (b) select a specific utterance; the results are presented in Figure 3. You can see that the learned characteristics for loudspeakers are very discriminatory, but there are still some variations caused by the linguistic content, as in the plot (b).A more quantitative test for the quality of features is to examine the extreme case where the test language has few images. Let's start with 20 images, which is actually the effective context size of the CT-DNN, so that only a single feature is produced. Table 3 and Table 4 show the results where the length of the speech functions is only a few frames. Let's start with 20 images, what the effective context size of the CT-DNN is created so that only a single feature is a CDNT-N."}, {"heading": "5. Conclusions", "text": "This paper presented a CT-DNN model to learn the sensitive characteristics of the speaker. Our experiments showed that the learned characteristic is highly discriminatory and can be used to achieve impressive performance when the test messages are short. [1] This result has far-reaching implications for research and technology: on the one hand, it means that the speaker characteristics are a kind of short-term pattern and should therefore be extracted through short-term analysis (including neuronal learning processes) rather than long-term likely modeling. [2] Our result suggests that the feature learning approach could / should be used when the test messages are short, a condition that many practical applications are interested in the work, for example, how the function should be modeled in a simple way when PLDA does not work."}], "references": [{"title": "Speaker verification using adapted gaussian mixture models", "author": ["D. Reynolds", "T. Quatieri", "R. Dunn"], "venue": "Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Support vector machines using gmm supervectors for speaker verification", "author": ["W. Campbell", "D. Sturim", "D. Reynolds"], "venue": "Signal Processing Letters, IEEE, vol. 13, no. 5, pp. 308\u2013311, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic linear discriminant analysis", "author": ["S. Ioffe"], "venue": "Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Odyssey, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1695\u2013 1699.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of text-independent speaker recognition: From features to supervectors", "author": ["T. Kinnunen", "H. Li"], "venue": "Speech communication, vol. 52, no. 1, pp. 12\u201340, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "M. Erik", "L.M. Ignacio", "G.-D. Javier"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, vol. 28, no. 4, 2014, pp. 357\u2013366.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "N. Shazeer"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5115\u20135119.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end attention based text-dependent speaker verification", "author": ["S.-X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "arXiv preprint arXiv:1701.00562, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep feature for text-dependent speaker verification", "author": ["Y. Liu", "Y. Qian", "N. Chen", "T. Fu", "Y. Zhang", "K. Yu"], "venue": "Speech Communication, vol. 73, pp. 1\u201313, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network-based speaker embeddings for end-to-end speaker verification", "author": ["D. Snyder", "P. Ghahremani", "D. Povey", "D. Garcia-Romero", "Y. Carmiel", "S. Khudanpur"], "venue": "SLT\u20192016, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Speaker recognition system based on deep neural networks and bottleneck features", "author": ["T. Yao", "C. Meng", "H. Liang", "L. Jia"], "venue": "Journal of Tsinghua University (Science and Technology), vol. 56, no. 11, pp. 1143\u20131148, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved deep speaker feature learning for text-dependent speaker recognition", "author": ["L. Li", "Y. Lin", "Z. Zhang", "D. Wang"], "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2015 Asia-Pacific. IEEE, 2015, pp. 426\u2013 429.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Machine Learning Research, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "For example, the famous Gaussian mixture modeluniversal background model (GMM-UBM) framework [1] and the subsequent subspace models, including the joint factor analysis approach [2] and the i-vector model [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "For example, the famous Gaussian mixture modeluniversal background model (GMM-UBM) framework [1] and the subsequent subspace models, including the joint factor analysis approach [2] and the i-vector model [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "For example, the famous Gaussian mixture modeluniversal background model (GMM-UBM) framework [1] and the subsequent subspace models, including the joint factor analysis approach [2] and the i-vector model [3].", "startOffset": 205, "endOffset": 208}, {"referenceID": 3, "context": ", the SVM model for the GMM-UBM approach [4] and the PLDA model for the i-vector approach[5].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": ", the SVM model for the GMM-UBM approach [4] and the PLDA model for the i-vector approach[5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "For example, the DNN-based i-vector method [6, 7].", "startOffset": 43, "endOffset": 49}, {"referenceID": 6, "context": "For example, the DNN-based i-vector method [6, 7].", "startOffset": 43, "endOffset": 49}, {"referenceID": 7, "context": "Driven by these motivations, many researchers put their effort in \u2018feature engineering\u2019 in the past several decades, and new features were proposed occasionally, from perspectives of different knowledge domains [8].", "startOffset": 211, "endOffset": 214}, {"referenceID": 8, "context": "This feature learning has been demonstrated to be very successful in ASR, where the learned features have shown to be highly representative for linguistic content and very robust against variations of other factors [9].", "startOffset": 215, "endOffset": 218}, {"referenceID": 9, "context": "on a text-dependent task [10].", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "[11] used an LSTMRNN to learn utterance-level representations directly and reported better performance than the i-vector system on the same text-dependent task when a large database was used (more than 4, 000 speakers).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] utilized convolutional neural networks (CNN) to learn speaker features and an attentionbased model to learn how to make decisions, again on a textdependent task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] used the DNN-learned features to build the conventional i-vector system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] migrated the DNN-based approach to text-independent tasks, and reported better performance than the i-vector system when the training data is sufficiently large (102k speakers).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Our work is a direct extension of the d-vector model presented by Ehsan et al [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "For example, the RNN-based utterancelevel representation learning [11] is attractive, but the RNN pooling shifts the focus to the entire sentence, rather than framelevel feature learning.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "The end-to-end neural models proposed by Snyder [14] and Zhang [12] both involve a back-end classifier, which weakens the feature learning component: it is unknown whether the speaker-discriminant information is learned by the classifier or by the feature extractor.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "The end-to-end neural models proposed by Snyder [14] and Zhang [12] both involve a back-end classifier, which weakens the feature learning component: it is unknown whether the speaker-discriminant information is learned by the classifier or by the feature extractor.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "[13] used an ASR-ASV multi-task DNN to produce frame-level features and substituted them for MFCC to construct GMM-UBM and i-vector systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a similar approach, though they used ASR-oriented features to train GMMs for splitting the acoustic space, and the original ASV-oriented features as the acoustic feature to construct the i-vector model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "This structure is an extension to the model proposed in [10], by using convolutional layers to extract local discriminative patterns from the temporal-frequency space, and time-delayed layers to increase the effective temporal context for each frame.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "In our study, the natural stochastic gradient descent (NSGD) [16] algorithm was employed to conduct the optimization.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "As in [10], the utterance-level representation of a speech segment can be simply derived by averaging the speaker features of all the frames of the speech segment.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "Following the name convention of the previous work [10, 17], the utterance-level representations derived from the CTDNN are called d-vectors.", "startOffset": 51, "endOffset": 59}, {"referenceID": 16, "context": "Following the name convention of the previous work [10, 17], the utterance-level representations derived from the CTDNN are called d-vectors.", "startOffset": 51, "endOffset": 59}, {"referenceID": 17, "context": "All the experiments were conducted with the Kaldi toolkit [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "[14], and our results give a more clear evidence to this trend.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The failure of PLDA with d-vectors is also a known issue in our previous work [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "It is also different from the observation in [14], where the d-vector system obtained more performance improvement than the i-vector system when the number of speakers got very large (102k).", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "To check the quality of the learned speaker feature, we use tSNE [19] to draw some feature samples from 20 speakers.", "startOffset": 65, "endOffset": 69}], "year": 2017, "abstractText": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional timedelay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce highquality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames.", "creator": "LaTeX with hyperref package"}}}