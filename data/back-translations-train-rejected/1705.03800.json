{"id": "1705.03800", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Hybrid Isolation Forest - Application to Intrusion Detection", "abstract": "From the identification of a drawback in the Isolation Forest (IF) algorithm that limits its use in the scope of anomaly detection, we propose two extensions that allow to firstly overcome the previously mention limitation and secondly to provide it with some supervised learning capability. The resulting Hybrid Isolation Forest (HIF) that we propose is first evaluated on a synthetic dataset to analyze the effect of the new meta-parameters that are introduced and verify that the addressed limitation of the IF algorithm is effectively overcame. We hen compare the two algorithms on the ISCX benchmark dataset, in the context of a network intrusion detection application. Our experiments show that HIF outperforms IF, but also challenges the 1-class and 2-classes SVM baselines with computational efficiency.", "histories": [["v1", "Wed, 10 May 2017 14:42:30 GMT  (1725kb,D)", "http://arxiv.org/abs/1705.03800v1", "24 pages, working paper"]], "COMMENTS": "24 pages, working paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pierre-fran\\c{c}ois marteau", "saeid soheily-khah", "nicolas b\\'echet"], "accepted": false, "id": "1705.03800"}, "pdf": {"name": "1705.03800.pdf", "metadata": {"source": "META", "title": "Hybrid Isolation Forest - Application to Intrusion Detection", "authors": ["PIERRE-FRAN\u00c7COIS MARTEAU"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "Hybrid Isolation Forest - Application to Intrusion DetectionPIERRE-FRANCCOIS MARTEAU, IRISA, CNRS, Universite Bretagne SudSAEID SOHEILY-KHAH, IRISA, CNRS, Universite Bretagne SudNICOLAS B\u00c9CHET, IRISA, CNRS, Universite Bretagne SudFrom the identification of a deficiency in the Isolation Forest (IF) algorithm that limits its use in the field of anomaly detection, we propose two extensions that will allow, firstly, to overcome the aforementioned limitation and, secondly, to provide it with some supervised learning capacities.The resulting Hybrid Isolation Forest (HIF) that we propose is evaluated first on the basis of a synthetic dataset to analyze the effect of the new meta parameters and, secondly, to verify whether the addressed limitation of the Isolation of the IF algorithm is effective."}, {"heading": "1 INTRODUCTION", "text": "In fact, we are in a position to be in a situation in which we are able to manoeuvre ourselves into a crisis, in which we are able to manoeuvre ourselves into a crisis, in which we are able to be in a situation in which we are in."}, {"heading": "2 ISOLATION FOREST AND ITS \u2019BLIND SPOT\u2019", "text": "The simple idea behind the isolation forest approach is that it is (generally) much easier to isolate an \"outlier\" from the rest of the data than to isolate an \"outlier\" from the rest of the data. In the context of a binary tree partitioning algorithm, this leads to the expectation that a shorter path is required to locate an \"outlier\" and a longer path to locate an \"outlier.\" This is illustrated in Fig.1, which shows that for a normally distributed 2D dataset, more separators are needed to separate the \"outlier\" xi from the rest of the data, compared to the number of separators needed to isolate the \"outlier\" x0."}, {"heading": "2.1 The Isolation Forest algorithm", "text": "The IF algorithm is an ensemble-based approach that builds a forest of random binary trees. Since an example S is randomly drawn from X, an isolation tree iT (S) is built recursively according to the (iTree) algorithm. (S, l, lmax) The isolation tree iT (S) is built recursively according to the (iTree) algorithm 1: manuscript subject to the ACM algorithm 1. (S, l, lmax) Input: S, l the current depth iT, lmax the maximum depth of the x algorithms."}, {"heading": "2.2 Existence of \u2019blind spots\u2019 in IF based anomaly detection", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3 HYBRID ISOLATION FOREST", "text": "In addition to the aforementioned approaches, we propose to overcome the disadvantage of the \"blind spot\" of IF by inserting directly into the IF framework two simple sources of information that allow the development of a more complex anomaly value: (1) the first is an unattended extension that exploits a distance from adjacent \"normal\" data, (2) the second is a monitored extension that exploits a distance from adjacent \"anomaly data.\""}, {"heading": "3.1 Adding a distance-based score", "text": "However, most of this data is located further away from the manuscript associated with the ACM \"normal\" data associated with the external node bucket found by the BST algorithm when evaluating the path length (see line 2 of algorithm 2). Therefore, the simple idea we propose to overcome the \"blind spot\" effect is to integrate this distance-based information into the IF score as follows: \u2022 first, the centering of the data associated with each external node of the hiTree when building the hiTree (training phase), corresponding to lines 2-5 of algorithm 3. \u2022 for each data set tested, its distance to the centering of the external node found by the BST algorithm is evaluated as the second element."}, {"heading": "3.2 Adding known anomalies", "text": "In some situations, some anomalies may have been detected and documented, which is the case for any monitored environment in which an incident is reported (e.g. in network monitoring and intruder detection).In such situations, incorporating this type of expertise into the IF framework can be a source for improvements in detecting new anomalies. In addition, this could provide a way to better balance, according to the application being targeted, the anomaly error detection rate and the false alarm rate. In order to implement this monitored extension of the isolation forest algorithm, we are developing a feature during the training phase that is detailed in Algorithm 4, which assigns a known (described) anomaly xa to the external node (the leaf of the iTree) that maps the manuscript subject to ACM Algorithm 4."}, {"heading": "3.3 Aggregating the scores", "text": "The IF score s (x, n) for data x and dataset size n, which is given in Equation (2), must be aggregated with the two new values sc (x) and sa (x). We start from a simple aggregation in two steps: (1) First, all three values are normalized so that they fit into the unit interval [0; 1]. In the train data, we evaluate the values for s (x), s (x), sc (x), sa (x), s (x), s (x), min (s), (s), (2), then we run two meta parameters \u03b11 and 2 (0; 1) and evaluate the following linear model: shif (x, n) = shif (x) = \u03b12 \u00b7 s (x), x x x x (x, n) \u00b7 sc (x)."}, {"heading": "3.4 Back to the synthetic experiment", "text": "To the previous setting (Fig.2), we add a third anomaly cluster consisting of the normal distribution with the mean (\u2212 3., \u2212 3.) and covariance (0.25, 0), (0,.25) (Xc, which also contains 1000 anomalies) in the lower left corner of the 2D gate, as shown in Fig. 5. Some anomalies labeled with \"red\" are also represented in black dots in the figure. We construct the HIF using the same meta parameter setting that we previously used for the construction of the IF, namely type = 64 and t = 512.3.4.1 Impact of the sc (x) score.: At this stage, we are looking at the HIF score, which leads to a reduced aggregation."}, {"heading": "3.5 Setting up meta parameters \u03b11 and \u03b12", "text": "Figures 6 and 8 show that a default value for \u03b11 can be set in [.2;.5]. If there are numerous unlabeled anomalies as well as \"normal\" test data in the test data, an exploratory analysis of the distribution of the HIF1 value can be used to isolate anomalies from the \"normal\" distribution. By varying \u03b11, the separation of distribution modes can vary and the search for an \"optimal\" separation can be performed. As an example, Figure 14 shows that the distribution of the HIF1 values for the \"normal\" train data distribution (in blue) and the test data (in red), in which \"normal\" and \"red\" anomalies are mixed together, represent an anomaly."}, {"heading": "3.6 Complexity of the HIF algorithm", "text": "Basically, the HIF algorithm has the same overall complexity as the IF algorithm, although some additional computing costs are incurred during the training and testing phase. At the training stage, the HIF algorithm must evaluate the complexity of the data attached to each of the external nodes during the training phase and during the testing phase during the maximum evaluation phase. The evaluation of a centridine depends on the number of elements contained in the external buckets (neb). It seems difficult to accurately estimate the expectation of the neb, yet Figure 15 presents the result of an empirical study showing that if the maximum height of the iTree lmax = logical evaluation phase of the iTree in the evaluation phase 0, the average neb value rises slightly faster than a logarithmic law."}, {"heading": "4 APPLICATION TO INTRUSION DETECTION IN NETWORK SYSTEMS", "text": "At this point, we present the results of the experiments carried out. First, we describe the data set that has been retained for conducting our experiments, then we specify the procedure for pre-processing the data before presenting and discussing the results. In this research, we are interested in both the packet payloads and the packet header information. While packet readers generally make up only a small part of the total network traffic data, packet payloads are more complicated. Accordingly, analyzing the packet payloads is more expensive than analyzing the packet header data, as it requires more calculation and pre-processing, so it is crucial to consider a suitable pre-processing process of the network traffic data."}, {"heading": "4.1 The ISCX dataset", "text": "The ISCX dataset 2012 [29], created at the Information Security Centre of Excellence at the University of New Brunswick, is used to conduct these experiments and to evaluate the performance of our proposed approaches; the entire ISCX dataset comprises over two million data packets, marked with 20 features, covering seven days of network activity (i.e. normal and attack); four different types of attack, referred to as Brute Force SSH, Infiltrating, HTTP DoS and DDoS, are carried out on different days. Despite some minor drawbacks, the ISCX dataset 2012 is the most up-to-date dataset available compared to the other alternative intruder detection datasets investigated [3, 21, 29]. As an input into the detection process, we use the pre-processed ISCX datasets, which consist of header and payload packets, which will be detailed later."}, {"heading": "4.2 Pre-processing of the data", "text": "Data pre-processing is a critical task that can even be considered the basic building block of intrusion detection. However, to convert categorical characteristics into fully numerical ones, we use the binary number representation, where we use binary numbers to represent an am-category attribute. However, if a categorical attribute takes its values in an infinite set of categories, we need to consider a different conversion approach, using a histogram of the distributions. In the next step, we add the number of source-target pairs to the characteristics in a predefined window size of the data streams. This additional function helps to detect network and IP scans, as well as distributed attacks. Table 5 presents the final characteristics after pre-processing, which are used in the experiments, but certainly not the smallest step, the normalization of the data. This additional step is crucial when we deal with the values, normalizing all characteristics according to the order of magnitude 1."}, {"heading": "4.3 Results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 CONCLUSION", "text": "From the construction of a synthetic dataset, we have a seemingly quite disadvantageous disadvantage in the Isolation Forest algorithm, the so-called \"blind spots,\" which embed unoccupied areas in the data as \"normal\" areas, even if they are far removed from the \"normal\" data distribution. To overcome this problem, we have proposed a second extension that introduces some centric distance calculations into the IF algorithm and shows that the \"blind spot\" effect disappears at very low computing costs. Manuscript sent to ACM In addition, we have introduced a second extension that provides a supervised capability to IF, which makes it possible to introduce known anomaly sites into the trees of the isolation forest. A distance-based score to these sites is the source for an additional score provided by the HIF."}], "references": [{"title": "Survey on anomaly detection using data mining techniques", "author": ["Shikha Agrawal", "Jitendra Agrawal"], "venue": "Procedia Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Graph based anomaly detection and description: A survey", "author": ["Leman Akoglu", "Hanghang Tong", "Danai Koutra"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Design of multilevel hybrid classifier with variant feature sets for intrusion detection system", "author": ["Asl\u0131han Akyol", "Mehmet Hacibeyo\u011flu", "Bekir Karlik"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Nearest-neighbor and clustering based anomaly detection algorithms for rapidminer", "author": ["Mennatallah Amer", "Markus Goldstein"], "venue": "Proceedings of the 3rd RapidMiner Community Meeting and Conferernce (RCOMM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Article: Network intrusion detection using clustering: A data mining approach", "author": ["S.Sathya Bama", "M.S.Irfan Ahmed", "A.Saravanan"], "venue": "International Journal of Computer Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Stochastic neighbor embedding (sne) for dimension reduction and visualization using arbitrary divergences", "author": ["Kerstin Bunte", "Sven Haase", "Michael Biehl", "Thomas Villmann"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Carreira-perpi\u00f1\u00e1n. The elastic embedding algorithm for dimensionality reduction", "author": ["\u00c1. Miguel"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Anomaly detection: A survey", "author": ["Varun Chandola", "Arindam Banerjee", "Vipin Kumar"], "venue": "ACM Comput. Surv.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A neural network component for an intrusion detection system", "author": ["H. Debar", "M. Becker", "D. Siboni"], "venue": "IEEE Computer Society Symposium on Research in Security and Privacy,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "One class random forests", "author": ["Chesner D\u00e9sir", "Simon Bernard", "Caroline Petitjean", "Laurent Heutte"], "venue": "Pattern Recogn.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "An anomaly detection approach based on isolation forest algorithm for streaming data using sliding window", "author": ["Zhiguo Ding", "Minrui Fei"], "venue": "IFAC Proceedings Volumes,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Anomaly detection support vector machine and its application to fault diagnosis", "author": ["R. Fujimaki"], "venue": "Eighth IEEE International Conference on Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "A study in using neural networks for anomaly and misuse detection", "author": ["Anup K. Ghosh", "Aaron Schwartzbard"], "venue": "In Proceedings of the 8th Conference on USENIX Security Symposium - Volume 8,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data", "author": ["Markus Goldstein", "Seiichi Uchida"], "venue": "PLOS ONE, 11(4):1\u201331,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Towards an information-theoretic framework for analyzing intrusion detection systems", "author": ["Guofei Gu", "Prahlad Fogla", "David Dagon", "Wenke Lee", "Boris \u0160kori\u0107"], "venue": "In 11th European Symposium on Research in Computer Security (ESORICS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Dimensionality reduction by supervised neighbor embedding using laplacian search", "author": ["Carlo Cattani Jianwei Zheng", "Hangke Zhang", "Wanliang Wang"], "venue": "Computational and Mathematical Methods in Medicine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Blind separation of sources, part 1: An adaptive algorithm based on neuromimetic architecture", "author": ["Christian Jutten", "Jeanny Herault"], "venue": "Signal Process.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "One-class classification with gaussian processes", "author": ["Michael Kemmler", "Erik Rodner", "Esther-Sabrina Wacker", "Joachim Denzler"], "venue": "Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Information-theoretic measures for anomaly detection", "author": ["Wenke Lee", "Dong Xiang"], "venue": "In Proceedings of the 2001 IEEE Symposium on Security and Privacy, SP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Cann: An intrusion detection system based on combining cluster centers and nearest neighbors", "author": ["Wei-Chao Lin", "Shih-Wen Ke", "Chih-Fong Tsai"], "venue": "Knowledge-Based Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Pcaplib: A system of extracting, classifying, and anonymizing real packet traces", "author": ["Ying-Dar Lin", "Po-Ching Lin", "Sheng-Hao Wang", "I-Wei Chen", "Yuan-Cheng Lai"], "venue": "IEEE Systems Journal,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Isolation forest", "author": ["F.T. Liu", "K.M. Ting", "Z.H. Zhou"], "venue": "In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM\u201908),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Isolation-based anomaly detection", "author": ["Fei Tony Liu", "Kai Ming Ting", "Zhi-Hua Zhou"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Spherical stochastic neighbor embedding of hyperspectral data", "author": ["Dalton Lunga", "Okan K. Ersoy"], "venue": "IEEE Trans. Geoscience and Remote Sensing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Data Structures and Algorithms with Object-Oriented Design", "author": ["Bruno Richard Preiss"], "venue": "Patterns in Java. wiley,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Intrusion detection with neural networks", "author": ["Jake Ryan", "Meng-Jang Lin", "Risto Miikkulainen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["Bernhard Sch\u00f6lkopf", "Alexander Smola", "Klaus-Robert M\u00fcller"], "venue": "Neural Comput.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "A Novel Isolation-Based Outlier Detection Method, pages 446\u2013456", "author": ["Yanhui Shen", "Huawen Liu", "Yanxia Wang", "Zhongyu Chen", "Guanghua Sun"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Toward developing a systematic approach to generate benchmark datasets for intrusion detection", "author": ["Ali Shiravi", "Hadi Shiravi", "Mahbod Tavallaee", "Ali A. Ghorbani"], "venue": "Computer Security,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "A quite exhaustive, although a bit old, review in anomaly detection has been proposed in [8], completed by a more recent comparative study [14].", "startOffset": 89, "endOffset": 92}, {"referenceID": 13, "context": "A quite exhaustive, although a bit old, review in anomaly detection has been proposed in [8], completed by a more recent comparative study [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "According to these studies, the state of the art methods can be distributed into five main categories: (1) Near neighbors and clustering based methods [4]: Near Neighbors methods rely on the assumption that a \u2019normal\u2019 instance occurs close to its near neighbors while an anomaly occurs far from its near neighbors.", "startOffset": 151, "endOffset": 154}, {"referenceID": 4, "context": "Similarly, cluster based methods rely on the assumption that a \u2019normal\u2019 instance occurs near its closest cluster centroid while an anomaly will occur far from its nearest cluster centroid [5, 20].", "startOffset": 188, "endOffset": 195}, {"referenceID": 19, "context": "Similarly, cluster based methods rely on the assumption that a \u2019normal\u2019 instance occurs near its closest cluster centroid while an anomaly will occur far from its nearest cluster centroid [5, 20].", "startOffset": 188, "endOffset": 195}, {"referenceID": 9, "context": "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.", "startOffset": 129, "endOffset": 133}, {"referenceID": 8, "context": "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.", "startOffset": 155, "endOffset": 166}, {"referenceID": 12, "context": "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.", "startOffset": 155, "endOffset": 166}, {"referenceID": 25, "context": "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.", "startOffset": 155, "endOffset": 166}, {"referenceID": 17, "context": "Popular approaches in this category are kernel based density models and the Gaussian Mixture Model (GMM), including recent advance in one-class GMM [18], (4) Information theoretic based methods use information theoretic measures [19], such as the entropy, the Kolmogorof complexity, the minimum description length, etc, to estimate the \u2019complexity\u2019 of the \u2019normal\u2019 dataset (or equivalently the complexity of the process behind the production of these data) [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Popular approaches in this category are kernel based density models and the Gaussian Mixture Model (GMM), including recent advance in one-class GMM [18], (4) Information theoretic based methods use information theoretic measures [19], such as the entropy, the Kolmogorof complexity, the minimum description length, etc, to estimate the \u2019complexity\u2019 of the \u2019normal\u2019 dataset (or equivalently the complexity of the process behind the production of these data) [15].", "startOffset": 229, "endOffset": 233}, {"referenceID": 14, "context": "Popular approaches in this category are kernel based density models and the Gaussian Mixture Model (GMM), including recent advance in one-class GMM [18], (4) Information theoretic based methods use information theoretic measures [19], such as the entropy, the Kolmogorof complexity, the minimum description length, etc, to estimate the \u2019complexity\u2019 of the \u2019normal\u2019 dataset (or equivalently the complexity of the process behind the production of these data) [15].", "startOffset": 457, "endOffset": 461}, {"referenceID": 1, "context": "(5) Spectral based method rely on the assumption that it is possible to embed the data into a lower dimensional subspace in which \u2019normal\u2019 data and anomalies are supposedly well separated [2].", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "A recent review [1], though much less detailed, identifies a supplemental category that stands in between supervised and unsupervised methods, and that authors refer to as \u2019hybrid methods\u2019.", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": "In 2008, Isolation Forest (IF) [22], a quite conceptually different approach to the previously referenced methods, has been proposed that went strangely below the radar of the previous review.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "IF has been successfully used in some applications, [23], [11] and extended recently in [28] to improve the selection of attributes and their split values when constructing the tree.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "IF has been successfully used in some applications, [23], [11] and extended recently in [28] to improve the selection of attributes and their split values when constructing the tree.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "IF has been successfully used in some applications, [23], [11] and extended recently in [28] to improve the selection of attributes and their split values when constructing the tree.", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "Principle of the IF algorithm (Figure is from [22]).", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "1 The Isolation Forest algorithm We reproduce hereinafter the description of the isolation forest algorithm as presented in [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "3 of [25] gives the average path length of unsuccessful search in BST as:", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "The kernel PCA [27] is a good example of such method.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "This can be feasible to some extend and in some framework, such as blind source separation, though it requires some knowledge about the data, such as the number of sources [17].", "startOffset": 172, "endOffset": 176}, {"referenceID": 5, "context": "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .", "startOffset": 286, "endOffset": 300}, {"referenceID": 6, "context": "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .", "startOffset": 286, "endOffset": 300}, {"referenceID": 15, "context": "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .", "startOffset": 286, "endOffset": 300}, {"referenceID": 23, "context": "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .", "startOffset": 286, "endOffset": 300}, {"referenceID": 28, "context": "1 The ISCX dataset The ISCX dataset 2012 [29], which has been prepared at the Information Security Centre of Excellence at the University of New Brunswick, is used to perform this experiments and evaluate the performance of our proposed approaches.", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "Despite some minor disadvantages, the ISCX dataset 2012 is the most up-to-date available dataset compared to the other aging explored datasets for intrusion detection [3, 21, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 20, "context": "Despite some minor disadvantages, the ISCX dataset 2012 is the most up-to-date available dataset compared to the other aging explored datasets for intrusion detection [3, 21, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 28, "context": "Despite some minor disadvantages, the ISCX dataset 2012 is the most up-to-date available dataset compared to the other aging explored datasets for intrusion detection [3, 21, 29].", "startOffset": 167, "endOffset": 178}], "year": 2017, "abstractText": "From the identification of a drawback in the Isolation Forest (IF) algorithm that limits its use in the scope of anomaly detection, we propose two extensions that allow to firstly overcome the previously mention limitation and secondly to provide it with some supervised learning capability. The resulting Hybrid Isolation Forest (HIF) that we propose is first evaluated on a synthetic dataset to analyze the effect of the new meta-parameters that are introduced and verify that the addressed limitation of the IF algorithm is effectively overcame. We hen compare the two algorithms on the ISCX benchmark dataset, in the context of a network intrusion detection application. Our experiments show that HIF outperforms IF, but also challenges the 1-class and 2-classes SVM baselines with computational efficiency.", "creator": "LaTeX with hyperref package"}}}