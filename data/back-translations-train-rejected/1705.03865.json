{"id": "1705.03865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Survey of Visual Question Answering: Datasets and Techniques", "abstract": "Visual question answering (or VQA) is a new and exciting problem that combines natural language processing and computer vision techniques. We present a survey of the various datasets and models that have been used to tackle this task. The first part of the survey details the various datasets for VQA and compares them along some common factors. The second part of this survey details the different approaches for VQA, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into the first three. Finally, we compare the performances of these approaches and provide some directions for future work.", "histories": [["v1", "Wed, 10 May 2017 17:30:17 GMT  (246kb,D)", "https://arxiv.org/abs/1705.03865v1", "9 pages, 3 figures, 3 tables"], ["v2", "Thu, 11 May 2017 06:46:52 GMT  (248kb,D)", "http://arxiv.org/abs/1705.03865v2", "10 pages, 3 figures, 3 tables Added references, corrected typos, made references less wordy"]], "COMMENTS": "9 pages, 3 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["akshay kumar gupta"], "accepted": false, "id": "1705.03865"}, "pdf": {"name": "1705.03865.pdf", "metadata": {"source": "CRF", "title": "Survey of Visual Question Answering: Datasets and Techniques", "authors": ["Akshay Kumar Gupta"], "emails": ["cs5130275@cse.iitd.ac.in"], "sections": [{"heading": "1 Introduction", "text": "The task is usually to show a computer an image and ask a question about that image, which the computer must answer. The answer could come in one of the following forms: a word, a phrase, a yes / no answer, selecting from several possible answers, or filling in the blank answer. Answering a visual question is an important and appealing task because it combines the areas of computer vision and processing natural language. Computer vision techniques must be used to understand the image, and NLP techniques must be used to effectively answer the question in the context of the image. This is a challenge because these two areas have historically used different methods and models to solve their respective tasks. This survey describes some prominent data sets and models that have been used to answer the visual question, and provides a comparison to the Q3 models that represent some of the possible data."}, {"heading": "2 Datasets", "text": "In the last 2-3 years, several large datasets for the VQA task have been published. We discuss these datasets below. Table 1 provides a summary of these datasets."}, {"heading": "2.1 DAQUAR (Malinowski and Fritz, 2014)", "text": "The DAtaset for QUestion Answering on Realworld images (or DAQUAR), released in 2015, was the first dataset and benchmark published for the VQA task. It takes images from the NYUDepth V2 dataset, which contains images along with their semantic segmentations. Each pixel of an image is labeled with an object class (or no object) out of 894 possible classes. The images are all interior scenes. There are a total of 1449 images (795 training, 654 test). The authors generated question pairs in two ways: 1) automated, using question templates. The authors defined 9 question templates, the answers of which can be extracted from the existing datasets of NYU-Depth V2. An example of a question template is \"How many [objects] are in [Image ID]?\" 2) Using human annotations, 5 participants were asked to fill out questions and answers with a restriction of the numbers that must be generated for either those classes or numbers."}, {"heading": "2.2 Visual7W (Zhu et al., 2016)", "text": "Visual 7W is a dataset generated using images from the MS-COCO dataset (Lin et al., 2014) for caption, recognition and segmentation; the Visual7W dataset takes its name from the creation of multiple choice questions from the form (Who, What, Where, When, Why, How and Which); workers from Amazon Mechanical Turk (AMT) were used to generate the questions; a separate set of three workers was used to evaluate the questions, and those with less than two positive votes were discarded; multiple choice answers were generated both automatically and by AMT workers. AMT employees were also asked to draw boxes with objects mentioned in the question in the picture, firstly to resolve textual ambiguities (e.g. an image has two red cars. Then the \"red car\" in the question could refer to either of these two); and secondly to allow for a visual response to the data set (47.339)."}, {"heading": "2.3 Visual Madlibs (Yu et al., 2015)", "text": "The Visual Madlibs dataset is both a fill-in and a multiple-choice dataset. Images are collected by MS-COCO. Descriptive fill-in questions are automatically generated from templates and object information. Each question generated in this way is answered by a group of 3 AMT employees. The answer can be a word or phrase. As an additional rating measure, multiple choices are also provided for the blanks. The dataset contains 10,738 images and 360,001 questions. Multiple-choice questions are evaluated using the accuracy metric."}, {"heading": "2.4 COCO-QA (Ren et al., 2015)", "text": "The COCO-QA dataset is another MS-COCO-based dataset. Both questions and answers are automatically generated from MS-COCO using captions and fall into four broad categories: object, number, color, and location. There is one question per picture, and the answers are one word. The dataset contains a total of 123,287 images. The evaluation is either based on accuracy or the WUPS score."}, {"heading": "2.5 FM-IQA (Gao et al., 2015)", "text": "The Freestyle Multilingual Image Question Answering Dataset (FM-IQA) takes images from the MS-COCO dataset and uses the Baidu crowdsourcing server to get people to generate questions and answers. Answers can be words, phrases or whole sentences. Question-answer pairs are available in both Chinese and English translation. The dataset contains 158,392 images and 316,193 questions. They suggest a human evaluation using a visual turning test, which may be one reason that this dataset has not gained much popularity."}, {"heading": "2.6 VQA (Antol et al., 2015)", "text": "The Visual Question Answering (VQA) dataset is the most commonly used dataset for the VQA task. This dataset was published as part of the Visual Question Answering (VQA) task. It consists of two parts: One dataset contains images from the real world of MS-COCO, and another dataset contains abstract clip part scenes created by models of humans and animals to eliminate the need to process loud images and argue only at a high level. Questions and answers are generated by crowd sourcing employees, and 10 answers are received for each question by unique workers. Answers are typically one word or a short phrase. Approximately 40% of the questions have a yes or no answer. Open answer generations as well as multiple choice formats are available for evaluation. Multiple choice questions have 18 candidate answers. To evaluate open answers, a machine-generated answer is provided by the VQA rating system, and if these answers are rated as # 750,000 or higher, then a QA people have corrected."}, {"heading": "3 Models", "text": "The VQA task was proposed after deep learning approaches had become popular due to their state-of-the-art performance in various vision and NLP tasks (Krizhevsky et al., 2012) (Bahdanau et al., 2014). As a result, almost all work on VQA in the literature includes deep learning approaches, as opposed to more classic approaches such as graphic models. There are a number of models that use a non-neural approach that are described in detail in the first subsection. In addition, several simple baselines used by the authors include non-neural methods that are also described; the second subsection describes deep learning models that do not include the use of attention-based techniques; the third subsection describes attention-based deep learning models for VQA; and the results of all models described are summarized in Tables 2 and 3."}, {"heading": "3.1 Non-deep learning approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Answer Type Prediction (ATP)", "text": "(Kafle and Kanan, 2016) propose a Bayesian framework for QQA, in which they predict the answer type for a question and use it to generate the report.The possible answer types vary according to the data sets they consider. For example, for COCO-QA they consider four answer types: object, color, count, and location. Their model calculates the probability of an answer type t given the image x and the question q, P (A = a, T = t | x, q) = P (A = a = t, q) P (A = a | T = t, q) P (T = t | q) P (x | q) (1) resulting from Baye's rule. You can then marginalize across all answer types to get P (A = a | x, q), the denominator being constant for a given question and an image, and therefore can be ignored."}, {"heading": "3.1.2 Multi-World QA (Malinowski and", "text": "Fritz, 2014) This paper models the probability of an answer to a question and an image asP (A = a | Q, W) = \u0442TP (A = a | T, W) P (T | Q) (2) Here T is a latent variable corresponding to a semantic tree derived from a semantic parser run on the question. W is the world that is a representation of the image. This can only be the original image or the image together with additional features derived from segmentation. P (A = a | T, W) is evaluated by means of a deterministic evaluation function. P (T | Q) is obtained by forming a simple log-linear model. This model is called SWQA (Single World Question Answering). The authors extend the SWQA model to a multi-world scenario to model uncertainties in segmentation and class labeling. Different labels lead to different worlds, so that the probability is now defined as A = W | Q (a)."}, {"heading": "3.2 Non-attention Deep Learning Models", "text": "The following model descriptions assume that the reader is familiar with CNNs (Krizhevsky et al., 2012) as well as with RNN variants such as Long Short Term Memory Units (LSTMs) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRUs) (Cho et al., 2014). Some approaches do not include the use of RNNs. These are discussed first."}, {"heading": "3.2.1 iBOWIMG", "text": "(Zhou et al., 2015) propose a basic model called iBOWIMG for VQA. They use the output of a later level of the pre-trained GoogLe Net model for image classification (Szegedy et al., 2015) to extract image attributes. Word embedding of each word in the question is taken as text attributes so that the text attributes are simple word bags. Image and text attributes are concatenated and Softmax regression is carried out across the response classes, demonstrating that this model achieves performance comparable to multiple RNN-based approaches on the VQA dataset."}, {"heading": "3.2.2 Full-CNN", "text": "(Ma et al., 2015) propose a CNN-only model, which we here refer to as Full CNN. They use three different CNNs: a CNN image to encode the image, a CNN question to encode the question, and a CNN to encode the image and question together and produce a common representation.The CNN image uses the same architecture as VGGnet (Simonyan and Zisserman, 2014) and receives a 4096-length vector from the second-to-last layer of this network. This is passed on by another fully connected layer to obtain the image representation vector of size 400. The CNN question includes three layers of folding + max pooling. The size of the revolutionary receptive field is set to 3. In other words, the core considers a word along with its immediate neighbors. The common CNN, which they refer to as a multimodal CNN model, performs the transformation across the question."}, {"heading": "3.2.4 Vis+LSTM (Ren et al., 2015)", "text": "This model is very similar to the AYN model. The model uses the last layer of VGGnet to get the image encoding. They use an LSTM to encode the question. Unlike the previous model, they provide the encoded image as the first \"word\" to this LSTM network before the question. The output of this LSTM is done by a fully connected, followed by a Softmax layer. They call this model Vis + LSTM. The authors also propose a 2Vis + BLSTM model that uses a bidirectional LSTM instead."}, {"heading": "3.2.5 Dynamic Parameter Prediction", "text": "UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU"}, {"heading": "3.3 Attention-based Deep Learning Techniques", "text": "Attention-based techniques are some of the most popular techniques used in many tasks such as machine translation (Bahdanau et al., 2014), captions (Xu et al., 2015), etc. When performing quality control tasks, attention models must focus on important parts of the image, the question, or both in order to provide an effective answer."}, {"heading": "3.3.1 Where to Look", "text": "(Shih et al., 2016) suggest an attention-based model, which is henceforth referred to as WTL. They use VGGnet to encode the image and concatenate the results of the last two layers of VGGnet to obtain an image encoding.The question arises from averaging the word vectors of each word in the question. An attention vector is calculated based on the amount of image characteristics and \u2212 \u2192 q is the embedding of the question in order to decide which region in the image is to be given meaning. This vector is calculated as follows: If V = (\u2212 \u2192 v1, \u2212 \u2192 v2. \u2212 vK) is the amount of image characteristics and \u2212 \u2192 q is the embedding of the question, then the meaning of the last region is calculated as gj = (A \u2212 \u2192 vj + bA) T (B \u2212 \u2192 q + bB).Attention weights are calculated based on the normalization \u2212 \u2192 g. The final image representation is a weighted sum of the different regions that is not associated with the loss of the question."}, {"heading": "3.3.2 Recurrent Spatial Attention (R-SA)", "text": "(Zhu et al., 2016) This model is one step above the previous model in two ways. Firstly, it uses LSTMs to encode the question, and secondly, it calculates the attention paid to the image repeatedly after scanning each word of the question. Specifically, we repeatedly calculate an attention-weighted sum of image features, rt, at each step t of the LSTM. rt goes as an additional input to the next step of the LSTM. The attention-weighted parts used to obtain rt are calculated using a dense + Softmax layer over the previous hidden state of the LSTM \u2212 1 and the image itself. Thus, as we read the question, we intuitively decide repeatedly which parts of the image to consider and which parts to consider, both from the current word and from the previous attention-weighted image \u2212 1. This model is evaluated based on the Visual7W dataset, which parts of the image to consider and which parts to consider, by both the current word and the previous attention-weighted image \u2212 1."}, {"heading": "3.3.3 Stacked Attention Networks (SAN)", "text": "(Yang et al., 2016) This model is similar in spirit to the previous model in that it repeatedly computes attention on the image to obtain finer-grained visual information to predict the answer. While the previous model performs this word for word, this model first encodes the entire question using an LSTM or CNN. This question encoding is used to treat the image with a similar equation as before. Then, the attention-weighted image is associated with the question encoding and used to re-compute attention on the original image. It can be repeated k times after the question and final image representation are used to predict the answer. The authors argue that this type of \"stacked\" attention helps the model iteratively discard unimportant regions of the image. The authors experiment with k = 1 and k = 2 and report the results on DAQUAR, COCO-QA and the final image representation on Datass.3.3, hierarchical attention et al."}, {"heading": "3.4 Other Models", "text": "The following models use more ideas than simply changing how you look after the image or question, and therefore do not fit into the previous sections."}, {"heading": "3.4.1 Neural Module Networks (NMNs)", "text": "(Andreas et al., 2016) This model involves the creation of a neural network on the fly for each individual image and question, by selecting from various modules and composing them to create the neural network. There are five types of modules: attention [c] (which calculates an attention card for a particular image and a given c; c can be \"dog,\" for example, then attention [dog] will try to find a dog), classification [c] (which outputs a distribution of labels that c belongs to a certain image and a given attention card; c can be \"color\"), attention [c] (which takes an attention card and converts it to c; c can be \"top,\" meaning to shift the attention upwards), measurement [c] (which outputs a distribution of labels that are based solely on the attention card; c) and combination [c] (which combines two attention cards, which may be \"like a module,\" or by means of a \"or by means of a\" to decide. \""}, {"heading": "3.4.2 Incorporating Knowledge Bases", "text": "(Wu et al., 2016b) introduce the Ask Me Anything (AMA) model, which attempts to use information from an external knowledge base to facilitate visual answers to questions. It first obtains a set of attributes such as object names, properties, etc. of the images based on captions, and the captioning model is trained on the MS-COCO dataset using standard captioning techniques. There are 256 possible attributes and the attribute generator is trained on MS-COCO, using a variation of the VGG network. The five best attributes are used to generate queries for the DBpedia database (Auer et al., 2007). Each query provides a text summary summarized using Doc2Vec (Le and Mikolov, 2014), which is passed on as additional input to the LSTM decoder, which generates the response. The authors show COVQA and QA data sets."}, {"heading": "4 Discussion and Future Work", "text": "As has been the trend in recent years, deep learning models outperform earlier graphical models based on approaches in all VQA datasets. However, it is interesting to note that the Answer Type Prediction (ATP) model performs better than the non-attention models, proving that the simple introduction of conventional and / or recurring neural networks is not enough: Identification of parts of the image that are relevant in principle is important. ATP is even competing with or better than some attention models such as Where to Look (WTL) and Stacked Attention Networks (SAN).Significant improvement is demonstrated by hierarchical co-attribution networks (CoAtt), which are the first answer to the question in addition to the image. This could be helpful, especially for longer questions that are more difficult to encode into a single vector representation of LSTMs / GRUs, so that first each word is encoded and then the image is used to be better."}, {"heading": "5 Conclusion", "text": "The field of VQA has grown by leaps and bounds, even though it was introduced only a few years ago. Deep learning methods for VQA remain the models that receive the most attention and are state-of-the-art. We have examined the most prominent of these models and listed their performance in various large datasets. Significant performance improvements are still evident in many datasets, meaning there is plenty of room for future innovation in this task."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Visual question answering (or VQA) is a new and exciting problem that combines natural language processing and computer vision techniques. We present a survey of the various datasets and models that have been used to tackle this task. The first part of this survey details the various datasets for VQA and compares them along some common factors. The second part of this survey details the different approaches for VQA, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into the first three. Finally, we compare the performances of these approaches and provide some directions for future work.", "creator": "LaTeX with hyperref package"}}}