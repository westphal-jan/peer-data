{"id": "1705.03967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "GQ($\\lambda$) Quick Reference and Implementation Guide", "abstract": "This document should serve as a quick reference for and guide to the implementation of linear GQ($\\lambda$), a gradient-based off-policy temporal-difference learning algorithm. Explanation of the intuition and theory behind the algorithm are provided elsewhere (e.g., Maei &amp; Sutton 2010, Maei 2011). If you questions or concerns about the content in this document or the attached java code please email Adam White (adam.white@ualberta.ca).", "histories": [["v1", "Wed, 10 May 2017 22:43:11 GMT  (4kb)", "http://arxiv.org/abs/1705.03967v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["adam white", "richard s sutton"], "accepted": false, "id": "1705.03967"}, "pdf": {"name": "1705.03967.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["(adam.white@ualberta.ca)."], "sections": [{"heading": null, "text": "ar Xiv: 170 5.03 967v 1 [cs.L G] 10 May 201 7 GQ (\u03bb) Quick Reference and Implementation GuideAdam White and Richard S. SuttonRevised 29 July 2014 This document should serve as a brief reference and guide for the implementation of linear GQ (\u03bb), a gradient-based learning algorithm for temporal differences outside politics. Explanations of the intuition and theory behind the algorithm can be found elsewhere (e.g. Maei & Sutton 2010, Maei 2011).If you have questions or concerns about the content of this document or the accompanying Java code, please send an e-mail to Adam White (adam.white @ ualberta.ca)."}, {"heading": "1 Requirements and Setting", "text": "For each use of GQ (\u03bb), you must provide three question functions indicating the quantity to be expected \u2022 \u2022 Quantity \u00b7 \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity \u00b7 Quantity"}, {"heading": "2 Algorithm Specification", "text": "Let us now specify GQ (\u03bb). Let us initialize w and e to zero and arbitrarily initialize \u03b8. Let us let the subscript t specify the current time step. Let us specify \u03c1t the \"importance-sampling\" ratio: \u03c1t = \u03c0 (St, At) b (St, At), (1) where St and At are the state and action that occur at the right time. Let \u03c6 t specify the expected next attribute vector, defined by:? t =? t? t? t? t? (St, a)? (St, a) (2) Then the following equations fully specify GQ? t = r? (St, At, St + 1) + \u03b3 (St + 1)."}, {"heading": "3 Pseudocode", "text": "The following pseudo code characterizes the algorithm and its application. Initializes \u03b8 arbitrarily and w = 0 repetition (for each episode): Initializes e = 0 S \u2190 initial state of episode repetition (for each step of the episode): A \u2190 of the policy b in state S selected action A, note the next state, S \"\u03c6.\" 0 For all a \"A (s):????? (S,\" a)? (S, \"a)? (S,\" a)? (S)?? (S, \"A)? (S,\" A)?? (S, \"A\")? (S \"),? (S\"),? (S \"),? (S\") (S \") (S\") (S \") (S\"), \"(? S,\" S (? S) (? S), \"S (? S) (? S),\" S (? S) (? S), \"S (? S) (? S),\" (? S (? S), \"S (? S) (? S),\" (? S (? S), \"S (? S),\" (? S (? S), \"(? S),\" S (? (? S), \"(? S),\" (? S (? S), \"(? (? S),\" (? S), \"(S (? S),\" (? (? S)."}, {"heading": "4 Code", "text": "The GQlambda.java and GQlambda.cpp files (in the arXiv source archive) contain implementations of the GQlearn function described in the pseudo-code. We have excluded optimizations (e.g. binary features or efficiency trace implementation) to ensure that the code is simple and easy to understand. We leave it up to the reader to provide environment code for linking to GQ (\u03bb) (e.g. using RL glue)."}, {"heading": "5 References", "text": "In Proceedings of the 27th International Conference on Machine Learning, Haifa, Israel.Maei, H. R. and Sutton, R. S. (2010): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial General Intelligence, pp. 91-96. Modayil, J., White, A., Sutton, R. S. (2014). Multi-timescale nexting in a reinforcement learning robot. Adaptive Behavior 22 (2): 146-160.Sutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press."}], "references": [{"title": "Toward off-policy learning control with function approximation", "author": ["H.R. Maei", "Szepesv\u00e1ri", "Cs", "S. Bhatnagar", "R.S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine", "citeRegEx": "Maei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2010}, {"title": "GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "In Proceedings of the Third Conference on Artificial General Intelligence,", "citeRegEx": "Maei and Sutton,? \\Q2010\\E", "shortCiteRegEx": "Maei and Sutton", "year": 2010}, {"title": "Multi-timescale nexting in a reinforcement learning robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Adaptive Behavior", "citeRegEx": "Modayil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Modayil et al\\.", "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "This document should serve as a quick reference for and guide to the implementation of linear GQ(\u03bb), a gradient-based off-policy temporal-difference learning algorithm. Explanation of the intuition and theory behind the algorithm are provided elsewhere (e.g., Maei & Sutton 2010, Maei 2011). If you questions or concerns about the content in this document or the attached java code please email Adam White (adam.white@ualberta.ca).", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}