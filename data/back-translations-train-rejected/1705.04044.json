{"id": "1705.04044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "End-to-end Recurrent Neural Network Models for Vietnamese Named Entity Recognition: Word-level vs. Character-level", "abstract": "This paper demonstrates end-to-end neural network architectures for Vietnamese named entity recognition. Our best model is the combination of bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network (CNN), Conditional Random Field (CRF), using pre-trained word embeddings as input, which achieves an F1 score of 88.59% on a standard test set. Our system is able to achieve a comparable performance to the first-rank system of the VLSP campaign without using any syntactic or hand-crafted features. We also give an extensive empirical study on using common deep learning models for Vietnamese NER, at both word and character level.", "histories": [["v1", "Thu, 11 May 2017 07:31:39 GMT  (148kb)", "http://arxiv.org/abs/1705.04044v1", "6 pages, submitted to PACLING 2017"], ["v2", "Tue, 11 Jul 2017 16:49:03 GMT  (218kb)", "http://arxiv.org/abs/1705.04044v2", "14 pages, 5 figures, 7 tables, accepted to PACLING 2017"], ["v3", "Fri, 21 Jul 2017 00:04:32 GMT  (218kb)", "http://arxiv.org/abs/1705.04044v3", "14 pages, 5 figures, 7 tables, accepted to PACLING 2017, fix CRF formular"]], "COMMENTS": "6 pages, submitted to PACLING 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thai-hoang pham", "phuong le-hong"], "accepted": false, "id": "1705.04044"}, "pdf": {"name": "1705.04044.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Phuong Le-Hong"], "emails": ["phamthaihoang.hn@gmail.com", "phuonglh@vnu.edu.vn"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.04 044v 1 [cs.C L] 11 May 2Keywords Vietnamese, called Entity Recognition, end-to-endI. INTRODUCTIONNamed Entity Recognition (NER) is a fundamental task in natural language processing and information extraction. It involves the identification of noun phrases and the classification of each of them into a predefined class. In 1995, the 6th Message Understanding Conference (MUC) 1 began with the evaluation of NER systems for English and in subsequent joint tasks of CoNLL 20022 and CoNLL 20033 conferences, language-independent NER systems were evaluated. In these evaluation tasks, four named entity type types were considered, including names of persons, organizations, locations and names of different entities that do not belong to these three types. More recently, Vietnamese language and language processing models (LSP Community 4) are being systematically organized to compare the evaluation campaign for the Vietnamese language."}, {"heading": "II. RELATED WORK", "text": "The first approach is characterized by the use of well-established sequence labeling models such as the Conditional Random Field (CRF), hidden markov model, support vector machine, maximum entropy, etc. The performance of these models is heavily dependent on handcrafted characteristics. In particular, most participants in CoNLL-2003 jointly tried to collect information other than the available training data such as gazetteers and5The first-rank system of the VLSP 2016 NER evaluation campaign has F1 = 88.78% on the testset.Uncommented data on the CoNLL-2003 shared task is the work of [1], which has achieved an F1 score of 88.76%. Afterwards [2] it exceeded it by using phrase functions extracted from an external database. Furthermore, the training of zero models along with related tasks helps to improve its performance."}, {"heading": "III. METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Long Short-Term Memory", "text": "Long-term memory (LSTM) [13] is a variant of RNN that is designed to deal with these problems of disappearing and exploding gradients [14], [15] when learning long-range sequences. LSTM networks are the same as RNN except that the hidden layer updates are replaced by memory cells. Basically, a memory cell consists of three multiplicative gates that control the portions of information to forget and pass on to the next time step. Consequently, it is better for using dependency data over long distances. It is calculated as follows: it = \u03c3 (Wiht \u2212 1 + Uixt + bi) ft = \u03c3 (Wfht \u2212 1 + Ufxt + bf) ct = ft-ct \u2212 1 + it-tanh (Wcht \u2212 1 + it-tanh)."}, {"heading": "B. Bidirectional Long Short-Term Memory", "text": "The original LSTM uses only earlier contexts for prediction. For many sequence marking tasks, it is advisable to take the contexts from two directions. Therefore, we use the bi-directional LSTM (Bi-LSTM) [16], [17] for both word and character systems."}, {"heading": "C. Convolutional Neural Network for Character-Embedding", "text": "This feature of CNN allows this network to have many neurons and therefore to express mathematically large models, while the number of actual parameters keeps relativity low. For NLP tasks, previous work has shown that CNN is likely to effectively extract morphological characteristics such as prefix and suffix [18], [9], [11]. For this reason, we include CNN in the word-level model to obtain richer information from both word and character characteristics. CNN, which we use in this paper, is described in Figure 1."}, {"heading": "D. Conditional Random Field", "text": "The Conditinal Random Field (CRF) [19] is a kind of graphical model developed to label the data sequence, but which predicts the results independently. CRF is therefore advantageous for investigating the correlations between the output data and jointly decoding the best label sequence. In the NER task, we implement the CRF at the top of the Bi-LSTM instead of the Softmax layer and accept the results of Bi-LSTM as inputs to this model. The parameter of the CRF is the transition matrix A, in which viti, j the transition point from day i to day j. The score of the input set x together with the sequence of tags y is calculated as inputs from Bi-LSTM."}, {"heading": "E. Word-level Model", "text": "We construct the CRF at the top level of the Bi-LSTM layer for the word level model. Input for this model is concatenating word embeddings and character-level features that we learned from the CNN layer. To create word embeddings for Vietnamese, we train a word grammar model using the word2vec6 tool on a dataset that consists of 7.3GB of text from 2 million articles collected through a Vietnamese news portal.7 The text is first normalized to lower case and all special characters are removed; the usual symbols such as the comma, semicolon, colon, period and percentage sign are replaced by the special token point, and all sequences are 6https: / / code.google.com / archive / p / word2vec / 7http: / www.baomoi.com."}, {"heading": "F. Character-level Model", "text": "We also use the Bi-LSTM-CRF architecture for the character model, but the input is the string instead of the word sequence. Training data is designed for labelling at the word level. Output is the sequence of labels where each character belongs to a specific word and not to a character. Therefore, it is necessary to convert the data set of word strings into strings. We use a simple method in which all characters of a word are labelled with the same tag. Thus, Figure 3 shows the labelling of all characters of a person named Entity. Likewise, all characters of a place, an organization and other characters are labelled with the letters L, G or M. The characters of other words and spaces are labelled with O. The conversion of all characters of a person named Entity shows the conversion of word level into character level of an example set Anh r\u0441i EU h\u00f4m qua (Great Britain left the EU yesterday) Since the characters of these units are relative in size, and 3dimensions are distributed by the animal dimensions."}, {"heading": "IV. RESULTS AND DISCUSSIONS", "text": "A. VLSP CorpusWe evaluate our system based on the VSLP NER Shared Task 2016 Corpus. This corpus consists of electronic newspapers that are published on the Web. There are four named entity types in this corpus, names of the person, location, organization and other named entities. Four types of NEs are compatible with their descriptions in the CoNLL Shared Task 2003. Examples of each entity type are described in the IData table and have been pre-processed with word segmentation and POS tagging. Since POS tags and chunking tags are automatically determined by public tools, they may contain errors. The format of this corpus follows that of the CoNLL 2003 Shared Task. It consists of five columns. The order of these columns are word, POS tag, chunking tag, named entity label and nested named entity label. Our system focuses on named entity only without labels so we do not use the fifth column labels."}, {"heading": "B. Evaluation Method", "text": "Performance is measured by the F1 score: F1 = 2 \u043c Accuracy \u043a Recallprecision + RecallPrecision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities that are present in the corpus and are found by the system. A named entity is only correct if it is an exact match with the corresponding entity in the data file. In the character stage model, we convert these results back to the word level sequence to be evaluated after predicting the designation for each character. The performance of our system is evaluated by the automatic evaluation script of the CoNLL joint task 2003.8."}, {"heading": "C. Results", "text": "In fact, most of the people who are able to move are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to"}, {"heading": "V. CONCLUSION", "text": "In this work, we have studied a variety of end-to-end recurrent neural network architectures at both the word and character levels for Vietnamese entity recognition. Our best end-to-end system is the combination of Bi-LSTM, CNN, and CRF models, using pre-trained word embedding as input, which achieves an F1 score of 88.59% on the standard test corpus recently released by the Vietnamese language and linguistic community. Our system is competitive with the world-class system of NERs sharing tasks associated with it, without using any craftsmanship characteristics."}], "references": [{"title": "Named entity recognition through classifier combination", "author": ["R. Florian", "A. Ittycheriah", "H. Jing", "T. Zhang"], "venue": "Proceedings of CoNLL-2003, W. Daelemans and M. Osborne, Eds. Edmonton, Canada, 2003, pp. 168\u2013171. 9This team provided a system without the technical report.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Phrase clustering for discriminative learning", "author": ["D. Lin", "X. Wu"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, vol. 2. Association for Computational Linguistics, 2009, pp. 1030\u20131038.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["G. Durrett", "D. Klein"], "venue": "Transactions of the Association for Computational Linguistics, vol. 2, pp. 477\u2013490, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint entity recognition and disambiguation", "author": ["G. Luo", "Z.N. Xiaojiang Huang", "Chin-Yew Lin"], "venue": "Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing. Association for Computational Linguistics, 2015, pp. 879\u2013888.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Symbolic and neural learning for named-entity recognition", "author": ["G. Petasis", "S. Petridis", "G. Paliouras", "V. Karkaletsis", "S. Perantonis", "C. Spyropoulos"], "venue": "Symposium on Computational Intelligence and Learning. Chios, Greece: Citeseer, 2000, pp. 58\u201366.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Named entity recognition with long shortterm memory", "author": ["J. Hammerton"], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL, vol. 4. Association for Computational Linguistics, 2003, pp. 172\u2013175.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["J.P. Chiu", "E. Nichols"], "venue": "Transactions of the Association for Computational Linguistics, vol. 4, pp. 357\u2013370, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "arXiv preprint arXiv:1603.01360, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end sequence labeling via bidirectional lstm-cnns-crf", "author": ["X. Ma", "E. Hovy"], "venue": "arXiv preprint arXiv:1603.01354, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Charner: Characterlevel named entity recognition", "author": ["O. Kuru", "O.A. Can", "D. Yuret"], "venue": "Proceedings of The 26th International Conference on Computational Linguistics, 2016, pp. 911\u2013921.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE transactions on neural networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "The 30th International Conference on Machine Learning, vol. 28, Atlanta, USA, 2013, pp. 1310\u20131318.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Proceedings of 2005 IEEE International Joint Conference on Neural Networks, vol. 4. Montreal, QC, Canada: IEEE, 2005, pp. 2047\u20132052.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. rahmand Mohamed", "G. Hinton"], "venue": "Proceedings of 2013 IEEE international conference on acoustics, speech and signal processing. Vancouver, BC, Canada: IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["C. dos Santos", "V. Guimaraes", "a. R. d. J. RJ Niter\u00f3i"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop, 2015, pp. 25\u201333.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of The Eighteenth International Conference on Machine Learning, vol. 1, 2001, pp. 282\u2013289.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "A hybrid approach to word segmentation of Vietnamese texts", "author": ["P. Le-Hong", "T.M.H. Nguyen", "A. Roussanaly", "T.V. Ho"], "venue": "Language and Automata Theory and Applications, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2008, vol. 5196, pp. 240\u2013249.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Vietnamese named entity recognition using token regular expressions and bidirectional inference", "author": ["P. Le-Hong"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Dsktlab-ner: Nested named entity recognition in vietnamese text", "author": ["T.C.V. Nguyen", "T.S. Pham", "T.H. Vuong", "N.V. Nguyen", "M.V. Tran"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Vietnamese named entity recognition at vlsp 2016 evaluation campaign", "author": ["T.S. Nguyen", "L.M. Nguyen", "X.C. Tran"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Named entity recognition in vietnamese text", "author": ["T.H. Le", "T.T.T. Nguyen", "T.H. Do", "X.T. Nguyen"], "venue": "Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing, Hanoi, Vietnam, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The best system at CoNLL-2003 shared task is the work of [1] which achieved an F1 score of 88.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "After that, [2] surpassed them by using phrase features extracted from an external database.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "For instance, [3] trained a CRF model for joint-learning three tasks, including coreference resolution, entity linking, and NER, and achieved the state-of-the-art result on OntoNotes dataset.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "With a similar approach, [4] gained the best performance on CoNLL-2003 shared task dataset.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "The first neural network model is the work of [5] that used a feed-forward neural network with one hidden layer.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "After that, [6] used a long short-term memory network for this problem.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Recently, [7] used a convolution neural network over a sequence of word embeddings with a conditional random field on the top.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "[8] used bidirectional LSTM with CRF layer for joint decoding.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Instead of using hand-crafted feature as [8], [9] proposed a hybrid model that combined bidirectional LSTM with convolutional neural networks (CNN) to learn both character-level and word-level representations.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "Instead of using hand-crafted feature as [8], [9] proposed a hybrid model that combined bidirectional LSTM with convolutional neural networks (CNN) to learn both character-level and word-level representations.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "Unlike [9], [10] used bidirectional LSTM to model both character and word-level information.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "Unlike [9], [10] used bidirectional LSTM to model both character and word-level information.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "The work of [11] proposed a truly end-to-end model that used only word embeddings for detecting entities.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Approaching this problem at the character-level sequence, the LSTM-CRF model of [12] achieved the nearly state-ofthe-art results in seven languages.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Long short-term memory (LSTM) [13] is a variant of RNN which is designed to deal with these gradient vanishing and exploding problems [14], [15] when learning with long-range sequences.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "Long short-term memory (LSTM) [13] is a variant of RNN which is designed to deal with these gradient vanishing and exploding problems [14], [15] when learning with long-range sequences.", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "Long short-term memory (LSTM) [13] is a variant of RNN which is designed to deal with these gradient vanishing and exploding problems [14], [15] when learning with long-range sequences.", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "Thus, we utilize the bidirectional LSTM (Bi-LSTM) [16], [17] for both word and character-level systems.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "Thus, we utilize the bidirectional LSTM (Bi-LSTM) [16], [17] for both word and character-level systems.", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively [18], [9], [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively [18], [9], [11].", "startOffset": 138, "endOffset": 141}, {"referenceID": 10, "context": "For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively [18], [9], [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "Conditinal Random Field (CRF) [19] is a type of graphical model designed for labeling sequence of data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The tokenization process follows the method described in [20].", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "59%, which is competitive with the best participating system [21] in that shared task.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Teams Models Performances [21] ME 88.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "62 [22] ME 84.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "08 [23] LSTM 83.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "80 [24] CRF 78.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "There is one work [23] that applied deep learning approach for this task.", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "They used the implementation provided by [10].", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "Previous works have shown that CNN is very useful to extract these features [18], [9], [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "Previous works have shown that CNN is very useful to extract these features [18], [9], [11].", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "Previous works have shown that CNN is very useful to extract these features [18], [9], [11].", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "This paper demonstrates end-to-end neural network architectures for Vietnamese named entity recognition. Our best model is the combination of bidirectional Long ShortTerm Memory (Bi-LSTM), Convolutional Neural Network (CNN), Conditional Random Field (CRF), using pre-trained word embeddings as input, which achieves an F1 score of 88.59% on a standard test set. Our system is able to achieve a comparable performance to the first-rank system of the VLSP campaign without using any syntactic or hand-crafted features. We also give an extensive empirical study on using common deep learning models for Vietnamese NER, at both word and character level. Keywords-Vietnamese, named entity recognition, end-to-end", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}