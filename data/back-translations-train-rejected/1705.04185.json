{"id": "1705.04185", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "A First Empirical Study of Emphatic Temporal Difference Learning", "abstract": "In this paper we present the first empirical study of the emphatic temporal-difference learning algorithm (ETD), comparing it with conventional temporal-difference learning, in particular, with linear TD(0), on on-policy and off-policy variations of the Mountain Car problem. The initial motivation for developing ETD was that it has good convergence properties under \\emph{off}-policy training (Sutton, Mahmood \\&amp; White 2016), but it is also a new algorithm for the \\emph{on}-policy case. In both our on-policy and off-policy experiments, we found that each method converged to a characteristic asymptotic level of error, with ETD better than TD(0). TD(0) achieved a still lower error level temporarily before falling back to its higher asymptote, whereas ETD never showed this kind of \"bounce\". In the off-policy case (in which TD(0) is not guaranteed to converge), ETD was significantly slower.", "histories": [["v1", "Thu, 11 May 2017 13:52:52 GMT  (2609kb,D)", "https://arxiv.org/abs/1705.04185v1", "5 pages, Accepted to NIPS Continual Learning and Deep Networks workshop, 2016"], ["v2", "Fri, 12 May 2017 16:49:38 GMT  (2609kb,D)", "http://arxiv.org/abs/1705.04185v2", "5 pages, Accepted to NIPS Continual Learning and Deep Networks workshop, 2016"]], "COMMENTS": "5 pages, Accepted to NIPS Continual Learning and Deep Networks workshop, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["sina ghiassian", "banafsheh rafiee", "richard s sutton"], "accepted": false, "id": "1705.04185"}, "pdf": {"name": "1705.04185.pdf", "metadata": {"source": "CRF", "title": "A First Empirical Study of Emphatic Temporal Difference Learning", "authors": ["Sina Ghiassian", "Banafsheh Rafiee"], "emails": ["ghiassia@ualberta.ca", "rafiee@ualberta.ca", "rsutton@ualberta.ca"], "sections": [{"heading": null, "text": "In this paper, we present the first empirical study of the emphatic learning algorithm of the time difference (ETD) and compare it to conventional learning of the time difference, especially linear TD (0), on political and non-political variations of the mountain car problem. The original motivation for the development of the ETD was that it had good convergence characteristics during non-political training (Sutton, Mahmood & White 2016), but it is also a new algorithm for the case of politics. Both in our experiments on politics and outside politics, we found that each method converged to a characteristic asymptotic error level, with ETD (0) being better than TD (0). TD (0) temporarily reached an even lower error level before falling back to its higher \"bounce,\" whereas ETD never showed this kind of \"bounce.\""}, {"heading": "1 Emphatic Temporal Difference Learning", "text": "We consider the problem of learning the value function for a Markov decision-making process and a given policy. An agent and the environment interact in discrete time steps, t = 0, 1, 2,..., where the environment is in a state of St, the agent selects an action, and as a result the environment gives a reward Rt + 1 and a next state St + 1. States are presented to the agent as trait vectors. We try to find a parameter vector, in such a way that the internal product Rn is so large that the expected return E [Rt + 1 + \u03b3Rt + 2 + 2Rt + 3 + \u00b7 \u00b7 \u00b7 \u00b7 Mahn. We try to find a parameter vector: A \u00d7 S \u2192 [0, 1] is a policy for selecting future actions. In fact, all actions are selected by an alternative policy."}, {"heading": "2 Stability of On-policy TD with Variable \u03bb: A Counterexample", "text": "In this section, we show that although the initial motivation for the development of the ETD was that it exhibited good convergence characteristics in the context of non-political training (Yu 2015), it is also a different algorithm in the context of internal training. To highlight the difference between the two, we present a simple example for which TD (\u03bb) is not convergent in the context of internal training, but exhibits convergent characteristics. Surprisingly, there is no guarantee that TD (\u03bb) exhibits convergence characteristics in the context of internal training. Yu recently presented an example (personal communication) in which state policy training (Tsitsiklis & Van Roy 1997) does not converge. The example is a simple Markov decision-making process consisting of two states in which the system simply moves from one state to another within one cycle. The process begins in each of the states with equal probability."}, {"heading": "3 Fixed-policy Mountain Car Testbed", "text": "For our experimental study, we used a new variation of the mountain-car steering problem (Sutton & Barto 1998) to form a prediction problem. The original mountain-car problem has a two-dimensional space, a position (between -1.2 and -0.6) and a speed (between -0.07 and 0.07) with three actions, full throttle forward, full throttle backward and 0 throttle. Each episode starts at the bottom of a hill (a uniform random number between -0.4 and -0.6). The reward is -1 in all time steps until the car reaches its destination at the top of the hill, which ends the episode. The task is not discounted. Our variation of the mountain-car problem has a fixed target policy, which consists of always pushing in the direction of the speed and not pushing in any direction when the speed is 0. We call the new variation of the mountain-car problem, the fixed policy of the mountain-car problem as the value set to push the speed, which we apply any direction, if the speed is the one we use."}, {"heading": "4 On-Policy Experiments", "text": "To approximate a value function for this problem, we used the tile encoding (Sutton 1996) with 5 tilts, 4 x 4 tiles each. Each instance of the algorithm was initialized with a 0-weight vector and then ran for 500,000 episodes. The entire process was repeated for 50 rounds. To generate learning curves for each instance of the two methods, we calculated the error measurement at the end of each episode and determined the error measurement over each run. See Figure 1a. We also conducted a parameter study of the asymptotic performance of both methods. \u2212 To do this, we determined the error curves of the last 1% of episodes for each run and then calculated the average and standard error over all 50 runs. See Figure 1b. To compare the performance of the TD and the ETD, we first need to understand how their errors changed, as the number of episodes reduced errors to a higher result of the ETD, not a lower level."}, {"heading": "5 Off-Policy Experiments", "text": "In this case, the target policy was the same as the policy in the case of politics and the behavioural policy consisted of selecting a random action 10% of the time and acting 90% of the time according to the target policy. Again, different instances of each method were created with different step size parameters; each instance of the method was run for 500,000 episodes and the entire process was repeated 50 times; the learning curves for the case of politics outside politics are shown in Figure 2a. The results of the parameter study are shown in Figure 2b. Similar to the case of politics, each method had its advantages and disadvantages; the ETD achieved better asymptotic performance when converged; this is while the TD was able to use larger step size values compared to the ETD and therefore converted much faster (Figure 2a); the values of the step size of the ETD had to be set small (in order of 10 \u2212 7) in order to control the b method (compared to Figure 2b), which had only a large step size (in D)."}, {"heading": "6 Conclusion", "text": "We conducted the first systematic empirical study of the emphatic learning method of time difference and showed that it can be used for problems with a relatively large state area with promising results. Although ETD was originally proposed as a non-political method, it can also be used as a reliable algorithm for non-political actions. According to our results, ETD seems to be slow in non-political cases, but achieves better asymptotic performance both in non-political and non-political cases. Despite the fact that our experiments are limited to a variation of the mountain car problem, we believe that our observations can lead to a better understanding of both TD and ETD methods."}, {"heading": "Acknowledgments", "text": "The authors thank Huizhen Yu for her insights and in particular for providing the counter-example. We are grateful for funding from Alberta Innovates Technology Futures and the Natural Sciences and Engineering Research Council of Canada."}, {"heading": "7 Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Stability of On-policy TD with Variable \u03bb: A Counterexample", "text": "Suppose politics induces an irreducible Markov chain with transition matrix P\u03c0 and a unique invariant probability distribution \u00b5 (i.e., \u00b5 > P\u03c0 = \u00b5 >), let D = diag (\u00b5) and let \u03a6 be a characteristic matrix with linear independent columns, the key matrix associated with the TD (\u03bb) algorithm is A = \u03a6 > D (I \u2212 P\u03bb\u03c0) \u03a6, where P\u03bb\u03c0 is a substochastic matrix determined by P\u03c0, \u03bb and the discount factor \u03b3, and for a constant \u03bb [0, 1] matrix A is positively defined (see e.g. Tsitsiklis and Van Roy 1997), thereby ensuring the stability of the algorithm. This positive definition property is largely based on the fact that we have \u00b5 > Pthroughs < \u00b5 >, which is generally not true when it is a function of states."}], "references": [{"title": "Learning to Predict by the Methods of Temporal Differences", "author": ["R.S. Sutton"], "venue": "Machine learning 3(1):9-44.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding", "author": ["Sutton", "R .S."], "venue": "Advances in Neural Information Processing Systems:1038-1044.", "citeRegEx": "Sutton and .S.,? 1996", "shortCiteRegEx": "Sutton and .S.", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "An Emphatic Approach to the Problem of Off-policy Temporal-difference Learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "The Journal of Machine Learning Research.", "citeRegEx": "Sutton et al\\.,? 2015", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "An Analysis of Temporal-difference Learning with Function Approximation", "author": ["Tsitsiklis", "J .N.", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control 42(5):674-690.", "citeRegEx": "Tsitsiklis et al\\.,? 1997", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "Proceedings of the Conference on Computational Learning Theory.", "citeRegEx": "Yu,? 2015", "shortCiteRegEx": "Yu", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "In this section we show that although the initial motivation for developing ETD was that it has good convergence properties under off-policy training (Yu 2015), it is also a different algorithm under on-policy training.", "startOffset": 150, "endOffset": 159}], "year": 2017, "abstractText": "In this paper we present the first empirical study of the emphatic temporaldifference learning algorithm (ETD), comparing it with conventional temporaldifference learning, in particular, with linear TD(0), on on-policy and off-policy variations of the Mountain Car problem. The initial motivation for developing ETD was that it has good convergence properties under off -policy training (Sutton, Mahmood & White 2016), but it is also a new algorithm for the on-policy case. In both our on-policy and off-policy experiments, we found that each method converged to a characteristic asymptotic level of error, with ETD better than TD(0). TD(0) achieved a still lower error level temporarily before falling back to its higher asymptote, whereas ETD never showed this kind of \u201cbounce\u201d. In the off-policy case (in which TD(0) is not guaranteed to converge), ETD was significantly slower. 1 Emphatic Temporal Difference Learning We consider the problem of learning the value function for a Markov decision process and a given policy. An agent and environment interact at discrete time steps, t = 0, 1, 2, . . ., at each of which the environment is in a state St, the agent selects an action At and as a result the environment emits a reward Rt+1 and a next state St+1. States are represented to the agent as feature vectors \u03c6t = \u03c6(St) \u2208 R. We seek to find a parameter vector, \u03b8t \u2208 R such that the inner product \u03b8> t \u03c6t approximates the expected return E [ Rt+1 + \u03b3Rt+2 + \u03b3 Rt+3 + \u00b7 \u00b7 \u00b7 | At:\u221e \u223c \u03c0 ] , where \u03c0 : A \u00d7 S \u2192 [0, 1] is a policy for selecting the future actions. In fact, all actions are selected by an alternate policy \u03bc. If \u03c0 = \u03bc, then the training is called on-policy, whereas if the two policies are different the training is called off-policy. We consider the special case of the emphatic temporal difference learning algorithm (ETD) in which bootstrapping is complete (\u03bb(s) = 0,\u2200s) and there is no discounting (\u03b3(s) = 1,\u2200s). Studying TD and ETD methods with complete bootstrapping is suitable because in this case the differences between them are maximized. As \u03bb approaches 1, the methods behave more similarly up to the point where they become equivalent when \u03bb = 1. By setting \u03bb = 0 and \u03b3 = 1, the ETD algorithm can be 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. ar X iv :1 70 5. 04 18 5v 2 [ cs .A I] 1 2 M ay 2 01 7 completely described by: \u03b8t+1 . = \u03b8t + \u03b1\u03c1tFt ( Rt+1 + \u03b8 T t \u03c6t+1 \u2212 \u03b8 t \u03c6t ) \u03c6t, Ft . = \u03c1t\u22121Ft\u22121 + 1, with F0 . = 1, \u03c1t . = \u03c0(At|St) \u03bc(At|St) , where \u03b1 > 0 is a step size parameter. F is the followon trace according to which the update at each time step is emphasized or de-emphasized. TD is obtained by removing the F from the first equation. Because of F , ETD is different from TD even in the on-policy case in which \u03c1 is always 1. For a thorough explanation of ETD see (Sutton, Mahmood & White 2016). 2 Stability of On-policy TD with Variable \u03bb: A Counterexample In this section we show that although the initial motivation for developing ETD was that it has good convergence properties under off-policy training (Yu 2015), it is also a different algorithm under on-policy training. To emphasize the difference between the two, we present a simple example for which TD(\u03bb) is not convergent under on-policy training but ETD is. It has long been known that TD(\u03bb) converges with any constant value of \u03bb under on-policy training (Tsitsiklis & Van Roy 1997). Surprisingly, TD(\u03bb) is not assured to converge with varying \u03bb even under on-policy training. Yu has recently presented a counterexample (personal communication) with state dependent \u03bb for which on-policy TD(\u03bb) is not convergent. The example is a simple Markov decision process consisting of two states in which the system simply moves from one state to another in a cycle. The process starts in each of the states with equal probability. Let \u03bb(S1) = 0 and \u03bb(S2) = 1, \u03c6(S1) = (3, 1) and \u03c6(S2) = (1, 1) and \u03b3 = 0.95. As shown below, the TD(\u03bb) key matrix for this problem is not positive definite. Moreover, both eigenvalues of the key matrix have negative real parts and thus TD(\u03bb) diverges in this case. S1 S2 Key matrix = ( \u22120.4862 0.1713 \u22120.7787 0.0738 ) This is while ETD is convergent under both off-policy and on-policy training with variable \u03bb. This example appears in more detail in the supplementary material. 3 Fixed-policy Mountain Car Testbed For our experimental study, we used a new variation of the mountain car control problem (Sutton & Barto 1998) to form a prediction problem. The original mountain car problem has a 2-dimensional space, position (between -1.2 and 0.6), and velocity (between -0.07 and 0.07) with three actions, full throttle forward, full throttle backward, and 0 throttle. Each episode starts around the bottom of a hill (a uniform random number between -0.4 and -0.6). The reward is -1 on all time steps until the car pasts its goal at the top of the hill, which ends the episode. The task is undiscounted. Our variation of the mountain car problem has a fixed target policy which is to always push towards the direction of the velocity and not to push in any direction when the velocity is 0. We call the new variation of the mountain car problem, the fixed-policy mountain car testbed. The performance measure we used is an estimation of the mean squared value error (MSVE) which reflects the mean squared difference between the true value function and the estimated value function, weighted according to how often each state is visited in the state space following the behavior policy:", "creator": "LaTeX with hyperref package"}}}