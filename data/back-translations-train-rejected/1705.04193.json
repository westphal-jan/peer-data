{"id": "1705.04193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Nonnegative Matrix Factorization with Transform Learning", "abstract": "Traditional NMF-based signal decomposition relies on the factorization of spectral data which is typically computed by means of the short-time Fourier transform. In this paper we propose to relax the choice of a pre-fixed transform and learn a short-time unitary transform together with the factorization, using a novel block-descent algorithm. This improves the fit between the processed data and its approximation and is in turn shown to induce better separation performance in a speech enhancement experiment.", "histories": [["v1", "Thu, 11 May 2017 14:12:23 GMT  (308kb,D)", "http://arxiv.org/abs/1705.04193v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dylan fagot", "c\\'edric f\\'evotte", "herwig wendt"], "accepted": false, "id": "1705.04193"}, "pdf": {"name": "1705.04193.pdf", "metadata": {"source": "CRF", "title": "NONNEGATIVE MATRIX FACTORIZATION WITH TRANSFORM LEARNING", "authors": ["Dylan Fagot", "C\u00e9dric F\u00e9votte", "Herwig Wendt"], "emails": ["firstname.lastname@irit.fr"], "sections": [{"heading": null, "text": "Index Terms - Nonnegative Matrix Factorization (NMF), Transform Learning, Single Channel Source Separation"}, {"heading": "1. INTRODUCTION", "text": "Nonnegative matrix factorization (NMF) has evolved into a privileged approach to spectral decomposition in several areas such as remote sensing and audio signal processing, leading to state-of-the-art results in source separation [1] or music transcription [2] in the latter field. V-RM-N + nonnegative data is typically the spectrogram | X-or-X-Z-2 of a temporal Signaly-RT, where X-RM-N is a short-term Fourier transformation (STFT) of y, | | denotes the initial absolute value and here denotes stationary exposures. NMF produces the approximate factorization V-RM-WH, (1) where W-RM-K + is a nonnegative matrix called a dictionary containing the spectral pattern characteristic of the data, while H-RK-N + is the nonnegative matrix containing the activation coefficients of the data."}, {"heading": "2. NMF MEETS TRANSFORM LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Learning a short-time unitary transform", "text": "As a first step, we propose in this paper to gently deviate from the traditional STFT setting by limiting the transformation \u03c6 (y) as a short-term uniform transformation, as well as the STFT. Let us use Y-RM \u00b7 N to denote the matrix containing adjacent and overlapping short periods of size M of y and the uniform complex Fourier matrix with coefficients \u03a6FT-CM \u00b7 M. (FT) qm = exp (j2\u03c0 (q \u2212 1) (m \u2212 1) / M). Among these notations, the power spectrogram of variation is simply given as | \u03a6FTY-2. As such, the traditional NMF can be defined as min W, HD-FTY-2 | WH) s.t. W-R-0, H-R-0, (3) where the notation A-R-0 indicates the negativity of A. We implicitly propose to loosen the pre-fixed form of transformation."}, {"heading": "2.2. Connection to other works", "text": "TL-NMF is inspired by the work of Ravishankar & Bresler [5] on the topic of frugal learning. Faced with a collection of data examples Y (such as images collected in the columns of Y), their workar Xiv: 170 5.04 193v 1 [cs.L G] 11 May 201 7Algorithm 1: TL-NMF Input: Y, \u03c4 Output: \u03a6, W, H s.t. | \u03a6Y: 2 \u2248 WH Initialize, W and H, while > \u03c4 doW (((WH). \u2212 2) HT (WH). \u2212 1 HT (WH). \u00b7 troHT% MM-Update [10]. \u00b7 H-T ((WH)."}, {"heading": "3. ALGORITHM", "text": "Like the objective function in (3), the objective function C (3) is not convex and the returned solution depends on the initialization. The blocks are the individual variables W, H and \u03a6, which in turn are updated until a convergence criterion is met. We apply to W and H the standard IS-NMF multiplicative method, which, for example, [10] can be derived from a method of Majorization Minimization. Now, let us turn our attention to updating the convergence process. We propose to apply a method of gradient descension with a selection of line search steps followed by a projection on the uniform constraint. The main advantage of this approach is that it yields an efficient but simple algorithm to find a uniform update of convergence V. The gradient of the objective function with respect to (w.r.t) convergence is given by the approach [11]."}, {"heading": "4. MUSIC DECOMPOSITION EXPERIMENT", "text": "In this section, we report on the results obtained with the proposed algorithm for the decomposition of real audio data y (t), consisting of a 23-page extract from Mamavatu by Susheela Raman, sampled down to fs = 16kHz. Y is constructed with 40ms long, 50% overlapping temporal segments, which are provided with a sinusoidal bell. This construction leads to M = 640 and N = 1191. The behavior of TL-NMF is compared with traditional IS-NMF, which we remember only TL-NMF with fixed transformation systems. Both algorithms are executed with the same holding threshold as the quantum steps = 10 \u2212 5 and the arbitrary decomposition rank K = 6.Figure 1 shows the objective functional values w.r.t. Iterations for the two approaches representing iterations for the two approaches are initialized with the same starting point, so that they reset the objective value = Iteri."}, {"heading": "5. SUPERVISED SOURCE SEPARATION", "text": "In the previous section, we reported on exploratory results that show how effective TL-NMF is in learning a transformation. We will now investigate whether learning an adaptive transformation is actually useful for source separation. To this end, we will consider a supervised NMF-based separation setting that follows the approach of [12]. Below, we will deal with the separation of speech and noise, but the method can be applied to any sound class."}, {"heading": "5.1. Principle", "text": "We assume that speech and sound training data (t) and yn (t) are made available to us, from which we form short-term matrices Ys and Yn of the sizes M \u00b7 Ns and M \u00b7 Nn, as described in Section 2.1. Given a noisy voice recording y (t) with short-term matrix Y, the traditionally monitored NMF amounts to estimating activation matrices Hs and Hn such as V \u2248 WsHs + WnHn, (7) it is subject to the scarcity of Hs and Hn, with V = | \u03a6FTY | 2, Ws = \u2012 FTYs | 2, Wn = \u2012 FTYn and Hs, as well as the time source and noise estimates being reconstructed in a second step by so-called Viennese filtering [3], based on the spectrogram estimates HIS = WsHs and V = WnHIS. In this section, we will again replace this method with an optimal form of the Y transformation within the given process."}, {"heading": "5.2. Algorithm", "text": "Name Ytrain = [Ys, Yn], Xtrain = \u03a6Ytrain, W = | Xtrain | 2, H = [HTs, H T n] T and V = WH. Given that W, H can be updated with algorithm 2: Supervised TL-NMF Input: Y, Ytrain, \u03c4 Output: E, H initialize, H while > \u03c4 doV = | \u03a6Y | 2, W = | \u03a6Ytrain | 2 H. \"H\" W \"((((WH)) - 2\" V \") WT (WH) - 1 + [\u03bbs1N \u00b7 Ns, \u03bbn1N \u00b7 Nn] TCompute\" and vice versa as in Section 5.2. \"The gradient of the objective function (1) can be derived with the first criterion end multiplication rules derived from the majorization minimization as in [10]. We will again use a gradient descending approach for updating the line\" The V \"function can be expressed with the first gradient object (8)."}, {"heading": "5.3. Speech enhancement experiment", "text": "We are looking at clean speech and sound data from the TIMIT corpus [13] and the CHIME challenge = 1059. For speech training data (t) we are using all expressions except the first one in the Zug / fcjf0 directory (a total of about 21 s). For sound training data yn (t) we are using 30s of the file BGD 150204 010 BUS.CH1.wav, which contains a noise recorded on a bus. A simulated mixed signal y (t) of the duration 3s is generated by mixing the remaining speech expressions with another segment of the sound file (as such, the test data is not included in the training data) using a signal-to-noise ratio of \u2212 10dB. The audio files sampling frequency is fs = 16kHz and short-term matrices Y, Ys and Yn are constructed using 40ms-long, 50% - overlapping segments = 14.9, as in section 659 dimensions = N."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "Specifically, we have proposed a block coordinate descendancy algorithm that, together with the dictionary W and the activation matrix H. To our knowledge, the proposed algorithm is the first operational approach for learning transformation in the context of NMF. Our preliminary experiments with real audio data show that automatic adjustment of transformation to the signal pays off when looking for latent factors that accurately represent the data. In particular, improving data adjustment enables source separation that compares very favorably with the state of the art. Note that while our presentation focused on the processing of audio data, the approach can be adapted to many other settings in which NMF is applied to pre-processed data. Future work will also include investigating the effects of initialization of culture, the influence of K on the learned form of value, and the problems in which NMF applies good to overcoming inconsistency."}, {"heading": "7. REFERENCES", "text": "[1] P. Smaragdis, C. Fe \u0301 votte, G. Mysore, N. Mohammadiha, and M. Hoffman, \"Static and Dynamic Source Separation Using Nonnegative Factorizations: A unified view,\" IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 66-75, May 2014. [2] E. Vincent, N. Bertin, and R. Badeau, \"Harmonic and inharmonic nonnegative matrix factorization for polyphonic pitch transcription,\" in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2008. [3] C. Fe \u0301 votte votte, N. Bertin, and J.-L. Durrieu \"Nonnegative matrix factorization with the itakura-saito divergence: With application to music analysis,\". Neural Computation, vol. 21, pp."}], "references": [{"title": "Static and dynamic source separation using nonnegative factorizations: A unified view", "author": ["P. Smaragdis", "C. F\u00e9votte", "G. Mysore", "N. Mohammadiha", "M. Hoffman"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 66\u201375, May 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Harmonic and inharmonic nonnegative matrix factorization for polyphonic pitch transcription", "author": ["E. Vincent", "N. Bertin", "R. Badeau"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonnegative matrix factorization with the itakura-saito divergence : With application to music analysis", "author": ["C. F\u00e9votte", "N. Bertin", "J.-L. Durrieu"], "venue": "Neural Computation, vol. 21, no. 3, pp. 793\u2013830, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal cost function and magnitude power for NMF-based speech separation and music interpolation", "author": ["B. King", "C. F\u00e9votte", "P. Smaragdis"], "venue": "Proc. IEEE International Workshop on Machine Learning for Signal Processing (MLSP), 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning sparsifying transforms", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 5, pp. 1072\u20131086, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep NMF for speech separation", "author": ["J.L. Roux", "J.R. Hershey", "F. Weninger"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural network alternative to non-negative audio models", "author": ["P. Smaragdis", "S. Venkataramani"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Low-rank time-frequency synthesis", "author": ["C. F\u00e9votte", "M. Kowalski"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-resolution signal decomposition with time-domain spectrogram factorization", "author": ["H. Kameoka"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for nonnegative matrix factorization with the \u03b2-divergence", "author": ["C. F\u00e9votte", "J. Idier"], "venue": "Neural Computation, vol. 23, no. 9, pp. 2421\u20132456, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimization algorithms exploiting unitary constraints", "author": ["J.H. Manton"], "venue": "IEEE Transactions on Signal Processing, vol. 50, no. 3, pp. 635\u2013650, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Supervised and semi-supervised separation of sounds from single-channel mixtures", "author": ["P. Smaragdis", "B. Raj", "M.V. Shashanka"], "venue": "Proc. International Conference on Independent Component Analysis and Signal Separation (ICA), 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Timit acoustic-phonetic continuous speech corpus LDC93S1", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett"], "venue": "Philadelphia: Linguistic Data Consortium, Tech. Rep., 1993.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. F\u00e9votte"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 1462\u20131469, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In the latter field, it has led to state-of-the-art results in source separation [1] or music transcription [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "In the latter field, it has led to state-of-the-art results in source separation [1] or music transcription [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "Used with power spectral data, it is known to underly a variance-structured Gaussian composite model that is relevant to the representation of audio signals [3] and has proven an efficient choice for audio source separation, e.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": ", [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "TL-NMF is inspired by the work of Ravishankar & Bresler [5] on learning sparsifying transforms.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "Initialize \u03a6, W and H while > \u03c4 do W\u2190W \u25e6 ((WH) \u25e6\u22122\u25e6|\u03a6Y|\u25e62)HT (WH)\u25e6\u22121HT % MM update [10] H\u2190 H \u25e6 W T ((WH)\u25e6\u22122\u25e6|\u03a6Y|\u25e62) WT (WH)\u25e6\u22121 % MM update [10] Normalize W and H to remove scale ambiguity Compute \u03b3 and \u03a9 as in Section 3 \u03a6\u2190 \u03c0 (\u03a6 + \u03b3\u03a9) Compute stopping criterion as in Eq.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Initialize \u03a6, W and H while > \u03c4 do W\u2190W \u25e6 ((WH) \u25e6\u22122\u25e6|\u03a6Y|\u25e62)HT (WH)\u25e6\u22121HT % MM update [10] H\u2190 H \u25e6 W T ((WH)\u25e6\u22122\u25e6|\u03a6Y|\u25e62) WT (WH)\u25e6\u22121 % MM update [10] Normalize W and H to remove scale ambiguity Compute \u03b3 and \u03a9 as in Section 3 \u03a6\u2190 \u03c0 (\u03a6 + \u03b3\u03a9) Compute stopping criterion as in Eq.", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "For instance, [6] considers a discriminative NMF setting and [7] studies nonnegative auto-encoders.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "For instance, [6] considers a discriminative NMF setting and [7] studies nonnegative auto-encoders.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Finally, note that TL-NMF still operates in a transformed domain and is not directly related to synthesis-based NMF models in which the raw data y(t) is modeled as y(t) = \u2211 k ck(t) where the spectrogram of ck(t) is penalized so as to be closely rank-one [8, 9].", "startOffset": 254, "endOffset": 260}, {"referenceID": 8, "context": "Finally, note that TL-NMF still operates in a transformed domain and is not directly related to synthesis-based NMF models in which the raw data y(t) is modeled as y(t) = \u2211 k ck(t) where the spectrogram of ck(t) is penalized so as to be closely rank-one [8, 9].", "startOffset": 254, "endOffset": 260}, {"referenceID": 9, "context": ", [10], that can be derived from a majorization-minimization procedure.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "We propose to use a gradient-descent procedure with a line-search step selection followed by a projection onto the unitary constraint, following the approach of [11].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "A suitable step-size \u03b3 is then chosen according to the Armijo rule so that the projection \u03c0 (\u03a6 + \u03b3\u03a9) of the updated transform onto the unitary constraint induces a significant decrease of the objective function [11].", "startOffset": 211, "endOffset": 215}, {"referenceID": 11, "context": "To this end, we consider a supervised NMF-based separation setting that follows the approach of [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "subject to sparsity of Hs and Hn, where V = |\u03a6FTY|, Ws = |\u03a6FTYs|, Wn = |\u03a6FTYn| [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Wiener filtering [3], based on the spectrogram estimates V\u0302s = WsHs and V\u0302n = WnHn.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "multiplicative rules derived from majorization-minimization as in [10].", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "(10), we again use a line-search step selection in the steepest natural gradient direction followed by a projection, like in Section 3 and following [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "We consider clean speech and noise data from the TIMIT corpus [13] and the CHIME challenge, respectively.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Source separation performance was assessed using the standard BSS eval criteria [14].", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "Traditional NMF-based signal decomposition relies on the factorization of spectral data which is typically computed by means of the short-time Fourier transform. In this paper we propose to relax the choice of a pre-fixed transform and learn a short-time unitary transform together with the factorization, using a novel block-descent algorithm. This improves the fit between the processed data and its approximation and is in turn shown to induce better separation performance in a speech enhancement experiment.", "creator": "LaTeX with hyperref package"}}}