{"id": "1705.04228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Incremental Learning Through Deep Adaptation", "abstract": "Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20%) in the number of required parameters while performing on par with more costly fine-tuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.", "histories": [["v1", "Thu, 11 May 2017 15:04:10 GMT  (554kb,D)", "http://arxiv.org/abs/1705.04228v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amir rosenfeld", "john k tsotsos"], "accepted": false, "id": "1705.04228"}, "pdf": {"name": "1705.04228.pdf", "metadata": {"source": "CRF", "title": "Incremental Learning Through Deep Adaptation", "authors": ["Amir Rosenfeld"], "emails": ["amir@cse.yorku.ca", "tsotsos@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object recognition [8], speech recognition [10], medical image analysis [19] - and much more - it is still the case that each model needs to be trained for each new task. Given two tasks such as classifying images from different domains, it is natural that solutions are expected to predict the class of an object in an image, but it is clear that each would require a different architecture or calculation for a set of related tasks such as classifying images from different domains."}, {"heading": "2 Related Work", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "3 Approach", "text": "Let's start with some notation. Let's make T a task to be learned. Specifically, let's use a deep Convolutionary Neural Network (DCNN) to learn a classifier to solve T. Most modern DCNN's follow a common structure: for each x-input, the DCNN calculates a representation of the input by traversing it through a series of l-layers."}, {"heading": "3.1 Adapting Representations", "text": "Suppose that we get two filters for this layer, which will enable us to search for a new user who is engaged in the search for a new user who is familiar with a new application principle, who is familiar with a new application principle, who is familiar with a new application principle, whose application principles are involved with the new application principles, who are familiar with the new application principles, who are familiar with the new application principles of the network, and whose application principles are involved with the new application principles of the new application principles, who are involved with the new application principles and the new application principles, who are familiar with the new application principles and the new application principles, who are familiar with the new application principles and the new application principles, who are familiar with the new application principles and the new application principles, who are familiar with the new application principles and the new application principles, who are familiar with the new application principles and the new application principles,"}, {"heading": "3.1.1 Multiple Controllers", "text": "The above description mentions a base network and a controller network. However, any number of controller networks can be connected to a single base network, with independently trained controller modules. The amortized number of parameters per task decreases inversely with the number of additional controllers. For example, if we construct 7 controllers using a base network, we need (1 + 0.22 \u043c 7) \u00b7 P = 2.54 \u00b7 P parameters, where P is the number for the base network alone. If each network had been independently trained, the total number of parameters would be 8 \u00b7 P, i.e. we use a ratio of 31% of the number of required parameters, each network would have been independently trained. In this case \u03b1 is extended to a hot vector of values determined by another subnetwork, whereby each controller network may have different effects on the final output (see Refsub: A-Unified-Network)."}, {"heading": "4 Experiments", "text": "We conduct several experiments to verify our method and examine various aspects of its behavior. We start by setting a baseline performance by training a separate module for each benchmark dataset (4.1.1). Next, we evaluate the benefits of transferring learning between dataset pairs (4.2). We then learn several variants of a single network that is able to classify images from all datasets simultaneously with only a modest increase in the number of parameters required and compare them with more expensive alternatives (4.3). Finally, we show how to expand this network by the ability to recognize the input domain and output a proper classification (4.3.4)."}, {"heading": "4.1 Datasets and Evaluation", "text": "Our evaluation protocol is similar to that of [2] in the following ways: First, we test our network on a variety of data sets, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23], GTSR [29], Omniglot [22], plankton image files [3], Human Sketch Dataset [6], and SVHN [24]. We reduce all images to a common size of 64 \u00d7 64 pixels, duplicate the gray images so that they have 3 channels, like the RGB datasets. We chop off all images by subtracting the mean pixel value and divide them by the variance per channel. This is done for each dataset individually. As [2], we select 80% for training and 20% for validation in datasets."}, {"heading": "4.1.1 Baselines", "text": "All experiments are carried out with the Adam Optimizer [13], with an initial learning rate of 1e-3 or 1e-4, depending on a few trial periods for each data set. The learning rate is halved after each 10 epochs. Most networks converge within the first 10-20 epochs, with mostly negligible improvements thereafter. Top-1 accuracy (%) is summarized in Table 2. In [2] a newer architecture was chosen, namely the ResNet-38 [12]. We compare the basic performance of our chosen architecture with that of [2] with ResNet-38. Our main goal is not to achieve state-of-the-art performance on independently acquired data sets. Nevertheless, the relative performance justifies the choice of VGG-B. Admittedly, it is not unlikely that small implementation details (e.g. the learning rate selection) are the cause for the performance differences between the GG-B groups."}, {"heading": "4.2 Transferability", "text": "We aim to use a network trained on one data set to work well on others. As an indicator of the representative power of the characteristics of each independently trained network, we test performance on other data sets, using N for fine-tuning. We test 3 different scenarios. FT load: fine-tuning only the last layer, leaving the rest frozen. FT-Full: fine-tuning all layers of the network. FT-Full-bn-off: same as ft-full, but freezing the parameters of the batch normalization layers - this proved useful in some cases. We define the portability of a source data set S w.r.t a target data set T as a top-1 accuracy achieved by fine-tuning a network trained on S to work on T. The results are summarized in Fig. 2: The figure shows some interesting phenomena."}, {"heading": "4.3 Control Networks", "text": "We tested the performance of controller networks based on base networks, which were initially trained on plankton or Caltech-256 datasets, asking how to initialize the weights of a control module. We tested several options. (1) Put W on an identity matrix (diagonal), corresponding to the controller module, which assumes a state that effectively mimics the behavior of the base network. (2) Put W on random noise (random). (3) Train an independent network from scratch for the new task; set W on the best possible linear approximation of the new weights with the base weights (linear _ approx). To test the best alternative of the three, we trained Nsketch \u2192 caltech256 for an epoch with each initialization and observed the loss function. Each experiment was repeated 5 times and the result averaged."}, {"heading": "4.3.1 Starting from a Randomly Initialized Base Network", "text": "To test the effectiveness of our method, we tested how well it could work without prior knowledge, e.g. with a randomly initialized base network. The total number of parameters for this architecture is 12M. However, since 10M was randomly initialized and only the controller modules and fully connected layers were learned, the effective number is actually 2M. We summarize the results in Tab. 2. It is noteworthy that the results of this initialization worked surprisingly well; the mean top-1 precision achieved by this network was 76.3%, slightly worse than Ncaltech \u2212 256 (79.9%), which is better than the initialization with Ndaimler, resulting in an average accuracy of 75%."}, {"heading": "4.3.2 Pre-trained Networks", "text": "We are now reviewing how well a network can function as a base network after seeing numerous training examples. We call Nimagenet the VGG-B architecture preschooled on the ImageNet [25] dataset. We train control networks for each of the 8 datasets and report the results in Table 2. This improves average performance by a significant amount (83.7% to 86.5%), but for both Sketch and Omniglot the performance is in favor of Nsketch. Note that these are the only two areas of strictly unnatural images. On Caltech256 we see an improvement from 88.2% (training from scratch) to 92.2%. Fine-tuning all parameters of VGG-B yields slightly better results, with an average accuracy of 87.7% compared to 86.5% achieved by control networks but requiring x5 parameters."}, {"heading": "4.3.3 Multiple Base Networks", "text": "Ideally, a good base network should have features that are sufficiently general for a controller network to use for any target task. In reality (and as we can see from the performance with only a single base network), this is not necessarily the case. In order to use two base networks simultaneously, we implemented a dual-controlled network using both Ncaltech \u2212 256 and Nsketch and connecting to these controller networks. Outputs of the functional parts of the resulting subnetworks were concatenated before the fully connected layer, resulting in exactly the same performance as Nsketch alone. However, by using selected controller modules per task group, we can dramatically improve the results: for each data set, the maximum performance network is the basis for the control module; i.e. we use the pre-designed VGG-B for all but Omniglot & Sketch. For the latter two, we use Netch as the base network."}, {"heading": "4.3.4 A Unified Network", "text": "Finally, we test the ability to form a single network that can determine the domain of an image as well as classify it. We train a classifier to output the dataset from which an image originates, using training images from the 8 different datasets. This turns out to be an extremely simple task compared to the other classification tasks, and the network (including the VGG-B architecture [28]) quickly learns to work with 100% accuracy. Armed with this dataset decider, named Ndc, we expand Nsketch so that for each input image, from each dataset Di, we set the controller scalar \u03b1i each Nsketch \u2192 Di to 1 if and only if Ndc otherwise considers the image to be from Di and to 0, creating a network that applies the correct controllers to each input image and classifies it within its own domain."}, {"heading": "5 Conclusions", "text": "We have presented a method to adapt an existing network to new tasks while maintaining the existing representation completely. Our method is able to approximate the average performance of complete learning, although it requires a fraction of the parameters (about 22% vs. 100%) for each newly learned task, provided you select a base network correctly and exceed the performance when using controller parameters based on two networks. Built into our method is the ability to easily switch the representation between the different learned tasks, allowing a single network to work seamlessly in different areas. We find it surprising that using combinations of existing representations produces results that are almost as useful for other tasks as if the entire network were being trained from scratch. The control parameter \u03b1 can be cast as a real vector, allowing a smooth transition between representations of different tasks. An example of the effect of such a smooth transition can be seen in Figure 3 (c), where learning tasks are already used to interpolarize between existing ones, allowing for a more linear representation of existing ones."}], "references": [{"title": "Integrated perception with recurrent multi-task neural networks", "author": ["H. Bilen", "A. Vedaldi"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Universal representations:The missing link between faces, text, planktons, and cat breeds, 2017", "author": ["Hakan Bilen", "Andrea Vedaldi"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Planktonset 1.0: Plankton imagery data collected from fg walton smith in straits of florida from 2014\u201306-03 to 2014\u201306-06 and 8  used in the 2015 national data science bowl (ncei accession 0127422)", "author": ["Robert K Cowen", "S Sponaugle", "K Robinson", "J Luo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In Icml,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["David Eigen", "Rob Fergus"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "How do humans sketch objects", "author": ["Mathias Eitz", "James Hays", "Marc Alexa"], "venue": "ACM Trans. Graph.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Catastrophic forgetting in connectionist networks", "author": ["Robert M French"], "venue": "Trends in cognitive sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Caltech-256 object category dataset", "author": ["Gregory Griffin", "Alex Holub", "Pietro Perona"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Learning without Forgetting", "author": ["Zhizhong Li", "Derek Hoiem"], "venue": "CoRR, abs/1606.09282,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A survey on deep learning in medical image analysis", "author": ["Geert Litjens", "Thijs Kooi", "Babak Ehteshami Bejnordi", "Arnaud Arindra Adiyoso Setio", "Francesco Ciompi", "Mohsen Ghafoorian", "Jeroen AWM van der Laak", "Bram van Ginneken", "Clara I S\u00e1nchez"], "venue": "arXiv preprint arXiv:1702.05747,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Cross-stitch Networks for Multi-task Learning", "author": ["Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert"], "venue": "CoRR, abs/1604.03539,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "An experimental study on pedestrian classification", "author": ["Stefan Munder", "Dariu M Gavrila"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "author": ["Johannes Stallkamp", "Marc Schlipsing", "Jan Salmen", "Christian Igel"], "venue": "Neural networks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Face alignment by coarse-to-fine shape searching. In CVPR, pages 4998\u20135006", "author": ["Shizhan Zhu", "Cheng Li", "Chen Change Loy", "Xiaoou Tang"], "venue": "IEEE Computer Society,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 170, "endOffset": 173}, {"referenceID": 9, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 194, "endOffset": 198}, {"referenceID": 17, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 222, "endOffset": 226}, {"referenceID": 27, "context": "Some examples include facial landmark localization [30], semantic segmentation [11], 3D-reasoning [5], object and part detection [1] and others.", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "Some examples include facial landmark localization [30], semantic segmentation [11], 3D-reasoning [5], object and part detection [1] and others.", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "Some examples include facial landmark localization [30], semantic segmentation [11], 3D-reasoning [5], object and part detection [1] and others.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "While all of these train different tasks on the same dataset, the recent work of [2] explores the ability of a single network to perform tasks on various image classification datasets.", "startOffset": 81, "endOffset": 84}, {"referenceID": 19, "context": "Our work bears some resemblance to that of [21], where two networks are trained jointly, with additional \u201ccross-stitch\u201d units, allowing each layer from one network to have as additional input linear combinations of outputs from a lower layer in another.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Adding a new ability to a neural net often results in so-called \u201ccatastrophic forgetting\u201d [7], hindering the network\u2019s ability to perform well on old tasks.", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "The simplest way to overcome this is by fixing all parameters of the network and using the output of its penultimate layer as a feature extractor, upon which a classifier may be trained [4, 27].", "startOffset": 186, "endOffset": 193}, {"referenceID": 24, "context": "The simplest way to overcome this is by fixing all parameters of the network and using the output of its penultimate layer as a feature extractor, upon which a classifier may be trained [4, 27].", "startOffset": 186, "endOffset": 193}, {"referenceID": 7, "context": "While guaranteed to leave the old performance unaltered, it is observed to yield results which are substantially inferior to fine-tuning the entire architecture [8].", "startOffset": 161, "endOffset": 164}, {"referenceID": 16, "context": "The work of [18] provides a succinct taxonomy of various variants of such methods.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "In [14], the learning rate of neurons is lowered if they are found to be important to the old task.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "It is also possible to drop the \u03a6CN term, if the network is fully convolutional, as in [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 158, "endOffset": 162}, {"referenceID": 21, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 172, "endOffset": 176}, {"referenceID": 26, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 184, "endOffset": 188}, {"referenceID": 20, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 199, "endOffset": 203}, {"referenceID": 2, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 228, "endOffset": 231}, {"referenceID": 5, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 255, "endOffset": 258}, {"referenceID": 22, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 268, "endOffset": 272}, {"referenceID": 1, "context": "Like [2] , we select 80% for training and 20% for validation in datasets where no fixed split is provided.", "startOffset": 5, "endOffset": 8}, {"referenceID": 25, "context": "Our network architecture is the B architecture described in [28].", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "As in [2] we include all the character categories in train and test time.", "startOffset": 6, "endOffset": 9}, {"referenceID": 15, "context": "The Animals with Attributes dataset [17] was excluded since at the time of writing of this paper, copyright issues prevented accessing the benchmark\u2019s images", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "\u2020according to [2] .", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "All experiments are done with the Adam optimizer [13], with an initial learning rate of 1e-3 or 1e-4, dependent on a few epochs of trial on each dataset.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "In [2], a more recent architecture was chosen, namely the ResNet-38 [12].", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In [2], a more recent architecture was chosen, namely the ResNet-38 [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "We compare the baseline performance of our chosen architecture those attained by [2] with ResNet-38.", "startOffset": 81, "endOffset": 84}, {"referenceID": 23, "context": "We denote by Nimagenet the VGG-B architecture which was pre-trained on the ImageNet [25] dataset.", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "Using a single base network Nsketch, we check the method\u2019s sensitivity to varying values of \u03b1 by varying it in the range [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 25, "context": "to the other classification tasks, and the network (also of architecture VGG-B [28]) quickly learns to perform at 100% accuracy.", "startOffset": 79, "endOffset": 83}], "year": 2017, "abstractText": "Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20%) in the number of required parameters while performing on par with more costly finetuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.", "creator": "LaTeX with hyperref package"}}}