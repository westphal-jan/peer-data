{"id": "1705.04249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "K-sets+: a Linear-time Clustering Algorithm for Data Points with a Sparse Similarity Measure", "abstract": "In this paper, we first propose a new iterative algorithm, called the K-sets+ algorithm for clustering data points in a semi-metric space, where the distance measure does not necessarily satisfy the triangular inequality. We show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We then extend the applicability of the K-sets+ algorithm from data points in a semi-metric space to data points that only have a symmetric similarity measure. Such an extension leads to great reduction of computational complexity. In particular, for an n * n similarity matrix with m nonzero elements in the matrix, the computational complexity of the K-sets+ algorithm is O((Kn + m)I), where I is the number of iterations. The memory complexity to achieve that computational complexity is O(Kn + m). As such, both the computational complexity and the memory complexity are linear in n when the n * n similarity matrix is sparse, i.e., m = O(n). We also conduct various experiments to show the effectiveness of the K-sets+ algorithm by using a synthetic dataset from the stochastic block model and a real network from the WonderNetwork website.", "histories": [["v1", "Thu, 11 May 2017 15:39:48 GMT  (301kb,D)", "http://arxiv.org/abs/1705.04249v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["cheng-shang chang", "chia-tai chang", "duan-shin lee", "li-heng liou"], "accepted": false, "id": "1705.04249"}, "pdf": {"name": "1705.04249.pdf", "metadata": {"source": "CRF", "title": "K-sets: a Linear-time Clustering Algorithm for Data Points with a Sparse Similarity Measure", "authors": ["Cheng-Shang Chang", "Chia-Tai Chang", "Duan-Shin Lee"], "emails": ["cschang@ee.nthu.edu.tw;", "s104064540@m104.nthu.edu.tw;", "lds@cs.nthu.edu.tw;", "dacapo1142@gmail.com"], "sections": [{"heading": null, "text": "In fact, most of them will be able to move to another world in which they are able, in which they are able to move, and in which they are able, in which they are able to move."}, {"heading": "II. CLUSTERING IN A SEMI-METRIC SPACE", "text": "In this paper, we will consider the cluster problem for data points in a semi-metric space. Specifically, we will consider a set of n data points, ig = {x1, x2,.., xn} and a distance measure d (x, y) for any two points x and y in \u0443. The distance measure d (\u00b7, \u00b7) is assumed to be semi-metric and fulfills the following three properties: (D1) (non-negativity) d (x, y) \u2265 0. (D2) (zero condition) d (x, x) = 0. (D3) (symmetry) d (x, y) = d (y, x). The semi-metric assumption is weaker than the metric assumption in [18], where the distance measure is assumed to satisfy the triangular inequality. In [18], the K-set algorithm for clustering data points in a metric space has been proposed. One of the most important contributions of this paper is to define the K-points in a semi-metric space and K-unifier spaces."}, {"heading": "A. Semi-cohesion measure", "text": "Consideration of a semi-metric d (x, x) for a semi-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-metric x-y-y-y-x-metric x-x-metric x-metric x-metric x-x-metric x-x-metric x-x-metric x-metric x-metric x-x-y-metric x-metric x-x-y-y-y-metric x-x-metric x-metric x-x-metric x-x-metric x-metric x-metric x-x-metric x-metric x-x-metric x-x-metric x-metric x-metric x-metric x-x-metric x-metric x-metric x-metric x-x-x-metric x-metric x-metric x-metric x-x-"}, {"heading": "B. Clusters in a semi-metric space", "text": "In this section, we define what a cluster is for various equivalent statements, what a cluster is in a semi-metric space. \"Definition 2: (Cluster) Consider a set of n data points (S1, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2, S2,"}, {"heading": "III. BEYOND SEMI-METRIC SPACES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Clustering with a symmetric similarity measure", "text": "In this section we extend the applicability of the K-sets + algorithm to the cluster problem with a symmetrical similarity measurement. (1) A similarity measurement is generally referred to as a bivariate function that measures how similar two data points are. (1) The cluster problem with a similarity measurement is that similar data points are bundled together. (1) For a symmetrical similarity problem (1) we have shown in Theorem 7 (i) that the K-sets + algorithms in Algorithm 1 can be applied monotonously to a local optimum of the optimization problem K k = 1 | Sk | g within a finite number of iterations. (2) Thus, the K-sets + algorithms can be applied to cluster measurement with a symmetrical similarity. But what is the physical meaning of the similarity returned by the K-sets + algorithms?"}, {"heading": "B. Computational complexity", "text": "In this section we address the computational complexity and memory complexity of K-sets = 1. For a n \u00b7 n symmetrical similarity matrix with only m \u00b7 n elements in the matrix, we can show that the computational complexity of K-sets + algorithm O ((Kn + m), where I am the number of iterations, is the memory complexity to achieve this computational complexity is O (Kn + m). Note that the main computational overview of K-sets + algorithm is mainly for calculating the adapted Sxi distance. With regard to (9) we know that you need g (x, S) and 1 | S | 2 g (S) to calculate these."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we evaluate the performance of the K-set + algorithm by conducting two experiments: (i) community recognition of signed networks generated by the stochastic block model in Section IV-A, and (ii) clustering of a real network from the WonderNetwork website [19] in Section IV-B."}, {"heading": "A. Community detection of signed networks with two communities", "text": "In this section, we perform experiments for the K-sets + algorithms using the pre-drawn networks from the stochastic block model. We follow the procedure in [20] to generate the test nets. Each test network consists of n nodes and two ground truth blocks, each of which has a negative edge between two nodes in two different blocks. All edges are independently generated by pin and pout."}, {"heading": "B. Clustering of a real network", "text": "In this section, we test the K-sets + algorithms on the real network from the WonderNetwork site [19]. In this dataset, there are 216 servers in different places and the latency (measured by travel time) between two servers of these 216 servers are recorded in real time. The dataset in our experiment is a snapshot of September 24, 2016. Forthis dataset, the triangular inequality is not always satisfied. For example, we note that latency (Adelaide, Athens) = 138, latency (Athens, Albania) = 138, latency (Adelaide, Albania) = 400, and 250 + 138 \u2264 400. In addition to the latency data, the WonderNetwork site also provides the geographical location of each server. We then use the Haversine formula to calculate the distance between two servers. In the WonderNetwork dataset, the latency, the latency, the latency, the latency, the latency, the latency, the latency and the latency, the latency, the latency, the latency, the latency, the latency, and the latency between the servers."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed the K-sets + algorithm for clustering data points in a semi-metric space and data points that exhibit only a symmetrical measure of similarity. We demonstrated that the K-sets + algorithm converges in a finite number of iterations and has the same guarantee of performance as the K-sets algorithm in [18]. Furthermore, both computational complexity and memory complexity in n are linear when the n \u00b7 n similarity matrix is sparse, i.e. m = O (n). To demonstrate the effectiveness of the K-set + algorithm, we conducted various experiments using a synthetic dataset from the stochastic block model and a real network from the WonderNetwork website [19]."}, {"heading": "APPENDIX A", "text": "In this section we prove theorem 7. (i) Suffice it to show that if x is in a row (S1 | | | | | 1 (S1 | > 1 and 2), the value of the objective function is increased (S1, S2, S2); if Sk (resp. S \u2032 k), k = 1, 2,., K, the partition is before (resp. R) the value of the objective function is before (resp. R) the value of the change. (ThenR) - R = 1, 2,. (S1\\ x), the partition is before (resp. R) the change. (S2, resp. R) - the objective element is before (resp."}, {"heading": "APPENDIX B", "text": "In this section we prove that Lemma 8. Since g (\u00b7, \u00b7) is symmetrical, unambiguously g (\u00b7, \u00b7) is also symmetrical. To see that (C2) is satisfied, note from (13) that (C3) is unambiguously g (x, y) = g (x, c) \u2212 g (x, c) \u2212 1 n g (x, c) + 1 n g (n, c) + 1 g (n, c) + 0. (32) To see that (C3) applies, note that g (x, c) = g (x, x) \u2212 2 g (x, c) + 1 n2 g (n, c) + (n \u2212 1) n (33) and g (y, y) = g (y) \u2212 2 n g (y, c) + 1 n2 g (i, c) + (n \u2212 1) n."}, {"heading": "APPENDIX C", "text": "In this section we prove that Lemma 9. Note from Definition 4 and Definition 6 results in that when using (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x, S) (x) (x), S) (x), S) (x), S), S), S (x), S), S (x), S (x), S), S (x), S (x), S), S (x), S), S (x), S (x), S (x), S), S (x), S (S), S (x), S (x), S (x), S (S), S (x), S (x), S (S), S (x), S (x), S (S), S (x), S (S), S (x), S (x)."}], "references": [{"title": "Mining of massive datasets", "author": ["A. Rajaraman", "J. Leskovec", "J.D. Ullman"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys (CSUR), vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Data clustering: 50 years beyond K-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory, vol. 28, no. 2, pp. 129\u2013137, 1982.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1982}, {"title": "k-means++ under approximation stability", "author": ["M. Agarwal", "R. Jaiswal", "A. Pal"], "venue": "Theory and Applications of Models of Computation. Springer, 2013, pp. 84\u201395.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding groups in data: an introduction to cluster analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A new partitioning around medoids algorithm", "author": ["M. Van der Laan", "K. Pollard", "J. Bryan"], "venue": "Journal of Statistical Computation and Simulation, vol. 73, no. 8, pp. 575\u2013584, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "A simple and fast algorithm for K-medoids clustering", "author": ["H.-S. Park", "C.-H. Jun"], "venue": "Expert Systems with Applications, vol. 36, no. 2, pp. 3336\u2013 3341, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, no. 1, pp. 176\u2013190, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral redemption in clustering sparse networks", "author": ["F. Krzakala", "C. Moore", "E. Mossel", "J. Neeman", "A. Sly", "L. Zdeborov\u00e1", "P. Zhang"], "venue": "Proceedings of the National Academy of Sciences, vol. 110, no. 52, pp. 20 935\u201320 940, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise.", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "in KDD, vol", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Cluster analysis: a further approach based on density estimation", "author": ["A. Cuevas", "M. Febrero", "R. Fraiman"], "venue": "Computational Statistics & Data Analysis, vol. 36, no. 4, pp. 441\u2013459, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "A density-based cluster validity approach using multi-representatives", "author": ["M. Halkidi", "M. Vazirgiannis"], "venue": "Pattern Recognition Letters, vol. 29, no. 6, pp. 773\u2013786, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering under approximation stability", "author": ["M.-F. Balcan", "A. Blum", "A. Gupta"], "venue": "Journal of the ACM (JACM), vol. 60, no. 2, p. 8, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A mathematical theory for clustering in metric spaces", "author": ["C.-S. Chang", "W. Liao", "Y.-S. Chen", "L.-H. Liou"], "venue": "IEEE Transactions on Network Science and Engineering, vol. 3, no. 1, pp. 2\u201316, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Community detection in signed networks: an error-correcting code approach", "author": ["S.-M. Lu", "L.-H. Liou", "C.-S. Chang", "D.-S. Lee"], "venue": "submitted for publication, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 206, "endOffset": 209}, {"referenceID": 3, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 211, "endOffset": 214}, {"referenceID": 4, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 250, "endOffset": 253}, {"referenceID": 6, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 255, "endOffset": 258}, {"referenceID": 7, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 260, "endOffset": 263}, {"referenceID": 8, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 311, "endOffset": 315}, {"referenceID": 9, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 317, "endOffset": 321}, {"referenceID": 10, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 323, "endOffset": 327}, {"referenceID": 11, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 329, "endOffset": 333}, {"referenceID": 12, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 371, "endOffset": 375}, {"referenceID": 13, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 377, "endOffset": 381}, {"referenceID": 14, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 383, "endOffset": 387}, {"referenceID": 15, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 389, "endOffset": 393}, {"referenceID": 16, "context": "Recently, a mathematical clustering theory was developed in [18] for clustering data points in a metric space.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "In addition to the definition of a cluster in a metric space, the K-sets algorithm was proposed in [18] to cluster data points in a metric space.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "The key innovation of the K-sets algorithm in [18] is the triangular distance that measures the distance from a data point to a set (of data points) by using the triangular inequality.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "It was shown in [18] that the Ksets algorithm converges in a finite number of iterations and outputs K disjoint sets such that any two sets of these K sets are two disjoint clusters when they are viewed in isolation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "The first contribution of this paper is to extend the clustering theory/algorithm in [18] to data points in a semi-metric space, where the distance measure does not necessarily satisfy the triangular inequality.", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "The semi-metric assumption is weaker than the metric assumption in [18], where the distance measure is assumed to satisfy the triangular inequality.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "In [18], the K-sets algorithm was proposed for clustering data points in a metric space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "As both Euclidean spaces and metric spaces are spacial cases of semimetric spaces, such a generalization allows us to unify the well-known K-means algorithm and the K-sets algorithm in [18].", "startOffset": 185, "endOffset": 189}, {"referenceID": 16, "context": "Analogous to the argument in [18], one can easily show the following duality theorem.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "Following the same argument in [18], one can also show a theorem for various equivalent statements for what a cluster is in a semi-metric space.", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Though the extensions of the duality result and the equivalent statements for clusters to semi-metric spaces are basically the same as those in [18], one problem arises when extending the K-sets algorithm to a semi-metric space.", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "Definition 4: (\u2206-distance [18]) For a symmetric bivariate function g(\u00b7, \u00b7) on a set of data points \u03a9 = {x1, x2, .", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "Since the \u2206-distance might not be nonnegative in a semimetric space, the proofs for the convergence and the performance guarantee of the K-sets algorithm in [18] are no longer valid.", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "We follow the procedure in [20] to generate the test networks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "Such a similarity matrix was suggested in [20] for community detection in signed networks as it allows us to \u201csee\u201d more than one step relationship between two nodes.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "We showed that the K-sets algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm in [18].", "startOffset": 152, "endOffset": 156}], "year": 2017, "abstractText": "In this paper, we first propose a new iterative algorithm, called the K-sets algorithm for clustering data points in a semi-metric space, where the distance measure does not necessarily satisfy the triangular inequality. We show that the K-sets algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We then extend the applicability of the K-sets algorithm from data points in a semi-metric space to data points that only have a symmetric similarity measure. Such an extension leads to great reduction of computational complexity. In particular, for an n\u00d7 n similarity matrix with m nonzero elements in the matrix, the computational complexity of the K-sets algorithm is O((Kn+m)I), where I is the number of iterations. The memory complexity to achieve that computational complexity is O(Kn + m). As such, both the computational complexity and the memory complexity are linear in n when the n \u00d7 n similarity matrix is sparse, i.e., m = O(n). We also conduct various experiments to show the effectiveness of the K-sets algorithm by using a synthetic dataset from the stochastic block model and a real network from the WonderNetwork website. keywords: Clustering; community detection", "creator": "LaTeX with hyperref package"}}}