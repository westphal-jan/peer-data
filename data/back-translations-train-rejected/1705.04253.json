{"id": "1705.04253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Sketching Word Vectors Through Hashing", "abstract": "We propose a new fast word embedding technique using hash functions. The method is a derandomization of a new type of random projections: By disregarding the classic constraint used in designing random projections (i.e., preserving pairwise distances in a particular normed space), our solution exploits extremely sparse non-negative random projections. Our experiments show that the proposed method can achieve competitive results, comparable to neural embedding learning techniques, however, with only a fraction of the computational complexity of these methods. While the proposed derandomization enhances the computational and space complexity of our method, the possibility of applying weighting methods such as positive pointwise mutual information (PPMI) to our models after their construction (and at a reduced dimensionality) imparts a high discriminatory power to the resulting embeddings. Obviously, this method comes with other known benefits of random projection-based techniques such as ease of update.", "histories": [["v1", "Thu, 11 May 2017 15:53:00 GMT  (203kb)", "http://arxiv.org/abs/1705.04253v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["behrang qasemizadeh", "laura kallmeyer", "peyman passban"], "accepted": false, "id": "1705.04253"}, "pdf": {"name": "1705.04253.pdf", "metadata": {"source": "CRF", "title": "Sketching Word Vectors Through Hashing", "authors": ["Behrang QasemiZadeh", "Laura Kallmeyer", "Peyman Passban"], "emails": ["zadeh@phil.hhu.de", "kallmeyer@phil.hhu.de", "ppassban@computing.dcu.ie"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.04 253v 1 [cs.C L] 11 May 2"}, {"heading": "1 Introduction", "text": "Word embedding techniques (i.e. the use of distributional frequencies to generate word vectors of reduced dimensionality) have become a cornerstone of modern natural language processing and information gathering systems. These quantifications of words are often justified by Harris \"distribution hypothesis [Harris, 1954]. It is believed that words of comparable linguistic properties occur in a similar set of\" contexts, \"e.g. words of similar meaning with a similar set of context words {c1,... cn}. This hypothesis implies that if this set of context words is randomly divided into m buckets, e.g. {c1,.. cx} 1,..., {cy,.. cn} m}, then contiguous words still occur with similar groups of buckets. We design a new random projection to exploit this conjecture, and accordingly we propose a new hash-based word, e.g. the results of this section of 5 followed by the summary of a short section of these words in section 4."}, {"heading": "2 The Proposed Embedding Technique", "text": "Lets take that we build an m-dimensional embedding for a word w that is co-recorded with a number of context words c = > # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "2.1 Method\u2019s Justification", "text": "The proposed method can be explained mathematically by applying the principles of dimensionality reduction by means of random projections.Let Cp \u00b7 n denotes the set of p-word vectors obtained by counting the coexistence of each target word ~ wi (1 \u2264 i \u2264 p) with each context element cj (1 \u2264 j \u2264 n).In most applications, n is a very large number in such a way that it obstructs processes (and causes the so-called curse of dimensionality problem).To address this problem, Cp \u00b7 n undergoes a series of \"transformations\" T to map the high-dimensional space Cp \u00b7 n to a space of reduced dimensionality Wp \u00b7 m (m \u00b2 n): Cp \u00d7 n \u00d7 Tn \u00b7 m = Wp \u00b7 m. In our proposed method, Tn \u00d7 m is a sparsely randomly generated matrix in which tij elements of T have the following distribution: tij = {0 with probability \u2212 1 m \u00b2."}, {"heading": "3 Related Work", "text": "These methods have been used to provide workable solutions to a number of problems that require an idea of the (approximate) search for the nearest neighbors, especially when retrieving information, e.g. identifying duplicated and nearly duplicated documents [Manku et al., 2007] and the consistency of strings [Dalvi et al., 2013; Michael, 1981], semantic labeling [Yang et al., 2016], and cross-media retrieval techniques [Wang et al., 2015] to name but a few. Naturally, these methods have also been applied to the problem of word embedding, either as a dimensionality reduction method [Bingham and Mannila, 2001; Kaski, 1998], or in the form of an incremental vector space construction technique that fuses the random projection-based dimensionality of the reduction process."}, {"heading": "4 Empirical Evaluation", "text": "We report on the results of empirical evaluations in both intrinsic and extrinsic constellations. In addition to an intrinsic evaluation using word-relation tests, we report on results when the method is used to form a neural part-of-speech tagger."}, {"heading": "4.1 Intrinsic Evaluation", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "4.2 Extrinsic Evaluation", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5 Conclusion", "text": "A method for sketching word vectors based on a hash-based algorithm is proposed; the proposed method is fast and requires a small amount of computational resources to build a model; however, as shown empirically, it can perform well in both intrinsic and extrinsic evaluation setups. Thanks to the random projection that this method implements, the development of embedding does not require offline training time: vectors can be used, updated, added and removed at any stage during the lifecycle of a system."}], "references": [{"title": "In HLT-NAACL \u201909", "author": ["EnekoAgirre", "EnriqueAlfonseca", "Keith Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa. A study on similarity", "relatedness using distributional", "wordnet-based approaches"], "venue": "pages 19\u2013 27, Stroudsburg, PA, USA,", "citeRegEx": "Agirre et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Random projection in dimensionality reduction: Applications to image and text data", "author": ["Bingham", "Mannila", "2001] Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201901,", "citeRegEx": "Bingham et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bingham et al\\.", "year": 2001}, {"title": "Normalized (pointwise) mutual information in collocation extraction", "author": ["Gerlof Bouma"], "venue": "Proceedings of the Biennial GSCL Conference,", "citeRegEx": "Bouma. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "andMarco Baroni", "author": ["Elia Bruni", "Nam Khanh Tran"], "venue": "Multimodal distributional semantics. J. Artif. Int. Res., 49(1):1\u201347, January", "citeRegEx": "Bruni et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Correlation: Parametric and Nonparametric Measures", "author": ["Peter Y. Chen", "Paula M. Popovich"], "venue": "Quantitative Applications in the Social Sciences. Sage Publications,", "citeRegEx": "Chen and Popovich. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "In WWW \u201913", "author": ["Nilesh Dalvi", "Vibhor Rastogi", "Anirban Dasgupta", "Anish Das Sarma", "Tamas Sarlos. Optimal hashing schemes for entity matching"], "venue": "pages 295\u2013306, New York, NY, USA,", "citeRegEx": "Dalvi et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "JASIS", "author": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman. Indexing by latent semantic analysis"], "venue": "41(6):391\u2013407,", "citeRegEx": "Deerwester et al.. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "In NAACL-HLT \u201915", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith. Retrofitting word vectors to semantic lexicons"], "venue": "pages 1606\u20131615,", "citeRegEx": "Faruqui et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Gabrilovich Evgenly", "Matias Yossi", "Rivlin Ehud", "Solan Zach", "Wolfman Gadi", "Ruppin Eytan"], "venue": "WWW \u201910,", "citeRegEx": "Finkelstein et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Technical report", "author": ["W. Nelson Francis", "Henry Kucera. Brown corpus manual"], "venue": "Brown University,", "citeRegEx": "Francis and Kucera. 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "PPDB: The paraphrase", "author": ["Ganitkevitch et al", "2013] Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Topsig: Topology preserving document signatures", "author": ["Shlomo Geva", "Christopher M. De Vries"], "venue": "CIKM \u201911, pages 333\u2013338, New York, NY, USA,", "citeRegEx": "Geva and De Vries. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Journal of the American Statistical Association", "author": ["Leo A. Goodman", "William H. Kruskal. Measures of association for cross classifications"], "venue": "49(268):732\u2013764,", "citeRegEx": "Goodman and Kruskal. 1954", "shortCiteRegEx": null, "year": 1954}, {"title": "In KDD \u201912", "author": ["Guy Halawi", "Gideon Dror", "Evgeniy Gabrilovich", "Yehuda Koren. Large-scale learning of word relatedness with constraints"], "venue": "pages 1406\u20131414, New York, NY, USA,", "citeRegEx": "Halawi et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Word", "author": ["Zellig S Harris. Distributional structure"], "venue": "10(2-3):146\u2013162,", "citeRegEx": "Harris. 1954", "shortCiteRegEx": null, "year": 1954}, {"title": "SimLex-999: Evaluating semantic models with genuine similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Comput. Linguist., 41(4):665\u2013 695, December", "citeRegEx": "Hill et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "pseudorandom generators", "author": ["Piotr Indyk. Stable distributions"], "venue": "embeddings and data stream computation. In 41st Annual Symposium on Foundations of Computer Science, pages 189\u2013197,", "citeRegEx": "Indyk. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "volume 26", "author": ["William Johnson", "Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. In Conference in modern analysis", "probability"], "venue": "pages 189\u2013206. AMS,", "citeRegEx": "Johnson and Lindenstrauss. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "Random indexing of text samples for latent semantic analysis", "author": ["P. Kanerva", "J. Kristofersson", "A. Holst"], "venue": "Proceedings of the 22nd Annual Conference of the Cognitive Science Society, volume 1036", "citeRegEx": "Kanerva et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Dimensionality reduction by random mapping: fast similarity computation for clustering", "author": ["S. Kaski"], "venue": "IEEE International Joint Conference on Neural Networks Proceedings, volume 1, pages 413\u2013418 vol.1, May", "citeRegEx": "Kaski. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Biometrika", "author": ["Maurice G. Kendall. A newmeasure of rank correlation"], "venue": "30(1-2):81\u201393,", "citeRegEx": "Kendall. 1938", "shortCiteRegEx": null, "year": 1938}, {"title": "volume 14", "author": ["Quoc V Le", "Tomas Mikolov. Distributed representations of sentences", "documents. In ICML"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In KDD \u201906", "author": ["Ping Li", "Trevor J. Hastie", "Kenneth W. Church. Very sparse random projections"], "venue": "pages 287\u2013296, New York, NY, USA,", "citeRegEx": "Li et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "pages 104\u2013113", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning. Better word representations with recursive neural networks for morphology. In CoNLL"], "venue": "ACL,", "citeRegEx": "Luong et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Detecting near-duplicates for web", "author": ["Manku et al", "2007] Gurmeet Singh Manku", "Arvind Jain", "Anish Das Sarma"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Technical report", "author": ["Michael. Fingerprinting by random polynomials"], "venue": "Center of Research in Computer Technology,", "citeRegEx": "Michael. 1981", "shortCiteRegEx": null, "year": 1981}, {"title": "CoRR", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean. Efficient estimation of word representations in vector space"], "venue": "abs/1301.3781,", "citeRegEx": "Mikolov et al.. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "In NIPS 26", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "Language and Cognitive Processes", "author": ["George A. Miller", "Walter G. Charles. Contextual correlates of semantic similarity"], "venue": "6(1):1\u201328,", "citeRegEx": "Miller and Charles. 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM, 38(11):39\u201341, November", "citeRegEx": "Miller. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "EMNLP \u201914, pages 1532\u20131543, Doha, Qatar,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Random manhattan integer indexing: Incremental l1 normed vector space construction", "author": ["Behrang Q. Zadeh", "Siegfried Handschuh"], "venue": "EMNLP, pages 1713\u20131723, Doha, Qatar,", "citeRegEx": "Q. Zadeh and Handschuh. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In NLDB", "author": ["Behrang QasemiZadeh. Random indexing revisited"], "venue": "pages 437\u2013442,", "citeRegEx": "QasemiZadeh. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A word at a time: Computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "WWW \u201911, pages 337\u2013346, New York, NY, USA,", "citeRegEx": "Radinsky et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "ACM", "author": ["Herbert Rubenstein", "John B. Goodenough. Contextual correlates of synonymy. Commun"], "venue": "8(10):627\u2013633,", "citeRegEx": "Rubenstein and Goodenough. 1965", "shortCiteRegEx": null, "year": 1965}, {"title": "In LREC\u201912", "author": ["Roland Schafer", "Felix Bildhauer. Building large corpora from the web using a new efficient tool chain"], "venue": "pages 486\u2013493,", "citeRegEx": "Schafer and Bildhauer. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney"], "venue": "EMCL \u201901, pages 491\u2013502, London, UK, UK,", "citeRegEx": "Turney. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "In IJCAI\u201915", "author": ["DiWang", "Xinbo Gao", "XiumeiWang", "Lihuo He. Semantic topic multimodal hashing for crossmedia retrieval"], "venue": "pages 3890\u20133896. AAAI Press,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In GWC-06", "author": ["Dongqiang Yang", "David M.W. Powers. Verb similarity on the taxonomy of wordnet"], "venue": "Jeju Island, Korea,", "citeRegEx": "Yang and Powers. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Poisketch: Semantic place labeling over user activity streams", "author": ["Dingqi Yang", "Bin Li", "Philippe Cudr\u00e9Mauroux"], "venue": "IJCAI 2016, pages 2697\u20132703,", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "These quantifications of words are often justified by Harris\u2019 Distributional Hypothesis [Harris, 1954].", "startOffset": 88, "endOffset": 102}, {"referenceID": 12, "context": "Once these embeddings are constructed, similarities between them are given by an appropriate correlation measure such as Goodman and Kruskal\u2019s \u03b3 coefficient [Goodman and Kruskal, 1954] (or, alternatively, Kenall\u2019s \u03c4b [Kendall, 1938]).", "startOffset": 157, "endOffset": 184}, {"referenceID": 21, "context": "Once these embeddings are constructed, similarities between them are given by an appropriate correlation measure such as Goodman and Kruskal\u2019s \u03b3 coefficient [Goodman and Kruskal, 1954] (or, alternatively, Kenall\u2019s \u03c4b [Kendall, 1938]).", "startOffset": 217, "endOffset": 232}, {"referenceID": 37, "context": ", through normalizing raw frequencies by the expected and marginal frequencies, such as the popular PPMI weighting [Turney, 2001; Bouma, 2009].", "startOffset": 115, "endOffset": 142}, {"referenceID": 2, "context": ", through normalizing raw frequencies by the expected and marginal frequencies, such as the popular PPMI weighting [Turney, 2001; Bouma, 2009].", "startOffset": 115, "endOffset": 142}, {"referenceID": 18, "context": ", the well-known lemmas proposed in [Johnson and Lindenstrauss, 1984] (for l2normed spaces), [Indyk, 2000] (for l1-normed spaces), and their subsequent refinements and generalizations such as [Li et al.", "startOffset": 36, "endOffset": 69}, {"referenceID": 17, "context": ", the well-known lemmas proposed in [Johnson and Lindenstrauss, 1984] (for l2normed spaces), [Indyk, 2000] (for l1-normed spaces), and their subsequent refinements and generalizations such as [Li et al.", "startOffset": 93, "endOffset": 106}, {"referenceID": 23, "context": ", the well-known lemmas proposed in [Johnson and Lindenstrauss, 1984] (for l2normed spaces), [Indyk, 2000] (for l1-normed spaces), and their subsequent refinements and generalizations such as [Li et al., 2006].", "startOffset": 192, "endOffset": 209}, {"referenceID": 5, "context": ", 2007] and string matching [Dalvi et al., 2013; Michael, 1981], semantic labeling [Yang et al.", "startOffset": 28, "endOffset": 63}, {"referenceID": 26, "context": ", 2007] and string matching [Dalvi et al., 2013; Michael, 1981], semantic labeling [Yang et al.", "startOffset": 28, "endOffset": 63}, {"referenceID": 40, "context": ", 2013; Michael, 1981], semantic labeling [Yang et al., 2016], and cross-media retrieval [Wang et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 38, "context": ", 2016], and cross-media retrieval [Wang et al., 2015] to name a few.", "startOffset": 35, "endOffset": 54}, {"referenceID": 20, "context": "Naturally, these methods have been also applied to the problem of word embedding, either as a dimensionality reduction method [Bingham and Mannila, 2001; Kaski, 1998], or in the form of an incremental vector space construction technique in which the random projection-based dimensionality reduction process is merged with the process of collecting word co-occurrence frequencies, e.", "startOffset": 126, "endOffset": 166}, {"referenceID": 19, "context": ", [Kanerva et al., 2000; Geva and De Vries, 2011; Q. Zadeh and Handschuh, 2014].", "startOffset": 2, "endOffset": 79}, {"referenceID": 11, "context": ", [Kanerva et al., 2000; Geva and De Vries, 2011; Q. Zadeh and Handschuh, 2014].", "startOffset": 2, "endOffset": 79}, {"referenceID": 32, "context": ", [Kanerva et al., 2000; Geva and De Vries, 2011; Q. Zadeh and Handschuh, 2014].", "startOffset": 2, "endOffset": 79}, {"referenceID": 33, "context": "employed sparse Gaussian random projections [QasemiZadeh, 2015] to build word vectors directly at a reduced dimensionality and showed that their proposed \u2018random indexing\u2019 technique yields results comparable to the latent semantic analysis technique of [Deerwester et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 6, "context": "employed sparse Gaussian random projections [QasemiZadeh, 2015] to build word vectors directly at a reduced dimensionality and showed that their proposed \u2018random indexing\u2019 technique yields results comparable to the latent semantic analysis technique of [Deerwester et al., 1990], which employs truncation using singular value decomposition for dimensionality reduction.", "startOffset": 253, "endOffset": 278}, {"referenceID": 28, "context": ", word2vec [Mikolov et al., 2013b] and GloVe [Pennington et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 31, "context": ", 2013b] and GloVe [Pennington et al., 2014].", "startOffset": 19, "endOffset": 44}, {"referenceID": 8, "context": "Relatedness tests in our experiments are WS353 [Finkelstein et al., 2001], WSS and WSR by [Agirre et al.", "startOffset": 47, "endOffset": 73}, {"referenceID": 0, "context": ", 2001], WSS and WSR by [Agirre et al., 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 29, "context": ", 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al.", "startOffset": 35, "endOffset": 61}, {"referenceID": 35, "context": ", 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al.", "startOffset": 71, "endOffset": 104}, {"referenceID": 24, "context": ", 2009], the classic tests of MC30 [Miller and Charles, 1991] and RG65 [Rubenstein and Goodenough, 1965], the Stanford Rare Word dataset RW [Luong et al., 2013], M287 by [Radinsky et al.", "startOffset": 140, "endOffset": 160}, {"referenceID": 34, "context": ", 2013], M287 by [Radinsky et al., 2011], M771 [Halawi et al.", "startOffset": 17, "endOffset": 40}, {"referenceID": 13, "context": ", 2011], M771 [Halawi et al., 2012], SL [Hill et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 15, "context": ", 2012], SL [Hill et al., 2015], YP130 verb relatedness [Yang and Powers, 2006], and MEN [Bruni et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 39, "context": ", 2015], YP130 verb relatedness [Yang and Powers, 2006], and MEN [Bruni et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 3, "context": ", 2015], YP130 verb relatedness [Yang and Powers, 2006], and MEN [Bruni et al., 2014].", "startOffset": 65, "endOffset": 85}, {"referenceID": 28, "context": "Baselines: For this input, as one baseline, we train a word2vec CBOW model [Mikolov et al., 2013b], one of the most popular word embedding techniques.", "startOffset": 75, "endOffset": 98}, {"referenceID": 0, "context": "Overall, when it comes to choosing a neural embedding learning method versus our method for addressing a semantic similarity task, our experiments suggest that the hash-based method is more suitable for similarity identification than relatedness (in the sense that described in [Agirre et al., 2009]), and for tasks that involve comparison of frequent and infrequent words (due to the nature of PPMI weighting).", "startOffset": 278, "endOffset": 299}, {"referenceID": 37, "context": ", as explained in [Turney, 2001] by querying a Web search engine.", "startOffset": 18, "endOffset": 32}, {"referenceID": 36, "context": "To show the effect of enlarging the size of input corpus on the method\u2019s performance, in addition to the Wikipedia corpus that we used initially, we feed another 4 billion tokens of web crawled text data [Schafer and Bildhauer, 2012] to our models.", "startOffset": 204, "endOffset": 233}, {"referenceID": 7, "context": "Apart from methods discussed above, \u2018retrofitting\u2019 (refining word embeddings using lexical relations available in lexical resources [Faruqui et al., 2015]) is another methodology for improving results.", "startOffset": 132, "endOffset": 154}, {"referenceID": 30, "context": "To show the impact of this \u2018retrofitting\u2019 method, we take vectors of dimensionality m = 500 from our earlier experiment, and update them by reading WordNet [Miller, 1995] and PPDB [Ganitkevitch et al.", "startOffset": 156, "endOffset": 170}, {"referenceID": 7, "context": "As discussed in [Faruqui et al., 2015], encoding this knowledge into neural embeddingmodels is not straightforward, and it demands certain modification in their learning algorithm.", "startOffset": 16, "endOffset": 38}, {"referenceID": 27, "context": "Likewise relatedness tasks, the word2vec family of embeddings methods [Mikolov et al., 2013a; Le and Mikolov, 2014] are a popular choice for this pre-training phase.", "startOffset": 70, "endOffset": 115}, {"referenceID": 22, "context": "Likewise relatedness tasks, the word2vec family of embeddings methods [Mikolov et al., 2013a; Le and Mikolov, 2014] are a popular choice for this pre-training phase.", "startOffset": 70, "endOffset": 115}, {"referenceID": 16, "context": "We witness that vectors produced using our hash-based technique are more suitable than word2vec embeddings, at least, in our part-of-speech tagging experiment using Long Short Term Memory (LSTM) units [Hochreiter and Schmidhuber, 1997].", "startOffset": 201, "endOffset": 235}, {"referenceID": 9, "context": "For evaluation, we use the tagged version of the Brown corpus [Francis and Kucera, 1979], which has 57,067 sentences and 313 different tags.", "startOffset": 62, "endOffset": 88}], "year": 2017, "abstractText": "We propose a new fast word embedding technique using hash functions. The method is a derandomization of a new type of random projections: By disregarding the classic constraint used in designing random projections (i.e., preserving pairwise distances in a particular normed space), our solution exploits extremely sparse non-negative random projections. Our experiments show that the proposed method can achieve competitive results, comparable to neural embedding learning techniques, however, with only a fraction of the computational complexity of these methods. While the proposed derandomization enhances the computational and space complexity of our method, the possibility of applying weighting methods such as positive pointwise mutual information (PPMI) to our models after their construction (and at a reduced dimensionality) imparts a high discriminatory power to the resulting embeddings. Obviously, this method comes with other known benefits of random projection-based techniques such as ease of update.", "creator": "LaTeX with hyperref package"}}}