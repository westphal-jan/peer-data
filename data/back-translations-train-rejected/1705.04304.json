{"id": "1705.04304", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit \"exposure bias\" -- they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.", "histories": [["v1", "Thu, 11 May 2017 17:39:35 GMT  (30kb)", "http://arxiv.org/abs/1705.04304v1", null], ["v2", "Fri, 19 May 2017 04:11:32 GMT  (156kb,D)", "http://arxiv.org/abs/1705.04304v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["romain paulus", "caiming xiong", "richard socher"], "accepted": false, "id": "1705.04304"}, "pdf": {"name": "1705.04304.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rpaulus@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.04 304v 1 [cs.C L] 11 May 201 7Attention-oriented, RNN-based abstract summary encoder decoder models have performed well on short input and output sequences, but for longer documents and summaries, these models often contain repetitive and incoherent phrases. We are introducing a neural network model with intra-attention and a new training method, which combines standard, monitored word prediction and reinforcement learning (RL). Models trained only with the former often exhibit exposure bias - they assume that the truth is conveyed at every step of the training. However, when standard word predictions are combined with RL's global sequence prediction training, the resulting summaries become more readable. We rate this model on CNN / Daily Mail and New York Times datasets. Our model achieves an absolute 1.16 UGE Daily Times model based on the earlier CNN / UGE models, also improving on the higher quality of the earlier CNN / UGE models."}, {"heading": "1 Introduction", "text": "By condensing large amounts of information into short, informative summaries, the summary can support many downstream applications, such as news commentary generation, research generation, and reporting generations.There are two prominent types of summarization algorithms. First, extractive summarization systems form summaries by copying portions of the input (Neto et al., 2002; Dorr et al., 2003). Second, abstract summarization systems generate new phrases, possibly rephrasing or using words that are not included in the original text (Chopra et al., 2016; Nallapati et al., 2016).Recently, we have neural network models (Nallapati et al., 2016; Zeng et al., 2016) based on the attentive encoder decoder decoder model for machine translation."}, {"heading": "2 Neural Intra-attention Model", "text": "In this section, we present our intra-attention model based on the encoder decoder network (Sutskever et al., 2014). In all our equations, x = {x1, x2,..., xn} represents the sequence of input (article) tokens, y = {y1, y2,..., yn \u2032 represents the sequence of output (summary) tokens, and ig denotes the vector chain operator. Our model reads the input sequence with a bidirectional LSTM encoder {RNNe fwd, RNe bwd}, which calculates hidden states from the embedding vectors of xi hei = [h e fwd i, h e bwd i]. We use a single LSTM decoder RNNd, which calculates hidden states hdt from the embedding vectors of yt. Both input and output vectors are coded from the same matrix = 0."}, {"heading": "2.1 Intra-temporal attention on input sequence", "text": "For each decryption step t, we use an intratemporal attention function to observe certain parts of the encrypted input sequence in addition to the decoder's own hidden state and the previously generated word (Sankaran et al., 2016). This kind of attention prevents the model from participating in different decryption steps via the same parts of the input. Nallapati et al. (2016) have shown that such intratemporal attention can reduce the number of repetitions when visiting long documents. We define eti as the attention value of the hidden input state hei at decryption step t: eti = f (h d t, h e i), (1) where f can be any function that returns a scalar eti from the hdt and h e i vectors. While some attention models use functions that are as simple as the dot product between the two vectors, we opt for a bilar function to get attention (f) when we shift the eti (hi) to a specific context."}, {"heading": "2.2 Intra-decoder attention", "text": "While this intratemporal attention function ensures that different parts of the encoded input sequence are used, our decoder can still generate repeated phrases based on its own hidden states, especially when generating long sequences. To prevent this, we want to include more information about the previously decrypted sequence in the decoder. Looking back at previous decoding steps, our model will be able to make more structured predictions and avoid the repetition of the same information, even if this information was generated many steps away. To achieve this, we will introduce an intra-decoder attention mechanism, which is not present in current encoder decoding models.For each decoding step t, our model calculates a new decoder context vector cdt. We set c d 1 to a vector of nulars, as the generated sequence is empty at the first decoding step. (For each decoding step t, our model calculates a new decoder context vector cdt.)"}, {"heading": "2.3 Token generation and pointer", "text": "To create a token mechanism, our decoder uses either a Softmax layer of the token generation or a pointer mechanism to copy rare or invisible words from the input sequence (Gulcehre et al., 2016; Nallapati et al., 2016). We define ut as a binary value equal to 1 when the pointer mechanism is used to output yt, and 0 otherwise. In the following equations, all probabilities are bound to yt,.., yt \u2212 1, x, even if not explicitly specified. Our token generation layer generates the fol-flowing probability distribution: p (yt | ut = 0) = softmax (Wout [h d-t-c e-t-t-t-t) + yt-out) (9) On the other hand, the pointer mechanism p uses the temporal attention weight distribution."}, {"heading": "2.4 Sharing decoder weights", "text": "In addition to using the same Wemb embedding matrix for the encoder and decoder sequences, we are introducing a certain weight distribution between this embedding matrix and the Wout matrix of the token generation layer: Wout = tanh (WembWproj) (13) The aim of this weight distribution is to use the syntactical and semantic information contained in the embedding matrix to improve the token generation function. Similar methods of weight distribution have been applied to language modeling (Inan et al., 2016; Press and Wolf, 2016). We believe that this method is even better applicable to sequence-to-sequence tasks such as summing, where the input and output sequences are closely linked, share the same vocabulary and a similar syntax. In practice, we have found that a summary model that uses such common weights much faster than Wemb and Wemout are used."}, {"heading": "2.5 Repetition avoidance at test time", "text": "Another way to avoid repetition arises from our observation that in both CNN / Daily Mail and NYT records, ground truth summaries almost never contain the same trigram twice. As a result of this observation, we force our decoder never to output the same trigram more than once during the test. We do this by setting p (yt) = 0 during the bar search when the output of yt would generate a trigram that already exists in the previously decrypted sequence of the current bar. Although this method makes assumptions about the output format and the dataset at hand, we believe that the majority of abstract summary tasks would benefit from this hard constraint. We apply this method to all of our models in the Experiments section."}, {"heading": "3 Hybrid Learning Objective", "text": "In this section, we explore various ways to train our encoder decoder model. In particular, we propose reinforcing learning-based algorithms and applying them to our summary."}, {"heading": "3.1 Supervised learning with teacher forcing", "text": "The most common method of creating a RNN decoder for sequence generation, called the teacher coercion algorithm (Williams and Zipser, 1989), minimizes loss with maximum probability at each decoding step. We define y * = {y * 1, y * 2,.., y * n \u00b2 as the output sequence for a given input sequence x. The training goal with maximum probability is to minimize the following loss: Lml = \u2212 n \u00b2 t = 1log (y * t | y * 1,.,.., y * t \u2212 1, x) (14) However, minimizing Lml does not always produce the best results with discrete evaluation metrics such as ROUGE (Lin, 2004). This phenomenon has been observed in sequence creation tasks similar to image captions with CIDEr (Rennie et al., 2016) when the results of sequence determination are considered as raGE (Lin, 2004)."}, {"heading": "3.2 Policy learning", "text": "One way to fix this is to learn a policy that maximizes a specific discrete measurement, rather than minimizing the maximum loss of probability that is possible through reinforcement learning. In our model, we use the self-critical training algorithm for the policy gradient (Rennie et al., 2016). For this training algorithm, we produce two separate output sequences for each training session: ys, which are scanned from the p (yst | y s 1,..., y s \u2212 1, x) probability distribution in each decryption time step, and y, the base output quantity that is achieved by maximizing the output probability distribution in each time step, essentially performing a greedy search. We define r (y) as a reward function for an output sequence y by comparing it with the basic truth sequence y \u2012 with the evaluation metric of our Choice.Lrl = (r (y) \u2212 r (s) n) as a reward function for an output sequence."}, {"heading": "3.3 Mixed training objective function", "text": "A potential problem with this reinforcement training goal is that optimization for a particular discrete measurement such as ROUGE does not guarantee an increase in the quality and legibility of the result. It is possible to play such discrete measurements and increase their value without actually increasing readability or relevance (Liu et al., 2016). While ROUGE measures the overlap of the n-gram between our generated summary and a reference sequence, humanity can be better measured by a language model normally measured by perplexity. Since our maximum probability training goal (Eq. 14) is essentially a conditional language model that calculates the probability of a token based on the previously predicted sequence {y1,. yt \u2212 1} and the input sequence x, we assume that it can help our political learning algorithm to generate more natural summaries. This motivates us to define a mixed learning function that combines \u2212 16 ml and \u2212 1, whereby \u2212 16 ml is equal to \u2212 1."}, {"heading": "4 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Neural encoder-decoder sequence models", "text": "Neural encoder decoder models are commonly used in NLP applications such as machine translation (Sutskever et al., 2014), abstract (Chopra et al., 2016; Nallapati et al., 2016), and question-answering (Hermann et al., 2015) These models use recursive neural networks (RNN), such as long-term short-term memory networks (LSTM) (Hochreiter et al., 2013; Pennington et al., 2014) to encode an input sentence into a fixed vector and generate a new output sequence from that vector, using a different RNN. To apply this sequence-to-sequence approach to natural language, word sequences (Mikolov et al., 2013; Pennington et al., 2014) are used to convert language marks into vectors that can be used as input for these networks. In 2016 attention mechanisms (these Bahau et al, 2014) make certain words more capable."}, {"heading": "4.2 Reinforcement learning for sequence generation", "text": "Reinforcement Learning (RL) is a method for training an agent to interact with a given environment in order to maximize a reward. RL has been used to solve a variety of problems, typically when an agent must perform individual actions before receiving a reward, or when the metric to be optimized is not differentiable and traditional supervised learning methods cannot be used. This is true for sequence generation tasks, since many of the metrics used to evaluate these tasks (such as BLEU, ROUGE, or METEOR) are not differentiable. To directly optimize this metric, Ranzato et al. (2015) have used the REINFORCE algorithm (Williams, 1992) to train various RNN-based models for sequence generation tasks, resulting in significant improvements over previous supervised learning methods. While their method requires an additional neural network called the critical model to predict the expected reward runs and to provide further selective functionality to the critical ones, RL does not require an additional neural network, called the critical model to predict 2016 rewards."}, {"heading": "4.3 Text summarization", "text": "Most summarization models studied in the past are extractive in nature (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015), which usually work by identifying the most important phrases in an input document and rearranging them in a new summary sequence; the newer abstract summarization models have a greater degree of freedom and can generate newer sequences; many abstract models such as Rush et al. (2015), Chopra et al. (2016), Zeng et al. (2016) and Nallapati et al. (2016) are all based on the neural encoder decoder architecture (Section 4.1); a well-researched set of summarization tasks is the Document Understanding Conference (DUC) 1. These summarization tasks are diverse, including short summaries of a single document and long summaries of multiple documents categorized by subject."}, {"heading": "5 Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 CNN/Daily Mail", "text": "We evaluate our model based on a modified version of the CNN / Daily Mail dataset (Hermann et al., 2015) and follow the same pre-processing steps described in Nallapati et al. (2016). We refer the reader to this work for a detailed description. The final dataset contains 286,817 training examples, 13,368 validation examples and 11,487 test examples. After restricting input lengths to 800 to 1 http: / / duc.nist.gov / kens and output lengths to 100 tokens, the average input and output lengths are 632 and 53 tokens respectively."}, {"heading": "5.2 New York Times", "text": "The New York Times (NYT) dataset (Sandhaus, 2008) is a large collection of articles published between 1996 and 2007. \"Although this dataset was used to train extractive summary systems (Hong and Nenkova, 2014; Li et al., 2016) or closely related models to predict the meaning of a phrase in an article (Yang and Nenkova, 2014; Nye and Nenkova, 2015; Hong et al., 2015), we are the first group to use an end-to-end abstract summary model for the meaning of a phrase in an article (Yang and Nenkova, 2014; Nye and Nenkova, 2015; Hong et al.) when we have an end-to-end abstract summary model on the article-abstract pairs of this dataset (Yang and Nenkova, 2014). While CNN / Daily Mail summaries have a similar wording to their corresponding articles, this abstract-to-to-to-to-to-abstract abstract model allows us to use the article-abstract pairs of this dataset (Yang and Nenkova, 2014)."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experiments", "text": "Setup: We evaluate the intra-decoder attention mechanism and mixed-objective learning by performing the following experiments on both sets of data. First, we perform a maximum probability training (ML) with and without intra-decoder attention (removal of cdt from equations 9 and 11 to disable intra-attention) and select the most powerful architecture. Next, we initialize our model with the best ML parameters and compare reinforcement learning (RL) with our mixed-objective learning (ML + RL) and follow our objective functions in equations 15 and 16. For ML training, we use the coercive teacher algorithm with the only difference that in each decoding step we use the previously generated token instead of the ground truth token as decoder input token yt \u2212 1, which reduces exposure distortions (Venkraet man, 2015 + 84)."}, {"heading": "6.2 Quantitative analysis", "text": "Our results for the CNN / Daily Mail dataset are presented in Table 1, and for the NYT dataset in Table 2. We note that the intra-decoder attention function helps our model achieve better ROUGE values on the CNN / Daily Mail dataset, but not on the NYT dataset. We believe that the difference in summary lengths between the CNN / Daily Mail and NYT datasets is one of the main reasons for this difference in result, as our intra-decoder was designed to improve performance over long output sequences. Further differences in the nature of the summaries and the degree of complexity and abstraction between these datasets could explain these intra-attention differences as well as the absolute ROUGE score differences between CNN / Daily Mail and NYT results. In addition, we can see that on all datasets both the RL and the ML + L models achieve significantly higher values than the state of the ML model of the Daily Mail in particular."}, {"heading": "6.3 Qualitative analysis", "text": "We perform human evaluations to ensure that our increase in ROUGE values is followed by an increase in human readability and quality. Specifically, we would like to know if the training goal ML + RL has improved readability compared to RL. Evaluation Arrangement: To perform this evaluation, we randomly select 100 test examples from the CNN / Daily Mail dataset. For each example, we then show the summary of the basic truths as well as summaries compiled by different models side by side. Results: Our human evaluation results are presented in Table 4. We can see that although RL has the highest ROUGE 1 and ROUGE L values, a rating of 1 to 10 is assigned to each summary that is the lower readability and 10 to the highest. Results: Our human evaluation results are presented in Table 4. Although RL has the highest ROUGE 1 and ROUGE L values, it provides the least readable summaries among our experiments."}, {"heading": "7 Conclusion", "text": "We introduced a new model and training method for CNN / Daily Mail that provides up-to-date results in text summary, improves the legibility of the generated summaries, and is better suited for long output sequences. We also run our abstract model for the first time on the NYT dataset. We saw that ROUGE values, despite their frequent use for evaluation, have their shortcomings and should not be the only metric to optimize the summary model for long sequences. We believe that our intra-attention decoder and our combined training target could be applied to other sequence-to-sequence tasks with long inputs and outputs, which is an interesting direction for further research."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1601.06733 .", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M Rush", "SEAS Harvard."], "venue": "Proceedings of NAACL-HLT16 pages 93\u201398.", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Heads: Headline generation as sequence prediction using an abstract feature-rich space", "author": ["Carlos A Colmenares", "Marina Litvak", "Amin Mantrach", "Fabrizio Silvestri."], "venue": "HLT-NAACL. pages 133\u2013142.", "citeRegEx": "Colmenares et al\\.,? 2015", "shortCiteRegEx": "Colmenares et al\\.", "year": 2015}, {"title": "Hedge trimmer: A parse-and-trim approach to headline generation", "author": ["Bonnie Dorr", "David Zajic", "Richard Schwartz."], "venue": "Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5. Association for Computational Linguistics, pages 1\u20138.", "citeRegEx": "Dorr et al\\.,? 2003", "shortCiteRegEx": "Dorr et al\\.", "year": 2003}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Katja Filippova", "Yasemin Altun."], "venue": "EMNLP. Citeseer, pages 1481\u20131491.", "citeRegEx": "Filippova and Altun.,? 2013", "shortCiteRegEx": "Filippova and Altun.", "year": 2013}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.08148 .", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "System combination for multi-document summarization", "author": ["Kai Hong", "Mitchell Marcus", "Ani Nenkova."], "venue": "EMNLP. pages 107\u2013117.", "citeRegEx": "Hong et al\\.,? 2015", "shortCiteRegEx": "Hong et al\\.", "year": 2015}, {"title": "Improving the estimation of word importance for news multidocument summarization-extended technical report", "author": ["Kai Hong", "Ani Nenkova"], "venue": null, "citeRegEx": "Hong and Nenkova.,? \\Q2014\\E", "shortCiteRegEx": "Hong and Nenkova.", "year": 2014}, {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01462 .", "citeRegEx": "Inan et al\\.,? 2016", "shortCiteRegEx": "Inan et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "The role of discourse units in near-extractive summarization", "author": ["Junyi Jessy Li", "Kapil Thadani", "Amanda Stent."], "venue": "17th Annual Meeting of the Special Interest Group on Discourse andDialogue. page 137.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop. Barcelona, Spain, volume 8.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Andrew Senior", "Fumin Wang", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1603.06744 .", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky."], "venue": "ACL (System Demonstrations). pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."], "venue": "arXiv preprint arXiv:1609.07843 .", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Bing Xiang"], "venue": null, "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Automatic text summarization using a machine learning approach", "author": ["Joel Larocca Neto", "Alex A Freitas", "Celso AA Kaestner."], "venue": "Brazilian Symposium on Artificial Intelligence. Springer, pages 205\u2013215.", "citeRegEx": "Neto et al\\.,? 2002", "shortCiteRegEx": "Neto et al\\.", "year": 2002}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Mohammad Norouzi", "Samy Bengio", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans"], "venue": null, "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Identification and characterization of newsworthy verbs in world news", "author": ["Benjamin Nye", "Ani Nenkova."], "venue": "HLT-NAACL. pages 1440\u20131445.", "citeRegEx": "Nye and Nenkova.,? 2015", "shortCiteRegEx": "Nye and Nenkova.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf."], "venue": "arXiv preprint arXiv:1608.05859 .", "citeRegEx": "Press and Wolf.,? 2016", "shortCiteRegEx": "Press and Wolf.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Self-critical sequence training for image captioning", "author": ["Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel."], "venue": "arXiv preprint arXiv:1612.00563 .", "citeRegEx": "Rennie et al\\.,? 2016", "shortCiteRegEx": "Rennie et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685 .", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "The new york times annotated corpus", "author": ["Evan Sandhaus."], "venue": "Linguistic Data Consortium, Philadelphia 6(12):e26752.", "citeRegEx": "Sandhaus.,? 2008", "shortCiteRegEx": "Sandhaus.", "year": 2008}, {"title": "Temporal attention model for neural machine translation", "author": ["Baskaran Sankaran", "Haitao Mi", "Yaser Al-Onaizan", "Abe Ittycheriah."], "venue": "arXiv preprint arXiv:1608.02927 .", "citeRegEx": "Sankaran et al\\.,? 2016", "shortCiteRegEx": "Sankaran et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improving multi-step prediction of learned time series models", "author": ["Arun Venkatraman", "Martial Hebert", "J Andrew Bagnell."], "venue": "AAAI. pages 3024\u2013 3030.", "citeRegEx": "Venkatraman et al\\.,? 2015", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems. pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser."], "venue": "Neural computation 1(2):270\u2013280.", "citeRegEx": "Williams and Zipser.,? 1989", "shortCiteRegEx": "Williams and Zipser.", "year": 1989}, {"title": "Detecting information-dense texts in multiple news domains", "author": ["Yinfei Yang", "Ani Nenkova."], "venue": "AAAI. pages 1650\u20131656.", "citeRegEx": "Yang and Nenkova.,? 2014", "shortCiteRegEx": "Yang and Nenkova.", "year": 2014}, {"title": "Efficient summarization with read-again and copy mechanism", "author": ["Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv preprint arXiv:1611.03382 .", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "First, extractive summarization systems form summaries by copying parts of the input (Neto et al., 2002; Dorr et al., 2003).", "startOffset": 85, "endOffset": 123}, {"referenceID": 4, "context": "First, extractive summarization systems form summaries by copying parts of the input (Neto et al., 2002; Dorr et al., 2003).", "startOffset": 85, "endOffset": 123}, {"referenceID": 2, "context": "Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text (Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016).", "startOffset": 134, "endOffset": 198}, {"referenceID": 20, "context": "Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text (Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016).", "startOffset": 134, "endOffset": 198}, {"referenceID": 37, "context": "Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text (Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016).", "startOffset": 134, "endOffset": 198}, {"referenceID": 20, "context": "Recently, neural network models (Nallapati et al., 2016; Zeng et al., 2016), based on the attentional encoder-decoder model for machine translation (Bahdanau et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 37, "context": "Recently, neural network models (Nallapati et al., 2016; Zeng et al., 2016), based on the attentional encoder-decoder model for machine translation (Bahdanau et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 0, "context": ", 2016), based on the attentional encoder-decoder model for machine translation (Bahdanau et al., 2014), were able to generate abstractive summaries with high ROUGE scores.", "startOffset": 80, "endOffset": 103}, {"referenceID": 0, "context": ", 2016), based on the attentional encoder-decoder model for machine translation (Bahdanau et al., 2014), were able to generate abstractive summaries with high ROUGE scores. However, these systems have typically focused on summarizing short input sequences (one or two sentences) to generate even shorter summaries. For example, the summaries on the DUC-2004 dataset generated by the state-of-the-art system by Zeng et al. (2016) are limited to 75 characters.", "startOffset": 81, "endOffset": 429}, {"referenceID": 7, "context": "(2016) also applied their abstractive summarization model on the CNN/Daily Mail dataset (Hermann et al., 2015), which contains input sequences of up to 800 tokens and multi-sentence summaries of up to 100 tokens.", "startOffset": 88, "endOffset": 110}, {"referenceID": 7, "context": "(2016) also applied their abstractive summarization model on the CNN/Daily Mail dataset (Hermann et al., 2015), which contains input sequences of up to 800 tokens and multi-sentence summaries of up to 100 tokens. The analysis by Nallapati et al. (2016) illustrate", "startOffset": 89, "endOffset": 253}, {"referenceID": 29, "context": "We present a new abstractive summarization model that achieves state-of-the-art results on the CNN/Daily Mail and similarly good results on the New York Times dataset (NYT) (Sandhaus, 2008).", "startOffset": 173, "endOffset": 189}, {"referenceID": 31, "context": "In this section, we present our intra-attention model based on the encoder-decoder network (Sutskever et al., 2014).", "startOffset": 91, "endOffset": 115}, {"referenceID": 30, "context": "At each decoding step t, we use an intra-temporal attention function to attend over specific parts of the encoded input sequence in addition to the decoder\u2019s own hidden state and the previouslygenerated word (Sankaran et al., 2016).", "startOffset": 208, "endOffset": 231}, {"referenceID": 20, "context": "Nallapati et al. (2016) have shown that such an intra-temporal attention can reduce the amount of repetitions when attending over long documents.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "A closely-related intra-RNN attention function has been introduced by Cheng et al. (2016) but their implementation works by modifying the underlying LSTM function, and they do not apply it to long sequence generation problems.", "startOffset": 70, "endOffset": 90}, {"referenceID": 6, "context": "We use a switch function that decides at each decoding step whether to use the token generation or the pointer (Gulcehre et al., 2016; Nallapati et al., 2016).", "startOffset": 111, "endOffset": 158}, {"referenceID": 20, "context": "We use a switch function that decides at each decoding step whether to use the token generation or the pointer (Gulcehre et al., 2016; Nallapati et al., 2016).", "startOffset": 111, "endOffset": 158}, {"referenceID": 11, "context": "Similar weight-sharing methods have been applied to language modeling (Inan et al., 2016; Press and Wolf, 2016).", "startOffset": 70, "endOffset": 111}, {"referenceID": 25, "context": "Similar weight-sharing methods have been applied to language modeling (Inan et al., 2016; Press and Wolf, 2016).", "startOffset": 70, "endOffset": 111}, {"referenceID": 35, "context": "The most widely used method to train a decoder RNN for sequence generation, called the teacher forcing\u201d algorithm (Williams and Zipser, 1989), minimizes a maximum-likelihood loss at each decoding step.", "startOffset": 114, "endOffset": 141}, {"referenceID": 14, "context": "However, minimizing Lml does not always produce the best results on discrete evaluation metrics such as ROUGE (Lin, 2004).", "startOffset": 110, "endOffset": 121}, {"referenceID": 27, "context": "This phenomenon has been observed with similar sequence generation tasks like image captioning with CIDEr (Rennie et al., 2016) and machine translation with BLEU (Wu et al.", "startOffset": 106, "endOffset": 127}, {"referenceID": 22, "context": ", 2016) and machine translation with BLEU (Wu et al., 2016; Norouzi et al., 2016).", "startOffset": 42, "endOffset": 81}, {"referenceID": 26, "context": "The first one, called exposure bias (Ranzato et al., 2015), comes from the fact that the network is fully supervised at each output token during training, always knowing the ground truth sequence up to the next token to predict, but does not have such supervision when testing, hence accumulating errors as it predicts the sequence.", "startOffset": 36, "endOffset": 58}, {"referenceID": 27, "context": "In our model, we use the self-critical policy gradient training algorithm (Rennie et al., 2016).", "startOffset": 74, "endOffset": 95}, {"referenceID": 16, "context": "It is possible to game such discrete metrics and increase their score without an actual increase in readability or relevance (Liu et al., 2016).", "startOffset": 125, "endOffset": 143}, {"referenceID": 31, "context": "Neural encoder-decoder models are widely used in NLP applications such as machine translation (Sutskever et al., 2014), summarization (Chopra et al.", "startOffset": 94, "endOffset": 118}, {"referenceID": 2, "context": ", 2014), summarization (Chopra et al., 2016; Nallapati et al., 2016), and question answering (Hermann et al.", "startOffset": 23, "endOffset": 68}, {"referenceID": 20, "context": ", 2014), summarization (Chopra et al., 2016; Nallapati et al., 2016), and question answering (Hermann et al.", "startOffset": 23, "endOffset": 68}, {"referenceID": 7, "context": ", 2016), and question answering (Hermann et al., 2015).", "startOffset": 32, "endOffset": 54}, {"referenceID": 8, "context": "These models use recurrent neural networks (RNN), such as long-short term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to encode an input sentence into a fixed vector, and create a new output sequence from that vector using another RNN.", "startOffset": 96, "endOffset": 130}, {"referenceID": 19, "context": "To apply this sequence-to-sequence approach to natural language, word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are used to convert language tokens to vectors that can be used as inputs for these networks.", "startOffset": 81, "endOffset": 128}, {"referenceID": 24, "context": "To apply this sequence-to-sequence approach to natural language, word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are used to convert language tokens to vectors that can be used as inputs for these networks.", "startOffset": 81, "endOffset": 128}, {"referenceID": 0, "context": "Attention mechanisms (Bahdanau et al., 2014) make these models more performant and scalable, allowing them to look back at parts of the encoded input sequence while the output is generated.", "startOffset": 21, "endOffset": 44}, {"referenceID": 33, "context": "One way to fix this is to allow the decoder network to point back to some specific words or subsequences of the input and copy them onto the output sequence (Vinyals et al., 2015; Ling et al., 2016).", "startOffset": 157, "endOffset": 198}, {"referenceID": 15, "context": "One way to fix this is to allow the decoder network to point back to some specific words or subsequences of the input and copy them onto the output sequence (Vinyals et al., 2015; Ling et al., 2016).", "startOffset": 157, "endOffset": 198}, {"referenceID": 0, "context": "Attention mechanisms (Bahdanau et al., 2014) make these models more performant and scalable, allowing them to look back at parts of the encoded input sequence while the output is generated. These models often use a fixed input and output vocabulary, which prevents them from learning representations for new words. One way to fix this is to allow the decoder network to point back to some specific words or subsequences of the input and copy them onto the output sequence (Vinyals et al., 2015; Ling et al., 2016). Gulcehre et al. (2016) and Merity et al.", "startOffset": 22, "endOffset": 538}, {"referenceID": 0, "context": "Attention mechanisms (Bahdanau et al., 2014) make these models more performant and scalable, allowing them to look back at parts of the encoded input sequence while the output is generated. These models often use a fixed input and output vocabulary, which prevents them from learning representations for new words. One way to fix this is to allow the decoder network to point back to some specific words or subsequences of the input and copy them onto the output sequence (Vinyals et al., 2015; Ling et al., 2016). Gulcehre et al. (2016) and Merity et al. (2016) combine this pointer mechanism with the original word generation layer in the decoder to allow the model to use either method at each decoding step.", "startOffset": 22, "endOffset": 563}, {"referenceID": 34, "context": "(2015) have applied the REINFORCE algorithm (Williams, 1992) to train various RNN-based models for sequence generation tasks, leading to significant improvements compared to previous supervised learning methods.", "startOffset": 44, "endOffset": 60}, {"referenceID": 26, "context": "In order to optimize that metric directly, Ranzato et al. (2015) have applied the REINFORCE algorithm (Williams, 1992) to train various RNN-based models for sequence generation tasks, leading to significant improvements compared to previous supervised learning methods.", "startOffset": 43, "endOffset": 65}, {"referenceID": 26, "context": "In order to optimize that metric directly, Ranzato et al. (2015) have applied the REINFORCE algorithm (Williams, 1992) to train various RNN-based models for sequence generation tasks, leading to significant improvements compared to previous supervised learning methods. While their method requires an additional neural network, called a critic model, to predict the expected reward and stabilize the objective function gradients, Rennie et al. (2016) designed a selfcritical sequence training method that does not require this critic model and lead to further improvements on image captioning tasks.", "startOffset": 43, "endOffset": 451}, {"referenceID": 21, "context": "Most summarization models studied in the past are extractive in nature (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015), which usually work by identifying the most important phrases of an input document and re-arranging them into a new summary sequence.", "startOffset": 71, "endOffset": 161}, {"referenceID": 4, "context": "Most summarization models studied in the past are extractive in nature (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015), which usually work by identifying the most important phrases of an input document and re-arranging them into a new summary sequence.", "startOffset": 71, "endOffset": 161}, {"referenceID": 5, "context": "Most summarization models studied in the past are extractive in nature (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015), which usually work by identifying the most important phrases of an input document and re-arranging them into a new summary sequence.", "startOffset": 71, "endOffset": 161}, {"referenceID": 3, "context": "Most summarization models studied in the past are extractive in nature (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015), which usually work by identifying the most important phrases of an input document and re-arranging them into a new summary sequence.", "startOffset": 71, "endOffset": 161}, {"referenceID": 4, "context": "Most abstractive summarization models have been evaluated on the DUC-2004 dataset, and outperform extractive models on that task (Dorr et al., 2003).", "startOffset": 129, "endOffset": 148}, {"referenceID": 2, "context": ", 2003; Filippova and Altun, 2013; Colmenares et al., 2015), which usually work by identifying the most important phrases of an input document and re-arranging them into a new summary sequence. The more recent abstractive summarization models have more degrees of freedom and can create more novel sequences. Many abstractive models such as Rush et al. (2015), Chopra et al.", "startOffset": 35, "endOffset": 360}, {"referenceID": 2, "context": "(2015), Chopra et al. (2016), Zeng et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "(2015), Chopra et al. (2016), Zeng et al. (2016) and Nallapati et al.", "startOffset": 8, "endOffset": 49}, {"referenceID": 2, "context": "(2015), Chopra et al. (2016), Zeng et al. (2016) and Nallapati et al. (2016) are all based on the neural encoder-decoder architecture (Section 4.", "startOffset": 8, "endOffset": 77}, {"referenceID": 20, "context": "Only Nallapati et al. (2016) have tried to apply abstractive summarization techniques to longer input and outputs with the CNN/Daily Mail dataset.", "startOffset": 5, "endOffset": 29}, {"referenceID": 7, "context": "We evaluate our model on a modified version of the CNN/Daily Mail dataset (Hermann et al., 2015), following the same pre-processing steps described in Nallapati et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 7, "context": "We evaluate our model on a modified version of the CNN/Daily Mail dataset (Hermann et al., 2015), following the same pre-processing steps described in Nallapati et al. (2016). We refer the reader to that paper for a detailed description.", "startOffset": 75, "endOffset": 175}, {"referenceID": 29, "context": "The New York Times (NYT) dataset (Sandhaus, 2008) is a large collection of articles published between 1996 and 2007.", "startOffset": 33, "endOffset": 49}, {"referenceID": 10, "context": "Even though this dataset has been used to train extractive summarization systems (Hong and Nenkova, 2014; Li et al., 2016) or closely-related models for predicting the importance of a phrase in an article (Yang and Nenkova, 2014; Nye and Nenkova, 2015; Hong et al.", "startOffset": 81, "endOffset": 122}, {"referenceID": 13, "context": "Even though this dataset has been used to train extractive summarization systems (Hong and Nenkova, 2014; Li et al., 2016) or closely-related models for predicting the importance of a phrase in an article (Yang and Nenkova, 2014; Nye and Nenkova, 2015; Hong et al.", "startOffset": 81, "endOffset": 122}, {"referenceID": 36, "context": ", 2016) or closely-related models for predicting the importance of a phrase in an article (Yang and Nenkova, 2014; Nye and Nenkova, 2015; Hong et al., 2015), we are the first group to run an end-to-end abstractive summarization model on the article-abstract pairs of this dataset.", "startOffset": 90, "endOffset": 156}, {"referenceID": 23, "context": ", 2016) or closely-related models for predicting the importance of a phrase in an article (Yang and Nenkova, 2014; Nye and Nenkova, 2015; Hong et al., 2015), we are the first group to run an end-to-end abstractive summarization model on the article-abstract pairs of this dataset.", "startOffset": 90, "endOffset": 156}, {"referenceID": 9, "context": ", 2016) or closely-related models for predicting the importance of a phrase in an article (Yang and Nenkova, 2014; Nye and Nenkova, 2015; Hong et al., 2015), we are the first group to run an end-to-end abstractive summarization model on the article-abstract pairs of this dataset.", "startOffset": 90, "endOffset": 156}, {"referenceID": 17, "context": "We tokenize the input and abstract pairs with the Stanford tokenizer (Manning et al., 2014).", "startOffset": 69, "endOffset": 91}, {"referenceID": 17, "context": "Pointer supervision: We run each input and abstract sequence through the Stanford named entity recognizer (NER) (Manning et al., 2014).", "startOffset": 112, "endOffset": 134}, {"referenceID": 32, "context": "instead of the ground-truth token as the decoder input token yt\u22121, which reduces exposure bias (Venkatraman et al., 2015).", "startOffset": 95, "endOffset": 121}, {"referenceID": 24, "context": "Input word embeddings are 100-dimensional and are initialized with GloVe (Pennington et al., 2014).", "startOffset": 73, "endOffset": 98}, {"referenceID": 20, "context": "words-lvt2k-temp-att (Nallapati et al., 2016) 35.", "startOffset": 21, "endOffset": 45}, {"referenceID": 12, "context": "with Adam (Kingma and Ba, 2014) with a batch size of 50 and a learning rate \u03b1 of 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 20, "context": "In particular, these methods clearly surpass the state-of-the-art model from Nallapati et al. (2016) on the CNN/Daily Mail dataset.", "startOffset": 77, "endOffset": 101}], "year": 2017, "abstractText": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit \u201cexposure bias\u201d \u2013 they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.", "creator": "LaTeX with hyperref package"}}}