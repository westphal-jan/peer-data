{"id": "1705.04350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Imagination improves Multimodal Translation", "abstract": "Multimodal machine translation is the task of translating sentences in a visual context. We decompose this problem into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.", "histories": [["v1", "Thu, 11 May 2017 18:49:17 GMT  (1023kb,D)", "http://arxiv.org/abs/1705.04350v1", "Under review"], ["v2", "Fri, 7 Jul 2017 09:18:55 GMT  (1023kb,D)", "http://arxiv.org/abs/1705.04350v2", "Clarified main contributions, minor correction to Equation 8, additional comparisons in Table 2, added more related work"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["desmond elliott", "\\'akos k\\'ad\\'ar"], "accepted": false, "id": "1705.04350"}, "pdf": {"name": "1705.04350.pdf", "metadata": {"source": "CRF", "title": "Imagination improves Multimodal Translation", "authors": ["Desmond Elliott", "\u00c1kos K\u00e1d\u00e1r"], "emails": ["d.elliott@ed.ac.uk,", "a.kadar@uvt.nl"], "sections": [{"heading": "1 Introduction", "text": "This is an emerging task in the field of multilingual multimodal natural language processing. Advances in this task could prove useful for translating captions of online news articles, and for multilingual closed captions in international television and cinema. Initial efforts have not convincingly shown that visual context can improve translation quality. In the results of the first multimodal translation shared task, only three systems are more successful than a purely text-based text model, and the best-performing image translation model was not equally effective with or without the visual features (Specia et al., 2016). It remains an open question how translation models should use visual contexts. Translation DecoderWe present a multi-task trained learning model, multimodal translations into learning a translation model and visually composite representations."}, {"heading": "2 Problem Formulation", "text": "Multimodal translation is the task of producing target language translation y, because the source language set x and additional contexts, such as an image v. Specifically, x should be a source language set consisting of N signs: x1, x2,..., xn and should be a target language set consisting of M signs: y1, y2,.., ym. Training data consists of tuplesD signs (x, y, v), where x is a description of the image v, and y is a translation of x.Multimodal translation has previously been presented as minimizing the negative log probability of a translation model additionally conditioned on the image, i.e. J (\u03b8) = \u2212 j log p (yj | y < j, x, v). Here we decompose the task (the implementation will be available upon publication).the problem into learning to translate and learn visually grouped representations."}, {"heading": "3 Imagination Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Shared Encoder", "text": "The encoder network of our model learns a representation of a sequence of N characters x1... n in the source language with a bidirectional recursive neural network (Schuster and Paliwal, 1997), which is divided between the various tasks, each character represented by a uniform vector xi, mapped by a learned matrix E into an embedded egg: ei = xi \u00b7 E (2) A sentence is processed by a pair of recurring neural networks, one capturing the sequence from left to right (forward) and the other the sequence from right to left (backward). The initial state of the encoder h \u2212 1 is a learned parameter: \u2212 \u2192 hi = \u2212 \u2192 RNN (\u2212 \u2192 hi \u2212 1, ei) (3) \u2022 hi = \u2190 \u2212 \u2212 RNN (\u2190 \u2212 hi \u2212 1, ei) (4) Each token in the source language input is represented by a concatenation."}, {"heading": "3.2 Neural Machine Translation Decoder", "text": "The decoder of the translation model is an attention-based recursive neural network (Bahdanau et al., 2015). Tokens in the decoder are represented by a uniform vector yj, which is mapped by a learned matrix into an embedded ej. Ey: ej = yj \u00b7 Ey (6) The inputs to the decoder are the previously predicted token yj \u2212 1, the previous decoder state dj \u2212 1, and a time-dependent context vector cj, which is calculated using the hidden states of the encoder: dj = RNN (dj \u2212 1, yj \u2212 1, ej) (7) The initial state of the decoder state d \u2212 1 is a nonlinear transformation of the mean of the encoder states: d-1 = tanh (1N \u00b2 i hi hi hi) (8) The context vector vj cj is a weighted sum of the encoded states, where there is a hidden state = N."}, {"heading": "3.3 Imaginet Decoder", "text": "The image prediction decoder is trained to predict the visual characteristic vector of the image associated with a sentence (Chrupa\u0142a et al., 2015). It encourages the common encoder to learn grounded representations for the source language.We represent a sentence by first generating a sequence of concatenated hidden state vectors in the encoder (Section 3.1). Then we take the average of the hidden states of the encoder to represent the sentence as a single vector h = 1N \u2211 N i hi. This is the input to a forward neural network that predicts the visual characteristic vector v \u0445 of the sentence with the parameters Wvis: v = tanh (Wvis \u00b7 h) (14).This decoder is trained to predict the true image vector v with a margin-based target that is d (\u00b7, \u00b7 \u00b7) v \u00b7 v \u00b7 v \u00b7 v \u00b7 v \u00b7 v (15)."}, {"heading": "4 Data", "text": "We evaluate our model using the Multi30K benchmark dataset (Elliott et al., 2016), which represents the largest collection of images with sets in multiple languages. This dataset contains 31,014 images paired with a set in English and a German translation: 29,000 instances are reserved for training, 1,014 for development, and 1,000 for evaluation. 22Multi30K also contains 155K independently collected descriptions for German and English. We do not use this data, resulting in vocabulary sizes of 10,214 types for English and 16,022 for German.We also use two external datasets to evaluate our model: the MS COCO dataset with English described images (Chen al., 2015) and the German vocabulary with 10,214 types for English and 16,022 for German."}, {"heading": "5 Experiments", "text": "We evaluate our multitasking approach with internal and external resources. First, we report on the results of models that have only been trained with the Multi30K dataset. Furthermore, we report on the results of training the IMAGINET decoder with the COCO dataset. Finally, we report on the results of integrating the external text parallel to the news commentary into our model. Everywhere we report on the performance of En \u2192 De-Translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) using reduced references."}, {"heading": "5.1 Hyperparameters", "text": "The encoder is a 1000D gated recurrent unit bidirectional recurrent neural network (Cho et al., 2014, GRU) with 620D embedding. We share all encoder parameters between the primary and additional task. The translation decoder is a 1000D GRU recursive neural network with a 2000D context vector over the encoder states and 620D word embedding (Sennrich et al., 2017).The Imaginet decoder is a single-layer feed network in which we learn the parameters Wvis-R2048x2000 to predict the true image vector with \u03b1 = 0.1 for the Imaginet target (Eq.15).The models are trained with the Adam Optimizer with the standard hyperparameters (Kingma and Ba, 2015) in minibatches of 80 instances. The translation task is defined as the primary task and conversion EU is achieved if the number of the gradients for both BL1 and BL0 is not increased."}, {"heading": "5.2 In-domain experiments", "text": "We begin by presenting the results of our multi-task model, which has been trained exclusively with the Multi30K dataset. We compare it with modern approaches and text-based baselines. Moses is the phrase-based machine translation model (Koehn et al., 2007) reported in (Specia et al., 2016). NMT is a purely text-based neural machine translation model. Calixtoet al. (2017a) is a model with double attention to the source language and the image. Calixto et al. (2017b) is a multimodel translation model that builds the decoder on semantic image vectors extracted from the VGG-19 CNN. Hitschler et al. (2016) uses visual features in a targeted retrieval model for translation. Toyama et al al al. (2016) is most similar to our approach: It is a multimodal NMT model that incorporates the preconceived and pre-conceived quellular language."}, {"heading": "5.3 External described image data", "text": "Let us remember from Section 2 that we are interested in scenarios where x, y and v come from different sources. We are now experimenting with separating the translation data from the described image data using Dimage: MS COCO dataset with 83K described image data and Dtext: Multi30K parallel text. Table 3 shows the results of this experiment. We note that there is no significant difference between training the IMAGINET decoder on in-domain data (Multi30K) and out-of-domain data (COCO), which confirms that we can separate the parallel text from the described image datas.3Due to the differences in the vocabulary of the repsective datasets, we do not practice examples in which more than 10% of the tokens are outside the vocabulary of the Multi30K dataset."}, {"heading": "5.4 External parallel text data", "text": "In these experiments, we link the Multi30K and News Commentary datasets to a single Dtext training dataset, similar to Freitag and Al-Onaizan (2016). We compare our model with Calixto et al. (2017a), which pre-translates and backtranslates its model on the WMT '15 English-German parallel text (Sennrich et al., 2016) additional sentences from the bilingual independent descriptions of the Multi30K dataset (footnote 2).Table 4 presents the results. The text-based NMT model with subwords is 1.2 meteor points lower than the decoding of the German text. Nevertheless, the model trained by concentrating the parallel texts is a 2.7 meteor point improvement over this baseline (+ NC) and corresponds to the performance of our multitasking model, which uses only in the domain data (Section 5.2), which we essentially link with an improvement of the parallel text."}, {"heading": "5.5 Ensemble results", "text": "Table 6 presents the results of the similarity of different randomly initialized models. We achieve a starting result of 57.6 Meteor for a model that is trained only on domain data. In models trained on subwords and out-of-domain data, the improvements are more pronounced. An ensemble of baselines trained on subwords is initially worse than an ensemble trained on Zmorge decomposted words. However, we always see an improvement compared to models trained on in- and out-of-domain data. Our best ensemble is trained on Multi30K parallel text, news com parallel text and COCO descriptions to set a new state-of-the-art result of 59.3 Meteor."}, {"heading": "5.6 Qualitative Examples", "text": "Table 5 shows examples of where the multitasking model improves or worsens translation performance compared to the base model; the first example shows that the base model makes a significant error in translating the children's pose by translating \"on their stomachs\" to \"on their faces\"; the middle example shows that the base model translates the dog by walking (\"running\") and then makes grammatical and sensory errors after the sentence marker; both models fail to translate the word \"dangling,\" which is a low-frequency word in the training data; there are cases where the baseline produces better translations than the multitasking model: in the example below, our model translates a bird flying through the water (\"through\") rather than \"over\" the water."}, {"heading": "6 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Does the model learn grounded representations?", "text": "We answer this question by evaluating the Imaginet decoder in an image set ranking task using a Source4We used the language set MT-ComparEval (Klejch et al., 2015), from which we predict its image vector v val. The predicted vector v can be compared with the true image vectors v in the evaluation data using the cosmic distance to generate a ranking order of the images. Our model provides an average of 11.0 for the true image compared to the predicted image vector. Figure 2 shows examples of the closest neighbors of the images predicted by our multitasking model. We can see that the combination of multitasking source language representations and the IMAGINET decoder leads to the prediction of relevant images, confirming that the shared encoder actually learns visually sound representations."}, {"heading": "6.2 The effect of visual feature vectors", "text": "It has already been shown that the choice of visual characteristics can influence the performance of visual and speech models (Jabri et al., 2016; Kiela et al., 2016). We compare the effect of training the IMAGINET decoder to predict various types of image characteristics, namely: 4096D characteristics extracted from the \"fc7\" layer of the VGG-19 model (Simonyan and Zisserman, 2015), 2048D characteristics extracted from the \"pool5 / 7x7 s1\" layer of the InceptionNet V3 (Szegedy et al., 2015), and 2048D characteristics extracted from the \"avg pool\" layer of the ResNet-50 (He et al., 2016). Table 7 shows the results of this experiment. There is a clear difference between predicting the 2048D vectors (Inception Vektor-3 and ResVG-50 and Vektor-19) and the Vektor-19-Vektor-2-Vector."}, {"heading": "7 Related work", "text": "Her work on multimodal models has focused on approaches that use either semantic or spatial image functions as inputs to a translation model 2016. Semantic image functions are typically used from the final pooling layer of an upstream object recognition CNN, e.g. \"pool5 / 7x7 s1\" in GoogLeNet et al., 2016), or as additional features in a5We are conditioning input to the encoder (Elliott et al., 2015). Huang et al., 2016, in the decoder (Libovicky), and as additional features in a5We are using the upstream models of Keras (Chollet, 2015)."}, {"heading": "8 Conclusion", "text": "We divide multimodal translation into two sub-problems: learning to translate and learning visually sound representations. In a multimodal learning framework, we show how these sub-problems can be solved by sharing an encoder between a translation model and an image prediction model. Our approach achieves state-of-the-art results on the Multi30K dataset without using images for translation. We show that training with separate parallel text and described image datasets does not harm performance and encourages future research on multitasking with different data sources. In addition, we still find improvements over image prediction when we improve our text base line with parallel text outside the domain. Future work will include adapting our decomposition to other NLP tasks that could benefit from outside the domain, such as semantic role labeling, dependency saving and question-answering; exploring methods of entering the (predicted) age into the coding model; experimenting with translating language;"}, {"heading": "Acknowledgments", "text": "We thank Joost Bastings for his multi-tasking model Nematus, Wilker Aziz for discussions on the formulation of the problem, Stella Frank for finding and explaining the qualitative examples and Afra Alishahi, Grzegorz Chrupa\u0142a and Philip Schulz for feedback on earlier drafts of the paper. DE thanks the support of the NWO Vici Scholarship No. 277-89-002 awarded to K. Sima'an, an Amazon Academic Research Award and an NVIDIA Academic Hardware Grant."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "author": ["J. Bingel", "A. S\u00f8gaard."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. pages 164\u2013169.", "citeRegEx": "Bingel and S\u00f8gaard.,? 2017", "shortCiteRegEx": "Bingel and S\u00f8gaard.", "year": 2017}, {"title": "Multimodal attention for neural machine translation", "author": ["Ozan Caglayan", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "CoRR abs/1609.03976", "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Dcu-uva multimodal mt system report", "author": ["Iacer Calixto", "Desmond Elliott", "Stella Frank."], "venue": "Proceedings of the First Conference on Machine Translation. pages 634\u2013638.", "citeRegEx": "Calixto et al\\.,? 2016", "shortCiteRegEx": "Calixto et al\\.", "year": 2016}, {"title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017a", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Incorporating global visual features into attention-based neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1701.06521.", "citeRegEx": "Calixto et al\\.,? 2017b", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine Learning 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick."], "venue": "CoRR abs/1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Learning language through pictures", "author": ["Grzegorz Chrupa\u0142a", "\u00c1kos K\u00e1d\u00e1r", "Afra Alishahi."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Chrupa\u0142a et al\\.,? 2015", "shortCiteRegEx": "Chrupa\u0142a et al\\.", "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. He", "D. Yu", "H. Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil. Sima\u2019an", "Lucia Specia"], "venue": "In Proceedings of the 5th Workshop on Vision and Language", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Fast domain adaptation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan."], "venue": "CoRR abs/1612.06897.", "citeRegEx": "Freitag and Al.Onaizan.,? 2016", "shortCiteRegEx": "Freitag and Al.Onaizan.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29, pages 1019\u20131027.", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pages 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["J. Hitschler", "S. Schamoni", "S. Riezler."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 2399\u2013 2409.", "citeRegEx": "Hitschler et al\\.,? 2016", "shortCiteRegEx": "Hitschler et al\\.", "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Attention-based multimodal neural machine translation", "author": ["P. Huang", "F. Liu", "S. Shiang", "J. Oh", "C. Dyer."], "venue": "Proceedings of the First Conference on Machine Translation. pages 639\u2013645.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten."], "venue": "CoRR abs/1606.08390.", "citeRegEx": "Jabri et al\\.,? 2016", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["Akos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi."], "venue": "arXiv preprint arXiv:1602.08952 .", "citeRegEx": "K\u00e1d\u00e1r et al\\.,? 2016", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "Comparing Data Sources and Architectures for Deep Visual Representation Learning in Semantics", "author": ["D. Kiela", "A.L. Ver\u0151", "S. Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 447\u2013456.", "citeRegEx": "Kiela et al\\.,? 2016", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "International Conference on Learning Representations .", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Mt-compareval: Graphical evaluation interface for machine translation development", "author": ["Ond\u0159ej Klejch", "Eleftherios Avramidis", "Aljoscha Burchardt", "Martin Popel."], "venue": "The Prague Bulletin of Mathematical Linguistics 104(1):63\u201374.", "citeRegEx": "Klejch et al\\.,? 2015", "shortCiteRegEx": "Klejch et al\\.", "year": 2015}, {"title": "Improving sentence compression by learning to predict gaze", "author": ["Sigrid Klerke", "Yoav Goldberg", "Anders S\u00f8gaard."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Klerke et al\\.,? 2016", "shortCiteRegEx": "Klerke et al\\.", "year": 2016}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens"], "venue": "In Proceedings of the 45th Annual meeting of Association", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Cuni system for wmt16 automatic post-editing and multimodal translation tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina."], "venue": "Proceedings of the First Conference on Machine Translation. pages 646\u2013654.", "citeRegEx": "Libovick\u00fd et al\\.,? 2016", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Zeroresource machine translation by multimodal encoder-decoder network with multimedia pivot", "author": ["Hideki Nakayama", "Noriki Nishida."], "venue": "CoRR abs/1611.04503.", "citeRegEx": "Nakayama and Nishida.,? 2016", "shortCiteRegEx": "Nakayama and Nishida.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Keystroke dynamics as signal for shallow syntactic parsing", "author": ["Barbara Plank."], "venue": "26th International Conference on Computational Linguistics. pages 609\u2013619.", "citeRegEx": "Plank.,? 2016", "shortCiteRegEx": "Plank.", "year": 2016}, {"title": "A correlational encoder decoder architecture for pivot based sequence generation", "author": ["Amrita Saha", "Mitesh M. Khapra", "Sarath Chandar", "Janarthanan Rajendran", "Kyunghyun Cho."], "venue": "26th International Conference on Computational Linguistics: Techni-", "citeRegEx": "Saha et al\\.,? 2016", "shortCiteRegEx": "Saha et al\\.", "year": 2016}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima."], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pages 5149\u20135152.", "citeRegEx": "Schuster and Nakajima.,? 2012", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Nematus: a Toolkit for Neural Machine Translation", "author": ["R. Sennrich", "O. Firat", "K. Cho", "A. Birch", "B. Haddow", "J. Hitschler", "M. Junczys-Dowmunt", "S. L\u00e4ubli", "A. Valerio Miceli Barone", "J. Mokry", "M. N\u0103dejde."], "venue": "CoRR abs/1703.04357.", "citeRegEx": "Sennrich et al\\.,? 2017", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 86\u201396.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Zmorge: A german morphological lexicon extracted from wiktionary", "author": ["Rico Sennrich", "Beat Kunz."], "venue": "Language Resources and Evaluation Conference. pages 1063\u20131067.", "citeRegEx": "Sennrich and Kunz.,? 2014", "shortCiteRegEx": "Sennrich and Kunz.", "year": 2014}, {"title": "Shef-multimodal: Grounding machine translation on images", "author": ["Kashif Shah", "Josiah Wang", "Lucia Specia."], "venue": "Proceedings of the First Conference on Machine Translation. pages 660\u2013665.", "citeRegEx": "Shah et al\\.,? 2016", "shortCiteRegEx": "Shah et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna."], "venue": "CoRR abs/1512.00567.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann."], "venue": "Eight International Conference on Language Resources and Evaluation (LREC\u201912).", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Neural machine translation with latent semantic of image and text", "author": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo."], "venue": "CoRR abs/1611.08459.", "citeRegEx": "Toyama et al\\.,? 2016", "shortCiteRegEx": "Toyama et al\\.", "year": 2016}, {"title": "Variational neural machine translation", "author": ["B. Zhang", "D. Xiong", "J. Su", "H. Duan", "M. Zhang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pages 521\u2013530.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 41, "context": "Multimodal machine translation is the task of translating sentences in context, such as images paired with a parallel text (Specia et al., 2016).", "startOffset": 123, "endOffset": 144}, {"referenceID": 41, "context": "In the results of the First Multimodal Translation Shared Task, only three systems outperformed an off-the-shelf text-only phrase-based machine translation model, and the best performing system was equally effective with or without the visual features (Specia et al., 2016).", "startOffset": 252, "endOffset": 273}, {"referenceID": 0, "context": "The translation decoder is an attention-based neural machine translation model (Bahdanau et al., 2015), and the image prediction decoder is trained to predict a global feature vector of an image that is associated with a sentence (Chrupa\u0142a et al.", "startOffset": 79, "endOffset": 102}, {"referenceID": 22, "context": "It has been shown that grounded representations are qualitatively different from their text-only counterparts (K\u00e1d\u00e1r et al., 2016) and correlate better with human similarity judgements (Chrupa\u0142a et al.", "startOffset": 110, "endOffset": 130}, {"referenceID": 10, "context": ", 2016) and correlate better with human similarity judgements (Chrupa\u0142a et al., 2015).", "startOffset": 62, "endOffset": 85}, {"referenceID": 19, "context": "We assess the success of the grounded learning by evaluating the image prediction model on an image\u2013sentence ranking task to determine if the shared representations are useful for image retrieval (Hodosh et al., 2013).", "startOffset": 196, "endOffset": 217}, {"referenceID": 14, "context": "We evaluate Imagination on the Multi30K dataset (Elliott et al., 2016) using a combination of in-domain and out-of-domain data.", "startOffset": 48, "endOffset": 70}, {"referenceID": 7, "context": "In the experiments with out-of-domain resources, we find that the improvement in translation quality holds when training the IMAGINET decoder on the MS COCO dataset of described images (Chen et al., 2015).", "startOffset": 185, "endOffset": 204}, {"referenceID": 43, "context": "Furthermore, if we significantly improve our text-only baseline using out-of-domain parallel text from the News Commentary corpus (Tiedemann, 2012), we still find improvements in translation quality from the auxiliary image prediction task.", "startOffset": 130, "endOffset": 147}, {"referenceID": 29, "context": "We train our multitask model following the approach of Luong et al. (2016). We define a primary task and an auxiliary task, and a set of parameters \u03b8 to be shared between the tasks.", "startOffset": 55, "endOffset": 75}, {"referenceID": 35, "context": "n in the source language with a bidirectional recurrent neural network (Schuster and Paliwal, 1997).", "startOffset": 71, "endOffset": 99}, {"referenceID": 0, "context": "The translation model decoder is an attentionbased recurrent neural network (Bahdanau et al., 2015).", "startOffset": 76, "endOffset": 99}, {"referenceID": 10, "context": "The image prediction decoder is trained to predict the visual feature vector of the image associated with a sentence (Chrupa\u0142a et al., 2015).", "startOffset": 117, "endOffset": 140}, {"referenceID": 14, "context": "We evaluate our model using the benchmark Multi30K dataset (Elliott et al., 2016), which is the largest collection of images paired with sentences in multiple languages.", "startOffset": 59, "endOffset": 81}, {"referenceID": 38, "context": "We additionally decompound the German text using Zmorge (Sennrich and Kunz, 2014).", "startOffset": 56, "endOffset": 81}, {"referenceID": 7, "context": "We also use two external datasets to evaluate our model: the MS COCO dataset of English described images (Chen et al., 2015), and the English-German News Commentary parallel corpus (Tiedemann, 2012).", "startOffset": 105, "endOffset": 124}, {"referenceID": 43, "context": ", 2015), and the English-German News Commentary parallel corpus (Tiedemann, 2012).", "startOffset": 64, "endOffset": 81}, {"referenceID": 34, "context": "When we perform experiments with the News Commentary corpus, we first calculate a 17,597 sub-word vocabulary using SentencePiece (Schuster and Nakajima, 2012) over the concatentation of the Multi30K and News Commentary datasets.", "startOffset": 129, "endOffset": 158}, {"referenceID": 42, "context": "Images are represented by 2048D vectors extracted from the \u2018pool5/7x7 s1\u2019 layer of the GoogLeNet v3 CNN (Szegedy et al., 2015).", "startOffset": 104, "endOffset": 126}, {"referenceID": 11, "context": "Throughout, we report performance of the En\u2192De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al.", "startOffset": 72, "endOffset": 99}, {"referenceID": 31, "context": "Throughout, we report performance of the En\u2192De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) against lowercased tokenized references.", "startOffset": 109, "endOffset": 132}, {"referenceID": 36, "context": "The translation decoder is a 1000D GRU recurrent neural network, with a 2000D context vector over the encoder states, and 620D word embeddings (Sennrich et al., 2017).", "startOffset": 143, "endOffset": 166}, {"referenceID": 24, "context": "The models are trained using the Adam optimiser with the default hyperparameters (Kingma and Ba, 2015) in minibatches of 80 instances.", "startOffset": 81, "endOffset": 102}, {"referenceID": 16, "context": "2 for the embeddings and the recurrent connections in both tasks (Gal and Ghahramani, 2016).", "startOffset": 65, "endOffset": 91}, {"referenceID": 27, "context": "Moses is the phrase-based machine translation model (Koehn et al., 2007) reported in (Specia et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 41, "context": ", 2007) reported in (Specia et al., 2016).", "startOffset": 20, "endOffset": 41}, {"referenceID": 3, "context": "Calixto et al. (2017b) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Calixto et al. (2017b) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN. Hitschler et al. (2016) uses visual features in a target-side retrieval model for translation.", "startOffset": 0, "endOffset": 165}, {"referenceID": 3, "context": "Calixto et al. (2017b) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN. Hitschler et al. (2016) uses visual features in a target-side retrieval model for translation. Toyama et al. (2016) is most comparable to our approach: it is a multimodal variational NMT model that infers latent variables to represent the source language semantics from the image and linguistic data.", "startOffset": 0, "endOffset": 257}, {"referenceID": 44, "context": "It also outperforms Toyama et al. (2016), which also only uses images for training.", "startOffset": 20, "endOffset": 41}, {"referenceID": 37, "context": "(2017a), who pre-train their model on the WMT\u201915 English-German parallel text and backtranslate (Sennrich et al., 2016) additional sentences from the bilingual independent descriptions in the Multi30K dataset (Footnote 2).", "startOffset": 96, "endOffset": 119}, {"referenceID": 12, "context": "In these experiments, we concatenate the Multi30K and News Commentary datasets into a single Dtext training dataset, similar to Freitag and Al-Onaizan (2016). We compare our model against Calixto et al.", "startOffset": 128, "endOffset": 158}, {"referenceID": 3, "context": "We compare our model against Calixto et al. (2017a), who pre-train their model on the WMT\u201915 English-German parallel text and backtranslate (Sennrich et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 25, "context": "We used MT-ComparEval (Klejch et al., 2015)", "startOffset": 22, "endOffset": 43}, {"referenceID": 21, "context": "It has previously been shown that the choice of visual features can affect the performance of vision and language models (Jabri et al., 2016; Kiela et al., 2016).", "startOffset": 121, "endOffset": 161}, {"referenceID": 23, "context": "It has previously been shown that the choice of visual features can affect the performance of vision and language models (Jabri et al., 2016; Kiela et al., 2016).", "startOffset": 121, "endOffset": 161}, {"referenceID": 40, "context": "We compare the effect of training the IMAGINET decoder to predict different types of image features, namely: 4096D features extracted from the \u2018fc7\u2018\u2019 layer of the VGG-19 model (Simonyan and Zisserman, 2015), 2048D features extracted from the \u2018pool5/7x7 s1\u2019 layer of InceptionNet V3 (Szegedy et al.", "startOffset": 176, "endOffset": 206}, {"referenceID": 42, "context": "We compare the effect of training the IMAGINET decoder to predict different types of image features, namely: 4096D features extracted from the \u2018fc7\u2018\u2019 layer of the VGG-19 model (Simonyan and Zisserman, 2015), 2048D features extracted from the \u2018pool5/7x7 s1\u2019 layer of InceptionNet V3 (Szegedy et al., 2015), and 2048D features extracted from \u2018avg pool\u2018 layer of ResNet-50 (He et al.", "startOffset": 282, "endOffset": 304}, {"referenceID": 17, "context": ", 2015), and 2048D features extracted from \u2018avg pool\u2018 layer of ResNet-50 (He et al., 2016).", "startOffset": 73, "endOffset": 90}, {"referenceID": 42, "context": "\u2018pool5/7x7 s1\u2019 in GoogLeNet (Szegedy et al., 2015).", "startOffset": 28, "endOffset": 50}, {"referenceID": 13, "context": "This type of feature vector has been used conditioning input to the encoder (Elliott et al., 2015; Huang et al., 2016), in the decoder (Libovick\u00fd et al.", "startOffset": 76, "endOffset": 118}, {"referenceID": 20, "context": "This type of feature vector has been used conditioning input to the encoder (Elliott et al., 2015; Huang et al., 2016), in the decoder (Libovick\u00fd et al.", "startOffset": 76, "endOffset": 118}, {"referenceID": 28, "context": ", 2016), in the decoder (Libovick\u00fd et al., 2016), or as additional features in a", "startOffset": 24, "endOffset": 48}, {"referenceID": 9, "context": "We used the pre-trained models from Keras (Chollet, 2015) downloaded from https://github.", "startOffset": 42, "endOffset": 57}, {"referenceID": 39, "context": "phrase-based translation model (Shah et al., 2016; Hitschler et al., 2016).", "startOffset": 31, "endOffset": 74}, {"referenceID": 18, "context": "phrase-based translation model (Shah et al., 2016; Hitschler et al., 2016).", "startOffset": 31, "endOffset": 74}, {"referenceID": 3, "context": "These features have been used in \u201cdouble-attention models\u201d, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a).", "startOffset": 179, "endOffset": 247}, {"referenceID": 2, "context": "These features have been used in \u201cdouble-attention models\u201d, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a).", "startOffset": 179, "endOffset": 247}, {"referenceID": 4, "context": "These features have been used in \u201cdouble-attention models\u201d, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a).", "startOffset": 179, "endOffset": 247}, {"referenceID": 45, "context": "(2016) extend the Variational Neural Machine Translation model (Zhang et al., 2016) by inferring latent variables to explicitly model the semantics of source sentences from both image and linguistic information.", "startOffset": 63, "endOffset": 83}, {"referenceID": 42, "context": "More related to our work are the recent papers by Toyama et al. (2016), Saha et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 32, "context": "(2016), Saha et al. (2016), and Nakayama and Nishida (2016).", "startOffset": 8, "endOffset": 27}, {"referenceID": 30, "context": "(2016), and Nakayama and Nishida (2016). Toyama et al.", "startOffset": 12, "endOffset": 40}, {"referenceID": 30, "context": "(2016), and Nakayama and Nishida (2016). Toyama et al. (2016) extend the Variational Neural Machine Translation model (Zhang et al.", "startOffset": 12, "endOffset": 62}, {"referenceID": 30, "context": "(2016), and Nakayama and Nishida (2016). Toyama et al. (2016) extend the Variational Neural Machine Translation model (Zhang et al., 2016) by inferring latent variables to explicitly model the semantics of source sentences from both image and linguistic information. Similar to our approach, their model does not condition the translation model on images for translation. They report improvements on the Multi30K data set when using multimodal information, however, their model adds additional parameters in the form of \u201cneural inferrer\u201d modules. In our multitask model, the grounded semantics are represented implicitly in the hidden states of the shared encoder. Furthermore, Toyama et al. (2016) assumes Source-Target-Image tuples as training data; whereas our multitask framework achieves equally good results if we train on separate Source-Image and Source-Target datasets.", "startOffset": 12, "endOffset": 699}, {"referenceID": 6, "context": "Multitask Learning improves the generalisability of a model by requiring it to be useful for more than one task (Caruana, 1997).", "startOffset": 112, "endOffset": 127}, {"referenceID": 26, "context": "This approach has seen a surge of attention and has recently been used to improve the performance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016).", "startOffset": 158, "endOffset": 179}, {"referenceID": 32, "context": ", 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016).", "startOffset": 128, "endOffset": 141}, {"referenceID": 12, "context": "In the translation literature, multitask learning has been used to learn a oneto-many languages translation model (Dong et al., 2015), and as a subtask of multitask sequence-tosequence learning (Luong et al.", "startOffset": 114, "endOffset": 133}, {"referenceID": 29, "context": ", 2015), and as a subtask of multitask sequence-tosequence learning (Luong et al., 2016).", "startOffset": 68, "endOffset": 88}, {"referenceID": 1, "context": "More recently, Bingel and S\u00f8gaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks.", "startOffset": 15, "endOffset": 41}], "year": 2017, "abstractText": "Multimodal machine translation is the task of translating sentences in a visual context. We decompose this problem into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoderdecoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.", "creator": "LaTeX with hyperref package"}}}