{"id": "1705.04400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Reducing Bias in Production Speech Models", "abstract": "Replacing hand-engineered pipelines with end-to-end deep learning systems has enabled strong results in applications like speech and object recognition. However, the causality and latency constraints of production systems put end-to-end speech models back into the underfitting regime and expose biases in the model that we show cannot be overcome by \"scaling up\", i.e., training bigger models on more data. In this work we systematically identify and address sources of bias, reducing error rates by up to 20% while remaining practical for deployment. We achieve this by utilizing improved neural architectures for streaming inference, solving optimization issues, and employing strategies that increase audio and label modelling versatility.", "histories": [["v1", "Thu, 11 May 2017 23:34:42 GMT  (3288kb,D)", "http://arxiv.org/abs/1705.04400v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["eric battenberg", "rewon child", "adam coates", "christopher fougner", "yashesh gaur", "jiaji huang", "heewoo jun", "ajay kannan", "markus kliegl", "atul kumar", "hairong liu", "vinay rao", "sanjeev satheesh", "david seetapun", "anuroop sriram", "zhenyao zhu"], "accepted": false, "id": "1705.04400"}, "pdf": {"name": "1705.04400.pdf", "metadata": {"source": "META", "title": "Reducing Bias in Production Speech Models", "authors": ["Eric Battenberg", "Rewon Child", "Adam Coates", "Christopher Fougner", "Yashesh Gaur", "Jiaji Huang", "Heewoo Jun", "Ajay Kannan", "Markus Kliegl", "Atul Kumar", "Hairong Liu", "Vinay Rao", "Sanjeev Satheesh", "David Seetapun", "Anuroop Sriram", "Zhenyao Zhu"], "emails": ["SVAIL@BAIDU.COM", "sanjeevsatheesh@baidu.com"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. Related Work", "text": "The most direct way to eliminate all distortions in input modeling is probably to learn a sufficiently meaningful model directly from raw waveforms as in (Sainath et al., 2015; Zhu et al., 2016) by parameterizing and learning these transformations. This work suggests that a non-trivial improvement in accuracy is difficult to achieve from raw waveform modeling alone without a significant increase in computing and storage requirements. (Wang et al., 2016a) introduced a traceable pro-channel energy intensification layer (PCEN) that parameterizes current normalization and the compression step typically handled by a static protocol transformation. Lookahead convolutions were proposed for streaming inferences (Wang et al., 2016b). Latency limited the bidirectional, recursive layers (BRLC) uncontextual and humidity conservation of modules were proposed in 2016 and 2016 modal contexts."}, {"heading": "3. Input modeling", "text": "In this section, we show that we can better model a variety of voice input by replacing this workflow with a traceable interface. While spectrograms strike an excellent balance between computational and representational quality, they have a wide dynamic range (Figure 1) and are prone to channel effects such as spatial impulse response, Lombard effects, and background noise. To alleviate the first problem, they are typically logged, and then normalized to mean and variance. However, this only moderately helps with any variations that may occur in the real world as previously described, and we expect the network to be resilient to these effects by exposing itself to such data. By relieving the network of the task of voice and channel normalization, it can deduct more from its ability to actually recognize a task."}, {"heading": "4. Latency Controlled Recurrent layers", "text": "Consider a typical use case for ASR systems in deployment. Audio is typically sent over the network in packets of short duration (e.g. 50-200 ms). Under these streaming conditions, it is imperative to improve accuracy and reduce the latency perceived by end users. It is observed that users tend to perceive the most the time between stopping speaking and the time when the last spoken word is presented to them. As a proxy for perceived latency, we measure the latency of the last transcription back to the user after the last audio packet has arrived on the server. 1To address the bias caused by the use of exclusively forward-facing models, we examine several structures, including predictive convolutions (Wang et al., 2016b) (LA-Conv) and latency-controlled bidirectional RNNs (in our case LC-GRU as recurring layers)."}, {"heading": "4.1. Accuracy and Serving Latency", "text": "We compare the Character Error Rate (CER) and the load-packet latency of using LA-Conv and LC-BGRU, along with those of Forward-GRU and Bidrectional GRU for ref-erences. The context size is set to 30 time steps for LA-Conv and LC-BGRU, and the preview timeframe for LC-BGRU is 5 to 25 every 5 steps. For latency experiments, we fix the packet size to 100 ms and send a packet from the client every 100 ms. We send 10 simultaneous streams to simulate a system under moderate load. As shown in Figure 4a, LA-Conv reduces almost half the gap between Forward GRU and bi-directional GRU, a model with three LC-BGRUs with preview to 25 each (yellow line) and bi-directional GRU (green line)."}, {"heading": "4.2. Loading BGRU as LC-BGRU", "text": "Since bidirectional GRUs (BGRU) can be considered an extreme case of LC-BGRUs with infinite context (as long as the statement duration), it is interesting to know if we could load a trained bidirectional GRU model as LC-BGRU, so that we do not need to retrain LC-BGRUs from scratch. However, we found that loading a model with 3 stacked bidirectional GRUs as stacked LC-BGRUs has resulted in a significant deterioration of performance compared to both the bidirectional baseline and a model with stacked LC-BGRUs over a large number of piece sizes and viewpoints. We can improve the performance of the model by instead splitting the input at each layer to a fixed size cW, so that it is smaller than the effective context. We run an LC-BGRU layer on an input length cW, then strip the last cW (c- each gram being sorted) with the GRU."}, {"heading": "5. Loss function", "text": "GramCTC's conditional assumption of independence forces the model to learn unimodal distributions of predicted label sequences. GramCTC (Liu et al., 2017) attempts to find a transformation of the output space in which the conditional assumption of independence of CTC is less harmful. Specifically, GramCTC attempts to predict word fragments, while traditional CTC-based end-to-end models aim to predict words. GramCTC learns to align target sequences and break them down into word segments or n-grams. N-Grams allow us to address the specifics of English spelling and pronunciation, where word fragments have a consistent pronunciation, but letters do not. For example, if the model is unsure how to spell a sound, it can choose to distribute the probability mass roughly evenly across all valid spellings of the sound and let the language model decide the most appropriate way to spell the word 1200 grammes. This is typically the first major data set to include only the most selective language loss."}, {"heading": "5.1. Forward-backward Process of GramCTC", "text": "The training process of GramCTC is very similar to that of CTC. The main difference is that several consecutive characters can form a valid gram. Therefore, the total number of states in the forward-backward process is much greater, as is the transition between these states. Figure 6 partially illustrates the dynamic programming process for the target sequence \"CAT.\" Here we assume that G contains all possible unigrams and bigrams. Thus, for each character in \"CAT\" there are three possible states associated with it: 1) the current character, 2) the bigram that ends with the current character, and 3) the space after the current character. At the beginning there is also a space. In total, we have 10 states."}, {"heading": "5.2. GramCTC vs CTC", "text": "GramCTC effectively reduces the learning load of the ASR network in two ways: 1) it splits sentences into meaningful n-grams and 2) it effectively reduces the number of output time steps. Both aspects simplify the rules that the network must learn, thus reducing the required network capacity of the ASR task. Table 1 compares the performance between CTC and GramCTC with the same network. There are some interesting differences. First, GramCTC's CERs are similar or even worse than CTC's, but GramCTC's WERs are always significantly better than CTC's. This is probably because GramCTC predicts in character chunks and the letters in the same stack are interdependent, i.e. more robust. Second, we also observe that the performance on the Dev set is relatively worse than that on the sustained move."}, {"heading": "6. Optimization Tricks", "text": "Eliminating optimization problems was a reliable way to improve performance in deep neural networks (Ioffe and Szegedy, 2015; He et al., 2016). Specifically for training recurring networks, several optimization tricks have been suggested - we have tried to use LayerNorm (Ba et al., 2016), Recurrent batch norm (Cooijmans et al., 2016) and NormProp (Arpit et al., 2016) without much success. Furthermore, we pay particular attention to optimizing layers correctly, and also use SortaGrad (Amodei et al., 2015). (Sak et al., 2015) suggests that CTC training could suffer from optimization problems and could be made more stable by providing alignment information during training. In this section, we examine how alignment information can be used effectively."}, {"heading": "6.1. Pre-training vs Joint-training", "text": "Using alignment information for training CTC models seems counterproductive, as CTC marginalizes over all alignments during training. However, the CTC loss is difficult to optimize because it simultaneously estimates network parameters and alignments. To simplify the problem, one can suggest an approach such as Expectation Maximization (EM), where the E-step calculates the expected log probability by marginalizing the rear alignments, and the M-step refines the model parameters by maximizing the expected log probability. However, it is impractical to calculate the rear alignments for all alignments, and we approach them by making only the most likely alignment. One step of the EM can be considered a pre-training approach by using alignment information - we start aligning a model with the most likely alignment models (making training with a Cross-Entropy (CE) loss for some eras easy)."}, {"heading": "6.2. Source of alignments", "text": "It is important for us to understand how accurate the alignment information must be, as different models have different alignments depending on architecture and training methods. We estimate the alignments using three \"reference models\" (models with only forward GRU, LC-BGRU and bidirectional GRU layers, all of which have been trained with multiple epochs of CTC minimization) and present the cross-correlation between the alignments generated by these models in Figure 7a. The location of the summit implies the delays between two alignments. It is obvious that the alignments are made by a forward model (and LC-BGRU) 5 (4) time steps later than those produced by a bidirectional model, an observation consistent with (Schuster and Paliwal, 1997). Therefore, it seems important to prepare a model with properly adjusted alignments, e.g. alignments from a bidirectional model are delayed by 5 steps in order to be used in the pre-training of a model."}, {"heading": "7. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Setup", "text": "In all cases, they are able to play by the rules."}, {"heading": "7.2. Results of Individual Changes", "text": "In the first half of Table 3, we show the effects of each of the individually applied changes. All the techniques proposed help to better adjust the training data as measured by CERs on the train set. Several observations stand out.1. Replacing the CTC loss with the GramCTC loss results in a lower WER, while the CERs on the train set are similar, suggesting that the loss encourages the model to learn words but pronounce words completely incorrectly when they are not known. This effect leads to less improvements in the application of the language model. 2. Applying the far-field augmentation to the same model results in a worse training error than expected. It shows a slight improvement over the remote control model, although our remote control phenomenon has a strong representation of the remote control target. 3. The biggest improvement over the remote control model is the addition of the LC-BGRU model, which greatly bridges the gap with bidirectional models. 4. (Less) articulation and pre-control models enhance the train by enhancing the alignment information on the train."}, {"heading": "7.3. Results of Incremental Changes", "text": "While we have designed the solutions to address different problems of the model, we should not expect every single improvement to be useful in combination with others. As an example, in the optimization section we see that models with bidirectional layers gain very little from using alignment information. We see in the second half of Table 3 that improvements actually address some of the difficulties in optimizing CTC models. There are 3 interesting models that are discussed. The model mix of joint training with 3 increasingly difficult losses (CE, CTC, and GramCTC, Mix-1) achieves the best results in the second half of Table 3, which does not stack up."}, {"heading": "8. Conclusion", "text": "In this paper, we identify multiple sources of bias in end-to-end speech systems that tend to promote very large neural network structures, making deployment impractical. To address these issues, several methods are proposed that allow us to develop a model that performs significantly better on our target development set, but is nonetheless good for streaming inferences. While the addition of entropy alignment training and GramCTC loss allow the models to better adjust training and validation data relative to WHO of a greedy maximum deciphering, they see much less benefit from language modeling integration. Using an LC-BRGU layer instead of predictive coils provides benefits across the board, as does the use of a PCEN layer at the front end. Finally, generalization of invisible data is improved by the addition of field extensions."}, {"heading": "9. Acknowledgements", "text": "We are grateful to the Baidu Speech Technology Group for all the IR coils and the Quality Assurance Team for helping to identify and understand useful metrics (such as First, Last and 98% packet latency), as well as the SVAIL systems team for helping develop the training platform and infrastructure."}], "references": [{"title": "Achieving human parity in conversational speech recognition", "author": ["Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1610.05256,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Trainable frontend for robust and far-field keyword spotting", "author": ["Yuxuan Wang", "Pascal Getreuer", "Thad Hughes", "Richard F Lyon", "Rif A Saurous"], "venue": "arXiv preprint arXiv:1607.05666,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Dense prediction on sequences with time-dilated convolutions for speech recognition", "author": ["Tom Sercu", "Vaibhava Goel"], "venue": "arXiv preprint arXiv:1611.09288,", "citeRegEx": "Sercu and Goel.,? \\Q2016\\E", "shortCiteRegEx": "Sercu and Goel.", "year": 2016}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts", "author": ["Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Peddinti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peddinti et al\\.", "year": 2015}, {"title": "Highway long short-term memory rnns for distant speech recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yao", "Sanjeev Khudanpur", "James R. Glass"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Gram-ctc: Automatic unit selection and target decomposition for sequence labelling", "author": ["Hairong Liu", "Zhenyao Zhu", "Xiangang Li", "Sanjeev Satheesh"], "venue": "arXiv preprint arXiv:1703.00096,", "citeRegEx": "Liu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Hasim Sak", "Andrew Senior", "Kanishka Rao", "Francoise Beaufays"], "venue": null, "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Tara N. Sainath", "Ron J. Weiss", "Andrew W. Senior", "Kevin W. Wilson", "Oriol Vinyals"], "venue": "In INTERSPEECH,", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Learning multiscale features directly from waveforms", "author": ["Zhenyao Zhu", "Jesse Engel", "Awni Y. Hannun"], "venue": "In INTERSPEECH,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "Lookahead convlution layer for unidirectional recurrent neural networks. 2016b. http://www.cs.cmu.edu/ dyogatam/papers/wang+etal.iclrworkshop2016.pdf", "author": ["Chong Wang", "Dani Yogatama", "Adam Coates", "Tony Han", "Awni Hannun", "Bo Xiao"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Training deep bidirectional lstm acoustic model for lvcsr by a context-sensitivechunk bptt approach", "author": ["Kai Chen", "Qiang Huo"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Chen and Huo.,? \\Q2016\\E", "shortCiteRegEx": "Chen and Huo.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["Ronan Collobert", "Christian Puhrsch", "Gabriel Synnaeve"], "venue": "arXiv preprint arXiv:1609.03193,", "citeRegEx": "Collobert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2016}, {"title": "Segmental recurrent neural networks for end-to-end speech recognition", "author": ["Liang Lu", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith", "Steve Renals"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez", "Pegah Ghahremani", "Vimal Manohar", "Xingyu Na", "Yiming Wang", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Povey et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2016}, {"title": "Optimal estimators for spectral restoration of noisy speech", "author": ["J. Porter", "S. Boll"], "venue": "In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP \u201984.,", "citeRegEx": "Porter and Boll.,? \\Q1984\\E", "shortCiteRegEx": "Porter and Boll.", "year": 1984}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["K. Cho D. Bahdanau", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau and Bengio.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Recurrent batch normalization", "author": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "Aaron C. Courville"], "venue": "CoRR, abs/1603.09025,", "citeRegEx": "Cooijmans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cooijmans et al\\.", "year": 2016}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "A study on data augmentation of reverberant speech for robust speech recognition", "author": ["Tom Ko", "Vijayaditya Peddinti", "Daniel Povey", "Michael Seltzer", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Ko et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ko et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning has helped speech systems attain very strong results on speech recognition tasks for multiple languages (Xiong et al., 2016; Amodei et al., 2015).", "startOffset": 118, "endOffset": 159}, {"referenceID": 0, "context": "Architectures for streaming inference: English ASR models greatly benefit from using information from a few time frames into the future (Xiong et al., 2016; Sercu and Goel, 2016; Peddinti et al., 2015).", "startOffset": 136, "endOffset": 201}, {"referenceID": 2, "context": "Architectures for streaming inference: English ASR models greatly benefit from using information from a few time frames into the future (Xiong et al., 2016; Sercu and Goel, 2016; Peddinti et al., 2015).", "startOffset": 136, "endOffset": 201}, {"referenceID": 3, "context": "Architectures for streaming inference: English ASR models greatly benefit from using information from a few time frames into the future (Xiong et al., 2016; Sercu and Goel, 2016; Peddinti et al., 2015).", "startOffset": 136, "endOffset": 201}, {"referenceID": 4, "context": "We show the effectiveness of Latency Constrained Bidirectional RNNs (Zhang et al., 2016) in controlling the latency while still being able to include future context.", "startOffset": 68, "endOffset": 88}, {"referenceID": 5, "context": "We show how GramCTC (Liu et al., 2017) finds the label space where this conditional independence is easier to manage.", "startOffset": 20, "endOffset": 38}, {"referenceID": 6, "context": "Optimization issues: Additionally, the CTC loss is notoriously unstable (Sak et al., 2015), despite making sequence labeling tractable, since it is forcing the model to align the input and output sequences, as well as recognize output labels.", "startOffset": 72, "endOffset": 90}, {"referenceID": 7, "context": "The most direct way to remove all bias in the inputmodeling is probably learning a sufficiently expressive model directly from raw waveforms as in (Sainath et al., 2015; Zhu et al., 2016) by parameterizing and learning these transformations.", "startOffset": 147, "endOffset": 187}, {"referenceID": 8, "context": "The most direct way to remove all bias in the inputmodeling is probably learning a sufficiently expressive model directly from raw waveforms as in (Sainath et al., 2015; Zhu et al., 2016) by parameterizing and learning these transformations.", "startOffset": 147, "endOffset": 187}, {"referenceID": 10, "context": "Latency constrained Bidirectional recurrent layers (LC-BRNN) and Context sensitive chunks (CSC) have been proposed in (Chen and Huo, 2016) for tractable sequence model training but not explored for streaming inference.", "startOffset": 118, "endOffset": 138}, {"referenceID": 3, "context": "Time delay neural networks (Peddinti et al., 2015) and Convolutional networks are also options for controlling the amount of future context.", "startOffset": 27, "endOffset": 50}, {"referenceID": 11, "context": "Alternatives have been proposed to relax the label independence assumption of the CTC loss - Attention models (Bahdanau et al., 2015; Chan et al., 2016), global normalization (Collobert et al.", "startOffset": 110, "endOffset": 152}, {"referenceID": 12, "context": "Alternatives have been proposed to relax the label independence assumption of the CTC loss - Attention models (Bahdanau et al., 2015; Chan et al., 2016), global normalization (Collobert et al.", "startOffset": 110, "endOffset": 152}, {"referenceID": 13, "context": ", 2016), global normalization (Collobert et al., 2016) and segmental RNNs (Lu et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 14, "context": ", 2016) and segmental RNNs (Lu et al., 2016) and more end-to-end losses like lattice free MMI (Maximum Mutual Information) (Povey et al.", "startOffset": 27, "endOffset": 44}, {"referenceID": 15, "context": ", 2016) and more end-to-end losses like lattice free MMI (Maximum Mutual Information) (Povey et al., 2016) are all promising approaches to address this problem.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": ", 2015) and by warm-starting CTC training from a model pre-trained by Cross-Entropy (CE) loss (using alignment information) (Sak et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 16, "context": "The latter is designed to approximate an optimized spectral subtraction curve (Porter and Boll, 1984) which helps to improve robustness to background noises.", "startOffset": 78, "endOffset": 101}, {"referenceID": 10, "context": "Bahdanau and Bengio, 2014) cells) (Chen and Huo, 2016; Zhang et al., 2016), which are illustrated in Figure 3.", "startOffset": 34, "endOffset": 74}, {"referenceID": 4, "context": "Bahdanau and Bengio, 2014) cells) (Chen and Huo, 2016; Zhang et al., 2016), which are illustrated in Figure 3.", "startOffset": 34, "endOffset": 74}, {"referenceID": 5, "context": "GramCTC (Liu et al., 2017) attempts to find a transformation of the output space where the conditional independence assumption made by CTC is less harmful.", "startOffset": 8, "endOffset": 26}, {"referenceID": 18, "context": "Removing optimization issues have been a reliable way of improving performance in deep neural networks (Ioffe and Szegedy, 2015; He et al., 2016).", "startOffset": 103, "endOffset": 145}, {"referenceID": 19, "context": "Removing optimization issues have been a reliable way of improving performance in deep neural networks (Ioffe and Szegedy, 2015; He et al., 2016).", "startOffset": 103, "endOffset": 145}, {"referenceID": 20, "context": ", 2016), Recurrent batch norm (Cooijmans et al., 2016) and NormProp (Arpit et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 6, "context": "(Sak et al., 2015) suggests that CTC training could be suffering from optimization issues and could be made more stable by providing alignment information during training.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "It is evident that alignments by a forward (and LC-BGRU) model are 5 (4) time-steps later than those by a bidirectional model, an observation that is consistent with (Schuster and Paliwal, 1997).", "startOffset": 166, "endOffset": 194}, {"referenceID": 22, "context": "For robustness to reverberant noise encountered in far-field recognition, we adopt room impulse response (RIR) augmentation as in (Ko et al., 2017), in which case, we randomly sample a subset of the data and convolve each instance with a random RIR signal.", "startOffset": 130, "endOffset": 147}], "year": 2017, "abstractText": "Replacing hand-engineered pipelines with endto-end deep learning systems has enabled strong results in applications like speech and object recognition. However, the causality and latency constraints of production systems put end-to-end speech models back into the underfitting regime and expose biases in the model that we show cannot be overcome by \u201cscaling up\u201d, i.e., training bigger models on more data. In this work we systematically identify and address sources of bias, reducing error rates by up to 20% while remaining practical for deployment. We achieve this by utilizing improved neural architectures for streaming inference, solving optimization issues, and employing strategies that increase audio and label modelling versatility.", "creator": "LaTeX with hyperref package"}}}