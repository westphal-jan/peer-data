{"id": "1705.04530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "A Survey of Question Answering for Math and Science Problem", "abstract": "Turing test was long considered the measure for artificial intelligence. But with the advances in AI, it has proved to be insufficient measure. We can now aim to mea- sure machine intelligence like we measure human intelligence. One of the widely accepted measure of intelligence is standardized math and science test. In this paper, we explore the progress we have made towards the goal of making a machine smart enough to pass the standardized test. We see the challenges and opportunities posed by the domain, and note that we are quite some ways from actually making a system as smart as a even a middle school scholar.", "histories": [["v1", "Wed, 10 May 2017 15:28:37 GMT  (2303kb,D)", "http://arxiv.org/abs/1705.04530v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["arindam bhattacharya"], "accepted": false, "id": "1705.04530"}, "pdf": {"name": "1705.04530.pdf", "metadata": {"source": "CRF", "title": "A Survey of Question Answering for Math and Science Problem", "authors": ["Arindam Bhattacharya"], "emails": ["arindam@cse.iitd.ac.in"], "sections": [{"heading": null, "text": "In this essay, we examine the progress we have made toward the goal of making a machine smart enough to pass the standardized test. We see the challenges and opportunities that this domain brings, and find that we are a long way from making a system as intelligent as even a middle school scholar."}, {"heading": "1 Introduction", "text": "The Turing Test, proposed by Alan Turing in 1950, states that if a system can exhibit conversation behavior that is indistinguishable from that of a human being during a conversation, that system could be considered intelligence (Turing, 1950). Since then, it has often been criticized as a poor measure of purpose (Hayes and Ford, 1995). Since then, it has also been suggested that standardized tests in mathematics and science could be a suitable measure for assessing machine intelligence (Clark and Etzioni, 2016). To this end, there are attempts to develop questions aimed at solving standardized problems in mathematics and science."}, {"heading": "2 Question Answering and the Math/Science domain", "text": "Answering questions (QA) is the task of generating or extracting an answer to a question asked in natural language. Modern QA systems can be divided into two major paradigms (Jurafsky and Martin, 2016). The first paradigm, which is called text-based (or IR-based) questions, is based on large volumes of text. Faced with a question, it uses methods of gathering information from the available corpus and retrieves documents that contain the answer. Candidates \"answers are then extracted from the text and ranked. The second paradigm is knowledge-based questions where we create a semantic representation of the question and use it to query fact databases.The focus on science and mathematics as domains for questions presents certain unique challenges. Solving these problems requires not only understanding the question for which we deny language processing, but also to maintain an internal representation of the problem and often perform symbolic calculations (Clark and Eangtzoni, 2016)."}, {"heading": "3 Question Answering for Science", "text": "This section focuses on scientific questions of standardized testing. Types of questions include basic fact-checking, conclusions and world knowledge, as well as diagrams. We also examine the state of the art in these types of questions."}, {"heading": "3.1 Dataset", "text": "The standardized test for science used for QA tasks is the New York Regents Science Exams (NYSED). Here are examples of types of questions in the test:"}, {"heading": "Basic Questions", "text": "(A) a crayon (B) a plastic spoon (C) an eraser (D) an iron nail (2). The movement of the soil by wind or water is called (A) condensation (B) evaporation (C) erosion (D) friction. (A) An IR-based QA system can answer these questions.Simple Conclusion These are questions that require a simple conclusion about known facts to come to the answer. Example: 1. What example describes an organism that absorbs nutrients? (A) a dog that burrows a bone (B) a girl that eats an apple (C) an insect that crawls on a leaf (D) A boy that plants tomatoes In order to answer this question, it is necessary to know that food requires nutrient intake and that an apple contains nutrients."}, {"heading": "More Complex World Knowledge", "text": "These questions require deeper world knowledge and more advanced liguistic skills in order for the system to understand questions and provide an answer. Example: 1. A student riding a bike observes that it moves faster on a slippery road than on a bumpy road. This is because the slippery road (A) less gravity (B) more gravity (C) less friction (D) more friction. To answer this question, the system must be aware that cycling means moving it, and logically derive the path from it [smooth - less friction - faster movement]."}, {"heading": "Diagram", "text": "The diagrams contain sketches, maps, diagrams, tables, etc. These often prove quite difficult for QA systems. Example: 1. Which letter in Diagram 3.1 indicates the plant structure that absorbs water and nutrients?"}, {"heading": "3.2 Models", "text": "Various approaches, ranging from a combination of information retrieval, statistics and inference to integer programming, are applied to meet the challenges mentioned above. (Daniel Khashabi et al., 2016) suggests a method of solving QS using Integer Linear Programming. Given semi-structured knowledge astables, the QA problem is formulated in such a way that a desirable support diagram is found (Fig. 2), which in turn is formulated as ILP. Figure 2: TableILP looks for the best support diagram (argument chains) that links the question to an answer, in this case June.The state-of-the-art system ARISTO (Clark et al., 2016) employs a combination of solvers to tackle the problem at various levels Notes Notes Notes Notes Notes Notes Notes Notes, Notes Notes Notes Solver, Pointwise Mutual Information Solver, Support Vector Machine Solver, Nams Support Solver Notes Notes Notes, Notes Solver Notes Notes Notes Notes, Notes System Notes Rules, Notes Rules Notes Notes Notes Notes, Notes Mutual Notes Solved, Notes Mutes Mutes, Names Support Vector Machine Solver, Names Support Vector Notes, Notes Rules Notes Notes Notes Notes, Notes Rules, Notes Notes Notes Rules, Notes Notes Notes Rules, Notes Mutes Mutes, Notes Mutes, Notes, Notes Mutes Mutes, Notes in-based on the above-based on the above-mentioned above-mentioned challenges. (Daniel Khashabi et al., 2016)"}, {"heading": "4 Question Answering for Mathematics", "text": "Mathematical questions cannot be solved by IR systems. The basic strategy in mathematics, especially arithmatic queens, is to understand the problem and formulate an equation that can be calculated. Geometry questions present difficulties due to their dependence on diagrams. Figure 3: Aristo uses five solvers, each using different types of knowledge to answer multiple-choice questions."}, {"heading": "4.1 Dataset", "text": "Standardized tests by (NYSED) are used for algebra questions, for geometry questions by SATs."}, {"heading": "Algebraic Problems", "text": "Algebraic problems present themselves as stories that require language processions. Example: 1. Molly is the owner of the Wafting Pie Company. This morning, her employees bake pumpkin pie with 816 eggs. If her employees use a total of 1339 eggs today, how many eggs did they use in the afternoon? Some of them also require modeling the world, for example: 1. Sara's High School won 5 basketball games this year. They lost 3 games. How many games did they play in total? 2. John has 8 orange balloons but lost 2 of them. How many orange balloons does John have now? Here, for the first time, the knowledge is asked that the game is either won or lost."}, {"heading": "Geometry Problems", "text": "Geometric problems combine arithmatic and diagrammatic thinking. Example: Fig.4"}, {"heading": "4.2 Models", "text": "In one of the earliest attempts to solve algebraic word problems, a simple verb categorization was used (Hosseini et al., 2014). The model extracted the verbs from the question and tried to formulate equations based on the verb category. In Fig. 6.A more complex (and up-to-date) system is presented, ALGES (Koncel-Kedziorski et al., 2015) uses Integer Linear Programming to convert the word problems into equation trees. In contrast to (Hosseini et al., 2014), which only covers +, \u2212, \u2212, \u2212, \u2212, /. They are also able to solve multi-sentence problems, unlike previous models. ALGES assumes that sentences of basic truths are extracted from the text and uses ILP to form trees that use the system."}, {"heading": "5 Conclusion", "text": "We have seen that the current state of the art is not good at solving standardized tests (the smartest AI could not pass high school). There is still a long way to go for AI. The field of QA systems on standardized mathematical and scientific questions is still in its infancy, with room for improvement, for example, in neural models, especially diagram-based questions. But the lack of results could also suggest that the current trend of data-driven AI may not be the only solution to all problems.We started the discussion with the question of how the Turing test is no longer the best measure of intelligence and agreed (Clark and Etzioni, 2016) that standardized tests would be a better judge of machine intelligence, which is more like how we measure human intelligence. But if the system could pass the standardized test, would it be intelligent? (Weston et al., 2015) argues that standardized math and science tests would be a better test for machine intelligence, which is more like how we measure human intelligence."}], "references": [{"title": "My Computer is an Honor Student - but how Intelligent is it? Standardized Tests as a Measure of AI", "author": ["Peter Clark", "Oren Etzioni."], "venue": "AI Magazine .", "citeRegEx": "Clark and Etzioni.,? 2016", "shortCiteRegEx": "Clark and Etzioni.", "year": 2016}, {"title": "Question Answering via Integer Programming over SemiStructured Knowledge", "author": ["Tushar Khot Daniel Khashabi", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni", "Dan Roth."], "venue": "IJCAI .", "citeRegEx": "Khashabi et al\\.,? 2016", "shortCiteRegEx": "Khashabi et al\\.", "year": 2016}, {"title": "Turing Test Considered Harmful", "author": ["P. Hayes", "K. Ford."], "venue": "IJCAI .", "citeRegEx": "Hayes and Ford.,? 1995", "shortCiteRegEx": "Hayes and Ford.", "year": 1995}, {"title": "Learning to Solve Arithmetic Word Problems with Verb Categorization", "author": ["Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman."], "venue": "ACL .", "citeRegEx": "Hosseini et al\\.,? 2014", "shortCiteRegEx": "Hosseini et al\\.", "year": 2014}, {"title": "Speech and Language Processing", "author": ["Daniel Jurafsky", "James H. Martin"], "venue": null, "citeRegEx": "Jurafsky and Martin.,? \\Q2016\\E", "shortCiteRegEx": "Jurafsky and Martin.", "year": 2016}, {"title": "Parsing Algebraic Word Problems into Equations", "author": ["Rik Koncel-Kedziorski", "Hannaneh Hajishirzi", "Ashish Sabharwal", "Oren Etzioni", "Siena Dumas Ang."], "venue": "ACL .", "citeRegEx": "Koncel.Kedziorski et al\\.,? 2015", "shortCiteRegEx": "Koncel.Kedziorski et al\\.", "year": 2015}, {"title": "New York Regents Science Exams", "author": ["NYSED."], "venue": "http://www. nysedregents.org/.", "citeRegEx": "NYSED.,? 2014", "shortCiteRegEx": "NYSED.", "year": 2014}, {"title": "Moving Beyond the Turing Test with the Allen AI Science Challenge", "author": ["Carissa Schoenick", "Peter Clark", "Oyvind Tafjord", "Peter Turney", "Oren Etzioni"], "venue": null, "citeRegEx": "Schoenick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schoenick et al\\.", "year": 2016}, {"title": "Diagram Understanding in Geometry Questions", "author": ["Min Joon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni."], "venue": "AAAI .", "citeRegEx": "Seo et al\\.,? 2014", "shortCiteRegEx": "Seo et al\\.", "year": 2014}, {"title": "Solving Geometry Problems: Combining Text and Diagram Interpretation", "author": ["Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni", "Clint Malcolm."], "venue": "EMNLP .", "citeRegEx": "Seo et al\\.,? 2015", "shortCiteRegEx": "Seo et al\\.", "year": 2015}, {"title": "Towards AI-complete Question Answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1502.05698 .", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Since then it has been often critisized of being a poor measure for the purpose (Hayes and Ford, 1995).", "startOffset": 80, "endOffset": 102}, {"referenceID": 0, "context": "It has also since been proposed that standardized tests in mathemetics and science could be a suitable measure for judging machine intelligence (Clark and Etzioni, 2016).", "startOffset": 144, "endOffset": 169}, {"referenceID": 4, "context": "Modern QA systems can be divided into two broad paradigms (Jurafsky and Martin, 2016).", "startOffset": 58, "endOffset": 85}, {"referenceID": 0, "context": "Solving these problems require not only understanding question for which we levarage language processing, but also to maintain an internal representation of the problem and often carry out symbolic computation (Clark and Etzioni, 2016).", "startOffset": 210, "endOffset": 235}, {"referenceID": 7, "context": "Using standardized test provides us with questions that are graduated by difficulty and multifaceted in nature: different questions explore different types of knowledge (Schoenick et al., 2016).", "startOffset": 169, "endOffset": 193}, {"referenceID": 3, "context": "One of the earliest attempt at solving algebraic word problems employed simple verb categorization (Hosseini et al., 2014).", "startOffset": 99, "endOffset": 122}, {"referenceID": 5, "context": "A more sophisticated (and current state of the art) system, ALGES (Koncel-Kedziorski et al., 2015) uses Integer Linear Programming to map the word problems into equation trees.", "startOffset": 66, "endOffset": 98}, {"referenceID": 3, "context": "In contrast to (Hosseini et al., 2014), which covered only +,\u2212, ALGES covers +,\u2212, \u2217, /.", "startOffset": 15, "endOffset": 38}, {"referenceID": 8, "context": "The initial work in this area aimed at aligning text with geometric diagrams (Seo et al., 2014).", "startOffset": 77, "endOffset": 95}, {"referenceID": 8, "context": "While (Seo et al., 2014) understands the diagram, it could not solve geometric problems.", "startOffset": 6, "endOffset": 24}, {"referenceID": 9, "context": "GeoS (Seo et al., 2015) builds on the previous model and can answer geometric questions.", "startOffset": 5, "endOffset": 23}, {"referenceID": 0, "context": "We started the discussion with how Turing test is no longer the best measure for intelligence and agreed with (Clark and Etzioni, 2016) that standardized tests would be a better judge of intelligence for machines, that is more in line with how we measure human intelligence.", "startOffset": 110, "endOffset": 135}, {"referenceID": 10, "context": "But if the system could pass the standardized test, would it be intelligent? (Weston et al., 2015) argues that standardized math and science tests can not test common sense, and hence not a true test for intelligence.", "startOffset": 77, "endOffset": 98}], "year": 2017, "abstractText": "Turing test was long considered the measure for artificial intelligence. But with the advances in AI, it has proved to be insufficient measure. We can now aim to measure machine intelligence like we measure human intelligence. One of the widely accepted measure of intelligence is standardized math and science test. In this paper, we explore the progress we have made towards the goal of making a machine smart enough to pass the standardized test. We see the challenges and opportunities posed by the domain, and note that we are quite some ways from actually making a system as smart as a even a middle school scholar.", "creator": "LaTeX with hyperref package"}}}