{"id": "1705.04804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2017", "title": "Automatically Redundant Features Removal for Unsupervised Feature Selection via Sparse Feature Graph", "abstract": "The redundant features existing in high dimensional datasets always affect the performance of learning and mining algorithms. How to detect and remove them is an important research topic in machine learning and data mining research. In this paper, we propose a graph based approach to find and remove those redundant features automatically for high dimensional data. Based on the framework of sparse learning based unsupervised feature selection, Sparse Feature Graph (SFG) is introduced not only to model the redundancy between two features, but also to disclose the group redundancy between two groups of features. With SFG, we can divide the whole features into different groups, and improve the intrinsic structure of data by removing detected redundant features. With accurate data structure, quality indicator vectors can be obtained to improve the learning performance of existing unsupervised feature selection algorithms such as multi-cluster feature selection (MCFS). Our experimental results on benchmark datasets show that the proposed SFG and feature redundancy remove algorithm can improve the performance of unsupervised feature selection algorithms consistently.", "histories": [["v1", "Sat, 13 May 2017 09:34:17 GMT  (5527kb)", "https://arxiv.org/abs/1705.04804v1", "submitted to ACM TKDD"], ["v2", "Fri, 30 Jun 2017 18:33:48 GMT  (4693kb)", "http://arxiv.org/abs/1705.04804v2", "correct several typo and format issues"]], "COMMENTS": "submitted to ACM TKDD", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuchu han", "hao huang", "hong qin"], "accepted": false, "id": "1705.04804"}, "pdf": {"name": "1705.04804.pdf", "metadata": {"source": "CRF", "title": "Automatically Redundant Features Removal for Unsupervised Feature Selection via Sparse Feature Graph", "authors": ["Shuchu Han"], "emails": ["shuchu.han@gmail.com", "haohuanghw@gmail.com", "qin@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "In fact, most of us are able to survive by not following the rules they have imposed on ourselves, \"he said in an interview with the Deutsche Presse-Agentur.\" It's not that we are able to outdo ourselves, \"he told the Deutsche Presse-Agentur.\" But it's not that we are able to outdo ourselves. \""}, {"heading": "1 Math Notation", "text": "Throughout this work, matrices are written in bold uppercase letters and vectors in bold lowercase letters. Let's have the data matrix represented as X-Rn \u00b7 d, while each row is an example (or instance) and each column signifies a attribute. If we look at the data matrix X = [x1, x2, \u00b7 \u00b7, xn] T, xi-Rd \u00b7 1 from the attributes page, it can be considered as F = XT = [f1, f2, \u00b7, fd], fi-Rn \u00b7 1 (1 \u2264 i \u2264 d)."}, {"heading": "2 Background and Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Unsupervised Feature Selection", "text": "Within the scope of unsupervised selection of features, we do not have identification information to determine the relevance of the features; instead, the similarity of the data or the diverse structure constructed from the entire feature space are used as criteria for selecting features. Of all these unattended feature selection algorithms, the most famous is MCFS. The MCFS algorithm is a sparse learning-based, unsupervised feature selection method that can be represented as Figure 1. Suppose that the input data has previously known K clusters (or an estimated K value by subject knowledge), and then we find a number of features that can approximate these eigenvectors by means of sparse linear regression."}, {"heading": "2.2 Adaptive Structure Learning for High Dimensional Data", "text": "As we can see, the MCFS uses whole characteristics to model the data structure, which means that the similarity graph, like the Gaussian similarity graph, is built from all the characteristics. This is problematic if the dimension of the data vector is higher. To be precise, the pair distance between any two data vectors becomes almost the same, and consequently the structural information obtained from the data is not accurate. This observation is the motivation of the unattended feature Selection with Adaptive Structure Learning (FSASL) algorithms proposed by Du et al. [5]. The idea of FSASL is to repeat MCFS iteratively by updating selected characteristics. It can be illustrated as follows: FASAL is an iterative algorithm that repeats irrelevant and noisy characteristics in order to obtain a better manifold structure, while improved structural information can help in the search for better relevant characteristics."}, {"heading": "2.3 Redundant Features", "text": "In this section, we describe our definition of attribute redundancy, which we use here, as attribute vector or linear combination of several attribute vectors.) To measure the redundancy between two vectors fi and fj, the square cosine similarity [22] is used: Rij = cos 2 (fi, fj). (2) By mathematically defining cosine similarity, it is easy to know that a higher value of Ri, j means a high redundancy that exists between fi and fj."}, {"heading": "3 Problem Statement", "text": "In this paper, our goal is to identify these redundant features that exist in high-dimensional data, and to obtain a more accurate intrinsic data structure. To be precise: Problem 1: Given high-dimensional data represented in the form of the attribute matrix X, how to remove these redundant features f (\u00b7) \u0445 XT for uncontrolled attribute selection algorithms such as MCFS? Technically, the MCFS algorithm does not contain redundant features. However, the performance of the MCFS depends on the quality of the indication vectors used to select features through sparse learning, and these indication vectors are highly related to the intrinsic structure of data described by the selected features and the distance metric provided. Thus, the MCFS algorithm uses all characteristics and Gaussian similarity to represent the intrinsic structure."}, {"heading": "4 Algorithm", "text": "In this section, we present our graph-based algorithm for detecting and eliminating redundant features present in high-dimensional data. First, we present the sparse feature graph that models redundancy between feature vectors. Second, we discuss the sparse representation error. In the last, we propose the local compressible subgraph to extract redundant feature groups."}, {"heading": "4.1 Sparse Feature Graph (SFG)", "text": "The most popular way to model redundancy among the feature vectors is the correlation like the Pearson correlation coefficient (PCC). The correlation value is defined by two feature vectors, and it is a pair measurement. However, there is also a breaking redundancy between a feature vector and a series of feature vectors according to the philosophy of the MCFS algorithm. In this section, we present SFG, which models redundancy not only between two feature vectors, but also between a feature vector and a series of feature vectors. The basic idea of the sparse feature diagram is to search for a sparse linear representation of the feature vectors, while all other feature vectors are used as a dictionary. For each feature vector fi in feature set F = [f1, f2, \u00b7, \u00b7, \u00b7 f\u00b7 fd], the following problem is the Serm:"}, {"heading": "4.2 Sparse Representation Error", "text": "In our modified OMP algorithm 1, we have a new stop criterion for searching for a sparse representation solution for each function vector fi =. Instead of continuing to search until we find a minimization error, we stop running while the solver can no longer reduce the length of the residual vector. To be precise, the 2-standard of the residual vector is monitored and the solver will stop as soon as the change of this value is small as a user-defined threshold. Algorithm 1: Orthogonal Matching Pursuit (OMP) Input: [f1, f2, \u00b7 \u00b7, fi \u2212, fi \u2212, fi \u2212 1, \u00b7 \u00b7, fd]."}, {"heading": "4.3 Local Compressible Subgraph", "text": "We group high correlated characteristics by local compressible subgraphs. The SFG is a weighted directed graph. With this graph, we must find all feature subsets that are very redundant. To archive this goal, we propose a local search algorithm with seed nodes to group these highly correlated characteristics into many subgraphs that are referred to in this study as local compressible subgraphs. Our local search algorithm comprises two steps, the first step being to sort all nodes by degree. By the definition of SFG, the higher-degree node means that it appears more frequently in the sparse representation of other nodes. The second step is a local bread-first search approach that finds all nodes with higher weight connections (on and off) to the growing subgraph. The detailed search result search algorithm can be described as follows: In Alg 3, Locator Drawing (3)."}, {"heading": "4.4 Redundant Feature Removal", "text": "The last step of our algorithm is the elimination of redundant features. For each locally compressible subgraph that we have found, we select the node that has the highest In-degree as the representative node of this locally compressible subgraph. Thus, the number of final feature vectors corresponds to the number of locally compressible subgraphs."}, {"heading": "5 Experiments", "text": "In this section, we present experimental results to demonstrate the effectiveness of our proposed algorithm. First, we evaluate the spectral cluster performance before and after the application of our algorithms. Second, we show the performance of MCFS with or without our algorithm. In the last section, the properties of the generated sparse graphs and the sensitivity of the parameters are discussed."}, {"heading": "5.1 Experiment Setup", "text": "Data sets. We select twelve high-dimensional real-world data sets [11] from three different domains: image, text, and biomedicine. The details of each data set are in Table 1. The data sets have sample sizes from 96 to 8293 and feature sizes from 1,024 to 18,933. In addition, the data sets have class names from 2 to 64. The purpose of this selection is to make the evaluation results more general by applying data sets with different characteristics. Normalization. The characteristics of each data set are normalized to have unit length, meaning that the H = 1 label is for all data sets. Evaluation metric. Our proposed algorithm is subject to unattended learning. Without losing generality, the cluster structure of the data is used for evaluation. To be precise, we measure spectral clustering capability with the P. code."}, {"heading": "5.2 Effectiveness of Redundant Features Removal", "text": "Our proposed algorithm removes many features in order to reduce the dimensional size of all data vectors. As a result, the pairwise Euclidean distance is changed and the cluster structure is influenced. To measure the effectiveness of our proposed algorithm, we check the spectral cluster performance before and after the removal of redundant features. If the NMI and ACC values are not altered too much and remain at the same level, the experimental results show that our proposed algorithm is correct and effective. The spectral cluster algorithm we used in our experiments is the NgJordan-Weiss (NJW) algorithm [16]. The Gaussian similarity graph is used here because the input and parameter \u03c3 is set to the mean value of the pairwise euclidean distance between all vectors. Our proposed LCS algorithm contains a parameter that is the threshold of redundancy."}, {"heading": "5.3 Performance of MCFS", "text": "Our proposed algorithm is aimed at selecting unattended features, and the quality of the indication vectors (or eigenvector-based spectral cluster performance) is an important factor in evaluating the effectiveness of our proposed algorithm. In this section, we evaluate the MCFS performance through the redundant features removed from us and compare them with the raw data that have no characteristics. Spectral cluster performance is measured with different results for different input data from the original overall feature data to the data processed by our proposed algorithm. We report the test results via image data sets and biological data sets in this section. In the case of text data sets, the feature vectors are very sparse, and our self-decomposition process has always failed, and we can only collect partial results. For a fair evaluation, we leave the test results of text data sets and biological data sets in this section, and the result from Table 9 to Table 17 shows the number of each of data set, from 1 to 0.11."}, {"heading": "5.4 Sparse Representation Errors", "text": "The meaning of these edge joints and edge weights is invalid, and they should be removed from the SFG because false joints worsen the accuracy of the redundancy relationship between features. To validate the sparse representation, we check the angle between the original feature vector and the linear weighted summation from which the vector (or signal from sparse encoding) results from its sparse representation. If the angle is less than a threshold, we remove all edges from the generated sparse feature diagram. To determine the threshold, we learn it from the empirical results of our selected twelve datasets. The distribution result (or histogram) of the angle values is shown in Figure 9."}, {"heading": "6 Related Works", "text": "Peng et al. [17] propose a greedy algorithm (called mRMR) to select characteristics with minimal redundancy and maximum dependence. Zhao et al. [27] develop an efficient algorithm for selecting spectral characteristics to minimize theredundance within the characteristics selected by the L2,1 standard. Recently, researchers have turned their attention to unattended feature selection with global minimized redundancy [22]. Several graph-based approaches are proposed in [15], [19]. The least known research paper is [13], which builds a sparse graph on the trait side and classifies characteristics by approximation error."}, {"heading": "7 Conclusion", "text": "The results of the experiment show that our algorithm is an effective method for obtaining the precise data structure information required for uncontrolled selection algorithms."}], "references": [{"title": "On the surprising behavior of distance metrics in high dimensional space", "author": ["Charu C Aggarwal", "Alexander Hinneburg", "Daniel A Keim"], "venue": "In International Conference on Database Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "When is nearest neighbor meaningful", "author": ["Kevin Beyer", "Jonathan Goldstein", "Raghu Ramakrishnan", "Uri Shaft"], "venue": "In International conference on database theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["Deng Cai", "Chiyuan Zhang", "Xiaofei He"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Feature selection for classification", "author": ["Manoranjan Dash", "Huan Liu"], "venue": "Intelligent data analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Unsupervised feature selection with adaptive structure learning", "author": ["Liang Du", "Yi-Dong Shen"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Feature selection for unsupervised learning", "author": ["Jennifer G Dy", "Carla E Brodley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["Ehsan Elhamifar", "Rene Vidal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A variance minimization criterion to feature selection using laplacian regularization", "author": ["Xiaofei He", "Ming Ji", "Chiyuan Zhang", "Hujun Bao"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2025}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["Chenping Hou", "Feiping Nie", "Xuelong Li", "Dongyun Yi", "Yi Wu"], "venue": "Cybernetics, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Toward optimal feature selection", "author": ["D KOLLER"], "venue": "In Proc. 13th International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Feature selection: A data perspective", "author": ["J. Li", "K. Cheng", "S. Wang", "F. Morstatter", "R. Trevino", "J. Tang", "H. Liu"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Zechao Li", "Yi Yang", "Jing Liu", "Xiaofang Zhou", "Hanqing Lu"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Sparsity score: A new filter feature selection method based on graph", "author": ["Mingxia Liu", "Dan Sun", "Daoqiang Zhang"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Global and local structure preservation for feature selection", "author": ["Xinwang Liu", "Lei Wang", "Jian Zhang", "Jianping Yin", "Huan Liu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Supervised feature selection in graphs with path coding penalties and network flows", "author": ["Julien Mairal", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "A fast clustering-based feature subset selection algorithm for high-dimensional data", "author": ["Qinbao Song", "Jingjie Ni", "Guangtao Wang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Feature selection via global redundancy minimization", "author": ["De Wang", "Feiping Nie", "Heng Huang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Feature selection with integrated relevance and redundancy optimization", "author": ["Xuerui Wang", "Andrew McCallum", "Xing Wei"], "venue": "In Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces", "author": ["Roger Weber", "Hans-J\u00f6rg Schek", "Stephen Blott"], "venue": "In VLDB,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "l2, 1norm regularized discriminative feature selection for unsupervised learning", "author": ["Yi Yang", "Heng Tao Shen", "Zhigang Ma", "Zi Huang", "Xiaofang Zhou"], "venue": "In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Scalable sparse subspace clustering by orthogonal matching pursuit", "author": ["Chong You", "Daniel Robinson", "Ren\u00e9 Vidal"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Efficient feature selection via analysis of relevance and redundancy", "author": ["Lei Yu", "Huan Liu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Efficient spectral feature selection with minimum redundancy", "author": ["Zheng Zhao", "Lei Wang", "Huan Liu"], "venue": "In AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "On similarity preserving feature selection", "author": ["Zheng Zhao", "Lei Wang", "Huan Liu", "Jieping Ye"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 60, "endOffset": 63}, {"referenceID": 13, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "When the dimensional size of data becomes high, or say, for high dimensional datasets, we will meet the curse of high dimensionality issue [2].", "startOffset": 139, "endOffset": 142}, {"referenceID": 22, "context": "inaccurate results [23] [1].", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "inaccurate results [23] [1].", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "This create an embarrassing chicken-and-egg problem [5] for unsupervised feature selection algorithms: \u201cthe success of feature selection depends on the quality of indication vectors which are related to the structure of data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "\u201d Most existing unsupervised feature selection algorithms use all original features [5] to build the similarity graph.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Without lose of generality, we categorize features into three groups [4]: relevant feature,irrelevant feature and redundant feature.", "startOffset": 69, "endOffset": 72}, {"referenceID": 17, "context": "For supervised feature selection algorithms [18] [20] [17], these indication vectors usually relate to class labels.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "For supervised feature selection algorithms [18] [20] [17], these indication vectors usually relate to class labels.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "For supervised feature selection algorithms [18] [20] [17], these indication vectors usually relate to class labels.", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "For unsupervised scenario [6] [3], as we mentioned early, they follow the structure of data.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "For unsupervised scenario [6] [3], as we mentioned early, they follow the structure of data.", "startOffset": 30, "endOffset": 33}, {"referenceID": 25, "context": "The formal definition of redundant feature is by [26] based on the Markov blanket given by [10].", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "The formal definition of redundant feature is by [26] based on the Markov blanket given by [10].", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "The first step is to build a Sparse Feature Graph (SFG) at feature side based on sparse representation concept from subspace clustering theory [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Unlike the feature redundancy defined bt Markov blanket [26] which is popular in existing research works, our definition of feature redundancy is based on the linear correlation between two vectors (the \u201cvector\u201d we used here could be a feature vector or a linear combination of several feature vectors.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": ") To measure the redundancy between two vectors fi and fj , squared cosine similarity[22] is used: Rij = cos (fi,fj).", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "This is the discussed \u2018chicken-and-egg\u201d problem [5] between structure characterization and feature selection.", "startOffset": 48, "endOffset": 51}, {"referenceID": 24, "context": "To solve the optimization problem 3, we use Orthogonal Matching Pursuit (OMP) solver [25] here since the number of features in our datasets is larger than 1,000.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "We select twelve real-world high dimensional datasets [11] from three different domains: Image, Text and Biomedical.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "The spectral clustering algorithm we used in our experiments is the NgJordan-Weiss (NJW) algorithm [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "Prestigious works include [26] which gives a formal definition of redundant features.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "[17] propose a greedy algorithm (named as mRMR) to select features with minimum redundancy and maximum dependency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] develop an efficient spectral feature selection algorithm to minimize the", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Recently, researchers pay attention to unsupervised feature selection with global minimized redundancy [22] [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Recently, researchers pay attention to unsupervised feature selection with global minimized redundancy [22] [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "Several graph based approaches are proposed in [15], [19].", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "Several graph based approaches are proposed in [15], [19].", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "The most closed research work to us is [13] which build a sparse graph at feature side and ranking features by approximation errors.", "startOffset": 39, "endOffset": 43}], "year": 2017, "abstractText": "The redundant features existing in high dimensional datasets always affect the performance of learning and mining algorithms. How to detect and remove them is an important research topic in machine learning and data mining research. In this paper, we propose a graph based approach to find and remove those redundant features automatically for high dimensional data. Based on sparse learning based unsupervised feature selection framework, Sparse Feature Graph (SFG) is introduced not only to model the redundancy between two features, but also to disclose the group redundancy between two groups of features. With SFG, we can divide the whole features into different groups, and improve the intrinsic structure of data by removing detected redundant features. With accurate data structure, quality indicator vectors can be obtained to improve the learning performance of existing unsupervised feature selection algorithms such as multi-cluster feature selection (MCFS). Our experimental results on benchmark datasets show that the proposed SFG and feature redundancy remove algorithm can improve the performance of unsupervised feature selection algorithms consistently.", "creator": "LaTeX with hyperref package"}}}