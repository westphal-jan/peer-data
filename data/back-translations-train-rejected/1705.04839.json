{"id": "1705.04839", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2017", "title": "Annotating and Modeling Empathy in Spoken Conversations", "abstract": "Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. The lack of an operational definition of empathy makes it difficult to measure it. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline.", "histories": [["v1", "Sat, 13 May 2017 14:49:08 GMT  (884kb,D)", "http://arxiv.org/abs/1705.04839v1", "Journal"]], "COMMENTS": "Journal", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["firoj alam", "morena danieli", "giuseppe riccardi"], "accepted": false, "id": "1705.04839"}, "pdf": {"name": "1705.04839.pdf", "metadata": {"source": "CRF", "title": "Annotating and Modeling Empathy in Spoken Conversations", "authors": ["Firoj Alam", "Morena Danieli", "Giuseppe Riccardi"], "emails": ["giuseppe.riccardi}@unitn.it"], "sections": [{"heading": null, "text": "Empathy, as it is defined in behavioral sciences, expresses a person's ability to recognize, understand, and respond to the emotions, attitudes, and beliefs of others. Lack of an operative definition of empathy makes it difficult to measure them. In this paper, we address two related problems in automatic affective behavioral analysis: the design of the annotation protocol and the automatic detection of empathy from spoken conversations. We propose and evaluate an empathy annotation scheme inspired by the modal model of emotions. The annotation scheme was evaluated using a corpus of real, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different levels of language and representation on which empathy can be conveyed, we examined characteristics derived from the lexical and acoustic space. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy."}, {"heading": "1. Introduction", "text": "We are interested in solving the problems."}, {"heading": "2. Background Research", "text": "In fact, most people who are able to move, to move and to move, to move, to move, to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move."}, {"heading": "3. The Modal Model of Emotions", "text": "Many psychologists have studied emotional episodes from the viewpoint of assessment dimensions. Gross [6] has provided evidence that concepts such as the emergence - the derivation from the expectations of relationships - and the unfolding - sequences that persist over time - can help explain emotional events. It has been shown that the temporal unfolding of emotions can be conceptualized and tested experimentally [26]. The modal model of emotions developed by Gross [6, 27] emphasizes the attentive and evaluating actions that underlie the emotional arousal process. In Figure 1, we provide the original scheme of the Gross Model. The central Attention Appraisal Procedure (which is included in the box) is influenced by the situation that is objectively defined in relation to physical or virtual spaces and objects. The situation forces the attention of the individual; it triggers an assessment process and leads to coordinated and formal responses."}, {"heading": "4. The Empathy Annotation Model", "text": "The aim of the annotation model for empathy is to provide empirical evidence for the reference definition and to provide commented signal examples for the formation of a computational model. In the following sections we describe the challenges of annotating real stimuli (Section 4.1), report on the qualitative data analysis of the dialogue corpus (Section 4.2), the operative definition of empathy (Section 4.3) in the context of call center conversations, the annotation process (Section 4.4) and the evaluation of the decisions of the annotators (Section 4.5)."}, {"heading": "4.1. Annotation of Real-Life Spoken Conversations", "text": "In general, the commentator can be presented with images or speech segments (stimuli) and a series of emotional labels, the stimulus being defined by the medium through which the emotion is transmitted, and its content and context. The medium can be language [29], image [30] or multimodal [31]. Content refers to the information encoded in the stimulus signal, such as the facial expression of anger or speech expression. The context of the stimulus is represented by the spatial or temporal signals that surround the stimulus. Knowledge of the context can be crucial in interpreting the cause of emotion manifestations. For a speech stimulus in a conversation, the context is represented by the preceding dialogue that rotates [29]. The above description of the annotation unit does not assume a clear relationship between the occurrence of an emotion and its corresponding expression."}, {"heading": "4.2. Qualitative Data Analysis", "text": "We have carried out qualitative analyses of the spoken corpus in order to gain insights into the manifestation of emotions and empathic phenomena in affective scenes.The analysis was carried out via a corpus of human-human dyadic Italian call centre conversations described in Section 5. we analysed one hundred conversations (more than 11 hours) and selected dialogues in which the voice signal showed the emergence of a number of basic emotions (e.g. frustration, anger) on the customer channel and empathy on the agent channel. In Table 1 we present an excerpt of dialogue with comments to illustrate the parallel linguistic, lexical and discursive keywords. The excerpt is reported in the first column of the table, where C is the customer and A is the agent. The situation is as follows: C calls because a payment is actually overdue: he is ashamed that he is not able to pay immediately, and has a lot of concerns."}, {"heading": "4.3. The Operational Definition of Empathy", "text": "Following the modal model of emotions and the annotation model discussed above, we first describe the context of the situation, the attention, the evaluation and the reaction components of the empathetic process. The context of the situation: In conversations in the call center, customers can ask for information or ask for help to solve technical or accounting problems. The operative definition of empathy that can be applied in this context requires that the commentators are informed about the social context and the task at hand. They are trained to focus on subdialogues in which the agent proposes solutions and clarifications (attention) based on the understanding of the customer's problem (evaluation). As a result, the agent's comments can prevent the customer's unpleasant feelings (reaction)."}, {"heading": "4.4. The Annotation Procedure", "text": "The typical approach of the comments selects a series of conversations and comments on each segment of the conversation. In this work, we focused on the first instance of neutral segment pairs with empathy within each conversation, to maximize the number of commented conversations with many speakers given limited resources. Annotators manually refined the boundaries of the segments generated by a standard segmentator for language and non-language. Annotators marked the empathy segments on the agent channel and the basic emotions (frustration and anger) on the customer channel. The reason for selecting these basic emotions was the limited resources for comments. Another reason was that we observed that these are the most common cases in such conversations. Annotators were instructed to select the candidate segment pairs with a decision-making process. Once the relevant language region was identified, the annotators could listen to the speech as often as they needed to assess whether the segment could be selected (with) any one."}, {"heading": "4.5. Evaluation of the Annotation Model", "text": "To assess the reliability of the annotation model, we designed the following evaluation task. Two annotations with a psychological background worked independently of each other through a series of 64 spoken conversations randomly selected from the call center corpus. The annotations were of similar age, ethnicity, and gender. We intended to assess whether the annotations could perceive a change in emotional state in the same initial position (neutral leading to empathy), as well as their correspondence with the assignment of the empathy label. The correspondence between the annotated segments between the two annotations is 53.1%, with the correspondence for the same position being 31.2%. To measure the reliability of the annotations, we calculated a match between the annotators by using the kappa statistics [36]. Kappa statistics are often used to exclude the degree of coincidence between the annotations by evaluating the probability that they match by the annotations."}, {"heading": "5. Corpus Analysis", "text": "The average duration of the conversations was 406 seconds.We analyzed the distribution of emotional sequences describing the affective scene in the conversations. We observed that empathy was perceived by commentators in 27.72% of the conversations as a whole, as can be seen in Table 2, in which the column represents the channel and the line of the agent. By analyzing only empathetic conversations (27.72%), we observed that when the agent manifested empathy, no frustration or anger was manifested by the client in 70% of cases. This affective scene can be explained in part by specific training of the agents in anticipating problems of the client and by appropriate actions. For the remaining 30% of conversations, no frustration or anger was shown in the client. Furthermore, this affective scene is explainable for 12.99% of customers that the customer's anger or anger was not detectable in both situations, while the frustration of the agent was manifested in both situations."}, {"heading": "5.1. Acoustic Feature Analysis", "text": "We examined and compared the pattern sequences of low acoustic characteristics before and after the insertion point, from the neutral to the empathy segment. An example of speech segment annotation is shown in Figure 2, where we plot the spectral centroid-1 characteristic patterns within the neutral and empathetically connoted spectrum. Each segment is 15 seconds long and the beginning is marked by a vertical bold line that distinguishes the left (context) and right segments commented with empathy. From the signal trend of this feature, we see that there is a marked profile change, which is correlated by its high statistical significance (p \u2212 value = 4.61E \u2212 51, d = 1.4 and t = \u2212 16.6 with two tailored two-sample t-test). The low features were extracted from both the left and right segments with 100 overlapping frames per second of extracted frames."}, {"heading": "5.2. Lexical Feature Analysis", "text": "Several categories of personnel who interact with customers or patients, including call center agents and doctors, will be trained to improve their communication skills and develop empathy in their interactions. [42, 43] For example, they will be asked to use phrases such as \"I understand\" when listening to the customer explaining their problem, or \"I would.\" For example, we will analyze lexical realization in empathetic and neutral segments by comparing the different word frequencies and POS distributions before proposing possible solutions."}, {"heading": "6. Automatic Classification of Empathy", "text": "In this section we describe the formation of an automatic classification system to detect empathy from spoken conversations. We report on experimental details of trait extraction, fusion and classification and discuss the results."}, {"heading": "6.1. Classification Task and Data-set", "text": "In the experiment of automatic classification, our goal was to investigate the empathic manifestations of the operator at the segment level. We selected a subset of the corpus, comprising a total of 526 conversations labeled with automatic speech transcriptions as well as neutrals, empathy segments. This subset of the corpus allows us to perform a complete computer model training and complete evaluation under noisy and clean input signal conditions. We divided the data set into 70%, 15% and 15% partitions and overlapped without loudspeakers. To train and evaluate the system, we extracted from these conversations a neutral-empathetic segment pair, which was obtained from manual annotations. Descriptive statistics of these segment pairs are included in Table 5, along with averages and standard deviations on the natural distribution of the data. As we see in the table, the segment length of the neutral segment is comparatively longer than the total pathic radiance from the beginning of this segment to the end of this 44 hours."}, {"heading": "6.2. Classification System", "text": "In Figure 3, we present a computational architecture of the automatic classification system that uses the agent's speech channel as input and then passes it on to the automatic speech vs. non-speech segmentator. Then, it generates a binary decision for each language segment of the agent's behavior in terms of neutrality versus empathy. To evaluate the relative impact of lexical features, we looked at the case of noisy transcriptions (left branch in Figure 3) provided by an automatic speech recognition system (ASR). We extracted, combined and selected acoustic features directly from the speech signal and generated the classifier training set. We implemented both fusion and decision fusion algorithms (bottom Figure 3) to examine the performance of different classifier configurations. This architecture design can be used in real-time application that combines all automated processes."}, {"heading": "6.3. Speech vs Non-Speech Segmenter", "text": "A HMM-based speech segmentator was trained on a set of 150 conversations with approximately 100 hours of spoken content, and Kaldi [45] was used for training and decoding processes, training data was processed using force-oriented transcriptions, and the Mel Frequency Cepstral Coefficient (MFCC) and its derivatives were used. Gaussian and bar widths were optimized with a development set of 50 conversations, and the final model was tested on a test of 50 conversations, with the system's F measure being 66.0% of the test set."}, {"heading": "6.4. Undersampling and Oversampling", "text": "The statistics in Table 5 manifest the problem of data imbalance for the two classes, empathy and neutral. Once the manual segments are processed by the automatic segmentator, the ratio of empathy / neutral marked segments is 6% versus 94%. A common approach to coping with this is oversampling or undersampling in the data or feature area. We have amplified the instance of the majority class (i.e. neutral) on the data level and sampled the minority class (i.e. empathy) on the feature level. In the literature it is reported that the combination of oversampling and undersampling often leads to better performance [46]. For subsampling, we defined a series of containers with different segment lengths, and then randomly selected K segments from each segment. We used K = 1 for this study."}, {"heading": "6.5. Feature Extraction", "text": "In this section we report on the algorithms used to obtain the acoustic, linguistic and psycholinguistic characteristics of the empathy recognition task. We discuss differences and similarities between the characteristics used in our experiments and those used in other tasks for emotion and personality recognition [48, 49]."}, {"heading": "6.5.1. Acoustic Features", "text": "Recent studies on emotion and personality perception showed that the paralinguistic characteristics of language are well represented by low levels [50, 48, 49]. We took a similar approach, extracting a very large number of low levels and their statistical functionalities using the openSMILE tool [39]. Low acoustic characteristics include the configuration files of the COMPARE-2013. We extracted low acoustic characteristics at about 100 frames per second. [52] To increase reproducibility, we made the configuration files of the function publicly available.2 We extracted low acoustic characteristics at about 100 frames per second. In terms of image quality, the screen size was 60 milliseconds.2https: / github.com / firojalam / openSMILE-configurations.0"}, {"heading": "6.5.2. Lexical Features", "text": "The automatic transcriptions were generated using a large vocabulary ASR system [53]. We designed an HMM-based ASR system with a subset of 1894 conversations, containing about 100 hours of spoken content and a lexicon of size. (1-3) The most active features of accumulation range from max., min and the range of square means to percentiles. (1-3) Quarterly numbers (1-3) cover the relative positions of max., min and the range of square means."}, {"heading": "6.5.3. Psycholinguistic Features", "text": "Similar to the lexical traits, we have extracted the so-called psycholinguistic traits from automatic transcriptions. In recent decades, Pennebaker et al. have developed psycholinguistic word categories using high-frequency words and developed the Linguistic Inquiry Word Count (LIWC). [54] These word categories are mainly used to examine gender, age, personality and health to estimate the correlation between these attributes and word usage (see [49, 34] and the references contained therein. Types of LIWC traits include the following: 1. General traits such as word count, average number of words per decade, percentage of words found in the dictionary and words longer than six letters and numbers. 2. Linguistic traits include pronouns and articles.3. Psychological traits include affects, cognition and biological phenomena. 4. Characteristics about personal concerns include work and 5."}, {"heading": "6.6. Feature Fusion and Selection", "text": "In fact, most people are able to decide for themselves what they want, and they don't."}, {"heading": "6.7. Classification and Evaluation", "text": "In fact, it is so that it is a matter of a way in which people are able to determine themselves how they have behaved. (...) It is not as if people are able to behave. (...) It is as if people were able to change the world. (...) It is as if people were able to change the world. (...) It is as if people were able to change the world. (...) It is as if people were able to change the world. (...) It is as if people were able to change the world. (...) It is as if people were able to change the world. (...) It is as if people were able to change the world. (...) It is as if they were able to change the world. (...) The world is as if the world is in the world, the world in the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) The world in the world, the world in the world. (...) It is as if the world in the world is in the world, the world in the world. (... It is in the world in the world, in the world in the world in the world. (...) It is as if it is in the world in the world in the world, in the world in the world in the world. (... It is in the world in the world in the world in the world."}, {"heading": "6.8. Results and Discussion", "text": "In Table 8, we report on the performance of the automatic classification system, which is trained on different types of characteristics: lexical (automatic transcriptions), acoustic and psycholinguistic. We report on test results for combination-based systems and classifier combinations. In the latter system, we used majority voting. To calculate the baseline, we randomly selected class identifiers based on the previous class distribution. For statistical significance, we calculated McNemar's significant test via the test set [63]. For individual characteristic systems, acoustic models provided the best performance compared to lexical and psycholinguistic tests alone. The results of the acoustic system are significantly better than random basal script tests with p < 2,2E \u2212 16. The acoustic system provides a useful and minor classification model when no automatic transcriptions are available."}, {"heading": "7. Conclusion", "text": "Empathy refers to an emotional state triggered by a shared emotional experience. In this paper, we have addressed the problem of observing and commenting on empathy manifestations in real spoken conversations; the commenting process describes the scene using the Situation-Attention-Assessment-Response model; we have operationalized the definition of empathy and designed and evaluated a commenting process of human-human dialogues in call centers; we have developed a system that automatically segments and classifies empathic events from spoken conversations; we have studied the effectiveness of acoustic, lexical and psycholinguistic traits and their combinations; the performance of our best classification system is very promising and well above the baseline; such a commenting model and classification services can lead to enhancements in other situations and applications in human-machine interaction and automatic behavioral analysis; and in future work, we plan to expand this work to include a comparative study of the simultaneous effects of entering and interacting with other agents."}, {"heading": "Acknowledgment", "text": "The research that led to these results was funded by the Seventh Framework Programme of the European Union (FP7 / 2007-2013) under Funding Agreement 610916-SENSEI."}], "references": [{"title": "Computational paralinguistics: emotion", "author": ["B. Schuller", "A. Batliner"], "venue": "affect and personality in speech and language processing, John Wiley & Sons", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Empathy and prosocial behavior", "author": ["M.L. Hoffman"], "venue": "Handbook of Emotions 3 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Lectures on the Experimental Psychology of the Thoughtprocesses", "author": ["E.B. Titchener"], "venue": "Macmillan", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1909}, {"title": "Mirror neurons and the simulation theory of mindreading", "author": ["V. Gallese", "A. Goldman"], "venue": "Trends in Cognitive Sciences 2 (12) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "D", "author": ["S. Baron-Cohen", "M. Lombardo", "H. Tager-Flusberg"], "venue": "Cohen (Eds.), Understanding Other Minds: Perspectives from Developmental Social Neuroscience, 3rd Edition, Oxford University Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "The emerging field of emotion regulation: An integrative review", "author": ["J.J. Gross"], "venue": "Review of General Psychology 2 (3) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "The neuroscience of empathy: progress", "author": ["J. Zaki", "K.N. Ochsner"], "venue": "pitfalls and promise, Nature neuroscience 15 (5) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "S", "author": ["R.A. Calvo"], "venue": "D\u2019Mello, Affect detection: An interdisciplinary review of models, methods, and their applications, IEEE Transactions on affective computing 1 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The many faces of empathy: Parsing empathic phenomena through a proximate", "author": ["S.D. Preston", "A.J. Hofelich"], "venue": "dynamic-systems view of representing the other in the self, Emotion Review 4 (1) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "The Social Neuroscience of Empathy", "author": ["C.D. Batson"], "venue": "MIT press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Human empathy through the lens of social neuroscience", "author": ["J. Decety", "C. Lamm"], "venue": "The Scientific World Journal 6 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Empathy: a motivated account", "author": ["J. Zaki"], "venue": "Psychological Bulletin", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "W", "author": ["P.R. Gesn"], "venue": "Ickes, The development of meaning contexts for empathic accuracy: Channel and sequence effects., Journal of Personality and Social Psychology 77 (4) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge", "author": ["B. Schuller", "A. Batliner", "S. Steidl", "D. Seppi"], "venue": "Speech Communication 53 (9) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Survey on speech emotion recognition: Features", "author": ["M. El Ayadi", "M.S. Kamel", "F. Karray"], "venue": "classification schemes, and databases, Pattern Recognition 44 (3) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Modelli prosodici emotivi per la sintesi dell\u2019italiano", "author": ["F. Tesser", "P. Cosi", "C. Drioli", "G. Tisato", "P. ISTC-CNR"], "venue": "in: Proc. of Associazione Italiana di Scienze della Voce (AISV)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Prosodic analysis of a multistyle corpus in the perspective of emotional speech synthesis", "author": ["E. Zovato", "S. Sandri", "S. Quazza", "L. Badino"], "venue": "in: Proc. of Interspeech, Vol. 2", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "The description of naturally occurring emotional speech", "author": ["E. Douglas-Cowie", "R. Cowie", "M. Schroeder"], "venue": "in: Proc. of 15th Int. Congr. of Phonetic Sciences", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic classification of emotions via global and local prosodic features on a multilingual emotional database", "author": ["A. Origlia", "V. Galat\u00e0", "B. Ludusan"], "venue": "in: Proc. of Speech Prosody", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "The SE- MAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent", "author": ["G. McKeown", "M. Valstar", "R. Cowie", "M. Pantic", "M. Schroder"], "venue": "IEEE Trans. on Affective Computing 3 (1) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions", "author": ["F. Ringeval", "A. Sonderegger", "J. Sauer", "D. Lalanne"], "venue": "in: Proc. of 10th IEEE Int. Conf. and Workshops on Automatic Face and Gesture Recognition (FG)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities", "author": ["A. Metallinou", "S. Narayanan"], "venue": "in: Proc. of 10th IEEE Int. Conf. and Workshops on Automatic Face and Gesture Recognition (FG)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Analyzing empathetic interactions based on the probabilistic modeling of the co-occurrence patterns of facial expressions in group meetings", "author": ["S. Kumano", "K. Otsuka", "D. Mikami", "J. Yamato"], "venue": "in: Proc. of IEEE Int. Conf. on Automatic Face & Gesture Recognition and Workshops ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Analyzing the language of therapist empathy in motivational interview based psychotherapy", "author": ["B. Xiao", "P. Georgiou", "S. Narayanan"], "venue": "in: Proc. of Asia Pacific Signal & Inf. Process. Assoc.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Bo", "author": ["B. Xiao"], "venue": "S. Daniel, I. Maarten Van, A. Zac E., G. David C., N. Panayiotis G., S. S., Modeling therapist empathy through prosody in drug addiction counseling, in: Proc. of Interspeech", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A systems approach to appraisal mechanisms in emotion", "author": ["D. Sander", "D. Grandjean", "K.R. Scherer"], "venue": "Neural Networks 18 (4) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Emotion regulation: Conceptual foundations", "author": ["J.J. Gross", "R.A. Thompson"], "venue": "Handbook of Emotion Regulation 3 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Emotion unfolding and affective scenes: A case study in spoken conversations", "author": ["M. Danieli", "G. Riccardi", "F. Alam"], "venue": "in: Proc. of Emotion Representations and Modelling for Companion Systems (ERM4CT) 2015\u201e ICMI", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Using context to improve emotion detection in spoken dialog systems", "author": ["J. Liscombe", "G. Riccardi", "D. Hakkani-Tur"], "venue": "in: Proc. of Interspeech", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic analysis of facial expressions: The state of the art", "author": ["M. Pantic", "L.J.M. Rothkrantz"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (12) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "Context-sensitive learning for enhanced audiovisual emotion classification", "author": ["A. Metallinou", "M. Wollmer", "A. Katsamanis", "F. Eyben", "B. Schuller", "S. Narayanan"], "venue": "IEEE Trans. on Affective Computing 3 (2) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Do facial movements express emotions or communicate motives", "author": ["B. Parkinson"], "venue": "Personality and Social Psychology Review 9 (4) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Emotion recognition in human-computer interaction", "author": ["R. Cowie", "E. Douglas-Cowie", "N. Tsapatsoulis", "G. Votsis", "S. Kollias", "W. Fellenz", "J.G. Taylor"], "venue": "Signal Processing Magazine, IEEE 18 (1) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Emotion unfolding and affective scenes: A case study in spoken conversations", "author": ["M. Danieli", "G. Riccardi", "F. Alam"], "venue": "in: Proc. of Emotion Representations and Modelling for Companion Systems (ERM4CT) 2015\u201e ICMI", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Transcribing and annotating spoken language with EXMAR- ALDA", "author": ["T. Schmidt"], "venue": "in: Proc. of LREC 2004 Workshop on XML-based Richly Annotated Corpora", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Assessing agreement on classification tasks: the kappa statistic", "author": ["J. Carletta"], "venue": "Computational linguistics 22 (2) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Emotv1: Annotation of real-life emotions for the specification of multimodal affective interfaces", "author": ["S. Abrilian", "L. Devillers", "S. Buisine", "J.-C. Martin"], "venue": "in: HCI International", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "An introduction to audio content analysis: Applications in signal processing and music informatics", "author": ["A. Lerch"], "venue": "John Wiley & Sons", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent developments in opensmile", "author": ["F. Eyben", "F. Weninger", "F. Gross", "B. Schuller"], "venue": "the munich open-source multimedia feature extractor, in: Proc. of the 21st ACM international conference on Multimedia (ACMM), ACM", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Prosody and empathic communication in psychotherapy interaction", "author": ["E. Weiste", "A. Per\u00e4kyl\u00e4"], "venue": "Psychotherapy Research 24 (6) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Let me see if i have this right", "author": ["J.L. Coulehan", "F.W. Platt", "B. Egener", "R. Frankel", "C.-T. Lin", "B. Lown", "W.H. Salazar"], "venue": ". . : words that help build empathy, Annals of Internal Medicine 135 (3) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "Keep Them Calling: Superior Service on the Telephone", "author": ["S. Barrett"], "venue": "American Media Inc", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1996}, {"title": "Segmenting into adequate units for automatic recognition of emotion-related episodes: a speech-based approach", "author": ["A. Batliner", "S. Steidl", "D. Seppi", "B. Schuller"], "venue": "Advances in Human-Computer Interaction 2010 ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "P", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian"], "venue": "Schwarz, et al., The kaldi speech recognition toolkit, in: Proc. of Automatic Speech Recognition and Understanding Workshop (ASRU)", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Smote: synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of artificial intelligence research ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Paralinguistics in speech and language state-of-the-art and the challenge", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "F. Burkhardt", "L. Devillers", "C. M\u00fcller", "S. Narayanan"], "venue": "Computer Speech & Language 27 (1) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Fusion of acoustic", "author": ["F. Alam", "G. Riccardi"], "venue": "linguistic and psycholinguistic features for speaker personality traits recognition, in: Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic emotion recognition: A benchmark comparison of performances", "author": ["B. Schuller", "B. Vlasenko", "F. Eyben", "G. Rigoll", "A. Wendemuth"], "venue": "in: Proc. of Automatic Speech Recognition and Understanding Workshop (ASRU)", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "E", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "A. Vinciarelli", "K. Scherer", "F. Ringeval", "M. Chetouani", "F. Weninger", "F. Eyben"], "venue": "Marchi, et al., The interspeech 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism, in: Proc. of Interspeech", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised recognition and clustering of speech overlaps in spoken conversations", "author": ["S.A. Chowdhury", "G. Riccardi", "F. Alam"], "venue": "in: Proc. of Workshop on Speech, Language and Audio in Multimedia - SLAM2014", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic inquiry and word count", "author": ["J.W. Pennebaker", "M.E. Francis", "R.J. Booth"], "venue": "Liwc", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2001}, {"title": "A", "author": ["F. Alparone", "S. Caso", "A. Agosti"], "venue": "Rellini, The italian liwc2001 dictionary., Tech. rep., LIWC.net, Austin, TX ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2004}, {"title": "The psychological meaning of words: Liwc and computerized text analysis methods", "author": ["Y.R. Tausczik", "J.W. Pennebaker"], "venue": "Journal of Language and Social Psychology 29 (1) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimating attributes: analysis and extensions of relief", "author": ["I. Kononenko"], "venue": "in: Proc. of Machine Learning: European Conference on Machine Learning (ECML), Springer", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1994}, {"title": "Comparative study of speaker personality traits recognition in conversational and broadcast news speech", "author": ["F. Alam", "G. Riccardi"], "venue": "in: Proc. of Interspeech, ISCA", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast Training of Support Vector Machines using Sequential Minimal Optimization", "author": ["J. Platt"], "venue": "MIT Press", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1998}, {"title": "A", "author": ["B. Schuller", "S. Steidl"], "venue": "Batliner, The interspeech 2009 emotion challenge., in: Proc. of Interspeech", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio segmentation-byclassification approach based on factor analysis in broadcast news domain", "author": ["D. Cast\u00e1n", "A. Ortega", "A. Miguel", "E. Lleida"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing 2014 (1) ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Note on the sampling error of the difference between correlated proportions or percentages", "author": ["Q. McNemar"], "venue": "Psychometrika 12 (2) ", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1947}, {"title": "Can we detect speakers\u2019 empathy?: A real-life case study", "author": ["A. Firoj", "D. Morena", "R. Giuseppe"], "venue": "in: 7th IEEE International Conference on Cognitive InfoCommunications", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "While early works on affective computing mostly focused on the recognition of basic emotions ([1]), in this work we investigate empathy.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "In this paper, we refer to the psychological definition of empathy by Hoffman, who defines it as \"an emotional state triggered by another\u2019s emotional state or situation, in which one feels what the other feels or would normally be expected to feel in his situation\" ([2]).", "startOffset": 267, "endOffset": 270}, {"referenceID": 2, "context": "The word empathy was coined by Titchener in 1909 [3] as a translation of the German term Einf\u00fchlung.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "These include action understanding, attribution of intentions (mind-reading), and recognition of emotions and sensations [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "In everyday life, empathy supports important aspects of inter-personal communication to an extent where some psychic diseases that affect the relationship with other persons, such as autism and Asperger syndrome, are explained in terms of impairment of empathic ability [5].", "startOffset": 270, "endOffset": 273}, {"referenceID": 5, "context": "To this end we adopt the modal model of emotions by Gross [6] as a promising framework for defining an operational concept of empathy.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "Psychology and Neuroscience Research Over the past decades there have been significant efforts in investigating empathy in the fields of psychology and neuroscience [7] [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "Psychology and Neuroscience Research Over the past decades there have been significant efforts in investigating empathy in the fields of psychology and neuroscience [7] [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "For example, the work in [9] accounts for different empathic phenomena occurring in the literature on empathy, and [10] examines eight distinct phenomena commonly labeled as empathy including emotional contagion, sympathy, projection, and", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "For example, the work in [9] accounts for different empathic phenomena occurring in the literature on empathy, and [10] examines eight distinct phenomena commonly labeled as empathy including emotional contagion, sympathy, projection, and", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "Decety and Lamm [11] observe that some of the different definitions of empathy may share the underlying concept of \"[.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "The work in [12] takes a different perspective on empathy manifestations by focusing on the role of motivations for explaining a particular feature of the empathic experience.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Both verbal and non-verbal levels of spoken communication [13] have been considered since both are suggested to embody the expressive potential of language.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "Major focus has been devoted to the paralinguistic features of emotional speech, on the basis of the experimental evidence that emotional information is mostly conveyed by those levels (see [14] for a state-of-the-art review).", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "The authors in [15] report a significant disparity among those corpora in terms of complexity of the annotated emotions, explicitness of the emotion definitions, and identification of the annotation units.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Most emotional corpora have been designed to perform specific tasks such as emotion recognition or emotional speech synthesis [16, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 16, "context": "Most emotional corpora have been designed to perform specific tasks such as emotion recognition or emotional speech synthesis [16, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "The HUMAINE project [18] and the emotion multilingual collection in [19] base their annotation schemes on sets of discrete emotional lexical items or basic emotions.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "The HUMAINE project [18] and the emotion multilingual collection in [19] base their annotation schemes on sets of discrete emotional lexical items or basic emotions.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "The models that foster this approach, such as those discussed in [20] and [21], require annotators to continuously assign values to emotional dimensions and sets of emotional descriptors.", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "The models that foster this approach, such as those discussed in [20] and [21], require annotators to continuously assign values to emotional dimensions and sets of emotional descriptors.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "Metallinou and Narayanan [22] emphasize that continuous annotation has several benefits, as well as some open challenges.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "In [22], authors report a high number of factors that may affect inter-annotator agreement, such as person-specific annotation delays and confidence in understanding emotional attributes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[23] studied four-party meeting conversations to estimate and classify empathy, antipathy and unconcerned emotional interactions utilizing facial expression, gaze and speech-silence features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "In [24] and [25], Xiao et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [24] and [25], Xiao et al.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "Gross [6] has provided evidence that concepts such as emergence \u2014 derivation from the expectations of relationships \u2014 and unfolding \u2014 sequences that persist over time \u2014 may help in explaining emotional events.", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "It has been shown that temporal unfolding of emotions can be conceptualized and experimentally tested [26].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": "Figure 1: The modal model of emotion [6].", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "by Gross [6, 27] emphasizes the attentional and appraisal acts underlying the emotion-arousing process.", "startOffset": 9, "endOffset": 16}, {"referenceID": 26, "context": "by Gross [6, 27] emphasizes the attentional and appraisal acts underlying the emotion-arousing process.", "startOffset": 9, "endOffset": 16}, {"referenceID": 27, "context": "The modal model of emotions provides a useful framework for describing the contextual dynamics of emotions within an affective scene([28]), since it decomposes the emotional process in terms of situation selection, attentional deployment, and situational modification.", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "The medium may be speech [29], image [30] or multimodal [31].", "startOffset": 25, "endOffset": 29}, {"referenceID": 29, "context": "The medium may be speech [29], image [30] or multimodal [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "The medium may be speech [29], image [30] or multimodal [31].", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "For a speech stimulus in a conversation the context is represented by the preceding dialog turns [29].", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "Last but not least there might be a non-deterministic relation between the emotion-expression and motive-communication ([32]).", "startOffset": 120, "endOffset": 124}, {"referenceID": 32, "context": "stimuli are sentences to be read and actors enacting affective scenes [33].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "The limitations of determining the temporal boundaries of the annotation units have motivated researchers to investigate the process of continuous annotation [22, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 19, "context": "The limitations of determining the temporal boundaries of the annotation units have motivated researchers to investigate the process of continuous annotation [22, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 33, "context": "This annotation protocol has been applied to annotate sequences of basic emotions occurring in affective scenes ([34]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 34, "context": "For the annotation task, the annotators used the EXMARaLDA Partitur Editor [35].", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": "To measure the reliability of the annotations we calculated inter-annotator agreement by using the kappa statistics [36].", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "50) maybe due to a variety of factors, including short audio clips or utterance ([37, 29]), multi-label annotation tasks, and annotator agreement when the annotation task is based on continuous and discrete label annotations [22].", "startOffset": 81, "endOffset": 89}, {"referenceID": 28, "context": "50) maybe due to a variety of factors, including short audio clips or utterance ([37, 29]), multi-label annotation tasks, and annotator agreement when the annotation task is based on continuous and discrete label annotations [22].", "startOffset": 81, "endOffset": 89}, {"referenceID": 21, "context": "50) maybe due to a variety of factors, including short audio clips or utterance ([37, 29]), multi-label annotation tasks, and annotator agreement when the annotation task is based on continuous and discrete label annotations [22].", "startOffset": 225, "endOffset": 229}, {"referenceID": 38, "context": "97, and hammingwindowing, using openSMILE [39].", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "It is defined as the frequency-weighted sum of the power spectrum normalized by its unweighted sum [38].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "Our analysis on the relevance of pitch and loudness for empathy signal realization is consistent with the findings of [40] and [25].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "Our analysis on the relevance of pitch and loudness for empathy signal realization is consistent with the findings of [40] and [25].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "Several categories of personnel who interact with customers or patients, including call center agents and physicians, are trained to improve their communication skills and develop empathy in their interactions through careful choice of words [42, 43].", "startOffset": 242, "endOffset": 250}, {"referenceID": 41, "context": "Several categories of personnel who interact with customers or patients, including call center agents and physicians, are trained to improve their communication skills and develop empathy in their interactions through careful choice of words [42, 43].", "startOffset": 242, "endOffset": 250}, {"referenceID": 42, "context": "It is quite usual that in real-world conversations, the distribution of neutral segments is significantly higher than the manifested emotions as can also be seen in [44].", "startOffset": 165, "endOffset": 169}, {"referenceID": 43, "context": "An HMM-based speech vs non-speech segmenter has been trained using a set of 150 conversations, containing approximately 100 hours of spoken content and used Kaldi [45] for the training and decoding processes.", "startOffset": 163, "endOffset": 167}, {"referenceID": 44, "context": "In the literature it is reported that the combination of oversampling and undersampling often leads to better performance [46].", "startOffset": 122, "endOffset": 126}, {"referenceID": 44, "context": "For oversampling we used Synthetic Minority Oversampling Technique (SMOTE) [46] and its open-source implementation in weka [47].", "startOffset": 75, "endOffset": 79}, {"referenceID": 45, "context": "For oversampling we used Synthetic Minority Oversampling Technique (SMOTE) [46] and its open-source implementation in weka [47].", "startOffset": 123, "endOffset": 127}, {"referenceID": 44, "context": "More details of this approach can be found in [46].", "startOffset": 46, "endOffset": 50}, {"referenceID": 46, "context": "We discuss differences and similarities of the features used in our experiments to those used in other emotion and personality recognition tasks [48, 49].", "startOffset": 145, "endOffset": 153}, {"referenceID": 47, "context": "We discuss differences and similarities of the features used in our experiments to those used in other emotion and personality recognition tasks [48, 49].", "startOffset": 145, "endOffset": 153}, {"referenceID": 48, "context": "Recent studies in emotion and personality recognition showed that the paralinguistic properties of speech are well represented by low-level features [50, 48, 49].", "startOffset": 149, "endOffset": 161}, {"referenceID": 46, "context": "Recent studies in emotion and personality recognition showed that the paralinguistic properties of speech are well represented by low-level features [50, 48, 49].", "startOffset": 149, "endOffset": 161}, {"referenceID": 47, "context": "Recent studies in emotion and personality recognition showed that the paralinguistic properties of speech are well represented by low-level features [50, 48, 49].", "startOffset": 149, "endOffset": 161}, {"referenceID": 38, "context": "We followed a similar approach and we extracted a very large number of low-level features and their statistical functionals using the openSMILE tool [39].", "startOffset": 149, "endOffset": 153}, {"referenceID": 49, "context": "The low-level acoustic features include the feature set of the computational paralinguistic challenge\u2019s (COMPARE-2013) feature set [51], Geneva minimalistic acoustic feature set [52] and few more formant features.", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 1, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 2, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 3, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 0, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 1, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 2, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 3, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 50, "context": "The automatic transcriptions were generated using a large vocabulary ASR system [53].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "For the training and decoding process, we used Kaldi [45].", "startOffset": 53, "endOffset": 57}, {"referenceID": 51, "context": "have designed psycholinguistic word categories using high frequency words and developed the Linguistic Inquiry Word Count (LIWC) [54].", "startOffset": 129, "endOffset": 133}, {"referenceID": 47, "context": "These word categories are mostly used to study gender, age, personality, and health to estimate the correlation between these attributes and word usage (see [49, 34] and the references therein).", "startOffset": 157, "endOffset": 165}, {"referenceID": 33, "context": "These word categories are mostly used to study gender, age, personality, and health to estimate the correlation between these attributes and word usage (see [49, 34] and the references therein).", "startOffset": 157, "endOffset": 165}, {"referenceID": 52, "context": "We used the dictionary that is available within LIWC for Italian [55].", "startOffset": 65, "endOffset": 69}, {"referenceID": 53, "context": "Some features are counts and others are relative frequencies (see [56]).", "startOffset": 66, "endOffset": 70}, {"referenceID": 54, "context": "For feature selection we used the Relief [57] feature selection technique.", "startOffset": 41, "endOffset": 45}, {"referenceID": 55, "context": "In [58], we comparatively evaluated the Relief method against other algorithms and it outperformed them in classification performance and computational cost.", "startOffset": 3, "endOffset": 7}, {"referenceID": 55, "context": "We then selected the optimal set of features by stopping when performance saturated or started decreasing [58].", "startOffset": 106, "endOffset": 110}, {"referenceID": 45, "context": "Since some feature selection algorithms do not support numeric feature values such as information gain and suffer from data sparseness such as Relief, we discretized feature values into 10 equal-frequency bins [47] as a pre-processing", "startOffset": 210, "endOffset": 214}, {"referenceID": 55, "context": "The equal-frequency binning approach and the size of the bin, k = 10, were empirically found optimal in other paralinguistic classification task [58].", "startOffset": 145, "endOffset": 149}, {"referenceID": 56, "context": "We designed our classification models using an open-source implementation of SMO [59] by the Weka machine learning toolkit [47], with its linear kernel for lexical and acoustic features, and its gaussian kernel for the psycholinguistic features.", "startOffset": 81, "endOffset": 85}, {"referenceID": 45, "context": "We designed our classification models using an open-source implementation of SMO [59] by the Weka machine learning toolkit [47], with its linear kernel for lexical and acoustic features, and its gaussian kernel for the psycholinguistic features.", "startOffset": 123, "endOffset": 127}, {"referenceID": 57, "context": "We have measured the performance of the system using the Un-weighted Average (UA), which has been widely used in the evaluation of paralinguistic tasks [60].", "startOffset": 152, "endOffset": 156}, {"referenceID": 58, "context": "errors as evaluated in similar cases by NIST in diarization tasks [61, 62].", "startOffset": 66, "endOffset": 74}, {"referenceID": 59, "context": "For the statistical significance, we have computed McNemar\u2019s significant test over the test set [63].", "startOffset": 96, "endOffset": 100}, {"referenceID": 47, "context": "Linear combination of lexical with acoustic features has not improved performance, despite its success in other paralinguistic tasks [49], linear combination in the feature space has not improved performance even when combined with feature selection.", "startOffset": 133, "endOffset": 137}, {"referenceID": 60, "context": "2% drop with automated transcriptions [64].", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. The lack of an operational definition of empathy makes it difficult to measure it. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline.", "creator": "LaTeX with hyperref package"}}}