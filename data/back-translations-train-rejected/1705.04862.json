{"id": "1705.04862", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2017", "title": "Efficient Parallel Methods for Deep Reinforcement Learning", "abstract": "We propose a novel framework for efficient parallelization of deep reinforcement learning algorithms, enabling these algorithms to learn from multiple actors on a single machine. The framework is algorithm agnostic and can be applied to on-policy, off-policy, value based and policy gradient based algorithms. Given its inherent parallelism, the framework can be efficiently implemented on a GPU, allowing the usage of powerful models while significantly reducing training time. We demonstrate the effectiveness of our framework by implementing an advantage actor-critic algorithm on a GPU, using on-policy experiences and employing synchronous updates. Our algorithm achieves state-of-the-art performance on the Atari domain after only a few hours of training. Our framework thus opens the door for much faster experimentation on demanding problem domains. Our implementation is open-source and is made public at", "histories": [["v1", "Sat, 13 May 2017 17:39:54 GMT  (2296kb,D)", "https://arxiv.org/abs/1705.04862v1", null], ["v2", "Tue, 16 May 2017 14:30:14 GMT  (2204kb,D)", "http://arxiv.org/abs/1705.04862v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alfredo v clemente", "humberto n castej\\'on", "arjun chandra"], "accepted": false, "id": "1705.04862"}, "pdf": {"name": "1705.04862.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Alfredo V. Clemente", "Humberto N. Castej\u00f3n"], "emails": ["alfredvc@stud.ntnu.no", "humberto.castejon@telenor.com", "arjun.chandra@telenor.com"], "sections": [{"heading": "1 INTRODUCTION AND RELATED WORK", "text": "This year it is more than ever before."}, {"heading": "2 BACKGROUND", "text": "Reinforcement learning algorithms attempt to learn a policy \u03c0 that maps states to actions in order to maximize the expected sum of cumulative rewards Rt = E\u03c0 [\u2211 \u221e k = 0 \u03b3 krt + k] for a discount factor 0 < \u03b3 < 1, where rt is the reward observed in the timeframe t. Current reinforcement learning algorithms represent the learned politics \u03c0 as a neural network, either implicitly with a value function or explicitly as a political function."}, {"heading": "2.1 BATCHING WITH STOCHASTIC GRADIENT DESCENT", "text": "Current amplification learning algorithms heavily use deep neural networks to extract both high-grade characteristics from the observations it obtains and approximates its policy or value functions. Consider the set of input-target pairs S = {(x0, y0), (x1, y1),... (xn, yn) generated by a function f (x). The goal of supervised learning with neural networks is to learn a parameterized function f (x, y0) that best approximates the function f (x), and the performance of f is measured by the empirical loss quantity L (\u03b8) = 1 | S | 0 s (f (xs), ys), where l (f (xs), ys) is called a loss function f, and gives a quantitative measure of how good f is in modeling f."}, {"heading": "2.2 MODEL-FREE VALUE BASED METHODS", "text": "Model-free value-based methods attempt to model the Q function q (st, at) = E\u03c0 [\u2211 \u221e k = 0 \u03b3 krt + k + 1 \u0442 = st, a = at], which delivers the expected return achieved by acting on the Q function and then following the policy. A policy can be extracted from the Q function with \u03c0 (st) = arg maxa \u2032 q (st, a \u2032). DQN (Mnih et al., 2013) learns a function Q (s, a; \u03b8) \u2248 q (s, a), represented as a revolutionary neural network of parameters. Model parameters are updated on the basis of model targets provided by the Bellman equation (s, a) = Es \"[r + \u03b3maxa \u2032 q (s, a) | s, a), a (s, a) (2) to generate the loss function L (\u03b8i) = (rt + i \u2212 max, a \u00b2, a \u00b2, a \u00b2 (a)."}, {"heading": "2.3 POLICY GRADIENT METHODS", "text": "Political gradient methods (Williams, 1992) learn directly a parameterized policy \u03c0 (a | s; \u03b8), which is possible due to the political gradient theorem (Sutton et al., 1999), which provides an unbiased estimate of the gradient with respect to political parameters. Sutton et al. (1999) propose an improvement of the basic political gradient progression by replacing the Q function with the advantage function A\u03c0 (s, a). (q (s, a) \u2212 v (s), where v (s) is the value function replaced by E\u03c0 [p (s) + k + 1 (rt + 1). If the environment function is continuously differentiable, the policy v (l) can be optimized."}, {"heading": "3 PARALLEL FRAMEWORK FOR DEEP REINFORCEMENT LEARNING", "text": "The architecture can be presented implicitly, as in value-based methods, or explicitly, as in political gradient methods. As suggested in Mnih et al. (2016), it is likely that by having several examples of environments in parallel, they will explore different places in the state space at a given time, reducing the correlation of the states encountered and contributing to the stabilization of the training. This approach can be motivated as an online experience repository where experiences from the distribution currently observed in the environment are scanned, rather than sampling consistently from previous experiences.At any time, the Master generates measures for all environmental examples by scanning the current policy. Note that policies can be scanned differently for each environment. A number of workers then apply the respective experiences to the respective environments observed, with the respective measures being efficiently applied to the respective environmental conditions."}, {"heading": "4 PARALLEL ADVANTAGE ACTOR CRITIC", "text": "This algorithm maintains a policy p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "5 EXPERIMENTS", "text": "We tested the performance of PAAC in 12 games of Atari 2600 using the Atari Learning Environment (Bellemare et al., 2013). The agent was developed in Python using TensorFlow (Abadi et al., 2015) and all performance experiments were performed on a computer with a 4-core Intel i7-4790K processor and an Nvidia GTX 980 Ti GPU."}, {"heading": "5.1 EXPERIMENTAL SETUP", "text": "In order to compare the results with other algorithms for the Atari domain, we follow the same pre-processing and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the maximum value per pixel from the last two frames is maintained. The frame is then scaled down from 210 \u00b7 160 pixels and 3 color channels to 84 \u00b7 84 pixels and a single color channel for pixel intensity. (Whenever an environment is restarted, the state is reset to the initial state and between 1 and 30 no-op actions are performed before control is transferred to the agent. The environment is restarted when the final state of the environment is reached. (Algorithm 1 Parallel Advantage Actor-Critic 1: Starting Point Time Counters N = 0 and Network Weights are weighted."}, {"heading": "5.2 RESULTS", "text": "The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al., 2016) and GA3C (Babaeizadeh et al., 2016) are presented in Table 1. After a few hours of training on a single computer, PAAC is able to trump Gorila in 8 games, and A3C FF in 8 games. Of the 9 games used to test GA3C, PAAC matches its performance in 2 of them and surpasses it in the remaining 7. In order to better understand the effect of actors (and batch size) on score, tests were performed with ne."}, {"heading": "6 CONCLUSION", "text": "In this thesis, we have introduced a parallel framework for in-depth reinforcement learning that can be efficiently paralleled on a GPU. The framework is flexible and can be used for on-policy and off-policy as well as value-based and policy-gradient-based algorithms. The implementation of the framework presented can reduce the training time for the Atari 2600 domain to a few hours while it remains state-of-the-art. Improvements in training time will allow the application of these algorithms to more demanding environments and the use of more powerful models."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi", "Mart\u0131n", "Agarwal", "Ashish", "Barham", "Paul", "Brevdo", "Eugene", "Chen", "Zhifeng", "Citro", "Craig", "Corrado", "Greg S", "Davis", "Andy", "Dean", "Jeffrey", "Devin", "Matthieu"], "venue": "Software available from tensorflow. org,", "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "GA3C: GPU-based A3C for Deep Reinforcement Learning", "author": ["M. Babaeizadeh", "I. Frosio", "S. Tyree", "J. Clemons", "J. Kautz"], "venue": null, "citeRegEx": "Babaeizadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Babaeizadeh et al\\.", "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Optimization Methods for Large-Scale Machine Learning", "author": ["L. Bottou", "F.E. Curtis", "J. Nocedal"], "venue": "ArXiv e-prints,", "citeRegEx": "Bottou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2016}, {"title": "Playing Atari with Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["V. Mnih", "A. Puigdom\u00e8nech Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Massively Parallel Methods for Deep Reinforcement Learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": null, "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "On the difficulty of training Recurrent Neural Networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen", "Niu", "Feng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 4, "context": "Additionally, when learning on-policy, the policy affects the distribution of encountered states, which in turn affects the policy, creating a feedback loop that may lead to divergence (Mnih et al., 2013).", "startOffset": 185, "endOffset": 204}, {"referenceID": 4, "context": "One is to store experiences in a large replay memory and employ off-policy RL methods (Mnih et al., 2013).", "startOffset": 86, "endOffset": 105}, {"referenceID": 5, "context": "Another is to execute multiple asynchronous agents in parallel, each interacting with an instance of the environment independently of each other (Mnih et al., 2016).", "startOffset": 145, "endOffset": 164}, {"referenceID": 7, "context": "In the General Reinforcement Learning Architecture (Gorila) (Nair et al., 2015), the DQN (Mnih et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 6, "context": ", 2015), the DQN (Mnih et al., 2015) algorithm is distributed across multiple machines.", "startOffset": 17, "endOffset": 36}, {"referenceID": 5, "context": "The distribution of the learning process is further explored in (Mnih et al., 2016), where multiple actor-learners are executed asynchronously on a single machine.", "startOffset": 64, "endOffset": 83}, {"referenceID": 9, "context": "The actor-learners compute gradients in parallel and update shared parameters asynchronously in a HOGWILD! (Recht et al., 2011) fashion.", "startOffset": 107, "endOffset": 127}, {"referenceID": 5, "context": "The asynchronous advantage actor-critic (A3C) algorithm (Mnih et al., 2016) was able to surpass the state of the art on the Atari domain at the time of publication, while training for 4 days on a single machine with 16 CPU cores.", "startOffset": 56, "endOffset": 75}, {"referenceID": 1, "context": "GA3C (Babaeizadeh et al., 2016) is a GPU implementation of A3C.", "startOffset": 5, "endOffset": 31}, {"referenceID": 3, "context": "Increasing nS\u2032 by a factor of k increases the time needed to calculate\u2207\u03b8L\u0304 by a factor of k\u2032, for k\u2032 \u2264 k, and reduces its variance proportionally to 1 k (Bottou et al., 2016).", "startOffset": 153, "endOffset": 174}, {"referenceID": 3, "context": "However there are some limits on the size of the learning rate, so that in general \u03b1\u2032 \u2264 k\u03b1 (Bottou et al., 2016).", "startOffset": 91, "endOffset": 112}, {"referenceID": 4, "context": "DQN (Mnih et al., 2013) learns a function Q(s, a; \u03b8) \u2248 q(s, a) represented as a convolutional neural network with parameters \u03b8.", "startOffset": 4, "endOffset": 23}, {"referenceID": 10, "context": "This is possible due to the policy gradient theorem (Sutton et al., 1999) \u2207\u03b8L(\u03b8) = Es,a [ q(s, a)\u2207\u03b8 log \u03c0(a|s; \u03b8) ] , (5) which provides an unbiased estimate of the gradient of the return with respect to the policy parameters.", "startOffset": 52, "endOffset": 73}, {"referenceID": 10, "context": "This is possible due to the policy gradient theorem (Sutton et al., 1999) \u2207\u03b8L(\u03b8) = Es,a [ q(s, a)\u2207\u03b8 log \u03c0(a|s; \u03b8) ] , (5) which provides an unbiased estimate of the gradient of the return with respect to the policy parameters. Sutton et al. (1999) propose an improvement upon the basic policy gradient update by replacing the Q function with the advantage function A\u03c0(s, a) = (q(s, a)\u2212 v(s)) where v(s) is the value function given by E\u03c0 [\u2211\u221e k=0 \u03b3 rt+k+1 \u2223\u2223s = st].", "startOffset": 53, "endOffset": 248}, {"referenceID": 4, "context": "\u2207\u03b8L(\u03b8) = Es,a [( q(s, a)\u2212 v(s) ) \u2207\u03b8 log \u03c0(a|s; \u03b8) ] (6) Mnih et al. (2016) learn an estimate V (s; \u03b8v) \u2248 v(s) of the value function, with both V (s; \u03b8v) and \u03c0(a|s; \u03b8) being represented as convolutional neural networks.", "startOffset": 56, "endOffset": 75}, {"referenceID": 4, "context": "As suggested in Mnih et al. (2016), by having multiple environments instances in parallel it is likely that they will be exploring different locations of the state space at any given time, which reduces the correlation of encountered states and helps stabilize training.", "startOffset": 16, "endOffset": 35}, {"referenceID": 10, "context": "The parameters \u03b8 of the policy network (the actor) are optimized via gradient ascent following \u2207\u03b8 log \u03c0(at|st; \u03b8)A(st, at; \u03b8, \u03b8v) + \u03b2\u2207\u03b8H(\u03c0(se,t; \u03b8)) (Sutton et al., 1999), where A(st, at; \u03b8, \u03b8v) = Q(st, at; \u03b8, \u03b8v) \u2212 V (st; \u03b8v) is an estimate of the advantage function, Q(st, at; \u03b8, \u03b8v) = \u2211n\u22121 k=0 \u03b3 rt+k + \u03b3 V (st+n; \u03b8v), with 0 < n \u2264 tmax, is", "startOffset": 149, "endOffset": 170}, {"referenceID": 4, "context": "4 PARALLEL ADVANTAGE ACTOR CRITIC We used the proposed framework to implement a version of the n-step advantage actor-critic algorithm proposed by Mnih et al. (2016). This algorithm maintains a policy \u03c0(at|st; \u03b8) and an estimate V (st; \u03b8v) of the value function, both approximated by deep neural networks.", "startOffset": 147, "endOffset": 166}, {"referenceID": 4, "context": "the n-step return estimation and H(\u03c0(se,t; \u03b8)) is the entropy of the policy \u03c0, which as suggested by Mnih et al. (2016) is added to improve exploration by discouraging premature convergence to suboptimal deterministic policies.", "startOffset": 101, "endOffset": 120}, {"referenceID": 2, "context": "We evaluated the performance of PAAC in 12 games from Atari 2600 using the Atari Learning Environment (Bellemare et al., 2013).", "startOffset": 102, "endOffset": 126}, {"referenceID": 0, "context": "The agent was developed in Python using TensorFlow (Abadi et al., 2015) and all performance experiments were run on a computer with a 4 core Intel i7-4790K processor and an Nvidia GTX 980 Ti GPU.", "startOffset": 51, "endOffset": 71}, {"referenceID": 4, "context": "To compare results with other algorithms for the Atari domain we follow the same pre-processing and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the per-pixel maximum value from the two latest frames is kept.", "startOffset": 123, "endOffset": 142}, {"referenceID": 5, "context": "As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to jointly model the policy and the value functions.", "startOffset": 6, "endOffset": 25}, {"referenceID": 5, "context": "The first variant, referred to as archnips, is the same architecture used by A3C FF (Mnih et al., 2016), which is a modified version of the architecture used in Mnih et al.", "startOffset": 84, "endOffset": 103}, {"referenceID": 8, "context": "Additionally gradient clipping (Pascanu et al., 2012) with a threshold of 40 was used.", "startOffset": 31, "endOffset": 53}, {"referenceID": 4, "context": "As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to jointly model the policy and the value functions. For the policy function, the output is a softmax with one node per action, while for the value function a single linear output node is used. Moreover, to compare the efficiency of PAAC for different model sizes, we implemented two variants of the policy and value convolutional network. The first variant, referred to as archnips, is the same architecture used by A3C FF (Mnih et al., 2016), which is a modified version of the architecture used in Mnih et al. (2013), adapted to an actor-critic algorithm.", "startOffset": 7, "endOffset": 619}, {"referenceID": 4, "context": "As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to jointly model the policy and the value functions. For the policy function, the output is a softmax with one node per action, while for the value function a single linear output node is used. Moreover, to compare the efficiency of PAAC for different model sizes, we implemented two variants of the policy and value convolutional network. The first variant, referred to as archnips, is the same architecture used by A3C FF (Mnih et al., 2016), which is a modified version of the architecture used in Mnih et al. (2013), adapted to an actor-critic algorithm. The second variant, archnature, is an adaptation of the architecture presented in Mnih et al. (2015). The networks were trained with RMSProp.", "startOffset": 7, "endOffset": 759}, {"referenceID": 7, "context": "The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 5, "context": ", 2015), A3C (Mnih et al., 2016) and GA3C (Babaeizadeh et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 1, "context": ", 2016) and GA3C (Babaeizadeh et al., 2016) are presented in Table 1.", "startOffset": 17, "endOffset": 43}, {"referenceID": 3, "context": "Gorila scores taken from Nair et al. (2015), A3C FF scores taken from Mnih et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 3, "context": "(2015), A3C FF scores taken from Mnih et al. (2016) and GA3C scores take from Babaeizadeh et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 1, "context": "(2016) and GA3C scores take from Babaeizadeh et al. (2016). Unavailable results are shown as N/A.", "startOffset": 33, "endOffset": 59}], "year": 2017, "abstractText": "We propose a novel framework for efficient parallelization of deep reinforcement learning algorithms, enabling these algorithms to learn from multiple actors on a single machine. The framework is algorithm agnostic and can be applied to on-policy, off-policy, value based and policy gradient based algorithms. Given its inherent parallelism, the framework can be efficiently implemented on a GPU, allowing the usage of powerful models while significantly reducing training time. We demonstrate the effectiveness of our framework by implementing an advantage actor-critic algorithm on a GPU, using on-policy experiences and employing synchronous updates. Our algorithm achieves stateof-the-art performance on the Atari domain after only a few hours of training. Our framework thus opens the door for much faster experimentation on demanding problem domains. Our implementation is open-source and is made public at https://github.com/alfredvc/paac.", "creator": "LaTeX with hyperref package"}}}