{"id": "1705.05020", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2017", "title": "Discrete-Continuous Splitting for Weakly Supervised Learning", "abstract": "This paper proposes an approach for tackling an abstract formulation of weakly supervised learning, which is posed as a joint optimization problem in the continuous model parameters and discrete label variables. We devise a novel decomposition of the latter into purely discrete and continuous subproblems within the framework of the Alternating Direction Method of Multipliers (ADMM), which allows to efficiently compute a local minimum of the nonconvex objective function. Our approach preserves integrality of the discrete label variables and admits a globally convergent kernel formulation. The resulting method implicitly alternates between a discrete and a continuous variable update, however, it is inherently different from simple alternating optimization (hard EM). In numerous experiments we illustrate that our method can learn a classifier from weak and abstract combinatorial supervision thereby being superior towards hard EM.", "histories": [["v1", "Sun, 14 May 2017 19:32:50 GMT  (345kb,D)", "https://arxiv.org/abs/1705.05020v1", null], ["v2", "Mon, 19 Jun 2017 15:51:08 GMT  (190kb,D)", "http://arxiv.org/abs/1705.05020v2", null], ["v3", "Wed, 16 Aug 2017 15:18:11 GMT  (190kb,D)", "http://arxiv.org/abs/1705.05020v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emanuel laude", "jan-hendrik lange", "frank r schmidt", "bjoern", "res", "daniel cremers"], "accepted": false, "id": "1705.05020"}, "pdf": {"name": "1705.05020.pdf", "metadata": {"source": "CRF", "title": "Discrete-Continuous Splitting for Weakly Supervised Learning", "authors": ["Emanuel Laude", "Jan-Hendrik Lange", "Frank R. Schmidt", "Bjoern Andres", "Daniel Cremers"], "emails": ["emanuel.laude@in.tum.de,", "jlange@mpi-inf.mpg.de,", "f.schmidt@cs.tum.edu,", "andres@mpi-inf.mpg.de,", "cremers@tum.de"], "sections": [{"heading": "1 Introduction", "text": "One of the most important challenges in machine learning is the precise annotation of training data, which has become a typical bottleneck of modern machine learning applications."}, {"heading": "1.1 Related Work", "text": "A typical approach to solving problems in the form (1) is to derive problems in depth (3) and in depth (3)."}, {"heading": "1.2 Contribution", "text": "This paper contains two papers. Firstly, we propose a division of the generally NP-hard weakly monitored learning problem (1) into purely discrete and purely continuous sub-problems; the discrete-continuous division that we propose has three advantages: (a) it is accessible for optimization by ADMM; (b) it guarantees global convergence to a critical point of objective function for classifiers parameterized by a core; and (c) it preserves the integrity of discrete designations (without post-processing or rounding); secondly, we solve challenging cases of problem (1) experimentally using ADMM and demonstrate the following advantages over alternative methods: Compared to alternate optimization by hard EM, which have been shown to be susceptible to poor local minima for classification loss functions in [41], our algorithm is robust against initialization and systematically produces good local optimization results."}, {"heading": "2 Discrete-Continuous Splitting", "text": "The rest of this section is broken down as follows: In Section 2.1, we describe the consensus ADMM framework used for discrete-continuous problem sharing (1) and interpret our step size update as a form of graduated non-convexity; in Section 2.2, we discuss a new discrete continuous proximal mapping that preserves the integrity of discrete variables and is therefore considered a central part of our method. In Section 2.3, we further describe a kernel variant of our formulation that allows a convergence guarantee and scales for a divisible supervisor as supervised training of a kernel SVM."}, {"heading": "2.1 Discrete-Continuous Consensus ADMM", "text": "Most of the time the idea is to write an objective solution. (3) The idea of ADMM is to loosen the linkages Yi = Y and gradually reinforce these constraints so that the \"independent\" solutions Yi + 1 lead to a consensus solution Yi = Y. (3) The idea of ADMM is to gradually strengthen the linkages Yi = Y and so that the \"independent\" solutions Yi + 1 lead to a consensus solution Yi = Y. However, this approach does not always offer discrete solutions. Therefore, a post-processing method is commonly used and the final (rounded) solution does not have to be close to the optimal (relaxed) solution."}, {"heading": "2.2 Discrete-Continuous Proximal Mapping", "text": "To get the final algorithm 1 we call discrete-continuous ADMM (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMM) = 1 \u00b7 W (DC-ADMMM) = 1 \u00b7 W (DC-ADM) = 1 \u00b7 W (DC-ADM) = 1 \u00b7 W (1 = 1 MW) = 1 \u00b7 W (ADM) = 1 (MW) = 1 (MW) = 1 (MW) (1 = 1) (MW) = 1 (MW) = 1 (MW) (1) (MW) = 1 (MW) = 1 (MW) = 1 (1)"}, {"heading": "2.3 A Tractable and Convergent Kernel-Formulation", "text": "Finally, we can consider another ADMM formulation that we find more attractive than the DC-ADMM in two respects: First, there is a convergence guarantee (under mild assumptions) by existing theory. We note that the convergence of ADMM in nonconvex optimization cannot be taken for granted, since it is a tool traditionally used in convex optimization. Repair terization for a convergence guarantee (under mild assumptions). For the kernel setting, we assume that the loss (\u00b7 yi) is given as a function of classified scores (xi) > W, where a possibly infinite dimensional map and the regulator is given as R (W)."}, {"heading": "3 Experiments", "text": "In this section, we provide illustrative experiments to show that our method is capable of learning a model in various poorly monitored scenarios."}, {"heading": "3.1 Proof of Concept", "text": "To prove the concept, we run two small toy experiments with synthetic data from 2D lunar shape distributions (600 samples, 150 per class). Instead of providing exact labels for training, we offer a synthetic complex combinatorial supervision, which is much weaker in the sense that it is highly ambiguous and yet contains complicated limitations. Learning from negative labels and pairwise constraints. In the first experiment, we base each training example on a randomly selected negative label, which I state as the true label of the training example, which only indicates that the training example i does not belong to class y. In addition, we offer algorithms with randomly structured pairs and should-not-link constraints, which indicate that certain pairs of training examples (i, j) belong to the same class (yi = yj) or to different classes (yy = yy)."}, {"heading": "3.2 Handwritten Digits Classification with Weak Supervision.", "text": "As a further experiment, we train a weakly monitored RBF kernel SVM on the popular handwritten MNIST dataset with digits. We limit ourselves to a balanced subset of 10,000 images from the original training set (which consists of 60,000 images from 10 different classes). Instead of using the real class labels from the training set, we randomly append to each training example k {1,.., 8} many negative labels that only refer to classes to which the training example does not belong. In a second setup, we additionally append randomly sampled picturesque must-links and non-links that agree with the basic truth to the algorithm. As a starting point, we use the same SVM model that was trained in a fully monitored manner, resulting in a test error of 3.37%. Figure 3 reports the increase in incorrectly predicted training labels and test errors compared to the base model, depending on the absolute number of labels, the actual number of labels can be increased by only 4%."}, {"heading": "3.3 Comparison with SDP Relaxation.", "text": "Finally, we compare with an SDP relaxation method for weakly monitored multinomial logistical regression of [17] in the task of semi-monitored learning. We consider the standard SSL benchmark [7] for comparison, which is also used by [17]. The benchmark is a collection of several data sets with different attribute dimensions and class numbers. Each data set is provided with 12 columns of l = 10 or l = 100 labeled and N \u2212 l unlabeled samples. While [17] contains entropy before labeling that favors an equal balance distribution, we adjust the supervisor S to limit the solution to a maximum of b = 3 or b = 20 percent of the same equilibrium distribution, respectively. We use a MATLAB implementation provided by the authors. For these experiments, we use the softmax losses and set the regulation parameter Kong = 0.0025, which corresponds in the model."}, {"heading": "4 Conclusion", "text": "In this paper, we have presented a novel algorithm for mixed-integer problems in the context of poorly supervised learning processes that allows discrete-continuous decomposition that, like hard EM, decouples discrete and continuous optimization, allowing complex discrete-continuous models to be solved. However, the approach differs from a discrete-continuous coordinate descend scheme in the following sense: The discrete-continuous sequence occurs within an approximate mapping of a nonsmooth nonconvex function, which conceals the discrete variable for ADMM. In various experiments, we have shown that our method can solve the hen-egg problem of finding good initializations."}, {"heading": "A Proofs", "text": "A.1 Evidence of observation 1 Evidence. Let the supervisor S be separable, i.e. \u03b2 (Y) = \u2211 i Si (yi) and let him be the Lagrange multipliers corresponding to the linear coupling constraints. Let him recognize that fennel-legend conjugate of \u00af (\u00b7, xi) is positive. Then, the Lagrange double problem of (4) asmax {0} Ni = 1 min W, {Wi} Ni = 1 N = 1 N = 1 \u00b2 is positive. (\u00b7, xi) (W) + < proposition of (4) Ni = 1 min W N = 1 < proposition of i = 1 < proposition of i, W > \u2212 max Wi < Ni = 1 N = minimal."}], "references": [{"title": "Fast and provably good seedings for kmeans", "author": ["O. Bachem", "M. Lucic", "S.H. Hassani", "A. Krause"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 55\u201363", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Constrained Clustering: Advances in Algorithms", "author": ["S. Basu", "I. Davidson", "K. Wagstaff"], "venue": "Theory, and Applications. Chapman & Hall/CRC, 1 edition", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained k-means clustering", "author": ["K. Bennett", "P. Bradley", "A. Demiriz"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Linear Network Optimization", "author": ["D.P. Bertsekas"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S.P. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, 3(1):1\u2013122", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A continuation method for semi-supervised svms", "author": ["O. Chapelle", "M. Chi", "A. Zien"], "venue": "W. W. Cohen and A. Moore, editors, Machine Learning, Proceedings of the Twenty-Third International Conference ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "editors", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": "Semi-Supervised Learning. MIT Press, Cambridge, MA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research, 2:265\u2013292", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D.P. Bertsekas"], "venue": "Math. Program., 55:293\u2013318", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Consensus-based distributed support vector machines", "author": ["P.A. Forero", "A. Cano", "G.B. Giannakis"], "venue": "Journal of Machine Learning Research, 11:1663\u20131707", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Chapter ix applications of the method of multipliers to variational inequalities", "author": ["D. Gabay"], "venue": "Studies in mathematics and its applications, 15:299\u2013331", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1983}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, 2(1):17\u201340", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1976}, {"title": "Robust k-means: a theoretical revisit", "author": ["A. Georgogiannis"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2883\u20132891", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Sur l\u2019approximation", "author": ["R. Glowinski", "A. Marroco"], "venue": "par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisation-dualit\u00e9 d\u2019une classe de probl\u00e8mes de dirichlet non lin\u00e9aires. Revue fran\u00e7aise d\u2019automatique, informatique, recherche op\u00e9rationnelle. Analyse num\u00e9rique, 9(2):41\u201376", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1975}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z. Luo", "M. Razaviyayn"], "venue": "SIAM Journal on Optimization, 26(1):337\u2013364", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Consensus-admm for general quadratically constrained quadratic programming", "author": ["K. Huang", "N.D. Sidiropoulos"], "venue": "IEEE Trans. Signal Processing, 64(20):5297\u20135310", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "A convex relaxation for weakly supervised classifiers", "author": ["A. Joulin", "F.R. Bach"], "venue": "Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Variable fixing algorithms for the continuous quadratic knapsack problem", "author": ["K.C. Kiwiel"], "venue": "Journal of Optimization Theory and Applications, 136(3):445\u2013458", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "MRF optimization via dual decomposition: Message-passing revisited", "author": ["N. Komodakis", "N. Paragios", "G. Tziritas"], "venue": "IEEE 11th International Conference on Computer Vision, ICCV 2007, Rio de Janeiro, Brazil, October 14-20, 2007, pages 1\u20138. IEEE Computer Society", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Analysis and optimization of loss functions for multiclass", "author": ["M. Lapin", "M. Hein", "B. Schiele"], "venue": "top-k, and multilabel classification. CoRR, abs/1612.03663", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Global convergence of splitting methods for nonconvex composite optimization", "author": ["G. Li", "T.K. Pong"], "venue": "SIAM Journal on Optimization, 25(4):2434\u20132460", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory, 28(2):129\u2013 136", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1982}, {"title": "A distributed approach for the optimal power-flow problem based on ADMM and sequential convex approximations", "author": ["S. Magn\u00fasson", "P.C. Weeraddana", "C. Fischione"], "venue": "IEEE Trans. Control of Network Systems, 2(3):238\u2013253", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "An augmented lagrangian approach to constrained MAP inference", "author": ["A.F.T. Martins", "M.A.T. Figueiredo", "P.M.Q. Aguiar", "N.A. Smith", "E.P. Xing"], "venue": "L. Getoor and T. Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pages 169\u2013176. Omnipress", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed non-convex admm-based inference in large-scale random fields", "author": ["O. Miksik", "V. Vineet", "P. P\u00e9rez", "P.H.S. Torr"], "venue": "M. F. Valstar, A. P. French, and T. P. Pridmore, editors, British Machine Vision Conference, BMVC 2014, Nottingham, UK, September 1-5, 2014. BMVA Press", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien", "author": ["J.-J. Moreau"], "venue": "Bulletin de la Soci\u00e9t\u00e9 math\u00e9matique de France, 93:273\u2013299", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1965}, {"title": "Nested mini-batch k-means", "author": ["J. Newling", "F. Fleuret"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1352\u20131360", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Variational Analysis", "author": ["R. Rockafellar", "R.-B. Wets"], "venue": "Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "D. P. Helmbold and R. C. Williamson, editors, Computational Learning Theory, 14th Annual Conference on Computational Learning Theory, COLT 2001 and 5th European Conference on Computational Learning Theory, EuroCOLT 2001, Amsterdam, The Netherlands, July 16-19, 2001, Proceedings, volume 2111 of Lecture Notes in Computer Science, pages 416\u2013426. Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K. M\u00fcller"], "venue": "Neural Computation, 10(5):1299\u20131319", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Jump-sparse and sparse recovery using potts functionals", "author": ["M. Storath", "A. Weinmann", "L. Demaret"], "venue": "IEEE Trans. Signal Processing, 62(14):3654\u20133666", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple effective heuristic for embedded mixed-integer quadratic programming", "author": ["R. Takapoui", "N. Moehle", "S. Boyd", "A. Bemporad"], "venue": "International Journal of Control, pages 1\u201323", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, 6:1453\u20131484", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "C. E. Brodley and A. P. Danyluk, editors, Proceedings of the Eighteenth International Conference on Machine Learning ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "Global convergence of ADMM in nonconvex nonsmooth optimization", "author": ["Y. Wang", "W. Yin", "J. Zeng"], "venue": "CoRR, abs/1511.06324", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "A constant-factor bi-criteria approximation guarantee for k-means++", "author": ["D. Wei"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 604\u2013612", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "The effective energy transformation scheme as a special continuation approach to global optimization with application to molecular conformation", "author": ["Z. Wu"], "venue": "SIAM Journal on Optimization, 6(3):748\u2013768", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1996}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pages 1537\u20131544", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "An empirical study of ADMM for nonconvex problems", "author": ["Z. Xu", "S. De", "M.A.T. Figueiredo", "C. Studer", "T. Goldstein"], "venue": "CoRR, abs/1612.03349", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning structural svms with latent variables", "author": ["C.J. Yu", "T. Joachims"], "venue": "A. P. Danyluk, L. Bottou, and M. L. Littman, editors, Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, volume 382 of ACM International Conference Proceeding Series, pages 1169\u20131176. ACM", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum margin clustering made practical", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "Z. Ghahramani, editor, Machine Learning, Proceedings of the Twenty-Fourth International Conference ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Global data association for multi-object tracking using network flows", "author": ["L. Zhang", "Y. Li", "R. Nevatia"], "venue": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient maximum margin clustering via cutting plane algorithm", "author": ["B. Zhao", "F. Wang", "C. Zhang"], "venue": "Proceedings of the SIAM International Conference on Data Mining, SDM 2008, April 24-26, 2008, Atlanta, Georgia, USA, pages 751\u2013762. SIAM", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "Typical choices are, for instance, the Crammer and Singer multi-class SVM loss [8], or the softmax loss, in combination with a linear classifier, in which case W \u2208 Rd\u00d7C .", "startOffset": 79, "endOffset": 82}, {"referenceID": 32, "context": "The loss may also be given as a structured or latent variable SVM loss [33, 40] or it may incorporate a nonlinear classifier such as a neural network.", "startOffset": 71, "endOffset": 79}, {"referenceID": 39, "context": "The loss may also be given as a structured or latent variable SVM loss [33, 40] or it may incorporate a nonlinear classifier such as a neural network.", "startOffset": 71, "endOffset": 79}, {"referenceID": 33, "context": "For a supervisor with only pairwise terms and a quadratic distance loss, the model (1) specializes to constrained k-means clustering [34, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 1, "context": "For a supervisor with only pairwise terms and a quadratic distance loss, the model (1) specializes to constrained k-means clustering [34, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 3, "context": "The supervisor can also be modeled in terms of a min cost flow objective [4] for an a-priori known flow network G with a-priori unknown costs [3, 2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "The supervisor can also be modeled in terms of a min cost flow objective [4] for an a-priori known flow network G with a-priori unknown costs [3, 2].", "startOffset": 142, "endOffset": 148}, {"referenceID": 1, "context": "The supervisor can also be modeled in terms of a min cost flow objective [4] for an a-priori known flow network G with a-priori unknown costs [3, 2].", "startOffset": 142, "endOffset": 148}, {"referenceID": 2, "context": "In [3, 2] this is used to efficiently model a constraint on the labeling Y, that balances the assignment of instances to different classes.", "startOffset": 3, "endOffset": 9}, {"referenceID": 1, "context": "In [3, 2] this is used to efficiently model a constraint on the labeling Y, that balances the assignment of instances to different classes.", "startOffset": 3, "endOffset": 9}, {"referenceID": 41, "context": "This may have applications for instance in multi-object tracking, which is often modeled in terms of a min cost flow problem [42].", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "A typical approach for tackling problems of the form (1) is via a coordinate descent scheme in the discrete and the continuous variables [3, 34, 2].", "startOffset": 137, "endOffset": 147}, {"referenceID": 33, "context": "A typical approach for tackling problems of the form (1) is via a coordinate descent scheme in the discrete and the continuous variables [3, 34, 2].", "startOffset": 137, "endOffset": 147}, {"referenceID": 1, "context": "A typical approach for tackling problems of the form (1) is via a coordinate descent scheme in the discrete and the continuous variables [3, 34, 2].", "startOffset": 137, "endOffset": 147}, {"referenceID": 21, "context": "The latter is commonly tackled by Lloyd\u2019s algorithm [22] that employs alternating optimization to compute a local minimum.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 12, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 35, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 26, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 0, "context": "The efficient computation of provably good seedings is therefore studied in [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 33, "context": "The k-means clustering problem can be extended so as to account for combinatorial soft and hard constraints on the clustering such as the discussed pairwise constraints [34] or a balancing constraint [3, 2].", "startOffset": 169, "endOffset": 173}, {"referenceID": 2, "context": "The k-means clustering problem can be extended so as to account for combinatorial soft and hard constraints on the clustering such as the discussed pairwise constraints [34] or a balancing constraint [3, 2].", "startOffset": 200, "endOffset": 206}, {"referenceID": 1, "context": "The k-means clustering problem can be extended so as to account for combinatorial soft and hard constraints on the clustering such as the discussed pairwise constraints [34] or a balancing constraint [3, 2].", "startOffset": 200, "endOffset": 206}, {"referenceID": 1, "context": "This is commonly referred to as constrained clustering [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 40, "context": "For loss functions that are more common in classification, such as the hinge loss, the alternating optimization scheme breaks down, because it converges early to a poor local optimum [41].", "startOffset": 183, "endOffset": 187}, {"referenceID": 37, "context": "Therefore, specialized algorithms based on SDP relaxations [38, 17] or CCCP [43] have been devised for a", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "Therefore, specialized algorithms based on SDP relaxations [38, 17] or CCCP [43] have been devised for a", "startOffset": 59, "endOffset": 67}, {"referenceID": 42, "context": "Therefore, specialized algorithms based on SDP relaxations [38, 17] or CCCP [43] have been devised for a", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "An SDP relaxation approach for weakly supervised learning in the context of multinomial logistic regression is proposed by [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "While convex SDP relaxation methods are robust regarding initialization, on the downside they generally do not scale well to larger problem instances: The approach by [17] has a per-iteration complexity of O(N).", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "This has two important drawbacks: (i) It is in general very difficult to strictly impose combinatorial constraints on the labeling and (ii) a post-processing (rounding) method is usually employed, which degrades the quality of the final solution compared to the optimal (relaxed) solution [17].", "startOffset": 289, "endOffset": 293}, {"referenceID": 13, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 16, "endOffset": 24}, {"referenceID": 11, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 16, "endOffset": 24}, {"referenceID": 10, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 114, "endOffset": 121}, {"referenceID": 8, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 114, "endOffset": 121}, {"referenceID": 30, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 24, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 22, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 15, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 31, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 15, "context": "In particular, the application of ADMM to quadratic mixed integer programs has recently been studied by [16, 32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 31, "context": "In particular, the application of ADMM to quadratic mixed integer programs has recently been studied by [16, 32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 31, "context": "Therefore, a rounding step may be integrated into the ADMM formulation [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "Whereas ADMM converges under very general assumptions for the convex case, the convergence in a more restrictive nonconvex setting has recently been proven [15, 21, 35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 20, "context": "Whereas ADMM converges under very general assumptions for the convex case, the convergence in a more restrictive nonconvex setting has recently been proven [15, 21, 35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 34, "context": "Whereas ADMM converges under very general assumptions for the convex case, the convergence in a more restrictive nonconvex setting has recently been proven [15, 21, 35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 40, "context": "Secondly, we solve challenging instances of Problem (1) experimentally, by means of ADMM, demonstrating the following advantages over alternative methods: Compared to alternating optimization by hard EM, which have been shown in [41] to be prone to bad local minima for classification loss functions, our algorithm is robust to initialization and systematically produces good local optima.", "startOffset": 229, "endOffset": 233}, {"referenceID": 16, "context": "Compared to the SDP relaxation proposed in [17] whose optimization has worst-case O(N) for a kernelized classifier, our algorithm has worst-case time complexity O(N) for a classifier parameterized by a kernel.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 24, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 22, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 15, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 9, "context": "Further, we note that in case the supervisor is the trivial zero function, our approach specializes to a decomposition that is typically used in supervised learning with ADMM [10, 5].", "startOffset": 175, "endOffset": 182}, {"referenceID": 4, "context": "Further, we note that in case the supervisor is the trivial zero function, our approach specializes to a decomposition that is typically used in supervised learning with ADMM [10, 5].", "startOffset": 175, "endOffset": 182}, {"referenceID": 38, "context": "Next we focus on the choice of the penalty parameter \u03c1, as a carefully chosen parameter may drastically improve the quality of the solutions found by ADMM in nonconvex optimization [39].", "startOffset": 181, "endOffset": 185}, {"referenceID": 23, "context": "In this section we provide an interpretation of this behavior in the context of graduated nonconvexity: In [24] the authors observe that consensus ADMM approaches a projected subgradient descent on the convex, negative Lagrangian dual function, referred to as dual decompostion (DD) [19], if the penalty parameter \u03c1 is close to zero.", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "In this section we provide an interpretation of this behavior in the context of graduated nonconvexity: In [24] the authors observe that consensus ADMM approaches a projected subgradient descent on the convex, negative Lagrangian dual function, referred to as dual decompostion (DD) [19], if the penalty parameter \u03c1 is close to zero.", "startOffset": 283, "endOffset": 287}, {"referenceID": 23, "context": "Whereas in [24] a convex objective is optimized, and therefore the problems solved by DD and ADMM are equivalent, in our case DD solves a convex relaxation that we characterize in the following Observation: Observation 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "In the light of this observation, we may informally interpret a nondecreasing update schedule for \u03c1 as a form of graduated nonconvexity [37].", "startOffset": 136, "endOffset": 140}, {"referenceID": 5, "context": "Continuation methods for graduated nonconvexity have for instance been applied for the optimization of semi-supervised versions of the SVM [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 34, "context": "Global convergence of Algorithm 1 can so far not be guaranteed by existing theory, since L is nonsmooth [35].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "Local convergence of DC-ADMM, may therefore be deduced from existing convergence results for convex objectives [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 25, "context": "semicontinuous function F at some point V are defined according to [26]:", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Distributed optimization is considered one of the main advantages of ADMM in supervised learning [10, 5].", "startOffset": 97, "endOffset": 104}, {"referenceID": 4, "context": "Distributed optimization is considered one of the main advantages of ADMM in supervised learning [10, 5].", "startOffset": 97, "endOffset": 104}, {"referenceID": 7, "context": "For the Crammer and Singer multiclass SVM loss [8] for instance, there exists an efficient variable fixing algorithm [18] for solving the dual.", "startOffset": 47, "endOffset": 50}, {"referenceID": 17, "context": "For the Crammer and Singer multiclass SVM loss [8] for instance, there exists an efficient variable fixing algorithm [18] for solving the dual.", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "Via the Lambert-W function the dual problem for the softmax loss can be reduced to a one-dimensional nonlinear equation [20], that may be solved by Newton\u2019s or Halley\u2019s method.", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "Then, for a fixed labeling Y, the representer theorem [29] states that the parameters W = \u03a6(X)\u03b1 can be substituted via their representation \u03b1 in terms of the matrix features \u03a6(X).", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "However, in practice, ADMM usually converges to modest accuracy\u2013sufficient in machine learning tasks\u2013within a few dozen iterations [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 34, "context": "The proof can be obtained by verifying that the assumptions in [35] are met.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 13, "endOffset": 23}, {"referenceID": 33, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 13, "endOffset": 23}, {"referenceID": 1, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 13, "endOffset": 23}, {"referenceID": 29, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Finally, we compare to an SDP relaxation method for weakly supervised multinomial logistic regression by [17] on the task of semi-supervised learning.", "startOffset": 105, "endOffset": 109}, {"referenceID": 6, "context": "We consider the standard SSL benchmark [7] for a comparison also used by [17].", "startOffset": 39, "endOffset": 42}, {"referenceID": 16, "context": "We consider the standard SSL benchmark [7] for a comparison also used by [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "Whereas [17] incorporates an entropy prior on the labeling which favors an equal balance distribution, we adapt the supervisor S in a way that it restricts the solution to deviate at most b = 3 resp.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "01 in the model by [17].", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "Table 1: Comparison with the method of [17] on the SSL benchmark [7].", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "Table 1: Comparison with the method of [17] on the SSL benchmark [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 34, "context": "[35]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "As K is invertible, assumption A3 in [35] holds trivially.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "The global convergence to a stationary point of the augmented Lagrangian of (14) is a direct consequence of Theorem 1 in [35].", "startOffset": 121, "endOffset": 125}], "year": 2017, "abstractText": "This paper introduces a novel algorithm for a class of weakly supervised learning tasks. The considered tasks are posed as joint optimization problems in the continuous model parameters and the (a-priori unknown) discrete label variables. In contrast to prior approaches such as convex relaxations, we decompose the nonconvex problem into purely discrete and purely continuous subproblems in a way that is amenable to distributed optimization by the Alternating Direction Method of Multipliers (ADMM). This approach preserves integrality of the discrete label variables and, for a reparameterized variant of the algorithm using kernels, guarantees global convergence to a critical point. The resulting method implicitly alternates between a discrete and a continuous variable update, however, it is inherently different from a discrete-continuous coordinate descent scheme (hard EM). In diverse experiments we show that our method can learn a classifier from weak supervision that takes the form of hard and soft constraints on the labeling and outperforms hard EM in this task.", "creator": "LaTeX with hyperref package"}}}