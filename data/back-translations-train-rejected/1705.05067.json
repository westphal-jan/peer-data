{"id": "1705.05067", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Online Learning Via Regularized Frequent Directions", "abstract": "Online Newton step algorithms usually achieve good performance with less training samples than first order methods, but require higher space and time complexity in each iteration. In this paper, we develop a new sketching strategy called regularized frequent direction (RFD) to improve the performance of online Newton algorithms. Unlike the standard frequent direction (FD) which only maintains a sketching matrix, the RFD introduces a regularization term additionally. The regularization provides an adaptive stepsize for update, which makes the algorithm more stable. The RFD also reduces the approximation error of FD with almost the same cost and makes the online learning more robust to hyperparameters. Empirical studies demonstrate that our approach outperforms sate-of-the-art second order online learning algorithms.", "histories": [["v1", "Mon, 15 May 2017 04:18:29 GMT  (314kb,D)", "http://arxiv.org/abs/1705.05067v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luo luo", "cheng chen", "zhihua zhang", "wu-jun li"], "accepted": false, "id": "1705.05067"}, "pdf": {"name": "1705.05067.pdf", "metadata": {"source": "CRF", "title": "Online Learning Via Regularized Frequent Directions", "authors": ["Luo Luo", "Cheng Chen", "Zhihua Zhang", "Wu-Jun Li"], "emails": ["zhzhang@math.pku.edu.cn", "liwujun@nju.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that it is a purely mental game, in which it is a matter of putting people in a position to put themselves in, in a position to put themselves in, in which they are, in which they are not, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, they, in which they, in which they, in which they, they, in which they, in which they, in which they, in which they, in which they, they, in which they, they, in which they, in which they, in which they,"}, {"heading": "2. Online Learning by Sketching", "text": "In this section, we first describe the structure of convex online learning and some classic algorithms. We then present the link between online learning and the sketching of second-order methods."}, {"heading": "2.1 Convex Online Learning", "text": "For a sequence of examples (x (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), t (t), t (t), t (t, t, t (t), t (t), t (t), t (t), t (t, t), (t), (t), (t, (t), (t), (t), (t), (t, (t), (t), (t), (t, (t), (t), (t), (t, (t), (t), (t), (t (t), (t), (t), (t (t), (t), (t (t), (t), (t (t), (t), (t (t), (t), (t), (t (t), (t), (t), (t (t), (t), (t (t), (t), (t), (t (t), (t), (t), (t (t), (t), (t), (t (t), (t), (t), (t, (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t"}, {"heading": "2.2 Efficient Algorithms by Sketching", "text": "In order to make the Newton step scalable, it is of course necessary to use sketching techniques [5]. Since the matrix H (t) in online learning has the form H (t) = \u03b10I (\u2212 \u2212 t) > A (t), where A (t) Rt \u00b7 d is the corresponding term in (2) such asA (t) = [g (1),. \u2212 g (\u2212 t) >, or A (t) = [\u221a \u00b51 + \u03b71g (1).,.,.,..,...,...,....,.....,..,.....,.....,.....,......,...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3. Regularized Frequent Directions", "text": "In order to make the matrix H (t) reversible and well conditioned, both the standard online newton and the outlined online newton methods usually require a fixed regularization term \u03b10I. The hyperparameter \u03b10 typically has to be set manually. On the other hand, conventional sketching algorithms, such as Frequent Direction, only take into account the gradient portion (A (t) > A (t) in H (t) and do not cover the regularization term. Intuitively, it is better to increase the factor of the identity matrix during the iteration, because there are more and more rows of A (t). These motivate us to propose a new sketching algorithm that is better suited for online learning."}, {"heading": "3.1 The Algorithm", "text": "The regularized frequent direction (RFD) is a variant of the frequent direction. For a given matrix A-RT-D, whose rows are sequential and fixed scalar, RFD calculates the sketch matrix B-Rm-d and \u03b1-R (m d) up to approximately \u03b10I + A-A by \u03b1I + B > B.Algorithm 2 Fast Frequent Directions 1: Input: A = [a (1),.., a (T) > RT-d, B (0) = 02m \u00b7 d2: for t = 1,.., T-3: Input (a (t) > into a zero value line of B (t \u2212 1) 4: if B (t \u2212 1) does not have a zero value line."}, {"heading": "3.2 Theoretical Analysis", "text": "We now explain why the regularization concept is updated by the rule of algorithm 3 and provide the approximation error limit of RFD (= 1). First, we give the following theory about the matrix approximation. \u2212 \u2212 Theorem 1 \u2212 \u2212 \u2212 Setpoint of M + Rd \u00b7 d and a positive integer k < d, let M = UD > the matrix of M. Let Uk denotes the matrix of the first k columns of U and \u03c3k is the uppermost k-th singular value of M. Then the pair (C + D), defined asC = Uk \u2212 3, is defined asC (A). \u2212 Should you define the matrix of the first k columns of U and \u03c3d (2 + D) / 2Algorithm is the uppermost k singular number of M. Then the pair (C + D), defined asC = Uk \u2212 3, is defined asC (A)."}, {"heading": "4. The Online Newton Step by RFD", "text": "The procedure is described in algorithm 5, which is similar to the outlined Online Newton Step (SON) [4], but with the new sketching method RFD. Algorithm 5 RFD for Online Newton Step 1: Input: \u03b1 (0) = \u03b10, m < d = O (1 / t) and B (0) = 0m \u00b7 d. 2: for t = 1,., T do3: Receivable example x (t), and loss function ft (w) 4: g (t) = p (t) ft (w)))) 5: Insert (n) and B (t) > in the m-th series of B (t \u2212 1)., T do3: Receivable example x (t), and loss function ft (w) 4: g (t) = p (t) ft (w)))."}, {"heading": "5. Experiments", "text": "In this section, we evaluate the performance of the Regular Frequent Instructions (RFD) and Online Newton Step by Step RFD (RFD-SON) on three real datasets, \"a9a,\" \"gisette\" and \"sido0,\" the details of which are listed in Table 1. \"sido0\" datasets are from Causality Workbench1 and the others can be downloaded from LIBSVM Repository2. Experiments are conducted in the Matlab and run on a server running Intel (R) Core (TM) i7-3770 CPU 3.40GHz \u00d7 2, 8GB RAM and Windows Server 2012 64-bit system."}, {"heading": "5.1 Matrix Approximation", "text": "First, we compare the approximation error of FD and RFD. For a given dataset, we use FD (algorithm 2) and RFD (algorithm 4) to approximate the covariance matrix A > A by B > B and \u03b1I + B > B, respectively; that is, the regulation term \u03b10 is not important for this assessment because it does not change the absolute error in the counter of error FD or error RFD. We report the relative spectral standard error by varying the sketch size m. Figure 1 shows the performance of the two algorithms. The relative error of FD is always lower than that of FD and the decrease is almost half of the comparison results."}, {"heading": "5.2 Online Learning", "text": "We evaluate the performance of RFD-SON. We use the least square loss (that is, ft (w) = (w > x) (t) \u2212 y (t) 2), and set Kt = \u2212 \u2212 w: | w > x (t) | \u2264 1}. In the experiments, we use the doubling space strategy for RFD sketches, which is in algorithm 6.Through the Woodbury formula, the parameter w (t) in O (md) can cost as a consequence (t + 1) = w (t) \u2212 1 \u03b1 (t) \u2212 1 \u03b1 (t) \u2212 1 \u03b1 (t) \u2212 datching (B (t) > H (t) > H (t) b)))), w (t + 1) = u (t + 1) \u2212 S (t) cost as a consequence (t) \u2212 1 \u03b1 (t) --1 \u03b1 (t) --H (t (t (t) \u2212 datching (t) \u2212 datching (t) > H (t) > H (t) > H (t) > H (t) > H (t) > H) > H (t) (t) > H (t) > H (t)."}, {"heading": "6. Conclusions", "text": "We have proposed a novel sketching method that regulates frequent directions (RFD) and has several nice properties. Both the theoretical analysis and the experiments show that RFD is much better than FD. The online learning algorithm with RFD achieves better accuracy than baselines and is more robust against the regulation parameters. The application of RFD is not limited to convex online optimization. We will try to use RFD to improve other optimization algorithms, including stochastic or non-convex cases."}, {"heading": "Appendix A: The Proof of Theorem 1", "text": "In this section, we first provide a series of words from the book \"Topics in Matrix Analysis\" (22), then we prove that theorems 1 and 2 can be found in the book and that we can prove the quality of Lemma 3. Lemma 1 (Theorem 3.4.5 of [22] Let A, B, Rm \u00b7 n be given, and assume that A, B and A \u2212 B have decreasing individual values, where q = min, n}. Define si (A); Lemma (A); 1 (B); Harry (B); and Harry (B)."}, {"heading": "Appendix B: The Proof of Theorem 2", "text": "Proof The algorithm 3 implies the singular values of \u03b1 (t \u2212 1) I + (B (t \u2212 1)) > B (t \u2212 1) are (\u03c3 (t \u2212 1) 1) 2 + \u03b1 (t \u2212 1) \u2265 \u00b7 \u00b7 \u2265 (\u03c3 (t \u2212 1) m) 2 + \u03b1 (t \u2212 1) \u2265 \u03b1 (t \u2212 1) = \u00b7 \u00b7 = \u03b1 (t \u2212 1).Then we can use theorem 1 by takingM = \u03b1 (t \u2212 1) I + (B (t \u2212 1))) > B (t \u2212 1)) > B (t \u2212 1), C = (V (t \u2212 1) > \u221a (\u03a3 (t \u2212 1) 2 \u2212 (\u03c3 (t \u2212 1) m) 2I = (B (t (t))) > I = (B (t (t (t) >, \u03b4 = [t \u2212 1) m) 2 + \u03b1 (t \u2212 1) + \u03b1 (t \u2212 1) / 2 = \u03b1 (t \u2212 1).Due to the last column of B () > (t) and B (t) (t) (t)."}, {"heading": "Appendix C: The Proof of Theorem 3", "text": "Proof In this proof, the B (t) \u2212 notation refers to the matrix after line 5 of algorithm 3 in the t-th iteration, and B (t) is the same for (t \u2212 1) th iteration (without inserting a (t)); in other words, we have V (t \u2212 1) (t \u2212 1) 2V (t \u2212 1) = (B (t \u2212 1)) > B (t \u2212 1) + a (t) (t) (t) (t) (t) (a (t)))))). Then we can earn the lower limit as follows: A > A \u2212 (B \u2212 1) (b \u2212 1). The lower limit after the first (t) > B (t \u2212 1) > B (t \u2212 1) > B (t) is the lower limit after the second (t)."}, {"heading": "Appendix D: The Proof of Theorem 4", "text": "Evidence We can compare \u03ba (MRFD) and \u0445 (MFD) by the fact that \u03b1 \u2265 \u03b10 can be derived as the following (MRFD) = \u03c3max (B > B) + \u03b1\u03b1 \u2264 \u03c3max (B > B) + \u03b10 \u03b10 = \u0445 (MFD). The other inequality can be derived as \u0445 (MRFD) = \u03c3max (B > B) + \u03b1\u03b1 \u2264 \u03c3max (A > A) + \u03b1\u03b1 \u2264 \u03c3max (A > A) + \u03b10 = \u0445 (M), whence the first inequality originates (3) and the others are easy to obtain."}, {"heading": "Appendix E: The Proof of Theorem 5", "text": "The proof that V (t) + \u2212 \u2212 \u2212 \u2212 T (t) \u2212 H (t) \u2212 H (t) \u2212 H (t) \u2212 H (t) \u2212 T (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t (t) (t) (t) (t) (t) (t (t) (t) (t) (t) (t (t) (t) (t) (t (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t) (t (t) (t) (t) (t (t) (t) (t) (t (t) (t) (t (t) (t) (t) (t (t) (t) (t (t) (t) (t (t) (t (t) (t) (t (t) (t (t) (t) (t) (t (t) (t) (t (t) (t (t) (t) (t (t) (t) (t) (t (t) (t) (t) (t) (t) (t) (t (t) (t) (t) (t) (t) (t) (t (t) (t) (t) (t) (t) (t (t) (t) (t (t) (t (t (t) (t) (t) (t) (t) (t (t (t) (t (t) (t) (t"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Online Newton step algorithms usually achieve good performance with less training samples than<lb>first order methods, but require higher space and time complexity in each iteration. In this paper,<lb>we develop a new sketching strategy called regularized frequent direction (RFD) to improve the<lb>performance of online Newton algorithms. Unlike the standard frequent direction (FD) which<lb>only maintains a sketching matrix, the RFD introduces a regularization term additionally. The<lb>regularization provides an adaptive stepsize for update, which makes the algorithm more stable.<lb>The RFD also reduces the approximation error of FD with almost the same cost and makes the<lb>online learning more robust to hyperparameters. Empirical studies demonstrate that our approach<lb>outperforms sate-of-the-art second order online learning algorithms.", "creator": "LaTeX with hyperref package"}}}