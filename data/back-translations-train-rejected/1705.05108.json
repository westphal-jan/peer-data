{"id": "1705.05108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Kernel Truncated Regression Representation for Robust Subspace Clustering", "abstract": "Subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering methods assume that the data could be linearly represented with each other in the input space. In practice, however, this assumption is hard to be satisfied. To achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps: 1) projecting the data into a hidden space in which the data can be linearly reconstructed from each other; 2) calculating the globally linear reconstruction coefficients in the kernel space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality, and then achieving clustering by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and capacity of clustering data points that lie in nonlinear subspaces. The first advantage makes our method efficient in handling large-scale data sets, and the second one enables the proposed method to address the nonlinear subspace clustering challenge. Extensive experiments on five real-world datasets demonstrate the effectiveness and the efficiency of the proposed method in comparison with ten state-of-the-art approaches regarding four evaluation metrics.", "histories": [["v1", "Mon, 15 May 2017 08:16:34 GMT  (1414kb)", "https://arxiv.org/abs/1705.05108v1", null], ["v2", "Tue, 23 May 2017 15:33:56 GMT  (1414kb)", "http://arxiv.org/abs/1705.05108v2", "12 pages"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["liangli zhen", "dezhong peng", "xin yao"], "accepted": false, "id": "1705.05108"}, "pdf": {"name": "1705.05108.pdf", "metadata": {"source": "CRF", "title": "Kernel Truncated Regression Representation for Robust Subspace Clustering", "authors": ["Liangli Zhen", "Dezhong Peng", "Xin Yao"], "emails": ["llzhen@outlook.com).", "(pengdz@scu.edu.cn).", "x.yao@cs.bham.ac.uk)."], "sections": [{"heading": null, "text": "In recent years, we have achieved the approaches of spatial clustering. [cs.C V] 23 May 201 7Index terms - kernel abbreviated regression; nonlinear clustering; spectral clustering; nuclear techniques. [TRO DUCTIONSubspace clustering is one of the most popular methods of data analysis, which has attracted increasing interests from numerous fields such as computer vision, image analysis, and signal processing. [1] By adopting high-dimensional data that lie in a union of low-dimensional subspaces, clustering aims to aim at a series of subspaces to adapt a particular data collection and cluster formation, based on the identified subspaces that have been proposed in recent decades many subspace cluster methods that can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5-7] and 4) spectral cluster approaches [20]."}, {"heading": "A. Kernel Truncated Regression Representation", "text": "For a given dataset {x2} n = 1, whereupon approx. R m, we define a matrix X = x1, x2,.. xn]. Leave you?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "B. KTRR for Robust Subspace Clustering", "text": "In this section, we present the method for achieving clustering in subspace by including the KTRR in the spectral cluster system = 2. For a given dataset X, which consists of n data points in R m, we assume that these points should be in a union of L low-dimensional nonlinear subspaces. We propose to project the data points into a different space, in which the mapped points can be represented linearly by these mapped points from the intrasuspace. From (11), we find that the representation coefficients do not require the projection function in explicit form, but are only needed in dot products. We can cause a core function to calculate these dot products, and obtain the representation coefficients above (13). Furthermore, the existence of the errors in the input dataset leads to some error connections between the data points from different subspaces. We propose to remove these errors by a hard swelling of the matrix."}, {"heading": "C. Computational Complexity Analysis", "text": "It takes O (n3) to get the matrixU, and O (mn2) to calculate all the solutions in (13) with the matrices U and K. Finally, it requires O (n) to find the largest coefficients in each column of matrix C. Putting these steps together, we get the computational complexity of KTRR as O (mn2 + n3). This computational complexity is the same as that of TRR, and isAlgorithm 1 Learning Kernel truncated regression representation for robust subspace clustering."}, {"heading": "A. Databases", "text": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35]. We specify the details of these databases as follows: \u2022 The ExYaleB database contains 2414 frontal facial images of 38 subjects and approximately 64 near-frontal images under different illumination per person, each image being manually cropped and normalized to the size of 32 x 32 pixels [36]. \u2022 The COIL 20 and COIL 100 databases each contain 20 and100 objects. The images of each object were separated by 5 degrees, as the object is rotated on a turntable and each object has 72 images. The size of each image is 32 x 32 pixels, with 256 grayscales per pixel. \u2022 The USPS handwritten digital database contains 1 classes (1)."}, {"heading": "B. Baselines and Evaluation Metrics", "text": "We compare KTRR2 with 10 state-of-art subspace clustering algorithms including abbreviated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], latent low-rank representation (LatLRR) [25], low-rank representation (LRRR1) with 2-norm [12], low-rank representation (LRR2) with 21-norm [12], sparse subspace clustering (SSC) [11], low-range representation (LRRRR1) and embedding (SMCE), local subspace analysis (LSA), and standard spectral clustering (SC) [8].For a fair comparison, we use spectral clustering framework [8] with different similarity matrices."}, {"heading": "D. Clustering on Clean Images", "text": "In this experiment, the KRK method is compared with the other ten states in the world, in which the KRK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK-RK"}, {"heading": "E. Clustering on Corrupted Images", "text": "In fact, most of them are able to survive themselves without there being a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, a process, in which there is a process, in which there is a process, a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is"}, {"heading": "F. Computational Time", "text": "Do you look at the results of the last few years?"}, {"heading": "G. Clustering Performance with Varying Number of Subjects", "text": "In this subsection, we examine the cluster performance of the proposed method with a different number of subjects in the COIL 100 image database. Experiments are carried out with the first t-classes of the database, with t increasing from 10 to 100 with an interval of 10. The cluster results are in Fig. 6.The results show that: \u2022 In general, cluster performance increases with the number of subjects, as the cluster difficulty increases with the number of subjects. \u2022 As the number of subjects increases, the NMI of the KTRR changes slightly from 100% to 90%. One possible reason for this is that the NMI is robust against data distribution (increasing number of subjects) [19]. \u2022 The proposed method achieves satisfactory performance in the COIL 100 database. It achieves a perfect cluster result for t = 10 and achieves satisfactory performance at t = 100 with accuracy, NMI, ARI and Fscore by 74%, 90%, 68% and 68%, respectively."}, {"heading": "H. Parameter Analysis", "text": "A larger \u03bb is suitable for highly corrupt databases and \u03b7 corresponds to the dimensionality of the corresponding subspace for the mapped data points. In order to evaluate the effects of \u03bb and k, we conduct the experiment on the databases ExYaleB and COIL20. We use the size of 10 \u2212 5 to 102 and from 1 to 50, the results are presented in Fig. 7 and Fig. 8. The following observations result from the results: \u2022 The proposed method achieves the best clustering results with the databases ExYaleB and \u03b7 with 0.1 and 5 on the database ExYaleB and 10 and 4 on the database COIL20. \u2022 The proposed method can achieve a satisfactory performance with the databases from 0.1 to 1 on the database ExYaleB, where the thresholds, NMI, ARI and FaleB have better values than TRIL20 and 75% on the database NIR, respectively, and 100% on the database NIR and NIB, whereby the accuracy is 80% and less helpful."}, {"heading": "I. Different Kernel Functions", "text": "The most commonly used core functions are polynomial cores, radial base functions and sigmoid intelligence results from the database. To investigate the performance of the proposed method using different cores, we examine six different core functions. The results on10USPS and MINIST databases are shown in Table VIII, from which we can derive the following observations: \u2022 The core function is determined by the MNIST database. \u2022 The kernel function is executed by the MNIST database. \u2022 The kernel function is executed by the MNIST database. \u2022 The kernel function is executed by the MNIST database. \u2022 The kernel function is executed by the MNIST database. (xi, xj) xxxj T database. = (x T i xj) 2 on the USPS database, which is different from the MNIST database. It is mainly caused by the fact that the images are from the USPS database."}], "references": [{"title": "Subspace clustering for high dimensional data: a review", "author": ["L. Parsons", "E. Haque", "H. Liu"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 6, no. 1, pp. 90\u2013105, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "k-plane clustering", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "Journal of Global Optimization, vol. 16, no. 1, pp. 23\u201332, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories", "author": ["S.R. Rao", "R. Tron", "R. Vidal", "Y. Ma"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Segmentation of multivariate mixed data via lossy coding and compression", "author": ["H. Derksen", "Y. Ma", "W. Hong", "J. Wright"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 9, p. 15461562, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "A multibody factorization method for independently moving objects", "author": ["J.P. Costeira", "T. Kanade"], "venue": "International Journal of Computer Vision, vol. 29, no. 3, pp. 159\u2013179, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Multibody grouping from motion images", "author": ["C.W. Gear"], "venue": "International Journal of Computer Vision, vol. 29, no. 2, pp. 133\u2013 150, 1998.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Generalized principal component analysis", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 12, pp. 1945\u20131959, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1945}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems, vol. 14, 2002, Conference Proceedings, pp. 849\u2013856.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning with l-graph for image analysis", "author": ["B. Cheng", "J. Yang", "S. Yan", "Y. Fu", "T.S. Huang"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 4, pp. 858\u2013866, April 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2765\u2013 2781, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 171\u2013184, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust and efficient subspace segmentation via least squares regression", "author": ["C.Y. Lu", "H. Min", "Z.Q. Zhao", "L. Zhu", "D.S. Huang", "S.C. Yan"], "venue": "Proc. of 12th Eur. Conf. Comput. Vis., Florence, Italy, Oct. 2012, pp. 347\u2013360.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Correntropy induced l2 graph for robust subspace clustering", "author": ["C. Lu", "J. Tang", "M. Lin", "L. Lin", "S. Yan", "Z. Lin"], "venue": "2013 IEEE International Conference on Computer Vision, Dec 2013, pp. 1801\u20131808.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel sparse subspace clustering", "author": ["V.M. Patel", "R. Vidal"], "venue": "Image Processing (ICIP), 2014 IEEE International Conference on. IEEE, 2014, Conference Proceedings, pp. 2849\u20132853.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Locally linear representation for image clustering", "author": ["L. Zhen", "Z. Yi", "X. Peng", "D. Peng"], "venue": "Electronics Letters, vol. 50, no. 13, pp. 942\u2013943, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust subspace clustering via thresholding ridge regression", "author": ["X. Peng", "Z. Yi", "H. Tang"], "venue": "The Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, Conference Proceedings, pp. 3827\u20133833.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral ensemble clustering", "author": ["H. Liu", "T. Liu", "J. Wu", "D. Tao", "Y. Fu"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 715\u2013724.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified framework for representation-based subspace clustering of outof-sample and large-scale data", "author": ["X. Peng", "H. Tang", "L. Zhang", "Z. Yi", "S. Xiao"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 27, no. 12, pp. 2499\u20132512, Dec 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Connections between nuclear norm and frobenius norm based representation", "author": ["X. Peng", "C. Lu", "Y. Zhang", "H. Tang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1\u20137, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2009, Conference Proceedings, pp. 2790\u20132797.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories", "author": ["S.R. Rao", "R. Tron", "R. Vidal", "Y. Ma"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph-based consensus clustering for class discovery from gene expression data", "author": ["Z. Yu", "H.-S. Wong", "H. Wang"], "venue": "Bioinformatics, vol. 23, no. 21, pp. 2888\u20132896, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust kernel lowrank representation", "author": ["S. Xiao", "M. Tan", "D. Xu", "Z.Y. Dong"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent low-rank representation for subspace segmentation and feature extraction", "author": ["G. Liu", "S. Yan"], "venue": "2011 International Conference on Computer Vision, Nov 2011, pp. 1615\u20131622.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse manifold clustering and embedding", "author": ["E. Elhamifar", "R. Vidal"], "venue": "Advances in Neural Information Processing Systems, 2011, Conference Proceedings, pp. 55\u201363.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep subspace clustering with sparsity prior", "author": ["X. Peng", "S. Xiao", "J. Feng", "W. Yau", "Z. Yi"], "venue": "Proceedings of the 25 International Joint Conference on Artificial Intelligence, New York, NY, USA, 9-15 July 2016, pp. 1925\u20131931. [Online]. Available: http://www.ijcai.org/Abstract/16/275", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Cascade subspace clustering", "author": ["X. Peng", "J. Feng", "J. Lu", "W.-Y. Yau", "Z. Yi"], "venue": "Proceedings of the 31th AAAI Conference on Artificial Intelligence. SFO, USA: AAAI, Feb. 2017, pp. 2478\u20132484.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge university press,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Kernel sparse subspace clustering", "author": ["V.M. Patel", "R. Vidal"], "venue": "2014 IEEE International Conference on Image Processing (ICIP). IEEE, 2014, pp. 2849\u20132853.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.-C. Lee", "J. Ho", "D.J. Kriegman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 5, pp. 684\u2013698, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Columbia object image library (coil-20)", "author": ["S.K.N.S.A. Nene", "H. Murase"], "venue": "Report, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Columbia object image library (coil-100)", "author": ["\u2014\u2014"], "venue": "Report, 1996.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 16, no. 5, pp. 550\u2013554, 1994.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "Pat-  12 tern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 8, pp. 1548\u20131560, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate", "author": ["J. Yan", "M. Pollefeys"], "venue": "European conference on computer vision. Springer, 2006, pp. 94\u2013106.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality preserving clustering for image database", "author": ["X. Zheng", "D. Cai", "X. He", "W.-Y. Ma", "X. Lin"], "venue": "Proceedings of the 12th annual ACM international conference on Multimedia. ACM, Conference Proceedings, pp. 885\u2013891.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 0}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification, vol. 2, no. 1, pp. 193\u2013218, 1985.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1985}, {"title": "A probabilistic interpretation of precision, recall and F-score, with implication for evaluation", "author": ["C. Goutte", "E. Gaussier"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Subspace clustering is one of the most popular techniques for data analysis, which has attracted increasing interests from numerous areas, such as computer vision, image analysis, and signal processing [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 1, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 183, "endOffset": 189}, {"referenceID": 3, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 183, "endOffset": 189}, {"referenceID": 4, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 215, "endOffset": 220}, {"referenceID": 5, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 215, "endOffset": 220}, {"referenceID": 6, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 215, "endOffset": 220}, {"referenceID": 7, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 8, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 9, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 10, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 11, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 12, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 13, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 14, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 15, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 16, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 17, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 18, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 19, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 10, "context": "For example, sparse subspace clustering (SSC) [11] assumes that each data point can be linearly represented by a few of other points.", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "Low-rank representation (LRR) [12] encourages the coefficient matrix to be low rank, such that it can capture the global structures of data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Different from SSC and LRR, truncated regression representation (TRR) [17, 19] takes Frobenius norm instead of l1- and nuclear-norm, which has shown promising performance in many real-world applications.", "startOffset": 70, "endOffset": 78}, {"referenceID": 18, "context": "Different from SSC and LRR, truncated regression representation (TRR) [17, 19] takes Frobenius norm instead of l1- and nuclear-norm, which has shown promising performance in many real-world applications.", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": "Like most existing subspace clustering algorithms [11\u2013 13, 21], the major disadvantage of TRR is that it may not give a satisfactory clustering result when data points cannot be linearly represented with each other.", "startOffset": 50, "endOffset": 62}, {"referenceID": 20, "context": "Like most existing subspace clustering algorithms [11\u2013 13, 21], the major disadvantage of TRR is that it may not give a satisfactory clustering result when data points cannot be linearly represented with each other.", "startOffset": 50, "endOffset": 62}, {"referenceID": 20, "context": "During past decades, some spectral clustering-based methods have been proposed to achieve subspace clustering in many applications such as image clustering [21], motion segmentation [22], and gene expression analysis [23].", "startOffset": 156, "endOffset": 160}, {"referenceID": 21, "context": "During past decades, some spectral clustering-based methods have been proposed to achieve subspace clustering in many applications such as image clustering [21], motion segmentation [22], and gene expression analysis [23].", "startOffset": 182, "endOffset": 186}, {"referenceID": 22, "context": "During past decades, some spectral clustering-based methods have been proposed to achieve subspace clustering in many applications such as image clustering [21], motion segmentation [22], and gene expression analysis [23].", "startOffset": 217, "endOffset": 221}, {"referenceID": 15, "context": ", pairwise distance-based strategy and linear representationbased strategy [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 11, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 12, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 14, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 18, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 25, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 10, "context": "For example, SSC [11] enforces the sparsity on C by adopting l1-norm via R(C) = \u2016C\u20161, LRR [12] obtains low rankness by using nuclear norm with R(C) = \u2016C\u2016\u2217.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "For example, SSC [11] enforces the sparsity on C by adopting l1-norm via R(C) = \u2016C\u20161, LRR [12] obtains low rankness by using nuclear norm with R(C) = \u2016C\u2016\u2217.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "To address this challenging issue, some recent works have been proposed [15, 24], however, the methods have the following two disadvantages: 1) the methods are computationally inefficient since they involve solving l1- or nuclear-norm minimization problem; 2) Like SSC and LRR, the methods", "startOffset": 72, "endOffset": 80}, {"referenceID": 23, "context": "To address this challenging issue, some recent works have been proposed [15, 24], however, the methods have the following two disadvantages: 1) the methods are computationally inefficient since they involve solving l1- or nuclear-norm minimization problem; 2) Like SSC and LRR, the methods", "startOffset": 72, "endOffset": 80}, {"referenceID": 26, "context": "recently proposed to achieve nonlinearity with deep structures [27, 28], which are first works to leverage deep learning and subspace clustering.", "startOffset": 63, "endOffset": 71}, {"referenceID": 27, "context": "recently proposed to achieve nonlinearity with deep structures [27, 28], which are first works to leverage deep learning and subspace clustering.", "startOffset": 63, "endOffset": 71}, {"referenceID": 14, "context": "After mapping X into a kernel space, the corresponding {\u03c6(xi)} n i=1 is generally believed lying in linear subspaces [15, 24].", "startOffset": 117, "endOffset": 125}, {"referenceID": 23, "context": "After mapping X into a kernel space, the corresponding {\u03c6(xi)} n i=1 is generally believed lying in linear subspaces [15, 24].", "startOffset": 117, "endOffset": 125}, {"referenceID": 28, "context": "For some choices of a kernel \u03ba(xi,xj): R \u00d7R \u2192 R, [29] has shown that \u03ba can get the dot product in the kernel space H induced by the mapping \u03c6.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "It has been proved that, under certain condition, the coefficients over intra-subspace data points are larger than those over inter-subspace data points [17].", "startOffset": 153, "endOffset": 157}, {"referenceID": 7, "context": "In this section, we present the method to achieve subspace clustering by incorporating the KTRR into spectral clustering framework [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Then, we compute the normalized Laplacian matrix [8]", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "2 1 [9], where 1 = [1, 1, .", "startOffset": 4, "endOffset": 7}, {"referenceID": 29, "context": "considerably less than that of KSSC (O(mn + tn))[30], KLRR(O(t(rX + r)n )[24], where t denotes the total number of iterations for the corresponding algorithm, rX is the rank of X, and r is the rank for partial SVD at each iteration of KLRR.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "considerably less than that of KSSC (O(mn + tn))[30], KLRR(O(t(rX + r)n )[24], where t denotes the total number of iterations for the corresponding algorithm, rX is the rank of X, and r is the rank for partial SVD at each iteration of KLRR.", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 31, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 149, "endOffset": 153}, {"referenceID": 32, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 196, "endOffset": 200}, {"referenceID": 33, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 223, "endOffset": 227}, {"referenceID": 35, "context": "\u2022 The ExYaleB database contains 2414 frontal face images of 38 subjects and around 64 near frontal images under different illuminations per individual, where each image is manually cropped and normalized to the size of 32\u00d732 pixels [36].", "startOffset": 232, "endOffset": 236}, {"referenceID": 35, "context": "The size of each image is 32\u00d7 32 pixels, with 256 grey levels per pixel [36].", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "In the experiment, we select 200 samples of each subject from the database randomly by following the strategy in [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 164, "endOffset": 168}, {"referenceID": 29, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 211, "endOffset": 215}, {"referenceID": 24, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 256, "endOffset": 260}, {"referenceID": 11, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 306, "endOffset": 310}, {"referenceID": 11, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 357, "endOffset": 361}, {"referenceID": 10, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 396, "endOffset": 400}, {"referenceID": 25, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 450, "endOffset": 454}, {"referenceID": 36, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 486, "endOffset": 490}, {"referenceID": 7, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 530, "endOffset": 533}, {"referenceID": 7, "context": "For a fair comparison, we use the same spectral clustering framework [8] with different similarity matrices obtained by the tested algorithms.", "startOffset": 69, "endOffset": 72}, {"referenceID": 23, "context": "Like [24], for all kernel-based algorithms, we adopt the commonly used Gaussian kernel on all datasets and use the default bandwidth parameter which is set to the mean of the distances between all the samples.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 16, "endOffset": 24}, {"referenceID": 37, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 16, "endOffset": 24}, {"referenceID": 9, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 62, "endOffset": 70}, {"referenceID": 37, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 62, "endOffset": 70}, {"referenceID": 38, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 102, "endOffset": 106}, {"referenceID": 39, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 14, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 16, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 23, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 25, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 7, "context": ", the strong connections exist among the samples from the same subject; \u2022 The obtained similarity matrix is a symmetric matrix which can be directly used for subspace clustering under the framework of spectral clustering [8].", "startOffset": 221, "endOffset": 224}, {"referenceID": 30, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 94, "endOffset": 98}, {"referenceID": 34, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are still inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are still inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are still inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "\u2022 The standard SC [8] is the fastest since its similarity graph is computing via the pairwise kernel distances among the input samples, while the KSSC [15] is the most timeconsuming method.", "startOffset": 18, "endOffset": 21}, {"referenceID": 14, "context": "\u2022 The standard SC [8] is the fastest since its similarity graph is computing via the pairwise kernel distances among the input samples, while the KSSC [15] is the most timeconsuming method.", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "\u2022 The time cost of the proposed method is very close to that of its linear version TRR [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "\u2022 The KTRR and TRR [17] algorithms are much faster than KSSC, SSC, KLRR, and LRR methods.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "The KTRR and TRR [17] algorithms both have analytical solutions, and only one pseudo-inverse operation is required for solving the representation problems of all data points for KTRR and TRR algorithms.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "The possible reason is that the NMI is robust to the data distribution (increasing subject number) [19].", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "Subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering methods assume that the data could be linearly represented with each other in the input space. In practice, however, this assumption is hard to be satisfied. To achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps: 1) projecting the data into a hidden space in which the data can be linearly reconstructed from each other; 2) calculating the globally linear reconstruction coefficients in the kernel space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality, and then achieving clustering by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and capacity of clustering data points that lie in nonlinear subspaces. The first advantage makes our method efficient in handling large-scale data sets, and the second one enables the proposed method to address the nonlinear subspace clustering challenge. Extensive experiments on five realworld datasets demonstrate the effectiveness and the efficiency of the proposed method in comparison with ten state-of-the-art approaches regarding four evaluation metrics.", "creator": "LaTeX with hyperref package"}}}