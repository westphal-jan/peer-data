{"id": "1705.05116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination", "abstract": "This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuo-motor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.", "histories": [["v1", "Mon, 15 May 2017 08:57:27 GMT  (3225kb,D)", "http://arxiv.org/abs/1705.05116v1", "2 pages, to appear in the Deep Learning for Robotic Vision (DLRV) Workshop in CVPR 2017"]], "COMMENTS": "2 pages, to appear in the Deep Learning for Robotic Vision (DLRV) Workshop in CVPR 2017", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG cs.SY", "authors": ["fangyi zhang", "j\\\"urgen leitner", "michael milford", "peter i corke"], "accepted": false, "id": "1705.05116"}, "pdf": {"name": "1705.05116.pdf", "metadata": {"source": "CRF", "title": "Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination", "authors": ["Fangyi Zhang", "J\u00fcrgen Leitner", "Michael Milford", "Peter I. Corke"], "emails": ["fangyi.zhang@hdr.qut.edu.au"], "sections": [{"heading": null, "text": "Improvement of hand-eye coordination in modular deep vision strategies (modular networks), where each module is trained independently of each other. Fine-tuning method benefits from weighted losses and significantly improves the performance of strategies for a robotic, planar achievement task."}, {"heading": "1. Introduction", "text": "Recent work has shown that robotic tasks based directly on real image data are improved using deep learning, for example, the acquisition of robot data [2]. However, these methods require large real-world data sets, which are expensive, to be acquired slowly and limit the overall applicability of the approach. To reduce the cost of real data collection, we used simulation to learn robotic skills to reach the level using the DeepMind DQN [3]. The DQN showed impressive results in the simulation, but showed fragility when transferred to a real robot and a real camera [4]. By introducing a bottleneck to separate the DQN into perception and control modules for independent training, the skills learned in the simulation (Fig. 1A) could be easily adapted to real-world scenarios (Fig. 1B), by combining only 1418 images from the real-world, we could improve the performance of the network by significantly [5] matching it to the network."}, {"heading": "2. Methodology", "text": "We consider the planar achievement task that is defined as the control of a 3 DoF-Roboterarms (Baxter robot = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "3. Experiments and Results", "text": "We evaluated the feasibility of the proposed approach on the basis of the parameters of the Euclidean distance error d (between the end effector and the target) and the average accumulated reward R (a larger accumulated reward means a faster and closer achievement of a target) in 400 simulated studies. For comparison, we evaluated three networks: Initial, Fine-tuned and CR. Initial is a combined network without end-to-end finetuning, referred to in [5] as EE2 (consisting of FT75 and CR). FT75 and CR are the selected perception and control modules that individually perform best. Fine-tuning is achieved by using the proposed approach. The CR works as a baseline indicating the performance cap. In fine-tuning, \u03b2 = 0.8, we used a learning rate between 0.01 and 0.001, which is used for the simulation of GPS, or a perceptual loss of battery size of 64 and a perceptual loss of GPS, respectively."}], "references": [{"title": "A theoretical framework for back-propagation", "author": ["Y. LeCun"], "venue": "Proceedings of the 1988 Connectionist Models Summer School,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P.P. Sampedro", "A. Krizhevsky", "D. Quillen"], "venue": "In International Symposium on Experimental Robotics (ISER),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "In Australasian Conference on Robotics and Automation (ACRA),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Transferring visionbased robotic reaching skills from simulation to real world", "author": ["F. Zhang", "J. Leitner", "B. Upcroft", "P. Corke"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "Recent work has demonstrated robotic tasks based directly on real image data using deep learning, for example robotic grasping [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "To reduce the cost of real dataset collection, we used simulation to learn robotic planar reaching skills using the DeepMind DQN [3].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "The DQN showed impressive results in simulation, but exhibited brittleness when transferred to a real robot and camera [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "1B) by using just 1418 real-world images [5].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "Preliminary studies show that a naive fine-tuning using Qlearning does not give the desired result [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "The architecture is similar to that in [5], but has an additional end-to-end fine-tuning process using weighted perception and task losses.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "Note that the values in \u0398 are normalized to the interval [0, 1].", "startOffset": 57, "endOffset": 63}, {"referenceID": 4, "context": "The network has the same architecture and training method to [5], but with an additional end-to-end fine-tuning using weighted losses, as shown in Fig.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "The control network is trained using KGPS [5] where network weights are updated using the Bellman equation which is equivalent to the loss function", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "The control network is updated using only Lq , while the perception network is updated using the weighted loss L = \u03b2Lp + (1\u2212 \u03b2)L q , where L q is a pseudo-loss which reflects the loss of Lq in the bottleneck (BN); \u03b2 \u2208 [0, 1] is a balancing weight.", "startOffset": 218, "endOffset": 224}, {"referenceID": 0, "context": "From the backpropagation algorithm [1], we can infer that \u03b4L = \u03b2\u03b4Lp + (1 \u2212 \u03b2)\u03b4LBN q , where \u03b4L is the gradients resulted by L; \u03b4Lp and \u03b4LBN q are the gradients resulting respectively from Lp and L q (equivalent to that resulting from Lq in the perception module).", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Initial is a combined network without end-to-end finetuning, which is labelled as EE2 in [5] (comprising FT75 and CR).", "startOffset": 89, "endOffset": 92}], "year": 2017, "abstractText": "This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuomotor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.", "creator": "LaTeX with hyperref package"}}}