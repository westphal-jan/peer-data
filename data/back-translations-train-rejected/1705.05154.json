{"id": "1705.05154", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Layerwise Systematic Scan: Deep Boltzmann Machines and Beyond", "abstract": "For Markov chain Monte Carlo methods, one of the greatest discrepancies between theory and system is the scan order - while most theoretical development on the mixing time analysis deals with random updates, real-world systems are implemented with systematic scans. We bridge this gap for models that exhibit a bipartite structure, including, most notably, the Restricted/Deep Boltzmann Machine. The de facto implementation for these models scans variables in a layerwise fashion. We show that the Gibbs sampler with a layerwise alternating scan order has its relaxation time (in terms of epochs) no larger than that of a random-update Gibbs sampler (in terms of variable updates). We also construct examples to show that this bound is asymptotically tight. Through standard inequalities, our result also implies a comparison on the mixing times.", "histories": [["v1", "Mon, 15 May 2017 11:00:25 GMT  (121kb,D)", "http://arxiv.org/abs/1705.05154v1", "15 pages"], ["v2", "Mon, 9 Oct 2017 11:03:50 GMT  (122kb,D)", "http://arxiv.org/abs/1705.05154v2", "v2: typo fixes and improved presentation"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["heng guo", "kaan kara", "ce zhang"], "accepted": false, "id": "1705.05154"}, "pdf": {"name": "1705.05154.pdf", "metadata": {"source": "CRF", "title": "LAYERWISE SYSTEMATIC SCAN: DEEP BOLTZMANN MACHINES AND BEYOND", "authors": ["HENG GUO"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Although most of the theoretical developments of the Monte Carlo Method have to do with random updates, they play a central role in machine learning and are more widespread than the backbone algorithm for models such as Deep Boltzmann Machines [23], random Dirichlet mappings [2], and factor diagrams in general. Given a number of random variables and a target distribution, there are two ways to choose which variable can be updated in the next iteration: (1) Random Update, where in each epoch a variable is randomly selected with a substitute; and (2) Systematic Scan, where in each epoch all variables are updated using a predetermined order."}, {"heading": "2. Related Work", "text": "Probably the most relevant work is the recent analysis by He et al. [11] on the effects of the scan order on the mixing time of Gibbs sampling. They have (1) constructed a variety of models in which the scan order can significantly change the mixing time in various ways, and (2) provided comparative results on the mixing time between random updates and a variant of systematic scans in which \"lazy\" movements are allowed. In this paper, we focus on a more specific case, i.e., bifurcated systems, and thus our limit is stronger - in fact, our limit can be exponentially stronger when the underlying chain is slowly mixed. Furthermore, our result does not alter the standard scan algorithm. Another related work is the recent analysis by Tosh [25] taking into account the mixing time of an alternating sampler for the Restricted Boltzmann Machine (RBM)."}, {"heading": "3. Preliminaries on Markov Chains", "text": "The matrix P (\u00b7) is the stationary distribution of the P (\u00b7) chain defined by P, reversible (in relation to \u03c0 (\u00b7)) if P fulfils the detailed equilibrium condition. P (\u00b7) is the stationary distribution of the P (\u00b7) chain, reversible (in relation to \u03c0 (\u00b7)), if P fulfils the detailed equilibrium condition. P (\u00b7) is the stationary distribution of the P (\u00b7) chain, reversible (in relation to \u03c0 (\u00b7)), if P fulfils the detailed equilibrium condition. P (\u00b7) The matrix P (\u00b7) is the stationary distribution of the P (\u00b7) chain)."}, {"heading": "4. Alternating Scan", "text": "To simplify the notations, we will consider discrete state spaces. Our methods generalize to general state spaces. Let's define V = {x1,.., xn} as a set of variables in which each variable takes values from some set of S. Let's define a distribution defined on SV. Let's be a configuration, namely: V \u2192 S. Let's define the configuration that matches x as if each variable defines values from x, s (v) = s for s'S. In other words, consider for each type of V, s (y), s (y) if x 6 = y; d if x = y.The (lazy) Gibbs sampler is defined in algorithm 1. Let's n = | V | be the total number of variables. The transition kernel PRU (where RU stands for \"random current current current current current\")."}, {"heading": "5. Bipartite Distributions in Machine Learning", "text": "The results we have developed so far can be applied to probability calculations."}, {"heading": "6. Experiments", "text": "In general, it is difficult to arrive at more specific settings under certain complexity assumptions (1) and lower limits (1) [12] (see also [12] for a comprehensive survey on this topic).We evaluate the mixing time in either exact and approximate but approximate ways, such as (1) the calculation directly using the transition matrix for small graphs, (2) the use of state symmetries for medium-sized graphs, and (3) the use of the coupling time as a proxy of the mixing time for large graphs. The mixing time on small graphs. Figure 2 and Figure 3 contains the comparison of the mixing time for small graphs (RBMs of up to 12 variables and DBMs of 4 layers and 3 variables per layer. We vary (1) number of variables, (2) factor functions (shown as the entries in the truth table in the header) or updates of the numbers."}, {"heading": "7. Concluding Remarks", "text": "In summary, we showed that for a bipartite distribution, the relaxation time of the alternating sampler (epoch-related) is no longer than that of the randomly updated sampler. This is asymptotically scarce and implies a (weaker) comparison result over the mixing time. Future directions include finer-grained comparative results that go beyond bipartite distributions."}, {"heading": "Appendix A. Proof of Theorem 3", "text": "In view of the structure in Theorem 3, we shall first repeat [9, Theorem 2,1] (note that the standard in [9] is twice as high as the total deviation). (8) Let us determine that the deviation is twice as high as the total deviation. (8) Let it be thus: = = Deviation (R (P)) and T: = Deviation (4e2hmin) Trel (P) = 1 1 1-2 Deviation (4e2 \u03c0min). Then, it is easy to verify that the deviation from the maximum deviation from the deviation (2) and from (8) is the deviation from the maximum deviation from the deviation (P)."}, {"heading": "Appendix B. Operator Norms and the Spectral Gap", "text": "We also consider the transition matrix P as an operator mapping functions to functions. Specifically, let f be a function f = > PPP (Pf = > Eigenq). We will not distinguish the matrix P from the operator P, since it is clear from the context. Note that Pf (x) is the expectation of f in relation to the distribution P (x, \u00b7). We can consider a eigenfunction f as a column vector in Pf, where Pf is simply the matrix multiplication."}, {"heading": "1\u2212 \u03bb(R(P )) = \u2016R(P )\u2212 S\u03c0\u2016\u03c0(by (10))", "text": "A reorganization of the terms results in the claim."}, {"heading": "Appendix C. Proof of Theorem 1", "text": "The transition matrix of a specific variable is the following: Tx (2), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), x (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), Tx (1), x (1), x (1), x, x (1), x, x (1), x, x, x (1), x, x (1), x, x (1), x, x, x (1), x, x (1), x, x (1), x, x (1), x (1), x, x, x (1), x, x (1), x, x (1), x, x (1), x, x (1), x, x, x (1), x (1), x, x (1, x (1), x (1), x (1), x (1), x (1), x (1), x, x (1), x, x (1), x (1), x (1, x (1), x (1), x (1), x, x (1), x (1), x (1), x (1), x (1, x (1), x (1), x, x (1), x (1), x (1, x (1), x (1), x (1), x (1), x (1), x (1), x (1, x (1), x (1), x (1), x (1), x (1), x (1),"}], "references": [{"title": "The computational complexity of estimating MCMC convergence time", "author": ["Nayantara Bhatnagar", "Andrej Bogdanov", "Elchanan Mossel"], "venue": "In RANDOM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Path coupling: A technique for proving rapid mixing in Markov chains", "author": ["Russ Bubley", "Martin E. Dyer"], "venue": "In FOCS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Analysis of systematic scan Metropolis algorithms using Iwahori-Hecke algebra techniques", "author": ["Persi Diaconis", "Arun Ram"], "venue": "Michigan Math. J.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Comparison theorems for reversible Markov chains", "author": ["Persi Diaconis", "Laurent Saloff-Coste"], "venue": "Ann. Appl. Probab., 3(3):696\u2013730,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Prescribing a system of random variables by conditional distributions", "author": ["R.L. Dobrushin"], "venue": "Theory Probab. Appl.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "Systematic scan for sampling colourings", "author": ["Martin E. Dyer", "Leslie Ann Goldberg", "Mark Jerrum"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Dobrushin conditions and systematic scan", "author": ["Martin E. Dyer", "Leslie Ann Goldberg", "Mark Jerrum"], "venue": "Combin. Probab. Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Eigenvalue bounds on convergence to stationary for nonreversible Markov chains, with an application to the exclusion process", "author": ["James A. Fill"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "A simple condition implying rapid mixing of single-site dynamics on spin systems", "author": ["Thomas P. Hayes"], "venue": "In FOCS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Scan order in Gibbs sampling: Models in which it matters and bounds on how much", "author": ["Bryan D. He", "Christopher De Sa", "Ioannis Mitliagkas", "Christopher R\u00e9"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Mixing time estimation in reversible Markov chains from a single sample path", "author": ["Daniel J. Hsu", "Aryeh Kontorovich", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Counting, Sampling and Integrating: Algorithms and Complexity", "author": ["Mark Jerrum"], "venue": "Lectures in Mathematics, ETH Zu\u0308rich. Birkha\u0308user,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Polynomial-time approximation algorithms for the Ising model", "author": ["Mark Jerrum", "Alistair Sinclair"], "venue": "SIAM J. Comput.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Bounds on the l spectrum for Markov chains and markov processes: A generalization of Cheeger\u2019s inequality", "author": ["Gregory F. Lawler", "Alan D. Sokal"], "venue": "Trans. Amer. Math. Soc.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1988}, {"title": "Markov chains and mixing times", "author": ["David A. Levin", "Yuval Peres", "Elizabeth L. Wilmer"], "venue": "American Mathematical Society, Providence, RI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Convergence rate of incremental aggregated gradient algorithms", "author": ["Asu Ozdaglar Mert G\u00fcrb\u00fczbalaban", "Pablo Parrilo"], "venue": "SIAM J. Optimiz.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Exact thresholds for Ising-Gibbs samplers on general graphs", "author": ["Elchanan Mossel", "Allan Sly"], "venue": "Ann. Probab.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Toward a noncommutative arithmetic-geometric mean inequality: Conjectures, case-studies, and consequences", "author": ["Benjamin Recht", "Christopher R\u00e9"], "venue": "In COLT, pages 11.1\u201311.24,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Surprising convergence properties of some simple Gibbs samplers under various scans", "author": ["Gareth O. Roberts", "Jeffrey S. Rosenthal"], "venue": "Int. J. Stat. Probab.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey Hinton"], "venue": "In AISTATS, pages 448\u2013455,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition", "author": ["P. Smolensky"], "venue": "Information Processing in Dynamical Systems: Foundations of Harmony Theory,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "Mixing rates for the alternating Gibbs sampler over restricted Boltzmann machines and friends", "author": ["Christopher Tosh"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Introduction Gibbs sampling, or the Markov chain Monte Carlo method in general, plays a central role in machine learning and have been widely implemented as the backbone algorithm for models such as Deep Boltzmann Machines [23], latent Dirichlet allocations [2], and factor graphs in general.", "startOffset": 223, "endOffset": 227}, {"referenceID": 1, "context": "Introduction Gibbs sampling, or the Markov chain Monte Carlo method in general, plays a central role in machine learning and have been widely implemented as the backbone algorithm for models such as Deep Boltzmann Machines [23], latent Dirichlet allocations [2], and factor graphs in general.", "startOffset": 258, "endOffset": 261}, {"referenceID": 12, "context": "Although most theoretical development on analyzing Gibbs sampling deals with random updates [13, 17], systematic scans are prevalent in real-world implementations due to their hardware-friendly nature (cache locality for factor graphs, SIMD for Deep Boltzmann Machines, etc.", "startOffset": 92, "endOffset": 100}, {"referenceID": 16, "context": "Although most theoretical development on analyzing Gibbs sampling deals with random updates [13, 17], systematic scans are prevalent in real-world implementations due to their hardware-friendly nature (cache locality for factor graphs, SIMD for Deep Boltzmann Machines, etc.", "startOffset": 92, "endOffset": 100}, {"referenceID": 10, "context": "The mixing time of these two update strategies can differ by some high polynomial factors in either directions [11, 22].", "startOffset": 111, "endOffset": 119}, {"referenceID": 20, "context": "The mixing time of these two update strategies can differ by some high polynomial factors in either directions [11, 22].", "startOffset": 111, "endOffset": 119}, {"referenceID": 7, "context": "Even more pathological examples were constructed for non-Gibbs Markov chains such that systematic scan is not even ergodic whereas the random-update sampler is rapidly mixing [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 2, "context": "As a consequence, theoretical results on rapidly mixing, such as [3, 20], do not readily apply to the scan algorithms used in practice.", "startOffset": 65, "endOffset": 72}, {"referenceID": 18, "context": "As a consequence, theoretical results on rapidly mixing, such as [3, 20], do not readily apply to the scan algorithms used in practice.", "startOffset": 65, "endOffset": 72}, {"referenceID": 16, "context": "The relaxation time (inverse spectral gap) governs various aspects of the mixing properties of a Markov chain, and is closely related to the (total variation) mixing time [17].", "startOffset": 171, "endOffset": 175}, {"referenceID": 10, "context": "By focusing on bipartite systems, we are able to obtain much stronger result than recent studies in the more general setting [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "We note that standard Markov chain comparison results, such as [5], do not seem to fit into our setting.", "startOffset": 63, "endOffset": 66}, {"referenceID": 21, "context": "In particular, our result is a rigorous justification of the popular layer-wise scan sampler for Deep Boltzmann Machines [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "Our result also applies to other models such as Restricted Boltzmann Machines [24] and, more generally, any bipartite factor graph.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "[11] about the impact of the scan order on the mixing time of the Gibbs sampling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Another related work is the recent analysis by Tosh [25] considering the mixing time of an alternating sampler for the Restricted Boltzmann Machine (RBM).", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Tosh showed that, under Dobrushin-like conditions [6], i.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "Known examples are usually restricted to very specific models [4] or under conditions to ensure that the correlations are sufficiently weak [7, 10, 8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "Known examples are usually restricted to very specific models [4] or under conditions to ensure that the correlations are sufficiently weak [7, 10, 8].", "startOffset": 140, "endOffset": 150}, {"referenceID": 9, "context": "Known examples are usually restricted to very specific models [4] or under conditions to ensure that the correlations are sufficiently weak [7, 10, 8].", "startOffset": 140, "endOffset": 150}, {"referenceID": 7, "context": "Known examples are usually restricted to very specific models [4] or under conditions to ensure that the correlations are sufficiently weak [7, 10, 8].", "startOffset": 140, "endOffset": 150}, {"referenceID": 5, "context": "Typical conditions of this sort are variants of the classical Dobrushin condition [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 17, "context": "Another line of related research is about the scan order in stochastic gradient descent [19, 21].", "startOffset": 88, "endOffset": 96}, {"referenceID": 19, "context": "Another line of related research is about the scan order in stochastic gradient descent [19, 21].", "startOffset": 88, "endOffset": 96}, {"referenceID": 16, "context": "where the choice of the constant 1 2e is merely for convenience and is not significant [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Instead, we use an extension developed by Fill [9].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "This imposes a technical difficulty as most of the theoretical tools of analyzing these chains are not suitable for irreversible chains, such as the Dirichlet form [5] or conductance bounds [14].", "startOffset": 164, "endOffset": 167}, {"referenceID": 13, "context": "This imposes a technical difficulty as most of the theoretical tools of analyzing these chains are not suitable for irreversible chains, such as the Dirichlet form [5] or conductance bounds [14].", "startOffset": 190, "endOffset": 194}, {"referenceID": 15, "context": "See for example [16].", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Unfortunately, this is not the case and we give an example (similar to the \u201ctwo islands\u201d example in [11]) where Tmix(PAS) Tmix(PRU ) and Trel(PAS) Trel(PRU ).", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "The relaxation time can be similarly bounded using a standard conductance argument [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "Indeed, as shown in [11], if we scan vertices alternatingly from the left and right, rather than scanning variables layerwise, the mixing time is smaller by a factor of n.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "These models are popularly used in applications such as image processing [18] and natural language processing [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "0 2 4 6 8 10 12 # Variables (b) RBM Factor=[0,10,10,10] 1E+0 1E+1 1E+2 1E+3 1E+4 1E+5 1E+6", "startOffset": 43, "endOffset": 55}, {"referenceID": 9, "context": "0 2 4 6 8 10 12 # Variables (b) RBM Factor=[0,10,10,10] 1E+0 1E+1 1E+2 1E+3 1E+4 1E+5 1E+6", "startOffset": 43, "endOffset": 55}, {"referenceID": 9, "context": "0 2 4 6 8 10 12 # Variables (b) RBM Factor=[0,10,10,10] 1E+0 1E+1 1E+2 1E+3 1E+4 1E+5 1E+6", "startOffset": 43, "endOffset": 55}, {"referenceID": 0, "context": "In general, it is hard under certain complexity assumptions [1] and lower bounds have been established for more concrete settings [12] (see also [12] for a comprehensive survey on this topic).", "startOffset": 60, "endOffset": 63}, {"referenceID": 11, "context": "In general, it is hard under certain complexity assumptions [1] and lower bounds have been established for more concrete settings [12] (see also [12] for a comprehensive survey on this topic).", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "In general, it is hard under certain complexity assumptions [1] and lower bounds have been established for more concrete settings [12] (see also [12] for a comprehensive survey on this topic).", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "[11] and is asymptotically the worst case of Theorem 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "We choose our parameters to stay within the rapidly mixing regime [20] and avoid exponential mixing times.", "startOffset": 66, "endOffset": 70}], "year": 2017, "abstractText": "For Markov chain Monte Carlo methods, one of the greatest discrepancies between theory and system is the scan order \u2014 while most theoretical development on the mixing time analysis deals with random updates, real-world systems are implemented with systematic scans. We bridge this gap for models that exhibit a bipartite structure, including, most notably, the Restricted/Deep Boltzmann Machine. The de facto implementation for these models scans variables in a layer-wise fashion. We show that the Gibbs sampler with a layerwise alternating scan order has its relaxation time (in terms of epochs) no larger than that of a random-update Gibbs sampler (in terms of variable updates). We also construct examples to show that this bound is asymptotically tight. Through standard inequalities, our result also implies a comparison on the mixing times.", "creator": "LaTeX with hyperref package"}}}