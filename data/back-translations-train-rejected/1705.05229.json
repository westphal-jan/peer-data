{"id": "1705.05229", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2017", "title": "Modeling of the Latent Embedding of Music using Deep Neural Network", "abstract": "While both the data volume and heterogeneity of the digital music content is huge, it has become increasingly important and convenient to build a recommendation or search system to facilitate surfacing these content to the user or consumer community. Most of the recommendation models fall into two primary species, collaborative filtering based and content based approaches. Variants of instantiations of collaborative filtering approach suffer from the common issues of so called \"cold start\" and \"long tail\" problems where there is not much user interaction data to reveal user opinions or affinities on the content and also the distortion towards the popular content. Content-based approaches are sometimes limited by the richness of the available content data resulting in a heavily biased and coarse recommendation result. In recent years, the deep neural network has enjoyed a great success in large-scale image and video recognitions. In this paper, we propose and experiment using deep convolutional neural network to imitate how human brain processes hierarchical structures in the auditory signals, such as music, speech, etc., at various timescales. This approach can be used to discover the latent factor models of the music based upon acoustic hyper-images that are extracted from the raw audio waves of music. These latent embeddings can be used either as features to feed to subsequent models, such as collaborative filtering, or to build similarity metrics between songs, or to classify music based on the labels for training such as genre, mood, sentiment, etc.", "histories": [["v1", "Fri, 12 May 2017 01:08:31 GMT  (3815kb,D)", "http://arxiv.org/abs/1705.05229v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["zhou xing", "eddy baik", "yan jiao", "nilesh kulkarni", "chris li", "gautam muralidhar", "marzieh parandehgheibi", "erik reed", "abhishek singhal", "fei xiao", "chris pouliot"], "accepted": false, "id": "1705.05229"}, "pdf": {"name": "1705.05229.pdf", "metadata": {"source": "CRF", "title": "Modeling of the Latent Embedding of Music using Deep Neural Network", "authors": ["Zhou Xing", "Eddy Baik", "Yan Jiao", "Nilesh Kulkarni", "Chris Li", "Gautam Muralidhar", "Marzieh Parandehgheibi", "Erik Reed", "Abhishek Singhal", "Fei Xiao", "Chris Pouliot"], "emails": ["chris.pouliot@nio.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "While the volume of data and the heterogeneity in the digital music market is huge, it is increasingly important and convenient to create automated recommendation and search systems to locate users and discover the relevant content. Most models are formulated according to the concept of collaborative filtering and tables of contents. While the collaborative filtering of the model has hosted more than 30 million songs, the music also flows into the present. Essentially, the user feedback signals learn or embed the user content, it is difficult to model the latent space in a so-called \"cold start\" in which the user music is included."}, {"heading": "2. DESCRIPTION OF THE DATASET", "text": "We use multiple MIR benchmark datasets [3; 4; 5] to train and test our deep learning model. These datasets contain raw audio files of songs labeled either by genre or emotion. The music genre dataset contains 1886 songs, all of which are encoded in mp3 format. Codec information such as sampling rate and bit rate are 44,100 Hz and 128 kb respectively."}, {"heading": "3. EXTRACTION OF THE ACOUSTIC FEATURES", "text": "We use a tool, FFmpeg [6], to decrypt audio files such as mp3, wav files, generating time series data of the sound wave. An example of the extracted sound wave signal for the song \"My Humps\" from \"Black Eye Peas\" is shown in Fig. 2, only 5 second snippets are shown here for illustration. The following feature engineering includes the generation of time frequency representations of the sound wave. Fourier transformation and constant Q transformation (CQT) can be used to generate power spectra of sound as shown in the upper two rows of Fig. 3, both in linear and logarithmic scale, frequency and chromatic scale (notes). Chroma grams of the 12 pitch classes can also be used by means of short-term Fourier transformations in combination with binning strategies [7], as shown on the left row."}, {"heading": "4. ARCHITECTURE OF THE CONVOLUTIONAL NEURAL NETWORK", "text": "It has been shown that the deep neural network performs a successful task in image classification and recognition [13], localized folding to the 3-dimensional (3D) neurons has been able to capture latent features as well as other explicit features on the image, such as patterns, colors, brightness, etc. Any entry in the 3D output volume, the axon, can also be interpreted as an output of a neuron that absorbs stimuli from only a small region in the input field, and that information is spatially shared with all neurons in the same layer. All of these dendrite signals are integrated by synapse together with the other axis. We have used various architectures of convolutionary neural networks with regard to the input layer dimension, the folding layer, the pooling layer, the receptive storage of the neuron, the filtering depth, the gradations, and other implementation options such as activation functions, the local reaction normalization, the CNN being used for experimenting with the optimistic F5 example."}, {"heading": "5. TRAINING AND CROSS VALIDATION", "text": "The 10-fold cross-validation of the MODELA is done with respect to predictions of music labels as well as genres. In total, there are 9 genres in the MIR dataset: Alternative, Blues, Electronics, Folk / Country, Funk / Soul / Rrnb, Jazz, Pop, Rap / Hip-Hop and Rock. Accuracies of the predicted music label at positions 1 and 3 are shown in Table 1, where the network architecture follows the notation of I for input layer, C for Constitutional layer with strip, receiver field and filter depth, L for local response normalization layer, P for maximum pool layer, F for fully connected layer with number of fully connected neurons, and O for output layer. To split the training and test sample, we have tried to select both random snippets for all songs for training and testing of the sample, as well as evenly following the song title of the test snippet to ensure that the same sample and the same snippet do not get validation results from both tracks at the same time."}, {"heading": "6. MUSIC EMBEDDING AND COMPOSING MUSIC USING TRAINED NET", "text": "We take the values of the neurons on the last fully connected layer, just before the output layer, as a latent embedding for each song. After a dimensionality reduction using PCA, we can visualize the distribution of the embedding of songs in the 3D space, as shown in Fig. 6. These latent vectors can also be used to provide content characteristics of each song, to construct song-to-song similarity, and thus to make a content-based recommendation. The trained weights of the neural network can also be used to compose a song where music labels or characteristics are provided in advance, and we can perform backward propagation to artificially construct the audio hyperimage, i.e. the auditory signals of the song. The use of Artificial Intelligence (AI) techniques to compose songs and even song lyrics, would be a promising application of this particular study."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we have proposed and experimented to use acoustic features of music to compose audio hyperimages, and thus to use the deep Convolutionary Neural Network to find a nonlinear mapping between these audio images and song labels. Predictions of the music labels were tested using a 10-fold cross-validation approach. We use the neurons in the last fully connected layer to embed all songs, and these latent features can be applied to subsequent models such as collaborative filtering, factoring machine, etc., to build a recommendation system. Further enhancements to this work may include using similar features and neural network models for speech or speech recognition, where the long-term scale of temporal structures of auditory signals may need to be captured by a recursive neural network structure with the intention of using language as a latent state variable."}, {"heading": "8. REFERENCES", "text": "[1] Morwaread M. Farbood, David J. Heeger, Gary Marcus, Uri Hasson, and Yulia Lerner. The neural processing of hierarchical structure in music and speech at different timscale. Front. Neurosci., 9, 2015. [2] Zhou Xing, Marzieh Parandehgheibi, Fei Xiao, Nilesh Kulkarni, and Chris Pouliot. Content-based recommendation for podcast audio-items using natural language processing techniques. [3] IEEE International Conference on Big Data, 2016. [T. Eerola and J. K. Vuoskoski. A comparison of the discrete and dimensional models of emotion in music. Psychology of Music, 39 (1), S. H. Mark Bryval, 2016. [3] T. Eerola and J. K. Vuoskoski. [3] Multilaboratories of the discrete and dimensional models of emotion."}], "references": [{"title": "The neural processing of hierarchical structure in music and speech at different timescales", "author": ["Morwaread M. Farbood", "David J. Heeger", "Gary Marcus", "Uri Hasson", "Yulia Lerner"], "venue": "Front. Neurosci.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Content-based recommendation for podcast audio-items using natural language processing techniques", "author": ["Zhou Xing", "Marzieh Parandehgheibi", "Fei Xiao", "Nilesh Kulkarni", "Chris Pouliot"], "venue": "IEEE International Conference on Big Data,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Vuoskoski. A comparison of the discrete and dimensional models of emotion in music", "author": ["J.K.T. Eerola"], "venue": "Psychology of Music,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "A benchmark dataset for audio classification and clustering", "author": ["Helge Homburg", "Ingo Mierswa", "Bulent Moller", "Katharina Morik", "Michael Wurst"], "venue": "Proc. of the International Symposium on Music Information Retrieval,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Automatic feature extraction for classifying audio data", "author": ["Ingo Mierswa", "Katharina Morik"], "venue": "Machine Learning Journal,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Audio thumbnailing of popular music using chroma-based representations", "author": ["Mark A. Bartsch", "Gregory H. Wakefield"], "venue": "IEEE Transactions on Multimedia, 7(1),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Cyclic tempogram - a mid-level tempo representation for music", "author": ["Peter Grosche", "Meinard Mller", "Frank Kurth"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["Beth Logan"], "venue": "Proc. Int. Symp. Music Information Retrieval (ISMIR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Music type classification by spectral contrast feature", "author": ["Dan Ning Jiang", "Lu Lie", "Hong Jiang Zhang", "Jian Hua Tao", "Lian Hong Cai"], "venue": "Multimedia and Expo,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Detecting harmonic change in musical audio", "author": ["C. Harte", "M. Sandler", "M. Gasser"], "venue": "Proceedings of the 1st ACM Workshop on Audio and Music Computing Multimedia,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Neuroscience studies [1] has shown that the human brain integrates these complex streams of audio information through various levels of voxel, auditory cortex (A1+), superior temporal gyrus (STG), and inferior frontal gyrus (IFG), etc.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "Figure 1: Re-printed from [1].", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "These latent embeddings of songs can be used either as features to feed to subsequent recommendation models, such as collaborative filtering, to alleviate the issues mentioned previously, or to build similarity metrics between songs, or simply to classify music into the targeted training classes such as genre, mood, etc [2].", "startOffset": 322, "endOffset": 325}, {"referenceID": 5, "context": "Chroma-gram of the 12 pitch classes can also be constructed using short-time Fourier transforms in combination with binning strategies [7] , as shown in the third row on the left.", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "The extraction of the local tempo and beat information is implemented using a mid-level representation of cyclic tempograms, where tempi differing by a power of two are identified [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "Perceptual scale of pitches such as melody spectrogram (MELspectrogram), mel-frequency cepstrum consisting of various Mel-frequency cepstral coefficients (MFCC) can be generated by taking discrete cosine transform of the mel logarithmic powers [9] as shown in the fourth row.", "startOffset": 244, "endOffset": 247}, {"referenceID": 8, "context": "We also investigate other acoustic signals such as spectral contrast [11] and music harmonics such as tonal centroid features (tonnetz) [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "We also investigate other acoustic signals such as spectral contrast [11] and music harmonics such as tonal centroid features (tonnetz) [12].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "The deep neural network has been shown to perform a successful job on image classification and recognitions [13], the localized convolution to the 3-dimensional (3D) neurons has been able to capture latent features as well as other explicit features on the image such as patterns, colors, brightness, etc.", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "While both the data volume and heterogeneity of the digital music content is huge, it has become increasingly important and convenient to build a recommendation or search system to facilitate surfacing these content to the user or consumer community. Most of the recommendation models fall into two primary species, collaborative filtering based and content based approaches. Variants of instantiations of collaborative filtering approach suffer from the common issues of so called \u201ccold start\u201d and \u201clong tail\u201d problems where there is not much user interaction data to reveal user opinions or affinities on the content and also the distortion towards the popular content. Content-based approaches are sometimes limited by the richness of the available content data resulting in a heavily biased and coarse recommendation result. In recent years, the deep neural network has enjoyed a great success in large-scale image and video recognitions. In this paper, we propose and experiment using deep convolutional neural network to imitate how human brain processes hierarchical structures in the auditory signals, such as music, speech, etc., at various timescales. This approach can be used to discover the latent factor models of the music based upon acoustic hyper-images that are extracted from the raw audio waves of music. These latent embeddings can be used either as features to feed to subsequent models, such as collaborative filtering, or to build similarity metrics between songs, or to classify music based on the labels for training such as genre, mood, sentiment, etc.", "creator": "LaTeX with hyperref package"}}}