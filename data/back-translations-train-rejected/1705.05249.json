{"id": "1705.05249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2017", "title": "CLBlast: A Tuned OpenCL BLAS Library", "abstract": "This work demonstrates how to accelerate dense linear algebra computations using CLBlast, an open-source OpenCL BLAS library providing optimized routines for a wide variety of devices. It is targeted at machine learning and HPC applications and thus provides a fast matrix-multiplication routine (GEMM) to accelerate the core of many applications (e.g. deep learning, iterative solvers, astrophysics, computational fluid dynamics, quantum chemistry). CLBlast has four main advantages over other BLAS libraries: 1) it is optimized for and tested on a large variety of OpenCL devices including less commonly used devices such as embedded and low-power GPUs, 2) it can be explicitly tuned for specific problem-sizes on specific hardware platforms, 3) it can perform operations in half-precision floating-point FP16 saving precious bandwidth, time and energy, 4) and it can combine multiple operations in a single batched routine, accelerating smaller problems significantly. This paper describes the library and demonstrates the advantages of CLBlast experimentally for different use-cases on a wide variety of OpenCL hardware.", "histories": [["v1", "Fri, 12 May 2017 17:16:59 GMT  (486kb,D)", "http://arxiv.org/abs/1705.05249v1", null]], "reviews": [], "SUBJECTS": "cs.MS cs.AI cs.DC", "authors": ["cedric nugteren"], "accepted": false, "id": "1705.05249"}, "pdf": {"name": "1705.05249.pdf", "metadata": {"source": "CRF", "title": "CLBlast: A Tuned OpenCL BLAS Library", "authors": ["Cedric Nugteren"], "emails": ["mail@cedricnugteren.nl"], "sections": [{"heading": null, "text": "Most of them, however, are not able to play by the rules. (BLAS) Most of them, however, are able to play by the rules. (BLAS) Most of them are able to play by the rules. (BLAS) Most of them, however, are not able to play by the rules. (BLAS) Most of them are not able to play by the rules. (BLAS) Most of them are not able to play by the rules. (BLAS) Most of them are not able to play by the rules. (BLAS) Most of them are able to play by the rules themselves. (BLAS) Most of them are able to play by the rules. (BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS-BLAS. \""}, {"heading": "II. RELATED WORK", "text": "From a technical point of view, AMD's clBLAS is the most closely related work: it is also a full OpenCL BLAS open source library. However, it does not have CLBlast's performance portability, problem-specific tuning, FP16 support and successive routines. Furthermore, from a technical point of view, it lacks proper testing on rare devices, it does not have a C + + interface, it requires a newer version of OpenCL (1.2 instead of 1.1), and its OpenCL cores are partially generated as strings and are therefore not easily readable or editable. NVIDIA's cuBLAS is also related, but it is only CUDA-based and therefore does not run on non-NVIDIA hardware. These two libraries (cuBLASar Xiv: 170 5.05 249v 1 [cs.M S] 1 2M ay2 017 and clBLAS] are developed on [GEM] optimizations."}, {"heading": "III. THE CLBLAST LIBRARY", "text": "CLBlast is an APACHE 2.0 licensed open source 2 OpenCL implementation of the BLAS API. The host code is written in C + + 11 and the kernel code in OpenCL C, compatible with any device that supports OpenCL 1.1 or later standards. There are automated build tests on Windows, macOS and Linux systems, and there is continuous integration through automated correctness testing on six different devices from three different vendors. CLBlast has an active community: There are third-party Java bindings, 8 contributors, 28 forks, 65 resolved issues, it is integrated into ArrayFire, and it is used in experimental OpenCL versions of Caffe3 and Tensorflow4."}, {"heading": "A. Library design", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves, \"he told the Deutsche Presse-Agentur."}, {"heading": "B. Parameterized kernels", "text": "All kernel implementations in CLBlast are highly parameterized to be feasible across multiple devices: they are not written for a particular device or OpenCL implementation, and this is achieved by creating some preprocessor constants that can be changed without affecting the correctness of the program. A simplified example of the axpy kernel in Figure 1 shows this: the local workgroup size is feasible (WGS), the amount of work per thread can vary (WPT), and the vector width is configurable (VW). Unlike clBLAS, we rely on the target compiler to perform the lowest optimizations, such as loop unrolling and pointer arithmetic, which increase kernel readability, portability, and maintainability."}, {"heading": "C. Performance tuning", "text": "In fact, you will be able to play by the rules without being able to play by the rules."}, {"heading": "IV. EXPERIMENTAL SET-UP", "text": "The following sections will contain some experimental results: This section explains the settings used in this essay. All results and diagrams are also available on-line7, as well as other experimental results for the same and other devices. In this essay, we report only results of unique precision and semi-precision. We test with the OpenCL devices listed in Table II. All experimental results presented in the following sections are obtained by the included CLBlast \"clients\": binaries that compare the runtime of CLBlast with other libraries. Clients first perform a warm-up run followed by 10 repeated runs. The reported results are based on the average time over these runs. We report GB / s for routines that are 7On-line attachments to this essay: http: / / cnugteren.github.io / clblast / typically bandwidth-bound (Level-1 and Level-2) and GLOPS (are routines normally for the 3)."}, {"heading": "V. PERFORMANCE ACROSS DEVICES", "text": "In fact, most of us are able to play by the rules we have set ourselves to play by, \"he said."}, {"heading": "VI. PROBLEM-SPECIFIC TUNING", "text": "Thanks to the included CLTune-based tuners and the user community, CLBlast is already optimized for a variety of devices. However, the tuning is currently only performed for a standard set of input arguments. For example, the GEMM routine is tuned by default to square matrices of dimensions m = 1024, n = 1024, k = 1024. To get the maximum performance from CLBlast, it is possible to set up specific routine arguments. It is even possible to set for several cases: CLBlast provides an API to change tuning parameters at runtime. Problem-specific tuning can be beneficial, for example, for deep learning applications where matrices have a specific size based on the layout of the neural network. To illustrate the benefits of problem-specific tunings, we have tuned the gem core for 9 different matrix sizes. We then benchmark the SGEMM routine for each of the 9 tuning parameters to the same problems."}, {"heading": "VII. HALF-PRECISION FLOATING-POINT", "text": "Traditionally used mainly in computer graphics and image processing applications, FP16 has seen renewed interest in the recent successes of deep learning. Devices with native FP16 at 2x FP32 speed can already be found in the embedded and low-power domains (e.g. Intel Skylake GPUs and ARM Mali GPUs) and the very high-end (NVIDIA Tesla P100). Newer AMD GPUs (Polaris and Vega) also support FP16, but for memory and energy savings only: they perform FP16 calculations at FP32 speed.CLBlast supports all routines in semi-precision mode, while other libraries lag behind current hardware advances and software requirements."}, {"heading": "VIII. BATCHED BLAS ROUTINES", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "IX. COMPARISON WITH OTHER LIBRARIES", "text": "So far, we have only compared CLBlast with its direct competitor clBLAS. Figure 7 also compares the performance with cuBLAS from NVIDIA on the two NVIDIA GPUs in our test suite. It is clear that cuBLAS is still superior to the two OpenCL libraries, especially for smaller problems: cuBLAS achieved much faster performance levels. The reason for this is twofold. Firstly, cuBLAS could be tuned for certain hardware at the assembly / PTX level, while CLBlast relies on the compiler to perform low-level optimizations. Secondly, specific instructions such as _ _ ldg for L1 caching are available at CUDA, but not at OpenCL. An in-depth analysis of CUDA vs OpenCL for GEMM can be found on-line11."}, {"heading": "X. FUTURE WORK", "text": "The current version of CLBlast is already mature and works well on a variety of platforms. However, we identify some topics of future work to further improve the library: \u2022 The current out-of-the-box tuning parameters are optimized for certain routine arguments (e.g. matrix size). In order to achieve optimal performance for a particular use case, the user currently needs to tune manually. However, if we were to run the tuners by default for a mixed set of arguments (e.g. small and large matrices), we could estimate tuning parameters for each use case. Choosing the argument mix and performing the estimate is not trivial and may require elaborate (machine-learned?) models of the cores and hardware. \u2022 The above idea can be applied in another dimension: predicting tuning parameters for invisible devices instead of invisible argument combinations."}, {"heading": "XI. CONCLUSIONS", "text": "CLBlast is an OpenCL BLAS library written in C + + 11 that performs well on a wide range of OpenCL devices thanks to its built-in autotuning support and generic OpenCL cores, providing a viable alternative to the de facto clBLAS standard. Library users can further improve performance by fine-tuning their specific hardware or even their specific application case (e.g. matrix size). Furthermore, this work shows that CLBlast is equipped with features that go beyond BLAS: half-accuracy floating point numbers (FP16) and bundled routines. Both are extremely beneficial for deep learning, where high precision is not always required and bundled operations are common. With the right hardware, CLBlast in FP16 mode can provide you with a factor of 2 performance gains and memory savings. Batching can improve performance to an order of size and application."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank all those involved in the CLBlast project and dividiti for their access to ARM Mali hardware."}], "references": [{"title": "Performance, Design, and Autotuning of Batched GEMM for GPUs", "author": ["A. Abdelfattah", "A. Haidar", "S. Tomov", "J. Dongarra"], "venue": "In 31st International Conference on High Performance Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "The Future of Microprocessors", "author": ["S. Borkar", "A.A. Chien"], "venue": "Commun. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "MAGMA Batched: A Batched BLAS Approach for Small Matrix Factorizations and Applications on GPUs", "author": ["T. Dong", "A. Haidar", "P. Luszczek", "S. Tomov", "A. Abdelfattah", "J. Dongarra"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Performance Upper Bound Analysis and Optimization of SGEMM on Fermi and Kepler GPUs", "author": ["J. Lai", "A. Seznec"], "venue": "In CGO \u201913: Code Generation and Optimization", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A Note on Auto-tuning GEMM for GPUs", "author": ["Y. Li", "J. Dongarra", "S. Tomov"], "venue": "In ICCS: Int. Conf. on Computational Science. Springer,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "High-Performance Matrix-Matrix Multiplications of Very Small Matrices", "author": ["I. Masliah", "A. Abdelfattah", "A. Haidar", "S. Tomov", "M. Baboulin", "J. Falcou", "J. Dongarra"], "venue": "In Euro-Par 2016: International Conference on Parallel and Distributed Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Performance Tuning of Matrix Multiplication in OpenCL on Different GPUs and CPUs", "author": ["K. Matsumoto", "N. Nakasato", "S. Sedukhin"], "venue": "In SC Companion", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Implementing Level-3 BLAS Routines in OpenCL on Different Processing Units", "author": ["K. Matsumoto", "N. Nakasato", "S. Sedukhin"], "venue": "Technical Report TR 2014-001,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "CLTune: A Generic Auto-Tuner for OpenCL Kernels", "author": ["C. Nugteren", "V. Codreanu"], "venue": "In MCSOC \u201915: International Symposium on Embedded Multicore/Many-core Systems-on-Chip,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Zounon. A Comparison of Potential Interfaces for Batched BLAS Computations", "author": ["S.D. Relton", "P. Valero-Lara"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Parallel Multi Channel Convolution using General Matrix Multiplication", "author": ["A. Vasudevan", "A. Anderson", "D. Gregg"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Why GEMM is at the heart of deep learning", "author": ["P. Warden"], "venue": "https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-ofdeep-learning/,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "INTRODUCTION Efficient and fast software has become more important than ever as transistor scaling benefits are diminishing [2], affecting all types of platforms: from embedded devices to desktops and supercomputers.", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "These routines are nowadays even more important due to their widespread use in deep learning: the most common and compute intensive layers in neural networks are the convolution layers (can be expressed as the GEMM routine) and the fully-connected layers (either GEMM or GEMV) [3], [12], [13].", "startOffset": 277, "endOffset": 280}, {"referenceID": 11, "context": "These routines are nowadays even more important due to their widespread use in deep learning: the most common and compute intensive layers in neural networks are the convolution layers (can be expressed as the GEMM routine) and the fully-connected layers (either GEMM or GEMV) [3], [12], [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 12, "context": "These routines are nowadays even more important due to their widespread use in deep learning: the most common and compute intensive layers in neural networks are the convolution layers (can be expressed as the GEMM routine) and the fully-connected layers (either GEMM or GEMV) [3], [12], [13].", "startOffset": 288, "endOffset": 292}, {"referenceID": 9, "context": "That way, they can be auto-tuned for a given OpenCL device through integration of the CLTune auto-tuner [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].", "startOffset": 156, "endOffset": 159}, {"referenceID": 7, "context": "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "From a scientific perspective, several works have previously published auto-tuning and optimization approaches for dense matrix-matrix multiplications [5], [6], [8], [9].", "startOffset": 166, "endOffset": 169}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "There are also several recent publications on batched GEMM operations and more in general on optimizing GEMM for small matrices [1], [4], [7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "There are also several recent publications on batched GEMM operations and more in general on optimizing GEMM for small matrices [1], [4], [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "There are also several recent publications on batched GEMM operations and more in general on optimizing GEMM for small matrices [1], [4], [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 10, "context": "Related to the batched operations in CLBlast is also a comparison article for possible standard interfaces and approaches [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 8, "context": "This is done for almost all routines, similarly to what is described in [9] for level-3 routines.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "Similar to [8], the CLBlast main kernel makes many assumptions on the input arguments which are handled by pre-processing and post-processing kernels (matrix sizes are a multiple of the work-group sizes, offsets are zero, matrix B is transposed, etc.", "startOffset": 11, "endOffset": 14}, {"referenceID": 9, "context": "For more details we refer to the CLTune paper which discusses an earlier version [10] and to the work of Matsumoto et al.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "which served as inspiration for the design of the kernel [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "For details on the CLTune library, we refer to [10].", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "This work demonstrates how to accelerate dense linear algebra computations using CLBlast, an open-source OpenCL BLAS library providing optimized routines for a wide variety of devices. It is targeted at machine learning and HPC applications and thus provides a fast matrix-multiplication routine (GEMM) to accelerate the core of many applications (e.g. deep learning, iterative solvers, astrophysics, computational fluid dynamics, quantum chemistry). CLBlast has four main advantages over other BLAS libraries: 1) it is optimized for and tested on a large variety of OpenCL devices including less commonly used devices such as embedded and low-power GPUs, 2) it can be explicitly tuned for specific problem-sizes on specific hardware platforms, 3) it can perform operations in halfprecision floating-point FP16 saving precious bandwidth, time and energy, 4) and it can combine multiple operations in a single batched routine, accelerating smaller problems significantly. This paper describes the library and demonstrates the advantages of CLBlast experimentally for different use-cases on a wide variety of OpenCL hardware.", "creator": "LaTeX with hyperref package"}}}