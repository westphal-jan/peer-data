{"id": "1705.05263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs", "abstract": "We train a generator by maximum likelihood and we also train the same generator architecture by Wasserstein GAN. We then compare the generated samples, exact log-probability densities and approximate Wasserstein distances. We show that an independent critic trained to approximate Wasserstein distance between the validation set and the generator distribution helps detect overfitting. Finally, we use ideas from the one-shot learning literature to develop a novel fast learning critic.", "histories": [["v1", "Mon, 15 May 2017 14:24:01 GMT  (3581kb,D)", "http://arxiv.org/abs/1705.05263v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ivo danihelka", "balaji lakshminarayanan", "benigno uria", "daan wierstra", "peter dayan"], "accepted": false, "id": "1705.05263"}, "pdf": {"name": "1705.05263.pdf", "metadata": {"source": "META", "title": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs", "authors": ["Ivo Danihelka", "Balaji Lakshminarayanan", "Benigno Uria", "Daan Wierstra", "Peter Dayan"], "emails": ["<danihelka@google.com>."], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "2. Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Generator", "text": "In all our experiments, the generator is a real, non-volume preserving transformation (NVP) (Dinh et al., 2016). The generator is shown schematically in Figure 1. The generator starts with latent noise z0 from the normal standard distribution, and the noise is transformed into a generated image z1 by a sequence of invertable transformations. Like other generators used in GANs, the generator does not add independent noise to the generated image. Unlike other GAN generators, we are able to calculate the log probability density of the generated image z1 by: log pZ1 (z1) = log pZ0 (z0) \u2212 log | det z0 | (1) where log pZ0 (z0) is the log probability density of the standard normal distribution. The determinant of the generator z1 z0 Jacobian matrix can be efficiently calculated if each transformation has a triangular Dinobian (h)."}, {"heading": "2.2. Critic", "text": "Wasserstein GAN (WGAN) (Arjovsky et al., 2017) uses a critic instead of a discriminator. The critic is trained to provide an approximation of the Waterstone distance between the real data distribution Pr and the generator distribution Pg. The approximation is based on Kantorovich's Rubinstein duality: W (Pr, Pg), max f Ex-economics Pr [f (x) \u2212 Ex-economics Pg [f (x)] (2), where f: RD \u2192 R is a continuous Lipschitz function. In practice, the critic f is a neural network with truncated weights to have derivatives. The critic is trained to generate high values on real samples and low values on generated samples. The difference in the expected critical values then approaches the Waterstone distance. The approximation is scaled by the Lipschitz constant for the critic (Arjovsky et al., 2017)."}, {"heading": "2.3. Datasets", "text": "We train on 28 x 28 images from MNIST (LeCun et al., 1998) and on 32 x 32 images from the CelebFaces Attributes Dataset (CelebA) (Liu et al., 2015). The CelebA dataset contains over 200,000 celebrity images. During the training we supplement the CelebA dataset with horizontal reflections of the training examples as from Dinh et al. (2016)."}, {"heading": "2.4. Data Preprocessing", "text": "We convert the discrete pixel intensities of {0,., 255} D into continuous noise probability images of [0, 256] D. The calculated log probability density will then be a lower limit for the log probability mass (Theis et al., 2015): This is a simple non-volume preserving transformation. If the noise image is z2 = x + u and the image with scaled intensities z1 = z2256, then the log probability density of the noise image is divided by 256. This is a simple nonvolume preserving transformation. If the noise image is z2 = x + u and the image with scaled intensities z1 = z2256 then describes the log probability density of the noise image: Log pZ2 (z2) = Log density to Log Density (the probability number = 251), where the Log density is (the probability number = 5) in the 251."}, {"heading": "3. Results", "text": "We trained the same generator architecture with maximum probability and using WGAN. We will now compare the effects of these two different goals."}, {"heading": "3.1. Generated Samples", "text": "Figure 2 shows examples of a generator trained with maximum probability, and another generator trained with WGAN. Both generators have the same number of parameters and the same architecture. WGAN-trained generators seem to produce more coherent faces, and the effect becomes clearer when looking at samples learned from a flatter generator in Figure 3. The flat generator NVP1 only generates locally coherent fields of view when trained with maximum probability."}, {"heading": "3.2. Log-probability Density", "text": "Figure 4 shows the negative log probability density for generators trained by Maximum Probability Estimation (MLE). The lowest generator NVP1 achieved the worse performance there. The generators show only a low degree of overmatch to the training set. The validation loss is slightly higher than the training losses.Generators trained by WGAN have their negative log probability density, which is shown in Figure 5. The figure has a completely different y-axis. The negative log probability densities deteriorate during training by WGAN. The training and validation losses overlap. A generator can obtain an infinite negative log probability if it assigns a zero probability to an example. To verify this possibility, we analyzed the negative log probabilities associated with all validation examples. Figure 6 shows the histogram of negative log probabilities, which are not shown by negative log probabilities for an example."}, {"heading": "3.3. Distribution of Latent Variables", "text": "We know that the distribution of the latent variables is the previous N (0, 1) if the given images originate from the generator. We are curious about the distribution of the latent variables if the given images originate from the validation set. In Figure 7, we show a 2D histogram of the first two latent variables z0 [1], z0 [2]. The histogram was obtained by deriving the latent variables for all examples from the validation set. When the generator was trained with maximum probability, the derived latent variables showed the following means and standard deviations: \u00b51 = 0.05, \u00b52 = 0.05, \u03c31 = 0.05, \u03c31 = 1.06, \u03c32 = 1.03. In contrast, the generator trained by WGAN had concluded latent variables with significantly greater standard deviations: \u00b51 = 0.02, \u00b52 = 0.05, \u03c31 = 1.95, if the generator had a different validation than the GAN."}, {"heading": "3.4. Partial Reconstructions", "text": "Real NVPs are reversible transformations and have perfect reconstructions. We can still visualize reconstructions from a partially re-sampled latent vector. Gregor et al. (2016) and Dinh et al. (2016) visualized a \"conceptual compression\" by deriving the latent variables and then re-sampling some of the latent variables from the normal N (0, 1) previously sampled. Subsequent reconstruction should still form a valid image. If the original image was generated by the generator, the partially re-sampled latent vector would still have the normal N (0, 1) distribution. Figure 8 shows the reconstructions when the first half or second half of the latent vector is re-sampled."}, {"heading": "3.5. Wasserstein Distance", "text": "We are now going to consider the question of how far it is worth confronting the question of how far it is a \"new\" and \"new\" country, a \"new\" and \"new\" world, a \"new\" world, a \"new\" and \"new\" world, a \"new\" and \"new\" world, a \"new\" and \"old\" world, a \"new\" world, a \"new\" world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new world, a new, a new world, a new world, a new world, a new world, a new world, a new, a new, a new world, a new world, a new world, a new, a new world, a new, a new world, a new, a new country, a new country, a new country, a new country, a new country, a new country, a new country."}, {"heading": "4. Fast Learning Critic", "text": "In order to achieve good results, WGAN needs a good critic. The critic needs to be retrained when the generator changes. It would be nice to reduce the number of retraining steps required. We will take ideas from a monitored environment where people used short-term memory to provide hot learning (Santoro et al., 2016; Vinyals et al., 2016). If we look at the optimization problem for the critic (Equation 2), we see that the optimal critic would have to be based on the data distribution and on the current generator distribution. Data distribution is stationary, so the StridedConv2DLeakyReLUBatchNorm + StridedConv2DLeakyReLUShape: [batch _ size, 32, 3] Shape: [num _ extra _ samples, 32, 3] the data distribution is stationary, so the StridedConv2DLeakyRevGat2Conv2ConxeD2X (Stridex) D1, Stridex _ 2Shape: 1."}, {"heading": "4.1. Architecture", "text": "The implemented critical architecture is in Figure 11. The architecture looks like a DCGAN discriminator (Radford et al., 2015) based on an embedding. Distribution is embedded through a network of activations (Figure 10) on the residual connections (Oord et al., 2016). Characteristics from the batch of additional generator samples are averaged over the batch dimension to generate the distribution embedding. Algorithm 1 shows the use of the critic. The embedding remains the same when the critic performs on real or generated samples. The critic would become the original WGAN critic if he uses zeroes instead of the distribution embedding. We use batch size 64 and we also use 64 additional generator samples.The weights used to create the embedding of the distribution do not need to be truncated and we do not cut them off. When training a generator, we do not use the gradient in additional generator samples."}, {"heading": "4.2. Fast Learning Results", "text": "Figure 12 compares training without and with the fast-learning critic. The critic was intentionally updated less frequently by gradient drop to demonstrate the benefits of fast learning. The critic was updated 100 times in the first 25 generator steps and also after all 500 generator steps, otherwise the critic is updated only 2 times per generator step. The independent critic was still updated at least 5 times per generator step to keep all measurements comparable.Without the fast-learning critic, the generator did not generate samples similar to the data examples. The fast-learning critic may be important for conditional models and video modeling."}, {"heading": "5. Discussion", "text": "In our experiments, we used two tools to evaluate generators. First, we used a true NIP to calculate the exact log probability densities; second, we used an independent critic to compare the approximate Waterstone distances on the validation set; the independent critic is very general; the critic only needs samples from two distributions; the approximate Waterstone distance from the independent critic allows us to compare different generator and critic architectures; and, if we take Waterstone distances into account, we should compare generators based on the approximate Waterstone distance to the validation set; and the log probability densities are less useful for generator comparison if the generators generate only a subset of data; and, on the other hand, we pay attention to log probabilities in lossless compression (Theis et al., 2015). When using real NVPs, we can even optimize both goals together (Figure 15 and Figure 16)."}, {"heading": "Acknowledgments", "text": "We would like to thank Mihaela Rosca, David Warde-Farley, Shakir Mohamed, Edward Lockhart, Arun Nair and Chris Burgess for many inspiring discussions and also the very helpful anonymous reviewer who suggested investigating the rank of the Jacobite matrix."}, {"heading": "A. Log-probability Density Ratio Evaluation", "text": "Real NVPs are not limited to the calculation of negative log probability densities of visible variables. For example, we were able to measure the gap between the unbiased KL estimation log q (z | x) \u2212 log p (z) \u2212 log p (z) and its approximation to GAN. Figure 18 shows that contrary variation bayes underestimate the KL divergence. The discriminator would have to output logit (D (x) = \u2212 KL to represent the problem. After measuring the problem, we can start thinking about how to mitigate it. It would be possible to use authoregressive discriminators (Oord et al., 2016) to break up the large KL divergence into several smaller terms: log q (z | x) p (z) = \u2211 i log q (zi | x, z1: i \u2212 zi (before Zip) (independent).)"}], "references": [{"title": "Towards Principled Methods for Training Generative Adversarial Networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.04862,", "citeRegEx": "Arjovsky and Bottou,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky and Bottou", "year": 2017}, {"title": "Density estimation using real NVP", "author": ["Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards conceptual compression", "author": ["Gregor", "Karol", "Besse", "Frederic", "Rezende", "Danilo Jimenez", "Danihelka", "Ivo", "Wierstra", "Daan"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Gregor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2016}, {"title": "Improved training of wasserstein gans", "author": ["Gulrajani", "Ishaan", "Ahmed", "Faruk", "Arjovsky", "Martin", "Dumoulin", "Vincent", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1704.00028,", "citeRegEx": "Gulrajani et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2017}, {"title": "Variational inference using implicit models, part I: Bayesian logistic regression", "author": ["Husz\u00e1r", "Ferenc"], "venue": null, "citeRegEx": "Husz\u00e1r and Ferenc.,? \\Q2017\\E", "shortCiteRegEx": "Husz\u00e1r and Ferenc.", "year": 2017}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Deep learning face attributes in the wild", "author": ["Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks", "author": ["L. Mescheder", "S. Nowozin", "A. Geiger"], "venue": "arXiv preprint arXiv:1701.04722,", "citeRegEx": "Mescheder et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mescheder et al\\.", "year": 2017}, {"title": "Kernel mean embedding of distributions: A review and beyonds", "author": ["Muandet", "Krikamol", "Fukumizu", "Kenji", "Sriperumbudur", "Bharath", "Sch\u00f6lkopf", "Bernhard"], "venue": "arXiv preprint arXiv:1605.09522,", "citeRegEx": "Muandet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Muandet et al\\.", "year": 2016}, {"title": "Conditional image generation with PixelCNN decoders", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Vinyals", "Oriol", "Espeholt", "Lasse", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1606.05328,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "arXiv preprint arXiv:1605.06065,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "On the quantitative analysis of decoder-based generative models", "author": ["Wu", "Yuhuai", "Burda", "Yuri", "Salakhutdinov", "Ruslan", "Grosse", "Roger"], "venue": "arXiv preprint arXiv:1611.04273,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Examples of explicit models are variational auto-encoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and PixelCNN (Oord et al.", "startOffset": 65, "endOffset": 111}, {"referenceID": 11, "context": ", 2014) and PixelCNN (Oord et al., 2016).", "startOffset": 21, "endOffset": 40}, {"referenceID": 2, "context": "Examples of implicit generative models are generative adversarial networks (GANs) (Goodfellow et al., 2014) and stochastic simulator models.", "startOffset": 82, "endOffset": 107}, {"referenceID": 15, "context": "However likelihood is not about human perception (Theis et al., 2015).", "startOffset": 49, "endOffset": 69}, {"referenceID": 1, "context": "Concretely, we use real non-volume preserving transformation (real NVP) (Dinh et al., 2016) as the generator.", "startOffset": 72, "endOffset": 91}, {"referenceID": 16, "context": "in this direction was the quantitative analysis by Wu et al. (2016) who evaluate the (approximate) likelihood of decoder based models using annealed importance sampling (AIS).", "startOffset": 51, "endOffset": 68}, {"referenceID": 4, "context": "The WGAN samples would look better if using the improved training of WGAN-GP (Gulrajani et al., 2017).", "startOffset": 77, "endOffset": 101}, {"referenceID": 1, "context": "In all our experiments, the generator is a real-valued non-volume preserving transformation (NVP) (Dinh et al., 2016).", "startOffset": 98, "endOffset": 117}, {"referenceID": 1, "context": "The determinant of the \u2202z1 \u2202z0 Jacobian matrix can be computed efficiently, if each transformation has a triangular Jacobian (Dinh et al., 2016).", "startOffset": 125, "endOffset": 144}, {"referenceID": 1, "context": "We use a generator with the convolutional multi-scale architecture designed by Dinh et al. (2016). Concretely, we use 3 different generator architectures with 1, 2 and 3 multiscale levels.", "startOffset": 79, "endOffset": 98}, {"referenceID": 12, "context": "We follow closely the original WGAN implementation1 and use a DCGAN (Radford et al., 2015) based critic.", "startOffset": 68, "endOffset": 90}, {"referenceID": 7, "context": "Datasets We train on 28 \u00d7 28 images from MNIST (LeCun et al., 1998) and on 32 \u00d7 32 images from the CelebFaces Attributes Dataset (CelebA) (Liu et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 8, "context": ", 1998) and on 32 \u00d7 32 images from the CelebFaces Attributes Dataset (CelebA) (Liu et al., 2015).", "startOffset": 78, "endOffset": 96}, {"referenceID": 1, "context": "During training we augment the CelebA dataset to also include horizontal flips of the training examples as done by Dinh et al. (2016).", "startOffset": 115, "endOffset": 134}, {"referenceID": 15, "context": "The computed log-probability density will be then a lower bound for the log-probability mass (Theis et al., 2015): \u222b", "startOffset": 93, "endOffset": 113}, {"referenceID": 17, "context": "Approximating the likelihood by annealed importance sampling (Wu et al., 2016) would not discover this problem, as their analysis assumes a Gaussian observation model with a fixed variance.", "startOffset": 61, "endOffset": 78}, {"referenceID": 2, "context": "We also obtained near-infinite negative log-probability densities when training GAN to minimize the Jensen-Shannon divergence (Goodfellow et al., 2014).", "startOffset": 126, "endOffset": 151}, {"referenceID": 2, "context": "Gregor et al. (2016) and Dinh et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "(2016) and Dinh et al. (2016) visualized \u2018conceptual compression\u2019 by inferring the latent variables and then resampling a part of the latent variables from the normalN(0, 1) prior.", "startOffset": 11, "endOffset": 30}, {"referenceID": 11, "context": "The gated convolutional layer from Oord et al. (2016). The channels are split and passed to the element-wise multiplicative interaction.", "startOffset": 35, "endOffset": 54}, {"referenceID": 14, "context": "We will take ideas from supervised setting where people used a short-term memory to provide oneshot learning (Santoro et al., 2016; Vinyals et al., 2016).", "startOffset": 109, "endOffset": 153}, {"referenceID": 16, "context": "We will take ideas from supervised setting where people used a short-term memory to provide oneshot learning (Santoro et al., 2016; Vinyals et al., 2016).", "startOffset": 109, "endOffset": 153}, {"referenceID": 10, "context": "The extra samples are processed to produce a learned embedding of the generator distribution (Muandet et al., 2016).", "startOffset": 93, "endOffset": 115}, {"referenceID": 12, "context": "The architecture looks like a DCGAN discriminator (Radford et al., 2015) conditioned on an embedding.", "startOffset": 50, "endOffset": 72}, {"referenceID": 11, "context": "gated activations (Figure 10) on the residual connections (Oord et al., 2016).", "startOffset": 58, "endOffset": 77}, {"referenceID": 15, "context": "On the other hand, when doing lossless compression, we care about log-probabilities (Theis et al., 2015).", "startOffset": 84, "endOffset": 104}, {"referenceID": 9, "context": "We show one additional usage of real NVPs for Adversarial Variational Bayes (Mescheder et al., 2017) evaluation in the appendix.", "startOffset": 76, "endOffset": 100}, {"referenceID": 9, "context": "For example, a real NVP can be used as an encoder in Adversarial Variational Bayes (Husz\u00e1r, 2017; Mescheder et al., 2017).", "startOffset": 83, "endOffset": 121}, {"referenceID": 11, "context": "It would be possible to use auto-regressive discriminators (Oord et al., 2016) to decompose the large KL divergence to multiple smaller terms:", "startOffset": 59, "endOffset": 78}], "year": 2017, "abstractText": "We train a generator by maximum likelihood and we also train the same generator architecture by Wasserstein GAN. We then compare the generated samples, exact log-probability densities and approximate Wasserstein distances. We show that an independent critic trained to approximate Wasserstein distance between the validation set and the generator distribution helps detect overfitting. Finally, we use ideas from the one-shot learning literature to develop a novel fast learning critic.", "creator": "LaTeX with hyperref package"}}}