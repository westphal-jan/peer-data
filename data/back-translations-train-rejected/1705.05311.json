{"id": "1705.05311", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Comparing Titles vs. Full-text for Multi-Label Classification of Scientific Papers and News Articles", "abstract": "Until today there has been no systematic comparison of how far document classification can be conducted using just the titles of the documents. However, methods using only the titles are very important since automated processing of titles has no legal barriers. Copyright laws often hinder automated document classification on full-text and even abstracts. In this paper, we compare established methods like Bayes, Rocchio, kNN, SVM, and logistic regression as well as recent methods like Learning to Rank and neural networks to the multi-label document classification problem. We demonstrate that classifications solely using the documents' titles can be very good and very close to the classification results using full-text. We use two established news corpora and two scientific document collections. The experiments are large-scale in terms of documents per corpus (up to 100,000) as well as number of labels (up to 10,000). The best method on title data is a modern variant of neural networks. For three datasets, the difference to full-text is very small. For one dataset, a stacking of logistic regression and decision trees performs slightly better than neural networks. Furthermore, we observe that the best methods on titles are even better than several state-of-the-art methods on full-text.", "histories": [["v1", "Mon, 15 May 2017 16:07:35 GMT  (79kb,D)", "http://arxiv.org/abs/1705.05311v1", "10 pages, 1 figure, 3 tables"], ["v2", "Wed, 27 Sep 2017 10:05:49 GMT  (133kb,D)", "http://arxiv.org/abs/1705.05311v2", "Accepted as SHORT PAPER by K-CAP 2017, 9 pages, 1 figure, 3 tables"]], "COMMENTS": "10 pages, 1 figure, 3 tables", "reviews": [], "SUBJECTS": "cs.DL cs.CL", "authors": ["lukas galke", "florian mai", "alan schelten", "dennis brunsch", "ansgar scherp"], "accepted": false, "id": "1705.05311"}, "pdf": {"name": "1705.05311.pdf", "metadata": {"source": "CRF", "title": "Comparing Titles vs. Full-Text for Multi-Label Classification of Scientific Papers and News Articles", "authors": ["Lukas Galke", "Florian Mai", "Alan Schelten", "Dennis Brunsch", "Ansgar Scherp"], "emails": ["lga@informatik.uni-", "stu96542@mail.uni-", "stu111405@informatik.uni-", "deb@informatik.uni-", "a.scherp@zbw.eu", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Machine Learning \u2192 Document Analysis; Word Processing; Keywords Multi-Label Classification; Document Analysis Permission to make digital or printed copies of all or part of this work for personal or class-internal use is granted free of charge, provided that copies are not made or distributed for profit or commercial purposes, and that copies bear this notice and the full quote on the first page. Copyrights to components of this work owned by others other than ACM must be recognized. Credit abstraction is permitted. Otherwise copying or re-publishing, posting on servers, or redistribution to lists require prior express permission and / or a fee."}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2. RELATED WORK", "text": "Subsequently, we will discuss in detail various methods for single labeling and multi-label classification of short texts. Finally, we will summarize our observations of the existing work in relation to our systematic comparisons of multi-label documentation classifications on titles and full texts. In our previous work, we have compared different concept methods such as tri-grams with each other. [50], RAKE-grams and LDA [4] and applied kNN for text classification. We could show that a simple kNN plus the dissemination of activations LDA-based classifications. As an alternative to kNN, Spyromitros et al al al."}, {"heading": "3. CLASSIFIERS", "text": "After vectorization and reweighting (TF-IDF, BM25) of the input text, the next step in our generic word processing pipeline is the application of various classifiers (see Figure 1). Based on the discussion of this work in Section 2, we compare Naive Bayes in two variants (multinomial and Bernoulli), as they are known for text classification. Furthermore, we use 1NN as a representative of the lazy learning family, which is known to perform well when having problems with multiple labels with many labels [28, 44]. Inspired by the related work, we use Logistic Regression (LR) in its optimization, which is performed by Averaged Stochastic Gradient Descent. Finally, we propose a variant of MultiValue Classification Stacking with LR and Rocchio as the base classifier and decision trees as meta classification."}, {"heading": "3.1 Bayes", "text": "The Naive Bayes Classifier is one of the most commonly used classifiers for text classification tasks [29]. We consider two Naive Bayes variants, multinomial and Bernoulli. In the multinomial variant, the characteristics of term or concept frequencies are generated by a multinomial distribution. Bernoulli variant only takes into account the occurrence of (binary) characteristics, punishing the absence of characteristics. Therefore, the Bernoulli variant is an intuitive approach for short texts such as titles, since duplicate words are rare, while the multinomial variant is more intuitive for full texts. For both variants, we use Lidstone Smoothing with \u03b1 = 10 \u2212 5."}, {"heading": "3.2 1NN", "text": "Motivated by the results of text classification on full texts in our previous paper [14], we use 1NN as a representative of kNN-based methods. Here, for a new sample x 6 \u0445 D, we calculate the closest neighbor among all training documents D using the cosine spacing. We use the designations associated with the next document as designations for the new sample x. As a neighborhood, we choose k = 1. This is based on experiments with kNN and other versions of it such as BRkNN-a and BRkNN-b [44]. Mostly, we found the optimal value of k at 1, which corresponds to our definition of 1NN."}, {"heading": "3.3 Generalized Linear Models", "text": "After decompiling the multi-label classification according to the method of binary relevance (R) and the loss of a linear classification (J2 standard), we are therefore left with m binary classification tasks. For each class, a classifier is trained to distinguish its corresponding class from all other classes by learning the parameters of a separating hyperplane in the attribute space (linear model). We can learn a generalized linear model [18] in which the separating hyperplane is specified by a linear combination of the input samples: w \u00b7 x \u2212 b = 0. The parameters w and b are optimized to minimize the regulated training error: 1n n n n \u00b2 n \u00b2 i = 1 J (yi, f (xi) + \u03b1R \u2212 x \u2212 x \u2212 b is the output of the model and R (w) is a regulation rate for the weights of the model (like the L2 standard)."}, {"heading": "3.4 Learning To Rank", "text": "The algorithm was originally developed by Huang et al. [17].Given a document d \"D from the entire corpus, let neighk (d) be the k documents from the training corpus most similar to d according to some similarity function sim (x).Let Lneighk (d) y (d\") be the pool of m can-didate labels. Our goal is to rank the candidates according to their likelihood to be sim (x).Let Lneighk (d) = d \"neighk\" (d \") be the pool of m-didate labels. Our goal is to rate the candidates according to their mapping to d."}, {"heading": "3.5 Multi-Layer Perceptron", "text": "Representing the neural network family, we use a fully networked feed-forward multilayer perceptron (MLP) [33], which is designed for the multiple labeling of text documents and consists of a hidden layer of 1,000 units activated with the usual rectifier [32]. For regularization, we use a dropout [16] with p = 0.5. The output layer consists of as many units as there are labels. For each unit, we apply the sigmoid activation function, which determines the probability for each label whether the label should be assigned or not. In order to convert the probability into a binary decision, Nam et al. applied a threshold learning technique. However, in our first experiments we found that the learned threshold provides rather unsatisfactory results in terms of the F measurement. Therefore, we use a uniform threshold of 0.2, which we empirically determined for all labels. We use Adam [22] to optimize the cross-section of a network."}, {"heading": "3.6 Multi-Value Classification Stacking", "text": "He\u00df et al. [15] introduced a generic stacking approach to multi-label classification, in which a base classifier is composed of a meta-classifier for each label. [15] The base classifier provides a (ranking) list of label predictions with confidence scores. Then, for each label, the meta-classifier takes the confidence score and the position in the ranking as inputs and outputs a binary decision for that label. \"The classifier then induces two label functions: Rk \u2192 Ln and Score: Rk \u00d7 L \u2192 R. The function label (x) designates the top-n label predictions for a document x sorted by its score score (x, l), which defines the confidence score score score."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "For all classification strategies, we have applied a full 10-fold cross-validation across all data sets. In the following sections, we present the data sets used in our experiments and their pre-processing. In addition, we present our evaluation metric of a sample-based F1 metric. We select a sample-based evaluation metric because it evaluates the classification quality of each document separately, reflecting the workflow of manual document classification as performed by domain experts in scientific digital libraries and journalists."}, {"heading": "4.1 Datasets", "text": "We have conducted our experiments on four sets of data: Two sets of data are sourced from academic libraries in economics and political science; we also use two sets of news from Reuters and the New York Times; Table 1 summarizes the basic statistics of the four sets of data; all documents are written in English; we also have a domain of annotations; the gold standard for scientific data sets is created by professionals in the scientific libraries; and the data sets are provided by journalists."}, {"heading": "4.2 Preprocessing", "text": "Prior to the extraction of character vectors, both the input text and the thesauri undergo a series of pre-processing steps, including the discarding of all characters except for sequences of alphabetical characters longer than two. Words associated with a hyphen are joined together (i.e. the hyphen is removed), and determined words are narrowed and lemmatized based on WordNet's morphological processing [46]."}, {"heading": "4.3 Sample-based F1 Measure", "text": "We evaluate the performance of a classifier \u03b3 in the sense of the well-known sample-based formula 1 measurement. Sample-based4http: / / zbw.eu / stw / versions / 9.0 / about.en.html 5http: / / www.fiv-iblk.de / eindex.htm 6http: / / www.fiv-iblk.de / information / thesaurus.htmTitlesV 19, 579 15, 419 32, 859 40, 736 w / d 7.07 (3.03) 8.13 (5.5) 12.21 (2.39) 4.46 (2.25) Full-text V 1, 340, 628 1, 165, 919 155, 339 270, 710 w / d 6, 750 (6, 854) 11, 255 (15, 565) 136 (114) 310 (294) Score F1 (d) for each document d in the test d in the category D and the average F-D (F-5) (F-D) in the category."}, {"heading": "5. RESULTS", "text": "The following sections describe the results of our experiments. As described in the introduction, there are too many different pipeline configurations to evaluate all of them. Therefore, we have taken a step-by-step approach, looking for a locally optimal solution for each step to find the best classification strategy."}, {"heading": "5.1 Results for Term-relevance Methods", "text": "We compare two vectorizations of the input text as shown in Figure 1. One vectorization is based on term frequencies and the other on concept frequencies (see also TFIDF versus CF-IDF in the corresponding paper). In addition, we experiment with the reweighting method BM25 using term frequencies and BM25C using concept frequencies. In addition, we apply a feature unification of term frequencies and concept frequencies. The feature unification is used in the modified methods CTF-IDF and BM25CT.As a classifier, we use 1NN with cosine spacing. The performance of kNN is based on the assumption that documents are well represented by the characteristics and that similar documents have similar labels. Therefore, we think that its classification performance is also a good indicator of the quality of the characteristics. Table 2 shows the results. When combining the thermvector with the concept vector, the performance is at least as good as the FIDF results are not as the other FIDF results."}, {"heading": "5.2 Results for Classifiers", "text": "We have shown that CTF-IDF is the best vectorization method. Therefore, we use CTF-IDF to compare the performance of the different classifiers, with the only exception being the Bayes classifiers, where we use the raw frequency of terms and concepts without reweighting, because the multinomial Bayes classifier is based on the frequency of words. In the case of Bernoulli Bayes, the characteristics are reduced to binary values. Please note that all experiments with full texts in which the MLP is involved reduce the attribute space to the 50,000 most common terms to account for the limitations in the GPU memory, which is 12GB. Furthermore, we observe that MLP optimization is running in numerical problems during full texts."}, {"heading": "6. DISCUSSION", "text": "In fact, the fact is that most of them will be able to show themselves, that they are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves."}, {"heading": "7. CONCLUSION", "text": "We have conducted a systematic comparison of established and current methods of multiple classification of documents to show that it is possible to deliver competitive text classification results based solely on bibliographic data. Specifically, for the scientific work of the economics and political science datasets, as well as the Reuters news dataset, the results are very close to full-text classifications. Although the full-text overall classification still performs better, the classification results for titles are very remarkable, opening up many new possibilities for the use of document classification even in applications where few input data such as titles are available. In addition, the use of titles is of great importance as there are no legal obstacles to their computational analysis.The use of abstracts and full text is often prohibited due to copyright limitations, and the best strategy for title-based classification is currently being explored by the ZBW for integration and productive use in a semi-automatic keyword suggestion system.To explore further research in the field, see our Application Code.1"}, {"heading": "8. REFERENCES", "text": "[1] G. Balikas and M. Amini. Multi-label, multi-classsclassification systems [multi-class intelligence] using polylingual information networks. In ECIR, pp. 723-728, 2016. [2] P. K. Bhowmick. Reader perspective emotion analysis in text through ensemble based multi-label classification framework. Computer and Information Science, 2 (4), 2009. [3] W. Bi and J. T. Kwok. Efficient multi-label classification with many labels. In ICML. ACM, 2013. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Machine Learning Research, 3, 2003. L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT '2010, pp. 177-186. Springer, 2010."}], "references": [{"title": "Multi-label, multi-class classification using polylingual embeddings", "author": ["G. Balikas", "M. Amini"], "venue": "ECIR, pages 723\u2013728,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Reader perspective emotion analysis in text through ensemble based multi-label classification framework", "author": ["P.K. Bhowmick"], "venue": "Computer and Information Science, 2(4),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient multi-label classification with many labels", "author": ["W. Bi", "J.T. Kwok"], "venue": "ICML. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Machine Learning Research, 3,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "The tradeoffs of large scale learning", "author": ["O. Bousquet", "L. Bottou"], "venue": "Advances in neural information processing systems.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "ACL-02 conference on Empirical methods in natural language processing-Volume 10,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "A case for automated large-scale semantic annotation", "author": ["S. Dill", "N. Eiron", "D. Gibson", "D. Gruhl", "R.V. Guha", "A. Jhingran", "T. Kanungo", "K.S. McCurley", "S. Rajagopalan", "A. Tomkins", "J.A. Tomlin", "J.Y. Zien"], "venue": "J. Web Sem., 1(1):115\u2013132,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Twitter news classification using SVM", "author": ["I. Dilrukshi", "K. De Zoysa", "A. Caldera"], "venue": "Computer Science & Education. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Classifying news stories to estimate the direction of a stock market index", "author": ["B. Drury", "L. Torgo", "J. Almeida"], "venue": "Information Systems and Technologies. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-label learning: a review of the state of the art and ongoing research", "author": ["E. Gibaja", "S. Ventura"], "venue": "Data Mining and Knowledge Discovery, 4(6),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "News personalization using the CF-IDF semantic recommender", "author": ["F. Goossen", "W. IJntema", "F. Frasincar", "F. Hogenboom", "U. Kaymak"], "venue": "Web Intelligence, Mining and Semantics. ACM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparison of different strategies for automated semantic document annotation", "author": ["G. Gro\u00dfe-B\u00f6lting", "C. Nishioka", "A. Scherp"], "venue": "Knowledge Capture. ACM,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-value classification of very short texts", "author": ["A. He\u00df", "P. Dopichaj", "C. Maa\u00df"], "venue": "Advances in Artificial Intelligence. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Recommending MeSH terms for annotating biomedical articles", "author": ["M. Huang", "A. N\u00e9v\u00e9ol", "Z. Lu"], "venue": "Am. Medical Informatics Association, 18(5),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Generalized linear models", "author": ["R.W.M.W.J.A. Nelder"], "venue": "Royal Statistical Society, 135(3),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1972}, {"title": "Supervised and semi-supervised text categorization using LSTM for region embeddings", "author": ["R. Johnson", "T. Zhang"], "venue": "ICML, pages 526\u2013534. JMLR.org,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Some effective techniques for naive bayes text classification", "author": ["S.-B. Kim", "K.-S. Han", "H.-C. Rim", "S.H. Myaeng"], "venue": "Knowledge and Data Engineering, 18(11),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Machine Learning Research, 5,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Effective document labeling with very few seed words: A topic model approach", "author": ["C. Li", "J. Xing", "A. Sun", "Z. Ma"], "venue": "CIKM, pages 85\u201394. ACM,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Tweetsift: Tweet topic classification based on entity knowledge base and topic enhanced word embedding", "author": ["Q. Li", "S. Shah", "X. Liu", "A. Nourbakhsh", "R. Fang"], "venue": "CIKM, pages 2429\u20132432. ACM,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Weighted multi-label classification model for sentiment analysis of online news", "author": ["X. Li", "H. Xie", "Y. Rao", "Y. Chen", "X. Liu", "H. Huang", "F.L. Wang"], "venue": "Big Data and Smart Computing. IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Pubmed related articles: a probabilistic topic-based model for content similarity", "author": ["J. Lin", "W.J. Wilbur"], "venue": "BMC bioinformatics, 8(1):1,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["G. Madjarov", "D. Kocev", "D. Gjorgjevikj", "S. Dzeroski"], "venue": "Pattern Recognition, 45(9),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to information", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Adding semantics to microblog posts", "author": ["E. Meij", "W. Weerkamp", "M. de Rijke"], "venue": "In WSDM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Distributional random oversampling for imbalanced text classification", "author": ["A. Moreo", "A. Esuli", "F. Sebastiani"], "venue": "SIGIR, pages 805\u2013808. ACM,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Large-scale multi-label text classification", "author": ["J. Nam", "J. Kim", "E.L. Men\u0107\u0131a", "I. Gurevych", "J. F\u00fcrnkranz"], "venue": "revisiting neural networks. In Machine Learning and Knowledge Discovery in Databases. Springer,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to classify short and sparse text & web with hidden topics from large-scale data collections", "author": ["X.-H. Phan", "L.-M. Nguyen", "S. Horiguchi"], "venue": "WWW. ACM,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic classification of online news headlines", "author": ["M.W. Pope"], "venue": "A Master\u2019s paper, University of North Carolina at Chapel Hill,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Okapi at TREC-7: automatic ad hoc, filtering, VLC and interactive track", "author": ["S.E. Robertson", "S. Walker", "M. Beaulieu", "P. Willett"], "venue": "Nist Special Publication SP,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "Automatic keyword extraction from individual documents", "author": ["S. Rose", "D. Engel", "N. Cramer", "W. Cowley"], "venue": "Text Mining,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-label classification of short text: A study on Wikipedia barnstars", "author": ["H. Sajnani", "S. Javanmardi", "D.W. McDonald", "C.V. Lopes"], "venue": "Analyzing Microtext. AAAI,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information processing & management, 24(5),", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1988}, {"title": "Evaluating multi-label classification of incident-related tweets", "author": ["A. Schulz", "E.L. Men\u0107\u0131a", "T.T. Dang", "B. Schmidt"], "venue": "Workshop on Making Sense of Microposts. CEUR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "A rapid-prototyping framework for extracting small-scale incident-related information in microblogs: Application of multi-label classification on tweets", "author": ["A. Schulz", "E.L. Men\u0107\u0131a", "B. Schmidt"], "venue": "Information Systems,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "Computing Surveys, 34(1),", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "Semi-supervised multi-label topic models for document classification and sentence labeling", "author": ["H. Soleimani", "D.J. Miller"], "venue": "CIKM, pages 105\u2013114. ACM,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "An empirical study of lazy multilabel classification algorithms", "author": ["E. Spyromitros", "G. Tsoumakas", "I. Vlahavas"], "venue": "Artificial Intelligence. Springer,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward a short text classification framework based on background knowledge discovery", "author": ["I. Taksa"], "venue": "Artificial Intelligence. WorldComp,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "About WordNet", "author": ["P. University"], "venue": "wordnet.princeton.edu,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Tweet2Vec: Learning tweet embeddings using character-level CNN-LSTM encoder-decoder", "author": ["S. Vosoughi", "P. Vijayaraghavan", "D. Roy"], "venue": "SIGIR, pages 1041\u20131044. ACM,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Concept-based short text classification and ranking", "author": ["F. Wang", "Z. Wang", "Z. Li", "J.-R. Wen"], "venue": "CIKM. ACM,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Association for Computational Linguistics. ACL,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "KEA: Practical automatic keyphrase extraction", "author": ["I.H. Witten", "G.W. Paynter", "E. Frank", "C. Gutwin", "C.G. Nevill-Manning"], "venue": "Digital libraries. ACM,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1999}, {"title": "A word distributed representation based framework for large-scale short text classification", "author": ["D. Yao", "J. Bi", "J. Huang", "J. Zhu"], "venue": "Neural Networks. IEEE,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "ICML. ACM,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2004}, {"title": "Short text classification based on feature extension using the n-gram model", "author": ["X. Zhang", "B. Wu"], "venue": "Fuzzy Systems and Knowledge Discovery. IEEE,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "They are the so far largest datasets of scientific documents used for document classification [14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "Existing comparisons of multi-label classification on documents only consider the case of full-text [28, 33, 44].", "startOffset": 100, "endOffset": 112}, {"referenceID": 31, "context": "Existing comparisons of multi-label classification on documents only consider the case of full-text [28, 33, 44].", "startOffset": 100, "endOffset": 112}, {"referenceID": 42, "context": "Existing comparisons of multi-label classification on documents only consider the case of full-text [28, 33, 44].", "startOffset": 100, "endOffset": 112}, {"referenceID": 26, "context": "Methods for multilabel classification over full-text have been compared in previous surveys [28, 33, 44].", "startOffset": 92, "endOffset": 104}, {"referenceID": 31, "context": "Methods for multilabel classification over full-text have been compared in previous surveys [28, 33, 44].", "startOffset": 92, "endOffset": 104}, {"referenceID": 42, "context": "Methods for multilabel classification over full-text have been compared in previous surveys [28, 33, 44].", "startOffset": 92, "endOffset": 104}, {"referenceID": 48, "context": "In our prior work, we have compared different concept extraction methods like Tri-grams [50], RAKE [37] and LDA [4] and applied kNN for text classification [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": "In our prior work, we have compared different concept extraction methods like Tri-grams [50], RAKE [37] and LDA [4] and applied kNN for text classification [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "In our prior work, we have compared different concept extraction methods like Tri-grams [50], RAKE [37] and LDA [4] and applied kNN for text classification [14].", "startOffset": 112, "endOffset": 115}, {"referenceID": 13, "context": "In our prior work, we have compared different concept extraction methods like Tri-grams [50], RAKE [37] and LDA [4] and applied kNN for text classification [14].", "startOffset": 156, "endOffset": 160}, {"referenceID": 13, "context": "We could show that a simple kNN plus spreading activation outperforms LDA-based classifiers [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 42, "context": "[44] suggested Binary RelevancekNN (BRkNN) that combines the problem transformation method BR with the kNN algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] used the probabilistic classifier Bernoulli Naive Bayes, where features are represented as boolean variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "Also the well-known Rocchio classifier [42] and classifier chains [12, 15] as well as Support Vector Machines (SVM) [49] are often used as baseline methods.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "Also the well-known Rocchio classifier [42] and classifier chains [12, 15] as well as Support Vector Machines (SVM) [49] are often used as baseline methods.", "startOffset": 66, "endOffset": 74}, {"referenceID": 14, "context": "Also the well-known Rocchio classifier [42] and classifier chains [12, 15] as well as Support Vector Machines (SVM) [49] are often used as baseline methods.", "startOffset": 66, "endOffset": 74}, {"referenceID": 47, "context": "Also the well-known Rocchio classifier [42] and classifier chains [12, 15] as well as Support Vector Machines (SVM) [49] are often used as baseline methods.", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "While focussing on transformations of the label space, Bi and Kwok compared different label selection methods and label transformation methods for multi-label classification [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 31, "context": "demonstrated very good results for large-scale multi-label text classification [33].", "startOffset": 79, "endOffset": 83}, {"referenceID": 37, "context": "In terms of scoring methods, most classical ones are TFIDF [39] and BM25 [36].", "startOffset": 59, "endOffset": 63}, {"referenceID": 34, "context": "In terms of scoring methods, most classical ones are TFIDF [39] and BM25 [36].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "[13] presented Concept Frequency-Inverse Document Frequency (CF-IDF), a feature re-weighting method that extends TF-IDF by replacing words with semantic concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Examples are news articles using headlines [11], social media such as micro blogging [10], and query classification [45].", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "Examples are news articles using headlines [11], social media such as micro blogging [10], and query classification [45].", "startOffset": 85, "endOffset": 89}, {"referenceID": 43, "context": "Examples are news articles using headlines [11], social media such as micro blogging [10], and query classification [45].", "startOffset": 116, "endOffset": 120}, {"referenceID": 51, "context": "[53] proposed an n-gram model for news headlines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "TweetSift [25] is a single-labeling approach for tweets.", "startOffset": 10, "endOffset": 14}, {"referenceID": 49, "context": "[51].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "present an approach for single-label classification of short text using LDA [34].", "startOffset": 76, "endOffset": 80}, {"referenceID": 46, "context": "[48] use an approach in which a short-text is represented as a set of concept vectors and a class is represented as a single concept vector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] where the authors used titles to classify news articles of the Reuters-21578 dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "com/ state that tags assigned by users should not be used as goldstandard [15] due to lack of experience, problems with synonyms, and others.", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "[9] developed with SemTag an approach for the automated tagging of a large corpus of web documents.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[30] investigated a novel method for linking tweets to Wikipedia articles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Sajnani [38] conducted multi-label classification on Barnstars with seven labels.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "Soleimani Miller [43] developed an approach for semi-supervised multilabel topic modeling on documents and sentences.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "Pope [35] classified online news headlines crawled from RSS feeds.", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "Johnson and Zhang [19] compare the Long Short-Term Memory method with one-hot convolutional neural networks.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Balikas and Amini [1] propose a polylingual text embedding that learns a language independent representation of texts using neural networks.", "startOffset": 18, "endOffset": 21}, {"referenceID": 45, "context": "[47] propose a tweet embedding using character-level CNN-LSTM encoder-decoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] who applied a list-wise learning to rank algorithm to identify suitable labels from a candidate pool inferred from the neighborhood of a document by taking into account only the titles and abstracts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] used a large number of labels and built a semi-automated tagging system for questions stated on an online web platform.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Moreo et al [31] propose an approach to generate synthetic documents for minority-classes to counterbalance the skewness of the training documents in the classes.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "[24] propose a method to learn classifiers from few training samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Other multi-labeling problems deal with incident detection in tweets [40, 41] or sentiment classification [2, 26].", "startOffset": 69, "endOffset": 77}, {"referenceID": 39, "context": "Other multi-labeling problems deal with incident detection in tweets [40, 41] or sentiment classification [2, 26].", "startOffset": 69, "endOffset": 77}, {"referenceID": 1, "context": "Other multi-labeling problems deal with incident detection in tweets [40, 41] or sentiment classification [2, 26].", "startOffset": 106, "endOffset": 113}, {"referenceID": 24, "context": "Other multi-labeling problems deal with incident detection in tweets [40, 41] or sentiment classification [2, 26].", "startOffset": 106, "endOffset": 113}, {"referenceID": 26, "context": "Rather, to our knowledge existing comparisons of multi-label classification for documents only consider the case of full-text [28, 33, 44].", "startOffset": 126, "endOffset": 138}, {"referenceID": 31, "context": "Rather, to our knowledge existing comparisons of multi-label classification for documents only consider the case of full-text [28, 33, 44].", "startOffset": 126, "endOffset": 138}, {"referenceID": 42, "context": "Rather, to our knowledge existing comparisons of multi-label classification for documents only consider the case of full-text [28, 33, 44].", "startOffset": 126, "endOffset": 138}, {"referenceID": 13, "context": "In fact, we use the so far largest datasets of scientific publications [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "Furthermore, we employ 1NN as a representative for the lazy learner family, which is known to perform well on multi-label problems with many labels [28, 44].", "startOffset": 148, "endOffset": 156}, {"referenceID": 42, "context": "Furthermore, we employ 1NN as a representative for the lazy learner family, which is known to perform well on multi-label problems with many labels [28, 44].", "startOffset": 148, "endOffset": 156}, {"referenceID": 27, "context": "The Naive Bayes classifier is one of the most commonly used classifiers for text classification tasks [29].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Motivated by the results for text classification on full-texts in our previous work [14], we apply 1NN as a representative of kNN-based methods.", "startOffset": 84, "endOffset": 88}, {"referenceID": 42, "context": "This is based on experiments with kNN and other versions of it like BRkNN-a and BRkNN-b [44].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "We can learn a generalized linear model [18], in which the separating hyperplane is specified by a linear combination of the input samples: w \u00b7 x \u2212 b = 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 50, "context": "Thus, we employ stochastic gradient descent as an optimizer for these generalized linear models, which not only runs in linear time, but also provides good generalization on largescale datasets [52, 7, 5].", "startOffset": 194, "endOffset": 204}, {"referenceID": 6, "context": "Thus, we employ stochastic gradient descent as an optimizer for these generalized linear models, which not only runs in linear time, but also provides good generalization on largescale datasets [52, 7, 5].", "startOffset": 194, "endOffset": 204}, {"referenceID": 4, "context": "Thus, we employ stochastic gradient descent as an optimizer for these generalized linear models, which not only runs in linear time, but also provides good generalization on largescale datasets [52, 7, 5].", "startOffset": 194, "endOffset": 204}, {"referenceID": 5, "context": "We apply the learning rate schedule \u03b7 = 1 \u03b1\u00b7(t0+t) , where t0 is chosen by a heuristic of L\u00e9on Bottou [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "Furthermore, we average the weights w over time, which allows higher learning rates and leads to a faster convergence [8, 6].", "startOffset": 118, "endOffset": 124}, {"referenceID": 5, "context": "Furthermore, we average the weights w over time, which allows higher learning rates and leads to a faster convergence [8, 6].", "startOffset": 118, "endOffset": 124}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "For learning the ranking, we use the features as in the original paper [17].", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "used the document similarity function pmra [27].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "As representative for the neural network family, we employ a fully connected feed-forward multi-layer perceptron (MLP) [33].", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "It is designed for multi-labeling of text documents and consists of one hidden layer with 1,000 units activated with the common rectifier [32].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "For regularization, we apply a dropout [16] with p = 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "We use Adam [22] to optimize the network towards minimum cross-entropy error.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "[15] introduced a generic stacking approach for multi-label classification where a base classifier is composed with meta classifiers for each label.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "Detected words were lower-cased and lemmatized based on the morphological processing of WordNet [46].", "startOffset": 96, "endOffset": 100}], "year": 2017, "abstractText": "Until today there has been no systematic comparison of how far document classification can be conducted using just the titles of the documents. However, methods using only the titles are very important since automated processing of titles has no legal barriers. Copyright laws often hinder automated document classification on full-text and even abstracts. In this paper, we compare established methods like Bayes, Rocchio, kNN, SVM, and logistic regression as well as recent methods like Learning to Rank and neural networks to the multi-label document classification problem. We demonstrate that classifications solely using the documents\u2019 titles can be very good and very close to the classification results using full-text. We use two established news corpora and two scientific document collections. The experiments are large-scale in terms of documents per corpus (up to 100, 000) as well as number of labels (up to 10, 000). The best method on title data is a modern variant of neural networks. For three datasets, the difference to full-text is very small. For one dataset, a stacking of logistic regression and decision trees performs slightly better than neural networks. Furthermore, we observe that the best methods on titles are even better than several state-of-the-art methods on full-text.", "creator": "LaTeX with hyperref package"}}}