{"id": "1705.05414", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Key-Value Retrieval Networks for Task-Oriented Dialogue", "abstract": "Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.", "histories": [["v1", "Mon, 15 May 2017 19:11:12 GMT  (963kb,D)", "http://arxiv.org/abs/1705.05414v1", null], ["v2", "Fri, 14 Jul 2017 00:45:59 GMT  (965kb,D)", "http://arxiv.org/abs/1705.05414v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mihail eric", "christopher d manning"], "accepted": false, "id": "1705.05414"}, "pdf": {"name": "1705.05414.pdf", "metadata": {"source": "CRF", "title": "Key-Value Retrieval Networks for Task-Oriented Dialogue", "authors": ["Mihail Eric", "Christopher D. Manning"], "emails": ["meric@cs.stanford.edu,", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Key-Value Retrieval Networks", "text": "While newer models of neural dialogue have explicitly modelled states of dialogue by faith and user intention trackers (Wen et al., 2016b; Dhingra et al., 2016; Henderson et al., 2014), we instead opt for implicit modeling of the state of dialogue on learned neural representations, creating a truly fully traceable system. Our model begins with an encoder decoder sequence architec-1The data will be made available at a later stage and supplemented by an attention-based retrieval mechanism that effectively reflects on a key value representation of the underlying knowledge base. We describe each component of our model in the following sections."}, {"heading": "2.1 Encoder", "text": "Considering a dialog between a user (u) and a system (s), we represent the dialog expressions as {(u1, s1), (u2, s2),..., (uk, sk)}, with k indicating the number of turns in the dialog. At the end of the dialog, we encode the aggregated dialog context composed of the characters (u1, s1,.., si \u2212 1, ui). By specifying x1,..., xm these characters, we first embed these characters using a trained embedding function that maps each mark to a fixed-dimensional vector. These mappings are fed into the encoder to create context-sensitive hidden representations h1,..., hm, repeating the repetition: hi = LSTM (Khalemb (xi), hi \u2212 1). (1), using a long-term short-term rider, the 1997 (and)."}, {"heading": "2.2 Decoder", "text": "The vanilla sequence-to-sequence decoder predicts the tokens of the system response si by first calculating the hidden states of the decoder over the recurring unit. We designate h-1,.., h-n as the hidden states of the decoder and y1,.., yn as the output tokens. We extend this decoder with an attention-based model (Bahdanau et al., 2015; Luong et al., 2015a) in which for each step t of the decoding an attention value ati is calculated for each hidden state hi of the encoder using the attention mechanism of (Vinyals et al., 2015). Formally, this attention can be described by the following equations: uti = w T tanh (W2 tanh (W1 [hi, h] t)))) (2) ati = Softmax (u t i) (m \u00b2 i = 1 Watit (4) (2) (W2) (U \u00b2 (2) (t) (1)."}, {"heading": "2.3 Key-Value Knowledge Base Retrieval", "text": "It is a question of whether it is a question of a way in which the people of the world are concerned not only with the world, but also with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world, with the world."}, {"heading": "3 A Multi-Turn, Multi-Domain Dialogue Dataset", "text": "In an effort to continue working on multi-domain dialog agents, we have built up a corpus of multi-turn dialogs in three different areas: calendar planning, weather information search, and point-of-interest navigation. Although these areas are different, they are all relevant to the overarching theme of tasks that users would expect from a sophisticated in-car personal assistant."}, {"heading": "3.1 Data Collection", "text": "The data for the multi-turn dialogs were collected by presenting a Wizard-of-Oz plan (Wen et al., 2016b). In our scheme, users had two potential modes to play: Driver and Car Assistant. In Driver mode, users were confronted with a task they were trying to extract from the car, as well as the dialogue history that was exchanged between the drivers and the auto assistant. An exemplary task could be: to find out what the temperature in San Mateo is, over the next two days, and the driver was then only responsible for contributing a single line of dialogue that adequately continued the discourse and defined the task. Tasks were randomly selected by selecting the values (5pm, Saturday, San Francisco, etc.) responsible for three to five slots, location, etc."}, {"heading": "4 Related Work", "text": "A number of papers by (Young et al., 2013) have addressed the problem through partially observable Markov decision-making processes and reinforcing learning with carefully designed spaces of action, although the number of different states of action often makes this approach brittle and computationally intractable. Recent successes of neural architectures on a number of traditional subtasks for processing natural language (Bahdanau et al., 2015; Sutskever et al., 2014; Vinyals et al., 2015) have motivated research on dialog agents that can effectively utilize distributed neural representations for dialogue state management, belief recording, and response generation. Recent work by (Wen et al., 2016b) has focused on systems with modular representations, states of belief, and generational components. These models learn to represent the intention of the user through intermediate monitoring, which end-to-end traverses. Other works by (Boron and Perez) are more recent (Boron 2016)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Details", "text": "To reduce lexical variability, in a pre-processing step, we map the variant surface expression of entities into a canonical form using named entity recognition and linkage. For example, the surface shape of 20 Main Street is mapped to the address of Pizza My Heart. During inference, our model outputs the canonical shapes of entities, and so we realize their surface shapes by performing the system output through an inverted lexicon. The inverted lexicon transforms the entities back into their surface shapes by taking samples from a multinomic distribution whose distribution parameters correspond to the frequency of a given surface shape for an entity as observed in the training and validation data."}, {"heading": "5.2 Hyperparameters", "text": "We trained using a cross-entropy loss and the Adam optimizer (Kingma and Ba, 2015) with learning rates from the interval [10 \u2212 4, 10 \u2212 3]. We applied dropout (Hinton et al., 2012) as regularization factor to the LSTM input and output. We also added an l2 regularization penalty for the model weights. We identified hyperparameters by random search and evaluated the validation subset of the data sustained. Dropout rates were sampled from [0.8, 0.9] and the L2 coefficient was sampled from [3 \u00b7 10 \u2212 6, 10 \u2212 5]. We used word embedding, hidden layers and cell sizes with a size of 200. We applied grade snippets with a client value of 10 to avoid gradient explosions during training. We used word embedding, hidden layers and cell sizes with a value of 200. We applied gradient snippets with a client value of 10 to avoid gradient explosions during training."}, {"heading": "5.3 Baseline Models", "text": "We provide several basic models for comparing the performance of the Key-Value Retrieval Network: \u2022 Rule-Based Model: This model is a traditional heuristic system with modular dialog status trackers, KB queries and components for generating natural language. It first performs a comprehensive domain-dependent keyword search in the user's utterances to detect intention. User utterances are also made available to a lexicon to extract all of the aforementioned units. Taken together, this information represents the dialog state up to a certain point in the dialog. This dialog state is used to query the KB if necessary, and the returned KB values are used to fill in predefined system responses. \u2022 Copy-Augmented Sequence-to-Sequence Network: This model is derived from the work of (Eric and Manning, 2017). It supplements a sequence-to-sequence architecture with encoder attention from the encoder, with an additional merit-based dialog that represents the KB hardcode encoding mechanism."}, {"heading": "5.4 Automatic Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.4.1 Metrics", "text": "Although previous work has shown that automated evaluation metrics often correlate poorly with human evaluations of dialogue agents (Liu et al., 2016), we report in Table 3. These metrics are provided for coarse-grained evaluations of dialogue word quality: \u2022 BLEU: We use the BLEU metric commonly used in evaluating machine translation systems (Papineni et al., 2002), which has also been used in the past to evaluate dialogue systems (Ritter et al., 2011; Li et al., 2016). We calculate the average BLEU value for all answers generated by the system and report primarily on these values to measure the ability of our model to accurately generate the speech patterns seen in our data. \u2022 Entity F1: Each human door's car assistant in the test data defines a gold set of entities. To calculate an entity F1, we calculate the microaverage across the entire set of system dialogues."}, {"heading": "5.4.2 Results", "text": "We see that our basic models, Copy Net, have the lowest aggregated entity in the world. Although they have the highest entity in the world, they are very weak in other areas, which indicates that they are not able to achieve their goals."}, {"heading": "5.5 Human Evaluation", "text": "We randomly generated 120 different scenarios across the three dialog areas, where a scenario is defined by an underlying KB and a user target for the dialog (e.g. finding the nearest gas station to avoid heavy traffic), and then linked Amazon Mechanical Turkers to one of our systems in a real-time chat environment in which each Turk played the role of the driver. We evaluated the rule-based Copy Net model and the key value retrieval network in each of the 120 scenarios, and coupled one Turk with another Turk for each of the scenarios to get ratings of human performance. At the end of the chat, the Turk was asked to rate the quality of his partner based on fluctuation, cooperation, and humanity on a scale from 1 to 5. The averages per pair are shown in Table 4.We see that the key value retrieval model outperforms the base models on all metrics, with particularly significant performance gains over the net comparability being based on human-only."}, {"heading": "6 Conclusion and Future Work", "text": "In this work, we have presented a novel neural task-oriented dialogue model that is capable of sustaining grounded discourse in a variety of areas by retrieving the knowledge presented in the world's knowledge databases. It smoothly integrates this world knowledge into natural-sounding system responses in a consistently traceable manner, without explicitly modelling the state of dialogue. Our model surpasses competitive heuristic and neuronal baselines in both automatic and human evaluation metrics. In addition, we have introduced a publicly accessible dialogue dataset in three areas in the personal assistance room in the car, which we hope will address the problem of data scarcity in task-oriented dialogue research.Future work will focus on closing the gap between the key-value retrieval network and human performance in the various metrics, including the development of new methods for robustly dealing with shared KB attributes, as well as a more pragmatic understanding of KB usage."}], "references": [{"title": "Frames: A corpus for adding memory to goal-oriented dialogue systems", "author": ["L. El Asri", "H. Schulz", "S. Sharma", "J. Zumer", "J. Harris", "E. Fine", "R. Mehrotra", "K. Suleman."], "venue": "http://www.maluuba.com/publications/ .", "citeRegEx": "Asri et al\\.,? 2017", "shortCiteRegEx": "Asri et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzimitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR2015).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The carnegie mellon communicator corpus", "author": ["Christina Bennett", "Alexander I. Rudnicky."], "venue": "ICSLP.", "citeRegEx": "Bennett and Rudnicky.,? 2002", "shortCiteRegEx": "Bennett and Rudnicky.", "year": 2002}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1605.07683 .", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng."], "venue": "arXiv preprint arXiv:1609.00777 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue", "author": ["Mihail Eric", "Christopher Manning."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computa-", "citeRegEx": "Eric and Manning.,? 2017", "shortCiteRegEx": "Eric and Manning.", "year": 2017}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "The atis spoken language systems pilot corpus", "author": ["C.T. Hemphill", "J.J. Godfrey", "G.R. Doddington."], "venue": "Proceedings of the DARPA speech and natural language workshop.", "citeRegEx": "Hemphill et al\\.,? 1990", "shortCiteRegEx": "Hemphill et al\\.", "year": 1990}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of the SIGDIAL 2014 Conference.", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580 .", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber."], "venue": "Neural Computation pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Smart reply: Automated response suggestion for email", "author": ["Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufman", "Balint Miklos", "Greg Corrado", "Andrew Tomkins", "Laszlo Lukacs", "Marina Ganea", "Peter Young", "Vivek Ramavajjala"], "venue": null, "citeRegEx": "Kannan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2016}, {"title": "Adam: a method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "International Conference on Learning Representations (ICLR2015).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Phil Blunsom", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Fumin Wang", "Andrew Senior."], "venue": "Proceedings of the 54th Annual Meeting of the As-", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "Proceedings of", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Gated endto-end memory networks", "author": ["Fei Liu", "Julien Perez."], "venue": "arXiv preprint arXiv:1610.04211 .", "citeRegEx": "Liu and Perez.,? 2016", "shortCiteRegEx": "Liu and Perez.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing pages 1412\u2013 1421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natu-", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Theodore Bluche", "Christopher Kermorvant", "Jerome Louradour."], "venue": "arXiv preprint arXiv:1312.4569v2 .", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B. Dolan."], "venue": "Empirical Methods in Natural Language Processing pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Random walk initialization for training very deep feed forward networks", "author": ["David Sussillo", "L.F. Abbott."], "venue": "arXiv preprint arXiv:1412.6558 .", "citeRegEx": "Sussillo and Abbott.,? 2015", "shortCiteRegEx": "Sussillo and Abbott.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141 ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "David Vandyke", "Milica Gasic", "Nikola Mrksic", "Lina. M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562 .", "citeRegEx": "Wen et al\\.,? 2016b", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Hybrid code networks: practical", "author": ["Jason D. Williams", "Kavosh Asadi", "Geoffrey Zweig"], "venue": null, "citeRegEx": "Williams et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "The dialog state tracking challenge", "author": ["Jason D. Williams", "Antoine Raux", "Deepak Ramachadran", "Alan Black."], "venue": "Proceedings of the SIGDIAL. Metz, France.", "citeRegEx": "Williams et al\\.,? 2013", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "POMDP-based statistical spoken dialog systems: a review", "author": ["Steve Young", "Milica Gasic", "Blaise Thomson", "Jason D. Williams."], "venue": "Proceedings of the IEEE 28(1):114\u2013133.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Neural dialogue agents present one of the most promising avenues for leveraging dialogue corpora to build statistical models directly from data by using powerful distributed representations (Bordes and Weston, 2016; Wen et al., 2016b; Dhingra et al., 2016).", "startOffset": 190, "endOffset": 256}, {"referenceID": 27, "context": "Neural dialogue agents present one of the most promising avenues for leveraging dialogue corpora to build statistical models directly from data by using powerful distributed representations (Bordes and Weston, 2016; Wen et al., 2016b; Dhingra et al., 2016).", "startOffset": 190, "endOffset": 256}, {"referenceID": 4, "context": "Neural dialogue agents present one of the most promising avenues for leveraging dialogue corpora to build statistical models directly from data by using powerful distributed representations (Bordes and Weston, 2016; Wen et al., 2016b; Dhingra et al., 2016).", "startOffset": 190, "endOffset": 256}, {"referenceID": 20, "context": "rent network architectures with an attention-based key-value retrieval mechanism over the entries of a knowledge base, which is inspired by recent work on key-value memory networks (Miller et al., 2016).", "startOffset": 181, "endOffset": 202}, {"referenceID": 13, "context": "Unlike other dialogue agents which only rely on prior dialogue history for generation (Kannan et al., 2016; Eric and Manning, 2017), our architecture is able to access and use database-style information, while still retaining the text generation advantages of recent neural models.", "startOffset": 86, "endOffset": 131}, {"referenceID": 5, "context": "Unlike other dialogue agents which only rely on prior dialogue history for generation (Kannan et al., 2016; Eric and Manning, 2017), our architecture is able to access and use database-style information, while still retaining the text generation advantages of recent neural models.", "startOffset": 86, "endOffset": 131}, {"referenceID": 27, "context": "user intent trackers (Wen et al., 2016b; Dhingra et al., 2016; Henderson et al., 2014), we choose instead to rely on learned neural representations for implicit modelling of dialogue state, forming a truly end-to-end trainable system.", "startOffset": 21, "endOffset": 86}, {"referenceID": 4, "context": "user intent trackers (Wen et al., 2016b; Dhingra et al., 2016; Henderson et al., 2014), we choose instead to rely on learned neural representations for implicit modelling of dialogue state, forming a truly end-to-end trainable system.", "startOffset": 21, "endOffset": 86}, {"referenceID": 9, "context": "user intent trackers (Wen et al., 2016b; Dhingra et al., 2016; Henderson et al., 2014), we choose instead to rely on learned neural representations for implicit modelling of dialogue state, forming a truly end-to-end trainable system.", "startOffset": 21, "endOffset": 86}, {"referenceID": 11, "context": "ory unit, as described by (Hochreiter and Schmidhuber, 1997).", "startOffset": 26, "endOffset": 60}, {"referenceID": 1, "context": "We extend this decoder with an attentionbased model (Bahdanau et al., 2015; Luong et al., 2015a), where, at every time step t of the decoding, an attention score ai is computed for each hidden state hi of the encoder, using the attention mechanism of (Vinyals et al.", "startOffset": 52, "endOffset": 96}, {"referenceID": 19, "context": "We extend this decoder with an attentionbased model (Bahdanau et al., 2015; Luong et al., 2015a), where, at every time step t of the decoding, an attention score ai is computed for each hidden state hi of the encoder, using the attention mechanism of (Vinyals et al.", "startOffset": 52, "endOffset": 96}, {"referenceID": 26, "context": ", 2015a), where, at every time step t of the decoding, an attention score ai is computed for each hidden state hi of the encoder, using the attention mechanism of (Vinyals et al., 2015).", "startOffset": 163, "endOffset": 185}, {"referenceID": 4, "context": ", 2016b), or 2) softly attend to the KB and combine this probability distribution with belief trackers as state input for a reinforcement learning policy (Dhingra et al., 2016).", "startOffset": 154, "endOffset": 176}, {"referenceID": 20, "context": "For storing the KB of a given dialogue, we take inspiration from the work of (Miller et al., 2016) which found that a key-value structured memory allowed for efficient machine reading of documents.", "startOffset": 77, "endOffset": 98}, {"referenceID": 12, "context": "Recent literature has shown that incorporating a copying mechanism into neural architectures improves performance on various sequenceto-sequence tasks (Jia and Liang, 2016; Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016; Eric and Manning, 2017).", "startOffset": 151, "endOffset": 255}, {"referenceID": 6, "context": "Recent literature has shown that incorporating a copying mechanism into neural architectures improves performance on various sequenceto-sequence tasks (Jia and Liang, 2016; Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016; Eric and Manning, 2017).", "startOffset": 151, "endOffset": 255}, {"referenceID": 16, "context": "Recent literature has shown that incorporating a copying mechanism into neural architectures improves performance on various sequenceto-sequence tasks (Jia and Liang, 2016; Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016; Eric and Manning, 2017).", "startOffset": 151, "endOffset": 255}, {"referenceID": 7, "context": "Recent literature has shown that incorporating a copying mechanism into neural architectures improves performance on various sequenceto-sequence tasks (Jia and Liang, 2016; Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016; Eric and Manning, 2017).", "startOffset": 151, "endOffset": 255}, {"referenceID": 5, "context": "Recent literature has shown that incorporating a copying mechanism into neural architectures improves performance on various sequenceto-sequence tasks (Jia and Liang, 2016; Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016; Eric and Manning, 2017).", "startOffset": 151, "endOffset": 255}, {"referenceID": 27, "context": "The data for the multi-turn dialogues was collected using a Wizard-of-Oz scheme inspired by that of (Wen et al., 2016b).", "startOffset": 100, "endOffset": 119}, {"referenceID": 30, "context": "One line of work by (Young et al., 2013) has tackled the problem using partially observable Markov decision processes and reinforcement", "startOffset": 20, "endOffset": 40}, {"referenceID": 1, "context": "The recent successes of neural architectures on a number of traditional natural language processing subtasks (Bahdanau et al., 2015; Sutskever et al., 2014; Vinyals et al., 2015) have motivated investigation into dialogue agents that can", "startOffset": 109, "endOffset": 178}, {"referenceID": 25, "context": "The recent successes of neural architectures on a number of traditional natural language processing subtasks (Bahdanau et al., 2015; Sutskever et al., 2014; Vinyals et al., 2015) have motivated investigation into dialogue agents that can", "startOffset": 109, "endOffset": 178}, {"referenceID": 26, "context": "The recent successes of neural architectures on a number of traditional natural language processing subtasks (Bahdanau et al., 2015; Sutskever et al., 2014; Vinyals et al., 2015) have motivated investigation into dialogue agents that can", "startOffset": 109, "endOffset": 178}, {"referenceID": 27, "context": "Recent work by (Wen et al., 2016b) has built systems with modularly-connected representation, belief state,", "startOffset": 15, "endOffset": 34}, {"referenceID": 3, "context": "Other work by (Bordes and Weston, 2016; Liu and Perez, 2016) stores dialogue context in a", "startOffset": 14, "endOffset": 60}, {"referenceID": 18, "context": "Other work by (Bordes and Weston, 2016; Liu and Perez, 2016) stores dialogue context in a", "startOffset": 14, "endOffset": 60}, {"referenceID": 28, "context": "oriented models which are amenable to both supervised learning and reinforcement learning and are able to incorporate domain-specific knowledge via explicitly-provided features and model-output restrictions (Williams et al., 2017).", "startOffset": 207, "endOffset": 230}, {"referenceID": 8, "context": "Several classical corpora have consisted of moderately-sized collections of dialogues related to travel-booking (Hemphill et al., 1990; Bennett and Rudnicky, 2002).", "startOffset": 112, "endOffset": 163}, {"referenceID": 2, "context": "Several classical corpora have consisted of moderately-sized collections of dialogues related to travel-booking (Hemphill et al., 1990; Bennett and Rudnicky, 2002).", "startOffset": 112, "endOffset": 163}, {"referenceID": 29, "context": "(Williams et al., 2013).", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "were designed to test systems for state tracking, recent work has chosen to repurpose this data by only using the transcripts of dialogues without state annotation for developing systems (Bordes and Weston, 2016; Williams et al., 2017).", "startOffset": 187, "endOffset": 235}, {"referenceID": 28, "context": "were designed to test systems for state tracking, recent work has chosen to repurpose this data by only using the transcripts of dialogues without state annotation for developing systems (Bordes and Weston, 2016; Williams et al., 2017).", "startOffset": 187, "endOffset": 235}, {"referenceID": 0, "context": "travel-booking dialogues collected in a Wizard-ofOz Scheme with elaborate semantic frames annotated (Asri et al., 2017).", "startOffset": 100, "endOffset": 119}, {"referenceID": 14, "context": "We trained using a cross-entropy loss and the Adam optimizer (Kingma and Ba, 2015) with learning rates sampled from the interval [10\u22124, 10\u22123].", "startOffset": 61, "endOffset": 82}, {"referenceID": 10, "context": "We applied dropout (Hinton et al., 2012) as a regularizer to the input and output of the LSTM.", "startOffset": 19, "endOffset": 40}, {"referenceID": 24, "context": "distribution in the style of (Sussillo and Abbott, 2015).", "startOffset": 29, "endOffset": 56}, {"referenceID": 22, "context": "We also added a bias of 1 to the LSTM cell forget gate in the style of (Pham et al., 2014).", "startOffset": 71, "endOffset": 90}, {"referenceID": 5, "context": "\u2022 Copy-Augmented Sequence-to-Sequence Network: This model is derived from the work of (Eric and Manning, 2017).", "startOffset": 86, "endOffset": 110}, {"referenceID": 5, "context": "Unlike the best performing model of (Eric and Manning, 2017), we do not enhance the inputs to the encoder with additional entity type features, as we found that the model performed worse on our data with this", "startOffset": 36, "endOffset": 60}, {"referenceID": 17, "context": "assessments of dialogue agents (Liu et al., 2016), we report a number of automatic metrics in Table 3.", "startOffset": 31, "endOffset": 49}, {"referenceID": 21, "context": "\u2022 BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al.", "startOffset": 92, "endOffset": 115}], "year": 2017, "abstractText": "Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rulebased system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.", "creator": "LaTeX with hyperref package"}}}