{"id": "1705.05551", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "New Reinforcement Learning Using a Chaotic Neural Network for Emergence of \"Thinking\" - \"Exploration\" Grows into \"Thinking\" through Learning -", "abstract": "Expectation for the emergence of higher functions is getting larger in the framework of end-to-end reinforcement learning using a recurrent neural network. However, the emergence of \"thinking\" that is a typical higher function is difficult to realize because \"thinking\" needs non fixed-point, flow-type attractors with both convergence and transition dynamics. Furthermore, in order to introduce \"inspiration\" or \"discovery\" in \"thinking\", not completely random but unexpected transition should be also required.", "histories": [["v1", "Tue, 16 May 2017 06:54:04 GMT  (2015kb)", "http://arxiv.org/abs/1705.05551v1", "The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2017, 5 pages, 6 figures"]], "COMMENTS": "The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2017, 5 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["katsunari shibata", "yuki goto"], "accepted": false, "id": "1705.05551"}, "pdf": {"name": "1705.05551.pdf", "metadata": {"source": "CRF", "title": "New Reinforcement Learning Using a Chaotic Neural Network for Emergence of \u201cThinking\u201d \u2014 \u201cExploration\u201d Grows into \u201cThinking\u201d through Learning \u2014", "authors": ["Katsunari Shibata", "Yuki Goto"], "emails": ["katsunarishibata@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.05 551v 1 [cs.A I] 1 6M ayIn analogy to \"chaotic itinerary,\" we hypothesized that \"exploration\" becomes \"thinking\" by learning, by building flowing attractors on chaotic random dynamics. It is expected that when rational dynamics are learned in a chaotic neural network (ChNN), the coexistence of rational state transitions, inspiring state transitions, and also random explorations for unknown situations can be realized. Based on the above idea, we have proposed new amplification factors that can be learned by means of a ChNN. The positioning of exploration is completely different from the conventional one. Here, the chaotic dynamic within the ChNN itself generates exploration factors. Since external random numbers are not used for stochastic action selection, exploration factors cannot be completely separated from the learning factor. Therefore, the combination of KNN and KNN is also completely different from the conventional one."}, {"heading": "Acknowledgements", "text": "This research was supported by JSPS KAKENHI grant numbers JP23500245, JP15K00360 and many of our group members * http: / / shws.cc.oita-u.ac.jp / \u02dc shibata / home.html"}, {"heading": "1 Introduction", "text": "The expectation for the emergence of artificial intelligence is growing these days, triggered by the recent results of reinforcement learning (RL) using a deep neural network (NN) [1, 2]. For about 20 years, our group has proposed that the eventual RL from sensors to motors that use a recurring NN (RNN) plays an important role in the emergence [3, 4]. In particular, unlike \"recognition,\" whose inputs are given as sensor signals or \"control,\" whose outputs are given as motor commands, higher functions are very difficult for human hands to design, and the approach to function generation by end-to-end RL is highly anticipated. Our group has shown that not only recognition and movement, but also memory, prediction, individuality and also activities similar to those occurring in the monkey brain during the use of tools [4]. We have also shown that a multitude of communication approaches arise within the same framework [5]."}, {"heading": "2 Difficulty in Emergence of \u201cThinking\u201d", "text": "The definition of \"thinking\" needs to be varied depending on the person. However, we can \"think\" even if we close our eyes and ears, and what we think does not change randomly, but logically or rationally. Therefore, we hope that many agree that to realize \"thinking\" rational multi-stage or fluid state transitions should be formed. As a kind of dynamic function, we have shown that in a RNN for simple tasks a multitude of memory functions arise [4]. It is not so difficult to form memories as fixed point convergence dynamics if the initial feedback link weights are set in such a way that the transition matrix for the connection is the identity matrix or close to it when approached linearly. This can also trigger the vanishing gradient problem in the error spread."}, {"heading": "3 Chaos and Hypothesis: Growth from \u201cExploration\u201d to \u201cThinking\u201d through Learning", "text": "Suppose we are standing on a fork-shaped road, as shown in Figure 1. Normally, we choose one of two options: to the right and to the left. We don't care about many other possible actions such as going straight and dancing. It's not the way we leave the goal, but the way we reach it. \"This can be considered a kind of\" exploration \"and a kind of\" thinking. \"The place where the wave takes place in the head is within the process before it makes a decision, and it looks like it is moving away from the goal."}, {"heading": "4 New Reinforcement Learning (RL) Using a Chaotic Neural Network (ChNN)", "text": "The positioning of the exploration in learning is completely different from the conventional one. Here, the chaotic dynamics inside the ChNN are actually the exploration factors in themselves. Since external random numbers are not used for stochastic action selection, exploration factors cannot be isolated from the output. Then, the learning method must be completely different from the conventional method. \u2212 Assuming that the movements are continuous, robot critique types are used as reinforcement of the learning architecture. To isolate the chaotic dynamics from the critic, the actor is implemented in a ChNN and the critic is implemented in another regular layer of the NN, as in Fig. 5. The inputs are the sensor signals for both networks. Here is only the learning of the actor ChNN, which is largely different from conventional amplification, explained."}], "references": [{"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A Huang"], "venue": "search, Nature,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Emergence of Intelligence through Reinforcement Learning ..., Advances in Reinforcement Learning, Intech, 99\u2013120", "author": ["K. Shibata"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Functions that Emerge through End-to-end Reinforcement Learning, RLDM 2017", "author": ["K. Shibata"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "A Variety of Communications that Emerge through Reinforcement Learning Using ..., RLDM 2017", "author": ["K. Shibata"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Reinforcement Learning with Internal-Dynamics-based Exploration Using a Chaotic Neural Network", "author": ["K. Shibata", "Y. Sakashita"], "venue": "Proc. of IJCNN (Int\u2019l Joint Conf. on Neural Networks)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Significance of Function Emergence Approach based on End-to-end Reinforcement Learning as suggested by Deep Learning, and Novel Reinforcement Learning Using a Chaotic ..", "author": ["K. Shibata", "Y. Goto"], "venue": "Cognitive Studies,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Emergence of Discrete and Abstract State ..", "author": ["Y Sawatsubashi"], "venue": "Robot Intelligence Tech. and App", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Emergence of Higher Exploration in Reinforcement Learning Using a Chaotic Neural Network", "author": ["Y. Goto", "K. Shibata"], "venue": "Neural Information Processing, Lecture Notes in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Learning of Deterministic Exploration and Temporal Abstraction", "author": ["K. Shibata"], "venue": "Proc. of SICE-ICCAS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "How brains make chaos in order to make sense ..", "author": ["C.A. Skarda", "W.J. Freeman"], "venue": "Behavioral and Brain Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Successive Learning in hetero-associative memory using chaotic neural networks", "author": ["Y. Osana", "M. Hagiwara"], "venue": "Int. J. Neural Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Expectation for the emergence of artificial intelligence is growing these days triggered by the recent results in reinforcement learning (RL) using a deep neural network (NN)[1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 1, "context": "Our group has propounded for around 20 years that end-toend RL from sensors to motors using a recurrent NN (RNN) plays an important role for the emergence[3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 2, "context": "Our group has propounded for around 20 years that end-toend RL from sensors to motors using a recurrent NN (RNN) plays an important role for the emergence[3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 2, "context": "Our group has shown that not only recognition and motion, but also memory, prediction, individuality, and also similar activities to those in monkey brain at tool use emerge[4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "We have also shown that a variety of communications emerge in the same framework[5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "Then our hypothesis that \u201cexploration\u201d grows into \u201cthinking\u201d through learning is introduced[6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "To realize the hypothesis, the use of a chaotic NN (ChNN) in RL and new deterministic RL for it are introduced[6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 5, "context": "Finally, it is shown that the new RL works in a simple task[7] though that cannot be called \u201cthinking\u201d yet and there are still many rooms for improvement.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "As a kind of dynamic functions, we have shown that a variety of memory-required functions emerge in an RNN in simple tasks[4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "After learning, a large change in the internal state could be observed in some degree, but the learning was very difficult[8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "It is not motor(actuator)-level lower exploration, but higher exploration supported by some prior or learned knowledge[9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "The author\u2019s group has thought that exploration should be generated inside of a recurrent NN (RNN) that generates motion commands[10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "reported that the activities on the olfactory bulb in rabbits become chaotic for unknown stimuli[12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "have shown the difference of chaotic property between known and unknown patterns on an associative memory using a ChNN, and also after an unknown pattern is learned, association to the pattern is formed as well as the other known patterns[13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 4, "context": "In order to realize the above idea, our group has proposed new reinforcement learning using a ChNN[6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Figure 4: Comparison of conventional RL and proposed RL(only actor network) [6]", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "It was already applied to several tasks[6][9].", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "It was already applied to several tasks[6][9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "In this paper, the result of a recent task in which a robot has two wheels and two visual sensors is shown[7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "Figure 6: Learning results[7].", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "In [6], it was observed that when the environment changed, Lyapunov exponent increased again.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "Expectation for the emergence of higher functions is getting larger in the framework of end-to-end comprehensive reinforcement learning using a recurrent neural network. However, the emergence of \u201cthinking\u201d that is a typical higher function is difficult to realize because \u201cthinking\u201d needs non fixed-point, flow-type attractors with both convergence and transition dynamics. Furthermore, in order to introduce \u201cinspiration\u201d or \u201cdiscovery\u201d in \u201cthinking\u201d, not completely random but unexpected transition should be also required. By analogy to \u201cchaotic itinerancy\u201d, we have hypothesized that \u201cexploration\u201d grows into \u201cthinking\u201d through learning by forming flow-type attractors on chaotic random-like dynamics. It is expected that if rational dynamics are learned in a chaotic neural network (ChNN), coexistence of rational state transition, inspiration-like state transition and also random-like exploration for unknown situation can be realized. Based on the above idea, we have proposed new reinforcement learning using a ChNN. The positioning of exploration is completely different from the conventional one. Here, the chaotic dynamics inside the ChNN produces exploration factors by itself. Since external random numbers for stochastic action selection are not used, exploration factors cannot be isolated from the output. Therefore, the learning method is also completely different from the conventional one. One variable named causality trace is put at each connection, and takes in andmaintains the input through the connection according to the change in its output. Using these causality traces and TD error, the connection weights except for the feedback connections are updated in the actor ChNN. In this paper, as the result of a recent simple task to see whether the new learning works appropriately or not, it is shown that a robot with two wheels and two visual sensors reaches a target while avoiding an obstacle after learning though there are still many rooms for improvement.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}