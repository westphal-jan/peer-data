{"id": "1705.05637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Text-based Adventures of the Golovin AI Agent", "abstract": "The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments.", "histories": [["v1", "Tue, 16 May 2017 10:55:08 GMT  (106kb,D)", "http://arxiv.org/abs/1705.05637v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bartosz kostka", "jaroslaw kwiecien", "jakub kowalski", "pawel rychlikowski"], "accepted": false, "id": "1705.05637"}, "pdf": {"name": "1705.05637.pdf", "metadata": {"source": "CRF", "title": "Text-based Adventures of the Golovin AI Agent", "authors": ["Bartosz Kostka", "Jaros\u0142aw Kwiecie\u0144", "Jakub Kowalski", "Pawe\u0142 Rychlikowski"], "emails": ["bartosz.kostka@stud.cs.uni.wroc.pl,", "jaroslaw.kwiecien@stud.cs.uni.wroc.pl,", "jko@cs.uni.wroc.pl", "prych@cs.uni.wroc.pl"], "sections": [{"heading": null, "text": "In this article, we present our approach to tackling the problem. Our agent, called Golovin, takes advantage of the limited scope. We use genre-related corporations (including fantasy books and decompiled games) to develop language models that are suitable for this domain. In addition, we are able to solve important tasks such as fighting invasion and navigating the board. We have confirmed the usefulness of these mechanisms by pinpointing the performance of the player. Finally, we show that our agent is playing on a level comparable to last year's winner."}, {"heading": "II. BACKGROUND", "text": "A. Interactive FictionInteractive Fiction (IF), originated in the 1970s, is a domain of text-based adventure or role-playing games in which the player uses text commands to control characters and influence the environment. One of the most famous examples is the Zork series, developed by Infocom. Formally, these are single-player, non-deterministic games with imperfect information. If the genre is closely related to MUDs (multi-user dungeons), but (as a single-player) is more focused on plot and puzzles than on fighting and interacting with other players. IF were popular until the late 1980s, when the graphical interfaces were available and so much more user-friendly, popular. Nevertheless, new IF games are still being created, and there are annual competitions for game designers (such as The Interactive Fiction Competition). When games generally (but not always) take place, they are played in some fantasy environments."}, {"heading": "B. Playing Text-Based Games", "text": "Although the challenge of playing text-based games is not often enough, there are several attempts described in the literature, mostly based on the MUD games and not on the classic IF games. Adventure games have been carefully reworked as a field of study for cognitive robotics. First, the authors identify the characteristics of the \"traditional adventure game environment\" to work out the specifics of the domain. Second, they count and discuss the existing challenges, including the need for healthy knowledge (learning, reworking, organizing and using), gradually revealing the state space and the scope for action to work out the specifics of the reward. In [13] the agent, who is able to live and survive in an existing MUD game, a layered architecture has been used: a sophisticated planning system consisting of reasoning mechanisms based on craft trees and action spaces, specifications and rewards."}, {"heading": "C. Natural Language Processing", "text": "Alan Turing states (roughly) in his famous paper [21] that \"intelligent behavior\" means \"understanding natural language and using it correctly in conversations with humans.\" Thus, the Turing test is the way to verify whether computers have acquired strong AI skills. Initial systems for processing natural language were based on rules. Thanks to the growing amount of text data and the increase in computer performance, the shift toward data-driven approaches (statistical or machine learning) can be observed in recent decades. Nowadays, NLP is very often performed \"almost from the ground up,\" as happened when [22] the authors used neural networks to solve many NLP tasks, including part-of-speech tagging, so-called entity recognition and semantic role labeling. The basis for this model was the neural language model (as a side effect), which is used by neural models."}, {"heading": "D. The Text-Based Adventure AI Competition", "text": "The first step in the right direction is to be able to move in the right direction, and to be able to move in the right direction, \"he told Welt am Sonntag."}, {"heading": "III. THE GAME PLAYING AGENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Overview", "text": "Our agent is characterized by the following features: \u2022 he uses a large number of predefined command patterns based on the selection of fantasy books; \u2022 he uses the game-specific behaviors that are natural for adventure games, such as combat mode, device management, motion strategy; \u2022 he uses 2http: / / frotz.sourceforge.net. 3The agent is open source and is available at https: / / github.com / danielricks / BYU-Agent-2016. \u2022 he stores and uses some aspects of the current game history; \u2022 he tries to imitate human behavior: After playing several games and exploring the game universe, he repeats the most promising sequence of commands. We treat the result achieved in this final process as a result of this game. The agent was called \"Golovin,\" your name being one of the answers that Dovin gave when he asked hey."}, {"heading": "B. Preprocessing", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "C. Playing Algorithm", "text": "eeisrcnlhsrc\u00fciieD rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "IV. EXPERIMENTS", "text": "Our experiments had two main objectives: to create the most effective means and to analyze how some parameters affect the performance of the agents. However, the most natural method of measuring the performance of the agents is to use the score given by the game (divided by the maximum score if we want to compare different games). However, there are many games in which our agent (as well as BYUAgent) has problems with not getting zero points. Therefore, we have decided to reward each positive score and to add an arbitrarily chosen constant of 0.2 to the positive mean. Therefore, the optimal (hypothetical) agent would receive 1.2 points in each game.We have selected 20 games for training purposes, for all of which the maximum score is known. The performance of the mean is an average (modified) score calculated on the basis of these games."}, {"heading": "A. Creating The Best Agent", "text": "The game of the agent is determined by some parameters, for example: \u2022 the amount of command patterns, \u2022 the number of synonyms from word2vec, \u2022 the number of elements we want to collect after visiting a new location, \u2022 the number of standard commands that are tried after the collection phase, \u2022 the number of commands that contain many words from the description (the actual reward is bk, where k is the number of such words, and b is a positive constant), \u2022 the number of commands that contain words that are to be used without good reason (neither in the state description nor in generated synonyms), the score is divided by pk, where k is the number of such words, and p is a constant, \u2022 how many commands should be executed before we try out motion commands. Furthermore, we wanted to check whether the use of combat mode or a map has an observable effect on the performance of the agent. The number of parameter combinations was equal to 0.2 when we decided on the total score search for 001 (we have specified 00.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000"}, {"heading": "B. Evaluation of Domain-based Mechanisms", "text": "We wanted to check if some of our agent's advanced features have a noticeable impact on agent performance. We reviewed the following 4 configurations with combat mode on and card off. Results are shown in Figure 1. You can see that the card is useful (but only to a certain extent) and the combat mode is undoubtedly useful."}, {"heading": "C. Evaluation of Language Model Sources", "text": "The results are shown in Figure 2. The5Spearmint is a package that performs Bayesian hyperparameter optimization. It allows to treat the optimized function as a black box and tries to select the parameters for the next run taking into account the knowledge gained in previous runs. See [35]. Optimal configuration uses only two sources: T and W6. However, we still believe that decompiled games can be a useful source of game commands, but they cannot be found in descriptions, but in the command interpreter - which requires more advanced automated code analysis. We have left it as future work."}, {"heading": "D. Gameplay Examples", "text": "While we are playing detective, our agent finds himself in a closet. We get the following state description: Game: You are in a closet. There is a weapon on the floor. It is better to get it. To get out, go to East. Our agent determines items: closet, weapon, floor, exit. Our agent selects from the commands listed in Table I. We see that the most important thing for the agent is the weapon and how to take it, which is reasonable and even suggested by the game. In addition, the agent also tries to search with synonyms of the word weapon to find the correct commands (for example: we can see that he recognizes the weapon as a type of weapon, and some weapons, such as knives, can be sharpened). Fortunately, after using a command weapon, the agent has obtained a black small gun Golovin. Another example comes from zork3. We start with the following location description: The difference between T + W and G + W is not very large."}, {"heading": "E. The Comparison with BYU-Agent", "text": "Finally, we confirm our approach by comparing it to the BYU agent. We used the same set of 50 Z machine games7 as in [15].The results of the comparison are given in Table II. The BYU agent was trained for 1000 epochs (each epoch contains 1000 game steps), and its score was recorded after each epoch.Since the learning curves vary according to the game, including degeneration of results (see [15, Figure 5]), we have taken as the main measurement the maximum score achieved over all epochs. As for the Golovin, we limited its playing time to 1000 steps (i.e. an equivalent of an epoch) and use our common sense in restarting the mechanism. The results of the BYU agent are obtained using the verb and the algorithm for reducing the action space, except for games characterized by an asterisk where the asterisk space is shortened, which is how we present the action space."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "We have presented an agent who is able to play every interactive fiction game created for human players on a level comparable to the winner of last year's text-based adventure competition. Due to the number of domain-based mechanisms, our agent can successfully handle the game in a limited number of steps available. Results of the presented experiments show that the mechanisms we embed (combat mode, mapping) and a selection of learning sources actually improve the agent's performance. Although the results are promising, we are still at the beginning of the process of creating an agent who really understands the descriptions of natural language in order to play the text-based adventure games efficiently. There are several future directions of work that we would like to highlight. First, and one of the most important, is the embedding of a learning mechanism: in-game learning, where the functionality is restarted to improve the efficiency of the player in a particular game; and preliminary learning of playing groups we consider to be able to plan a very useful learning from the insights we gain from the agent's behavior."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Szymon Malik for his valuable contribution in the early stages of the development of Golovin. We would also like to thank Nancy Fulda for providing helpful answers to our questions and providing up-to-date results from the BYU agent."}], "references": [{"title": "Deep Blue", "author": ["M. Campbell", "A.J. Hoane", "F. Hsu"], "venue": "Artificial intelligence, vol. 134, no. 1, pp. 57\u201383, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "Nature, vol. 529, pp. 484\u2013503, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "General Game Playing: Overview of the AAAI Competition", "author": ["M. Genesereth", "N. Love", "B. Pell"], "venue": "AI Magazine, vol. 26, pp. 62\u201372, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "General Game Playing: Game Description Language Specification", "author": ["N. Love", "T. Hinrichs", "D. Haley", "E. Schkufza", "M. Genesereth"], "venue": "Stanford Logic Group, Tech. Rep., 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "A General Game Description Language for Incomplete Information Games", "author": ["M. Thielscher"], "venue": "AAAI Conference on Artificial Intelligence, 2010, pp. 994\u2013999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards a Real-time Game Description Language", "author": ["J. Kowalski", "A. Kisielewicz"], "venue": "International Conference on Agents and Artificial Intelligence, vol. 2, 2016, pp. 494\u2013499.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "The 2014 General Video Game Playing Competition", "author": ["D. Perez", "S. Samothrakis", "J. Togelius", "T. Schaul", "S. Lucas", "A. Cou\u00ebtoux", "J. Lee", "C. Lim", "T. Thompson"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 8, no. 3, pp. 229\u2013243, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "General Video Game AI: Competition, Challenges and Opportunities", "author": ["D. Perez", "S. Samothrakis", "J. Togelius", "T. Schaul", "S.M. Lucas"], "venue": "AAAI Conference on Artificial Intelligence, 2016, pp. 4335\u20134337.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "General Video Game Level Generation", "author": ["A. Khalifa", "D. Perez", "S. Lucas", "J. Togelius"], "venue": "Genetic and Evolutionary Computation Conference, 2016, pp. 253\u2013259.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergent Tangled Graph Representations for Atari Game Playing Agents", "author": ["S. Kelly", "M.I. Heywood"], "venue": "EuroGP 2017: Genetic Programming, ser. LNCS, 2017, vol. 10196, pp. 64\u201379.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "being-in-the-world", "author": ["M.A. DePristo", "R. Zubek"], "venue": "Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment, 2001, pp. 31\u201334.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Adventure games: A challenge for cognitive robotics", "author": ["E. Amir", "P. Doyle"], "venue": "Proc. Int. Cognitive Robotics Workshop, 2002, pp. 148\u2013 155.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "What can you do with a rock? Affordance extraction via word embeddings", "author": ["N. Fulda", "D. Ricks", "B. Murdoch", "D. Wingate"], "venue": "International Joint Conference on Artificial Intelligence, 2017, (to appear).", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Put my galakmid coin into the dispenser and kick it: Computational linguistics and theorem proving in a computer game", "author": ["A. Koller", "R. Debusmann", "M. Gabsdil", "K. Striegnitz"], "venue": "Journal of Logic, Language and Information, vol. 13, no. 2, pp. 187\u2013206, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Knowledge-gathering agents in adventure games", "author": ["B. Hlubocky", "E. Amir"], "venue": "AAAI-04 workshop on Challenges in Game AI, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Goal Achievement in Partially Known, Partially Observable Domains", "author": ["A. Chang", "E. Amir"], "venue": "Proceedings of the Sixteenth International Conference on International Conference on Automated Planning and Scheduling, 2006, pp. 203\u2013211.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Language Understanding for Text-based Games using Deep Reinforcement Learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1\u201311.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, vol. 59, no. 236, pp. 433\u2013460, 1950. [Online]. Available: http: //www.jstor.org/stable/2251299", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1950}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "CoRR, vol. abs/1103.0398, 2011. [Online]. Available: http://arxiv.org/abs/1103.0398", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, vol. abs/1301.3781, 2013. [Online]. Available: http://arxiv.org/abs/1301.3781", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ser. ACL \u201996. Stroudsburg, PA, USA: Association for Computational Linguistics, 1996, pp. 310\u2013318. [Online]. Available: http://dx.doi.org/ 10.3115/981863.981904", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JOURNAL OF MACHINE LEARNING RE- SEARCH, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Feed-forward networks with attention can solve some long-term memory problems", "author": ["C. Raffel", "D.P.W. Ellis"], "venue": "CoRR, vol. abs/1512.08756, 2015. [Online]. Available: http://arxiv.org/abs/1512.08756", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["J. Cheng", "L. Dong", "M. Lapata"], "venue": "CoRR, vol. abs/1601.06733, 2016. [Online]. Available: http://arxiv.org/abs/1601.06733", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Report on a general problem solving program", "author": ["A. Newell", "J.C. Shaw", "H.A. Simon"], "venue": "Proceedings of the International Conference on Information Processing, 1959, pp. 256\u2013264.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1959}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1992}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "2013, arXiv:1301.3781 [cs.CL].", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["A M."], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Application of artificial neural networks with attention mechanism for discovering distant dependencies in time series", "author": ["S. Malik"], "venue": "Bachelor Thesis, University of Wroc\u0142aw, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "author": ["S. Branavan", "D. Silver", "R. Barzilay"], "venue": "Journal of Artificial Intelligence Research, vol. 43, pp. 661\u2013704, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural Language Processing with Python, 1st ed", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2012, pp. 2951\u20132959.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "This strategy allowed to beat the single games which were set as the milestones for the AI development: Chess [1] and Go [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "This strategy allowed to beat the single games which were set as the milestones for the AI development: Chess [1] and Go [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Currently, there are two main, well-established GGP domains providing their own game specification languages and competitions [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "in 2005 and it is based on the Game Description Language (GDL), which can describe all finite, turn-based, deterministic games with full information [5], and its extensions (GDL-II [6] and rtGDL [7]).", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "in 2005 and it is based on the Game Description Language (GDL), which can describe all finite, turn-based, deterministic games with full information [5], and its extensions (GDL-II [6] and rtGDL [7]).", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "in 2005 and it is based on the Game Description Language (GDL), which can describe all finite, turn-based, deterministic games with full information [5], and its extensions (GDL-II [6] and rtGDL [7]).", "startOffset": 195, "endOffset": 198}, {"referenceID": 6, "context": "The second one is the General Video Game AI framework (GVGAI) from 2014, which focuses on arcade video games [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "ple tracks, including procedural content generation challenges [9], [10].", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "ple tracks, including procedural content generation challenges [9], [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "In some sense, this approach is coherent with the experiments on learning Atari 2600 games using the Arcade Learning Environment (ALE), where the agent\u2019s inputs were only raw screen capture and a score counter [11], [12].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "In some sense, this approach is coherent with the experiments on learning Atari 2600 games using the Arcade Learning Environment (ALE), where the agent\u2019s inputs were only raw screen capture and a score counter [11], [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 11, "context": "Although some approaches tackling similar problems exist since early 2000s ([13], [14]), we are still at the entry point for this kind of problems, which are closely related to the general problem solving.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Although some approaches tackling similar problems exist since early 2000s ([13], [14]), we are still at the entry point for this kind of problems, which are closely related to the general problem solving.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "Also, we tested our agent against the winner of the last year competition [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "Adventure games has been carefully revised as the field of study for the cognitive robotics in [14].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "In [13], the agent able to live and survive in an existing MUD game have been described.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "While not directly-related to playing algorithms, it is worth to note the usage of computational linguistics and theorem proving to build an engine for playing text-based adventure games [16].", "startOffset": 187, "endOffset": 191}, {"referenceID": 15, "context": "The approach focused on tracking the state of the world in text-based games, and translating it into the first-order logic, has been presented in [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "The extension of the above approach, presents the agent that can solve puzzle-like tasks in partially observable domain that is not known in advance, assuming actions are deterministic and without conditional effects [18].", "startOffset": 217, "endOffset": 221}, {"referenceID": 17, "context": "Recently, an advanced MUD playing agent has been described in [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "First, responsible for converting textual descriptions to state representation is based on the Long Short-term Memory (LSTM) networks [20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "Second, uses Deep Q-Networks [11] to learn approximated evaluations for each action in a given state.", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "Alan Turing in his famous paper [21] state (approximately) that \u201cexhibit", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "Nowadays, NLP very often is done \u201calmost from scratch\u201d, as it was done if [22] where the authors have used neural network in order to solve many NLP tasks, including part-of-speech tagging, named entity recognition and semantic role labeling.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "One of the most popular is the one proposed in [23] that uses very simple, linear language model and is suitable to large collections of texts.", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "This task was traditionally done using Markov models (with some smoothing procedures, see [24]).", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "We will use both word embeddings (to model words similarity) and LSTM neural networks [25] with attention mechanism (see [26] and [27]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "We will use both word embeddings (to model words similarity) and LSTM neural networks [25] with attention mechanism (see [26] and [27]).", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "We will use both word embeddings (to model words similarity) and LSTM neural networks [25] with attention mechanism (see [26] and [27]).", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "Solver, the task stated nearly six decades ago [28].", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "The idea behind the agent has been described in [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "It uses Q-learning [29] to estimate the utility of an action in a given game state, identified as the hash of its textual description.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "Based on the word2vec [30], an algorithm mapping words into a vector representations based on their contextual similarities, and the Wikipedia as the word corpus, the verb-noun affordances are generated.", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "For this task we use word2vec [30] (and its implementation in TensorFlow [31]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "For this task we use word2vec [30] (and its implementation in TensorFlow [31]).", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "We use the LSTM neural networks operating on words [25], augmented by the attention mechanism ([26] and [27]).", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "We use the LSTM neural networks operating on words [25], augmented by the attention mechanism ([26] and [27]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 25, "context": "We use the LSTM neural networks operating on words [25], augmented by the attention mechanism ([26] and [27]).", "startOffset": 104, "endOffset": 108}, {"referenceID": 30, "context": "This combination was previously tested in [32].", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "com (instead of using Wikipedia, as in [15]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "2) Commands: In order to secure out agent against overfitting, we fix the set of games used in tests (the same 50 games as in [15]).", "startOffset": 126, "endOffset": 130}, {"referenceID": 31, "context": "to learn how to play Civilization II game [33].", "startOffset": 42, "endOffset": 46}, {"referenceID": 32, "context": "After splitting texts into sentences, we parsed them using PCFG parser from NLTK package [34].", "startOffset": 89, "endOffset": 93}, {"referenceID": 33, "context": "See [35].", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "We were using the same set of 50 Z-machine games7 as in [15].", "startOffset": 56, "endOffset": 60}], "year": 2017, "abstractText": "The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments. In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map. We validated usefulness of these mechanisms, measuring agent\u2019s performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.", "creator": "LaTeX with hyperref package"}}}