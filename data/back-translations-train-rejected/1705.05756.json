{"id": "1705.05756", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "All-relevant feature selection using multidimensional filters with exhaustive search", "abstract": "This paper describes a method for identification of the informative variables in the information system with discrete decision variables. It is targeted specifically towards discovery of the variables that are non-informative when considered alone, but are informative when the synergistic interactions between multiple variables are considered. To this end, the mutual entropy of all possible k-tuples of variables with decision variable is computed. Then, for each variable the maximal information gain due to interactions with other variables is obtained. For non-informative variables this quantity conforms to the well known statistical distributions. This allows for discerning truly informative variables from non-informative ones. For demonstration of the approach, the method is applied to several synthetic datasets that involve complex multidimensional interactions between variables. It is capable of identifying most important informative variables, even in the case when the dimensionality of the analysis is smaller than the true dimensionality of the problem. What is more, the high sensitivity of the algorithm allows for detection of the influence of nuisance variables on the response variable.", "histories": [["v1", "Tue, 16 May 2017 15:11:10 GMT  (1364kb,D)", "http://arxiv.org/abs/1705.05756v1", "27 pages, 11 figures, 3 tables"]], "COMMENTS": "27 pages, 11 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["krzysztof mnich", "witold r rudnicki"], "accepted": false, "id": "1705.05756"}, "pdf": {"name": "1705.05756.pdf", "metadata": {"source": "CRF", "title": "All-relevant feature selection using multidimensional filters with exhaustive search", "authors": ["Krzysztof Mnich"], "emails": ["k.mnich@uwb.edu.pl", "w.rudnicki@uwb.edu.pl"], "sections": [{"heading": null, "text": "Keywords: all-relevant feature selection, mutual information, multidimensional filters"}, {"heading": "1 Introduction", "text": "Modern data sets often contain a very large number of variables, most of which are irrelevant to the phenomena to be studied. There are three main approaches to dealing with such data sets: you can either select the most relevant variables before modelling (filtering) them, use dimensionality reduction techniques [45, 36, 40, 23], or apply the modeling method that performs variable selection as part of model building; the last option can be exercised in two flavours; wrappers can either use the modeling method with an embedded feature selection or apply the wrapper method [12]; the known examples of the first approach are lasso [41] or the elastic network [52] within the domain of linear models; wrappers can be based on various machine learning algorithms, such as SVM [7] - as in the SVM-RE algorithm - [13 or 28] random forest models."}, {"heading": "1.1 Short review of filtering methods", "text": "Numerous filtering approaches proposed in the literature fall into two general classes. In the first, a simple measure is calculated for the association between decision variables and descriptive variables, and the variables are classified according to this measure. Association can be determined, for example, by the t-test for population means, the signal-to-noise ratio, the correlations between decision and descriptors, and various measures based on information tropie [8, 39]. Inclusion in the relevant set is determined by applying the statistical test used by either the FWER [18] or FDR [4] method. In many cases, the set is heuristically shortened to the uppermost N characteristics when the selection of N is dictated by convenience [21]. Simple univariate filtering can overlook the variables that affect the phenomenon under investigation only in interaction with other characteristics. The number of overlooked but important characteristics can be even higher if the characteristics are considered relevant even if the characteristics are not very variable, as some of them are used in combination with other characteristics."}, {"heading": "1.2 Case for all-relevant feature selection", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move,"}, {"heading": "1.3 Current study", "text": "The aim of the current study is to introduce a rigorous approach to the problem of all-relevant variable selection, which includes variables that are relevant due to non-trivial interactions with other variables, variables that are strongly correlated and those that are weakly linked to the decision variable. To this end, we propose to use multi-dimensional, exhaustive analyses of the mutual information between decision variables and descriptor variables. Such analysis requires both a large number of experimental samples and massive calculations that were not feasible until recently. However, the rapid development of experimental technology and the growth of computing power, especially using the GPU for the calculation, allows an exhaustive search for pairs and drills of variables in the context of GWAS [10, 19] as well as gene expression. Currently, even higher-dimensional interactions can be studied in a reasonable amount of time when a smaller number of variables is measured. Nevertheless, technological developments have not been overmatched by a systemic advance that is necessary to identify the interactions."}, {"heading": "2 Some previous approaches to the multivariate exhaustive", "text": ""}, {"heading": "2.1 Regression-based approach", "text": "An example of the regression model of the interaction between 2 variables X1, X2 and the response variable Y is a linear regression: Y = \u03b10 + \u03b11X1 + \u03b12X2 + \u03b112X1X2 (1) In the presence of the interaction, the empirically determined value of \u03b112 of 0 differs in a statistically significant amount. However, there are some limitations of the regression test for interactions. First, such a test is limited to the detection of monotonous interactions. This is not a serious problem in the case of continuous decision variables, since there are usually some monotonous components of the interactions. The problem occurs with discrete response variables and possibly also with continuous variables with strongly non-monotonous interactions. In the case of discrete response variables, for example, VanderWeele shows in [43] that the interaction between the various terms is less effective than the interaction between 12 ression variables."}, {"heading": "2.2 Information measure of synergy", "text": "The best-known synergy quantity used in information theory is the interaction information of the variables \u03bd = {Y, X1,...., Xk}: Iint (\u03bd) = \u2212 \u2211. The function becomes positive if k-dimensional synergy is present, while less-dimensional interactions do not contribute to the positive terms. For a pair of variables X1, X2, the interaction information is: Iint (Y, X1, X2) = \u2212 H (Y, X2) = negative interaction."}, {"heading": "2.3 Multivariate test of association", "text": "Problems with synergistic measures led some researchers to investigate a relationship between the response variable and the entire k-tuple of variables treated as a single feature. In such a case, the features that are not relevant on their own but have synergistic relationships with other features can be identified as relevant. However, there is a significant risk of false positive results, since most k-tuples containing at least one feature that is correlated with the response variable would be reported as relevant. In the case of high-dimensional datasets with numerous relevant variables, the number of reported k-tuples can be extremely high [16, 10]. It may be possible to report only those k-tuples where none of the variables has been identified as relevant in the low-dimensional search, but it is difficult to implement them efficiently. Moreover, such an approach would also overlook k-tuples with variables that are relevant in the universal variable test but are popular in strong synergistic relationships."}, {"heading": "3 Searching for relevant variables", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3.1 The statistics of conditional mutual information", "text": "Calculation of conditional mutual information is simple and efficient for discrete variables. In the case of continuous variables, there are two possibilities. Either you can build an estimate of a multidimensional probability density function, or you can discredit variables. In the current study, we use quantitative discretization, which method is both simple and robust. To avoid overadjustment, discretization is not well matched to the test functions - the descriptive variables are divided into three equipotent categories, whereas the decision variable is based on the power of two periodicities. Let's leave out the response variable Y, the tested variable X, and the variable {Si}, i = 1, k \u2212 1, the discrete variables with the number of categories CY, CX, {CSi}, {CSi}, {CSi}, and the analyzed dataset values of the variable for N objects, respectively. The obvious estimation of the probability that the isable is based on a certain value, xx, is xp."}, {"heading": "4 The algorithm", "text": "The theory described above leads to the algorithm of selecting multivariate characteristics. It consists of the following steps: \u2022 If the response variable and the explanatory variables are continuous, they are discredited first. \u2022 For each k-tuple of variables Sk, the contingency table is created, which contains the number of objects for which the variables take certain values. Mathematically, this step is the most expensive, but it is simple, so that it can be executed in parallel, e.g. using GPU. \u2022 For each variable X-tuples, the conditional mutual information between the response variable Y and X is calculated, calculating the rest of the k-tupel Sk-1. Subsequently, the corresponding perc2 p-value is calculated. \u2022 For each variable, the minimum p-value for all k-tuples is recorded as pmin (X). In a special case, if all explanatory variables have the same number of categories (e.g. all discredited Y-values correspond to all last two steps), the Y-1 can be attributed."}, {"heading": "5 Tests on Synthetic Datasets", "text": "To investigate the performance of the method, we have created a complex artificial dataset containing 351 descriptive variables, defined for 5000 objects. There are two general classes of variables in this dataset. The first consists of variables drawn from a (\u2212 1, 1) uniform distribution, with possible addition of random noise. The second is obtained as a linear combination of primary variables, also with possible addition of random noise. Random noise is drawn from the (\u2212 0.15, 0.15) uniform distributions. More precisely, the dataset consists of the following groups of variables: 1. 3 base variables drawn from the uniform distribution. The response variable is a function of these variables only. 2. 3 base variables with 15% random noise adde.3. 20 linear combinations of the base variables (belonging to group 1) with random coefficients. 4. 20 base combinations of the base variables belong to the group 1 random ones."}, {"heading": "6 Results", "text": "The three response functions generated for the same set of descriptive variables present a wide range of difficulties - the simple problem in the case of sphere, the middle problem in the case of exclusive or, and the 3D chessboard is the most difficult, as can be seen in Table 1, where the number of variables determined as relevant is given. It is evident that although the original problems are formulated in three dimensions, the presence of linear variable combinations can reduce the apparent dimensionality of the problem."}, {"heading": "6.1 An easy problem - a 3D sphere", "text": "Extending the analysis to higher dimensions does not result in large changes in information gain, see Figure 2. Furthermore, the relative ranking of the groups of variables remains largely the same; the ranking of the variables is in line with intuition - the pure base variables are rated higher than noise base variables, which are rated higher than linear combinations of pure variables; the noisy linear combinations are rated lower than pure linear combinations, and the disturbing variables are rated the lowest among informative variables. Nevertheless, increasing the dimensionality of the analysis enables higher sensitivity in the case of loud linear combination and disturbance variables. In particular, the transition from one to two dimensional analysis improves the ranking of noisy linear combinations and disturbance variables, see Table 2."}, {"heading": "6.2 Intermediate difficulty - a 3D XOR", "text": "The three-dimensional XOR analysis is therefore an example of a problem for which the multidimensional analysis and the all-relevant selection of characteristics based on the determined variables is necessary to fully understand the system. If only 3 base variables are taken into account, it is an example of pure 3-dimensional synergy, where no variable is informative unless all 3 are analyzed together. However, the introduction of linear combinations of the base variables along with the descriptive variables is significantly aligned to the analysis.The one-dimensional analysis cannot discover the meaning of the base variables unless half of the pure linear combinations of the base variables along with 2 noisants are considered relevant, see Table 1. The unique approach to this problem discloses a number of two-dimensional variables that can be further used for the construction of predictive models or for design marker sets. However, this sentence does not contain any of the original pure variables that were not used to create the problem nor noisy."}, {"heading": "6.3 Hard problem - 3D checkerboard", "text": "The third example of the informative response function generated with the same descriptor set is a three-dimensional product of sine functions that generates three-dimensional 4 x 4 x 4 checkerboard-like patterns; this function is another example of pure synergy when analyzed only for base descriptors, with a more complex distribution of the response function; in addition, three equipotent columns of descriptors do not correspond well to the variation of the decision function, which makes analysis more difficult; this imbalance results in much lower information gains attributed to informative variables than in the previous example, and lower sensitivity, especially in lower dimensions; only two linear combinations of base variables were recognized as relevant in a one-dimensional analysis; the sensitivity of two-dimensional analysis is higher, which leads to the discovery of all pure linear combinations, along with two base variables; and finally, the three-dimensional base analysis revealed all the pure variables, as well as some of the most distorted."}, {"heading": "6.4 Random decision function", "text": "This test is performed to verify that the method is not too aggressive and can designate random variables as relevant when there are no real relevant variables. It is passed because no variable has been designated as relevant, either in 1D, 2D or 3D analysis, and the ranking of the variables is completely random."}, {"heading": "6.5 Classification using identified relevant variables", "text": "The results of the feature selection were also used to build predictive random forest models to test whether, by extending the search for informative variables to a higher dimension, the modelling results could potentially be improved. In particular, we were interested in the problem where, on the one hand, a significant fraction of the relevant variables were not discovered by means of one-dimensional analysis, but, on the other, several relevant variables were found. Therefore, we focused on the 3D XOR, since two other problems are either too simple (3D sphere) or too heavy (3D chessboard). it turns out that the one-dimensional analysis reveals too few variables for the creation of high-quality models, either using all or only three-dimensional variables, see Table 3. However, both 2- and 3-dimensional analyses find a subset of variables sufficient to create high-quality models, with the quality being comparable to those created using all informative variables, the results may be significantly inferior worse based on the six-dimensional models, whereas the three-dimensional models may be significantly inferior worse based on the three-dimensional ones."}, {"heading": "7 Conclusions", "text": "In the current study, we explored the possibility of identifying relevant variables when the response variable is a complex multidimensional function of the descriptive characteristics with nonlinear and synergistic interactions between variables. To this end, we proposed a strict methodology for identifying all relevant variables through full multidimensional analysis, allowing either incorrect discovery control using the Benjamin Hochberg method or familial error rate, for example by means of Bonferoni correction. We tested the methodology using complex multidimensional datasets where the response variable contained a three-dimensional nonlinear function of 3 descriptors and descriptor sets, including linear combinations of base variables, loud copies of relevant variables, loud combinations of base variables, nuisation variables, and random noise."}, {"heading": "A Statistical toolbox", "text": "The probability distribution of the conditional mutual distribution and the estimation of the conditional mutual information (10) can be rewritten in the form: I (Y; X; S) = 1 N (S) = 1 N (S) = 1 N (S) = 1 (S) = 1 (S) = 1 (S) = 1 (S) = 1 (S) = 1 (S) = 1 (S) = 2 (S) = 2 (S) = 2 (S) = 2 (S) = 2 (S) = 2 (S) = 1) = 2 (S) = 2 (S) = 2 (S) = 2) = 2 (S) = 2 (S) = 2 (S) = 2 (S) = 2 (S) = 2 (S) = 2 (S)."}, {"heading": "B Examples", "text": "Xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}], "references": [{"title": "Distinct types of diffuse large b-cell lymphoma identified by gene expression", "author": ["Ash A Alizadeh", "Michael B Eisen", "R Eric Davis", "Chi Ma", "Izidore S Lossos", "Andreas Rosenwald", "Jennifer C Boldrick", "Hajeer Sabet", "Truc Tran", "Xin Yu"], "venue": "profiling. Nature,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "The large-sample distribution of the likelihood ratio for testing composite hypotheses", "author": ["D. Anastassiou"], "venue": "Mol. Syst. Biol.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Mendel\u2019s principles of heredity, by W", "author": ["William Bateson", "Gregor Mendel"], "venue": "Bateson.  Cambridge [Eng.]University Press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1909}, {"title": "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing", "author": ["Yoav Benjamini", "Yosef Hochberg"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Statistical dependence measure for feature selection in microarray datasets", "author": ["Ver\u00f3nica Bol\u00f3n-Canedo", "Sohan Seth", "Noelia S\u00e1nchez-Marono", "Amparo Alonso- Betanzos", "Jose C Principe"], "venue": "ESANN,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Comparison of discrimination methods for the classification of tumors using gene expression data", "author": ["Sandrine Dudoit", "Jane Fridlyand", "Terence P Speed"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "A large genome-wide association study of age-related macular degeneration highlights contributions of rare and common variants", "author": ["Lars G Fritsche", "Wilmar Igl", "Jessica N Cooke Bailey", "Felix Grassmann", "Sebanti Sengupta", "Jennifer L Bragg-Gresham", "Kathryn P Burdon", "Scott J Hebbring", "Cindy Wen", "Mathias Gorski"], "venue": "Nature genetics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Gwis - model-free, fast and exhaustive search for epistatic interactions in case-control gwas", "author": ["Benjamin Goudey", "David Rawlinson", "Qiao Wang", "Fan Shi", "Herman Ferra", "Richard M. Campbell", "Linda Stern", "Michael T. Inouye", "Cheng Soon Ong", "Adam Kowalczyk"], "venue": "BMC Genomics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Measuring statistical dependence with hilbert-schmidt norms", "author": ["Arthur Gretton", "Olivier Bousquet", "Alex Smola", "Bernhard Sch\u00f6lkopf"], "venue": "In International conference on algorithmic learning theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "Journal of machine learning research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Isabelle Guyon", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Clinical utility of microarray-based gene expression profiling in the diagnosis and subclassification of leukemia: report from the international microarray innovations in leukemia study group", "author": ["Torsten Haferlach", "Alexander Kohlmann", "Lothar Wieczorek", "Giuseppe Basso", "Geertruy Te Kronnie", "Marie-Christine B\u00e9n\u00e9", "John De Vos", "Jesus M Hern\u00e1ndez", "Wolf- Karsten Hofmann", "Ken I Mills"], "venue": "Journal of Clinical Oncology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Correlation-based feature selection of discrete and numeric class machine learning", "author": ["Mark A Hall"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Intersnp: genome-wide interaction analysis guided by a priori information", "author": ["Christine Herold", "Michael Steffens", "Felix F Brockschmidt", "Max P Baur", "Tim Becker"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Genome-wide association studies for common diseases and complex traits", "author": ["Joel N Hirschhorn", "Mark J Daly"], "venue": "Nature Reviews Genetics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "A sharper bonferroni procedure for multiple tests of significance", "author": ["Yosef Hochberg"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "An information-gain approach to detecting three-way epistatic interactions in genetic association studies", "author": ["Ting Hu", "Yuanzhu Chen", "Jeff W Kiralis", "Ryan L Collins", "Christian Wejse", "Giorgio Sirugo", "Scott M Williams", "Jason H Moore"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Filter versus wrapper gene selection approaches in dna microarray domains", "author": ["I\u00f1aki Inza", "Pedro Larra\u00f1aga", "Rosa Blanco", "Antonio J Cerrolaza"], "venue": "Artificial intelligence in medicine,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Feature selection and classification for microarray data analysis: Evolutionary methods for identifying predictive genes", "author": ["Thanyaluk Jirapech-Umpai", "Stuart Aitken"], "venue": "BMC bioinformatics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Glide: Gpu-based linear regression for detection of epistasis", "author": ["Tony Kam-Thong", "C-A Azencott", "Lawrence Cayton", "Benno P\u00fctz", "Andr\u00e9 Altmann", "Nazanin Karbalai", "Philipp G S\u00e4mann", "Bernhard Sch\u00f6lkopf", "Bertram M\u00fcller-Myhsok", "Karsten M Borgwardt"], "venue": "Human heredity,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Subsystem identification through dimensionality reduction of large-scale gene expression data", "author": ["Philip M Kim", "Bruce Tidor"], "venue": "Genome research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H. John"], "venue": "Artif. Intell.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Estimating attributes: analysis and extensions of relief", "author": ["Igor Kononenko"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Information of interactions in complex systems", "author": ["Klaus Krippendorff"], "venue": "International Journal of General Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Boruta A System for Feature Selection", "author": ["Miron B. Kursa", "Aleksander Jankowski", "Witold R. Rudnicki"], "venue": "Fundamenta Informaticae,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Classification and regression by randomforest", "author": ["Andy Liaw", "Matthew Wiener"], "venue": "R News,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Criteria for the use of omics-based predictors in clinical trials: explanation and elaboration", "author": ["Lisa M McShane", "Margaret M Cavenagh", "Tracy G Lively", "David A Eberhard", "William L Bigbee", "P Mickey Williams", "Jill P Mesirov", "Mei-Yin C Polley", "Kelly Y Kim", "James V Tricoli"], "venue": "BMC medicine,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Informationtheoretic feature selection in microarray data using variable complementarity", "author": ["Patrick Emmanuel Meyer", "Colas Schretter", "Gianluca Bontempi"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Gene subset selection in microarray data using entropic filtering for cancer classification", "author": ["F\u00e9lix F Gonz\u00e1lez Navarro", "Ll\u00fa\u0131s A Belanche Mu\u00f1oz"], "venue": "Expert Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "A Language and Environment for Statistical Computing", "author": ["R R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Feature selection via dependence maximization", "author": ["Le Song", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Ranking a Random Feature for Variable and Feature Selection", "author": ["Herve Stoppiglia", "Gerard Dreyfus", "Remi Dubois", "Yacine Oussar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Rankgene: identification of diagnostic genes based on expression", "author": ["Yang Su", "TM Murali", "Vladimir Pavlovic", "Michael Schaffer", "Simon Kasif"], "venue": "data. Bioinformatics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1996}, {"title": "Travelling the world of gene\u2013gene interactions", "author": ["Kristel Van Steen"], "venue": "Briefings in bioinformatics, page bbr012,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Epistatic interactions", "author": ["Tyler J VanderWeele"], "venue": "Statistical Applications in Genetics and Molecular Biology,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "A review of feature selection methods based on mutual information", "author": ["Jorge R Vergara", "Pablo A Est\u00e9vez"], "venue": "Neural computing and applications,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Singular value decomposition and principal component analysis. In A practical approach to microarray data analysis, pages 91\u2013109", "author": ["Michael E Wall", "Andreas Rechtsteiner", "Luis M Rocha"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}, {"title": "Boost: A fast approach to detecting gene-gene interactions in genome-wide case-control studies", "author": ["Xiang Wan", "Can Yang", "Qiang Yang", "Hong Xue", "Xiaodan Fan", "Nelson L.S. Tang", "Weichuan Yu"], "venue": "CoRR, abs/1001.5130,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Selecting feature subset for high dimensional data via the propositional foil rules", "author": ["Guangtao Wang", "Qinbao Song", "Baowen Xu", "Yuming Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "The large-sample distribution of the likelihood ratio for testing composite hypotheses", "author": ["S.S. Wilks"], "venue": "Ann. Math. Statist., 9(1):60\u201362,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1938}, {"title": "Feature selection for high-dimensional data: A fast correlationbased filter solution", "author": ["Lei Yu", "Huan Liu"], "venue": "In ICML,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2003}, {"title": "Efficient feature selection via analysis of relevance and redundancy", "author": ["Lei Yu", "Huan Liu"], "venue": "Journal of machine learning research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2004}, {"title": "Searching for interacting features", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In ijcai,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2005}], "referenceMentions": [{"referenceID": 41, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 32, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 36, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 20, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 9, "context": "Once can either use the modelling method with an embedded feature selection, or apply the wrapper method [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 37, "context": "The well-known examples of the first approach are lasso [41] or elastic network [52] within domain of linear models.", "startOffset": 56, "endOffset": 60}, {"referenceID": 48, "context": "The well-known examples of the first approach are lasso [41] or elastic network [52] within domain of linear models.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "The wrappers can be based on various machine learning algorithms, such as SVM [7] \u2013 as in the SVM-RFE algorithm [13], or random forest [6] \u2013 as in the Boruta algorithm [28].", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "The wrappers can be based on various machine learning algorithms, such as SVM [7] \u2013 as in the SVM-RFE algorithm [13], or random forest [6] \u2013 as in the Boruta algorithm [28].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "One of its common applications is discovery of genes associated with diseases, either within the context of genome wide association studies (GWAS)[17], or analysis of gene expression profiles [1].", "startOffset": 146, "endOffset": 150}, {"referenceID": 0, "context": "One of its common applications is discovery of genes associated with diseases, either within the context of genome wide association studies (GWAS)[17], or analysis of gene expression profiles [1].", "startOffset": 192, "endOffset": 195}, {"referenceID": 11, "context": "The number of variables analysed in gene expression studies can be as large as over fifty thousands [14], however the number is dwarfed by the GWAS where over ten millions of variables can be analysed [9].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "The number of variables analysed in gene expression studies can be as large as over fifty thousands [14], however the number is dwarfed by the GWAS where over ten millions of variables can be analysed [9].", "startOffset": 201, "endOffset": 204}, {"referenceID": 17, "context": "With the number of variables that high the computational cost of wrapper and embedded methods is so high that the initial filtering is de-facto standard approach, even if other methods are used later for building final models or construction of the sets of marker genes [20, 30, 14].", "startOffset": 270, "endOffset": 282}, {"referenceID": 26, "context": "With the number of variables that high the computational cost of wrapper and embedded methods is so high that the initial filtering is de-facto standard approach, even if other methods are used later for building final models or construction of the sets of marker genes [20, 30, 14].", "startOffset": 270, "endOffset": 282}, {"referenceID": 11, "context": "With the number of variables that high the computational cost of wrapper and embedded methods is so high that the initial filtering is de-facto standard approach, even if other methods are used later for building final models or construction of the sets of marker genes [20, 30, 14].", "startOffset": 270, "endOffset": 282}, {"referenceID": 5, "context": "The association can be measured using for example t-test for population means, signal to noise ratio, correlations between decision and descriptors, as well as various measures based on information entropy [8, 39].", "startOffset": 206, "endOffset": 213}, {"referenceID": 35, "context": "The association can be measured using for example t-test for population means, signal to noise ratio, correlations between decision and descriptors, as well as various measures based on information entropy [8, 39].", "startOffset": 206, "endOffset": 213}, {"referenceID": 15, "context": "The inclusion to the relevant set is decided by application of the statistical test using either FWER [18], or FDR [4] method.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The inclusion to the relevant set is decided by application of the statistical test using either FWER [18], or FDR [4] method.", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "In many cases the set is heuristically truncated to top N features, when selection of N is dictated by convenience [21].", "startOffset": 115, "endOffset": 119}, {"referenceID": 47, "context": "In some cases, the optimal subset is obtained in a two-step procedure, where in the first step the ranking is obtained and then the selection of non-redundant optimal feature set is performed [51].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 110, "endOffset": 118}, {"referenceID": 45, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 110, "endOffset": 118}, {"referenceID": 47, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 161, "endOffset": 169}, {"referenceID": 28, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 161, "endOffset": 169}, {"referenceID": 4, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 211, "endOffset": 214}, {"referenceID": 22, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 29, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 27, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 43, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 8, "context": "Recently, the unifying approach based on the Hilbert-Schmidt Independence Criterion (HSIC) [11] has been proposed [37].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "Recently, the unifying approach based on the Hilbert-Schmidt Independence Criterion (HSIC) [11] has been proposed [37].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "The example of the algorithm that both returns the full list of relevant variables and is able to discover variables that are involved in complex non-linear interactions is Relief-f [26].", "startOffset": 182, "endOffset": 186}, {"referenceID": 31, "context": "Unfortunately, it is biased against weaker and correlated variables [35].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "However, rapid development of experimental technology as well as the growth of the computing power, especially using GPU for computation, allows for exhaustive search of pairs and triplets of variables in the context of GWAS [10, 19] as well as gene expression.", "startOffset": 225, "endOffset": 233}, {"referenceID": 16, "context": "However, rapid development of experimental technology as well as the growth of the computing power, especially using GPU for computation, allows for exhaustive search of pairs and triplets of variables in the context of GWAS [10, 19] as well as gene expression.", "startOffset": 225, "endOffset": 233}, {"referenceID": 23, "context": "In particular, it has been shown [27, 19] that the standard measure of synergy can give misleading results in higher-dimensional cases.", "startOffset": 33, "endOffset": 41}, {"referenceID": 16, "context": "In particular, it has been shown [27, 19] that the standard measure of synergy can give misleading results in higher-dimensional cases.", "startOffset": 33, "endOffset": 41}, {"referenceID": 38, "context": "1 Regression-based approach The regression-based tests are widely used for detection of the multivariate interactions [42, 43, 22].", "startOffset": 118, "endOffset": 130}, {"referenceID": 39, "context": "1 Regression-based approach The regression-based tests are widely used for detection of the multivariate interactions [42, 43, 22].", "startOffset": 118, "endOffset": 130}, {"referenceID": 19, "context": "1 Regression-based approach The regression-based tests are widely used for detection of the multivariate interactions [42, 43, 22].", "startOffset": 118, "endOffset": 130}, {"referenceID": 39, "context": "For discrete response variables, it has been shown by VanderWeele in [43], that the positive value of \u03b112 is equivalent to a very strong probabilistic condition.", "startOffset": 69, "endOffset": 73}, {"referenceID": 42, "context": "2 Information measure of synergy Several approaches have been proposed for directly searching for synergies, using information theory measure [46, 19, 2].", "startOffset": 142, "endOffset": 153}, {"referenceID": 16, "context": "2 Information measure of synergy Several approaches have been proposed for directly searching for synergies, using information theory measure [46, 19, 2].", "startOffset": 142, "endOffset": 153}, {"referenceID": 1, "context": "2 Information measure of synergy Several approaches have been proposed for directly searching for synergies, using information theory measure [46, 19, 2].", "startOffset": 142, "endOffset": 153}, {"referenceID": 1, "context": "where \u2211 \u03c4\u2286\u03bd denotes summation over all subsets of \u03bd, and H(\u03c4) is an entropy of the subset \u03c4 [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "The unexpected behaviour of Iint had been reported for subsets of 3 or more variables [19, 2].", "startOffset": 86, "endOffset": 93}, {"referenceID": 1, "context": "The unexpected behaviour of Iint had been reported for subsets of 3 or more variables [19, 2].", "startOffset": 86, "endOffset": 93}, {"referenceID": 13, "context": "In the case of highdimensional datasets with numerous relevant variables, the number of reported k-tuples can be extremly large [16, 10].", "startOffset": 128, "endOffset": 136}, {"referenceID": 7, "context": "In the case of highdimensional datasets with numerous relevant variables, the number of reported k-tuples can be extremly large [16, 10].", "startOffset": 128, "endOffset": 136}, {"referenceID": 21, "context": "Identification of all informative variables can be conveniently described using the notion of weak relevance, introduced by Kohavi and John in [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 46, "context": "Yu and Liu [50] proposed to split weak-relevance into two classes.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 13, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 28, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 40, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 3, "context": "Bonferroni correction) or FDC (like Benjamini-Hochberg [4]) methods.", "startOffset": 55, "endOffset": 58}, {"referenceID": 30, "context": "To this end, the variables selected by the procedure were used to build a predictive models using Random Forest classifier [6] implemented in R package randomForest [34, 29].", "startOffset": 165, "endOffset": 173}, {"referenceID": 25, "context": "To this end, the variables selected by the procedure were used to build a predictive models using Random Forest classifier [6] implemented in R package randomForest [34, 29].", "startOffset": 165, "endOffset": 173}, {"referenceID": 21, "context": "The tests on artificial data confirmed that the new approach is able to identify all or nearly all the weakly relevant variables (in the sense of Kohavi-John [24]), when the dimensionality of the analysis matches the true dimensionality of the problem.", "startOffset": 158, "endOffset": 162}, {"referenceID": 44, "context": "It has been proved by Wilks [48], that 2 \u2211", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "If it is not the case, one can extend the dataset using the noninformative contrast variables [38, 28].", "startOffset": 94, "endOffset": 102}, {"referenceID": 24, "context": "If it is not the case, one can extend the dataset using the noninformative contrast variables [38, 28].", "startOffset": 94, "endOffset": 102}, {"referenceID": 2, "context": "The dependence between Y and X1, X2 is a classical example of the epistasis in genetics, as defined by Bateson in 1909 [3].", "startOffset": 119, "endOffset": 122}], "year": 2017, "abstractText": "This paper describes a method for identification of the informative variables in the information system with discrete decision variables. It is targeted specifically towards discovery of the variables that are non-informative when considered alone, but are informative when the synergistic interactions between multiple variables are considered. To this end, the mutual entropy of all possible k-tuples of variables with decision variable is computed. Then, for each variable the maximal information gain due to interactions with other variables is obtained. For non-informative variables this quantity conforms to the well known statistical distributions. This allows for discerning truly informative variables from non-informative ones. For demonstration of the approach, the method is applied to several synthetic datasets that involve complex multidimensional interactions between variables. It is capable of identifying most important informative variables, even in the case when the dimensionality of the analysis is smaller than the true dimensionality of the problem. What is more, the high sensitivity of the algorithm allows for detection of the influence of nuisance variables on the response variable.", "creator": "LaTeX with hyperref package"}}}