{"id": "1705.05769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees", "abstract": "This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An HFIT produces an optimum treelike structure, i.e., a natural hierarchical structure that accommodates simplicity by combining several low-dimensional fuzzy inference systems (FISs). Such a natural hierarchical structure provides a high degree of approximation accuracy. The construction of HFIT takes place in two phases. Firstly, a nondominated sorting based multiobjective genetic programming (MOGP) is applied to obtain a simple tree structure (a low complexity model) with a high accuracy. Secondly, the differential evolution algorithm is applied to optimize the obtained tree's parameters. In the derived tree, each node acquires a different input's combination, where the evolutionary process governs the input's combination. Hence, HFIT nodes are heterogeneous in nature, which leads to a high diversity among the rules generated by the HFIT. Additionally, the HFIT provides an automatic feature selection because it uses MOGP for the tree's structural optimization that accepts inputs only relevant to the knowledge contained in data. The HFIT was studied in the context of both type-1 and type-2 FISs, and its performance was evaluated through six application problems. Moreover, the proposed multiobjective HFIT was compared both theoretically and empirically with recently proposed FISs methods from the literature, such as McIT2FIS, TSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the obtained results, it was found that the HFIT provided less complex and highly accurate models compared to the models produced by the most of other methods. Hence, the proposed HFIT is an efficient and competitive alternative to the other FISs for function approximation and feature selection.", "histories": [["v1", "Tue, 16 May 2017 15:34:19 GMT  (1234kb,D)", "http://arxiv.org/abs/1705.05769v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["varun kumar ojha", "vaclav snasel", "ajith abraham"], "accepted": false, "id": "1705.05769"}, "pdf": {"name": "1705.05769.pdf", "metadata": {"source": "CRF", "title": "Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees", "authors": ["Varun Kumar Ojha", "Ajith Abraham"], "emails": ["ojha@arch.ethz.ch", "vaclav.snasel@vsb.cz", "ajith.abraham@ieee.org"], "sections": [{"heading": null, "text": "The idea behind it is that it is a way in which people are able to move in two phases in which they do not follow the rules. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is new. (...) The idea behind it is not new. (...) The idea behind it is not new. (...) The idea behind it is new. (...) The idea behind it is new. (...) The idea behind it is new. (...) The idea behind it is new. (...) The idea behind it is not new."}, {"heading": "II. TSK FUZZY INFERENCE SYSTEMS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Type-1 Fuzzy Inference Systems", "text": "A TSK type FIS is governed by the IF-THEN rules of the form [1]: Ri: IF x1 is Ai1 AND.. AND xdi is Aidi THEN y is Bi (1), where Ri is the i-th rule in a FIS, Ai1,..., Aidi is the T1FSs, Bi is a function of an input vector x = < x1, x2,..., xdi >, which returns a crisp output y, and di is the total number of inputs submitted to the i-th rule. Note that the number of inputs may vary from rule to rule. Therefore, the dimension of the inputs in a rule is called di. In TSK, the function Bi is usually compiled as: Bi = c 0 i + di \u0432j = 1 cjixj (2), where cji for j = 0 to d i are the free parameters in the subsequent part of a rule."}, {"heading": "B. Type-2 Fuzzy Inference Systems", "text": "The three axes of T2FS are defined as follows: the x-axis is referred to as the primary variable, where the y-axis is referred to as the secondary variable (or primary variable) (or primary variable (or primary variable, which is referred to by u), and the z-axis is referred to as the MF value (or secondary MF value), which is referred to by \u00b5. In a universal X set, a T2FS A has the form: A (x, u), \u00b5A axis is referred to as the MF value (x, u)."}, {"heading": "III. MULTIOBJECTIVE OPTIMIZATION OF HIERARCHICAL FUZZY INFERENCE TREES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Hierarchical Tree Formation", "text": "A hierarchical fuzzy inference tree (HFIT) is a tree-based system whose hierarchical structure is analogous to a multi-layered feedback NN, in which the nodes (the low-dimensional FISs) are linked by weighted linkages. The concept of building a hierarchical fuzzy inference tree is based on the flexible neural tree proposed by Chen et al. [56], which has two yield phases. Firstly, an evolutionary algorithm is used in the tree phase to construct / optimize a tree-like structure. Secondly, a genotype representing the underlying parameters of the tree phase is optimized in the parameter tuning phase. To create an optimal tree-based model, a population of randomly generated trees is first formed. As soon as a satisfactory tree structure (a tree with a small approximation error and low complexity) is created, the FIT phase is clearly displayed until the tuning phase."}, {"heading": "B. Tree Encoding", "text": "A HFIT G is a collection of nodes V and terminal node T: G = V-T = {v2, v3,..., vtn}, where vj (j = 2, 3,.., tn) denotes a non-leaf statement and has 2 \u2264 j \u2264 tn arguments. The statement of the leaf node x1, x2,... xd takes no argument and represents the input variable / statement. A typical HFIT is shown in Fig. 3 (a), whereas Fig. 3 (b) illustrates the node Ni of a HFIT that requires ni inputs. The inputs zij, x1, x2,..., xd} for j = 1 to ni to a node Ni come either from the input layer or from other nodes in HFIT. Each node in a HFIT receives a weighted input, where this target is also set to 1 with the weighting HFIT to reduce the weight."}, {"heading": "C. Rule Formation at the Nodes", "text": "The rules for type-1 FIS node: Each node in an HFIT is a type-1 or type-2 FIS. Therefore, the rules for a node are created as follows: taking into account a reference to node N1 (18), where the shot strength f 1ij is: f 1ij = \u00b5A11i (x1) \u00b5A12j (x2), for i = 1, 2 and j = 1, 2 x 2 (19) Similar to node N1, N2 has two inputs and if each input to node N2 is divided into two T1FSs, then output y2 from node N2 is calculated in a similar manner."}, {"heading": "D. Structure Tuning (Pareto-based Multiobjective Optimization)", "text": "This year it is more than ever before."}, {"heading": "E. Parameter Tuning", "text": "In the structural optimization phase, an optimal phenotype (HFIT) was derived with the parameters initially determined by random conjecture. Therefore, the obtained phenotype was further tuned in the parameter tuning phase using a parameter optimization algorithm. To optimize the parameters of the derived phenotype, its parameters were mapped to a genotype, i.e., to a real vector called a solution vector. Selecting the best phenotype in a single objective training was based exclusively on a comparison of the RMSEs. However, choosing a solution in a multi-objective training is a difficult choice. In this work, after the multi-objective training of HFIT, the best solution for parameter tuning from the Pareto front was selected."}, {"heading": "IV. THEORETICAL EVALUATION", "text": "The efficiency of the proposed HFIT is based on the combined influence of three fundamental operations involved in the development of the model: Tree construction by MOGP, which hierarchically combines several low-dimensional fuzzy systems, and parameters tuned by differential evolution (DE). Hence, HFIT has many different characteristics that define its predictive efficiency compared to many models used for comparison from the literature. Following are the characteristics of HFIT: 1) Convergence capability of evolutionary class algorithms (EA) or, for that matter, MOGP. 2) Approach capability of the developed hierarchical fuzzy system (tree model). 3) Convergence capability of DE in matching the parameters of the tree. Subsequent discussions analyze each of these characteristics theoretically separately."}, {"heading": "A. Optimal tree structure through MOGP convergence", "text": "The evaluation of the convergence of the evolutionary class algorithms indicates that it is also possible to look at the MOGP convergence to another space. Theoretical studies of the EAs, carried out from different perspectives, show that an optimal solution is indeed possible in a finite time. First, Goldberg and Sergret [67] can show the convergence property of GD using a finite Markov chain analysis in which they consider GD with a finite population and recombination and mutation operators. [68] Banach Fixpoint Theorem of Population described [69] states that on a metric space a constructive mapping f has a unique fixed point, i.e., for one element x, f x (x) = x. Therefore, Banach Fixpoint Theorem can explain MOGP with only the assumption that there should be an improvement of the population (not necessarily the optimal solution) from one generation to another."}, {"heading": "B. Approximation ability of hierarchical fuzzy inference tree", "text": "This section describes the approximation capability of an HFIT system, which is a result of the MOGP operation. Theoretical studies on specific cases of the hierarchical fuzzy system are contained in [27], [32]. < p < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "C. Optimal parameter through differential evolution convergence", "text": "A probable view of DE convergence followed by a description of the global convergence condition for DE is described in [75]. They show that DE actually converges to an optimal solution. Similarly, Zhang and Sanderson [74] investigated the various properties of DE, such as mutation, crossover, and recombination operators that affect DE convergence. DE follows a property similar to the EA class algorithms described in Section IV-A. Therefore, its global convergence capability is no different from that of MOGP and actually finds an optimal parameter vector for HFIT."}, {"heading": "D. Comparative study of HFIT with other models", "text": "This year it is more than ever before."}, {"heading": "V. EMPIRICAL EVALUATION", "text": "This section describes the evaluated results of the proposed algorithms T1HFITS, T1HFITM, T2HFITS and T2HFITM using six example problems. Suppose that the datasets in the examples are of the form: (X, d), where X = (x1, x2,.., xN) is the set of input vectors and d = < d1, d2,..., dN > is the desired output vector. Here, the dataset N has input / output pattern (pairs) and if the vector y = < y1, y2,.., yN > is the predicted output vector, then the performance ofan algorithm for the dataset (X, d) can be measured using RMSE E, as in (26) and the correlation coefficient r between the desired output vectors d and y as: r = 1 (di \u2212 d)."}, {"heading": "A. Example 1\u2014System Identification", "text": "This year, it has reached the stage where it will be able to achieve the objectives mentioned, and in the way that they are able to achieve them."}, {"heading": "C. Example 3\u2014Miles-Per-Gallon Prediction Problem", "text": "In order to evaluate the performance of the proposed algorithms, a real MPG problem was used. The aim of this example was to predict or estimate the fuel consumption in MPG. The MPG data set was collected from the UCI repository. However, this data set has six input variants, but in this example three variables (x1 = weight, x3 = acceleration) were selected. In the training process, 50% (196 samples) of the samples were randomized."}, {"heading": "D. Example 4\u2014Abalone Age Prediction", "text": "In this example, a prediction problem was used to predict a person's age based on their physical measurements; the abalone data set was collected from the UCI Machine Learning Repository [91]. It has 4177 data samples, each of which has seven input variables (x1 = length, x2 = diameter, x4 = total weight, x5 = total weight) and one output variable (rings). To train the proposed algorithms, randomized samples were taken for training and 20% (835 samples) remained samples for testing."}, {"heading": "E. Example 5\u2014Box-Jenkins Gas Furnace Problem", "text": "The aim of this example was to predict the CO2 concentration from the gas flow rate. The gas furnace system is modeled from a series of samples in the form of: y (k) = f (y (k \u2212 1), u (k \u2212 4). 100% (296 samples) of the samples were used to form the proposed models, as mentioned in [24]. In order to show an average performance of the proposed algorithms, the training process was also repeated ten times, and the collected results are summarized in Table VIII (a). The performance of the proposed algorithms (the best results) were compared with the algorithms reported in the literature (Table VIII (b)."}, {"heading": "F. Example 6\u2014Poly (lactic-co-glycolic acid) (PLGA) micro- and nanoparticle dissolution rate", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "VI. DISCUSSION", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "VII. CONCLUSIONS", "text": "The use of a fuzzy inference system (FIS) for data mining inherently requires a multi-objective solution, and the proposed multi-objective design for a hierarchical fuzzy inference tree (HFIT) is a viable option that constructs a tree-like model whose nodes are low-dimensional FIS. The proposed HFIT was developed for both type 1 and type 2 FIS, and each node in HFIT implements a Takagi-Sugeno-Kang model. Both Type 1 and Type 2 FIS problems were investigated in the context of single-objective and multi-objective optimization using genetic programming. Therefore, four versions of HFIT were examined: T1HFITS, T1HFITM, T2HFITS and T2HFITM. The parameters of membership functions and the resulting parts of the rules were optimized by proposing a FIT solution."}], "references": [{"title": "Fuzzy identification of systems and its applications to modeling and control", "author": ["T. Takagi", "M. Sugeno"], "venue": "IEEE Trans. Syst. Man Cybern., vol. 15, no. 1, pp. 116\u2013132, 1985.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Fuzzy sets", "author": ["L. Zadeh"], "venue": "Inf. Control, vol. 8, no. 3, pp. 338 \u2013 353, 1965.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1965}, {"title": "The concept of a linguistic variable and its application to approximate reasoning", "author": ["L.A. Zadeh"], "venue": "Inf. Sci., vol. 8, no. 3, pp. 199\u2013249, 1975.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1975}, {"title": "Type-2 fuzzy logic systems", "author": ["N.N. Karnik", "J.M. Mendel", "Q. Liang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 7, no. 6, pp. 643\u2013658, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "A hierarchical type-2 fuzzy logic control architecture for autonomous mobile robots", "author": ["H.A. Hagras"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 4, pp. 524\u2013539, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Intelligent behavior as an adaptation to the task environment", "author": ["L.B. Booker"], "venue": "Ph.D. dissertation, University of Michigan, Ann Arbor, MI, USA, 1982.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  46", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1982}, {"title": "A hybrid fuzzy genetics-based machine learning algorithm: hybridization of Michigan approach and Pittsburgh approach", "author": ["H. Ishibuchi", "T. Nakashima", "T. Kuroda"], "venue": "1999 Int. Conf. Systems, Man, and Cybernetics, 1999. IEEE SMC\u201999 Conf. Proc., vol. 1, pp. 296\u2013301.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "A learning system based on genetic adaptive algorithms", "author": ["S.F. Smith"], "venue": "Ph.D. dissertation, University of Pittsburgh, Pittsburgh, PA, USA, 1980.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1980}, {"title": "ANFIS: adaptive-network-based fuzzy inference system", "author": ["J.-S.R. Jang"], "venue": "IEEE Trans. Syst. Man Cybern., vol. 23, no. 3, pp. 665\u2013685, 1993.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Dynamic fuzzy neural networks\u2013a novel approach to function approximation", "author": ["S. Wu", "M.J. Er"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 30, no. 2, pp. 358\u2013364, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "A hybrid approach for design of stable adaptive fuzzy controllers employing lyapunov theory and particle swarm optimization", "author": ["K.D. Sharma", "A. Chatterjee", "A. Rakshit"], "venue": "IEEE Trans. Fuzzy Syst., vol. 17, no. 2, pp. 329\u2013342, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimization of the carpool service problem via a fuzzy-controlled genetic algorithm", "author": ["S.-C. Huang", "M.-K. Jiau", "C.-H. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 23, no. 5, pp. 1698\u20131712, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization of type-2 fuzzy systems based on bio-inspired methods: a concise review", "author": ["O. Castillo", "P. Melin"], "venue": "Inf. Sci., vol. 205, pp. 1\u201319, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Revisiting evolutionary fuzzy systems: Taxonomy, applications, new trends and challenges", "author": ["A. Fern\u00e1ndez", "V. L\u00f3pez", "M.J. del Jesus", "F. Herrera"], "venue": "Knowledge-Based Syst., vol. 80, pp. 109\u2013121, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "An online self-constructing neural fuzzy inference network and its applications", "author": ["C.-F. Juang", "C.-T. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 6, no. 1, pp. 12\u201332, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "A self-evolving interval type-2 fuzzy neural network with online structure and parameter learning", "author": ["C.-F. Juang", "Y.-W. Tsao"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 6, pp. 1411\u20131424, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Reduced interval type-2 neural fuzzy system using weighted bound-set boundary operation for computation speedup and chip implementation", "author": ["C.-F. Juang", "K.-J. Juang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 3, pp. 477\u2013491, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "DENFIS: dynamic evolving neural-fuzzy inference system and its application for time-series prediction", "author": ["N.K. Kasabov", "Q. Song"], "venue": "IEEE Trans. Fuzzy Syst., vol. 10, no. 2, pp. 144\u2013154, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "SaFIN: A self-adaptive fuzzy inference network", "author": ["S.W. Tung", "C. Quek", "C. Guan"], "venue": "IEEE Trans. Neural Netw, vol. 22, no. 12, pp. 1928\u20131940, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1928}, {"title": "A mutually recurrent interval type-2 neural fuzzy system (MRIT2NFS) with self-evolving structure and parameters", "author": ["Y.-Y. Lin", "J.-Y. Chang", "N.R. Pal", "C.-T. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 3, pp. 492\u2013509, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A TSK-type-based self-evolving compensatory interval type-2 fuzzy neural network (TSCIT2FNN) and its applications", "author": ["Y.-Y. Lin", "J.-Y. Chang", "C.-T. Lin"], "venue": "IEEE Trans. Ind. Electron., vol. 61, no. 1, pp. 447\u2013459, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Simplified interval type-2 fuzzy neural networks", "author": ["Y.-Y. Lin", "S.-H. Liao", "J.-Y. Chang", "C.-T. Lin"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 5, pp. 959\u2013969, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "GT2FC: an online growing interval type-2 self-learning fuzzy classifier", "author": ["A. Bouchachia", "C. Vanaret"], "venue": "IEEE Trans. Fuzzy Syst., vol. 22, no. 4, pp. 999\u20131018, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "An evolving interval type-2 neurofuzzy inference system and its metacognitive sequential learning algorithm", "author": ["A.K. Das", "K. Subramanian", "S. Sundaram"], "venue": "IEEE Trans. Fuzzy Syst., vol. 23, no. 6, pp. 2080\u20132093, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical fuzzy control", "author": ["G. Raju", "J. Zhou", "R.A. Kisner"], "venue": "Int. J. Control, vol. 54, no. 5, pp. 1201\u20131216, 1991.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1991}, {"title": "High dimensional neurofuzzy systems: overcoming the curse of dimensionality", "author": ["M. Brown", "K. Bossley", "D. Mills", "C. Harris"], "venue": "Proc. 1995 IEEE Int. Fuzzy Syst., 1995. Int. Jt. Conf. of the 4th Int. Conf. Fuzzy Syst. and The 2nd Int. Fuzzy Eng. Symp., vol. 4, pp. 2139\u20132146.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  47", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Analysis and design of hierarchical fuzzy systems", "author": ["L.-X. Wang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 7, no. 5, pp. 617\u2013624, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Hierarchical genetic fuzzy systems", "author": ["M.R. Delgado", "F. Von Zuben", "F. Gomide"], "venue": "Inf. Sci., vol. 136, no. 1, pp. 29\u201352, 2001.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling of hierarchical fuzzy systems", "author": ["M.-L. Lee", "H.-Y. Chung", "F.-M. Yu"], "venue": "Fuzzy Sets Syst., vol. 138, no. 2, pp. 343\u2013361, 2003.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Approximation capabilities of hierarchical fuzzy systems", "author": ["X.-J. Zeng", "J.A. Keane"], "venue": "IEEE Trans. Fuzzy Syst., vol. 13, no. 5, pp. 659\u2013672, 2005.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "A review of the construction of hierarchical fuzzy systems", "author": ["V. Torra"], "venue": "Int. J. Intell. Syst., vol. 17, no. 5, pp. 531\u2013543, 2002.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "A class of hierarchical fuzzy systems with constraints on the fuzzy rules", "author": ["M.G. Joo", "J.S. Lee"], "venue": "IEEE Trans. Fuzzy Syst., vol. 13, no. 2, pp. 194\u2013203, 2005.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Hierarchical fuzzy rule based classification systems with genetic rule selection for imbalanced data-sets", "author": ["A. Fern\u00e1ndez", "M.J. del Jesus", "F. Herrera"], "venue": "Int. J. Approximate Reasoning, vol. 50, no. 3, pp. 561\u2013577, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive fuzzy hierarchical sliding-mode control for the trajectory tracking of uncertain underactuated nonlinear dynamic systems", "author": ["C.-L. Hwang", "C.-C. Chiang", "Y.-W. Yeh"], "venue": "IEEE Trans. Fuzzy Syst., vol. 22, no. 2, pp. 286\u2013299, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic design of hierarchical takagi\u2013sugeno type fuzzy systems using evolutionary algorithms", "author": ["Y. Chen", "B. Yang", "A. Abraham", "L. Peng"], "venue": "IEEE Trans. Fuzzy Syst., vol. 15, no. 3, pp. 385\u2013397, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic incremental program evolution", "author": ["R. Salustowicz", "J. Schmidhuber"], "venue": "Evol. Comput., vol. 5, no. 2, pp. 123\u2013141, 1997.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Two-mode indirect adaptive control approach for the synchronization of uncertain chaotic systems by the use of a hierarchical interval type-2 fuzzy neural network", "author": ["A. Mohammadzadeh", "O. Kaynak", "M. Teshnehlab"], "venue": "IEEE Trans. Fuzzy Syst., vol. 22, no. 5, pp. 1301\u20131312, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiobjective genetic fuzzy systems: review and future research directions", "author": ["H. Ishibuchi"], "venue": "IEEE Int. Fuzzy Syst. Conf., 2007. FUZZ-IEEE 2007, pp. 1\u20136.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Single-objective and two-objective genetic algorithms for selecting linguistic rules for pattern classification problems", "author": ["H. Ishibuchi", "T. Murata", "I. T\u00fcrk\u015fen"], "venue": "Fuzzy Sets Syst., vol. 89, no. 2, pp. 135\u2013150, 1997.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1997}, {"title": "A multi-objective genetic algorithm for tuning and rule selection to obtain accurate and compact linguistic fuzzy rule-based systems", "author": ["R. Alcal\u00e1", "M.J. Gacto", "F. Herrera", "J. Alcal\u00e1-Fdez"], "venue": "Int. J. Uncertainty Fuzziness Knowledge Based Syst., vol. 15, no. 05, pp. 539\u2013557, 2007.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "A historical review of evolutionary learning methods for Mamdani-type fuzzy rule-based systems: Designing interpretable genetic fuzzy systems", "author": ["O. Cord\u00f3n"], "venue": "Int. J. Approximate Reasoning, vol. 52, no. 6, pp. 894\u2013913, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Designing fuzzy inference systems from data: an interpretability-oriented review", "author": ["S. Guillaume"], "venue": "IEEE Trans. Fuzzy Syst., vol. 9, no. 3, pp. 426\u2013443, 2001.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "Analysis of interpretability-accuracy trade-off of fuzzy systems by multiobjective fuzzy genetics-based machine learning", "author": ["H. Ishibuchi", "Y. Nojima"], "venue": "Int. J. Approximate Reasoning, vol. 44, no. 1, pp. 4\u201331, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptation and application of multi-objective evolutionary algorithms for rule reduction and parameter tuning of fuzzy rule-based systems", "author": ["M.J. Gacto", "R. Alcal\u00e1", "F. Herrera"], "venue": "Soft Comput., vol. 13, no. 5, pp. 419\u2013436, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "NMEEF-SD: non-dominated multiobjective evolutionary algorithm for extracting fuzzy rules in subgroup discovery", "author": ["C.J. Carmona", "P. Gonz\u00e1lez", "M.J. del Jesus", "F. Herrera"], "venue": "IEEE Trans. Fuzzy Syst., vol. 18, no. 5, pp. 958\u2013970, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiobjective optimization and comparison of nonsingleton type-1 and singleton interval type-2 fuzzy logic systems", "author": ["A.B. Cara", "C. Wagner", "H. Hagras", "H. Pomares", "I. Rojas"], "venue": "IEEE Trans. Fuzzy syst., vol. 21, no. 3, pp. 459\u2013476, 2013.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  48", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-objective hierarchical genetic algorithm for interpretable fuzzy rule-based knowledge extraction", "author": ["H. Wang", "S. Kwong", "Y. Jin", "W. Wei", "K.-F. Man"], "venue": "Fuzzy Sets Syst., vol. 149, no. 1, pp. 149\u2013186, 2005.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic tuning of a fuzzy visual system using evolutionary algorithms: single-objective versus multiobjective approaches", "author": ["R. Munoz-Salinas", "E. Aguirre", "O. Cord\u00f3n", "M. Garc\u0131\u0301a-Silvente"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 2, pp. 485\u2013501, 2008.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "A multiobjective evolutionary approach to concurrently learn rule and data bases of linguistic fuzzy-rule-based systems", "author": ["R. Alcal\u00e1", "P. Ducange", "F. Herrera", "B. Lazzerini", "F. Marcelloni"], "venue": "IEEE Trans. Fuzzy Syst., vol. 17, no. 5, pp. 1106\u20131122, 2009.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning knowledge bases of multi-objective evolutionary fuzzy systems by simultaneously optimizing accuracy, complexity and partition integrity", "author": ["M. Antonelli", "P. Ducange", "B. Lazzerini", "F. Marcelloni"], "venue": "Soft Comput., vol. 15, no. 12, pp. 2335\u20132354, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Genetic training instance selection in multiobjective evolutionary fuzzy systems: A coevolutionary approach", "author": ["M. Antonelli", "P. Ducange", "F. Marcelloni"], "venue": "IEEE Trans. Fuzzy Syst., vol. 20, no. 2, pp. 276\u2013290, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "A review of the application of multiobjective evolutionary fuzzy systems: Current status and further directions", "author": ["M. Fazzolari", "R. Alcala", "Y. Nojima", "H. Ishibuchi", "F. Herrera"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 1, pp. 45\u201365, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "A field guide to genetic programming", "author": ["R. Poli", "W.B. Langdon", "N.F. McPhee", "J.R. Koza"], "venue": "Lulu. com,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "Differential evolution algorithm with strategy adaptation for global numerical optimization", "author": ["A.K. Qin", "V.L. Huang", "P.N. Suganthan"], "venue": "IEEE Trans. Evol. Comput., vol. 13, no. 2, pp. 398\u2013417, 2009.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "On KM algorithms for solving type-2 fuzzy set problems", "author": ["J.M. Mendel"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 3, pp. 426\u2013446, 2013.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Time-series forecasting using flexible neural tree model", "author": ["Y. Chen", "B. Yang", "J. Dong", "A. Abraham"], "venue": "Inf. Sci., vol. 174, no. 3, pp. 219\u2013235, 2005.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2005}, {"title": "Evolutionary multi-objective optimization for simultaneous generation of signal-type and symbol-type representations", "author": ["Y. Jin", "B. Sendhoff", "E. K\u00f6rner"], "venue": "Evolutionary Multi-Criterion Optimization, ser. Lecture Notes in Computer Science, vol. 3410. Springer, 2005, pp. 752\u2013766.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2005}, {"title": "A fast elitist non-dominated sorting genetic algorithm for multiobjective optimization: NSGA-II", "author": ["K. Deb", "S. Agrawal", "A. Pratap", "T. Meyarivan"], "venue": "Parallel Problem Solving from Nature PPSN VI, ser. Lecture Notes in Computer Science. Springer, 2000, vol. 1917, pp. 849\u2013858.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2000}, {"title": "Introduction to Evolutionary Computing, 2nd ed", "author": ["A.E. Eiben", "J.E. Smith"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2003}, {"title": "Ensemble of heterogeneous flexible neural trees using multiobjective genetic programming", "author": ["V.K. Ojha", "A. Abraham", "V. Sn\u00e1\u0161el"], "venue": "Appl. Soft Comput., vol. 52, pp. 909\u2013924, 2017.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2017}, {"title": "A powerful and efficient algorithm for numerical function optimization: Artificial bee colony (ABC) algorithm", "author": ["D. Karaboga", "B. Basturk"], "venue": "J. Global Optim., vol. 39, no. 3, pp. 459\u2013471, 2007.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "Particle swarm optimization", "author": ["R. Poli", "J. Kennedy", "T. Blackwell"], "venue": "Swarm Intell., vol. 1, no. 1, pp. 33\u201357, 2007.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2007}, {"title": "Recent advances in differential evolution\u2013an updated survey", "author": ["S. Das", "S.S. Mullick", "P. Suganthan"], "venue": "Swarm and Evolutionary Computation, vol. 27, pp. 1\u201330, 2016.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical mathematical optimization: an introduction to basic optimization theory and classical and new gradient-based algorithms", "author": ["J. Snyman"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2005}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proc. IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1990}, {"title": "Kalman filtering and Neural Networks", "author": ["S.S. Haykin"], "venue": "Wiley Online Library,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2001}, {"title": "Finite markov chain analysis of genetic algorithms", "author": ["D.E. Goldberg", "P. Segrest"], "venue": "Proc. of the 2nd Int. Conf. on Genetic Algorithms and Their Appl. Hillsdale, NJ, USA: L. Erlbaum Associates Inc., 1987, pp. 1\u20138.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1987}, {"title": "Contractive mapping genetic algorithms and their convergence", "author": ["A. Szalas", "Z. Michalewicz"], "venue": "Dept. of Computer Science, University of North Carolina at Charlotte, Tech. Rep. Technical Report 006-1993, 1993.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1993}, {"title": "Sur les op\u00e9rations dans les ensembles abstraits et leur application aux \u00e9quations int\u00e9grales (in French)", "author": ["S. Banach"], "venue": "Fund. Math., vol. 3, no. 1, pp. 133\u2013181, 1922.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 1922}, {"title": "The evolution of evolvability in genetic programming", "author": ["L. Altenberg"], "venue": "Advances in Genetic Programming, K. E. Kinnear Jr., Ed., vol. 3. MIT Press, 1994, pp. 47\u201374.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 1994}, {"title": "On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition", "author": ["A.N. Kolmogorov"], "venue": "Transl. Amer. Math. Soc., vol. 28, no. 2, pp. 55\u201359, 1963.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1963}, {"title": "Modeling and convergence analysis of a continuous multi-objective differential evolution algorithm", "author": ["F. Xue", "A.C. Sanderson", "R.J. Graves"], "venue": "Evolutionary Computation, 2005. The 2005 IEEE Congress on, vol. 1. IEEE, 2005, pp. 228\u2013235.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2005}, {"title": "Theoretical analysis of differential evolution", "author": ["J. Zhang", "A.C. Sanderson"], "venue": "Adaptive Differential Evolution. Springer, 2009, pp. 15\u201338.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2009}, {"title": "Sufficient conditions for global convergence of differential evolution algorithm", "author": ["Z. Hu", "S. Xiong", "Q. Su", "X. Zhang"], "venue": "J. Appl. Math., vol. 2013, 2013.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Subsethood-product fuzzy neural inference system (SuPFuNIS)", "author": ["S. Paul", "S. Kumar"], "venue": "IEEE Trans. Neural Netw, vol. 13, no. 3, pp. 578\u2013599, 2002.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2002}, {"title": "Differential evolution\u2013a simple and efficient heuristic for global optimization over continuous spaces", "author": ["R. Storn", "K. Price"], "venue": "J. Glob. Optim., vol. 11, no. 4, pp. 341\u2013359, Dec 1997.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1997}, {"title": "Evolving fuzzy neural networks for supervised/unsupervised online knowledge-based learning", "author": ["N. Kasabov"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 31, no. 6, pp. 902\u2013918, 2001.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2001}, {"title": "An ART-based fuzzy adaptive learning control network", "author": ["C.-J. Lin", "C.-T. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 5, no. 4, pp. 477\u2013496, 1997.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1997}, {"title": "Granular neural networks with evolutionary interval learning", "author": ["Y.-Q. Zhang", "B. Jin", "Y. Tang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 2, pp. 309\u2013319, 2008.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2008}, {"title": "HyFIS: adaptive neuro-fuzzy inference systems and their application to nonlinear dynamical systems", "author": ["J. Kim", "N. Kasabov"], "venue": "Neural Netw., vol. 12, no. 9, pp. 1301\u20131319, 1999.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 1999}, {"title": "Multilevel fuzzy relational systems: structure and identification", "author": ["J.-C. Duan", "F.-L. Chung"], "venue": "Soft Comput., vol. 6, no. 2, pp. 71\u201386, 2002.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2002}, {"title": "Radial basis function based adaptive fuzzy systems and their applications to system identification and prediction", "author": ["K.B. Cho", "B.H. Wang"], "venue": "Fuzzy Sets Syst., vol. 83, no. 3, pp. 325\u2013339, 1996.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 1996}, {"title": "Support vector learning mechanism for fuzzy rule-based modeling: a new approach", "author": ["J.-H. Chiang", "P.-Y. Hao"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 1, pp. 1\u201312, 2004.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2004}, {"title": "eT2FIS: an evolving type-2 neural fuzzy inference system", "author": ["S.W. Tung", "C. Quek", "C. Guan"], "venue": "Inf. Sci., vol. 220, pp. 124\u2013148, 2013.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2013}, {"title": "An interval type-2 fuzzy-neural network with support-vector regression for noisy regression problems", "author": ["C.-F. Juang", "R.-B. Huang", "W.-Y. Cheng"], "venue": "IEEE Trans. Fuzzy Syst., vol. 18, no. 4, pp. 686\u2013699, 2010.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2010}, {"title": "An approach to online identification of Takagi-Sugeno fuzzy models", "author": ["P.P. Angelov", "D.P. Filev"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 34, no. 1, pp. 484\u2013498, 2004.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  50", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2004}, {"title": "Uncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions", "author": ["J.M. Mendel"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2001}, {"title": "Computing derivatives in interval type-2 fuzzy logic systems", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 1, pp. 84\u201398, 2004.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2004}, {"title": "Identification and control of dynamical systems using neural networks", "author": ["K.S. Narendra", "K. Parthasarathy"], "venue": "IEEE Trans. Neural Netw, vol. 1, no. 1, pp. 4\u201327, 1990.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 1990}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013, accessed on: 01.05.2016. [Online]. Available: http: //archive.ics.uci.edu/ml", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2013}, {"title": "A generalized concept for fuzzy rule interpolation", "author": ["P. Baranyi", "L.T. K\u00f3czy", "T.T.D. Gedeon"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 6, pp. 820\u2013837, 2004.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2004}, {"title": "Fuzzy interpolative reasoning for sparse fuzzy-rule-based systems based on the areas of fuzzy sets", "author": ["Y.-C. Chang", "S.-M. Chen", "C.-J. Liau"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 5, pp. 1285\u20131301, 2008.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2008}, {"title": "Fuzzy interpolation and extrapolation: A practical approach", "author": ["Z. Huang", "Q. Shen"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 1, pp. 13\u201328, 2008.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2008}, {"title": "Weighted fuzzy rule interpolation based on GA-based weight-learning techniques", "author": ["S.-M. Chen", "Y.-C. Chang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 19, no. 4, pp. 729\u2013744, 2011.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2011}, {"title": "Time Series Analysis, Forecasting and Control", "author": ["G.E.P. Box", "G.M. Jenkins"], "venue": null, "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1976}, {"title": "Heuristic modeling of macromolecule release from PLGA microspheres", "author": ["J. Szlek", "A. Pac\u0142awski", "R. Lau", "R. Jachowicz", "A. Mendyk"], "venue": "Int. J. Nanomed., vol. 8, no. 1, pp. 4601\u20134611, 2013.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2013}, {"title": "Dimensionality reduction, and function approximation of poly (lactic-co-glycolic acid) micro-and nanoparticle dissolution rate", "author": ["V.K. Ojha", "K. Jackowski", "A. Abraham", "V. Sn\u00e1\u0161el"], "venue": "Int. J. Nanomed., vol. 10, pp. 1119 \u2013 1129, 2015.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2015}, {"title": "Synthesis and characterization of PLGA nanoparticles", "author": ["C.E. Astete", "C.M. Sabliov"], "venue": "J. Biomater. Sci., Polym. Ed., vol. 17, no. 3, pp. 247\u2013289, 2006.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "Designing materials for biology and medicine", "author": ["R. Langer", "D.A. Tirrell"], "venue": "Nature, vol. 428, no. 6982, pp. 487\u2013492, 2004.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2004}, {"title": "Poly lactic-co-glycolic acid (PLGA) as biodegradable controlled drug delivery carrier", "author": ["H.K. Makadia", "S.J. Siegel"], "venue": "Polymers (Basel), vol. 3, no. 3, pp. 1377\u20131397, 2011.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble of heterogeneous flexible neural tree for the approximation and feature-selection of poly (lactic-co-glycolic acid) micro-and nanoparticle", "author": ["V.K. Ojha", "A. Abraham", "V. Snasel"], "venue": "Proc. the 2nd Int. Afro-European Conf. for Ind. Adv. AECIA 2015, 2016, pp. 155\u2013165.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The Takagi\u2013Sugeno\u2013Kang (TSK) is a widely used FIS model [1].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Unlike the crisp output of a T1FS membership function (MF) [2], the output of a T2FS MF is fuzzy in nature [3].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Unlike the crisp output of a T1FS membership function (MF) [2], the output of a T2FS MF is fuzzy in nature [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Such nature of the T2FS MFs is advantageous in processing uncertain information more effectively than with T1FS MFs [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Hence, a T2FIS can overcome the inability of a T1FIS to fully handle or accommodate the linguistic and numerical uncertainties associated with a changing and dynamic environment [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "The interval T2FIS (IT2FIS) reduces the computational cost by employing a simplified T2FS, known as interval T2FS (IT2FS) [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "An IT2FS MF is bounded by a lower MF (LMF) and an upper MF (UMF), and the area between the LMF and UMF is called the footprint of uncertainty [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "Such a form of rule optimization is often achieved by mapping the rule\u2019s parameters onto a real-valued genetic vector, and it is known as the Michigan Approach [6].", "startOffset": 160, "endOffset": 163}, {"referenceID": 6, "context": "is often achieved by mapping the rules onto a binary-valued genetic vector [7], and it is known as the Pittsburgh Approach [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "is often achieved by mapping the rules onto a binary-valued genetic vector [7], and it is known as the Pittsburgh Approach [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "However, FIS optimization is not limited only to its mapping onto the genetic vector, but a structural/network-like implementation of FIS is often performed [9].", "startOffset": 157, "endOffset": 160}, {"referenceID": 9, "context": "Additionally, TSK-based hierarchical self-organizing learning dynamics have also been proposed [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "Moreover, several researchers have focused on the FIS and neural network (NN) integration and its parameter optimization using various learning methods including gradient-decent and the metaheuristic algorithms [11]\u2013[14].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "Moreover, several researchers have focused on the FIS and neural network (NN) integration and its parameter optimization using various learning methods including gradient-decent and the metaheuristic algorithms [11]\u2013[14].", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "[15], is a six layered network structure whose optimization begins with no rule and then rules are incrementally added during the learning process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Later, in [16], SONFIN\u2019s concept was extended for the construction of T2FIS, where a self-evolving IT2FIS (SEIT2FNN) that implements a TSK-type FIS model was proposed, and the parameters of the evolved structure were tuned by using the Kalman-filtering algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Additionally, a simplified type-reduction process for SEIT2FNN was proposed in [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Like SONFIN, in [18], a TSK-type FIS model, called a dynamic evolving neural-fuzzy inference system (DENFIS), was proposed, which evolved incrementally by choosing active rules from a set of rules and employed an evolving clustering method to partition the input space and the least-square estimator to optimize its parameters.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "[19] proposed a self-adaptive fuzzy inference network (SaFIN) that applied a categorical learning induced partitioning algorithm to eliminate two limitations: 1) the need for predefined numbers of fuzzy clusters and 2) the stability\u2013plasticity trade-off that addresses the difficulty in finding a balance between past knowledge and current knowledge during the learning process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In [20], to improve the efficiency of IT2FIS, a mutually recurrent interval type-2 neural fuzzy system (MRIT2NFS) was proposed which used weighted feedback loops in the antecedent parts of the formed rules and applied gradient-decent learning and a Kalman-filter algorithm to tune the recurrent weights and the rules\u2019 parameters, respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [21], a self-evolving T2FIS model", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Further, a simplified interval type-2 fuzzy NN with a simplified type-reduction process (SIT2FIS) was proposed in [22], and a growing online self-learning IT2FIS that used the dynamics of a growing Gaussian mixture model was proposed in [23].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "Further, a simplified interval type-2 fuzzy NN with a simplified type-reduction process (SIT2FIS) was proposed in [22], and a growing online self-learning IT2FIS that used the dynamics of a growing Gaussian mixture model was proposed in [23].", "startOffset": 237, "endOffset": 241}, {"referenceID": 23, "context": "Recently, in [24], a meta-cognitive interval type-2 neuro FIS (McIT2FIS) was proposed, which employs a self-regulatory metacognitive system that extracts the knowledge contained in minimal samples by accepting or discarding data samples based on sample\u2019s contribution to knowledge.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Contrary to this, a hierarchical FIS (HFIS) constructs an FIS by using a hierarchical arrangement of several low-dimensional fuzzy subsystems [25].", "startOffset": 142, "endOffset": 146}, {"referenceID": 25, "context": "Moreover, HFIS design overcomes the curse of dimensionality [26], and it possesses a universal approximation ability [27]\u2013[30].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "Moreover, HFIS design overcomes the curse of dimensionality [26], and it possesses a universal approximation ability [27]\u2013[30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 29, "context": "Moreover, HFIS design overcomes the curse of dimensionality [26], and it possesses a universal approximation ability [27]\u2013[30].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "[31] summarized the contributions where the expert\u2019s role in the HFIS design process was minimized/eliminated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "For example, in [32], HFIS was realized as a feedforward network like structure in which the output of the previous layer\u2019s subsystem was only fed to the consequent part of the next layer, and so on.", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "Similarly, in [33], a two-layered HFIS was developed, where, for each layer, the knowledge bases (KB) were generated by linguistics rule generation method and the KB rules were selected by genetic algorithm (GA).", "startOffset": 14, "endOffset": 18}, {"referenceID": 33, "context": "In [34], an adaptive fuzzy hierarchical sliding-mode control method was proposed, which was an arrangement of many subsystems, and the top layer accommodated all the subsystems\u2019 outputs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "Moreover, in [35], to optimize the structure of a hierarchical arrangement of low-dimensional TSK-type FISs, probabilistic incremental program evolution [36] was employed.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "Moreover, in [35], to optimize the structure of a hierarchical arrangement of low-dimensional TSK-type FISs, probabilistic incremental program evolution [36] was employed.", "startOffset": 153, "endOffset": 157}, {"referenceID": 4, "context": "Similarly, the importance of the hierarchical arrangements of the low-dimensional T2FSs is explained in [5], [37].", "startOffset": 104, "endOffset": 107}, {"referenceID": 36, "context": "Similarly, the importance of the hierarchical arrangements of the low-dimensional T2FSs is explained in [5], [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 37, "context": ", self-organizing fuzzy NN and HFIS models), multiobjective optimization is inherent since accuracy maximization and complexity minimization are two desirable objectives [38].", "startOffset": 170, "endOffset": 174}, {"referenceID": 38, "context": "Hence, to make trade-offs between interpretability and accuracy, or, in other words, to make trade-offs between approximation error minimization and complexity minimization, a multiobjective orientation of FIS optimization can be used [39]\u2013 [41].", "startOffset": 235, "endOffset": 239}, {"referenceID": 40, "context": "Hence, to make trade-offs between interpretability and accuracy, or, in other words, to make trade-offs between approximation error minimization and complexity minimization, a multiobjective orientation of FIS optimization can be used [39]\u2013 [41].", "startOffset": 241, "endOffset": 245}, {"referenceID": 40, "context": "[41], [42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[41], [42].", "startOffset": 6, "endOffset": 10}, {"referenceID": 42, "context": "[43]\u2013[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[43]\u2013[46].", "startOffset": 5, "endOffset": 9}, {"referenceID": 46, "context": "Similarly, in [47]\u2013[50], simultaneous learning of KB was proposed, which included feature selection, rule complexity minimization together with approximation error minimization, etc.", "startOffset": 14, "endOffset": 18}, {"referenceID": 49, "context": "Similarly, in [47]\u2013[50], simultaneous learning of KB was proposed, which included feature selection, rule complexity minimization together with approximation error minimization, etc.", "startOffset": 19, "endOffset": 23}, {"referenceID": 50, "context": "Moreover, in [51], a co-evolutionary approach that aimed at combining a multiobjective approach with a single objective approach was presented where, at first, a multiobjective GA determined a Pareto-optimal solution by finding a trade-off between accuracy and rule complexity.", "startOffset": 13, "endOffset": 17}, {"referenceID": 51, "context": "A summary of research works focused on multiobjective optimization of FIS is provided in [52].", "startOffset": 89, "endOffset": 93}, {"referenceID": 52, "context": "Unlike the self-organizing paradigm that has a network-like structure and uses a clustering algorithm for partitioning of input space, the proposed HFIT constructs a tree-like structure and uses the dynamics of the evolutionary algorithm for partitioning input space [53].", "startOffset": 267, "endOffset": 271}, {"referenceID": 53, "context": "The parameter tuning of the HFIT was performed by the differential evolution (DE) algorithm [54], which is a metaheuristic algorithm inspired by the dynamics of the evolutionary process.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "Hence, they are useful in finding the appropriate parameter values for an FIS [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "In the construction of type-2 HFITs, the type-reduction algorithm of the Karnik-Mendel method described in [4] was used with an improvement in its termination criteria.", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "A TSK-type FIS is governed by the IF\u2013THEN rules of the form [1]:", "startOffset": 60, "endOffset": 63}, {"referenceID": 54, "context": "A T2FS \u00c3 is characterized by a 3-dimensional (3-D) MF [55].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Hence, in a universal set X , a T2FS \u00c3 has the form: \u00c3 = {((x, u) , \u03bc\u00c3 (x, u)) |\u2200x \u2208 X, \u2200u \u2208 [0, 1]} (6)", "startOffset": 93, "endOffset": 99}, {"referenceID": 3, "context": "In this work, the LMF was defined as [4]:", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "and the UMF was defined as [4]:", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "In this work, the center of set type-reducer ycos, prescribed in [4], was used:", "startOffset": 65, "endOffset": 68}, {"referenceID": 55, "context": "[56], which has two", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Thereafter, the type-reduction of the node is performed as described in [4], where the left and right intervals y l and y 1 r are computed as per (14) and (15).", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "During type-reduction [4], an early stopping mechanism was adopted to reduce computation time.", "startOffset": 22, "endOffset": 25}, {"referenceID": 56, "context": ", the number of parameters c(w) in the model) [57].", "startOffset": 46, "endOffset": 50}, {"referenceID": 57, "context": "Hence, a Pareto-based multiobjective optimization can be applied to obtained a Pareto set of nondominated solutions, in which no one objective function can be improved without a simultaneous detriment to at least one of the other objectives of the solution [58] Therefore, an HFIT that offers the lowest approximation error and simplest structure is the most desirable one.", "startOffset": 257, "endOffset": 261}, {"referenceID": 57, "context": "The proposed MOGP acquires the nondominated sorting algorithm [58] for computing Paretooptimal solutions from an initial population of fuzzy inference trees.", "startOffset": 62, "endOffset": 66}, {"referenceID": 57, "context": "Diversity in population was maintained by measuring the crowding distance among the individuals [58].", "startOffset": 96, "endOffset": 100}, {"referenceID": 58, "context": "A detailed description of the crossover operation in genetic programming is available in [59], [60].", "startOffset": 89, "endOffset": 93}, {"referenceID": 59, "context": "A detailed description of the crossover operation in genetic programming is available in [59], [60].", "startOffset": 95, "endOffset": 99}, {"referenceID": 58, "context": "5) Mutation: The mutation operators used in HFIT are as follows [59], [60]: a) Replacing a randomly selected terminal xi \u2208 T with a newly generated terminal xj \u2208 T for j 6= i.", "startOffset": 64, "endOffset": 68}, {"referenceID": 59, "context": "5) Mutation: The mutation operators used in HFIT are as follows [59], [60]: a) Replacing a randomly selected terminal xi \u2208 T with a newly generated terminal xj \u2208 T for j 6= i.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "The Gaussian means m1 and m2 for type-2 Gaussian MF (7) were defined as: m1 = m+ \u03bb\u03c3 and m2 = m\u2212 \u03bb\u03c3, where \u03bb \u2208 [0, 1] is a random variable taken from uniform distribution and m is the center of the Gaussian means m1 and m2 taken from [0, 1].", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "The Gaussian means m1 and m2 for type-2 Gaussian MF (7) were defined as: m1 = m+ \u03bb\u03c3 and m2 = m\u2212 \u03bb\u03c3, where \u03bb \u2208 [0, 1] is a random variable taken from uniform distribution and m is the center of the Gaussian means m1 and m2 taken from [0, 1].", "startOffset": 233, "endOffset": 239}, {"referenceID": 0, "context": "Similarly, the variance \u03c3 of type-2 Gaussian MF (7) was taken from [0, 1].", "startOffset": 67, "endOffset": 73}, {"referenceID": 58, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 91, "endOffset": 95}, {"referenceID": 58, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 116, "endOffset": 120}, {"referenceID": 60, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 144, "endOffset": 148}, {"referenceID": 61, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 62, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 163, "endOffset": 167}, {"referenceID": 63, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 64, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 217, "endOffset": 221}, {"referenceID": 65, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 237, "endOffset": 241}, {"referenceID": 53, "context": "In this work, the differential evolution (DE) version \u201cDE/rand-to-best/1/bin\u201d [54] was used, which is a metaheuristic algorithm that uses a crossover operator inspired by the dynamics of \u201cnatural selection.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "where rj \u2208 [0, 1] is a uniform random sample, cr \u2208 [0, 1] is the crossover rate, and F \u2208 [0, 2] is the differential weight.", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where rj \u2208 [0, 1] is a uniform random sample, cr \u2208 [0, 1] is the crossover rate, and F \u2208 [0, 2] is the differential weight.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "where rj \u2208 [0, 1] is a uniform random sample, cr \u2208 [0, 1] is the crossover rate, and F \u2208 [0, 2] is the differential weight.", "startOffset": 89, "endOffset": 95}, {"referenceID": 66, "context": "Initially, Goldberg and Sergret [67] showed convergence property of GA using a finite Markov chain analysis, where they considered GA with a finite population and recombination and mutation operators.", "startOffset": 32, "endOffset": 36}, {"referenceID": 67, "context": "A different viewpoint of MOGP convergence (EAs in general) can be referred to as by using Banach fixpoint theorem described in [68].", "startOffset": 127, "endOffset": 131}, {"referenceID": 68, "context": "Banach fixpoint theorem [69] states that on a metric space a constructive mapping f has a unique fixpoint, i.", "startOffset": 24, "endOffset": 28}, {"referenceID": 69, "context": "Altenberg [71] showed that by maintaining genetic operators, such as selection, crossover, and mutation, the evolvability of genetic programming can be increased.", "startOffset": 10, "endOffset": 14}, {"referenceID": 69, "context": "Additionally, Altenberg [71] analyzed the probability of a population containing fitter individuals than the previous population and offered the subsequent proof.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "Theoretical studies of special cases of the hierarchical fuzzy systems are provided in [27], [32].", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "Theoretical studies of special cases of the hierarchical fuzzy systems are provided in [27], [32].", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "In HFIT, not only a cascaded hierarchy of fuzzy system (a fuzzy subsystem takes input only from its previous fuzzy subsystem [27]) can be produced, but a general hierarchical fuzzy system, in which a fuzzy subsystem can take inputs from any previous layer fuzzy subsystem, can be produced.", "startOffset": 125, "endOffset": 129}, {"referenceID": 29, "context": "A hierarchical fuzzy system described in [30] resembles the hierarchical fuzzy system produced by HFIT.", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "To show the approximation capability of the proposed HFIT, it requires coming to the conclusion that the proposed HFIT is analogous to the hierarchical fuzzy system described by Zeng and Keane [30].", "startOffset": 193, "endOffset": 197}, {"referenceID": 29, "context": "Let\u2019s perform an analogy between the proposed HFIT and the concept of a natural hierarchical fuzzy system described by Zeng and Keane [30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": ", the individual functions can be decomposed [30].", "startOffset": 45, "endOffset": 49}, {"referenceID": 70, "context": "Now, from Kolmogorov\u2019s Theorem [72], the following can be stated: Any continuous function N(x1, .", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "Proof of Theorem 2 can be found in [30].", "startOffset": 35, "endOffset": 39}, {"referenceID": 71, "context": "Convergence property and efficiency of DE is well studied [73], [74].", "startOffset": 58, "endOffset": 62}, {"referenceID": 72, "context": "Convergence property and efficiency of DE is well studied [73], [74].", "startOffset": 64, "endOffset": 68}, {"referenceID": 73, "context": "A probabilistic viewpoint of DE convergence followed by a description of global convergence condition for DE is described in [75].", "startOffset": 125, "endOffset": 129}, {"referenceID": 72, "context": "Similarly, Zhang and Sanderson [74] studied the various property of DE, such as mutation, crossover and recombination operators that influence the DE convergence.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "However, the overlapping of the membership function of the fuzzy sets is another common problem with clustering based input space partitioning [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 74, "context": "In [76], authors pointed out four different cases of membership function\u2019s overlapping and proposed subsethood method to transmit the overlapping information to the rules layer.", "startOffset": 3, "endOffset": 7}, {"referenceID": 75, "context": ", DE) [77].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Algorithm training parameter Value Maximum depth (layers) of a tree 4 Maximum inputs to an FIS node 4 Membership function search range [0,1] GP population 50 CP mutation probability pm 0.", "startOffset": 135, "endOffset": 140}, {"referenceID": 17, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 101, "endOffset": 105}, {"referenceID": 76, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 172, "endOffset": 176}, {"referenceID": 77, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 248, "endOffset": 252}, {"referenceID": 78, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 336, "endOffset": 340}, {"referenceID": 34, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 395, "endOffset": 399}, {"referenceID": 79, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 474, "endOffset": 478}, {"referenceID": 80, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 559, "endOffset": 563}, {"referenceID": 81, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 658, "endOffset": 662}, {"referenceID": 18, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 750, "endOffset": 754}, {"referenceID": 14, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 835, "endOffset": 839}, {"referenceID": 74, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 928, "endOffset": 932}, {"referenceID": 82, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 1018, "endOffset": 1022}, {"referenceID": 83, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 16, "endOffset": 20}, {"referenceID": 84, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 222, "endOffset": 226}, {"referenceID": 85, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 334, "endOffset": 338}, {"referenceID": 16, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 436, "endOffset": 440}, {"referenceID": 16, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 514, "endOffset": 518}, {"referenceID": 15, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 596, "endOffset": 600}, {"referenceID": 21, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 661, "endOffset": 665}, {"referenceID": 86, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 751, "endOffset": 755}, {"referenceID": 87, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 826, "endOffset": 830}, {"referenceID": 20, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 908, "endOffset": 912}, {"referenceID": 15, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 128, "endOffset": 132}, {"referenceID": 20, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 140, "endOffset": 144}, {"referenceID": 84, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 146, "endOffset": 150}, {"referenceID": 88, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": ", 400 as mentioned in [17].", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "For the performance comparisons, the SaFIN result was collected from [19], and FALCON and SONFIN from [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "For the performance comparisons, the SaFIN result was collected from [19], and FALCON and SONFIN from [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 195, "endOffset": 199}, {"referenceID": 84, "context": "In this example, the objective was to predict x(k) using the past outputs of the time series as mentioned in [86].", "startOffset": 109, "endOffset": 113}, {"referenceID": 84, "context": "2 [86].", "startOffset": 2, "endOffset": 6}, {"referenceID": 84, "context": "From the generated clean patterns, as mentioned in [86], the first 500 patterns (clean training set) were used for training purposes and the second 500 patterns (clean test set) were used for test purposes.", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 85, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 205, "endOffset": 209}, {"referenceID": 84, "context": "3 to the original data x(k) as described in [86].", "startOffset": 44, "endOffset": 48}, {"referenceID": 84, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 83, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 160, "endOffset": 164}, {"referenceID": 84, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 219, "endOffset": 223}, {"referenceID": 21, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 241, "endOffset": 245}, {"referenceID": 89, "context": "The MPG dataset was collected from the UCI machine learning repository [91].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "This dataset has 392 samples, each of which has six input variables, but in this example, as mentioned in [24], three variables (x1 = weight, x2 = acceleration, and x3 = model year) were selected.", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "For the comparisons, the T1FLS result was collected from [17] and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 57, "endOffset": 61}, {"referenceID": 23, "context": "For the comparisons, the T1FLS result was collected from [17] and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 89, "context": "The Abalone dataset was collected from the UCI machine learning repository [91].", "startOffset": 75, "endOffset": 79}, {"referenceID": 16, "context": "For the comparisons, the results of General, HS, CCL, and Chen&Cheng were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "For the comparisons, the results of General, HS, CCL, and Chen&Cheng were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 185, "endOffset": 189}, {"referenceID": 90, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 23, "endOffset": 27}, {"referenceID": 91, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 33, "endOffset": 37}, {"referenceID": 92, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 42, "endOffset": 46}, {"referenceID": 93, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 60, "endOffset": 64}, {"referenceID": 94, "context": "In this example, the Box and Jenkins gas furnace dataset that was taken from [96], which has 296 data samples.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "For the training of the proposed models, as mentioned in [24],", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "To compare the performance of the algorithms, the results of T1-NFS and GNN were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "To compare the performance of the algorithms, the results of T1-NFS and GNN were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 192, "endOffset": 196}, {"referenceID": 16, "context": "66 minutes for the training [17].", "startOffset": 28, "endOffset": 32}, {"referenceID": 95, "context": "As per the dataset provided in [97], [98], this problem has 747 samples and a total of 300 input features, which influence the PLGA protein particle\u2019s dissolution rate [99].", "startOffset": 31, "endOffset": 35}, {"referenceID": 96, "context": "As per the dataset provided in [97], [98], this problem has 747 samples and a total of 300 input features, which influence the PLGA protein particle\u2019s dissolution rate [99].", "startOffset": 37, "endOffset": 41}, {"referenceID": 97, "context": "As per the dataset provided in [97], [98], this problem has 747 samples and a total of 300 input features, which influence the PLGA protein particle\u2019s dissolution rate [99].", "startOffset": 168, "endOffset": 172}, {"referenceID": 98, "context": "The PLGA dissolution profile prediction is a significant problem since it plays a crucial role in the medical application and toxicity evaluation of PLGA-based microparticles dosages [100].", "startOffset": 183, "endOffset": 188}, {"referenceID": 99, "context": "It is also used as a filler, as an excipient, and as an active pharmaceutical ingredient because it acts as a catalyst for drug absorption/dissolution/solubility [101].", "startOffset": 162, "endOffset": 167}, {"referenceID": 95, "context": "of features MLP [97] 14.", "startOffset": 16, "endOffset": 20}, {"referenceID": 100, "context": "3 17 HFIT [102] 13.", "startOffset": 10, "endOffset": 15}, {"referenceID": 96, "context": "2 15 REP Tree [98] 13.", "startOffset": 14, "endOffset": 18}, {"referenceID": 96, "context": "3 15 GPR [98] 14.", "startOffset": 9, "endOffset": 13}, {"referenceID": 96, "context": "9 15 MLP [98] 15.", "startOffset": 9, "endOffset": 13}, {"referenceID": 95, "context": "2 15 MLP [97] 15.", "startOffset": 9, "endOffset": 13}], "year": 2017, "abstractText": "This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An HFIT produces an optimum tree-like structure. Specifically, a natural hierarchical structure that accommodates simplicity by combining several low-dimensional fuzzy inference systems (FISs). Such a natural hierarchical structure provides a high degree of approximation accuracy. The construction of HFIT takes place in two phases. Firstly, a nondominated sorting based multiobjective genetic programming (MOGP) is applied to obtain a simple tree structure (low model\u2019s complexity) with a high accuracy. Secondly, the differential evolution algorithm is applied to optimize the obtained tree\u2019s parameters. In the obtained tree, each node has a different input\u2019s combination, where the evolutionary process governs the input\u2019s combination. Hence, HFIT nodes are heterogeneous in nature, which leads to a high diversity among the rules generated by the HFIT. Additionally, the HFIT provides an automatic feature selection because it uses MOGP for the tree\u2019s structural optimization that accept inputs only relevant to the knowledge contained in data. The HFIT was studied in the context of both type-1 and type-2 FISs, and its performance was evaluated through six application problems. Moreover, the proposed multiobjective HFIT was compared both theoretically and empirically with recently proposed FISs methods from the literature, such as McIT2FIS, TSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the obtained results, it was found that the HFIT provided less complex and highly accurate models compared to the models produced by most of the other methods. Hence, the proposed HFIT is an efficient and competitive alternative to the other FISs for function approximation and feature selection. V K Ojha is with the Chair of Information Architecture, ETH Zurich, Zurich, Switzerland e-mail: ojha@arch.ethz.ch V Sn\u00e1\u0161el is with the Dept. of Computer Science, Technical University of Ostrava, Czech Republic, e-mail: vaclav.snasel@vsb.cz A Abraham is with Machine Intelligence Research Labs, Washington, USA, e-mail: ajith.abraham@ieee.org This work was supported by the IPROCOM Marie Curie initial training network, funded through the People Programme (Marie Curie Actions) of the European Union\u2019s Seventh Framework Programme FP7/2007-2013/ under REA Grant Agreement No. 316555. Manuscript received Month xx, yyyy; revised Month xx, yyyy. ar X iv :1 70 5. 05 76 9v 1 [ cs .A I] 1 6 M ay 2 01 7 JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY 2", "creator": "LaTeX with hyperref package"}}}