{"id": "1705.05940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Subregular Complexity and Deep Learning", "abstract": "This paper presents experiments illustrating how formal language theory can shed light on deep learning. We train naive Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) on six formal languages drawn from the Strictly Local (SL) and Strictly Piecewise (SP) classes. These classes are relevant to computational linguistics and among the simplest in a mathematically well-understood hierarchy of subregular classes. SL and SP classes encode local and long-distance dependencies, respectively. The results show four of the six languages were learned remarkably well, but overfitting arguably occurred with the simplest SL language and undergeneralization with the most complex SP pattern. Even though LSTMs were developed to handle long-distance dependencies, the latter result shows they stymie naive LSTMs in contrast to local dependencies. While it remains to be seen which of the many variants of LSTMs may learn SP languages well, this result speaks to the larger point that the judicial use of formal language theory can illuminate the inner workings of RNNs.", "histories": [["v1", "Tue, 16 May 2017 22:13:45 GMT  (4057kb,A)", "http://arxiv.org/abs/1705.05940v1", null], ["v2", "Mon, 3 Jul 2017 02:18:14 GMT  (4057kb,A)", "http://arxiv.org/abs/1705.05940v2", null], ["v3", "Sat, 14 Oct 2017 18:24:08 GMT  (76kb)", "http://arxiv.org/abs/1705.05940v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["enes avcu", "chihiro shibata", "jeffrey heinz"], "accepted": false, "id": "1705.05940"}, "pdf": {"name": "1705.05940.pdf", "metadata": {"source": "CRF", "title": "Subregular Complexity and Deep Learning", "authors": ["Enes Avcu", "Chihiro Shibata"], "emails": ["enesavc@udel.edu", "shibatachh@stf.teu.ac.jp", "heinz@udel.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.05 940v 1 [cs.C L] 16 May 201 7ing how formal language theory can shed light on deep learning. We train naive Long-Term Memory (LSTM) Recurrent Neural Networks (RNs) on six formal languages derived from the classes Strict Local (SL) and Strict Piecewise (SP). These classes are relevant to computational linguistics and are among the simplest in a mathematically well understood hierarchy of subregular classes. SL and SP classes encode local and remote dependencies respectively. Results show that four of the six languages were remarkably well learned, but the revision was probably done with the simplest SL language and subgeneralization using the most complex SP pattern. Although LSTMs were developed to deal with dependencies over long dities, the latter result shows that they stylize naive LSTMs as opposed to local dependencies. While it remains to be seen which of the many variants of the legal language can learn from the larger MSTPs to the larger ones."}, {"heading": "1 Investigating Deep Learning", "text": "This paper suggests that formal language theory provides a systematic way to better understand the types of patterns that deep learning networks can learn (LeCun et al., 2015).The main ideas of this approach are illustrated by experiments showing how well LSTM networks (Hochreiter and Schmidhuber, 1997) can learn various formal languages. This complexity of a language is not based on automation-theoretical yardsticks, such as the size of the minimal deterministic automaton, but has a model-theoretical basis (Enderton, 2001; Rogers et al., 2011).This complexity of a language is not based on automation-theoretical yardsticks, such as the size of the minimal deterministic automaton, but on the type of logic and model needed to specify it (Rogers et al., 2013)."}, {"heading": "2 Motivation and background", "text": "In the 1990s, many studies were aimed at learning formal languages with neural networks. If the goal was to predict the next symbol of a character string drawn from a regular language, first-order RNNs were used (Casey, 1996; Smith, A.W., 1989); the target languages here were based on Reber grammar (Reber, 1967); when the goal was to determine whether a string was grammatical, second-order RNNNs were used (Pollack, 1991; Watrous and Kuhn, 1992; Giles et al., 1992); the target languages were the regular languages Tomita (Tomita, 1982) had studied; later research focused on non-regulatory languages (Schmidhuber et al., 2002; Chalup and Blair, 2003; Prez-Ortiz et al., 2003); however, this line of research has not been pursued more recently to test our knowledge; the reasons for making formal languages of learning as valid today as it was decades ago."}, {"heading": "3 Subregular Complexity", "text": "It is not as if this kind of language exists because it has multiple characterizations in terms of logic, automation, regular expressions, and abstract algebra. Cognitive interpretations of these classes also exist (Rogers et al., 2010). As many authors discuss, these classes are natural because they have multiple characterizations in terms of logic, automation, and regular expressions. Cognitive interpretations of these classes also exist (Rogers and Pullum, 2011).SL is the formal linguistic basis of the ngram models (Jurafsky, 2008) and SP models of phonology (Heinz, 2010)."}, {"heading": "4 The Experiments", "text": "In this study, six formal terms were defined, which is what it is about, namely in the form of two, three, four, five, five, five, five, six, six, six, seven, seven, seven, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, eight, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve"}, {"heading": "5 Results", "text": "Accuracy curve inspection indicates that accuracy has stabilized in all cases well before the 100th epoch. Therefore, accuracy results after the 100th epoch of training are given in Table 1 (accuracy less than 80% is bold; note that the probability level is 50%.) There are three observations that are worth reporting. First, the accuracy of the experiments targeting the SL4, SL8, SP2 and SP4 string sets improved, but then significantly from 10k to 100k. The v100 LSTM significantly deteriorated from 1k to 10k and then improved slightly. These results indicate that the LSTMs, especially the v10 and v30 versions, exceed the data. Third, across all SPM types, the SP2 tests and SP2 tests are incorrect."}, {"heading": "6 Discussion", "text": "What explains the worse results in the SL2 and SP8 experiments? In general, there are two possibilities: the training data was poor and / or the LSTMs were too naive. Consider the possibility that the training was inadequate. Casual inspections revealed that the data generation was not really random because some words were generated disproportionately more than others. Grammar input algorithms can be used to determine the data quality. In this way, it is worthwhile to know how RPNI (Oncina and Garcia, 1992) demonstrably identifies each regular language with positive and negative data when the data is of sufficient quality. Thus, RPNI can tell us if a training set contains all the information needed to distinguish one regular language from any other."}, {"heading": "7 Conclusion", "text": "We believe that the experiments presented here help to show how formal language theory can more clearly show the advantages and disadvantages of different RNN models than tests with real data sets. The main reason for this is that we can control the nature of the target patterns and their complexity, as illustrated here by comparing SL and SP languages that encode local or remote dependencies, and by the k value. In addition, as Test1 and Test2 have shown, we can also control the test data to better understand how the RNNs generalize. Finally, we identified future research goals with less na\u00efve RNNs, more complex form languages, and the integration of grammatical conclusions to confirm the sufficiency of the training data."}], "references": [{"title": "The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction", "author": ["M Casey"], "venue": "Neural computation,", "citeRegEx": "Casey.,? \\Q1996\\E", "shortCiteRegEx": "Casey.", "year": 1996}, {"title": "Chalup and Alan D", "author": ["StephanK"], "venue": "Blair.", "citeRegEx": "Chalup and Blair2003", "shortCiteRegEx": null, "year": 2003}, {"title": "A Mathematical Introduction to Logic", "author": ["Herbert B. Enderton"], "venue": null, "citeRegEx": "Enderton.,? \\Q2001\\E", "shortCiteRegEx": "Enderton.", "year": 2001}, {"title": "Enrique Vidal", "author": ["Pedro Garcia"], "venue": "and Jos\u00e9 Oncina.", "citeRegEx": "Garcia et al.1990", "shortCiteRegEx": null, "year": 1990}, {"title": "G Z Sun", "author": ["C L Giles", "C B Miller", "D Chen", "H H Chen"], "venue": "and Y C Lee.", "citeRegEx": "Giles et al.1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Jan Koutn\u0131\u0301k", "author": ["Klaus Greff", "Rupesh Kumar Srivastava"], "venue": "Bas R. Steunebrink, and J\u00fcrgen Schmidhuber.", "citeRegEx": "Greff et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Anna Kasprzik", "author": ["Jeffrey Heinz"], "venue": "and Timo K\u00f6tzing.", "citeRegEx": "Heinz et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning long-distance phonotactics", "author": ["Jeffrey Heinz"], "venue": "Linguistic Inquiry,", "citeRegEx": "Heinz.,? \\Q2010\\E", "shortCiteRegEx": "Heinz.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S Hochreiter", "J Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Foma: a finite-state compiler and library. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 29\u201332", "author": ["Mans Hulden"], "venue": null, "citeRegEx": "Hulden.,? \\Q2009\\E", "shortCiteRegEx": "Hulden.", "year": 2009}, {"title": "Speech and Language Processing: An Introduction to Natural Language Processing, SpeechRecognition, and Computational Linguistics", "author": ["Jurafsky", "Martin2008] Daniel Jurafsky", "James Martin"], "venue": null, "citeRegEx": "Jurafsky et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jurafsky et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Yoshua Bengio", "author": ["Yann LeCun"], "venue": "and Geoffrey Hinton.", "citeRegEx": "LeCun et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Identifying regular languages in polynomial time. In Advances In Structural And Syntactic Pattern Recognition, Volume 5 Of Series In Machine Perception And Artificial Intelligence, pages", "author": ["Oncina", "Garcia1992] Jose Oncina", "Pedro Garcia"], "venue": null, "citeRegEx": "Oncina et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Oncina et al\\.", "year": 1992}, {"title": "The Induction of Dynamical Recognizers", "author": ["Jordan B. Pollack"], "venue": "Machine Learning,", "citeRegEx": "Pollack.,? \\Q1991\\E", "shortCiteRegEx": "Pollack.", "year": 1991}, {"title": "Douglas Eck", "author": ["Juan Antonio Prez-Ortiz", "Felix A. Gers"], "venue": "and Jrgen Schmidhuber.", "citeRegEx": "Prez.Ortiz et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Implicit learning of artificial grammars", "author": ["Arthur S. Reber"], "venue": "Journal of Verbal Learning and Verbal Behavior,", "citeRegEx": "Reber.,? \\Q1967\\E", "shortCiteRegEx": "Reber.", "year": 1967}, {"title": "Aural pattern recognition experiments and the subregular hierarchy", "author": ["Rogers", "Pullum2011] James Rogers", "Geoffrey Pullum"], "venue": "Journal of Logic, Language and Information,", "citeRegEx": "Rogers et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2011}, {"title": "David Wellcome", "author": ["James Rogers", "Jeffrey Heinz", "Gil Bailey", "Matt Edlefsen", "Molly Visscher"], "venue": "and Sean Wibel.", "citeRegEx": "Rogers et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Dakotah Lambert", "author": ["James Rogers", "Jeffrey Heinz", "Margaret Fero", "Jeremy Hurst"], "venue": "and Sean Wibel.", "citeRegEx": "Rogers et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and D", "author": ["J. Schmidhuber", "F. Gers"], "venue": "Eck.", "citeRegEx": "Schmidhuber et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Piecewise testable events", "author": ["Imre Simon"], "venue": "In Automata Theory and Formal Languages,", "citeRegEx": "Simon.,? \\Q1975\\E", "shortCiteRegEx": "Simon.", "year": 1975}, {"title": "Smith", "author": ["D Zipser"], "venue": "A.W.", "citeRegEx": "Smith. A.W.1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Classifying regular events in symbolic logic", "author": ["Wolfgang Thomas"], "venue": "Journal of Computer and Systems Sciences,", "citeRegEx": "Thomas.,? \\Q1982\\E", "shortCiteRegEx": "Thomas.", "year": 1982}, {"title": "Learning of construction of finite automata from examples using hill-climbing", "author": ["Masaru Tomita"], "venue": "Proc. Fourth Int. Cog. Sci. Conf., pages", "citeRegEx": "Tomita.,? \\Q1982\\E", "shortCiteRegEx": "Tomita.", "year": 1982}, {"title": "Induction of Finite-State Automata Using Second-Order Recurrent Networks", "author": ["Watrous", "Kuhn1992] Raymond L Watrous", "G M Kuhn"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Watrous et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watrous et al\\.", "year": 1992}], "referenceMentions": [], "year": 2017, "abstractText": "This paper presents experiments illustrating how formal language theory can shed light on deep learning. We train naive Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) on six formal languages drawn from the Strictly Local (SL) and Strictly Piecewise (SP) classes. These classes are relevant to computational linguistics and among the simplest in a mathematically well-understood hierarchy of subregular classes. SL and SP classes encode local and long-distance dependencies, respectively. The results show four of the six languages were learned remarkably well, but overfitting arguably occurred with the simplest SL language and undergeneralization with the most complex SP pattern. Even though LSTMs were developed to handle long-distance dependencies, the latter result shows they stymie naive LSTMs in contrast to local dependencies. While it remains to be seen which of the many variants of LSTMsmay learn SP languages well, this result speaks to the larger point that the judicial use of formal language theory can illuminate the inner workings of RNNs. 1 Investigating Deep Learning This paper proposes that formal language theory provides a systematic way to better understand the kinds of patterns deep learning networks (LeCun et al., 2015) are able to learn. The main ideas of this approach are illustrated with experiments testing how well LSTM networks (Hochreiter and Schmidhuber, 1997) can learn different formal languages. The formal languages are drawn from subclasses of the regular languages. These are wellunderstood theoretically to form a complexity hierarchy (McNaughton and Papert, 1971; Rogers et al., 2010; Rogers and Pullum, 2011). This complexity of a language is not based on automata-theoretic measures, such as the size of the minimal deterministic automaton, but instead has a model-theoretic basis (Enderton, 2001). It is determined by the kind of logic and model needed to specify it (Rogers et al., 2013). This approach is not entirely without precedent. Earlier research used formal languages to probe the learning capabilities of neural networks. Section 2 highlights some of this work to make clearer our own motivation and contribution. We are particularly interested in understanding how well LSTMs can learn SP languages. As will be explained in \u00a73, SP languages are simple regular languages which encode only certain types of long-distance dependencies. LSTMs were specifically designed to model long-distance dependencies within temporal sequences more accurately. We compare the performance of LSTMs on SP languages with their performance on learning SL languages, another simple class of regular languages, but one which only encodes local dependencies in sequences. We trained LSTMs on both positive and negative examples of SL and SP languages. We controlled the training and testing data for word length, varied the amount of training data, and varied the LSTMs with respect to vector size. The experimental details are explained in \u00a74. The results, presented in \u00a75, are somewhat unexpected. While LSTMs performed above chance in all of our experiments, LSTMs showed both overfitting and undergeneralization. Specifically, overfitting was observed in the simplest SL language and undergeneralization was observed in the most complex SP language. Implications of these results are explained in \u00a76. These results are unexpected because it is known that LSTMs can learn some context-sensitive formal languages exhibiting long-distance dependencies with uncanny precision (Prez-Ortiz et al., 2003). 2 Motivation and background In the 1990s, many studies aimed to learn formal languages with neural networks. When the aim was to predict the next symbol of a string drawn from a regular language, first-order RNNs were used (Casey, 1996; Smith, A.W., 1989). The target languages here were based on the Reber grammar (Reber, 1967). When the aim was to decide whether a string is grammatical, second-order RNNs were used (Pollack, 1991; Watrous and Kuhn, 1992; Giles et al., 1992). Here the target languages were the regular languages studied by Tomita (Tomita, 1982). Later research targeted nonregular languages (Schmidhuber et al., 2002; Chalup and Blair, 2003; Prez-Ortiz et al., 2003). However, this line of research has not been pursued recently to our knowledge. The reasons for making formal languages the targets of learning are as valid today as they were decades ago. First, the grammars generating the formal languages are known. Therefore training and test data can be generated as desired. Thus, the scientist can run controlled experiments to see whether particular generalizations are reliably acquired under particular training regimens. Importantly, the relative complexity of different formal languages may provide additional insight. If it is found that formal languages of one type are more readily learned than formal languages of another type in some set of experiments then the difference between these classes may be said to meaningfully capture some property that the RNNs in the experiments are incapable of capturing. Subsequent work may lead to proofs and theorems about which LSTMs can reliably infer formal languages from certain classes and which cannot. It may also lead to new network architectures which overcome identified hurdles. The primary difference between the present paper and past research, beyond the development in neural networks, is that the regular languages chosen here are known to have certain properties. The Reber grammars and Tomita languages were not understood in terms of their abstract properties or pattern complexity. Even though Regular NC PT SP LTT LT SL \u272d\u272d\u272d\u272d \u272d\u272d\u272d \u25d7 \u25d7\u25d7 Figure 1: Subregular language classes with inclusion shown left-to-right. subregular distinctions had already been studied (McNaughton and Papert, 1971), it went unrecognized how that branch of computer science could inform neural network learning. 3 Subregular Complexity Figure 1 shows proper inclusion relationships of well-studied classes of subregular languages. The Strictly Local (SL), Locally Testable (LT), and Non-Counting (NC) classes were studied by (McNaughton and Papert, 1971). The Locally Threshold Testable (LTT) class was introduced and studied by (Thomas, 1982). The Piecewise Testable (PT) class was introduced and studied by (Simon, 1975). The Strictly Piecewise (SP) class was studied by (Rogers et al., 2010). As many authors discuss, these classes are natural because they have multiple characterizations in terms of logic, automata, regular expressions, and abstract algebra. Cognitive interpretations of these classes also exist (Rogers and Pullum, 2011; Rogers et al., 2013). SL is the formal language-theoretic basis of ngram models (Jurafsky and Martin, 2008) and SP models aspects of phonology (Heinz, 2010). We only provide characterizations for SL and SP. Let \u03a3 denote a finite set of symbols, the alphabet, and \u03a3\u2217 the set of elements of the free monoid of \u03a3 under concatenation. We refer to these elements both as strings and as words. The ith symbol in word w is denoted wi. Left and right word boundary markers (\u22ca and \u22c9, respectively) are symbols not in \u03a3. A stringset (also called formal language) is a subset of \u03a3\u2217. If u and v are strings, uv denotes their concatenation. For all u, v, w, x \u2208 \u03a3\u2217, if x = uwv then then w is a substring of x. If x \u2208 \u03a3\u2217w1\u03a3 \u2217w2\u03a3 \u2217 . . . wn\u03a3 \u2217 then w is a subsequence of x. A substring (subsequence) of length k is called a k-factor (k-subsequence). Let factork(w) denote the set of substrings of w of length k. Let subseq k (w) denote the set of subsequences of w up to length k. The domains of these functions extend to languages in the normal way. A stringset L is Strictly k-Local (SLk) iff whenever there is a string x of length k \u2212 1 and strings u1, v1, u2, v2 \u2208 \u03a3 \u2217, such that u1xv1, u2xv2 \u2208 L then u1xv2 \u2208 L. We say L is closed under suffix substitution. L is SL if L \u2208 SLk for some k (Rogers and Pullum, 2011). As discussed in (McNaughton and Papert, 1971; Rogers and Pullum, 2011), SLk languages can also be characterized by a finite set of kfactors as follows. A SLk grammar is a set of k-factors G \u2286 factork({\u22ca}\u03a3 \u2217{\u22c9}). Symbols \u22ca and \u22c9 denote left and right word edges, respectively. The language of G is the stringset L(G) = {w | factork(w) \u2286 G}. The grammar G is the set of permissible k-factors. Any k-factor w in factork({\u22ca}\u03a3 \u2217{\u22c9}) which is not in G is thus forbidden and consequently all strings containing w as a substring are not in L(G). Since for each k, factork({\u22ca}\u03a3 \u2217{\u22c9}) is finite, SL stringsets can be defined with grammars that only contain forbidden k-factors as we do in \u00a74. A stringset L is Strictly k-Piecewise (SPk) iff subseq k (w) \u2286 subseq k (L) implies w \u2208 L. L is SP if there is a k such that it belong to SPk; equivalently, L belongs to SP iff L is closed under subsequence (Rogers et al., 2010). SPk stringsets can also be defined with a finite set of k-subsequences (Rogers et al., 2010). In fact the parallel to SLk is near perfect. A SPk grammar is a set of ksubsequences G \u2286 subseq k (\u03a3\u2217). The language of G is the stringset L(G) = {w | subseq k (w) \u2286 G}. The grammar G is the set of permissible k-subsequences. Any k-subsequence w in subseq k (\u03a3\u2217) which is not in G is thus forbidden so strings containing w as a subsequence are not in L(G). Since for each k, subseq k (\u03a3\u2217) is finite, SP stringsets can be defined with grammars containing forbidden k-subsequences as in \u00a74. SL and SP classes form infinite hierarchies of language classes based on k (Rogers and Pullum, 2011; Rogers et al., 2010). In particular for all k \u2208 N, SLk ( SLk+1 and SPk ( SPk+1. Consequently, for any SL (SP) stringset, the smallest k value for which it is SLk (SPk) is another measure of its complexity. Additionally, for given k, the SLk and SPk classes are learnable in the limit from positive data (Garcia et al., 1990; Heinz et al., 2012). These learners are efficient in time and data (de la Higuera, 1997). 4 The Experiments Here we describe the target languages, the training data, the test sets, and the LSTM architecture. In this study, six formal target languages were defined in order for training and testing purposes. Languages are referred to by the class they belong to. SL2, SL4 and SL8 were defined according to the following forbidden substrings {\u22cab, aa, bb, a\u22c9}, {\u22cabbb, aaaa, bbbb, aaa\u22c9} and {\u22cabbbbbbb, aaaaaaaa, bbbbbbbb, aaaaaaa\u22c9}, respectively. SP2, SP4 and SP8 were defined according to the following forbidden subsequences {ab}, {abba} and {abbaabba}, respectively. In each case, we let \u03a3 = {a, b, c, d}. These grammars were implemented as finitestate machines using foma, a publicy available, open-source platform (Hulden, 2009). The number of states in the minimal deterministic automata recognizing SL2, SL4, SL8, SP2, SP4, and SP8 languages were 3, 7, 15, 2, 4 and 8, respectively. Training data was generated with foma. For each language L we generated three training data sets, which we call 1k, 10k, and 100k because they contained 1,000, 10,000, and 100,000 words, respectively. Half of the words in each training set were positive examples (so they belonged to L) and half were negative examples (so they did not belong to L). Training words were between length 1 and 25. For the positive examples there were 20, 200, and 2,000 words of each length. These were generated randomly using foma, so training sets contained duplicates. For the negative examples, we wanted to provide 20, 200, and 2,000 words of each length, respectively. However, as the k value increases, there is no negative data for shorter words since all shorter words belong to L. In this case, we generated 20\u00d7k, 200\u00d7k, and 2,000\u00d7k of words of length k and 20, 200, and 2,000 words for the lengths between k+1 and 25. For each language L and each training regimen T for L, we developed two test sets, which we call Test1 and Test2. Test1 and Test2 contain 1,000, 10,000, or 100,000 words depending on whether T is 1k, 10k, or 100k, respectively. Half of the test words belong to L and half do not. Test1 and Test2 only contain novel words. Novel positive words belong to L but do not belong to the positive examples in T. Novel negative words do not belong to L and do not belong to the negative examples in T. The difference between the two test sets has to do with word length. Test1 words are no longer", "creator": "LaTeX with hyperref package"}}}