{"id": "1705.05983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "AI, Native Supercomputing and The Revival of Moore's Law", "abstract": "Based on Alan Turing's proposition on AI and computing machinery, which shaped Computing as we know it today, I argue that the new AI machine should understand linear algebra natively. In such a machine, a computing unit does not need to keep the legacy of a universal computing core. The data can be delivered to the computing units, and the results can be collected from them through Collective Streaming, reminiscent of Collective Communication in Supercomputing. There is no need for a deep memory hierarchy as in a GPU, nor a fine-grain mesh as in a systolic array.", "histories": [["v1", "Wed, 17 May 2017 02:15:27 GMT  (2483kb)", "http://arxiv.org/abs/1705.05983v1", "17 pages, 13 figures; to be published in IEEE APSIPA Transaction on Signal and Information Processing as an invited paper on Industrial Technology Advances"], ["v2", "Tue, 23 May 2017 16:30:39 GMT  (2490kb)", "http://arxiv.org/abs/1705.05983v2", "17 pages, 13 figures; to be published in IEEE APSIPA Transaction on Signal and Information Processing as an invited paper on Industrial Technology Advances"]], "COMMENTS": "17 pages, 13 figures; to be published in IEEE APSIPA Transaction on Signal and Information Processing as an invited paper on Industrial Technology Advances", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["chien-ping lu"], "accepted": false, "id": "1705.05983"}, "pdf": {"name": "1705.05983.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "At the dawn of the computer age, Alan Turing proposed that the artificial intelligence machine, rather than encompassing many different specific machines, should be a universal digital computer, modelled on human computers that perform calculations in pencil on paper. Convinced that a digital computer would be much faster, more industrious, and more patient than a human, he reckoned that artificial intelligence would be advanced as software. In modern terminology, a universal computer was designed to understand a language known as the Instruction Set Architecture (ISA), and software would be translated into the ISA. Since then, universal computers have become exponentially faster and more energy efficient through Moore's Law, while software has become more sophisticated. Although software has not yet made any machine think, it has fundamentally changed the way we live."}, {"heading": "AI and the Universal Computer", "text": "What kind of computer machinery do we need to bring artificial intelligence to the human level? At the beginning of the twentieth century, one of the founding fathers, Alan Turing, believed that AI could be approached as software running on a universal computer. It was a revolutionary idea, as during his time the term \"computer\" was commonly referred to as a human performing calculations with pencil on paper. Turing referred to a machine as a \"digital computer\" to distinguish it from the human one. In the context of AI, Alan Turing is referred to for his imitation game, or later the Turing Test, in which a machine strives to exhibit intelligence to distinguish itself from a human in the eyes of a questioner."}, {"heading": "The Perfect Marriage between The Universal Computer and Moore\u2019s Law", "text": "In fact, it is as if most people who are able are able to determine for themselves what they want and what they want to do, and that they have to do it. In fact, it is as if they are able to determine for themselves what they want and what they want. In fact, it is as if they are able to determine for themselves what they want to do. In fact, it is as if they are able to determine for themselves what they want and what they want to do. In fact, it is as if they are able to determine for themselves what they have to do. In fact, it is as if they are able to determine for themselves what they want to do."}, {"heading": "The Slowdown of Moore\u2019s Law", "text": "The turning point came in 2005, when the transistors, while doubling in numbers, were neither faster nor more energy efficient than before due to the collapse of Dennard Scaling. Intel wasted little time in burying the race for faster clock speed, and introduced multicore to run multiple \"cores\" in parallel. A universal computer became a CPU core, replaced by a synonym for parallel computing in the CPU community. It was expected that there would be a smooth transition from Neumann to his parallel heir, and the race for the faster clock frequency was replaced by a higher core, starting with a sea of cores, to a sea of cores."}, {"heading": "AI and Moore\u2019s Law", "text": "It is not the first time that we find ourselves in a situation in which we are in a situation in which we are in a situation in which we are no longer able to move; it is the first time that we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are no longer able to move; it is the first time that we are in a situation in which we are in a situation in which we are in a situation in which we are no longer able to move."}, {"heading": "Deep Learning and the New AI Machine", "text": "With deep learning, intelligence is not directly coded by programmers, but indirectly acquired through the extraction of training data sets and then encoded in the various forms of neural networks. Acquisition and manifestation of intelligence can be formulated as calculations governed by a compact set of linear algebra primitives, analogous to those defined in BLAS (Basic Linear Algebra Subprograms), the basic application programming interface used in Supercomputing and High Performance Computing (HPC). AI with deep learning and supercomputing effectively speak the same language as dialectical deviations in numerical precision and slight differences in domain-specific requirements. As already mentioned, the massive and bulky parallelism under the von Neumann paradigm is not suitable to handle the computing machinery and software. On the other hand, the patterns of parallelism can be summarized in domain-specific hardware requirements."}, {"heading": "Why Linear Algebra?", "text": "The basic primitives in deep learning are tensors, high-dimensional data arrays that are used to represent layers of deep neural networks. A deep learning task can be described as a tensor computation graph (Figure 3): A tensor computation graph is effectively a piece of AI software. Tensors can be unfolded into two-dimensional matrices and matrix calculations are executed through matrix cores (see Figure 4). Matrix cores refer to CPU or GPU programs that implement different types of matrix calculations involving many MAC (multiply) operations, such a matrix-centric approach is described in Sharan Chetlur (2014)."}, {"heading": "The TPU and Systolic Arrays", "text": "In the highly anticipated paper \"In-Datacenter Performance Analysis of a Tensor Processing Unit\" (Jouppi, 2017), Google revealed the technical details and performance metrics of the Tensor Processing Unit (TPU), which is built around a matrix multiplication unit based on systolic arrays. What is striking is the decision of the TPU design team to use a systolic array. A systolic array is a specific spatial data stream machine. A processing element (PE) in a systolic array operates in enclosure with its neighbors. Each PE in a systolic array is essentially an MAC unit with some adhesive logic to store and forward data. By comparison, a processing unit (PE) in a mesh-connected parallel processor is a systolic core with a systolic core, while a systolic core is a systolic core with multiple units in a peripheral processor, a systolic core is a systolic core with multiple units in a single front."}, {"heading": "Spatial Dataflow Architecture", "text": "Like a systolic array, the building block of such a generic spatial data flow depends more on the spatial order than on the actual situation, which is often referred to as PE, which is typically a MAC unit with some adhesive logic. Mesh topology is a strikingly popular way to organize PEs, for example, Google's TPU (Jouppi, 2017), the DianNao family (Chen, 2014), MIT's Eyeriss Eyeriss (Sze, 2017). See Figure 6. It seems logical to use a mesh topology to organize the PEs on a 2-dimensional chip when there are many PEs and regularity is desirable. Such an arrangement leads to the following two mesh-centric assumptions: 1. The distance for a piece of data to travel over the time period of a clock is fixed as if between two adjacent PEs, although it may be much farther; 2. It depends on the upstream to adjacent PE."}, {"heading": "Matrix Multiplication According to Supercomputing", "text": "Let's take a look at the most time-consuming part of deep learning: matrix multiplication, which has always been at the center of supercomputing. Modern parallel matrix multiplication performance on modern supercomputers is achieved with the following two major advances: 1. Scalable matrix multiplication algorithms 2. Efficient collective communication with logarithmic overhead Scalable matrix multiplication algorithms See Figure 10 for the demonstration of matrix multiplication in outer products. Calculations are two-dimensional, but both the data and the communication with each other are one-dimensional. The width of a block column and a block row can be a constant and is independent of the number of nodes. On a systolic array, the calculations are also divided into outer products. However, the width of the block column / row must correspond to the systolic distribution superordinate algorithms in order to achieve optimal performance. Otherwise, the problems are poorly documented for universal SUMA systems."}, {"heading": "Native Supercomputing", "text": "It is interesting to compare how matrix multiplication is achieved with a systolic array and a supercomputer, even though they are on completely different scales: One is on the chip and each node is a PE; the other is on the scale of a data center, and each node is a computing cluster (Figure 11). Transmissions are implemented as data transmission to the right, and reductions (a synonym for \"accumulate\" in the terminology of collective communication) are translated as temporary subtotals in a systolic array and gather along the path. Compared to an algorithm like SUMMA, transmissions on a supercomputer happen in two dimensions between the nodes, while reductions are achieved at each node. There is no dependence, i.e. no data flow, but collective communication between the nodes involved. Since the reduction is in place, the number of nodes in both dimensions is independent of the inner dimension of the matrices."}, {"heading": "Why Collective Streaming?", "text": "In many conventional parallel processors, including the GPU, a core as a universal computer not only needs to support many functions other than MAC, but also retrieve data from memory and expects the data to be shared by a memory hierarchy. As a result, it requires a significant investment in space and energy for generic functions, multiple layers of caches, scratch memory, and register files. Collective streaming enables computing units to include only MAC units without a memory hierarchy. In a spatial data stream computer such as a systolic array, a PE still retains the legacy of a core that needs to communicate with other PEs, causing latency and making scaling more difficult. Collective streaming allows orders of magnitude for more MAC units without sacrificing latency. A programmable data stream computer is expected to resolve dependencies between fine-grained data elements. Given the collective dependencies between data elements, it will be a more manageable data-space machine than a data stream machine."}, {"heading": "Conclusion", "text": "As already mentioned, Turing has modeled a universal AI machine that has hired a human computer to do calculations with a pencil on paper. According to Turing, it is the software that tells him step by step what to do to make him think. With deep learning, the software will be like a CEO providing the resources and planning for the workflow to train the machine. It involves transforming the time-consuming tasks through a software stack running on old universal computers into the most efficient computing resources. The new AI machine will be built on the achievements of the previous computing revolution. However, the workhorse will be the computing resources that perform linear algebraic tasks natively. AI was the inspiration behind the earlier computing components as we know them today."}], "references": [{"title": "The Attack of Killer Micros", "author": [], "venue": "Supercomputing", "citeRegEx": "Brooks,? \\Q1989\\E", "shortCiteRegEx": "Brooks", "year": 1989}, {"title": "February). Your Favorite Parallel Algirithms Might Not Be as Fast as You Think", "author": ["D.C. Fisher"], "venue": null, "citeRegEx": "Fisher,? \\Q1988\\E", "shortCiteRegEx": "Fisher", "year": 1988}, {"title": "On communication determinism in parallel HPC applications", "author": ["A.G. Franck Cappello"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "Cappello,? \\Q2010\\E", "shortCiteRegEx": "Cappello", "year": 2010}, {"title": "Dark Silicon and the End of Multicore Scaling", "author": ["E.B. ICCCN. Hadi Esmaeilzadeh"], "venue": null, "citeRegEx": "Esmaeilzadeh,? \\Q2011\\E", "shortCiteRegEx": "Esmaeilzadeh", "year": 2011}, {"title": "In-Datacenter Performance Analysis of a Tensor Processing Unit", "author": [], "venue": null, "citeRegEx": "Jouppi,? \\Q2017\\E", "shortCiteRegEx": "Jouppi", "year": 2017}, {"title": "SUMMA: Scalable Universal Matrix Multiplication Algorithm", "author": [], "venue": null, "citeRegEx": "Geijn,? \\Q1995\\E", "shortCiteRegEx": "Geijn", "year": 1995}, {"title": "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software", "author": ["H. Sutter"], "venue": null, "citeRegEx": "Sutter,? \\Q2005\\E", "shortCiteRegEx": "Sutter", "year": 2005}, {"title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey", "author": ["Sze", "Mar"], "venue": null, "citeRegEx": "Sze et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sze et al\\.", "year": 2017}, {"title": "Programmers' Handbook for the Manchester Electronic Computer", "author": ["A. Turing"], "venue": "Computing Machinery and Intelligence. Mind,", "citeRegEx": "265", "shortCiteRegEx": "265", "year": 1950}], "referenceMentions": [{"referenceID": 6, "context": "Around the same time, programmers were asked to take on the challenge of writing and managing a sea of programs, or \u201cthreads\u201d (Sutter, 2005).", "startOffset": 126, "endOffset": 140}, {"referenceID": 3, "context": "Some prominent research on Dark Silicon, such as \u201cDark Silicon and the End of Multicore Scaling\u201d by Hadi Esmaeilzadeh (2011), confused the physical limitation in semiconductor with that from Amdahl\u2019s law, and prematurely declared the death of parallelism along with the slowdown of Moore\u2019s Law.", "startOffset": 105, "endOffset": 125}, {"referenceID": 4, "context": "8 The TPU and Systolic Arrays In the highly-anticipated paper, \u201cIn-Datacenter Performance Analysis of a Tensor Processing Unit\u201d (Jouppi, 2017), Google disclosed the technical details and performance metrics of the Tensor Processing Unit (TPU).", "startOffset": 128, "endOffset": 142}, {"referenceID": 4, "context": "9 way to organize PEs, for examples, Google\u2019s TPU (Jouppi, 2017), the DianNao family (Chen, 2014), MIT\u2019s Eyeriss (Sze, 2017).", "startOffset": 50, "endOffset": 64}, {"referenceID": 1, "context": "Suppose a problem requires I inputs, K outputs, and T computations, then the asymptotic running time to solve the problem on a ddimensional mesh is given by Fisher\u2019s bound (Fisher, 1988):", "startOffset": 172, "endOffset": 186}, {"referenceID": 0, "context": "Today\u2019s distributed supercomputers are descendants of \u201cKiller Micro\u201d (Brooks, 1989), which were considered aliens invading the land of supercomputing in the early 90s.", "startOffset": 69, "endOffset": 83}], "year": 2017, "abstractText": "Artificial Intelligence (AI) was the inspiration that shaped Computing as we know it today. In this article, I explore why and how AI would continue to inspire Computing and reinvent it when Moore\u2019s Law is running out of steam. At the dawn of Computing, Alan Turing proposed that instead of comprising many different specific machines, the computing machinery for AI should be a universal digital computer, modeled after human computers, which carry out calculations with pencil on paper. Based on the belief that a digital computer would be significantly faster, more diligent and patient than a human, he anticipated that AI would be advanced as software. In modern terminology, a universal computer would be designed to understand a language known as an Instruction Set Architecture (ISA), and software would be translated into the ISA. Since then, universal computers have become exponentially faster and more energy efficient through Moore\u2019s Law, while software has grown more sophisticated. Even though software has not yet made a machine think, it has been changing how we live fundamentally. The computing revolution started when the software was decoupled from the computing machinery. Since the slowdown of Moore\u2019s Law in 2005, the universal computer is no longer improving exponentially in terms of speed and energy efficiency. It has to carry ISA legacy, and cannot be aggressively modified to save energy. Turing\u2019s proposition of AI-as-software is challenged, and the temptation of making many domain-specific AI machines emerges. Thanks to Deep Learning, software can stay decoupled from the computing machinery in the language of linear algebra, which it has in common with Supercomputing. A new universal computer for AI understands such language natively to then become a Native Supercomputer. AI has been and will still be the inspiration for Computing. The quest to make machines think continues amid the slowdown of Moore\u2019s Law. AI might not only maximize the remaining benefits of Moore\u2019s Law, but also revive Moore\u2019s Law beyond current technology.", "creator": "Word"}}}