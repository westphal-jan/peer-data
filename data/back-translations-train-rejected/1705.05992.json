{"id": "1705.05992", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Frame Stacking and Retaining for Recurrent Neural Network Acoustic Model", "abstract": "Frame stacking is broadly applied in end-to-end neural network training like connectionist temporal classification (CTC), and it leads to more accurate models and faster decoding. However, it is not well-suited to conventional neural network based on context-dependent state acoustic model, if the decoder is unchanged. In this paper, we propose a novel frame retaining method which is applied in decoding. The system which combined frame retaining with frame stacking could reduces the time consumption of both training and decoding. Long short-term memory (LSTM) recurrent neural networks (RNNs) using it achieve almost linear training speedup and reduces relative 41\\% real time factor (RTF). At the same time, recognition performance is no degradation or improves sightly on Shenma voice search dataset in Mandarin.", "histories": [["v1", "Wed, 17 May 2017 02:34:27 GMT  (182kb)", "http://arxiv.org/abs/1705.05992v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu tian", "jun zhang", "zejun ma", "yi he", "juan wei"], "accepted": false, "id": "1705.05992"}, "pdf": {"name": "1705.05992.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wj80290}@alibaba-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 5.05 992v 1 [cs.C L] 17 May 2Network training such as connectionist time classification (CTC), and it leads to more accurate models and faster decoding. However, it is not well suited for conventional neural networks based on context-dependent state-dependent acoustic models if the decoder remains unchanged. In this paper, we propose a novel method of frame preservation that is used in decoding. The system, which combines frame preservation with frame stacking, could reduce time consumption in both training and decoding. Long-term short-term memory (LSTM) recurring neural networks (RNNNs) that use it achieve near-linear training acceleration and reduce relative real-time factor (RTF) by 41%. At the same time, the detection performance is not a deterioration or a significant improvement in Shenma voice search data in Mandarin."}, {"heading": "1. INTRODUCTION", "text": "In recent years, the number of those able to reform has multiplied."}, {"heading": "2. FRAME STACKING AND RETAINING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Non-overlapping Frame Stacking", "text": "In traditional acoustic modeling systems, features with frame segmentation are extracted, and they are calculated all fixed steps on fixed frame windows. Frame stacking is a type of frame re-segmentation that stacks timed adjacent frames into a super frame. There are two types of frame stacks that overlap and do not overlap, as shown in Figure 1. They could result in a linear reduction in input frame stacking, and the degree of this depends on the layer step of overlapping one or the frame window from the non-overlapping frame. As the original function is extracted with slider windows, there is no need to reuse sliding windows in the frame stack. Therefore, we prefer non-overlapping frame stacking for RNNNs that has a time storage structure. For speech recognition applications, DNN's input frames always contain context information by packing temporal, sequential, left, and right frames."}, {"heading": "2.2. Frame Retaining in Decoding", "text": "Frame stacking could significantly shorten the training time, and it could also have the same effect on the decoding phase. It has been demonstrated in CTC systems [9]. Since CTC is modelled at the telephone level, the granularity after stacking is still suitable for decoding CTC. But conventional RNs are state modelling and weighted finite state transducer (WFST), accordingly, the granularity is too large to decode them. To maintain the granularity of the decoding, frame maintenance is suggested, as shown in Figure 2.The size of the frame stacking window is referred to as N. After N consecutive frames are extracted in a signal stream, they are stacked to a superframe in the same way as in the training phase. Consequently, the superframe retains the time for N frames with frame maintenance method. The adjacent frame after having similar characteristics to a superframe is stacked to a superframe, the superframe only needs the time for each frame to decode the superframe to have its properties \u2212 in order to represent the superframe."}, {"heading": "2.3. Acoustic Model Trained with Cross-Entropy", "text": "Let x = x1,.., xT denote an input sequence of T-acoustic characteristic vectors, where xt-R N and w denote an output word sequence. The acoustic probability is broken down as follows: p (x-w) = T-t = 1p (xt-lt) p (lt-lt-1), where l1,.. lT is the label sequence determined by existing models. In hybrid decoding, the emission probability of HMM could be represented as p (xt-lt) = p (lt-xt) p (xt) / p (lt). \u2212 The posterior label is given by the output of a neural network acoustic model, and it could be calculated using a context of N-frames with frame stacking. The label before p (lt) is counted by the labeling of the existing model. \u2212 The acoustic model of the neural network is first trained to include the cross-sectional component (followed by the loss of the loxil)."}, {"heading": "2.4. Sequence Discriminative Training", "text": "CE provides a kind of frame-by-frame discriminatory training criterion, but it is not sufficient for speech recognition, which is a sequence problem. Sequence discriminatory training using the state minimum bayes risk (sMBR) has shown that it further improves the performance of neural networks first trained with CE [14, 15]. The model first trained with CE loss is frame-accurate, and it is further trained with sMBR to obtain sequence-consistent accuracy. Frame stacking and retention are also used in sMBR training, and it receives near-linear acceleration. Furthermore, based on the frame-by-frame model, only part of the dataset is required for sMBR training."}, {"heading": "3. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experiments Setup", "text": "The data set is generated from anonymous online searches in Mandarin, and the sampling rate of all audio files recorded by mobile phones is 16 kHz. This data set consists of many different conditions, such as different noise, even low signal-to-noise, babbling, dialects, accents, hesitations, etc. The data set is separated into training set, validation set, and test set, and the quantity of which is shown in Table 1. The three sets are divided by speaker to avoid utterances by the same speaker appearing in three sentences at the same time. Shenma language search test sets are called Shenma tests.LSTM RNNNs surpass traditional voice recognition RNNNs, particularly low LSTM RNs, because of their long range dependencies, which are more accurate for system recognition."}, {"heading": "FS FR CER", "text": "The 5 gram language model is used in the decoder, and the vocabulary is as large as 76,000. It has shown that block-wise update filtering (BMUF) exceeds the traditional method of averaging models and is used in the synchronization phase [17]. Block learning rate and block impulse are 1 and 0.9. After synchronization with the BMUF, the exponential moving average method (EMA) continues to update the model without interference [18]. The training system is based on the MPI-based HPC cluster, which uses 8 GPUs. Each GPU processes unoverlapped subsets from the entire large dataset in parallel.Local models from distributed workers synchronize with each other in a decentralized manner."}, {"heading": "3.2. Results", "text": "Frame stacking reduces the number of input frames and thus results in near-linear acceleration of the workout. However, if the model is applied directly in the decoder, it results in large CER degradation, because the modeling duration does not match. The decoding network is generated in such a way that it is suitable for the original modeling duration. N frames correspond only to one input feature vector of the decoder for frame stacking, while N frames for the original modeling correspond to N frames. Therefore, the frame maintenance in the decoder may correspond to the number of input feature vectors and frames. We refer to the number of non-overlapping stacked frames as FS and the times of a superframe as FR. As shown in Table 2, if a superframe is stacked by 3 frames in 4-layer LSTM models, FR = 1 increases the 415% CER accuracy, and the other modeling duration of the work as FR. If this only differs from the performance of the superframe by 1, then the superframe stacking duration cannot be maintained."}, {"heading": "FS FR CER RTF", "text": "CERs and RTFs of 4-layer LSTM models with different numbers of stacked frames and matching frame conservation are shown in Table 3. Neighboring frames have similar properties, so no information is omitted in the stacking process. It does not reduce detection performance and even improves it, as shown in Table 3. For our system, it is optimal that both FS and FR are set as 3. It reduces relative RTF by 41% and improves accuracy slightly."}, {"heading": "4. CONCLUSION", "text": "In this paper, we propose that frame stacking is used in conventional neural networks with frame stacking. Frame stacking and retention parameters should be the same so that they have the same modeling duration. It results in near-linear training acceleration and faster decoding, while the performance of speech recognition does not decrease. Unidirectional LSTM models are trained to verify on a large scale. RTF reduces relative speech recognition by 41% and character accuracy significantly improves compared to the use of frame stacking and reaining."}, {"heading": "5. REFERENCES", "text": "\"It is not like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"It is like if.\" \"\" It is like if. \"\" \"It is like if.\" \"\" It is like if. \"\" It is like if. \"\" \"It is like if.\" \"\" \"It is like if.\". \"\" \"\" \"It is like if.\". \"\" \"\" It is like if. \".\" \"\" \"It is like if.\". \"\" \"\" \"It is like if..\" \"\" \"\" \"It is like if...............\" \"\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \"It is like if.....\" \"\" \"\" \"\" \"\" \"\" It is like if................. \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"It is like...............\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".....\" \"\" \"\" \"\" \"\" \"\" \"\"...... \"\" \"\" \"\" \"\" \"\"..... \"\" \"\" \"\" \"\"...... \"\" \"\" \"\" \"\" \"....\" \"\" \"\" \"\" \"\"..... \"\" \"....\" \"\" \"\" \"\" \"\"..... \"\" \"\".... \"\" \"\" \"\" \"\" \"...........\" \"\" \"\" \"\" \"\"... \"\" \"\" \"............\" \"\" \"\" \"\" \"\" \"\" \".........\" \"\" \""}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "AndrewW Senior", "Fran\u00e7oise Beaufays"], "venue": "IN- TERSPEECH, 2014, pp. 338\u2013342.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimizing bottleneck features for lvcsr", "author": ["Frantisek Grezl", "Petr Fousek"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4729\u20134732.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Phoneme recognition using spectral envelope and modulation frequency features", "author": ["Samuel Thomas", "Sriram Ganapathy", "Hynek Hermansky"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 4453\u20134456.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiframe deep neural networks for acoustic modeling", "author": ["Vincent Vanhoucke", "Matthieu Devin", "Georg Heigold"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7582\u20137585.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature frame stacking in rnn-based tandem asr systems-learned vs. predefined context", "author": ["Martin W\u00f6llmer", "Bj\u00f6rn W Schuller", "Gerhard Rigoll"], "venue": "INTER- SPEECH, 2011, pp. 1233\u20131236.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modelling with cd-ctcsmbr lstm rnns", "author": ["Ha\u015fim Sak", "F\u00e9lix de Chaumont Quitry", "Tara Sainath", "Kanishka Rao"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Context dependent phone models for lstm rnn acoustic modelling", "author": ["Andrew Senior", "Ha\u015fim Sak", "Izhak Shafran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4585\u20134589.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Lower frame rate neural network acoustic models", "author": ["Golan Pundak", "Tara N Sainath"], "venue": "Interspeech 2016, pp. 22\u201326, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan Irsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering", "author": ["Kai Chen", "Qiang Huo"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Exponential moving average model in parallel speech recognition training", "author": ["Tian Xu", "Zhang Jun", "Ma Zejun", "He Yi", "Wei Juan"], "venue": "arXiv preprint arXiv:1703.01024, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "In the last few years, deep neural networks (DNNs) combined with hiddenMarkovmodels (HMMs) have been widely employed in acoustic modeling for large vocabulary speech recognition [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "More recently, Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, have been shown to outperform DNNs [2, 3, 4].", "startOffset": 131, "endOffset": 140}, {"referenceID": 2, "context": "More recently, Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, have been shown to outperform DNNs [2, 3, 4].", "startOffset": 131, "endOffset": 140}, {"referenceID": 3, "context": "More recently, Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, have been shown to outperform DNNs [2, 3, 4].", "startOffset": 131, "endOffset": 140}, {"referenceID": 4, "context": "Frame stacking is a effective way that DNNs could learn past and future context knowledge [5, 6, 7].", "startOffset": 90, "endOffset": 99}, {"referenceID": 5, "context": "Frame stacking is a effective way that DNNs could learn past and future context knowledge [5, 6, 7].", "startOffset": 90, "endOffset": 99}, {"referenceID": 6, "context": "Frame stacking is a effective way that DNNs could learn past and future context knowledge [5, 6, 7].", "startOffset": 90, "endOffset": 99}, {"referenceID": 7, "context": "Though RNNs are able to remember long-term information, frame stacking could also provide useful contextual information [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "However, the neural networks combined with connectionist temporal classification (CTC) criterion gives the frame stacking rebirth [9, 10].", "startOffset": 130, "endOffset": 137}, {"referenceID": 9, "context": "However, the neural networks combined with connectionist temporal classification (CTC) criterion gives the frame stacking rebirth [9, 10].", "startOffset": 130, "endOffset": 137}, {"referenceID": 10, "context": "Connectionist temporal classification (CTC) criterion provides a mechanism to learn an neural network while mapping a input frame sequence to a output label sequence [11].", "startOffset": 166, "endOffset": 170}, {"referenceID": 11, "context": "CTC-LSTM acoustic models using context dependent phones (CD-phones) perform as well as conventional models [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "It is an intuitional way to remodel HMM structure in order to match the modeling unit, and decoding network is needed to rebuild correspondingly [13].", "startOffset": 145, "endOffset": 149}, {"referenceID": 8, "context": "It has been demonstrated in CTC systems [9].", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "Sequence discriminative training using state-level minimum bayes risk (sMBR) has shown to further improve performance of neural networks first trained with CE [14, 15].", "startOffset": 159, "endOffset": 167}, {"referenceID": 14, "context": "Sequence discriminative training using state-level minimum bayes risk (sMBR) has shown to further improve performance of neural networks first trained with CE [14, 15].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [16, 10].", "startOffset": 185, "endOffset": 193}, {"referenceID": 9, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [16, 10].", "startOffset": 185, "endOffset": 193}, {"referenceID": 16, "context": "It has shown that blockwisemodel-update filtering (BMUF) outperforms traditional model averaging method, and it is utilized at the synchronization phase [17].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "After synchronizing with BMUF, exponential moving average (EMA) method further updates the model in non-interference way [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "Decentralized method makes full use of computing resource, and we employ the MPI-based mesh AllReduce method [18].", "startOffset": 109, "endOffset": 113}], "year": 2017, "abstractText": "Frame stacking is broadly applied in end-to-end neural network training like connectionist temporal classification (CTC), and it leads to more accurate models and faster decoding. However, it is not well-suited to conventional neural network based on context-dependent state acoustic model, if the decoder is unchanged. In this paper, we propose a novel frame retaining method which is applied in decoding. The system which combined frame retaining with frame stacking could reduces the time consumption of both training and decoding. Long short-term memory (LSTM) recurrent neural networks (RNNs) using it achieve almost linear training speedup and reduces relative 41% real time factor (RTF). At the same time, recognition performance is no degradation or improves sightly on Shenma voice search dataset in Mandarin.", "creator": "LaTeX with hyperref package"}}}