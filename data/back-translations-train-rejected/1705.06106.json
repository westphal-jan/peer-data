{"id": "1705.06106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models", "abstract": "We present a semi-supervised way of training a character-based encoder-decoder recurrent neural network for morphological reinflection, the task of generating one inflected word form from another. This is achieved by using unlabeled tokens or random string as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.9% improvement over state-of-the-art baselines for 8 different languages.", "histories": [["v1", "Wed, 17 May 2017 11:48:15 GMT  (192kb,D)", "http://arxiv.org/abs/1705.06106v1", null], ["v2", "Fri, 21 Jul 2017 13:02:20 GMT  (460kb,D)", "http://arxiv.org/abs/1705.06106v2", "Accepted at SCLeM 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katharina kann", "hinrich sch\\\"utze"], "accepted": false, "id": "1705.06106"}, "pdf": {"name": "1705.06106.pdf", "metadata": {"source": "CRF", "title": "Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models", "authors": ["Katharina Kann"], "emails": ["kann@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "In this context, it has to be said that the people we have mentioned are people who are in a position to behave as if they were in a position, in a position to be in a position."}, {"heading": "2 Model Description", "text": "The protocol probability for joint training on the tasks of MRI and autocoding is: ar Xiv: 170 5.06 106v 1 [cs.C L] 17 May 201 7L (\u03b8) = \u2211 (l, src, trg) Every day (ftrg (l) | fsrc (l))) (1) + w \u2211 W log p\u03b8 (w | w), where T is the MRI training data, w is a weighting factor, fsrc (l) and ftrg (l) are the source form and target form of a given problem l, or W is a set of words in the language of the system. Parameters are shared across the two tasks, resulting in a portion of information. We obtain this by giving our model data from both sets simultaneously and marking each sample with a task-specific input character x."}, {"heading": "3 Experiments", "text": "This year it is more than ever before."}, {"heading": "4 Analyses", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Amount of Unlabeled Data", "text": "The resulting accuracy for Arabic and German can be seen in Figure 2. The other languages behave similarly to German. Performance loss in reducing training data varies greatly between languages depending on how regular and therefore \"easy to learn\" they are. As for the quantity of unlabeled samples, it seems that, although in individual cases different ratios are slightly better, the use of 4 unlabeled samples for each labeled sample seems to be a good general rule. The only exception is Arabic in the 132 environment, where the use of half as many unlabeled samples yields much better results. We explain this with the Semitic language, which is templative. Since words in Arabic paradigms do not share a coherent tribe, we expect that indicating the model has too much bias to copy them, in this case achieving significantly better results if we do not even use this ratio for a low resource ratio."}, {"heading": "4.2 Autoencoding of Random Strings", "text": "This suggests that any random combination of characters from the alphabet of the language could be autocoded to improve performance in resource-poor environments. To verify this, we train the model on new sets of data with 132 of the labeled samples from SIGMORPHON task 3 and the optimal number of unlabeled samples for each language, cf. \u00a7 4.1. However, the unlabeled samples are now random strings ranging in length from 3 to 20. All models are trained as before. Accuracy on the official test set is shown in Table 2 and compared to (i) training without unlabeled samples and (i) data improved by corpus words. Several aspects of the results stand out. Firstly, for Arabic, the gap to performance with corpus words is the largest, showing that the tendency of languages to copy the strain plays an important role in spotting. Secondly, for some languages, the performance gains for corpses and this random sample are very comparable."}, {"heading": "5 Related Work", "text": "However, several MRI systems have been developed for the SIGMORPHON 2016 Shared Task (Cotterell et al., 2016), e.g. (Nicolai et al., 2016; Taji et al., 2016; Kann und Sch\u00fctzten, 2016; Aharoni et al., 2016; O \ufffd stling, 2016). Encoder decoders Neural Networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) work best, so we extend them for this work. Previous work on completing the paradigm (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013) is rarer, e.g. (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally monitored and unattended methods for morphology."}, {"heading": "6 Conclusion", "text": "We presented a method for semi-supervised training of a state-of-the-art low-resource MRI model, using words from an unmarked corpus. We found that the best ratio between marked and unmarked data depends on the morphological typology of the language. Finally, we showed that automatic encoding of random strings also improves performance, in some languages as well as the use of corpus words."}, {"heading": "Acknowledgments", "text": "We thank Siemens for its financial support of this research."}], "references": [{"title": "Improving sequence to sequence learning for morphological inflection generation: The BIU-MIT systems for the SIGMORPHON 2016 shared task for morphological reinflection", "author": ["Roee Aharoni", "Yoav Goldberg", "Yonatan Belinkov."], "venue": "SIGMORPHON.", "citeRegEx": "Aharoni et al\\.,? 2016", "shortCiteRegEx": "Aharoni et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1606.04596 .", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "SSST .", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The SIGMORPHON 2016 shared task\u2014 morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "SIGMORPHON.", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "TSLP 4(1):3.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Graphical models over multiple strings", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "EMNLP.", "citeRegEx": "Dreyer and Eisner.,? 2009", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2009}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "EMNLP.", "citeRegEx": "Dreyer and Eisner.,? 2011", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2011}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "NAACL.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "NAACL.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Unsupervised learning of the morphology of a natural language", "author": ["John Goldsmith."], "venue": "Computational linguistics 27(2):153\u2013198.", "citeRegEx": "Goldsmith.,? 2001", "shortCiteRegEx": "Goldsmith.", "year": 2001}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "author": ["Thanh-Le Ha", "Jan Niehues", "Alexander Waibel."], "venue": "arXiv preprint arXiv:1611.04798 .", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "One-shot neural cross-lingual transfer for paradig completion", "author": ["Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "ACL.", "citeRegEx": "Kann et al\\.,? 2017", "shortCiteRegEx": "Kann et al\\.", "year": 2017}, {"title": "MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "ACL.", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak."], "venue": "NAACL.", "citeRegEx": "Nicolai et al\\.,? 2015", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "Morphological reinflection via discriminative string transduction", "author": ["Garrett Nicolai", "Bradley Hauer", "Adam St Arnaud", "Grzegorz Kondrak."], "venue": "SIGMORPHON.", "citeRegEx": "Nicolai et al\\.,? 2016", "shortCiteRegEx": "Nicolai et al\\.", "year": 2016}, {"title": "Morphological reinflection with convolutional neural networks", "author": ["Robert \u00d6stling."], "venue": "SIGMORPHON.", "citeRegEx": "\u00d6stling.,? 2016", "shortCiteRegEx": "\u00d6stling.", "year": 2016}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Hoifung Poon", "Colin Cherry", "Kristina Toutanova."], "venue": "NAACL.", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Unsupervised pretraining for sequence to sequence learning", "author": ["Prajit Ramachandran", "Peter J Liu", "Quoc V Le."], "venue": "arXiv preprint arXiv:1611.02683 .", "citeRegEx": "Ramachandran et al\\.,? 2016", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2016}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "ACL.", "citeRegEx": "Snyder and Barzilay.,? 2008", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The columbia university - new york university abu dhabi SIGMORPHON 2016 morphological reinflection shared task submission", "author": ["Dima Taji", "Ramy Eskander", "Nizar Habash", "Owen Rambow."], "venue": "SIGMORPHON.", "citeRegEx": "Taji et al\\.,? 2016", "shortCiteRegEx": "Taji et al\\.", "year": 2016}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "Journal of Machine Learning", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Recently, there have been some advances on the topic, motivated by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016).", "startOffset": 129, "endOffset": 153}, {"referenceID": 10, "context": "Neural sequence-to-sequence models, specifically attention-based encoder-decoder models, outperformed all other approaches (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016).", "startOffset": 123, "endOffset": 169}, {"referenceID": 14, "context": "Neural sequence-to-sequence models, specifically attention-based encoder-decoder models, outperformed all other approaches (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016).", "startOffset": 123, "endOffset": 169}, {"referenceID": 24, "context": "We achieve this by treating unlabeled words as training samples for an autoencoding (Vincent et al., 2010) task and multi-task training (cf.", "startOffset": 84, "endOffset": 106}, {"referenceID": 14, "context": "Following (Kann and Sch\u00fctze, 2016), we employ a neural encoder-decoder model.", "startOffset": 10, "endOffset": 34}, {"referenceID": 14, "context": "For the input of the encoder, we adapt the format by Kann and Sch\u00fctze (2016), but modify it to be able to handle unlabeled data: Given the set of morphological subtags M each target tag is composed of (e.", "startOffset": 53, "endOffset": 77}, {"referenceID": 4, "context": "work (GRU) (Cho et al., 2014b), i.", "startOffset": 11, "endOffset": 30}, {"referenceID": 1, "context": "A detailed description of the model can be found in Bahdanau et al. (2015). 3 Experiments", "startOffset": 52, "endOffset": 75}, {"referenceID": 5, "context": "We experiment on the Task 3 dataset of the SIGMORPHON 2016 shared task on MRI (Cotterell et al., 2016).", "startOffset": 78, "endOffset": 102}, {"referenceID": 14, "context": "We mainly adopt the hyperparameters of (Kann and Sch\u00fctze, 2016).", "startOffset": 39, "endOffset": 63}, {"referenceID": 25, "context": "Embedding are 300dimensional, the size of all hidden layers is 100 and for training we use ADADELTA (Zeiler, 2012) with a batch size of 20.", "startOffset": 100, "endOffset": 114}, {"referenceID": 5, "context": "The second baseline is the official baseline 2016 SIGMORPHON shared task baseline (SIG16) (Cotterell et al., 2016), which is similar in spirit to the system described by Nicolai et al.", "startOffset": 90, "endOffset": 114}, {"referenceID": 5, "context": "The selection of operations is made by an averaged perceptron, using the binary features described in (Cotterell et al., 2016).", "startOffset": 102, "endOffset": 126}, {"referenceID": 5, "context": "The second baseline is the official baseline 2016 SIGMORPHON shared task baseline (SIG16) (Cotterell et al., 2016), which is similar in spirit to the system described by Nicolai et al. (2015). The system treats the prediction of edit operations to be performed on the input string as a sequential decision-making problem, greedily choosing each edit action given the previously chosen actions.", "startOffset": 91, "endOffset": 192}, {"referenceID": 14, "context": "Table 2: Accuracies for MED (Kann and Sch\u00fctze (2016)), MED+corpus and MED+random.", "startOffset": 29, "endOffset": 53}, {"referenceID": 5, "context": "For the SIGMORPHON 2016 shared task (Cotterell et al., 2016), multiple MRI systems have been developed, e.", "startOffset": 36, "endOffset": 60}, {"referenceID": 16, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 23, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 14, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 0, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 17, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 3, "context": "Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work.", "startOffset": 32, "endOffset": 98}, {"referenceID": 22, "context": "Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work.", "startOffset": 32, "endOffset": 98}, {"referenceID": 1, "context": "Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work.", "startOffset": 32, "endOffset": 98}, {"referenceID": 10, "context": "Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013).", "startOffset": 45, "endOffset": 115}, {"referenceID": 15, "context": "Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013).", "startOffset": 45, "endOffset": 115}, {"referenceID": 9, "context": "Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013).", "startOffset": 45, "endOffset": 115}, {"referenceID": 7, "context": ", (Dreyer and Eisner, 2009).", "startOffset": 2, "endOffset": 27}, {"referenceID": 8, "context": ", Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008).", "startOffset": 140, "endOffset": 211}, {"referenceID": 18, "context": ", Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008).", "startOffset": 140, "endOffset": 211}, {"referenceID": 20, "context": ", Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008).", "startOffset": 140, "endOffset": 211}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al.", "startOffset": 32, "endOffset": 550}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al.", "startOffset": 32, "endOffset": 571}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language.", "startOffset": 32, "endOffset": 909}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al.", "startOffset": 32, "endOffset": 1310}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al.", "startOffset": 32, "endOffset": 1333}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al. (2011); Ramachandran et al.", "startOffset": 32, "endOffset": 1355}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al. (2011); Ramachandran et al. (2016). Those approaches differ from ours, due to a fundamental difference between the two tasks: For MRI, the source vocabulary and the target vocabulary are mostly the same.", "startOffset": 32, "endOffset": 1383}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al. (2011); Ramachandran et al. (2016). Those approaches differ from ours, due to a fundamental difference between the two tasks: For MRI, the source vocabulary and the target vocabulary are mostly the same. This makes it intuitive for MRI to train the final model jointly on MRI and autoencoding. One case where this has been done is the work by Ha et al. (2016). However, they apply their method exclusively to MT.", "startOffset": 32, "endOffset": 1708}], "year": 2017, "abstractText": "We present a semi-supervised way of training a character-based encoderdecoder recurrent neural network for morphological reinflection, the task of generating one inflected word form from another. This is achieved by using unlabeled tokens or random string as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.9% improvement over state-of-the-art baselines for 8 different languages.", "creator": "LaTeX with hyperref package"}}}